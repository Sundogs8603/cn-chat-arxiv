<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01345</link><description>&lt;p&gt;
&#36339;&#36807;$\textbackslash n$: &#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#20449;&#24687;&#29702;&#35299;&#19982;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;LVLMs&#20173;&#28982;&#38754;&#20020;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#29983;&#25104;&#19982;&#35270;&#35273;&#20449;&#24687;&#20013;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#30456;&#20851;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#35748;&#20026;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30830;&#23450;&#20102;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#65288;'$\textbackslash n\textbackslash n$'&#65289;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#21363;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#20869;&#23481;&#32463;&#24120;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#35821;&#20041;&#25913;&#21464;&#12290;&#36825;&#31181;&#27169;&#24335;&#20351;&#24471;&#27169;&#22411;&#25512;&#26029;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21518;&#30340;&#20869;&#23481;&#24212;&#26126;&#26174;&#19981;&#21516;&#20110;&#21069;&#38754;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#20855;&#26377;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#30340;&#24369;&#30417;&#30563;AI&#38169;&#35823;&#20462;&#27491;&#22120;&#26469;&#22788;&#29702;AI&#38169;&#35823;&#12290;&#20462;&#27491;&#22120;&#36890;&#36807;&#25209;&#20934;&#25110;&#25298;&#32477;&#24213;&#23618;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#27010;&#29575;&#30028;&#38480;&#20445;&#35777;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00899</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#23398;&#20064;&#22120;&#23454;&#29616;&#20855;&#26377;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#30340;AI&#38169;&#35823;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Learners for Correction of AI Errors with Provable Performance Guarantees
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00899
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#20351;&#29992;&#20855;&#26377;&#21487;&#35777;&#26126;&#24615;&#33021;&#20445;&#35777;&#30340;&#24369;&#30417;&#30563;AI&#38169;&#35823;&#20462;&#27491;&#22120;&#26469;&#22788;&#29702;AI&#38169;&#35823;&#12290;&#20462;&#27491;&#22120;&#36890;&#36807;&#25209;&#20934;&#25110;&#25298;&#32477;&#24213;&#23618;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#27010;&#29575;&#30028;&#38480;&#20445;&#35777;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;AI&#38169;&#35823;&#65292;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#20808;&#39564;&#24615;&#33021;&#20445;&#35777;&#30340;&#24369;&#30417;&#30563;AI&#38169;&#35823;&#20462;&#27491;&#22120;&#12290;&#36825;&#20123;AI&#20462;&#27491;&#22120;&#26159;&#36741;&#21161;&#26144;&#23556;&#65292;&#20854;&#20316;&#29992;&#26159;&#36890;&#36807;&#25209;&#20934;&#25110;&#25298;&#32477;&#20197;&#35843;&#33410;&#20043;&#21069;&#26500;&#24314;&#30340;&#24213;&#23618;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;&#25298;&#32477;&#19968;&#20010;&#20915;&#31574;&#21487;&#20197;&#29992;&#20316;&#24314;&#35758;&#25918;&#24323;&#20570;&#20986;&#20915;&#31574;&#30340;&#20449;&#21495;&#12290;&#35813;&#24037;&#20316;&#30340;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#37325;&#28857;&#26159;&#36890;&#36807;&#23545;&#38169;&#35823;&#20915;&#31574;&#30340;&#27010;&#29575;&#30028;&#38480;&#25552;&#20379;&#36825;&#20123;&#26032;&#30340;AI&#20462;&#27491;&#22120;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#36825;&#20123;&#30028;&#38480;&#26159;&#20998;&#24067;&#19981;&#21487;&#30693;&#30340;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#23545;&#25968;&#25454;&#32500;&#24230;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#31034;&#20363;&#35828;&#26126;&#20102;&#35813;&#26694;&#26550;&#22914;&#20309;&#24212;&#29992;&#20110;&#25913;&#21892;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new methodology for handling AI errors by introducing weakly supervised AI error correctors with a priori performance guarantees. These AI correctors are auxiliary maps whose role is to moderate the decisions of some previously constructed underlying classifier by either approving or rejecting its decisions. The rejection of a decision can be used as a signal to suggest abstaining from making a decision. A key technical focus of the work is in providing performance guarantees for these new AI correctors through bounds on the probabilities of incorrect decisions. These bounds are distribution agnostic and do not rely on assumptions on the data dimension. Our empirical example illustrates how the framework can be applied to improve the performance of an image classifier in a challenging real-world task where training data are scarce.
&lt;/p&gt;</description></item><item><title>HarmBench&#26159;&#19968;&#20010;&#20026;&#33258;&#21160;&#32418;&#38431;&#21644;&#24378;&#22823;&#25298;&#32477;&#35774;&#35745;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;18&#31181;&#32418;&#38431;&#26041;&#27861;&#21644;33&#20010;&#30446;&#26631;LLM&#21644;&#38450;&#24481;&#30340;&#27604;&#36739;&#65292;&#24471;&#20986;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;LLM&#22312;&#21508;&#31181;&#25915;&#20987;&#19979;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04249</link><description>&lt;p&gt;
HarmBench&#65306;&#29992;&#20110;&#33258;&#21160;&#32418;&#38431;&#21644;&#24378;&#22823;&#25298;&#32477;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04249
&lt;/p&gt;
&lt;p&gt;
HarmBench&#26159;&#19968;&#20010;&#20026;&#33258;&#21160;&#32418;&#38431;&#21644;&#24378;&#22823;&#25298;&#32477;&#35774;&#35745;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;18&#31181;&#32418;&#38431;&#26041;&#27861;&#21644;33&#20010;&#30446;&#26631;LLM&#21644;&#38450;&#24481;&#30340;&#27604;&#36739;&#65292;&#24471;&#20986;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;LLM&#22312;&#21508;&#31181;&#25915;&#20987;&#19979;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#32418;&#38431;&#20855;&#26377;&#21457;&#29616;&#21644;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#28982;&#32780;&#35813;&#39046;&#22495;&#32570;&#20047;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#26469;&#20005;&#26684;&#35780;&#20272;&#26032;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;HarmBench&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#32418;&#38431;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#32418;&#38431;&#35780;&#20272;&#20013;&#30830;&#23450;&#20102;&#20960;&#20010;&#20197;&#21069;&#26410;&#32771;&#34385;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#29305;&#24615;&#65292;&#24182;&#31995;&#32479;&#22320;&#35774;&#35745;&#20102;HarmBench&#20197;&#28385;&#36275;&#36825;&#20123;&#26631;&#20934;&#12290;&#20351;&#29992;HarmBench&#65292;&#25105;&#20204;&#23545;18&#31181;&#32418;&#38431;&#26041;&#27861;&#21644;33&#20010;&#30446;&#26631;LLM&#21644;&#38450;&#24481;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#27604;&#36739;&#65292;&#24471;&#21040;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#22312;&#21508;&#31181;&#25915;&#20987;&#19979;&#30340;&#31283;&#20581;&#24615;&#65292;&#23637;&#31034;&#20102;HarmBench&#22914;&#20309;&#20419;&#36827;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#20849;&#21516;&#24320;&#21457;&#12290;&#25105;&#20204;&#22312;https://github.com/centerforaisafety/HarmBench&#19978;&#24320;&#28304;&#20102;HarmBench&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;Mamba&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;Transformer&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;SSMs&#22312;&#26631;&#20934;&#22238;&#24402;&#20219;&#21153;&#20013;&#19982;Transformer&#24615;&#33021;&#30456;&#24403;&#65292;&#22312;&#31232;&#30095;&#22855;&#20598;&#23398;&#20064;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#22312;&#28041;&#21450;&#38750;&#26631;&#20934;&#26816;&#32034;&#21151;&#33021;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.04248</link><description>&lt;p&gt;
Mamba&#33021;&#23398;&#20064;&#22914;&#20309;&#23398;&#20064;&#21527;&#65311;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#20219;&#21153;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;Mamba&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;Transformer&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;SSMs&#22312;&#26631;&#20934;&#22238;&#24402;&#20219;&#21153;&#20013;&#19982;Transformer&#24615;&#33021;&#30456;&#24403;&#65292;&#22312;&#31232;&#30095;&#22855;&#20598;&#23398;&#20064;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#22312;&#28041;&#21450;&#38750;&#26631;&#20934;&#26816;&#32034;&#21151;&#33021;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#35821;&#35328;&#24314;&#27169;&#20013;&#26367;&#20195;Transformer&#32593;&#32476;&#30340;&#36873;&#25321;&#65292;&#36890;&#36807;&#24341;&#20837;&#38376;&#25511;&#12289;&#21367;&#31215;&#21644;&#22522;&#20110;&#36755;&#20837;&#30340;&#20196;&#29260;&#36873;&#25321;&#26469;&#32531;&#35299;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#20108;&#27425;&#25104;&#26412;&#12290;&#34429;&#28982;SSMs&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#65292;&#20294;&#19982;Transformer&#30456;&#27604;&#65292;&#23427;&#20204;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#26041;&#38754;&#20173;&#28982;&#26159;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;SSMs&#30340;ICL&#24615;&#33021;&#65292;&#30528;&#37325;&#30740;&#31350;&#20102;Mamba&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#19982;Transformer&#27169;&#22411;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26631;&#20934;&#22238;&#24402;ICL&#20219;&#21153;&#20013;&#65292;SSMs&#30340;&#34920;&#29616;&#19982;Transformer&#30456;&#24403;&#65292;&#32780;&#22312;&#31232;&#30095;&#22855;&#20598;&#23398;&#20064;&#31561;&#20219;&#21153;&#20013;&#36229;&#36807;&#20102;Transformer&#12290;&#28982;&#32780;&#65292;&#22312;&#28041;&#21450;&#38750;&#26631;&#20934;&#26816;&#32034;&#21151;&#33021;&#30340;&#20219;&#21153;&#20013;&#65292;SSMs&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;\variant&#65292;&#23427;&#23558;Mamba&#19982;&#27880;&#24847;&#21147;&#24211;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-space models (SSMs), such as Mamba Gu &amp; Dao (2034), have been proposed as alternatives to Transformer networks in language modeling, by incorporating gating, convolutions, and input-dependent token selection to mitigate the quadratic cost of multi-head attention. Although SSMs exhibit competitive performance, their in-context learning (ICL) capabilities, a remarkable emergent property of modern language models that enables task execution without parameter optimization, remain underexplored compared to Transformers. In this study, we evaluate the ICL performance of SSMs, focusing on Mamba, against Transformer models across various tasks. Our results show that SSMs perform comparably to Transformers in standard regression ICL tasks, while outperforming them in tasks like sparse parity learning. However, SSMs fall short in tasks involving non-standard retrieval functionality. To address these limitations, we introduce a hybrid model, \variant, that combines Mamba with attention bloc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#19982;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04247</link><description>&lt;p&gt;
&#20248;&#20808;&#23433;&#20840;&#20445;&#38556;&#32780;&#38750;&#33258;&#27835;&#65306;&#31185;&#23398;&#20013;LLM&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#19982;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#22312;&#21508;&#20010;&#23398;&#31185;&#20013;&#33258;&#20027;&#36827;&#34892;&#23454;&#39564;&#21644;&#20419;&#36827;&#31185;&#23398;&#21457;&#29616;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#21069;&#26223;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#33021;&#21147;&#38750;&#24120;&#26377;&#21069;&#36884;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#19968;&#20123;&#26032;&#30340;&#28431;&#27934;&#65292;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#31354;&#30333;&#65292;&#23578;&#26410;&#23545;&#36825;&#20123;&#28431;&#27934;&#36827;&#34892;&#20840;&#38754;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#35823;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#38656;&#27714;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#20808;&#20840;&#38754;&#27010;&#36848;&#20102;&#31185;&#23398;LLM&#26426;&#22120;&#20154;&#22266;&#26377;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#24847;&#22270;&#12289;&#29305;&#23450;&#30340;&#31185;&#23398;&#39046;&#22495;&#20197;&#21450;&#23427;&#20204;&#23545;&#22806;&#37096;&#29615;&#22659;&#21487;&#33021;&#36896;&#25104;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#28431;&#27934;&#30340;&#36215;&#28304;&#21644;&#25552;&#20379;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provid
&lt;/p&gt;</description></item><item><title>CAST&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26367;&#20195;&#20196;&#29260;&#36827;&#34892;&#32858;&#31867;&#33258;&#27880;&#24847;&#21147;&#30340;&#39640;&#25928;Transformer&#26426;&#21046;&#65292;&#36890;&#36807;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#23454;&#29616;&#20102;&#22312;&#25972;&#20010;&#36755;&#20837;&#24207;&#21015;&#19978;&#30340;&#20449;&#24687;&#27969;&#21160;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.04239</link><description>&lt;p&gt;
CAST&#65306;&#21033;&#29992;&#26367;&#20195;&#20196;&#29260;&#36827;&#34892;&#32858;&#31867;&#33258;&#27880;&#24847;&#21147;&#30340;&#39640;&#25928;Transformer
&lt;/p&gt;
&lt;p&gt;
CAST: Clustering Self-Attention using Surrogate Tokens for Efficient Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04239
&lt;/p&gt;
&lt;p&gt;
CAST&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26367;&#20195;&#20196;&#29260;&#36827;&#34892;&#32858;&#31867;&#33258;&#27880;&#24847;&#21147;&#30340;&#39640;&#25928;Transformer&#26426;&#21046;&#65292;&#36890;&#36807;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#23454;&#29616;&#20102;&#22312;&#25972;&#20010;&#36755;&#20837;&#24207;&#21015;&#19978;&#30340;&#20449;&#24687;&#27969;&#21160;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#24050;&#32463;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#20219;&#21153;&#12290;&#23427;&#22522;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#36825;&#26159;&#19968;&#31181;&#26412;&#36136;&#19978;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#25805;&#20316;&#65292;&#20854;&#35745;&#31639;&#22797;&#26434;&#24615;&#21576;&#20108;&#27425;&#22686;&#38271;&#65306;&#20869;&#23384;&#20351;&#29992;&#21644;&#35745;&#31639;&#26102;&#38388;&#19982;&#36755;&#20837;&#24207;&#21015;&#30340;&#38271;&#24230;&#21576;&#20108;&#27425;&#22686;&#21152;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;Transformers&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21033;&#29992;&#26367;&#20195;&#20196;&#29260;&#36827;&#34892;&#32858;&#31867;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;CAST&#65289;&#65292;&#20197;&#20248;&#21270;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;Transformer&#12290;CAST&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#26367;&#20195;&#20196;&#29260;&#26500;&#24314;&#32858;&#31867;&#20146;&#21644;&#30697;&#38453;&#65292;&#29992;&#20110;&#23545;&#36755;&#20837;&#24207;&#21015;&#36827;&#34892;&#32858;&#31867;&#24182;&#29983;&#25104;&#26032;&#30340;&#32858;&#31867;&#25688;&#35201;&#12290;&#28982;&#21518;&#23558;&#27599;&#20010;&#32858;&#31867;&#20869;&#30340;&#33258;&#27880;&#24847;&#21147;&#19982;&#20854;&#20182;&#32858;&#31867;&#30340;&#32858;&#31867;&#25688;&#35201;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#25972;&#20010;&#36755;&#20837;&#24207;&#21015;&#19978;&#30340;&#20449;&#24687;&#27969;&#21160;&#12290;CAST&#36890;&#36807;&#23558;&#22797;&#26434;&#24230;&#20174;O(N^2)&#38477;&#20302;&#21040;O(&#945;N)&#65292;&#20854;&#20013;N&#20026;&#24207;&#21015;&#38271;&#24230;&#65292;&#945;&#20026;&#24120;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Transformer architecture has shown to be a powerful tool for a wide range of tasks. It is based on the self-attention mechanism, which is an inherently computationally expensive operation with quadratic computational complexity: memory usage and compute time increase quadratically with the length of the input sequences, thus limiting the application of Transformers. In this work, we propose a novel Clustering self-Attention mechanism using Surrogate Tokens (CAST), to optimize the attention computation and achieve efficient transformers. CAST utilizes learnable surrogate tokens to construct a cluster affinity matrix, used to cluster the input sequence and generate novel cluster summaries. The self-attention from within each cluster is then combined with the cluster summaries of other clusters, enabling information flow across the entire input sequence. CAST improves efficiency by reducing the complexity from $O(N^2)$ to $O(\alpha N)$ where N is the sequence length, and {\alpha} is c
&lt;/p&gt;</description></item><item><title>MusicRL&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#20248;&#21270;&#30340;&#38899;&#20048;&#29983;&#25104;&#31995;&#32479;&#12290;&#23427;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#31163;&#25955;&#38899;&#39057;&#20196;&#29260;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#26368;&#22823;&#21270;&#24207;&#21015;&#32423;&#22870;&#21169;&#12290;&#22312;&#37096;&#32626;&#21518;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#25910;&#38598;&#30340;&#37197;&#23545;&#20559;&#22909;&#25968;&#25454;&#38598;&#26469;&#36827;&#19968;&#27493;&#20248;&#21270;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2402.04229</link><description>&lt;p&gt;
MusicRL: &#23558;&#38899;&#20048;&#29983;&#25104;&#23545;&#40784;&#21040;&#20154;&#31867;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
MusicRL: Aligning Music Generation to Human Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04229
&lt;/p&gt;
&lt;p&gt;
MusicRL&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#20248;&#21270;&#30340;&#38899;&#20048;&#29983;&#25104;&#31995;&#32479;&#12290;&#23427;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#31163;&#25955;&#38899;&#39057;&#20196;&#29260;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#26368;&#22823;&#21270;&#24207;&#21015;&#32423;&#22870;&#21169;&#12290;&#22312;&#37096;&#32626;&#21518;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#25910;&#38598;&#30340;&#37197;&#23545;&#20559;&#22909;&#25968;&#25454;&#38598;&#26469;&#36827;&#19968;&#27493;&#20248;&#21270;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MusicRL&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#20248;&#21270;&#30340;&#38899;&#20048;&#29983;&#25104;&#31995;&#32479;&#12290;&#25991;&#26412;&#21040;&#38899;&#20048;&#30340;&#27169;&#22411;&#37492;&#36175;&#24230;&#29305;&#21035;&#20027;&#35266;&#65292;&#22240;&#20026;&#38899;&#20048;&#24615;&#30340;&#27010;&#24565;&#20197;&#21450;&#25991;&#26412;&#32972;&#21518;&#30340;&#20855;&#20307;&#24847;&#22270;&#37117;&#26159;&#29992;&#25143;&#30456;&#20851;&#30340;&#65288;&#20363;&#22914;&#65292;&#8220;&#27963;&#21147;&#22235;&#28322;&#30340;&#20581;&#36523;&#38899;&#20048;&#8221;&#21487;&#20197;&#23545;&#24212;&#22797;&#21476;&#21513;&#20182;&#29420;&#22863;&#25110;&#32773;&#30005;&#23376;&#27969;&#34892;&#38899;&#20048;&#33410;&#25293;&#65289;&#12290;&#36825;&#19981;&#20165;&#20351;&#24471;&#36825;&#31867;&#27169;&#22411;&#30340;&#30417;&#30563;&#35757;&#32451;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#32780;&#19988;&#20063;&#38656;&#35201;&#22312;&#37096;&#32626;&#21518;&#23558;&#36830;&#32493;&#30340;&#20154;&#31867;&#21453;&#39304;&#25972;&#21512;&#21040;&#20854;&#24494;&#35843;&#20013;&#12290;MusicRL&#26159;&#19968;&#20010;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#31163;&#25955;&#38899;&#39057;&#20196;&#29260;&#33258;&#22238;&#24402;MusicLM&#65288;Agostinelli&#31561;&#20154;&#65292;2023&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24494;&#35843;&#20197;&#26368;&#22823;&#21270;&#24207;&#21015;&#32423;&#22870;&#21169;&#12290;&#25105;&#20204;&#20174;&#36873;&#23450;&#30340;&#35780;&#20215;&#32773;&#37027;&#37324;&#35774;&#35745;&#19982;&#25991;&#26412;&#19968;&#33268;&#24615;&#21644;&#38899;&#39057;&#36136;&#37327;&#30456;&#20851;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#23558;MusicLM&#24494;&#35843;&#20026;MusicRL-R&#12290;&#25105;&#20204;&#21521;&#29992;&#25143;&#37096;&#32626;MusicLM&#24182;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;30&#19975;&#20010;&#37197;&#23545;&#20559;&#22909;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose MusicRL, the first music generation system finetuned from human feedback. Appreciation of text-to-music models is particularly subjective since the concept of musicality as well as the specific intention behind a caption are user-dependent (e.g. a caption such as "upbeat work-out music" can map to a retro guitar solo or a techno pop beat). Not only this makes supervised training of such models challenging, but it also calls for integrating continuous human feedback in their post-deployment finetuning. MusicRL is a pretrained autoregressive MusicLM (Agostinelli et al., 2023) model of discrete audio tokens finetuned with reinforcement learning to maximise sequence-level rewards. We design reward functions related specifically to text-adherence and audio quality with the help from selected raters, and use those to finetune MusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial dataset comprising 300,000 pairwise preferences. Using Reinforcement Learning fr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36164;&#28304;&#24863;&#30693;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#39044;&#27979;&#29992;&#25143;&#26410;&#26469;&#30340;&#20869;&#23481;&#35831;&#27714;&#65292;&#24182;&#20943;&#36731;&#26080;&#32447;&#35270;&#39057;&#32531;&#23384;&#32593;&#32476;&#20013;&#22238;&#31243;&#27969;&#37327;&#25317;&#22622;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04216</link><description>&lt;p&gt;
&#26080;&#32447;&#35270;&#39057;&#32531;&#23384;&#32593;&#32476;&#20013;&#30340;&#36164;&#28304;&#24863;&#30693;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Resource-Aware Hierarchical Federated Learning in Wireless Video Caching Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04216
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36164;&#28304;&#24863;&#30693;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#39044;&#27979;&#29992;&#25143;&#26410;&#26469;&#30340;&#20869;&#23481;&#35831;&#27714;&#65292;&#24182;&#20943;&#36731;&#26080;&#32447;&#35270;&#39057;&#32531;&#23384;&#32593;&#32476;&#20013;&#22238;&#31243;&#27969;&#37327;&#25317;&#22622;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#32447;&#35270;&#39057;&#32531;&#23384;&#32593;&#32476;&#20013;&#65292;&#36890;&#36807;&#23558;&#24453;&#35831;&#27714;&#20869;&#23481;&#23384;&#20648;&#22312;&#19981;&#21516;&#32423;&#21035;&#19978;&#65292;&#21487;&#20197;&#20943;&#36731;&#30001;&#23569;&#25968;&#28909;&#38376;&#25991;&#20214;&#30340;&#35270;&#39057;&#27969;&#37327;&#36896;&#25104;&#30340;&#22238;&#31243;&#25317;&#22622;&#12290;&#36890;&#24120;&#65292;&#20869;&#23481;&#26381;&#21153;&#25552;&#20379;&#21830;&#65288;CSP&#65289;&#25317;&#26377;&#20869;&#23481;&#65292;&#29992;&#25143;&#20351;&#29992;&#20854;&#65288;&#26080;&#32447;&#65289;&#20114;&#32852;&#32593;&#26381;&#21153;&#25552;&#20379;&#21830;&#65288;ISP&#65289;&#20174;CSP&#35831;&#27714;&#20854;&#39318;&#36873;&#20869;&#23481;&#12290;&#30001;&#20110;&#36825;&#20123;&#21442;&#19982;&#26041;&#19981;&#20250;&#36879;&#38706;&#20854;&#31169;&#23494;&#20449;&#24687;&#21644;&#21830;&#19994;&#26426;&#23494;&#65292;&#20256;&#32479;&#25216;&#26415;&#21487;&#33021;&#26080;&#27861;&#29992;&#20110;&#39044;&#27979;&#29992;&#25143;&#26410;&#26469;&#38656;&#27714;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#20986;&#20110;&#36825;&#20010;&#21407;&#22240;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36164;&#28304;&#24863;&#30693;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;RawHFL&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#39044;&#27979;&#29992;&#25143;&#26410;&#26469;&#30340;&#20869;&#23481;&#35831;&#27714;&#12290;&#37319;&#29992;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#25968;&#25454;&#33719;&#21462;&#25216;&#26415;&#65292;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#20854;&#35831;&#27714;&#30340;&#20869;&#23481;&#26356;&#26032;&#20854;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#32593;&#32476;&#21644;&#20854;&#20182;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#65292;&#32771;&#34385;&#21040;&#21482;&#26377;&#19968;&#37096;&#20998;&#29992;&#25143;&#21442;&#19982;&#27169;&#22411;&#35757;&#32451;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;
&lt;/p&gt;
&lt;p&gt;
Backhaul traffic congestion caused by the video traffic of a few popular files can be alleviated by storing the to-be-requested content at various levels in wireless video caching networks. Typically, content service providers (CSPs) own the content, and the users request their preferred content from the CSPs using their (wireless) internet service providers (ISPs). As these parties do not reveal their private information and business secrets, traditional techniques may not be readily used to predict the dynamic changes in users' future demands. Motivated by this, we propose a novel resource-aware hierarchical federated learning (RawHFL) solution for predicting user's future content requests. A practical data acquisition technique is used that allows the user to update its local training dataset based on its requested content. Besides, since networking and other computational resources are limited, considering that only a subset of the users participate in the model training, we derive
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#21464;&#20998;Shapley&#32593;&#32476;&#65292;&#36890;&#36807;&#27010;&#29575;&#21270;&#30340;&#26041;&#27861;&#31616;&#21270;&#20102;&#35745;&#31639;Shapley&#20540;&#30340;&#36807;&#31243;&#65292;&#24182;&#35299;&#20915;&#20102;&#20272;&#35745;&#27169;&#22411;&#36793;&#38469;&#20540;&#21644;&#22788;&#29702;&#35299;&#37322;&#21487;&#21464;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.04211</link><description>&lt;p&gt;
&#21464;&#20998;Shapley&#32593;&#32476;&#65306;&#19968;&#31181;&#27010;&#29575;&#21270;&#30340;&#26041;&#27861;&#29992;&#20110;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#33258;&#35299;&#37322;Shapley&#20540;
&lt;/p&gt;
&lt;p&gt;
Variational Shapley Network: A Probabilistic Approach to Self-Explaining Shapley values with Uncertainty Quantification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#21464;&#20998;Shapley&#32593;&#32476;&#65292;&#36890;&#36807;&#27010;&#29575;&#21270;&#30340;&#26041;&#27861;&#31616;&#21270;&#20102;&#35745;&#31639;Shapley&#20540;&#30340;&#36807;&#31243;&#65292;&#24182;&#35299;&#20915;&#20102;&#20272;&#35745;&#27169;&#22411;&#36793;&#38469;&#20540;&#21644;&#22788;&#29702;&#35299;&#37322;&#21487;&#21464;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shapley&#20540;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#38416;&#26126;&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#30340;&#22522;&#30784;&#24037;&#20855;&#12290;&#23613;&#31649;&#23427;&#20204;&#34987;&#24191;&#27867;&#37319;&#29992;&#24182;&#20855;&#26377;&#28385;&#36275;&#37325;&#35201;&#21487;&#35299;&#37322;&#24615;&#20844;&#29702;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#20294;&#22312;&#20272;&#35745;&#36807;&#31243;&#20013;&#20173;&#28982;&#23384;&#22312;&#35745;&#31639;&#25361;&#25112;&#65292;&#21253;&#25324;&#65288;i&#65289;&#23545;&#27169;&#22411;&#22312;&#25152;&#26377;&#21487;&#33021;&#30340;&#36755;&#20837;&#29305;&#24449;&#32452;&#21512;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#65288;ii&#65289;&#20272;&#35745;&#27169;&#22411;&#30340;&#36793;&#38469;&#20540;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#22788;&#29702;&#35299;&#37322;&#30340;&#21487;&#21464;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#35299;&#37322;&#26041;&#27861;&#65292;&#26174;&#33879;&#31616;&#21270;&#20102;Shapley&#20540;&#30340;&#35745;&#31639;&#65292;&#21482;&#38656;&#35201;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#12290;&#37492;&#20110;Shapley&#20540;&#30340;&#30830;&#23450;&#24615;&#22788;&#29702;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#38480;&#21046;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#27010;&#29575;&#26694;&#26550;&#32435;&#20837;&#20854;&#20013;&#20197;&#25429;&#25417;&#35299;&#37322;&#20013;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#19982;&#20854;&#20182;&#26367;&#20195;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#19981;&#30452;&#25509;&#20381;&#36182;&#20110;&#35266;&#27979;&#25968;&#25454;&#31354;&#38388;&#26469;&#20272;&#35745;&#36793;&#38469;&#20540;&#65307;&#30456;&#21453;&#65292;&#23427;&#20351;&#29992;&#20174;&#28508;&#22312;&#30340;&#12289;&#29305;&#23450;&#20110;&#29305;&#24449;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#27966;&#29983;&#20986;&#30340;&#21487;&#36866;&#24212;&#30340;&#22522;&#32447;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shapley values have emerged as a foundational tool in machine learning (ML) for elucidating model decision-making processes. Despite their widespread adoption and unique ability to satisfy essential explainability axioms, computational challenges persist in their estimation when ($i$) evaluating a model over all possible subset of input feature combinations, ($ii$) estimating model marginals, and ($iii$) addressing variability in explanations. We introduce a novel, self-explaining method that simplifies the computation of Shapley values significantly, requiring only a single forward pass. Recognizing the deterministic treatment of Shapley values as a limitation, we explore incorporating a probabilistic framework to capture the inherent uncertainty in explanations. Unlike alternatives, our technique does not rely directly on the observed data space to estimate marginals; instead, it uses adaptable baseline values derived from a latent, feature-specific embedding space, generated by a no
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#24182;&#27604;&#36739;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#38750;&#21361;&#37325;&#30149;&#24739;&#32773;&#22312;&#25509;&#19979;&#26469;48&#23567;&#26102;&#20869;&#21457;&#23637;&#20026;2&#26399;&#25110;&#26356;&#39640;&#31243;&#24230;&#30340;&#24613;&#24615;&#32958;&#25439;&#20260;&#12290;&#20869;&#22806;&#37096;&#39564;&#35777;&#21644;&#20122;&#32452;&#20998;&#26512;&#34920;&#26126;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04209</link><description>&lt;p&gt;
&#38750;&#21361;&#37325;&#30149;&#24739;&#32773;&#24613;&#24615;&#32958;&#25439;&#20260;&#39044;&#27979;&#65306;&#22238;&#39038;&#24615;&#22806;&#37096;&#21644;&#20869;&#37096;&#39564;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Acute kidney injury prediction for non-critical care patients: a retrospective external and internal validation study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04209
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#24182;&#27604;&#36739;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#38750;&#21361;&#37325;&#30149;&#24739;&#32773;&#22312;&#25509;&#19979;&#26469;48&#23567;&#26102;&#20869;&#21457;&#23637;&#20026;2&#26399;&#25110;&#26356;&#39640;&#31243;&#24230;&#30340;&#24613;&#24615;&#32958;&#25439;&#20260;&#12290;&#20869;&#22806;&#37096;&#39564;&#35777;&#21644;&#20122;&#32452;&#20998;&#26512;&#34920;&#26126;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#24613;&#24615;&#32958;&#25439;&#20260;&#65288;AKI&#65289;&#65292;&#21363;&#32958;&#25490;&#27844;&#21151;&#33021;&#19979;&#38477;&#65292;&#22312;&#20303;&#38498;&#24739;&#32773;&#20013;&#21457;&#29983;&#30340;&#27604;&#20363;&#39640;&#36798;18%&#12290;AKI&#30340;&#36827;&#23637;&#21487;&#33021;&#23548;&#33268;&#19981;&#21487;&#36870;&#30340;&#32958;&#33039;&#25439;&#20260;&#12290;&#26041;&#27861;&#65306;&#26412;&#22238;&#39038;&#24615;&#38431;&#21015;&#30740;&#31350;&#21253;&#25324;&#22312;&#21305;&#20857;&#22561;&#22823;&#23398;&#21307;&#30103;&#20013;&#24515;&#65288;UPMC&#65289;&#30340;&#38750;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#25509;&#21463;&#27835;&#30103;&#30340;&#25104;&#24180;&#24739;&#32773;&#65288;n=46,815&#65289;&#65292;&#20197;&#21450;&#22312;&#20315;&#32599;&#37324;&#36798;&#22823;&#23398;&#20581;&#24247;&#20013;&#24515;&#65288;UFH&#65289;&#25509;&#21463;&#27835;&#30103;&#30340;&#25104;&#24180;&#24739;&#32773;&#65288;n=127,202&#65289;&#12290;&#25105;&#20204;&#24320;&#21457;&#24182;&#27604;&#36739;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#25509;&#19979;&#26469;48&#23567;&#26102;&#20869;&#36827;&#23637;&#33267;2&#26399;&#25110;&#26356;&#39640;&#31243;&#24230;&#30340;AKI&#12290;&#25105;&#20204;&#20998;&#21035;&#20026;&#21508;&#20010;&#21307;&#38498;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#65288;&#22312;UFH&#19978;&#35757;&#32451;&#30340;UFH&#27169;&#22411;&#65292;&#20197;&#21450;&#22312;UPMC&#19978;&#35757;&#32451;&#30340;UPMC&#27169;&#22411;&#65289;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#21307;&#38498;&#30340;&#21457;&#23637;&#38431;&#21015;&#24739;&#32773;&#24320;&#21457;&#20102;&#19968;&#20010;&#29420;&#31435;&#30340;&#27169;&#22411;&#65288;UFH-UPMC&#27169;&#22411;&#65289;&#12290;&#25105;&#20204;&#22312;&#27599;&#20010;&#21307;&#38498;&#20869;&#22806;&#37096;&#39564;&#35777;&#20102;&#36825;&#20123;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#24615;&#21035;&#21644;&#31181;&#26063;&#30340;&#20122;&#32452;&#20998;&#26512;&#12290;&#32467;&#26524;&#65306;UFH&#21644;UPMC&#30340;&#24739;&#32773;&#20013;&#65292;2&#26399;&#25110;&#26356;&#39640;&#31243;&#24230;&#30340;AKI&#20998;&#21035;&#21457;&#29983;&#22312;3%&#65288;n=3,257&#65289;&#21644;8%&#65288;n=2,296&#65289;&#30340;&#24739;&#32773;&#20013;&#12290;roc&#31215;&#20998;&#29575;
&lt;/p&gt;
&lt;p&gt;
Background: Acute kidney injury (AKI), the decline of kidney excretory function, occurs in up to 18% of hospitalized admissions. Progression of AKI may lead to irreversible kidney damage. Methods: This retrospective cohort study includes adult patients admitted to a non-intensive care unit at the University of Pittsburgh Medical Center (UPMC) (n = 46,815) and University of Florida Health (UFH) (n = 127,202). We developed and compared deep learning and conventional machine learning models to predict progression to Stage 2 or higher AKI within the next 48 hours. We trained local models for each site (UFH Model trained on UFH, UPMC Model trained on UPMC) and a separate model with a development cohort of patients from both sites (UFH-UPMC Model). We internally and externally validated the models on each site and performed subgroup analyses across sex and race. Results: Stage 2 or higher AKI occurred in 3% (n=3,257) and 8% (n=2,296) of UFH and UPMC patients, respectively. Area under the rec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20843;&#21350;&#30340;&#26799;&#24230;&#32534;&#30721;&#20998;&#25955;&#24335;&#23398;&#20064;&#26041;&#27861;&#65288;GOCO&#65289;&#65292;&#20197;&#35299;&#20915;&#23384;&#22312;&#24310;&#36831;&#33410;&#28857;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#38382;&#39064;&#12290;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#23398;&#20064;&#24615;&#33021;&#19978;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04193</link><description>&lt;p&gt;
&#36991;&#20813;&#24310;&#36831;&#33410;&#28857;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Gradient Coding in Decentralized Learning for Evading Stragglers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20843;&#21350;&#30340;&#26799;&#24230;&#32534;&#30721;&#20998;&#25955;&#24335;&#23398;&#20064;&#26041;&#27861;&#65288;GOCO&#65289;&#65292;&#20197;&#35299;&#20915;&#23384;&#22312;&#24310;&#36831;&#33410;&#28857;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#38382;&#39064;&#12290;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#23398;&#20064;&#24615;&#33021;&#19978;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#23384;&#22312;&#24310;&#36831;&#33410;&#28857;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#38382;&#39064;&#12290;&#23613;&#31649;&#26799;&#24230;&#32534;&#30721;&#25216;&#26415;&#24050;&#32463;&#34987;&#24320;&#21457;&#29992;&#20110;&#20998;&#24067;&#24335;&#23398;&#20064;&#20197;&#36991;&#20813;&#24310;&#36831;&#33410;&#28857;&#65292;&#21363;&#35774;&#22791;&#20351;&#29992;&#20887;&#20313;&#35757;&#32451;&#25968;&#25454;&#21457;&#36865;&#32534;&#30721;&#26799;&#24230;&#65292;&#20294;&#26159;&#23558;&#36825;&#20123;&#25216;&#26415;&#30452;&#25509;&#24212;&#29992;&#20110;&#20998;&#25955;&#24335;&#23398;&#20064;&#22330;&#26223;&#26159;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20843;&#21350;&#30340;&#26799;&#24230;&#32534;&#30721;&#20998;&#25955;&#24335;&#23398;&#20064;&#26041;&#27861;&#65288;GOCO&#65289;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#20026;&#20102;&#36991;&#20813;&#24310;&#36831;&#33410;&#28857;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#21442;&#25968;&#21521;&#37327;&#20351;&#29992;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#32534;&#30721;&#26694;&#26550;&#30340;&#32534;&#30721;&#26799;&#24230;&#36827;&#34892;&#26412;&#22320;&#26356;&#26032;&#65292;&#28982;&#21518;&#20197;&#20843;&#21350;&#26041;&#24335;&#36827;&#34892;&#24179;&#22343;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;GOCO&#22312;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#19979;&#30340;&#25910;&#25947;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#23398;&#20064;&#24615;&#33021;&#19978;&#30456;&#23545;&#20110;&#22522;&#20934;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider a decentralized learning problem in the presence of stragglers. Although gradient coding techniques have been developed for distributed learning to evade stragglers, where the devices send encoded gradients with redundant training data, it is difficult to apply those techniques directly to decentralized learning scenarios. To deal with this problem, we propose a new gossip-based decentralized learning method with gradient coding (GOCO). In the proposed method, to avoid the negative impact of stragglers, the parameter vectors are updated locally using encoded gradients based on the framework of stochastic gradient coding and then averaged in a gossip-based manner. We analyze the convergence performance of GOCO for strongly convex loss functions. And we also provide simulation results to demonstrate the superiority of the proposed method in terms of learning performance compared with the baseline methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38598;&#25104;&#27169;&#22411;&#39044;&#27979;&#23433;&#20840;&#35748;&#35777;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#19982;&#22522;&#20110;&#31649;&#36947;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#35268;&#21010;&#26469;&#32416;&#27491;&#23398;&#20064;&#20195;&#29702;&#30340;&#34892;&#21160;&#65292;&#26368;&#23567;&#21270;&#23433;&#20840;&#32422;&#26463;&#36829;&#35268;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20102;&#26174;&#33879;&#36739;&#23569;&#30340;&#32422;&#26463;&#36829;&#35268;&#12290;</title><link>https://arxiv.org/abs/2402.04182</link><description>&lt;p&gt;
&#29992;&#38598;&#25104;&#27169;&#22411;&#39044;&#27979;&#23433;&#20840;&#35748;&#35777;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Ensemble Model Predictive Safety Certification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38598;&#25104;&#27169;&#22411;&#39044;&#27979;&#23433;&#20840;&#35748;&#35777;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#19982;&#22522;&#20110;&#31649;&#36947;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#35268;&#21010;&#26469;&#32416;&#27491;&#23398;&#20064;&#20195;&#29702;&#30340;&#34892;&#21160;&#65292;&#26368;&#23567;&#21270;&#23433;&#20840;&#32422;&#26463;&#36829;&#35268;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20102;&#26174;&#33879;&#36739;&#23569;&#30340;&#32422;&#26463;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38656;&#35201;&#25506;&#32034;&#25165;&#33021;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#26080;&#30417;&#30563;&#25506;&#32034;&#38459;&#27490;&#20102;&#36825;&#20123;&#31639;&#27861;&#22312;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#19978;&#30340;&#37096;&#32626;&#65292;&#24182;&#38480;&#21046;&#20102;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38598;&#25104;&#27169;&#22411;&#39044;&#27979;&#23433;&#20840;&#35748;&#35777;&#30340;&#26032;&#31639;&#27861;&#65292;&#23558;&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#19982;&#22522;&#20110;&#31649;&#36947;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#35268;&#21010;&#26469;&#32416;&#27491;&#23398;&#20064;&#20195;&#29702;&#30340;&#34892;&#21160;&#65292;&#23558;&#23433;&#20840;&#32422;&#26463;&#36829;&#35268;&#38477;&#33267;&#26368;&#20302;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20165;&#38656;&#35201;&#30001;&#23433;&#20840;&#25511;&#21046;&#22120;&#29983;&#25104;&#30340;&#31163;&#32447;&#25968;&#25454;&#26469;&#20943;&#23569;&#23545;&#23454;&#38469;&#31995;&#32479;&#30340;&#20808;&#21069;&#30693;&#35782;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#21487;&#27604;&#36739;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#36739;&#23569;&#30340;&#32422;&#26463;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning algorithms need exploration to learn. However, unsupervised exploration prevents the deployment of such algorithms on safety-critical tasks and limits real-world deployment. In this paper, we propose a new algorithm called Ensemble Model Predictive Safety Certification that combines model-based deep reinforcement learning with tube-based model predictive control to correct the actions taken by a learning agent, keeping safety constraint violations at a minimum through planning. Our approach aims to reduce the amount of prior knowledge about the actual system by requiring only offline data generated by a safe controller. Our results show that we can achieve significantly fewer constraint violations than comparable reinforcement learning methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#36716;&#31227;&#23398;&#20064;&#29615;&#22659;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23610;&#24230;&#34892;&#20026;&#65292;&#21457;&#29616;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#19982;&#19979;&#28216;&#25968;&#25454;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#23545;&#19979;&#28216;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.04177</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#23610;&#24230;&#24459;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws for Downstream Task Performance of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#36716;&#31227;&#23398;&#20064;&#29615;&#22659;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23610;&#24230;&#34892;&#20026;&#65292;&#21457;&#29616;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#19982;&#19979;&#28216;&#25968;&#25454;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#23545;&#19979;&#28216;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23610;&#24230;&#24459;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35265;&#35299;&#65292;&#21487;&#20197;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35774;&#35745;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#30740;&#31350;&#39044;&#35757;&#32451;&#65288;&#19978;&#28216;&#65289;&#25439;&#22833;&#30340;&#23610;&#24230;&#24459;&#12290;&#28982;&#32780;&#65292;&#22312;&#36716;&#31227;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;LLM&#20808;&#22312;&#26080;&#30417;&#30563;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#36890;&#24120;&#20063;&#20851;&#24515;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36716;&#31227;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#23610;&#24230;&#34892;&#20026;&#65292;&#20854;&#20013;LLM&#34987;&#24494;&#35843;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#36873;&#25321;&#21644;&#22823;&#23567;&#23545;&#19979;&#28216;&#24615;&#33021;&#65288;&#32763;&#35793;&#36136;&#37327;&#65289;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#20102;&#20004;&#20010;&#35780;&#20215;&#25351;&#26631;&#65306;&#19979;&#28216;&#20132;&#21449;&#29109;&#21644;BLEU&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#19982;&#19979;&#28216;&#25968;&#25454;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#26174;&#33879;&#24433;&#21709;&#23610;&#24230;&#34892;&#20026;&#12290;&#22312;&#20805;&#20998;&#19968;&#33268;&#24615;&#24773;&#20917;&#19979;&#65292;&#19979;&#28216;&#20132;&#21449;&#29109;&#21644;BLEU&#20998;&#25968;&#37117;&#20250;&#36880;&#28176;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling laws provide important insights that can guide the design of large language models (LLMs). Existing work has primarily focused on studying scaling laws for pretraining (upstream) loss. However, in transfer learning settings, in which LLMs are pretrained on an unsupervised dataset and then finetuned on a downstream task, we often also care about the downstream performance. In this work, we study the scaling behavior in a transfer learning setting, where LLMs are finetuned for machine translation tasks. Specifically, we investigate how the choice of the pretraining data and its size affect downstream performance (translation quality) as judged by two metrics: downstream cross-entropy and BLEU score. Our experiments indicate that the size of the finetuning dataset and the distribution alignment between the pretraining and downstream data significantly influence the scaling behavior. With sufficient alignment, both downstream cross-entropy and BLEU score improve monotonically with 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#20449;&#24687;&#30340;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#32467;&#26500;&#21270;&#30340;&#35268;&#21017;&#25163;&#20876;&#20316;&#20026;&#30693;&#35782;&#28304;&#65292;&#21033;&#29992;&#24773;&#22659;&#24863;&#30693;&#30340;&#22870;&#21169;&#35774;&#35745;&#35780;&#20272;&#23398;&#20064;&#36712;&#36857;&#65292;&#20197;&#23454;&#29616;&#23545;&#38656;&#35201;&#25511;&#21046;&#20132;&#36890;&#35268;&#21017;&#20363;&#22806;&#30340;&#24773;&#26223;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.04168</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#30340;&#22686;&#24378;&#23398;&#20064;&#29992;&#20110;&#24773;&#22659;&#24863;&#30693;&#20132;&#36890;&#35268;&#21017;&#20363;&#22806;
&lt;/p&gt;
&lt;p&gt;
Informed Reinforcement Learning for Situation-Aware Traffic Rule Exceptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#20449;&#24687;&#30340;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#32467;&#26500;&#21270;&#30340;&#35268;&#21017;&#25163;&#20876;&#20316;&#20026;&#30693;&#35782;&#28304;&#65292;&#21033;&#29992;&#24773;&#22659;&#24863;&#30693;&#30340;&#22870;&#21169;&#35774;&#35745;&#35780;&#20272;&#23398;&#20064;&#36712;&#36857;&#65292;&#20197;&#23454;&#29616;&#23545;&#38656;&#35201;&#25511;&#21046;&#20132;&#36890;&#35268;&#21017;&#20363;&#22806;&#30340;&#24773;&#26223;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#24378;&#23398;&#20064;&#26159;&#19968;&#20010;&#38750;&#24120;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20855;&#26377;&#24456;&#22810;&#26377;&#21069;&#26223;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#65292;&#36890;&#24120;&#21482;&#30740;&#31350;&#38750;&#24120;&#31616;&#21333;&#30340;&#22330;&#26223;&#12290;&#24120;&#35265;&#30340;&#26041;&#27861;&#20351;&#29992;&#38750;&#21487;&#35299;&#37322;&#30340;&#25511;&#21046;&#21629;&#20196;&#20316;&#20026;&#21160;&#20316;&#31354;&#38388;&#65292;&#20197;&#21450;&#32570;&#20047;&#32467;&#26500;&#30340;&#22870;&#21169;&#35774;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20449;&#24687;&#30340;&#22686;&#24378;&#23398;&#20064;&#65292;&#23558;&#32467;&#26500;&#21270;&#30340;&#35268;&#21017;&#25163;&#20876;&#20316;&#20026;&#30693;&#35782;&#28304;&#38598;&#25104;&#36827;&#26469;&#12290;&#25105;&#20204;&#23398;&#20064;&#36712;&#36857;&#65292;&#24182;&#20351;&#29992;&#24773;&#22659;&#24863;&#30693;&#30340;&#22870;&#21169;&#35774;&#35745;&#26469;&#35780;&#20272;&#23427;&#20204;&#65292;&#20174;&#32780;&#20135;&#29983;&#21160;&#24577;&#22870;&#21169;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#23398;&#20064;&#38656;&#35201;&#25511;&#21046;&#20132;&#36890;&#35268;&#21017;&#20363;&#22806;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#24847;&#22686;&#24378;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#23637;&#31034;&#20102;&#26368;&#36817;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20195;&#29702;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#39640;&#23436;&#25104;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning is a highly active research field with promising advancements. In the field of autonomous driving, however, often very simple scenarios are being examined. Common approaches use non-interpretable control commands as the action space and unstructured reward designs which lack structure. In this work, we introduce Informed Reinforcement Learning, where a structured rulebook is integrated as a knowledge source. We learn trajectories and asses them with a situation-aware reward design, leading to a dynamic reward which allows the agent to learn situations which require controlled traffic rule exceptions. Our method is applicable to arbitrary RL models. We successfully demonstrate high completion rates of complex scenarios with recent model-based agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#28201;&#21644;&#24494;&#31215;&#20998;&#30340;&#29702;&#35770;&#21644;&#24037;&#20855;&#65292;&#26469;&#25913;&#36827;&#30446;&#21069;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#25968;&#23398;&#25197;&#26354;&#26041;&#27861;&#65292;&#29305;&#21035;&#24378;&#35843;&#19982;&#20960;&#20309;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04163</link><description>&lt;p&gt;
Tempered Calculus for ML: &#24212;&#29992;&#20110;&#21452;&#26354;&#27169;&#22411;&#23884;&#20837;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tempered Calculus for ML: Application to Hyperbolic Model Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#28201;&#21644;&#24494;&#31215;&#20998;&#30340;&#29702;&#35770;&#21644;&#24037;&#20855;&#65292;&#26469;&#25913;&#36827;&#30446;&#21069;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#25968;&#23398;&#25197;&#26354;&#26041;&#27861;&#65292;&#29305;&#21035;&#24378;&#35843;&#19982;&#20960;&#20309;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#25968;&#23398;&#25197;&#26354;&#26412;&#36136;&#19978;&#37117;&#26159;&#31215;&#20998;&#30340;&#65306;$f$-divergences, Bregman divergences, (regularized) optimal transport distances, integral probability metrics, geodesic distances&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#22522;&#20110;&#28201;&#21644;&#24494;&#31215;&#20998;&#30340;&#29702;&#35770;&#21644;&#24037;&#20855;&#65292;&#21487;&#20197;&#24110;&#21161;&#25913;&#36827;&#36825;&#20123;&#25197;&#26354;&#20197;&#26356;&#22909;&#22320;&#28385;&#36275;&#26426;&#22120;&#23398;&#20064;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#20174;&#24191;&#20041;&#30340;&#40654;&#26364;&#31215;&#20998;&#24320;&#22987;&#65292;&#36825;&#31181;&#31215;&#20998;&#36824;&#21253;&#25324;&#19981;&#20005;&#26684;&#21487;&#21152;&#30340;&#20989;&#25968;&#65292;&#32780;&#26159;&#26356;&#19968;&#33324;&#22320;&#26159;$t$-&#21487;&#21152;&#30340;&#65292;&#23601;&#20687;&#38750;&#26497;&#38480;&#32479;&#35745;&#21147;&#23398;&#20013;&#30340;&#24773;&#20917;&#19968;&#26679;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;Volterra&#30340;&#20056;&#31215;&#31215;&#20998;&#20316;&#20026;&#29305;&#20363;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#65288;&#27431;&#20960;&#37324;&#24471;&#65289;&#23548;&#25968;&#30340;&#25193;&#23637;&#26469;&#25512;&#24191;&#22522;&#26412;&#23450;&#29702;&#12290;&#36825;&#20123;&#25512;&#24191;&#20197;&#21450;&#19968;&#31995;&#21015;&#26356;&#20855;&#20307;&#30340;&#23450;&#29702;&#20026;&#32467;&#26524;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#20197;&#31616;&#21333;&#30340;&#26041;&#24335;&#19987;&#38376;&#35774;&#35745;&#12289;&#25913;&#21464;&#25110;&#25913;&#21464;&#25197;&#26354;&#24230;&#37327;&#30340;&#22522;&#26412;&#29305;&#24615;&#65292;&#29305;&#21035;&#24378;&#35843;&#19982;&#20960;&#20309;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most mathematical distortions used in ML are fundamentally integral in nature: $f$-divergences, Bregman divergences, (regularized) optimal transport distances, integral probability metrics, geodesic distances, etc. In this paper, we unveil a grounded theory and tools which can help improve these distortions to better cope with ML requirements. We start with a generalization of Riemann integration that also encapsulates functions that are not strictly additive but are, more generally, $t$-additive, as in nonextensive statistical mechanics. Notably, this recovers Volterra's product integral as a special case. We then generalize the Fundamental Theorem of calculus using an extension of the (Euclidean) derivative. This, along with a series of more specific Theorems, serves as a basis for results showing how one can specifically design, alter, or change fundamental properties of distortion measures in a simple way, with a special emphasis on geometric- and ML-related properties that are the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#39034;&#24207;&#24314;&#27169;&#33021;&#21147;&#65292;&#29702;&#35770;&#19978;&#21051;&#30011;&#20102;&#21333;&#23618;Transformer&#30340;&#25439;&#22833;&#26223;&#35266;&#24182;&#21457;&#29616;&#20102;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#22351;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.04161</link><description>&lt;p&gt;
&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#35268;&#33539;&#20998;&#26512;&#26694;&#26550;&#65306;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#30740;&#31350;Transformer&#30340;&#39034;&#24207;&#24314;&#27169;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Attention with Markov: A Framework for Principled Analysis of Transformers via Markov Chains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#39034;&#24207;&#24314;&#27169;&#33021;&#21147;&#65292;&#29702;&#35770;&#19978;&#21051;&#30011;&#20102;&#21333;&#23618;Transformer&#30340;&#25439;&#22833;&#26223;&#35266;&#24182;&#21457;&#29616;&#20102;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#22351;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;Transformer&#22312;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22312;&#20869;&#30340;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36807;&#31243;&#65292;&#27169;&#22411;&#22312;&#27492;&#36807;&#31243;&#20013;&#36890;&#36807;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#22312;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#35270;&#35282;&#65292;&#20801;&#35768;&#29702;&#35770;&#21644;&#31995;&#32479;&#23454;&#39564;&#26469;&#30740;&#31350;Transformer&#30340;&#39034;&#24207;&#24314;&#27169;&#33021;&#21147;&#12290;&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#30340;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#24314;&#27169;&#20026;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#28304;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#31995;&#32479;&#22320;&#30740;&#31350;&#25968;&#25454;&#20998;&#24067;&#29305;&#24615;&#12289;Transformer&#26550;&#26500;&#12289;&#23398;&#21040;&#30340;&#20998;&#24067;&#21644;&#26368;&#32456;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#21051;&#30011;&#20102;&#21333;&#23618;Transformer&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#24182;&#23637;&#31034;&#20102;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#22351;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#23384;&#22312;&#65292;&#36825;&#21462;&#20915;&#20110;&#20855;&#20307;&#30340;&#25968;&#25454;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages. A key ingredient behind their success is the generative pretraining procedure, during which these models are trained on a large text corpus in an auto-regressive manner. To shed light on this phenomenon, we propose a new framework that allows both theory and systematic experiments to study the sequential modeling capabilities of transformers through the lens of Markov chains. Inspired by the Markovianity of natural languages, we model the data as a Markovian source and utilize this framework to systematically study the interplay between the data-distributional properties, the transformer architecture, the learnt distribution, and the final model performance. In particular, we theoretically characterize the loss landscape of single-layer transformers and show the existence of global minima and bad local minima contingent upon the specific data chara
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#22686;&#24378;&#24418;&#24335;&#30340;&#20219;&#21153;&#25351;&#23548;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#24182;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)</title><link>https://arxiv.org/abs/2402.04154</link><description>&lt;p&gt;
&#12298;&#35835;&#29609;&#28216;&#25103;&#65288;R2-Play&#65289;: &#22810;&#27169;&#24577;&#28216;&#25103;&#25351;&#23548;&#19979;&#30340;&#20915;&#31574; Transformer&#12299;
&lt;/p&gt;
&lt;p&gt;
Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#20026;&#26234;&#33021;&#20307;&#25552;&#20379;&#22686;&#24378;&#24418;&#24335;&#30340;&#20219;&#21153;&#25351;&#23548;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#24182;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#30340;&#25104;&#21151;&#24212;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#24320;&#21457;&#19968;&#27454;&#36890;&#29992;&#26234;&#33021;&#20307;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#30446;&#26631;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#20219;&#21153;&#30340;&#22823;&#37327;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#20219;&#21153;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#22312;&#25193;&#23637;&#21040;&#26032;&#20219;&#21153;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23558;&#25991;&#26412;&#25351;&#23548;&#25110;&#35270;&#35273;&#36712;&#36857;&#25972;&#21512;&#21040;&#20915;&#31574;&#32593;&#32476;&#20013;&#65292;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#65292;&#20195;&#34920;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#35266;&#23519;&#21040;&#20165;&#20381;&#36182;&#20110;&#25991;&#26412;&#25351;&#23548;&#25110;&#35270;&#35273;&#36712;&#36857;&#23545;&#20110;&#20934;&#30830;&#20256;&#36798;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#22686;&#24378;&#26234;&#33021;&#20307;&#20219;&#21153;&#25351;&#23548;&#30340;&#24418;&#24335;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#28216;&#25103;&#25351;&#23548;&#65292;&#20174;&#32780;&#23454;&#29616;"&#35835;&#29609;&#28216;&#25103;"&#30340;&#33021;&#21147;&#12290;&#21463;&#21040;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#35270;&#35273;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#38271;&#26399;&#35270;&#35273;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#32452;... (&#20869;&#23481;&#22826;&#38271;&#65292;&#26080;&#27861;&#32487;&#32493;&#26174;&#31034;)
&lt;/p&gt;
&lt;p&gt;
Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning.However, these works encounter challenges in extending their capabilities to new tasks.Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction.However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks.This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a "read-to-play" capability.Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#30340;&#22810;&#28304;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20010;&#25968;&#25454;&#28304;&#20043;&#38388;&#36136;&#37327;&#21644;&#20840;&#38754;&#24615;&#24046;&#24322;&#32473;&#31995;&#32479;&#20248;&#21270;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04146</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22810;&#28304;&#25968;&#25454;&#34701;&#21512;&#36890;&#36807;&#28508;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Interpretable Multi-Source Data Fusion Through Latent Variable Gaussian Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04146
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#30340;&#22810;&#28304;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20010;&#25968;&#25454;&#28304;&#20043;&#38388;&#36136;&#37327;&#21644;&#20840;&#38754;&#24615;&#24046;&#24322;&#32473;&#31995;&#32479;&#20248;&#21270;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#20986;&#29616;&#65292;&#21508;&#20010;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#24050;&#32463;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26367;&#20195;&#27169;&#22411;&#26469;&#24314;&#27169;&#26469;&#33258;&#22823;&#37327;&#20449;&#24687;&#28304;&#65288;&#25968;&#25454;&#65289;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;&#36825;&#31181;&#22686;&#21152;&#23548;&#33268;&#20102;&#24320;&#21457;&#20986;&#29992;&#20110;&#25191;&#34892;&#29305;&#23450;&#21151;&#33021;&#30340;&#20248;&#36234;&#31995;&#32479;&#25152;&#38656;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#30340;&#26174;&#33879;&#38477;&#20302;&#12290;&#36825;&#26679;&#30340;&#26367;&#20195;&#27169;&#22411;&#24448;&#24448;&#24191;&#27867;&#22320;&#34701;&#21512;&#22810;&#20010;&#25968;&#25454;&#26469;&#28304;&#65292;&#21487;&#33021;&#26159;&#21457;&#34920;&#30340;&#35770;&#25991;&#12289;&#19987;&#21033;&#12289;&#24320;&#25918;&#36164;&#28304;&#24211;&#25110;&#20854;&#20182;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#20449;&#24687;&#26469;&#28304;&#30340;&#22522;&#30784;&#29289;&#29702;&#21442;&#25968;&#30340;&#36136;&#37327;&#21644;&#20840;&#38754;&#24615;&#30340;&#24046;&#24322;&#65292;&#21487;&#33021;&#23545;&#31995;&#32479;&#20248;&#21270;&#36807;&#31243;&#20135;&#29983;&#21518;&#32493;&#24433;&#21709;&#65292;&#21364;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#65288;LVGP&#65289;&#30340;&#22810;&#28304;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of artificial intelligence (AI) and machine learning (ML), various domains of science and engineering communites has leveraged data-driven surrogates to model complex systems from numerous sources of information (data). The proliferation has led to significant reduction in cost and time involved in development of superior systems designed to perform specific functionalities. A high proposition of such surrogates are built extensively fusing multiple sources of data, may it be published papers, patents, open repositories, or other resources. However, not much attention has been paid to the differences in quality and comprehensiveness of the known and unknown underlying physical parameters of the information sources that could have downstream implications during system optimization. Towards resolving this issue, a multi-source data fusion framework based on Latent Variable Gaussian Process (LVGP) is proposed. The individual data sources are tagged as a characteristic cate
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#34394;&#25311;&#24322;&#24120;&#20540;&#26469;&#25913;&#21892;&#26080;&#38656;&#22238;&#39038;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#21516;&#20219;&#21153;&#38388;&#30340;&#31867;&#21035;&#28151;&#28102;&#38382;&#39064;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;&#25552;&#31034;&#26597;&#35810;&#21644;&#32452;&#21512;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.04129</link><description>&lt;p&gt;
OVOR&#65306;&#19968;&#31181;&#20351;&#29992;&#34394;&#25311;&#24322;&#24120;&#20540;&#27491;&#21017;&#21270;&#30340;OnePrompt&#26041;&#27861;&#65292;&#23454;&#29616;&#26080;&#38656;&#22238;&#39038;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04129
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#34394;&#25311;&#24322;&#24120;&#20540;&#26469;&#25913;&#21892;&#26080;&#38656;&#22238;&#39038;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#21516;&#20219;&#21153;&#38388;&#30340;&#31867;&#21035;&#28151;&#28102;&#38382;&#39064;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;&#25552;&#31034;&#26597;&#35810;&#21644;&#32452;&#21512;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#65292;&#22312;&#26080;&#38656;&#22238;&#39038;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#35774;&#32622;&#20013;&#21487;&#20197;&#23454;&#29616;&#27604;&#33879;&#21517;&#30340;&#22522;&#20110;&#22238;&#39038;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#26080;&#38656;&#22238;&#39038;&#30340;CIL&#26041;&#27861;&#22312;&#21306;&#20998;&#19981;&#21516;&#20219;&#21153;&#30340;&#31867;&#21035;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#24182;&#26410;&#19968;&#21516;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34394;&#25311;&#24322;&#24120;&#20540;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#32039;&#32553;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#36793;&#30028;&#65292;&#20943;&#36731;&#19981;&#21516;&#20219;&#21153;&#38388;&#31867;&#21035;&#30340;&#28151;&#28102;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#19968;&#20010;&#23384;&#20648;&#21508;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#30340;&#38598;&#21512;&#65292;&#20197;&#38450;&#27490;&#26032;&#20219;&#21153;&#30340;&#30693;&#35782;&#35206;&#30422;&#20808;&#21069;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#23548;&#33268;&#39069;&#22806;&#30340;&#26597;&#35810;&#21644;&#32452;&#21512;&#36866;&#24403;&#25552;&#31034;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#22312;&#35770;&#25991;&#20013;&#25581;&#31034;&#65292;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#39069;&#22806;&#24320;&#38144;&#32780;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#31616;&#21270;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#19982;&#20808;&#21069;&#26368;&#26032;&#29366;&#24577;-of-the-art&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that by using large pre-trained models along with learnable prompts, rehearsal-free methods for class-incremental learning (CIL) settings can achieve superior performance to prominent rehearsal-based ones. Rehearsal-free CIL methods struggle with distinguishing classes from different tasks, as those are not trained together. In this work we propose a regularization method based on virtual outliers to tighten decision boundaries of the classifier, such that confusion of classes among different tasks is mitigated. Recent prompt-based methods often require a pool of task-specific prompts, in order to prevent overwriting knowledge of previous tasks with that of the new task, leading to extra computation in querying and composing an appropriate prompt from the pool. This additional cost can be eliminated, without sacrificing accuracy, as we reveal in the paper. We illustrate that a simplified prompt-based method can achieve results comparable to previous state-of-the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31185;&#23398;&#35821;&#35328;&#24314;&#27169;&#65288;SLM&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35299;&#20915;&#20998;&#23376;&#24314;&#27169;&#21644;&#35774;&#35745;&#20013;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22810;&#27169;&#24577;&#22522;&#20934;&#21644;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#19982;&#25968;&#25454;&#27169;&#24577;&#21305;&#37197;&#30340;&#37327;&#21270;&#20449;&#24687;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#30693;&#35782;&#23398;&#20064;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.04119</link><description>&lt;p&gt;
&#31185;&#23398;&#35821;&#35328;&#24314;&#27169;&#65306;&#20998;&#23376;&#31185;&#23398;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Scientific Language Modeling: A Quantitative Review of Large Language Models in Molecular Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31185;&#23398;&#35821;&#35328;&#24314;&#27169;&#65288;SLM&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35299;&#20915;&#20998;&#23376;&#24314;&#27169;&#21644;&#35774;&#35745;&#20013;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22810;&#27169;&#24577;&#22522;&#20934;&#21644;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#19982;&#25968;&#25454;&#27169;&#24577;&#21305;&#37197;&#30340;&#37327;&#21270;&#20449;&#24687;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#30693;&#35782;&#23398;&#20064;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#20998;&#23376;&#24314;&#27169;&#21644;&#35774;&#35745;&#23545;&#20110;&#21457;&#29616;&#21644;&#25506;&#32034;&#26032;&#22411;&#20998;&#23376;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24341;&#20837;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#35270;&#35282;&#20026;&#31185;&#23398;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#31185;&#23398;&#35821;&#35328;&#24314;&#27169;&#65288;SLM&#65289;&#30340;&#30740;&#31350;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#22914;&#20309;&#37327;&#21270;&#27169;&#22411;&#19982;&#25968;&#25454;&#27169;&#24577;&#20043;&#38388;&#30340;&#21305;&#37197;&#20197;&#21450;&#22914;&#20309;&#35782;&#21035;&#27169;&#22411;&#30340;&#30693;&#35782;&#23398;&#20064;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ChEBI-20-MM&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#65292;&#24182;&#36827;&#34892;&#20102;1263&#20010;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#27169;&#22411;&#19982;&#25968;&#25454;&#27169;&#24577;&#30340;&#20860;&#23481;&#24615;&#21644;&#30693;&#35782;&#33719;&#21462;&#33021;&#21147;&#12290;&#36890;&#36807;&#27169;&#24577;&#36716;&#31227;&#27010;&#29575;&#30697;&#38453;&#65292;&#25105;&#20204;&#20026;&#20219;&#21153;&#25552;&#20379;&#20102;&#26368;&#21512;&#36866;&#30340;&#27169;&#24577;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#32479;&#35745;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26412;&#22320;&#21270;&#26469;&#21457;&#29616;&#29305;&#23450;&#19978;&#19979;&#25991;&#30340;&#30693;&#35782;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient molecular modeling and design are crucial for the discovery and exploration of novel molecules, and the incorporation of deep learning methods has revolutionized this field. In particular, large language models (LLMs) offer a fresh approach to tackle scientific problems from a natural language processing (NLP) perspective, introducing a research paradigm called scientific language modeling (SLM). However, two key issues remain: how to quantify the match between model and data modalities and how to identify the knowledge-learning preferences of models. To address these challenges, we propose a multi-modal benchmark, named ChEBI-20-MM, and perform 1263 experiments to assess the model's compatibility with data modalities and knowledge acquisition. Through the modal transition probability matrix, we provide insights into the most suitable modalities for tasks. Furthermore, we introduce a statistically interpretable approach to discover context-specific knowledge mapping by locali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37327;&#21270;&#20102;&#32852;&#37030;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#20013;&#24322;&#36136;&#24615;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;SCAFFLSA&#20316;&#20026;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#26469;&#28040;&#38500;&#27492;&#20559;&#24046;&#12290;&#22312;&#32852;&#37030;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#20013;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04114</link><description>&lt;p&gt;
SCAFFLSA&#65306;&#37327;&#21270;&#21644;&#28040;&#38500;&#32852;&#37030;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#21644;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#20013;&#30340;&#24322;&#36136;&#24615;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
SCAFFLSA: Quantifying and Eliminating Heterogeneity Bias in Federated Linear Stochastic Approximation and Temporal Difference Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37327;&#21270;&#20102;&#32852;&#37030;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#20013;&#24322;&#36136;&#24615;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;SCAFFLSA&#20316;&#20026;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#26469;&#28040;&#38500;&#27492;&#20559;&#24046;&#12290;&#22312;&#32852;&#37030;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#20013;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#32852;&#37030;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65288;FedLSA&#65289;&#36827;&#34892;&#20102;&#38750;&#28176;&#36827;&#20998;&#26512;&#12290;&#25105;&#20204;&#26126;&#30830;&#37327;&#21270;&#20102;&#24322;&#36136;&#20195;&#29702;&#26412;&#22320;&#35757;&#32451;&#24341;&#20837;&#30340;&#20559;&#24046;&#65292;&#24182;&#30740;&#31350;&#20102;&#35813;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;FedLSA&#30340;&#36890;&#20449;&#22797;&#26434;&#24615;&#19982;&#25152;&#38656;&#31934;&#24230; $\epsilon$ &#21576;&#22810;&#39033;&#24335;&#20851;&#31995;&#65292;&#36825;&#38480;&#21046;&#20102;&#32852;&#37030;&#30340;&#22909;&#22788;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SCAFFLSA&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;FedLSA&#21464;&#20307;&#65292;&#23427;&#20351;&#29992;&#25511;&#21046;&#21464;&#37327;&#26469;&#26657;&#27491;&#26412;&#22320;&#35757;&#32451;&#30340;&#20559;&#24046;&#65292;&#24182;&#22312;&#19981;&#23545;&#32479;&#35745;&#24322;&#36136;&#24615;&#20570;&#20986;&#20219;&#20309;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#32852;&#37030;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#65292;&#24182;&#20998;&#26512;&#20102;&#30456;&#24212;&#30340;&#22797;&#26434;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we perform a non-asymptotic analysis of the federated linear stochastic approximation (FedLSA) algorithm. We explicitly quantify the bias introduced by local training with heterogeneous agents, and investigate the sample complexity of the algorithm. We show that the communication complexity of FedLSA scales polynomially with the desired precision $\epsilon$, which limits the benefits of federation. To overcome this, we propose SCAFFLSA, a novel variant of FedLSA, that uses control variates to correct the bias of local training, and prove its convergence without assumptions on statistical heterogeneity. We apply the proposed methodology to federated temporal difference learning with linear function approximation, and analyze the corresponding complexity improvements.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20107;&#20214;&#25551;&#36848;&#30340;&#26426;&#22120;&#23398;&#20064;&#20915;&#31574;&#25903;&#25345;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21015;&#36710;&#31649;&#29702;&#31995;&#32479;&#20013;&#33258;&#21160;&#20998;&#37197;&#24310;&#36831;&#24402;&#22240;&#20195;&#30721;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20998;&#23618;&#26041;&#27861;&#27604;&#25153;&#24179;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#20173;&#28982;&#19981;&#21450;&#25163;&#21160;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04108</link><description>&lt;p&gt;
&#22312;&#21015;&#36710;&#31649;&#29702;&#31995;&#32479;&#20013;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#36827;&#34892;&#20998;&#23618;&#24310;&#36831;&#24402;&#22240;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Delay Attribution Classification using Unstructured Text in Train Management Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20107;&#20214;&#25551;&#36848;&#30340;&#26426;&#22120;&#23398;&#20064;&#20915;&#31574;&#25903;&#25345;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21015;&#36710;&#31649;&#29702;&#31995;&#32479;&#20013;&#33258;&#21160;&#20998;&#37197;&#24310;&#36831;&#24402;&#22240;&#20195;&#30721;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20998;&#23618;&#26041;&#27861;&#27604;&#25153;&#24179;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#20173;&#28982;&#19981;&#21450;&#25163;&#21160;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27431;&#30431;&#25351;&#20196;&#35201;&#27714;&#23545;&#21015;&#36710;&#24310;&#36831;&#36827;&#34892;&#31995;&#32479;&#24615;&#36319;&#36394;&#12290;&#22312;&#29790;&#20856;&#65292;&#29790;&#20856;&#20132;&#36890;&#31649;&#29702;&#23616;&#27880;&#20876;&#24182;&#20998;&#37197;&#36866;&#24403;&#30340;&#24310;&#36831;&#24402;&#22240;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;&#24310;&#36831;&#24402;&#22240;&#20195;&#30721;&#26159;&#25163;&#21160;&#20998;&#37197;&#30340;&#65292;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20107;&#20214;&#25551;&#36848;&#36827;&#34892;&#24310;&#36831;&#24402;&#22240;&#20195;&#30721;&#20998;&#37197;&#30340;&#26426;&#22120;&#23398;&#20064;&#20915;&#31574;&#25903;&#25345;&#12290;&#20351;&#29992;TF-IDF&#36716;&#25442;&#25991;&#26412;&#65292;&#24182;&#35780;&#20272;&#20102;&#20004;&#20010;&#27169;&#22411;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#19982;&#38543;&#26426;&#22343;&#21248;&#20998;&#31867;&#22120;&#20197;&#21450;&#29790;&#20856;&#20132;&#36890;&#31649;&#29702;&#23616;&#30340;&#20998;&#31867;&#24615;&#33021;&#36827;&#34892;&#23545;&#27604;&#12290;&#27492;&#22806;&#65292;&#35813;&#38382;&#39064;&#34987;&#24314;&#27169;&#20026;&#20998;&#23618;&#21644;&#25153;&#24179;&#20004;&#31181;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20998;&#23618;&#26041;&#27861;&#27604;&#25153;&#24179;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#20004;&#31181;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#38543;&#26426;&#22343;&#21248;&#20998;&#31867;&#22120;&#65292;&#20294;&#34920;&#29616;&#19981;&#21450;&#25163;&#21160;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
EU directives stipulate a systematic follow-up of train delays. In Sweden, the Swedish Transport Administration registers and assigns an appropriate delay attribution code. However, this delay attribution code is assigned manually, which is a complex task. In this paper, a machine learning-based decision support for assigning delay attribution codes based on event descriptions is investigated. The text is transformed using TF-IDF, and two models, Random Forest and Support Vector Machine, are evaluated against a random uniform classifier and the classification performance of the Swedish Transport Administration. Further, the problem is modeled as both a hierarchical and flat approach. The results indicate that a hierarchical approach performs better than a flat approach. Both approaches perform better than the random uniform classifier but perform worse than the manual classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#23458;&#25143;&#20998;&#32676;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#22522;&#20110;&#33521;&#22269;&#30340;&#22312;&#32447;&#38646;&#21806;&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#65292;&#20351;&#29992;RFM&#26694;&#26550;&#21644;&#26368;&#20808;&#36827;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#38646;&#21806;&#24066;&#22330;&#34892;&#19994;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.04103</link><description>&lt;p&gt;
&#33521;&#22269;&#38646;&#21806;&#24066;&#22330;&#30340;&#23458;&#25143;&#20998;&#32676;&#31639;&#27861;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
An Exploration of Clustering Algorithms for Customer Segmentation in the UK Retail Market
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#23458;&#25143;&#20998;&#32676;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#22522;&#20110;&#33521;&#22269;&#30340;&#22312;&#32447;&#38646;&#21806;&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#65292;&#20351;&#29992;RFM&#26694;&#26550;&#21644;&#26368;&#20808;&#36827;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#38646;&#21806;&#24066;&#22330;&#34892;&#19994;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#22312;&#32447;&#36141;&#20080;&#30340;&#24847;&#35782;&#26174;&#33879;&#25552;&#39640;&#65292;&#36825;&#23548;&#33268;&#20102;&#22312;&#32447;&#38646;&#21806;&#24179;&#21488;&#30340;&#20986;&#29616;&#65292;&#23545;&#20110;&#23458;&#25143;&#36141;&#20080;&#34892;&#20026;&#30340;&#26356;&#22909;&#29702;&#35299;&#30340;&#38656;&#27714;&#20063;&#22240;&#27492;&#20135;&#29983;&#12290;&#38646;&#21806;&#20844;&#21496;&#38754;&#20020;&#30528;&#22788;&#29702;&#22823;&#37327;&#23458;&#25143;&#36141;&#20080;&#30340;&#21387;&#21147;&#65292;&#36825;&#38656;&#35201;&#26356;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#23458;&#25143;&#20998;&#32676;&#26041;&#27861;&#12290;&#23458;&#25143;&#20998;&#32676;&#26159;&#19968;&#31181;&#24066;&#22330;&#20998;&#26512;&#24037;&#20855;&#65292;&#26377;&#21161;&#20110;&#23458;&#25143;&#20013;&#24515;&#26381;&#21153;&#65292;&#20174;&#32780;&#25552;&#39640;&#30408;&#21033;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#23458;&#25143;&#20998;&#32676;&#27169;&#22411;&#65292;&#25913;&#21892;&#38646;&#21806;&#24066;&#22330;&#34892;&#19994;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#26469;&#33258;UCI&#26426;&#22120;&#23398;&#20064;&#24211;&#30340;&#22522;&#20110;&#33521;&#22269;&#30340;&#22312;&#32447;&#38646;&#21806;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#38646;&#21806;&#25968;&#25454;&#38598;&#21253;&#21547;541,909&#20010;&#23458;&#25143;&#35760;&#24405;&#21644;8&#20010;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;RFM&#65288;&#26368;&#36817;&#24615;&#12289;&#39057;&#29575;&#21644;&#36135;&#24065;&#65289;&#26694;&#26550;&#26469;&#37327;&#21270;&#23458;&#25143;&#20215;&#20540;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#32858;&#31867;&#31639;&#27861;&#65288;SOTA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, peoples awareness of online purchases has significantly risen. This has given rise to online retail platforms and the need for a better understanding of customer purchasing behaviour. Retail companies are pressed with the need to deal with a high volume of customer purchases, which requires sophisticated approaches to perform more accurate and efficient customer segmentation. Customer segmentation is a marketing analytical tool that aids customer-centric service and thus enhances profitability. In this paper, we aim to develop a customer segmentation model to improve decision-making processes in the retail market industry. To achieve this, we employed a UK-based online retail dataset obtained from the UCI machine learning repository. The retail dataset consists of 541,909 customer records and eight features. Our study adopted the RFM (recency, frequency, and monetary) framework to quantify customer values. Thereafter, we compared several state-of-the-art (SOTA) clustering alg
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;BERT&#21644;RoBERTa&#36827;&#34892;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#65292;&#24182;&#20934;&#22791;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65288;D2&#65289;&#20197;&#24212;&#23545;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#31867;&#21035;&#21644;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04088</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Use of a Large Language Model for Cyberbullying Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04088
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;BERT&#21644;RoBERTa&#36827;&#34892;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#65292;&#24182;&#20934;&#22791;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65288;D2&#65289;&#20197;&#24212;&#23545;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#31867;&#21035;&#21644;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#30427;&#34892;&#20026;&#24694;&#24847;&#29992;&#25143;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#32593;&#32476;&#27450;&#20940;&#28192;&#36947;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#32593;&#32476;&#27450;&#20940;&#26159;&#24403;&#20170;&#32593;&#32476;&#19990;&#30028;&#20013;&#26368;&#26222;&#36941;&#30340;&#29616;&#35937;&#65292;&#23545;&#20844;&#27665;&#30340;&#24515;&#29702;&#21644;&#36523;&#20307;&#20581;&#24247;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#12290;&#36825;&#23601;&#38656;&#35201;&#24320;&#21457;&#19968;&#20010;&#24378;&#22823;&#30340;&#31995;&#32479;&#65292;&#20197;&#38459;&#27490;&#22312;&#32447;&#35770;&#22363;&#12289;&#21338;&#23458;&#21644;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#27450;&#20940;&#20869;&#23481;&#65292;&#20197;&#31649;&#29702;&#20854;&#23545;&#25105;&#20204;&#31038;&#20250;&#30340;&#24433;&#21709;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#24230;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#27867;&#21270;&#38382;&#39064;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#24182;&#19981;&#31283;&#23450;&#12290;&#36817;&#24180;&#26469;&#65292;&#20687;BERT&#21644;RoBERTa&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;LLM&#22312;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#26041;&#38754;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#25105;&#20204;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#12290;&#25105;&#20204;&#20174;&#29616;&#26377;&#30740;&#31350;&#65288;Formspring&#21644;Twitter&#65289;&#20013;&#20934;&#22791;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65288;D2&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominance of social media has added to the channels of bullying for perpetrators. Unfortunately, cyberbullying (CB) is the most prevalent phenomenon in todays cyber world, and is a severe threat to the mental and physical health of citizens. This opens the need to develop a robust system to prevent bullying content from online forums, blogs, and social media platforms to manage the impact in our society. Several machine learning (ML) algorithms have been proposed for this purpose. However, their performances are not consistent due to high class imbalance and generalisation issues. In recent years, large language models (LLMs) like BERT and RoBERTa have achieved state-of-the-art (SOTA) results in several natural language processing (NLP) tasks. Unfortunately, the LLMs have not been applied extensively for CB detection. In our paper, we explored the use of these models for cyberbullying (CB) detection. We have prepared a new dataset (D2) from existing studies (Formspring and Twitter)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;CLIP&#35757;&#32451;&#26041;&#27861;&#30340;&#19968;&#20010;&#32463;&#20856;&#31639;&#27861;&#65292;&#31216;&#20026;&#39640;&#26031;&#21028;&#21035;&#20998;&#26512;&#65288;GDA&#65289;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;CLIP&#30340;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#29305;&#24449;&#20998;&#24067;&#30340;&#20272;&#35745;&#65292;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;CLIP&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04087</link><description>&lt;p&gt;
&#19968;&#20010;&#38590;&#20197;&#36229;&#36234;&#30340;&#22522;&#32447;&#29992;&#20110;&#26080;&#38656;&#35757;&#32451;&#30340;CLIP&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;CLIP&#35757;&#32451;&#26041;&#27861;&#30340;&#19968;&#20010;&#32463;&#20856;&#31639;&#27861;&#65292;&#31216;&#20026;&#39640;&#26031;&#21028;&#21035;&#20998;&#26512;&#65288;GDA&#65289;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;CLIP&#30340;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#29305;&#24449;&#20998;&#24067;&#30340;&#20272;&#35745;&#65292;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;CLIP&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#22240;&#20854;&#26174;&#33879;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#32780;&#21463;&#21040;&#38738;&#30544;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#24320;&#21457;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#22914;&#25552;&#31034;&#23398;&#20064;&#21644;&#36866;&#37197;&#22120;&#65292;&#20197;&#25552;&#39640;CLIP&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#23545;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#26469;&#35828;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#19968;&#20010;&#32463;&#20856;&#31639;&#27861;&#65292;&#39640;&#26031;&#21028;&#21035;&#20998;&#26512;&#65288;GDA&#65289;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;CLIP&#30340;&#19979;&#28216;&#20998;&#31867;&#12290;&#36890;&#24120;&#65292;GDA&#20551;&#35774;&#27599;&#20010;&#31867;&#21035;&#30340;&#29305;&#24449;&#37117;&#36981;&#24490;&#20855;&#26377;&#30456;&#21516;&#21327;&#26041;&#24046;&#30340;&#39640;&#26031;&#20998;&#24067;&#12290;&#36890;&#36807;&#21033;&#29992;&#36125;&#21494;&#26031;&#20844;&#24335;&#65292;&#21487;&#20197;&#29992;&#31867;&#21035;&#30340;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#26469;&#34920;&#31034;&#20998;&#31867;&#22120;&#65292;&#36825;&#21487;&#20197;&#22312;&#26080;&#38656;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#24471;&#21040;&#12290;&#20026;&#20102;&#25972;&#21512;&#26469;&#33258;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;CLIP&#20013;&#30340;&#21407;&#22987;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#38598;&#25104;&#12290;&#22312;17&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32467;&#26524;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pretraining (CLIP) has gained popularity for its remarkable zero-shot capacity. Recent research has focused on developing efficient fine-tuning methods, such as prompt learning and adapter, to enhance CLIP's performance in downstream tasks. However, these methods still require additional training time and computational resources, which is undesirable for devices with limited resources. In this paper, we revisit a classical algorithm, Gaussian Discriminant Analysis (GDA), and apply it to the downstream classification of CLIP. Typically, GDA assumes that features of each class follow Gaussian distributions with identical covariance. By leveraging Bayes' formula, the classifier can be expressed in terms of the class means and covariance, which can be estimated from the data without the need for training. To integrate knowledge from both visual and textual modalities, we ensemble it with the original zero-shot classifier within CLIP. Extensive results on 17 datas
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#21487;&#35777;&#23398;&#20064;&#22810;&#22836;&#27880;&#24847;&#21147;&#23618;&#30340;&#30740;&#31350;&#65292;&#24182;&#32473;&#20986;&#20102;&#35813;&#38382;&#39064;&#30340;&#38750;&#24179;&#20961;&#30340;&#19978;&#19979;&#30028;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.04084</link><description>&lt;p&gt;
&#21487;&#35777;&#23398;&#20064;&#22810;&#22836;&#27880;&#24847;&#21147;&#23618;
&lt;/p&gt;
&lt;p&gt;
Provably learning a multi-head attention layer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04084
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#21487;&#35777;&#23398;&#20064;&#22810;&#22836;&#27880;&#24847;&#21147;&#23618;&#30340;&#30740;&#31350;&#65292;&#24182;&#32473;&#20986;&#20102;&#35813;&#38382;&#39064;&#30340;&#38750;&#24179;&#20961;&#30340;&#19978;&#19979;&#30028;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#22836;&#27880;&#24847;&#21147;&#23618;&#26159;&#21464;&#24418;&#22120;&#26550;&#26500;&#30340;&#20851;&#38190;&#32452;&#20214;&#20043;&#19968;&#65292;&#20351;&#20854;&#19982;&#20256;&#32479;&#30340;&#21069;&#39304;&#27169;&#22411;&#26377;&#25152;&#21306;&#21035;&#12290;&#36890;&#36807;&#32473;&#23450;&#24207;&#21015;&#38271;&#24230; $k$&#65292;&#27880;&#24847;&#21147;&#30697;&#38453; $\mathbf{\Theta}_1,\ldots,\mathbf{\Theta}_m\in\mathbb{R}^{d\times d}$&#65292;&#20197;&#21450;&#25237;&#24433;&#30697;&#38453; $\mathbf{W}_1,\ldots,\mathbf{W}_m\in\mathbb{R}^{d\times d}$&#65292;&#30456;&#24212;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#23618; $F: \mathbb{R}^{k\times d}\to \mathbb{R}^{k\times d}$ &#36890;&#36807; $F(\mathbf{X}) \triangleq \sum^m_{i=1} \mathrm{softmax}(\mathbf{X}\mathbf{\Theta}_i\mathbf{X}^\top)\mathbf{X}\mathbf{W}_i$ &#26469;&#36716;&#21270;&#38271;&#24230;&#20026; $k$ &#30340; $d$ &#32500;&#20196;&#29260;&#24207;&#21015; $\mathbf{X}\in\mathbb{R}^{k\times d}$&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#36890;&#36807;&#38543;&#26426;&#31034;&#20363;&#21487;&#35777;&#23398;&#20064;&#22810;&#22836;&#27880;&#24847;&#21147;&#23618;&#65292;&#24182;&#20026;&#35813;&#38382;&#39064;&#32473;&#20986;&#20102;&#39318;&#20010;&#38750;&#24179;&#20961;&#30340;&#19978;&#19979;&#30028;&#38480;&#21046;&#65306;&#20551;&#35774; $\{\mathbf{W}_i, \mathbf{\Theta}_i\}$ &#28385;&#36275;&#26576;&#20123;&#38750;&#36864;&#21270;&#26465;&#20214;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010; $(dk)^{O(m^3)}$ &#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multi-head attention layer is one of the key components of the transformer architecture that sets it apart from traditional feed-forward models. Given a sequence length $k$, attention matrices $\mathbf{\Theta}_1,\ldots,\mathbf{\Theta}_m\in\mathbb{R}^{d\times d}$, and projection matrices $\mathbf{W}_1,\ldots,\mathbf{W}_m\in\mathbb{R}^{d\times d}$, the corresponding multi-head attention layer $F: \mathbb{R}^{k\times d}\to \mathbb{R}^{k\times d}$ transforms length-$k$ sequences of $d$-dimensional tokens $\mathbf{X}\in\mathbb{R}^{k\times d}$ via $F(\mathbf{X}) \triangleq \sum^m_{i=1} \mathrm{softmax}(\mathbf{X}\mathbf{\Theta}_i\mathbf{X}^\top)\mathbf{X}\mathbf{W}_i$. In this work, we initiate the study of provably learning a multi-head attention layer from random examples and give the first nontrivial upper and lower bounds for this problem:   - Provided $\{\mathbf{W}_i, \mathbf{\Theta}_i\}$ satisfy certain non-degeneracy conditions, we give a $(dk)^{O(m^3)}$-time algorithm that learns
&lt;/p&gt;</description></item><item><title>&#25151;&#20215;&#39044;&#27979;&#26159;&#25151;&#22320;&#20135;&#21644;&#25269;&#25276;&#36151;&#27454;&#31561;&#22810;&#20010;&#39046;&#22495;&#30340;&#22522;&#26412;&#35201;&#27714;&#65292;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#25151;&#20215;&#39044;&#27979;&#31639;&#27861;XGBoost&#65292;&#36890;&#36807;&#27604;&#36739;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#34920;&#29616;&#65292;&#24182;&#30830;&#23450;&#20851;&#38190;&#22240;&#32032;&#65292;&#32467;&#26524;&#34920;&#26126;XGBoost&#26159;&#26368;&#20339;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.04082</link><description>&lt;p&gt;
&#19968;&#31181;&#20248;&#21270;&#30340;&#25151;&#20215;&#39044;&#27979;&#31639;&#27861;&#65306;XGBoost
&lt;/p&gt;
&lt;p&gt;
An Optimal House Price Prediction Algorithm: XGBoost
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04082
&lt;/p&gt;
&lt;p&gt;
&#25151;&#20215;&#39044;&#27979;&#26159;&#25151;&#22320;&#20135;&#21644;&#25269;&#25276;&#36151;&#27454;&#31561;&#22810;&#20010;&#39046;&#22495;&#30340;&#22522;&#26412;&#35201;&#27714;&#65292;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#25151;&#20215;&#39044;&#27979;&#31639;&#27861;XGBoost&#65292;&#36890;&#36807;&#27604;&#36739;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#34920;&#29616;&#65292;&#24182;&#30830;&#23450;&#20851;&#38190;&#22240;&#32032;&#65292;&#32467;&#26524;&#34920;&#26126;XGBoost&#26159;&#26368;&#20339;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#25151;&#20215;&#26159;&#25151;&#22320;&#20135;&#21644;&#25269;&#25276;&#36151;&#27454;&#31561;&#22810;&#20010;&#39046;&#22495;&#30340;&#22522;&#26412;&#35201;&#27714;&#12290;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#65292;&#25151;&#20135;&#20215;&#20540;&#19981;&#20165;&#20165;&#30001;&#20854;&#29289;&#29702;&#23646;&#24615;&#20915;&#23450;&#65292;&#32780;&#19988;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#20854;&#21608;&#22260;&#31038;&#21306;&#30340;&#24433;&#21709;&#12290;&#22312;&#24179;&#34913;&#39044;&#31639;&#38480;&#21046;&#30340;&#21516;&#26102;&#28385;&#36275;&#20010;&#20154;&#22810;&#26679;&#21270;&#30340;&#20303;&#25151;&#38656;&#27714;&#26159;&#25151;&#22320;&#20135;&#24320;&#21457;&#21830;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#25151;&#20215;&#39044;&#27979;&#38382;&#39064;&#20316;&#20026;&#22238;&#24402;&#20219;&#21153;&#36827;&#34892;&#25506;&#35752;&#65292;&#24182;&#37319;&#29992;&#20102;&#33021;&#22815;&#34920;&#36798;&#29420;&#31435;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#32654;&#22269;&#29233;&#33655;&#21326;&#24030;&#22467;&#22982;&#26031;&#24066;&#30340;&#20303;&#25151;&#25968;&#25454;&#38598;&#65292;&#27604;&#36739;&#20102;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#12289;&#38543;&#26426;&#26862;&#26519;&#22238;&#24402;&#12289;XGBoost&#12289;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#31639;&#27861;&#22312;&#25151;&#20215;&#39044;&#27979;&#20013;&#30340;&#34920;&#29616;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24433;&#21709;&#25151;&#23627;&#25104;&#26412;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#32467;&#26524;&#26174;&#31034;XGBoost&#26159;&#26368;&#20339;&#30340;&#25151;&#20215;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
An accurate prediction of house prices is a fundamental requirement for various sectors including real estate and mortgage lending. It is widely recognized that a property value is not solely determined by its physical attributes but is significantly influenced by its surrounding neighbourhood. Meeting the diverse housing needs of individuals while balancing budget constraints is a primary concern for real estate developers. To this end, we addressed the house price prediction problem as a regression task and thus employed various machine learning techniques capable of expressing the significance of independent variables. We made use of the housing dataset of Ames City in Iowa, USA to compare support vector regressor, random forest regressor, XGBoost, multilayer perceptron and multiple linear regression algorithms for house price prediction. Afterwards, we identified the key factors that influence housing costs. Our results show that XGBoost is the best performing model for house price
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25193;&#20805;&#26435;&#37325;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;MixUp&#26041;&#27861;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#26435;&#37325;&#31354;&#38388;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04081</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#20805;&#25913;&#36827;&#26435;&#37325;&#31354;&#38388;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improved Generalization of Weight Space Networks via Augmentations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04081
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25193;&#20805;&#26435;&#37325;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;MixUp&#26041;&#27861;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#26435;&#37325;&#31354;&#38388;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26435;&#37325;&#31354;&#38388;&#65288;DWS&#65289;&#20013;&#30340;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#22788;&#29702;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#36827;&#34892;&#23398;&#20064;&#65292;&#23427;&#22312;2D&#21644;3D&#31070;&#32463;&#22330;&#65288;INRs&#65292;NeRFs&#65289;&#20197;&#21450;&#23545;&#20854;&#20182;&#31867;&#22411;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25512;&#29702;&#26041;&#38754;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#26435;&#37325;&#31354;&#38388;&#27169;&#22411;&#24448;&#24448;&#23481;&#26131;&#21463;&#21040;&#36807;&#25311;&#21512;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#20102;&#36807;&#25311;&#21512;&#30340;&#21407;&#22240;&#65292;&#24182;&#21457;&#29616;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#26159;DWS&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;&#34429;&#28982;&#19968;&#20010;&#32473;&#23450;&#30340;&#23545;&#35937;&#21487;&#20197;&#34987;&#35768;&#22810;&#19981;&#21516;&#30340;&#26435;&#37325;&#37197;&#32622;&#25152;&#34920;&#31034;&#65292;&#20294;&#20856;&#22411;&#30340;INR&#35757;&#32451;&#38598;&#26410;&#33021;&#25429;&#25417;&#21040;&#34920;&#31034;&#21516;&#19968;&#23545;&#35937;&#30340;&#19981;&#21516;INR&#20043;&#38388;&#30340;&#21464;&#24322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26435;&#37325;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#25193;&#20805;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26435;&#37325;&#31354;&#38388;&#30340;MixUp&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#35774;&#32622;&#20013;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#25552;&#21319;&#31867;&#20284;&#20110;&#25317;&#26377;&#22810;&#36798;10&#20493;&#30340;&#25968;&#25454;&#37327;&#12290;&#22312;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20013;&#65292;&#23427;&#20204;&#20135;&#29983;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning in deep weight spaces (DWS), where neural networks process the weights of other neural networks, is an emerging research direction, with applications to 2D and 3D neural fields (INRs, NeRFs), as well as making inferences about other types of neural networks. Unfortunately, weight space models tend to suffer from substantial overfitting. We empirically analyze the reasons for this overfitting and find that a key reason is the lack of diversity in DWS datasets. While a given object can be represented by many different weight configurations, typical INR training sets fail to capture variability across INRs that represent the same object. To address this, we explore strategies for data augmentation in weight spaces and propose a MixUp method adapted for weight spaces. We demonstrate the effectiveness of these methods in two setups. In classification, they improve performance similarly to having up to 10 times more data. In self-supervised contrastive learning, they yield substanti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29109;&#27491;&#21017;&#21270;&#30340;&#25193;&#25955;&#31574;&#30053;&#19982;Q-&#38598;&#21512;&#30456;&#32467;&#21512;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#19968;&#20010;&#22797;&#26434;&#30340;&#21160;&#20316;&#20998;&#24067;&#36716;&#21270;&#20026;&#26631;&#20934;&#39640;&#26031;&#20998;&#24067;&#65292;&#28982;&#21518;&#20351;&#29992;&#36870;&#26102;&#38388;SDE&#37319;&#26679;&#21160;&#20316;&#65292;&#20197;&#25913;&#21892;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;Q-&#38598;&#21512;&#30340;&#19979;&#20449;&#24515;&#30028;&#23454;&#29616;&#26356;&#24378;&#20581;&#30340;&#31574;&#30053;&#25913;&#36827;&#12290;&#22312;D4RL&#22522;&#20934;&#20219;&#21153;&#30340;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04080</link><description>&lt;p&gt;
&#29109;&#27491;&#21017;&#21270;&#25193;&#25955;&#31574;&#30053;&#19982;Q-&#38598;&#21512;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29109;&#27491;&#21017;&#21270;&#30340;&#25193;&#25955;&#31574;&#30053;&#19982;Q-&#38598;&#21512;&#30456;&#32467;&#21512;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#19968;&#20010;&#22797;&#26434;&#30340;&#21160;&#20316;&#20998;&#24067;&#36716;&#21270;&#20026;&#26631;&#20934;&#39640;&#26031;&#20998;&#24067;&#65292;&#28982;&#21518;&#20351;&#29992;&#36870;&#26102;&#38388;SDE&#37319;&#26679;&#21160;&#20316;&#65292;&#20197;&#25913;&#21892;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;Q-&#38598;&#21512;&#30340;&#19979;&#20449;&#24515;&#30028;&#23454;&#29616;&#26356;&#24378;&#20581;&#30340;&#31574;&#30053;&#25913;&#36827;&#12290;&#22312;D4RL&#22522;&#20934;&#20219;&#21153;&#30340;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35757;&#32451;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25193;&#25955;&#31574;&#30053;&#30340;&#20808;&#36827;&#25216;&#26415;&#12290;&#26680;&#24515;&#26159;&#19968;&#20010;&#22343;&#20540;&#22238;&#24402;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#65292;&#23427;&#23558;&#22797;&#26434;&#30340;&#21160;&#20316;&#20998;&#24067;&#36716;&#21270;&#20026;&#26631;&#20934;&#39640;&#26031;&#20998;&#24067;&#65292;&#28982;&#21518;&#22312;&#29615;&#22659;&#29366;&#24577;&#26465;&#20214;&#19979;&#20351;&#29992;&#30456;&#24212;&#30340;&#36870;&#26102;&#38388;SDE&#37319;&#26679;&#21160;&#20316;&#65292;&#31867;&#20284;&#20110;&#20856;&#22411;&#30340;&#25193;&#25955;&#31574;&#30053;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#19968;&#20010;SDE&#26377;&#35299;&#65292;&#25105;&#20204;&#21487;&#20197;&#29992;&#23427;&#26469;&#35745;&#31639;&#31574;&#30053;&#30340;&#23545;&#25968;&#27010;&#29575;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#29109;&#27491;&#21017;&#39033;&#65292;&#25913;&#36827;&#20102;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;&#20026;&#20102;&#20943;&#36731;&#26469;&#33258;&#20998;&#24067;&#22806;&#25968;&#25454;&#28857;&#30340;&#19981;&#20934;&#30830;&#20540;&#20989;&#25968;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#23398;&#20064;Q-&#38598;&#21512;&#30340;&#19979;&#20449;&#24515;&#30028;&#20197;&#23454;&#29616;&#26356;&#24378;&#20581;&#30340;&#31574;&#30053;&#25913;&#36827;&#12290;&#36890;&#36807;&#23558;&#29109;&#27491;&#21017;&#21270;&#25193;&#25955;&#31574;&#30053;&#19982;Q-&#38598;&#21512;&#32467;&#21512;&#24212;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;D4RL&#22522;&#20934;&#20219;&#21153;&#30340;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;\href{https://github.com/ruoqizzz/Entro}{https://github.com/ruoqizzz/Entro}&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents advanced techniques of training diffusion policies for offline reinforcement learning (RL). At the core is a mean-reverting stochastic differential equation (SDE) that transfers a complex action distribution into a standard Gaussian and then samples actions conditioned on the environment state with a corresponding reverse-time SDE, like a typical diffusion policy. We show that such an SDE has a solution that we can use to calculate the log probability of the policy, yielding an entropy regularizer that improves the exploration of offline datasets. To mitigate the impact of inaccurate value functions from out-of-distribution data points, we further propose to learn the lower confidence bound of Q-ensembles for more robust policy improvement. By combining the entropy-regularized diffusion policy with Q-ensembles in offline RL, our method achieves state-of-the-art performance on most tasks in D4RL benchmarks. Code is available at \href{https://github.com/ruoqizzz/Entro
&lt;/p&gt;</description></item><item><title>&#26816;&#32034;&#20197;&#35299;&#37322;&#65288;R2E&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Shapley&#20540;&#30830;&#23450;&#35777;&#25454;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#40657;&#30418;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#33647;&#29289;&#38774;&#28857;&#37492;&#23450;&#20219;&#21153;&#20013;&#65292;R2E&#27169;&#22411;&#22312;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#22522;&#22240;&#23398;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.04068</link><description>&lt;p&gt;
&#26816;&#32034;&#20197;&#35299;&#37322;&#65306;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#25454;&#39537;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Retrieve to Explain: Evidence-driven Predictions with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04068
&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#20197;&#35299;&#37322;&#65288;R2E&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Shapley&#20540;&#30830;&#23450;&#35777;&#25454;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#40657;&#30418;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#33647;&#29289;&#38774;&#28857;&#37492;&#23450;&#20219;&#21153;&#20013;&#65292;R2E&#27169;&#22411;&#22312;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#22522;&#22240;&#23398;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#35821;&#35328;&#27169;&#22411;&#65292;&#24448;&#24448;&#38590;&#20197;&#28145;&#20837;&#20998;&#26512;&#12290;&#40657;&#30418;&#27169;&#22411;&#21487;&#33021;&#25513;&#30422;&#20102;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#38382;&#39064;&#21644;&#26377;&#23475;&#20559;&#24046;&#12290;&#23545;&#20110;&#20154;&#26426;&#21327;&#20316;&#36807;&#31243;&#26469;&#35828;&#65292;&#19981;&#36879;&#26126;&#30340;&#39044;&#27979;&#21487;&#33021;&#23548;&#33268;&#32570;&#20047;&#20449;&#20219;&#65292;&#38480;&#21046;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21363;&#20351;&#27169;&#22411;&#30340;&#24615;&#33021;&#24456;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26816;&#32034;&#20197;&#35299;&#37322;&#65288;Retrieve to Explain&#65292;&#31616;&#31216;R2E&#65289;&#12290;R2E&#26159;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26681;&#25454;&#25991;&#26723;&#35821;&#26009;&#24211;&#20013;&#30340;&#35777;&#25454;&#65292;&#20351;&#29992;Shapley&#20540;&#26469;&#30830;&#23450;&#35777;&#25454;&#23545;&#26368;&#32456;&#39044;&#27979;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#27169;&#26495;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#32435;&#20837;&#20854;&#20013;&#12290;R2E&#33021;&#22815;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#26032;&#30340;&#35777;&#25454;&#65292;&#24182;&#19988;&#33021;&#22815;&#36890;&#36807;&#27169;&#26495;&#21270;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#32435;&#20837;&#21040;&#33258;&#28982;&#35821;&#35328;&#20013;&#12290;&#25105;&#20204;&#22312;&#36890;&#36807;&#20998;&#26512;&#24050;&#21457;&#34920;&#30340;&#31185;&#23398;&#25991;&#29486;&#36827;&#34892;&#33647;&#29289;&#38774;&#28857;&#37492;&#23450;&#30340;&#23454;&#38469;&#26696;&#20363;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#26041;&#38754;&#20248;&#20110;&#34892;&#19994;&#26631;&#20934;&#30340;&#22522;&#22240;&#23398;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models, particularly language models, are notoriously difficult to introspect. Black-box models can mask both issues in model training and harmful biases. For human-in-the-loop processes, opaque predictions can drive lack of trust, limiting a model's impact even when it performs effectively. To address these issues, we introduce Retrieve to Explain (R2E). R2E is a retrieval-based language model that prioritizes amongst a pre-defined set of possible answers to a research question based on the evidence in a document corpus, using Shapley values to identify the relative importance of pieces of evidence to the final prediction. R2E can adapt to new evidence without retraining, and incorporate structured data through templating into natural language. We assess on the use case of drug target identification from published scientific literature, where we show that the model outperforms an industry-standard genetics-based approach on predicting clinical trial outcomes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#36866;&#29992;&#20110;&#20851;&#31995;&#36229;&#22270;&#30340;&#38142;&#25509;&#39044;&#27979;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20123;&#26694;&#26550;&#22312;&#21508;&#31181;&#20851;&#31995;&#36229;&#22270;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04062</link><description>&lt;p&gt;
&#20351;&#29992;&#20851;&#31995;&#36229;&#22270;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Link Prediction with Relational Hypergraphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#36866;&#29992;&#20110;&#20851;&#31995;&#36229;&#22270;&#30340;&#38142;&#25509;&#39044;&#27979;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20123;&#26694;&#26550;&#22312;&#21508;&#31181;&#20851;&#31995;&#36229;&#22270;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#24050;&#32463;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#30740;&#31350;&#65292;&#23548;&#33268;&#20102;&#20855;&#26377;&#25104;&#21151;&#24212;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#20016;&#23500;&#26223;&#35266;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#20307;&#31995;&#32467;&#26500;&#30340;&#25104;&#21151;&#36716;&#31227;&#21040;&#20351;&#29992;&#20851;&#31995;&#36229;&#22270;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20851;&#31995;&#36229;&#36793;&#30340;&#23384;&#22312;&#20351;&#24471;&#38142;&#25509;&#39044;&#27979;&#25104;&#20026;&#22312;&#19981;&#21516;&#36873;&#25321;&#30340;k&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#20219;&#21153;&#65292;&#36825;&#27604;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#35201;&#22256;&#38590;&#24471;&#22810;&#65292;&#22240;&#20026;&#27599;&#20010;&#20851;&#31995;&#37117;&#26159;&#20108;&#36827;&#21046;&#30340;&#65288;k=2&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20351;&#29992;&#20851;&#31995;&#36229;&#22270;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#30456;&#24212;&#30340;&#20851;&#31995;Weisfeiler-Leman&#31639;&#27861;&#20197;&#21450;&#19968;&#20123;&#33258;&#28982;&#36923;&#36753;&#24418;&#24335;&#23545;&#29983;&#25104;&#30340;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#22312;&#21508;&#31181;&#20851;&#31995;&#36229;&#22270;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction with knowledge graphs has been thoroughly studied in graph machine learning, leading to a rich landscape of graph neural network architectures with successful applications. Nonetheless, it remains challenging to transfer the success of these architectures to link prediction with relational hypergraphs. The presence of relational hyperedges makes link prediction a task between $k$ nodes for varying choices of $k$, which is substantially harder than link prediction with knowledge graphs, where every relation is binary ($k=2$). In this paper, we propose two frameworks for link prediction with relational hypergraphs and conduct a thorough analysis of the expressive power of the resulting model architectures via corresponding relational Weisfeiler-Leman algorithms, and also via some natural logical formalisms. Through extensive empirical analysis, we validate the power of the proposed model architectures on various relational hypergraph benchmarks. The resulting model archit
&lt;/p&gt;</description></item><item><title>TopoNav&#26159;&#19968;&#31181;&#25299;&#25169;&#23548;&#33322;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20027;&#21160;&#25299;&#25169;&#26144;&#23556;&#12289;&#20869;&#37096;&#22870;&#21169;&#26426;&#21046;&#21644;&#23618;&#27425;&#21270;&#30446;&#26631;&#20248;&#20808;&#32423;&#30340;&#32452;&#21512;&#26469;&#23454;&#29616;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#39640;&#25928;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.04061</link><description>&lt;p&gt;
TopoNav&#65306;&#33410;&#32422;&#22870;&#21169;&#29615;&#22659;&#20013;&#39640;&#25928;&#25506;&#32034;&#30340;&#25299;&#25169;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
TopoNav: Topological Navigation for Efficient Exploration in Sparse Reward Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04061
&lt;/p&gt;
&lt;p&gt;
TopoNav&#26159;&#19968;&#31181;&#25299;&#25169;&#23548;&#33322;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#20027;&#21160;&#25299;&#25169;&#26144;&#23556;&#12289;&#20869;&#37096;&#22870;&#21169;&#26426;&#21046;&#21644;&#23618;&#27425;&#21270;&#30446;&#26631;&#20248;&#20808;&#32423;&#30340;&#32452;&#21512;&#26469;&#23454;&#29616;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#39640;&#25928;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#21306;&#22495;&#30340;&#25506;&#32034;&#38754;&#20020;&#30528;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#8212;&#8212;&#22312;&#27809;&#26377;&#20808;&#21069;&#22320;&#22270;&#21644;&#26377;&#38480;&#22806;&#37096;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#23548;&#33322;&#12290;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#65292;&#36825;&#20010;&#25361;&#25112;&#26356;&#21152;&#20005;&#23803;&#65292;&#20256;&#32479;&#30340;&#25506;&#32034;&#25216;&#26415;&#24448;&#24448;&#22833;&#36133;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TopoNav&#65292;&#19968;&#31181;&#20840;&#26032;&#30340;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#23454;&#29616;&#39640;&#25928;&#12289;&#36866;&#24212;&#24615;&#24378;&#19988;&#30446;&#26631;&#23548;&#21521;&#30340;&#25506;&#32034;&#12290;TopoNav&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#26159;&#20027;&#21160;&#25299;&#25169;&#26144;&#23556;&#12289;&#20869;&#37096;&#22870;&#21169;&#26426;&#21046;&#21644;&#23618;&#27425;&#21270;&#30446;&#26631;&#20248;&#20808;&#32423;&#12290;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#65292;TopoNav&#26500;&#24314;&#20102;&#21160;&#24577;&#25299;&#25169;&#22320;&#22270;&#65292;&#25429;&#33719;&#20851;&#38190;&#20301;&#32622;&#21644;&#36335;&#24452;&#12290;&#23427;&#21033;&#29992;&#20869;&#37096;&#22870;&#21169;&#26469;&#25351;&#23548;&#26426;&#22120;&#20154;&#26397;&#30528;&#22320;&#22270;&#20013;&#25351;&#23450;&#30340;&#23376;&#30446;&#26631;&#21069;&#36827;&#65292;&#20419;&#36827;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#30340;&#32467;&#26500;&#21270;&#25506;&#32034;&#12290;&#20026;&#20102;&#30830;&#20445;&#39640;&#25928;&#23548;&#33322;&#65292;TopoNav&#37319;&#29992;&#20102;&#20998;&#23618;&#30446;&#26631;&#39537;&#21160;&#30340;&#20027;&#21160;&#25299;&#25169;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20248;&#20808;&#32771;&#34385;&#26368;&#32039;&#24613;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous robots exploring unknown areas face a significant challenge -- navigating effectively without prior maps and with limited external feedback. This challenge intensifies in sparse reward environments, where traditional exploration techniques often fail. In this paper, we introduce TopoNav, a novel framework that empowers robots to overcome these constraints and achieve efficient, adaptable, and goal-oriented exploration. TopoNav's fundamental building blocks are active topological mapping, intrinsic reward mechanisms, and hierarchical objective prioritization. Throughout its exploration, TopoNav constructs a dynamic topological map that captures key locations and pathways. It utilizes intrinsic rewards to guide the robot towards designated sub-goals within this map, fostering structured exploration even in sparse reward settings. To ensure efficient navigation, TopoNav employs the Hierarchical Objective-Driven Active Topologies framework, enabling the robot to prioritize immed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#32508;&#36848;&#19981;&#21516;&#30340;&#26041;&#27861;&#20197;&#21450;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#38480;&#21046;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#25913;&#36827;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04059</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Multivariate Time Series Imputation: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#32508;&#36848;&#19981;&#21516;&#30340;&#26041;&#27861;&#20197;&#21450;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#38480;&#21046;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#25913;&#36827;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26222;&#36941;&#23384;&#22312;&#30340;&#32570;&#22833;&#20540;&#23548;&#33268;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#37096;&#20998;&#35266;&#27979;&#65292;&#30772;&#22351;&#20102;&#26102;&#38388;&#24207;&#21015;&#30340;&#23436;&#25972;&#24615;&#65292;&#38459;&#30861;&#20102;&#26377;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#26512;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#25554;&#34917;&#26041;&#27861;&#22312;&#25552;&#39640;&#25439;&#22351;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#23545;&#26368;&#36817;&#25552;&#20986;&#30340;&#28145;&#24230;&#23398;&#20064;&#25554;&#34917;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24378;&#35843;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#38480;&#21046;&#26469;&#36827;&#34892;&#20102;&#32467;&#26500;&#21270;&#30340;&#32508;&#36848;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23454;&#35777;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#25913;&#36827;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#26410;&#26469;&#30740;&#31350;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#25152;&#26377;&#20195;&#30721;&#21644;&#37197;&#32622;&#65292;&#21253;&#25324;&#23450;&#26399;&#32500;&#25252;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#35770;&#25991;&#21015;&#34920;&#65292;&#21487;&#20197;&#22312;&#20197;&#19979;&#20301;&#32622;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ubiquitous missing values cause the multivariate time series data to be partially observed, destroying the integrity of time series and hindering the effective time series data analysis. Recently deep learning imputation methods have demonstrated remarkable success in elevating the quality of corrupted time series data, subsequently enhancing performance in downstream tasks. In this paper, we conduct a comprehensive survey on the recently proposed deep learning imputation methods. First, we propose a taxonomy for the reviewed methods, and then provide a structured review of these methods by highlighting their strengths and limitations. We also conduct empirical experiments to study different methods and compare their enhancement for downstream tasks. Finally, the open issues for future research on multivariate time series imputation are pointed out. All code and configurations of this work, including a regularly maintained multivariate time series imputation paper list, can be foun
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#26356;&#28789;&#27963;&#30340;PAC-Bayesian&#20803;&#23398;&#20064;&#65292;&#20801;&#35768;&#26356;&#28789;&#27963;&#30340;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#25552;&#20379;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#21487;&#36866;&#29992;&#20110;&#20998;&#26512;&#21644;&#35774;&#35745;&#21508;&#31181;&#20803;&#23398;&#20064;&#26426;&#21046;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25913;&#21892;&#20102;&#39044;&#27979;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.04054</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#26356;&#28789;&#27963;&#30340;PAC-Bayesian&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
More Flexible PAC-Bayesian Meta-Learning by Learning Learning Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04054
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#26356;&#28789;&#27963;&#30340;PAC-Bayesian&#20803;&#23398;&#20064;&#65292;&#20801;&#35768;&#26356;&#28789;&#27963;&#30340;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#25552;&#20379;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#21487;&#36866;&#29992;&#20110;&#20998;&#26512;&#21644;&#35774;&#35745;&#21508;&#31181;&#20803;&#23398;&#20064;&#26426;&#21046;&#65292;&#24182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25913;&#21892;&#20102;&#39044;&#27979;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;PAC-Bayesian&#29702;&#35770;&#30740;&#31350;&#20803;&#23398;&#20064;&#26041;&#27861;&#30340;&#26032;&#26694;&#26550;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#20854;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#23427;&#20801;&#35768;&#22312;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#20013;&#26356;&#20855;&#28789;&#27963;&#24615;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#21482;&#33021;&#36890;&#36807;&#23398;&#20064;&#27169;&#22411;&#30340;&#20808;&#39564;&#20998;&#24067;&#38388;&#25509;&#21457;&#29983;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#30340;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#26356;&#30452;&#25509;&#22320;&#34920;&#36798;&#20102;&#20803;&#23398;&#20064;&#30340;&#36807;&#31243;&#65292;&#21363;&#23398;&#20064;&#36866;&#29992;&#20110;&#23558;&#26469;&#20219;&#21153;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#28789;&#27963;&#24615;&#20351;&#20854;&#36866;&#29992;&#20110;&#20998;&#26512;&#21508;&#31181;&#20803;&#23398;&#20064;&#26426;&#21046;&#29978;&#33267;&#35774;&#35745;&#26032;&#30340;&#26426;&#21046;&#12290;&#38500;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#23454;&#38469;&#20803;&#23398;&#20064;&#26426;&#21046;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#39640;&#20102;&#39044;&#27979;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new framework for studying meta-learning methods using PAC-Bayesian theory. Its main advantage over previous work is that it allows for more flexibility in how the transfer of knowledge between tasks is realized. For previous approaches, this could only happen indirectly, by means of learning prior distributions over models. In contrast, the new generalization bounds that we prove express the process of meta-learning much more directly as learning the learning algorithm that should be used for future tasks. The flexibility of our framework makes it suitable to analyze a wide range of meta-learning mechanisms and even design new mechanisms. Other than our theoretical contributions we also show empirically that our framework improves the prediction quality in practical meta-learning mechanisms.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#25490;&#21015;&#30340;&#26435;&#37325;&#21305;&#37197;&#20998;&#26512;&#32447;&#24615;&#27169;&#24335;&#36830;&#25509;&#24615;&#65292;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#20102;&#36890;&#36807;&#26435;&#37325;&#21305;&#37197;&#25214;&#21040;&#30340;&#25490;&#21015;&#21487;&#20197;&#25913;&#21464;&#26435;&#37325;&#30697;&#38453;&#22855;&#24322;&#21521;&#37327;&#30340;&#26041;&#21521;&#65292;&#20294;&#19981;&#33021;&#25913;&#21464;&#22855;&#24322;&#20540;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#29702;&#35299;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#22312;&#27169;&#22411;&#21512;&#24182;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.04051</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25490;&#21015;&#30340;&#26435;&#37325;&#21305;&#37197;&#20998;&#26512;&#32447;&#24615;&#27169;&#24335;&#36830;&#25509;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analysis of Linear Mode Connectivity via Permutation-Based Weight Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04051
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25490;&#21015;&#30340;&#26435;&#37325;&#21305;&#37197;&#20998;&#26512;&#32447;&#24615;&#27169;&#24335;&#36830;&#25509;&#24615;&#65292;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#20102;&#36890;&#36807;&#26435;&#37325;&#21305;&#37197;&#25214;&#21040;&#30340;&#25490;&#21015;&#21487;&#20197;&#25913;&#21464;&#26435;&#37325;&#30697;&#38453;&#22855;&#24322;&#21521;&#37327;&#30340;&#26041;&#21521;&#65292;&#20294;&#19981;&#33021;&#25913;&#21464;&#22855;&#24322;&#20540;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#29702;&#35299;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#22312;&#27169;&#22411;&#21512;&#24182;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Ainsworth&#31561;&#20154;&#23637;&#31034;&#20102;&#20351;&#29992;&#26435;&#37325;&#21305;&#37197;&#65288;WM&#65289;&#26469;&#26368;&#23567;&#21270;&#25490;&#21015;&#25628;&#32034;&#27169;&#22411;&#21442;&#25968;&#20013;&#30340;$L_2$&#36317;&#31163;&#26377;&#25928;&#22320;&#35782;&#21035;&#28385;&#36275;&#32447;&#24615;&#27169;&#24335;&#36830;&#25509;&#24615;&#65288;LMC&#65289;&#30340;&#25490;&#21015;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#65292;&#22312;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#31181;&#23376;&#30340;&#29420;&#31435;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#32447;&#24615;&#36335;&#24452;&#19978;&#30340;&#25439;&#22833;&#20445;&#25345;&#20960;&#20046;&#24658;&#23450;&#12290;&#26412;&#25991;&#36890;&#36807;WM&#25552;&#20379;&#20102;LMC&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#22312;&#27169;&#22411;&#21512;&#24182;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23454;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;WM&#25214;&#21040;&#30340;&#25490;&#21015;&#24182;&#19981;&#26174;&#30528;&#20943;&#23569;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;$L_2$&#36317;&#31163;&#65292;&#32780;LMC&#30340;&#20986;&#29616;&#24182;&#19981;&#20165;&#20165;&#26159;&#30001;&#20110;WM&#26412;&#36523;&#30340;&#36317;&#31163;&#20943;&#23567;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#65292;&#34920;&#26126;&#25490;&#21015;&#21487;&#20197;&#25913;&#21464;&#27599;&#23618;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#21521;&#37327;&#30340;&#26041;&#21521;&#65292;&#20294;&#19981;&#33021;&#25913;&#21464;&#22855;&#24322;&#20540;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;&#65292;WM&#25214;&#21040;&#30340;&#25490;&#21015;&#20027;&#35201;&#25913;&#21464;&#20102;&#26435;&#37325;&#30697;&#38453;&#30340;&#26041;&#21521;&#65292;&#32780;&#19981;&#26159;&#22855;&#24322;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Ainsworth et al. showed that using weight matching (WM) to minimize the $L_2$ distance in a permutation search of model parameters effectively identifies permutations that satisfy linear mode connectivity (LMC), in which the loss along a linear path between two independently trained models with different seeds remains nearly constant. This paper provides a theoretical analysis of LMC using WM, which is crucial for understanding stochastic gradient descent's effectiveness and its application in areas like model merging. We first experimentally and theoretically show that permutations found by WM do not significantly reduce the $L_2$ distance between two models and the occurrence of LMC is not merely due to distance reduction by WM in itself. We then provide theoretical insights showing that permutations can change the directions of the singular vectors, but not the singular values, of the weight matrices in each layer. This finding shows that permutations found by WM mainly al
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#24494;&#35843;&#40657;&#30418;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#26377;&#36755;&#20837;&#25552;&#31034;&#21644;&#36755;&#20986;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#25552;&#31034;&#29983;&#25104;&#27169;&#22359;&#21644;&#19968;&#20010;&#39044;&#27979;&#20248;&#21270;&#27169;&#22359;&#65292;&#24182;&#24341;&#20837;&#20102;&#36741;&#21161;&#30340;&#39044;&#27979;&#19968;&#33268;&#24615;&#25439;&#22833;&#36827;&#34892;&#19968;&#33268;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.04050</link><description>&lt;p&gt;
&#36830;&#25509;&#28857;&#65306;&#21327;&#20316;&#24494;&#35843;&#40657;&#30418;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#24494;&#35843;&#40657;&#30418;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21482;&#26377;&#36755;&#20837;&#25552;&#31034;&#21644;&#36755;&#20986;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#25552;&#31034;&#29983;&#25104;&#27169;&#22359;&#21644;&#19968;&#20010;&#39044;&#27979;&#20248;&#21270;&#27169;&#22359;&#65292;&#24182;&#24341;&#20837;&#20102;&#36741;&#21161;&#30340;&#39044;&#27979;&#19968;&#33268;&#24615;&#25439;&#22833;&#36827;&#34892;&#19968;&#33268;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20154;&#20204;&#22312;&#23558;&#20854;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#26102;&#25237;&#20837;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#12290;&#23613;&#31649;&#22312;&#35774;&#35745;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#32780;&#23545;&#20110;&#20445;&#25252;&#27169;&#22411;&#25152;&#26377;&#26435;&#65292;&#27169;&#22411;&#25152;&#26377;&#32773;&#36890;&#24120;&#36873;&#25321;&#23558;&#20854;&#20316;&#20026;&#40657;&#30418;&#25552;&#20379;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#24494;&#35843;&#65288;CraFT&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#40657;&#30418;VLMs fine-tuning&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#20854;&#20013;&#21482;&#33021;&#35775;&#38382;&#27169;&#22411;&#30340;&#36755;&#20837;&#25552;&#31034;&#21644;&#36755;&#20986;&#39044;&#27979;&#12290;CraFT&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65292;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#25991;&#26412;&#25552;&#31034;&#30340;&#25552;&#31034;&#29983;&#25104;&#27169;&#22359;&#65292;&#19968;&#20010;&#29992;&#20110;&#20197;&#27531;&#24046;&#26041;&#24335;&#22686;&#24378;&#36755;&#20986;&#39044;&#27979;&#30340;&#39044;&#27979;&#20248;&#21270;&#27169;&#22359;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36741;&#21161;&#30340;&#39044;&#27979;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#20419;&#36827;&#36825;&#20123;&#27169;&#22359;&#20043;&#38388;&#30340;&#19968;&#33268;&#20248;&#21270;&#12290;&#36825;&#20123;&#27169;&#22359;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#21327;&#20316;&#35757;&#32451;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of pretrained vision-language models (VLMs), considerable efforts have been devoted to fine-tuning them for downstream tasks. Despite the progress made in designing efficient fine-tuning methods, such methods require access to the model's parameters, which can be challenging as model owners often opt to provide their models as a black box to safeguard model ownership. This paper proposes a \textbf{C}ollabo\textbf{ra}tive \textbf{F}ine-\textbf{T}uning (\textbf{CraFT}) approach for fine-tuning black-box VLMs to downstream tasks, where one only has access to the input prompts and the output predictions of the model. CraFT comprises two modules, a prompt generation module for learning text prompts and a prediction refinement module for enhancing output predictions in residual style. Additionally, we introduce an auxiliary prediction-consistent loss to promote consistent optimization across these modules. These modules are optimized by a novel collaborative training algor
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32852;&#21512;&#25193;&#25955;&#33410;&#28857;&#21644;&#36793;&#23646;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22270;&#24418;&#29983;&#25104;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#25152;&#26377;&#22270;&#32452;&#20214;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#27169;&#22359;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#33410;&#28857;&#12289;&#36793;&#21644;&#37051;&#25509;&#20449;&#24687;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04046</link><description>&lt;p&gt;
&#36890;&#36807;&#33410;&#28857;&#21644;&#36793;&#23646;&#24615;&#30340;&#32852;&#21512;&#25193;&#25955;&#65292;&#23454;&#29616;&#22270;&#24418;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling of Graphs via Joint Diffusion of Node and Edge Attributes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04046
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#25193;&#25955;&#33410;&#28857;&#21644;&#36793;&#23646;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22270;&#24418;&#29983;&#25104;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#25152;&#26377;&#22270;&#32452;&#20214;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#27169;&#22359;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#33410;&#28857;&#12289;&#36793;&#21644;&#37051;&#25509;&#20449;&#24687;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#29983;&#25104;&#26159;&#21508;&#31181;&#24037;&#31243;&#21644;&#31185;&#23398;&#23398;&#31185;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#36793;&#23646;&#24615;&#30340;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20123;&#20851;&#38190;&#24212;&#29992;&#20013;&#36793;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#20351;&#24471;&#20808;&#21069;&#30340;&#26041;&#27861;&#22312;&#36825;&#20123;&#24773;&#22659;&#20013;&#21487;&#33021;&#19981;&#36866;&#29992;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#23384;&#22312;&#19968;&#20123;&#31616;&#21333;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#20294;&#32463;&#39564;&#35843;&#26597;&#26174;&#31034;&#23427;&#20204;&#30340;&#25928;&#26524;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#24456;&#22909;&#22320;&#27169;&#25311;&#22270;&#32452;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33410;&#28857;&#21644;&#36793;&#30340;&#32852;&#21512;&#35780;&#20998;&#27169;&#22411;&#65292;&#29992;&#20110;&#22270;&#24418;&#29983;&#25104;&#65292;&#32771;&#34385;&#20102;&#25152;&#26377;&#22270;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#21019;&#26032;&#28857;&#65306;(i) &#23558;&#33410;&#28857;&#21644;&#36793;&#23646;&#24615;&#32467;&#21512;&#22312;&#19968;&#20010;&#27880;&#24847;&#27169;&#22359;&#20013;&#65292;&#22522;&#20110;&#36825;&#20004;&#20010;&#22240;&#32032;&#29983;&#25104;&#26679;&#26412;&#65307;(ii) &#22312;&#22270;&#24418;&#25193;&#25955;&#36807;&#31243;&#20013;&#65292;&#33410;&#28857;&#12289;&#36793;&#21644;&#37051;&#25509;&#20449;&#24687;&#30456;&#20114;&#20381;&#36182;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#23454;&#38469;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#36793;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph generation is integral to various engineering and scientific disciplines. Nevertheless, existing methodologies tend to overlook the generation of edge attributes. However, we identify critical applications where edge attributes are essential, making prior methods potentially unsuitable in such contexts. Moreover, while trivial adaptations are available, empirical investigations reveal their limited efficacy as they do not properly model the interplay among graph components. To address this, we propose a joint score-based model of nodes and edges for graph generation that considers all graph components. Our approach offers two key novelties: (i) node and edge attributes are combined in an attention module that generates samples based on the two ingredients; and (ii) node, edge and adjacency information are mutually dependent during the graph diffusion process. We evaluate our method on challenging benchmarks involving real-world and synthetic datasets in which edge features are cr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PAC-Bayesian&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#26469;&#30740;&#31350;&#23545;&#25239;&#40065;&#26834;&#24615;&#27867;&#21270;&#30028;&#38480;&#38382;&#39064;&#65292;&#38024;&#23545;&#20004;&#31181;&#27969;&#34892;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#22270;&#19978;&#25193;&#25955;&#30697;&#38453;&#30340;&#35889;&#33539;&#25968;&#12289;&#26435;&#37325;&#30340;&#35889;&#33539;&#25968;&#21644;&#25200;&#21160;&#22240;&#23376;&#23545;&#27169;&#22411;&#30340;&#40065;&#26834;&#27867;&#21270;&#30028;&#38480;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.04038</link><description>&lt;p&gt;
PAC-Bayesian Adversarially Robust Generalization Bounds for Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
PAC-Bayesian Adversarially Robust Generalization Bounds for Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PAC-Bayesian&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#26469;&#30740;&#31350;&#23545;&#25239;&#40065;&#26834;&#24615;&#27867;&#21270;&#30028;&#38480;&#38382;&#39064;&#65292;&#38024;&#23545;&#20004;&#31181;&#27969;&#34892;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#22270;&#19978;&#25193;&#25955;&#30697;&#38453;&#30340;&#35889;&#33539;&#25968;&#12289;&#26435;&#37325;&#30340;&#35889;&#33539;&#25968;&#21644;&#25200;&#21160;&#22240;&#23376;&#23545;&#27169;&#22411;&#30340;&#40065;&#26834;&#27867;&#21270;&#30028;&#38480;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#19982;&#22270;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#24191;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;GNNs&#20063;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#25239;&#40065;&#26834;&#24615;&#27867;&#21270;&#22312;&#24314;&#31435;&#26377;&#25928;&#30340;&#25269;&#24481;&#23545;&#25239;&#25915;&#20987;&#30340;&#38450;&#24481;&#31639;&#27861;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;PAC-Bayesian&#26694;&#26550;&#65292;&#20026;&#20004;&#31181;&#27969;&#34892;&#30340;GNNs&#65292;&#21363;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#21644;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20379;&#20102;&#23545;&#25239;&#40065;&#26834;&#27867;&#21270;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#22270;&#19978;&#25193;&#25955;&#30697;&#38453;&#30340;&#35889;&#33539;&#25968;&#12289;&#26435;&#37325;&#30340;&#35889;&#33539;&#25968;&#20197;&#21450;&#25200;&#21160;&#22240;&#23376;&#23545;&#20004;&#20010;&#27169;&#22411;&#30340;&#40065;&#26834;&#27867;&#21270;&#30028;&#38480;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#26159;&#65288;Liao&#31561;&#20154;&#65292;2020&#65289;&#20013;&#32467;&#26524;&#30340;&#38750;&#24179;&#20961;&#25512;&#24191;&#65292;&#20174;&#26631;&#20934;&#35774;&#32622;&#25193;&#23637;&#21040;&#23545;&#25239;&#35774;&#32622;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#26368;&#22823;&#33410;&#28857;&#24230;&#30340;&#25351;&#25968;&#20381;&#36182;&#12290;&#20316;&#20026;&#25512;&#35770;&#65292;&#25105;&#20204;&#24471;&#20986;&#26356;&#22909;&#30340;&#30028;&#38480;...
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have gained popularity for various graph-related tasks. However, similar to deep neural networks, GNNs are also vulnerable to adversarial attacks. Empirical studies have shown that adversarially robust generalization has a pivotal role in establishing effective defense algorithms against adversarial attacks. In this paper, we contribute by providing adversarially robust generalization bounds for two kinds of popular GNNs, graph convolutional network (GCN) and message passing graph neural network, using the PAC-Bayesian framework. Our result reveals that spectral norm of the diffusion matrix on the graph and spectral norm of the weights as well as the perturbation factor govern the robust generalization bounds of both models. Our bounds are nontrivial generalizations of the results developed in (Liao et al., 2020) from the standard setting to adversarial setting while avoiding exponential dependence of the maximum node degree. As corollaries, we derive bette
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;&#22270;&#31070;&#32463;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#24615;&#28431;&#27934;&#65292;&#36890;&#36807;&#36793;&#37325;&#26500;&#25915;&#20987;&#21487;&#20197;&#25512;&#26029;&#20986;&#25935;&#24863;&#30340;&#25299;&#25169;&#20449;&#24687;&#65292;&#24182;&#25506;&#35752;&#20102;&#22122;&#22768;&#32858;&#21512;&#26426;&#21046;&#20135;&#29983;&#30340;&#38544;&#31169;&#22270;&#34920;&#31034;&#23545;&#35813;&#25915;&#20987;&#30340;&#38887;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04033</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#24418;&#34920;&#31034;&#21487;&#35777;&#26126;&#30340;&#38544;&#31169;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
On provable privacy vulnerabilities of graph representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04033
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;&#22270;&#31070;&#32463;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#24615;&#28431;&#27934;&#65292;&#36890;&#36807;&#36793;&#37325;&#26500;&#25915;&#20987;&#21487;&#20197;&#25512;&#26029;&#20986;&#25935;&#24863;&#30340;&#25299;&#25169;&#20449;&#24687;&#65292;&#24182;&#25506;&#35752;&#20102;&#22122;&#22768;&#32858;&#21512;&#26426;&#21046;&#20135;&#29983;&#30340;&#38544;&#31169;&#22270;&#34920;&#31034;&#23545;&#35813;&#25915;&#20987;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;(GRL)&#23545;&#20110;&#20174;&#22797;&#26434;&#30340;&#32593;&#32476;&#32467;&#26500;&#20013;&#25552;&#21462;&#27934;&#35265;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#23433;&#20840;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#20123;&#34920;&#31034;&#20013;&#21487;&#33021;&#23384;&#22312;&#38544;&#31169;&#28431;&#27934;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#24615;&#28431;&#27934;&#65292;&#21487;&#20197;&#36890;&#36807;&#36793;&#37325;&#26500;&#25915;&#20987;&#25512;&#26029;&#20986;&#25935;&#24863;&#30340;&#25299;&#25169;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#35299;&#20915;&#20102;&#22522;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#36793;&#37325;&#26500;&#25915;&#20987;(COSERA)&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#65292;&#35777;&#26126;&#38543;&#30528;&#22270;&#30340;&#35268;&#27169;&#22686;&#21152;&#65292;&#36825;&#31181;&#25915;&#20987;&#21487;&#20197;&#23436;&#32654;&#22320;&#37325;&#26500;&#31232;&#30095;&#30340;Erdos Renyi&#22270;&#19982;&#29420;&#31435;&#38543;&#26426;&#29305;&#24449;&#12290;&#21453;&#20043;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31232;&#30095;&#24615;&#23545;COSERA&#30340;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;&#22122;&#22768;&#32858;&#21512;(NAG)&#26426;&#21046;&#20135;&#29983;&#30340;(&#21487;&#35777;&#26126;&#30340;)&#38544;&#31169;&#22270;&#34920;&#31034;&#23545;COSERA&#25915;&#20987;&#30340;&#38887;&#24615;&#12290;&#25105;&#20204;&#23454;&#35777;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Graph representation learning (GRL) is critical for extracting insights from complex network structures, but it also raises security concerns due to potential privacy vulnerabilities in these representations. This paper investigates the structural vulnerabilities in graph neural models where sensitive topological information can be inferred through edge reconstruction attacks. Our research primarily addresses the theoretical underpinnings of cosine-similarity-based edge reconstruction attacks (COSERA), providing theoretical and empirical evidence that such attacks can perfectly reconstruct sparse Erdos Renyi graphs with independent random features as graph size increases. Conversely, we establish that sparsity is a critical factor for COSERA's effectiveness, as demonstrated through analysis and experiments on stochastic block models. Finally, we explore the resilience of (provably) private graph representations produced via noisy aggregation (NAG) mechanism against COSERA. We empirical
&lt;/p&gt;</description></item><item><title>Polyp-DDPM&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#21106;&#25513;&#30721;&#29983;&#25104;&#36924;&#30495;&#30340;&#32963;&#32928;&#36947;&#24687;&#32905;&#22270;&#20687;&#65292;&#25552;&#21319;&#20102;&#20998;&#21106;&#25928;&#26524;&#65292;&#24182;&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#20998;&#21106;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#20026;&#35757;&#32451;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#12289;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20351;&#24471;&#20998;&#21106;&#27169;&#22411;&#36798;&#21040;&#19982;&#30495;&#23454;&#22270;&#20687;&#30456;&#27604;&#21487;&#27604;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04031</link><description>&lt;p&gt;
Polyp-DDPM: &#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#20041;&#24687;&#32905;&#21512;&#25104;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20998;&#21106;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04031
&lt;/p&gt;
&lt;p&gt;
Polyp-DDPM&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#21106;&#25513;&#30721;&#29983;&#25104;&#36924;&#30495;&#30340;&#32963;&#32928;&#36947;&#24687;&#32905;&#22270;&#20687;&#65292;&#25552;&#21319;&#20102;&#20998;&#21106;&#25928;&#26524;&#65292;&#24182;&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#20998;&#21106;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#20026;&#35757;&#32451;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#12289;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20351;&#24471;&#20998;&#21106;&#27169;&#22411;&#36798;&#21040;&#19982;&#30495;&#23454;&#22270;&#20687;&#30456;&#27604;&#21487;&#27604;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Polyp-DDPM&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26465;&#20214;&#25513;&#30721;&#19978;&#29983;&#25104;&#36924;&#30495;&#30340;&#24687;&#32905;&#22270;&#20687;&#65292;&#26088;&#22312;&#22686;&#24378;&#32963;&#32928;&#36947;&#24687;&#32905;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38480;&#21046;&#12289;&#39640;&#26114;&#30340;&#27880;&#37322;&#25104;&#26412;&#21644;&#38544;&#31169;&#38382;&#39064;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#26465;&#20214;&#21270;&#20110;&#20998;&#21106;&#25513;&#30721;&#65288;&#34920;&#31034;&#24322;&#24120;&#21306;&#22495;&#30340;&#20108;&#36827;&#21046;&#25513;&#30721;&#65289;&#65292;Polyp-DDPM&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#20998;&#21106;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65288;Frechet Inception Distance (FID) &#35780;&#20998;&#20026;78.47&#65292;&#32780;&#39640;&#20110;83.79&#30340;&#35780;&#20998;&#65307;Intersection over Union (IoU) &#20026;0.7156&#65292;&#32780;&#22522;&#20934;&#27169;&#22411;&#21512;&#25104;&#22270;&#20687;&#20026;0.6694&#20197;&#19979;&#65292;&#30495;&#23454;&#25968;&#25454;&#20026;0.7067&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#20102;&#39640;&#36136;&#37327;&#12289;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#24687;&#32905;&#20998;&#21106;&#27169;&#22411;&#19982;&#30495;&#23454;&#22270;&#20687;&#30340;&#21487;&#27604;&#24615;&#65292;&#24182;&#25552;&#20379;&#26356;&#22823;&#30340;&#25968;&#25454;&#22686;&#24378;&#33021;&#21147;&#20197;&#25913;&#21892;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces Polyp-DDPM, a diffusion-based method for generating realistic images of polyps conditioned on masks, aimed at enhancing the segmentation of gastrointestinal (GI) tract polyps. Our approach addresses the challenges of data limitations, high annotation costs, and privacy concerns associated with medical images. By conditioning the diffusion model on segmentation masks-binary masks that represent abnormal areas-Polyp-DDPM outperforms state-of-the-art methods in terms of image quality (achieving a Frechet Inception Distance (FID) score of 78.47, compared to scores above 83.79) and segmentation performance (achieving an Intersection over Union (IoU) of 0.7156, versus less than 0.6694 for synthetic images from baseline models and 0.7067 for real data). Our method generates a high-quality, diverse synthetic dataset for training, thereby enhancing polyp segmentation models to be comparable with real images and offering greater data augmentation capabilities to improve seg
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20197;&#26356;&#24555;&#30340;&#36895;&#24230;&#36817;&#20284;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#65292;&#36890;&#36807;&#30452;&#25509;&#35757;&#32451;NN&#26469;&#32469;&#36807;&#21019;&#24314;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#37327;&#23376;&#21270;&#23398;&#25968;&#25454;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.04030</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#36890;&#36807;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#20943;&#23569;&#37327;&#23376;&#21270;&#23398;&#25968;&#25454;&#30340;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Reducing the Cost of Quantum Chemical Data By Backpropagating Through Density Functional Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04030
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20197;&#26356;&#24555;&#30340;&#36895;&#24230;&#36817;&#20284;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#65292;&#36890;&#36807;&#30452;&#25509;&#35757;&#32451;NN&#26469;&#32469;&#36807;&#21019;&#24314;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#37327;&#23376;&#21270;&#23398;&#25968;&#25454;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#20934;&#30830;&#39044;&#27979;&#20998;&#23376;&#30340;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;&#65292;&#20294;&#22797;&#26434;&#24230;&#20026; $O(N_{\text{electrons}}^3)$&#12290;Sch\"utt&#31561;&#20154;&#65288;2019&#24180;&#65289;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#25104;&#21151;&#22320;&#20197; 1000 &#20493;&#30340;&#36895;&#24230;&#36817;&#20284;DFT&#12290;&#22312;&#25193;&#23637;&#21040;&#36739;&#22823;&#20998;&#23376;&#26102;&#38754;&#20020;&#30340;&#26368;&#22823;&#38382;&#39064;&#21487;&#33021;&#26159;DFT&#26631;&#31614;&#30340;&#25104;&#26412;&#12290;&#20363;&#22914;&#65292;&#21019;&#24314;PCQ&#25968;&#25454;&#38598;&#65288;Nakata&#65286;Shimazaki&#65292;2017&#24180;&#65289;&#32791;&#36153;&#20102;&#25968;&#24180;&#26102;&#38388;&#65292;&#32780;&#22312;&#19968;&#20010;&#26143;&#26399;&#20869;&#29992;&#20110;&#35757;&#32451;&#21518;&#32493;&#30340;NN&#12290;DFT&#36890;&#36807;&#26368;&#23567;&#21270;&#33021;&#37327; $E(\cdot )$ &#20316;&#20026;&#8220;&#25439;&#22833;&#20989;&#25968;&#8221;&#26469;&#26631;&#35760;&#20998;&#23376;&#12290;&#25105;&#20204;&#36890;&#36807;&#30452;&#25509;&#23558; $E(\cdot )$ &#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#26469;&#30452;&#25509;&#35757;&#32451;NN&#65292;&#36991;&#20813;&#20102;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;Sch\"utt&#31561;&#20154;&#65288;2019&#24180;&#65289;&#33457;&#36153;&#20102;626&#23567;&#26102;&#21019;&#24314;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#29992;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#20182;&#20204;&#30340;NN&#38656;&#35201;160&#23567;&#26102;&#65292;&#24635;&#20849;786&#23567;&#26102;&#65307;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;31&#23567;&#26102;&#20869;&#23454;&#29616;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Density Functional Theory (DFT) accurately predicts the quantum chemical properties of molecules, but scales as $O(N_{\text{electrons}}^3)$. Sch\"utt et al. (2019) successfully approximate DFT 1000x faster with Neural Networks (NN). Arguably, the biggest problem one faces when scaling to larger molecules is the cost of DFT labels. For example, it took years to create the PCQ dataset (Nakata &amp; Shimazaki, 2017) on which subsequent NNs are trained within a week. DFT labels molecules by minimizing energy $E(\cdot )$ as a "loss function." We bypass dataset creation by directly training NNs with $E(\cdot )$ as a loss function. For comparison, Sch\"utt et al. (2019) spent 626 hours creating a dataset on which they trained their NN for 160h, for a total of 786h; our method achieves comparable performance within 31h.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#20985;&#28145;&#24230;&#22343;&#34913;&#65288;pcDEQ&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#38750;&#36127;&#26435;&#37325;&#21644;&#20985;&#20989;&#25968;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#30830;&#20445;&#20102;&#27169;&#22411;&#30340;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04029</link><description>&lt;p&gt;
&#27491;&#20985;&#28145;&#24230;&#22343;&#34913;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Positive concave deep equilibrium models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04029
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#20985;&#28145;&#24230;&#22343;&#34913;&#65288;pcDEQ&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#38750;&#36127;&#26435;&#37325;&#21644;&#20985;&#20989;&#25968;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#30830;&#20445;&#20102;&#27169;&#22411;&#30340;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22343;&#34913;&#65288;DEQ&#65289;&#27169;&#22411;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31181;&#20869;&#23384;&#25928;&#29575;&#39640;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#27714;&#35299;&#19968;&#20010;&#22266;&#23450;&#28857;&#26041;&#31243;&#32780;&#19981;&#26159;&#26174;&#24335;&#22320;&#35745;&#31639;&#36755;&#20986;&#65292;&#20351;&#23427;&#20204;&#19982;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#26377;&#25152;&#19981;&#21516;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DEQ&#27169;&#22411;&#24448;&#24448;&#32570;&#20047;&#23545;&#22266;&#23450;&#28857;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#30340;&#24418;&#24335;&#20445;&#35777;&#65292;&#24182;&#19988;&#35745;&#31639;&#22266;&#23450;&#28857;&#30340;&#25968;&#20540;&#26041;&#26696;&#30340;&#25910;&#25947;&#24615;&#20063;&#27809;&#26377;&#24471;&#21040;&#24418;&#24335;&#19978;&#30340;&#35777;&#26126;&#12290;&#22240;&#27492;&#65292;&#22312;&#23454;&#36341;&#20013;DEQ&#27169;&#22411;&#21487;&#33021;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DEQ&#27169;&#22411;&#31867;&#65292;&#31216;&#20026;&#27491;&#20985;&#28145;&#24230;&#22343;&#34913;&#65288;pcDEQ&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#38750;&#32447;&#24615;Perron-Frobenius&#29702;&#35770;&#65292;&#24378;&#21046;&#26045;&#21152;&#38750;&#36127;&#26435;&#37325;&#21644;&#22312;&#27491;&#21322;&#36724;&#19978;&#26159;&#20985;&#20989;&#25968;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;&#36890;&#36807;&#24341;&#20837;&#36825;&#20123;&#32422;&#26463;&#65292;&#25105;&#20204;&#21487;&#20197;&#36731;&#26494;&#22320;&#30830;&#20445;&#22266;&#23450;&#28857;&#30340;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep equilibrium (DEQ) models are widely recognized as a memory efficient alternative to standard neural networks, achieving state-of-the-art performance in language modeling and computer vision tasks. These models solve a fixed point equation instead of explicitly computing the output, which sets them apart from standard neural networks. However, existing DEQ models often lack formal guarantees of the existence and uniqueness of the fixed point, and the convergence of the numerical scheme used for computing the fixed point is not formally established. As a result, DEQ models are potentially unstable in practice. To address these drawbacks, we introduce a novel class of DEQ models called positive concave deep equilibrium (pcDEQ) models. Our approach, which is based on nonlinear Perron-Frobenius theory, enforces nonnegative weights and activation functions that are concave on the positive orthant. By imposing these constraints, we can easily ensure the existence and uniqueness of the fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;600&#20010;&#20027;&#39064;&#26631;&#31614;&#30340;&#26032;&#38395;&#26631;&#39064;&#21644;2600&#20010;&#26410;&#26631;&#31614;&#30340;&#38463;&#23572;&#24052;&#23612;&#20122;&#35821;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#21516;&#26102;&#20063;&#35777;&#23454;&#20102;&#22522;&#26412;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04028</link><description>&lt;p&gt;
AlbNews&#65306;&#38463;&#23572;&#24052;&#23612;&#20122;&#35821;&#20027;&#39064;&#24314;&#27169;&#26631;&#39064;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
AlbNews: A Corpus of Headlines for Topic Modeling in Albanian
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;600&#20010;&#20027;&#39064;&#26631;&#31614;&#30340;&#26032;&#38395;&#26631;&#39064;&#21644;2600&#20010;&#26410;&#26631;&#31614;&#30340;&#38463;&#23572;&#24052;&#23612;&#20122;&#35821;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#21516;&#26102;&#20063;&#35777;&#23454;&#20102;&#22522;&#26412;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#24052;&#23612;&#20122;&#35821;&#31561;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#31232;&#32570;&#65292;&#36825;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#30740;&#31350;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#38556;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AlbNews&#65292;&#19968;&#20010;&#21253;&#21547;600&#20010;&#20027;&#39064;&#26631;&#31614;&#30340;&#26032;&#38395;&#26631;&#39064;&#21644;2600&#20010;&#26410;&#26631;&#31614;&#30340;&#38463;&#23572;&#24052;&#23612;&#20122;&#35821;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#25968;&#25454;&#21487;&#29992;&#20110;&#36827;&#34892;&#20027;&#39064;&#24314;&#27169;&#30740;&#31350;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#20351;&#29992;AlbNews&#26679;&#26412;&#35757;&#32451;&#30340;&#19968;&#20123;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#21021;&#22987;&#20998;&#31867;&#24471;&#20998;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#22522;&#26412;&#27169;&#22411;&#32988;&#36807;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#21487;&#20316;&#20026;&#26410;&#26469;&#23454;&#39564;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The scarcity of available text corpora for low-resource languages like Albanian is a serious hurdle for research in natural language processing tasks. This paper introduces AlbNews, a collection of 600 topically labeled news headlines and 2600 unlabeled ones in Albanian. The data can be freely used for conducting topic modeling research. We report the initial classification scores of some traditional machine learning classifiers trained with the AlbNews samples. These results show that basic models outrun the ensemble learning ones and can serve as a baseline for future experiments.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21040;&#32039;&#25903;&#25345;&#22522;&#30340;&#26680;&#20998;&#32452;&#30340;&#36890;&#29992;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#21487;&#20197;&#29992;&#20110;&#38477;&#20302;&#39640;&#26031;&#36807;&#31243;&#30340;&#35757;&#32451;&#21644;&#39044;&#27979;&#26102;&#38388;&#65292;&#24182;&#19988;&#36890;&#36807;&#36866;&#24403;&#30340;&#32447;&#24615;&#32452;&#21512;&#20135;&#29983;&#20102;$m$&#20010;&#32039;&#25903;&#25345;&#30340;&#26680;&#20998;&#32452;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.04022</link><description>&lt;p&gt;
&#19968;&#31181;&#20174;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21040;&#32039;&#25903;&#25345;&#22522;&#30340;&#26680;&#20998;&#32452;&#30340;&#36890;&#29992;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A General Theory for Kernel Packets: from state space model to compactly supported basis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04022
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21040;&#32039;&#25903;&#25345;&#22522;&#30340;&#26680;&#20998;&#32452;&#30340;&#36890;&#29992;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#21487;&#20197;&#29992;&#20110;&#38477;&#20302;&#39640;&#26031;&#36807;&#31243;&#30340;&#35757;&#32451;&#21644;&#39044;&#27979;&#26102;&#38388;&#65292;&#24182;&#19988;&#36890;&#36807;&#36866;&#24403;&#30340;&#32447;&#24615;&#32452;&#21512;&#20135;&#29983;&#20102;$m$&#20010;&#32039;&#25903;&#25345;&#30340;&#26680;&#20998;&#32452;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#30340;&#29366;&#24577;&#31354;&#38388;&#65288;SS&#65289;&#27169;&#22411;&#20844;&#24335;&#21487;&#20197;&#23558;&#20854;&#35757;&#32451;&#21644;&#39044;&#27979;&#26102;&#38388;&#38477;&#20302;&#21040;O&#65288;n&#65289;&#65288;n&#20026;&#25968;&#25454;&#28857;&#20010;&#25968;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;m&#32500;&#30340;GP&#30340;SS&#27169;&#22411;&#20844;&#24335;&#31561;&#20215;&#20110;&#25105;&#20204;&#24341;&#20837;&#30340;&#19968;&#20010;&#27010;&#24565;&#65292;&#31216;&#20026;&#36890;&#29992;&#21491;&#26680;&#20998;&#32452;&#65288;KP&#65289;&#65306;&#19968;&#31181;&#29992;&#20110;GP&#21327;&#26041;&#24046;&#20989;&#25968;K&#30340;&#21464;&#25442;&#65292;&#20351;&#24471;&#23545;&#20110;&#20219;&#24847;$t \leq t_1$&#65292;$0 \leq j \leq m-1$&#21644;$m+1$&#20010;&#36830;&#32493;&#28857;$t_i$&#65292;&#37117;&#28385;&#36275;$\sum_{i=0}^{m}a_iD_t^{(j)}K(t,t_i)=0$&#65292;&#20854;&#20013;${D}_t^{(j)}f(t)$&#34920;&#31034;&#22312;$t$&#19978;&#20316;&#29992;&#30340;&#31532;j&#38454;&#23548;&#25968;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#24605;&#24819;&#25193;&#23637;&#21040;&#20102;GP&#30340;&#21521;&#21518;SS&#27169;&#22411;&#20844;&#24335;&#65292;&#24471;&#21040;&#20102;&#19979;&#19968;&#20010;$m$&#20010;&#36830;&#32493;&#28857;&#30340;&#24038;&#26680;&#20998;&#32452;&#30340;&#27010;&#24565;&#65306;$\sum_{i=0}^{m}b_i{D}_t^{(j)}K(t,t_{m+i})=0$&#65292;&#23545;&#20110;&#20219;&#24847;$t\geq t_{2m}$&#12290;&#36890;&#36807;&#32467;&#21512;&#24038;&#21491;&#26680;&#20998;&#32452;&#65292;&#21487;&#20197;&#35777;&#26126;&#36825;&#20123;&#21327;&#26041;&#24046;&#20989;&#25968;&#30340;&#36866;&#24403;&#32447;&#24615;&#32452;&#21512;&#20135;&#29983;&#20102;$m$&#20010;&#32039;&#25903;&#25345;&#30340;&#26680;&#20998;&#32452;&#20989;&#25968;&#65306;&#23545;&#20110;&#20219;&#24847;$t\not\in(t_0,t_{2m})$&#21644;$j=0,\cdots,m-1$&#65292;$\phi^{(j)}(t)=0$&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well known that the state space (SS) model formulation of a Gaussian process (GP) can lower its training and prediction time both to O(n) for n data points. We prove that an $m$-dimensional SS model formulation of GP is equivalent to a concept we introduce as the general right Kernel Packet (KP): a transformation for the GP covariance function $K$ such that $\sum_{i=0}^{m}a_iD_t^{(j)}K(t,t_i)=0$ holds for any $t \leq t_1$, 0 $\leq j \leq m-1$, and $m+1$ consecutive points $t_i$, where ${D}_t^{(j)}f(t) $ denotes $j$-th order derivative acting on $t$. We extend this idea to the backward SS model formulation of the GP, leading to the concept of the left KP for next $m$ consecutive points: $\sum_{i=0}^{m}b_i{D}_t^{(j)}K(t,t_{m+i})=0$ for any $t\geq t_{2m}$. By combining both left and right KPs, we can prove that a suitable linear combination of these covariance functions yields $m$ compactly supported KP functions: $\phi^{(j)}(t)=0$ for any $t\not\in(t_0,t_{2m})$ and $j=0,\cdots,m-1$
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;NextGen NHTS&#20986;&#21457;&#22320;-&#30446;&#30340;&#22320;&#25968;&#25454;&#38598;&#65292;&#25506;&#31350;&#20154;&#21475;&#21644;&#23601;&#19994;&#29305;&#24449;&#23545;&#21345;&#36710;&#27969;&#37327;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#25913;&#21892;&#20132;&#36890;&#35268;&#21010;&#21644;&#25237;&#36164;&#20915;&#31574;&#38750;&#24120;&#20851;&#38190;&#12290;</title><link>https://arxiv.org/abs/2402.04019</link><description>&lt;p&gt;
&#25506;&#31350;&#20154;&#21475;&#21644;&#23601;&#19994;&#29305;&#24449;&#23545;&#21345;&#36710;&#27969;&#37327;&#30340;&#24433;&#21709;&#65306;&#23545;NextGen NHTS&#20986;&#21457;&#22320;-&#30446;&#30340;&#22320;&#25968;&#25454;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exploring the Effects of Population and Employment Characteristics on Truck Flows: An Analysis of NextGen NHTS Origin-Destination Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;NextGen NHTS&#20986;&#21457;&#22320;-&#30446;&#30340;&#22320;&#25968;&#25454;&#38598;&#65292;&#25506;&#31350;&#20154;&#21475;&#21644;&#23601;&#19994;&#29305;&#24449;&#23545;&#21345;&#36710;&#27969;&#37327;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#25913;&#21892;&#20132;&#36890;&#35268;&#21010;&#21644;&#25237;&#36164;&#20915;&#31574;&#38750;&#24120;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21345;&#36710;&#36816;&#36755;&#22240;&#20854;&#28789;&#27963;&#24615;&#65288;&#33021;&#22815;&#26041;&#20415;&#22320;&#21040;&#36798;&#25552;&#21462;&#21644;&#21368;&#36135;&#28857;&#65289;&#21644;&#26356;&#24555;&#30340;&#20132;&#36135;&#36895;&#24230;&#31561;&#20248;&#21183;&#65292;&#20173;&#28982;&#26159;&#32654;&#22269;&#36135;&#29289;&#36816;&#36755;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#30001;&#20110;&#21345;&#36710;&#36816;&#36755;&#25215;&#36733;&#20102;&#22823;&#37327;&#36135;&#29289;&#65292;&#20102;&#35299;&#20154;&#21475;&#21644;&#23601;&#19994;&#29305;&#24449;&#23545;&#21345;&#36710;&#27969;&#37327;&#30340;&#24433;&#21709;&#23545;&#20110;&#26356;&#22909;&#30340;&#20132;&#36890;&#35268;&#21010;&#21644;&#25237;&#36164;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#32654;&#22269;&#32852;&#37030;&#20844;&#36335;&#31649;&#29702;&#23616;&#20316;&#20026;Next Generation National Household Travel Survey&#39033;&#30446;&#30340;&#19968;&#37096;&#20998;&#65292;&#20844;&#24067;&#20102;&#19968;&#32452;&#21345;&#36710;&#20986;&#34892;&#36215;-&#27490;&#28857;&#25968;&#25454;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;2020&#24180;&#22312;&#27599;&#20010;&#24030;&#21644;&#21326;&#30427;&#39039;&#29305;&#21306;&#20869;583&#20010;&#39044;&#23450;&#20041;&#21306;&#22495;&#20043;&#38388;&#30340;&#21345;&#36710;&#20986;&#34892;&#24635;&#25968;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#21306;&#22495;&#23618;&#38754;&#30340;&#20154;&#21475;&#21644;&#23601;&#19994;&#29305;&#24449;&#25968;&#25454;&#20174;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#21644;&#21439;&#32423;&#21830;&#19994;&#27169;&#24335;&#25968;&#25454;&#21152;&#20837;&#21040;&#36215;-&#27490;&#28857;&#32423;&#21035;&#30340;&#21345;&#36710;&#20986;&#34892;&#27969;&#37327;&#25968;&#25454;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Truck transportation remains the dominant mode of US freight transportation because of its advantages, such as the flexibility of accessing pickup and drop-off points and faster delivery. Because of the massive freight volume transported by trucks, understanding the effects of population and employment characteristics on truck flows is critical for better transportation planning and investment decisions. The US Federal Highway Administration published a truck travel origin-destination data set as part of the Next Generation National Household Travel Survey program. This data set contains the total number of truck trips in 2020 within and between 583 predefined zones encompassing metropolitan and nonmetropolitan statistical areas within each state and Washington, DC. In this study, origin-destination-level truck trip flow data was augmented to include zone-level population and employment characteristics from the US Census Bureau. Census population and County Business Patterns data were 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#37327;&#21270;&#36817;&#20284;&#27491;&#20132;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;QORNNs&#65289;&#26469;&#35299;&#20915;&#27491;&#20132;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;ORNNs&#65289;&#20013;&#21442;&#25968;&#36807;&#22810;&#30340;&#38382;&#39064;&#65292;&#37319;&#29992;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#31574;&#30053;&#21644;&#19977;&#31181;&#34701;&#20837;&#27491;&#20132;&#32422;&#26463;&#21644;&#37327;&#21270;&#26435;&#37325;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#21462;&#24471;&#20102;&#19982;s&#30456;&#20284;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04012</link><description>&lt;p&gt;
&#37327;&#21270;&#36817;&#20284;&#27491;&#20132;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Quantized Approximately Orthogonal Recurrent Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#37327;&#21270;&#36817;&#20284;&#27491;&#20132;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;QORNNs&#65289;&#26469;&#35299;&#20915;&#27491;&#20132;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;ORNNs&#65289;&#20013;&#21442;&#25968;&#36807;&#22810;&#30340;&#38382;&#39064;&#65292;&#37319;&#29992;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#31574;&#30053;&#21644;&#19977;&#31181;&#34701;&#20837;&#27491;&#20132;&#32422;&#26463;&#21644;&#37327;&#21270;&#26435;&#37325;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#21462;&#24471;&#20102;&#19982;s&#30456;&#20284;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#20132;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;ORNN&#65289;&#26159;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#36873;&#25321;&#65292;&#29992;&#20110;&#23398;&#20064;&#28041;&#21450;&#20855;&#26377;&#38271;&#26399;&#20381;&#36182;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#21644;&#35745;&#31639;&#31283;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32593;&#32476;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#21442;&#25968;&#25165;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#36825;&#22312;&#21151;&#29575;&#21463;&#38480;&#30340;&#29615;&#22659;&#65288;&#22914;&#32039;&#20945;&#35774;&#22791;&#65289;&#20013;&#21487;&#33021;&#26159;&#38459;&#30861;&#22240;&#32032;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#12290;&#26500;&#24314;&#36825;&#26679;&#30340;&#32593;&#32476;&#20173;&#28982;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#20854;&#22266;&#26377;&#30340;&#19981;&#31283;&#23450;&#24615;&#26159;&#34987;&#35748;&#21487;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;ORNN&#20013;&#30340;&#24490;&#29615;&#21644;&#36755;&#20837;&#26435;&#37325;&#30697;&#38453;&#30340;&#37327;&#21270;&#65292;&#23548;&#33268;&#20102;&#37327;&#21270;&#36817;&#20284;&#27491;&#20132;RNN&#65288;QORNN&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#31574;&#30053;&#21644;&#19977;&#31181;&#34701;&#20837;&#27491;&#20132;&#32422;&#26463;&#21644;&#37327;&#21270;&#26435;&#37325;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#31639;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#20351;&#29992;QAT&#30340;&#20248;&#21183;&#12290;&#26368;&#39640;&#25928;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#19982;s&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Orthogonal recurrent neural networks (ORNNs) are an appealing option for learning tasks involving time series with long-term dependencies, thanks to their simplicity and computational stability. However, these networks often require a substantial number of parameters to perform well, which can be prohibitive in power-constrained environments, such as compact devices. One approach to address this issue is neural network quantization. The construction of such networks remains an open problem, acknowledged for its inherent instability.In this paper, we explore the quantization of the recurrent and input weight matrices in ORNNs, leading to Quantized approximately Orthogonal RNNs (QORNNs). We investigate one post-training quantization (PTQ) strategy and three quantization-aware training (QAT) algorithms that incorporate orthogonal constraints and quantized weights. Empirical results demonstrate the advantages of employing QAT over PTQ. The most efficient model achieves results similar to s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21487;&#29992;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#38024;&#23545;&#30417;&#30563;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#36890;&#36807;&#29983;&#25104;&#38590;&#20197;&#23519;&#35273;&#30340;&#22122;&#38899;&#21644;&#21046;&#36896;&#19981;&#21487;&#23398;&#20064;&#30340;&#31034;&#20363;&#26469;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#20351;&#29992;&#25968;&#25454;&#65292;&#24182;&#23454;&#29616;&#20102;&#30417;&#30563;&#21644;&#23545;&#27604;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#26032;&#26368;&#22351;&#24773;&#20917;&#30340;&#19981;&#21487;&#23398;&#20064;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04010</link><description>&lt;p&gt;
&#38024;&#23545;&#30417;&#30563;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#39640;&#25928;&#21487;&#29992;&#24615;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Efficient Availability Attacks against Supervised and Contrastive Learning Simultaneously
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21487;&#29992;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#38024;&#23545;&#30417;&#30563;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#65292;&#36890;&#36807;&#29983;&#25104;&#38590;&#20197;&#23519;&#35273;&#30340;&#22122;&#38899;&#21644;&#21046;&#36896;&#19981;&#21487;&#23398;&#20064;&#30340;&#31034;&#20363;&#26469;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#20351;&#29992;&#25968;&#25454;&#65292;&#24182;&#23454;&#29616;&#20102;&#30417;&#30563;&#21644;&#23545;&#27604;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#26032;&#26368;&#22351;&#24773;&#20917;&#30340;&#19981;&#21487;&#23398;&#20064;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#29992;&#24615;&#25915;&#20987;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#38590;&#20197;&#23519;&#35273;&#30340;&#22122;&#38899;&#21644;&#21046;&#36896;&#19981;&#21487;&#23398;&#20064;&#30340;&#31034;&#20363;&#26469;&#38450;&#27490;&#31169;&#20154;&#25968;&#25454;&#21644;&#21830;&#19994;&#25968;&#25454;&#38598;&#30340;&#26410;&#32463;&#25480;&#26435;&#20351;&#29992;&#12290;&#24403;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#22833;&#36133;&#26102;&#65292;&#24694;&#24847;&#30340;&#25968;&#25454;&#25910;&#38598;&#32773;&#21487;&#33021;&#20250;&#36716;&#21521;&#23545;&#27604;&#23398;&#20064;&#31639;&#27861;&#20197;&#32469;&#36807;&#20445;&#25252;&#12290;&#36890;&#36807;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#26080;&#27861;&#21516;&#26102;&#23454;&#29616;&#30417;&#30563;&#21644;&#23545;&#27604;&#30340;&#19981;&#21487;&#23398;&#20064;&#24615;&#65292;&#36825;&#32473;&#25968;&#25454;&#20445;&#25252;&#24102;&#26469;&#20102;&#39118;&#38505;&#12290;&#19982;&#22522;&#20110;&#23545;&#27604;&#35823;&#24046;&#26368;&#23567;&#21270;&#30340;&#26368;&#26032;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#30417;&#30563;&#35823;&#24046;&#26368;&#23567;&#21270;&#25110;&#26368;&#22823;&#21270;&#26694;&#26550;&#20013;&#20351;&#29992;&#31867;&#20284;&#23545;&#27604;&#30340;&#25968;&#25454;&#22686;&#24378;&#26469;&#33719;&#24471;&#23545;&#30417;&#30563;&#21644;&#23545;&#27604;&#23398;&#20064;&#22343;&#26377;&#25928;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;AUE&#21644;AAP&#25915;&#20987;&#22312;&#20943;&#23569;&#35745;&#31639;&#28040;&#32791;&#30340;&#21069;&#25552;&#19979;&#65292;&#23454;&#29616;&#20102;&#30417;&#30563;&#21644;&#23545;&#27604;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#26032;&#26368;&#22351;&#24773;&#20917;&#30340;&#19981;&#21487;&#23398;&#20064;&#24615;&#65292;&#23637;&#31034;&#20102;&#26410;&#26469;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Availability attacks can prevent the unauthorized use of private data and commercial datasets by generating imperceptible noise and making unlearnable examples before release. Ideally, the obtained unlearnability prevents algorithms from training usable models. When supervised learning (SL) algorithms have failed, a malicious data collector possibly resorts to contrastive learning (CL) algorithms to bypass the protection. Through evaluation, we have found that most of the existing methods are unable to achieve both supervised and contrastive unlearnability, which poses risks to data protection. Different from recent methods based on contrastive error minimization, we employ contrastive-like data augmentations in supervised error minimization or maximization frameworks to obtain attacks effective for both SL and CL. Our proposed AUE and AAP attacks achieve state-of-the-art worst-case unlearnability across SL and CL algorithms with less computation consumption, showcasing prospects in re
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#27010;&#29575;&#20998;&#24067;&#26469;&#37327;&#21270;&#26799;&#24230;&#32500;&#24230;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#33719;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04005</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#19981;&#30830;&#23450;&#24615;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Bayesian Uncertainty for Gradient Aggregation in Multi-Task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04005
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#27010;&#29575;&#20998;&#24067;&#26469;&#37327;&#21270;&#26799;&#24230;&#32500;&#24230;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#33719;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#26085;&#30410;&#31361;&#20986;&#65292;&#38656;&#35201;&#21516;&#26102;&#25191;&#34892;&#22810;&#20010;&#25512;&#29702;&#20219;&#21153;&#30340;&#38656;&#27714;&#20063;&#22312;&#22686;&#38271;&#12290;&#20026;&#27599;&#20010;&#20219;&#21153;&#36816;&#34892;&#19987;&#29992;&#27169;&#22411;&#22312;&#35745;&#31639;&#19978;&#21313;&#20998;&#26114;&#36149;&#65292;&#22240;&#27492;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#30340;&#20852;&#36259;&#20063;&#36234;&#26469;&#36234;&#22823;&#12290;MTL&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#33021;&#39640;&#25928;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#30340;&#21333;&#20010;&#27169;&#22411;&#12290;&#36890;&#36807;&#35745;&#31639;&#27599;&#20010;&#20219;&#21153;&#30340;&#21333;&#19968;&#26799;&#24230;&#24182;&#23558;&#23427;&#20204;&#32858;&#21512;&#36215;&#26469;&#20197;&#33719;&#24471;&#32467;&#21512;&#30340;&#26356;&#26032;&#26041;&#21521;&#26469;&#20248;&#21270;MTL&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#32771;&#34385;&#21040;&#19968;&#20010;&#37325;&#35201;&#30340;&#26041;&#38754;&#65292;&#21363;&#26799;&#24230;&#32500;&#24230;&#30340;&#25935;&#24863;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21033;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#12290;&#25105;&#20204;&#20026;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#25918;&#32622;&#19968;&#20010;&#27010;&#29575;&#20998;&#24067;&#65292;&#36825;&#21448;&#24341;&#36215;&#20102;&#20219;&#21153;&#26799;&#24230;&#30340;&#20998;&#24067;&#12290;&#36825;&#20123;&#39069;&#22806;&#30340;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#27599;&#20010;&#26799;&#24230;&#32500;&#24230;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#22312;&#32858;&#21512;&#23427;&#20204;&#26102;&#23558;&#20854;&#32435;&#20837;&#32771;&#34385;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#26041;&#27861;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning becomes more prominent there is a growing demand to perform several inference tasks in parallel. Running a dedicated model for each task is computationally expensive and therefore there is a great interest in multi-task learning (MTL). MTL aims at learning a single model that solves several tasks efficiently. Optimizing MTL models is often achieved by computing a single gradient per task and aggregating them for obtaining a combined update direction. However, these approaches do not consider an important aspect, the sensitivity in the gradient dimensions. Here, we introduce a novel gradient aggregation approach using Bayesian inference. We place a probability distribution over the task-specific parameters, which in turn induce a distribution over the gradients of the tasks. This additional valuable information allows us to quantify the uncertainty in each of the gradients dimensions, which can then be factored in when aggregating them. We empirically demonstrate the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38142;&#24335;&#24605;&#32500;&#20013;&#30340;&#22122;&#22768;&#23545;LLM&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#24182;&#24320;&#21457;&#20102;&#36861;&#36394;&#25972;&#25968;&#26694;&#26550;&#26469;&#29983;&#25104;&#21487;&#23450;&#21046;&#30340;&#22122;&#22768;&#25191;&#34892;&#36319;&#36394;&#12290;&#36890;&#36807;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#31639;&#27861;&#21487;&#35299;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#22122;&#22768;&#30340;&#31867;&#22411;&#21644;&#24378;&#24230;&#23545;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.04004</link><description>&lt;p&gt;
&#20102;&#35299;&#31639;&#27861;&#24335;&#24605;&#32500;&#38142;&#20013;&#22122;&#22768;&#23545;LLM&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effect of Noise in LLM Training Data with Algorithmic Chains of Thought
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38142;&#24335;&#24605;&#32500;&#20013;&#30340;&#22122;&#22768;&#23545;LLM&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#24182;&#24320;&#21457;&#20102;&#36861;&#36394;&#25972;&#25968;&#26694;&#26550;&#26469;&#29983;&#25104;&#21487;&#23450;&#21046;&#30340;&#22122;&#22768;&#25191;&#34892;&#36319;&#36394;&#12290;&#36890;&#36807;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#31639;&#27861;&#21487;&#35299;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#22122;&#22768;&#30340;&#31867;&#22411;&#21644;&#24378;&#24230;&#23545;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#20250;&#20351;&#29992;&#25968;&#19975;&#20159;&#20010;&#26631;&#35760;&#30340;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#25991;&#26412;&#36136;&#37327;&#21508;&#24322;&#12290;&#22312;&#35757;&#32451;&#30340;&#20004;&#20010;&#38454;&#27573;&#20013;&#65292;&#36890;&#24120;&#20250;&#26681;&#25454;&#21551;&#21457;&#24335;&#26041;&#27861;&#36807;&#28388;&#25481;&#8220;&#20302;&#36136;&#37327;&#8221;&#25110;&#8220;&#26377;&#22122;&#22768;&#8221;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#28982;&#32780;&#24456;&#23569;&#26377;&#20154;&#37327;&#21270;&#22320;&#20102;&#35299;&#22122;&#22768;&#30340;&#31867;&#22411;&#25110;&#24378;&#24230;&#22914;&#20309;&#24433;&#21709;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#20013;&#30340;&#22122;&#22768;&#22914;&#20309;&#24433;&#21709;&#22312;&#31639;&#27861;&#21487;&#35299;&#20219;&#21153;&#30340;&#39640;&#24230;&#25511;&#21046;&#29615;&#22659;&#19979;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36861;&#36394;&#25972;&#25968;&#65288;TInt&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;&#20219;&#24847;&#25972;&#25968;&#21015;&#34920;&#19978;&#30340;&#31639;&#26415;&#20989;&#25968;&#29983;&#25104;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#22122;&#22768;&#25191;&#34892;&#36319;&#36394;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#22122;&#22768;&#65306;&#23616;&#37096;&#24418;&#24335;&#30340;&#38745;&#24577;&#22122;&#22768;&#65292;&#22312;&#35745;&#31639;CoT&#36319;&#36394;&#21518;&#24212;&#29992;&#65307;&#20197;&#21450;&#20840;&#23616;&#24418;&#24335;&#30340;&#21160;&#24577;&#22122;&#22768;&#65292;&#22312;&#35745;&#31639;&#20013;&#20256;&#25773;&#36319;&#36394;&#20013;&#30340;&#38169;&#35823;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#27979;&#35797;&#24615;&#33021;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
During both pretraining and fine-tuning, Large Language Models (\textbf{LLMs}) are trained on trillions of tokens of text of widely varying quality. Both phases of training typically involve heuristically filtering out ``low-quality'' or \textit{noisy} training samples, yet little is known quantitatively about how the type or intensity of noise affects downstream performance. In this work, we study how noise in chain of thought (\textbf{CoT}) impacts task performance in the highly-controlled setting of algorithmically solvable tasks. First, we develop the Traced Integer (\textbf{TInt}) framework to generate highly customizable noised execution traces for any arithmetic function on lists of integers. We then define two types of noise: \textit{static} noise, a local form of noise which is applied after the CoT trace is computed, and \textit{dynamic} noise, a global form of noise which propagates errors in the trace as it is computed. We then evaluate the test performance of pretrained mo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#28176;&#21464;&#33609;&#22270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#21644;&#25439;&#22833;&#22320;&#35980;&#30740;&#31350;&#12290;&#20316;&#32773;&#22312;&#19977;&#20010;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03994</link><description>&lt;p&gt;
&#20351;&#29992;&#28176;&#21464;&#33609;&#22270;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#21644;&#25439;&#22833;&#22320;&#35980;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Gradient Sketches for Training Data Attribution and Studying the Loss Landscape
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#28176;&#21464;&#33609;&#22270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#21644;&#25439;&#22833;&#22320;&#35980;&#30740;&#31350;&#12290;&#20316;&#32773;&#22312;&#19977;&#20010;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#25237;&#24433;&#25110;&#28176;&#21464;&#21644;Hessian&#21521;&#37327;&#20056;&#31215;&#30340;&#33609;&#22270;&#22312;&#38656;&#35201;&#23384;&#20648;&#35768;&#22810;&#36825;&#20123;&#21521;&#37327;&#24182;&#20445;&#30041;&#20851;&#20110;&#23427;&#20204;&#30340;&#30456;&#23545;&#20960;&#20309;&#20449;&#24687;&#30340;&#24212;&#29992;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20004;&#20010;&#37325;&#35201;&#22330;&#26223;&#26159;&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#65288;&#36319;&#36394;&#27169;&#22411;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#34892;&#20026;&#65289;&#65292;&#20854;&#20013;&#38656;&#35201;&#23384;&#20648;&#27599;&#20010;&#35757;&#32451;&#31034;&#20363;&#30340;&#28176;&#21464;&#65292;&#20197;&#21450;Hessian&#30340;&#39057;&#35889;&#30740;&#31350;&#65288;&#20998;&#26512;&#35757;&#32451;&#21160;&#24577;&#65289;&#65292;&#20854;&#20013;&#38656;&#35201;&#23384;&#20648;&#22810;&#20010;Hessian&#21521;&#37327;&#20056;&#31215;&#12290;&#34429;&#28982;&#20351;&#29992;&#23494;&#38598;&#30697;&#38453;&#30340;&#33609;&#22270;&#26131;&#20110;&#23454;&#29616;&#65292;&#20294;&#23427;&#20204;&#21463;&#23384;&#20648;&#38480;&#21046;&#65292;&#19981;&#33021;&#25193;&#23637;&#21040;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#31070;&#32463;&#32593;&#32476;&#20869;&#22312;&#32500;&#24230;&#30340;&#30740;&#31350;&#24037;&#20316;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#21487;&#20280;&#32553;&#33609;&#22270;&#31639;&#27861;&#30340;&#35774;&#35745;&#31354;&#38388;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#65292;Hessian&#35889;&#20998;&#26512;&#21644;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#26102;&#30340;&#20869;&#22312;&#32500;&#24230;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Random projections or sketches of gradients and Hessian vector products play an essential role in applications where one needs to store many such vectors while retaining accurate information about their relative geometry. Two important scenarios are training data attribution (tracing a model's behavior to the training data), where one needs to store a gradient for each training example, and the study of the spectrum of the Hessian (to analyze the training dynamics), where one needs to store multiple Hessian vector products. While sketches that use dense matrices are easy to implement, they are memory bound and cannot be scaled to modern neural networks. Motivated by work on the intrinsic dimension of neural networks, we propose and study a design space for scalable sketching algorithms. We demonstrate the efficacy of our approach in three applications: training data attribution, the analysis of the Hessian spectrum and the computation of the intrinsic dimension when fine-tuning pre-tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#26230;&#20307;&#29983;&#25104;&#20013;&#32771;&#34385;&#20102;&#31354;&#38388;&#32676;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#23481;&#26131;&#23454;&#29616;&#30340;&#31561;&#25928;&#24418;&#24335;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;DiffCSP++&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03992</link><description>&lt;p&gt;
&#31354;&#38388;&#32676;&#32422;&#26463;&#30340;&#26230;&#20307;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Space Group Constrained Crystal Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#26230;&#20307;&#29983;&#25104;&#20013;&#32771;&#34385;&#20102;&#31354;&#38388;&#32676;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#23481;&#26131;&#23454;&#29616;&#30340;&#31561;&#25928;&#24418;&#24335;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;DiffCSP++&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26230;&#20307;&#26159;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#19994;&#24212;&#29992;&#30340;&#22522;&#30784;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#29992;&#20110;&#29983;&#25104;&#26230;&#20307;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#24456;&#23569;&#32771;&#34385;&#21040;&#31354;&#38388;&#32676;&#32422;&#26463;&#65292;&#32780;&#31354;&#38388;&#32676;&#32422;&#26463;&#23545;&#20110;&#25551;&#36848;&#26230;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#21644;&#35768;&#22810;&#29702;&#24819;&#29305;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#31354;&#38388;&#32676;&#32422;&#26463;&#26159;&#20855;&#26377;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#31354;&#38388;&#32676;&#32422;&#26463;&#31616;&#21270;&#20026;&#26356;&#23481;&#26131;&#25163;&#24037;&#25918;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#31561;&#25928;&#24418;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#31354;&#38388;&#32676;&#32422;&#26463;&#20998;&#20026;&#20004;&#37096;&#20998;&#65306;&#26230;&#26684;&#30697;&#38453;&#19981;&#21464;&#23545;&#25968;&#31354;&#38388;&#30340;&#22522;&#30784;&#32422;&#26463;&#21644;&#20998;&#25968;&#22352;&#26631;&#30340;Wyckoff&#20301;&#32622;&#32422;&#26463;&#12290;&#22522;&#20110;&#36825;&#20123;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiffCSP++&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#23637;&#20102;&#20043;&#21069;&#24037;&#20316;DiffCSP&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#32771;&#34385;&#20102;&#31354;&#38388;&#32676;&#32422;&#26463;&#12290;&#23454;&#39564;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crystals are the foundation of numerous scientific and industrial applications. While various learning-based approaches have been proposed for crystal generation, existing methods seldom consider the space group constraint which is crucial in describing the geometry of crystals and closely relevant to many desirable properties. However, considering space group constraint is challenging owing to its diverse and nontrivial forms. In this paper, we reduce the space group constraint into an equivalent formulation that is more tractable to be handcrafted into the generation process. In particular, we translate the space group constraint into two parts: the basis constraint of the invariant logarithmic space of the lattice matrix and the Wyckoff position constraint of the fractional coordinates. Upon the derived constraints, we then propose DiffCSP++, a novel diffusion model that has enhanced a previous work DiffCSP by further taking space group constraint into account. Experiments on severa
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#34928;&#20943;&#21644;&#23567;&#30340;&#31867;&#20869;&#21464;&#21270;&#19982;&#20302;&#31209;&#20559;&#24046;&#29616;&#35937;&#26377;&#20851;</title><link>https://arxiv.org/abs/2402.03991</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#34928;&#20943;&#21644;&#31867;&#20869;&#21464;&#21270;&#23567;&#20250;&#23548;&#33268;&#20302;&#31209;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Neural Rank Collapse: Weight Decay and Small Within-Class Variability Yield Low-Rank Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03991
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#34928;&#20943;&#21644;&#23567;&#30340;&#31867;&#20869;&#21464;&#21270;&#19982;&#20302;&#31209;&#20559;&#24046;&#29616;&#35937;&#26377;&#20851;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#26174;&#31034;&#20102;&#19968;&#20010;&#38544;&#21547;&#30340;&#20302;&#31209;&#20559;&#24046;&#29616;&#35937;&#65306;&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#30697;&#38453;&#24448;&#24448;&#36817;&#20284;&#20026;&#20302;&#31209;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25110;&#20174;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#20013;&#21435;&#38500;&#30456;&#23545;&#36739;&#23567;&#30340;&#22855;&#24322;&#20540;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#29978;&#33267;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#20302;&#31209;&#20559;&#24046;&#30340;&#29702;&#35770;&#30740;&#31350;&#37117;&#28041;&#21450;&#21040;&#31616;&#21270;&#30340;&#32447;&#24615;&#28145;&#24230;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#24102;&#26377;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#21644;&#26435;&#37325;&#34928;&#20943;&#21442;&#25968;&#30340;&#36890;&#29992;&#32593;&#32476;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#31070;&#32463;&#31209;&#23849;&#28291;&#29616;&#35937;&#65292;&#23427;&#23558;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#30340;&#20302;&#31209;&#20559;&#24046;&#19982;&#32593;&#32476;&#30340;&#31070;&#32463;&#23849;&#28291;&#29305;&#24615;&#32852;&#31995;&#36215;&#26469;&#65306;&#38543;&#30528;&#26435;&#37325;&#34928;&#20943;&#21442;&#25968;&#30340;&#22686;&#21152;&#65292;&#32593;&#32476;&#20013;&#27599;&#19968;&#23618;&#30340;&#31209;&#21576;&#27604;&#20363;&#36882;&#20943;&#65292;&#19982;&#21069;&#38754;&#23618;&#30340;&#38544;&#34255;&#31354;&#38388;&#23884;&#20837;&#30340;&#31867;&#20869;&#21464;&#21270;&#25104;&#21453;&#27604;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#24471;&#21040;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in deep learning has shown strong empirical and theoretical evidence of an implicit low-rank bias: weight matrices in deep networks tend to be approximately low-rank and removing relatively small singular values during training or from available trained models may significantly reduce model size while maintaining or even improving model performance. However, the majority of the theoretical investigations around low-rank bias in neural networks deal with oversimplified deep linear networks. In this work, we consider general networks with nonlinear activations and the weight decay parameter, and we show the presence of an intriguing neural rank collapse phenomenon, connecting the low-rank bias of trained networks with networks' neural collapse properties: as the weight decay parameter grows, the rank of each layer in the network decreases proportionally to the within-class variability of the hidden-space embeddings of the previous layers. Our theoretical findings are supporte
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#20013;&#30340;&#24635;&#26799;&#24230;&#26041;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#25209;&#27425;&#22823;&#23567;&#26377;&#21161;&#20110;&#20943;&#23567;&#21063;&#37319;&#27171;&#24341;&#36215;&#30340;&#26041;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20248;&#21270;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03990</link><description>&lt;p&gt;
&#21063;&#37319;&#27171;&#24182;&#19981;&#26159;&#39764;&#27861;: &#22823;&#25209;&#37327;&#22823;&#23567;&#28858;&#20160;&#40636;&#36969;&#29992;&#26044;&#24046;&#20998;&#38577;&#31169;&#38568;&#27231;&#20778;&#21270;
&lt;/p&gt;
&lt;p&gt;
Subsampling is not Magic: Why Large Batch Sizes Work for Differentially Private Stochastic Optimisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03990
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#20013;&#30340;&#24635;&#26799;&#24230;&#26041;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#25209;&#27425;&#22823;&#23567;&#26377;&#21161;&#20110;&#20943;&#23567;&#21063;&#37319;&#27171;&#24341;&#36215;&#30340;&#26041;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20248;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20497;&#30740;&#31350;&#20102;&#25209;&#27425;&#22823;&#23567;&#23565;&#24046;&#20998;&#38577;&#31169;&#38568;&#27231;&#26799;&#24230;&#19979;&#38477;&#65288;DP-SGD&#65289;&#20013;&#32317;&#26799;&#24230;&#26041;&#24046;&#30340;&#24433;&#38911;&#65292;&#23563;&#27714;&#23565;&#22823;&#25209;&#27425;&#22823;&#23567;&#26377;&#29992;&#24615;&#30340;&#29702;&#35542;&#35299;&#37323;&#12290;&#30001;&#26044;DP-SGD&#26159;&#29694;&#20195;&#24046;&#20998;&#38577;&#31169;&#28145;&#24230;&#23416;&#32722;&#30340;&#22522;&#30990;&#65292;&#20854;&#24615;&#36074;&#24050;&#34987;&#24291;&#27867;&#30740;&#31350;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#22312;&#23526;&#36368;&#20013;&#30332;&#29694;&#22823;&#25209;&#27425;&#22823;&#23567;&#26377;&#30410;&#12290;&#28982;&#32780;&#65292;&#23565;&#26044;&#36889;&#31278;&#22909;&#34389;&#30340;&#29702;&#35542;&#35299;&#37323;&#30446;&#21069;&#26368;&#22810;&#21482;&#33021;&#35498;&#26159;&#21855;&#30332;&#24335;&#30340;&#12290;&#25105;&#20497;&#39318;&#20808;&#35264;&#23519;&#21040;&#65292;&#22312;DP-SGD&#20013;&#65292;&#32317;&#26799;&#24230;&#26041;&#24046;&#21487;&#20197;&#20998;&#35299;&#28858;&#30001;&#21063;&#37319;&#27171;&#21644;&#22122;&#32882;&#24341;&#36215;&#30340;&#26041;&#24046;&#12290;&#28982;&#24460;&#65292;&#25105;&#20497;&#35657;&#26126;&#22312;&#28961;&#38480;&#27425;&#36845;&#20195;&#30340;&#26997;&#38480;&#24773;&#27841;&#19979;&#65292;&#26377;&#25928;&#30340;&#22122;&#32882;&#24341;&#36215;&#30340;&#26041;&#24046;&#23565;&#25209;&#27425;&#22823;&#23567;&#26159;&#19981;&#35722;&#30340;&#12290;&#21097;&#19979;&#30340;&#21063;&#37319;&#27171;&#24341;&#36215;&#30340;&#26041;&#24046;&#38568;&#33879;&#25209;&#27425;&#22823;&#23567;&#30340;&#22686;&#22823;&#32780;&#28187;&#23567;&#65292;&#22240;&#27492;&#22823;&#25209;&#27425;&#22823;&#23567;&#28187;&#23567;&#20102;&#26377;&#25928;&#30340;&#32317;&#26799;&#24230;&#26041;&#24046;&#12290;&#25105;&#20497;&#22312;&#25976;&#20540;&#19978;&#30906;&#35469;&#36889;&#31278;&#28472;&#36914;&#30340;&#24773;&#27841;&#22312;&#23526;&#38555;&#29872;&#22659;&#20013;&#26159;&#30456;&#38364;&#30340;&#65292;&#30070;&#25209;&#27425;&#22823;&#23567;&#19981;&#23567;&#30340;&#26178;&#20505;&#26371;&#36215;&#20316;&#29992;&#65292;&#20006;&#19988;&#30332;&#29694;
&lt;/p&gt;
&lt;p&gt;
We study the effect of the batch size to the total gradient variance in differentially private stochastic gradient descent (DP-SGD), seeking a theoretical explanation for the usefulness of large batch sizes. As DP-SGD is the basis of modern DP deep learning, its properties have been widely studied, and recent works have empirically found large batch sizes to be beneficial. However, theoretical explanations of this benefit are currently heuristic at best. We first observe that the total gradient variance in DP-SGD can be decomposed into subsampling-induced and noise-induced variances. We then prove that in the limit of an infinite number of iterations, the effective noise-induced variance is invariant to the batch size. The remaining subsampling-induced variance decreases with larger batch sizes, so large batches reduce the effective total gradient variance. We confirm numerically that the asymptotic regime is relevant in practical settings when the batch size is not small, and find tha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#65292;&#22686;&#21152;&#20102;&#23545;&#20854;&#29702;&#35770;&#29702;&#35299;&#12290;&#23454;&#39564;&#35777;&#26126;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#23545;&#20110;&#39640;&#26041;&#24046;&#30340;&#19979;&#28216;&#39044;&#27979;&#22120;&#29305;&#21035;&#26377;&#30410;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#32463;&#39564;&#27861;&#21017;&#29992;&#20110;&#36873;&#25321;&#36866;&#24403;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.03985</link><description>&lt;p&gt;
&#23545;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#38598;&#25104;&#36827;&#34892;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Bias-Variance Decomposition for Ensembles over Multiple Synthetic Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#65292;&#22686;&#21152;&#20102;&#23545;&#20854;&#29702;&#35770;&#29702;&#35299;&#12290;&#23454;&#39564;&#35777;&#26126;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#23545;&#20110;&#39640;&#26041;&#24046;&#30340;&#19979;&#28216;&#39044;&#27979;&#22120;&#29305;&#21035;&#26377;&#30410;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#32463;&#39564;&#27861;&#21017;&#29992;&#20110;&#36873;&#25321;&#36866;&#24403;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#20026;&#30417;&#30563;&#23398;&#20064;&#29983;&#25104;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#22909;&#22788;&#65292;&#21253;&#25324;&#22686;&#21152;&#20934;&#30830;&#24615;&#12289;&#26356;&#26377;&#25928;&#30340;&#27169;&#22411;&#36873;&#25321;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#36825;&#20123;&#22909;&#22788;&#22312;&#32463;&#39564;&#19978;&#26377;&#26126;&#30830;&#30340;&#25903;&#25345;&#65292;&#20294;&#23545;&#23427;&#20204;&#30340;&#29702;&#35770;&#29702;&#35299;&#30446;&#21069;&#38750;&#24120;&#26377;&#38480;&#12290;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#20351;&#29992;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#20960;&#31181;&#35774;&#32622;&#30340;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#65292;&#26469;&#22686;&#21152;&#29702;&#35770;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#39044;&#27979;&#65292;&#23545;&#20110;&#39640;&#26041;&#24046;&#30340;&#19979;&#28216;&#39044;&#27979;&#22120;&#65292;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#23558;&#29305;&#21035;&#26377;&#30410;&#65292;&#24182;&#20026;&#22343;&#26041;&#35823;&#24046;&#21644;Brier&#20998;&#25968;&#30340;&#24773;&#20917;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#32463;&#39564;&#27861;&#21017;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#25968;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#19968;&#20010;&#38598;&#25104;&#22312;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20960;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#20197;&#21450;&#19979;&#28216;&#39044;&#27979;&#22120;&#19978;&#30340;&#24615;&#33021;&#26469;&#30740;&#31350;&#25105;&#20204;&#30340;&#29702;&#35770;&#22312;&#23454;&#36341;&#20013;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#27934;&#23519;&#20063;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have highlighted the benefits of generating multiple synthetic datasets for supervised learning, from increased accuracy to more effective model selection and uncertainty estimation. These benefits have clear empirical support, but the theoretical understanding of them is currently very light. We seek to increase the theoretical understanding by deriving bias-variance decompositions for several settings of using multiple synthetic datasets. Our theory predicts multiple synthetic datasets to be especially beneficial for high-variance downstream predictors, and yields a simple rule of thumb to select the appropriate number of synthetic datasets in the case of mean-squared error and Brier score. We investigate how our theory works in practice by evaluating the performance of an ensemble over many synthetic datasets for several real datasets and downstream predictors. The results follow our theory, showing that our insights are also practically relevant.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23485;&#26494;&#20551;&#35774;&#19979;&#30340;&#38543;&#26426;&#20248;&#21270;&#20013;Adam&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22122;&#22768;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36825;&#20010;&#27169;&#22411;&#19979;&#65292;Adam&#31639;&#27861;&#21487;&#20197;&#20197;&#36739;&#39640;&#30340;&#27010;&#29575;&#39640;&#25928;&#22320;&#23547;&#25214;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#19982;&#20854;&#20182;&#38543;&#26426;&#19968;&#38454;&#31639;&#27861;&#30456;&#27604;&#65292;Adam&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#33258;&#36866;&#24212;&#24615;&#33021;&#65292;&#26080;&#38656;&#35843;&#25972;&#27493;&#38271;&#21644;&#38382;&#39064;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.03982</link><description>&lt;p&gt;
&#22312;&#23485;&#26494;&#20551;&#35774;&#19979;&#20851;&#20110;&#38543;&#26426;&#20248;&#21270;&#20013;Adam&#25910;&#25947;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Convergence of Adam for Stochastic Optimization under Relaxed Assumptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23485;&#26494;&#20551;&#35774;&#19979;&#30340;&#38543;&#26426;&#20248;&#21270;&#20013;Adam&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22122;&#22768;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36825;&#20010;&#27169;&#22411;&#19979;&#65292;Adam&#31639;&#27861;&#21487;&#20197;&#20197;&#36739;&#39640;&#30340;&#27010;&#29575;&#39640;&#25928;&#22320;&#23547;&#25214;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#19982;&#20854;&#20182;&#38543;&#26426;&#19968;&#38454;&#31639;&#27861;&#30456;&#27604;&#65292;Adam&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#33258;&#36866;&#24212;&#24615;&#33021;&#65292;&#26080;&#38656;&#35843;&#25972;&#27493;&#38271;&#21644;&#38382;&#39064;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#24615;&#21160;&#37327;&#35780;&#20272;&#65288;Adam&#65289;&#31639;&#27861;&#22312;&#35757;&#32451;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22312;&#38750;&#20984;&#20809;&#28369;&#22330;&#26223;&#19979;&#65292;&#29305;&#21035;&#26159;&#22312;&#21487;&#33021;&#23384;&#22312;&#26080;&#30028;&#26799;&#24230;&#21644;&#20223;&#23556;&#26041;&#24046;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;Adam&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#26465;&#20214;&#19979;&#30340;&#26222;&#36890;Adam&#31639;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22122;&#22768;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#25511;&#21046;&#30528;&#20223;&#23556;&#26041;&#24046;&#22122;&#22768;&#12289;&#26377;&#30028;&#22122;&#22768;&#21644;&#27425;&#39640;&#26031;&#22122;&#22768;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36825;&#20010;&#36890;&#29992;&#22122;&#22768;&#27169;&#22411;&#19979;&#65292;Adam&#31639;&#27861;&#21487;&#20197;&#20197;$\mathcal{O}(\text{poly}(\log T)/\sqrt{T})$&#30340;&#27010;&#29575;&#39640;&#25928;&#22320;&#23547;&#25214;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#65292;&#20854;&#20013;$T$&#34920;&#31034;&#24635;&#36845;&#20195;&#27425;&#25968;&#65292;&#19982;&#38543;&#26426;&#19968;&#38454;&#31639;&#27861;&#30340;&#26356;&#24213;&#25928;&#29575;&#30456;&#21305;&#37197;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#65292;Adam&#31639;&#27861;&#26080;&#38656;&#35843;&#25972;&#27493;&#38271;&#21644;&#20219;&#20309;&#38382;&#39064;&#21442;&#25968;&#65292;&#20855;&#26377;&#27604;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26356;&#22909;&#30340;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Adaptive Momentum Estimation (Adam) algorithm is highly effective in training various deep learning tasks. Despite this, there's limited theoretical understanding for Adam, especially when focusing on its vanilla form in non-convex smooth scenarios with potential unbounded gradients and affine variance noise. In this paper, we study vanilla Adam under these challenging conditions. We introduce a comprehensive noise model which governs affine variance noise, bounded noise and sub-Gaussian noise. We show that Adam can find a stationary point with a $\mathcal{O}(\text{poly}(\log T)/\sqrt{T})$ rate in high probability under this general noise model where $T$ denotes total number iterations, matching the lower rate of stochastic first-order algorithms up to logarithm factors. More importantly, we reveal that Adam is free of tuning step-sizes with any problem-parameters, yielding a better adaptation property than the Stochastic Gradient Descent under the same conditions. We also provide 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#31070;&#32463;&#23849;&#28291;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#26631;&#31614;&#24179;&#28369;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22312;&#26631;&#31614;&#24179;&#28369;&#35757;&#32451;&#19979;&#26356;&#24555;&#22320;&#25910;&#25947;&#21040;&#31070;&#32463;&#23849;&#28291;&#35299;&#65292;&#24182;&#36798;&#21040;&#26356;&#24378;&#30340;&#31070;&#32463;&#23849;&#28291;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#26631;&#31614;&#24179;&#28369;&#25439;&#22833;&#19979;&#30340;&#27169;&#22411;&#22312;&#30456;&#21516;&#30340;NC1&#27700;&#24179;&#19979;&#34920;&#29616;&#20986;&#21152;&#24378;&#30340;NC2&#65292;&#24182;&#21487;&#22312;&#29702;&#35770;&#19978;&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2402.03979</link><description>&lt;p&gt;
&#20132;&#21449;&#29109;&#19982;&#26631;&#31614;&#24179;&#28369;&#65306;&#31070;&#32463;&#23849;&#28291;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Cross Entropy versus Label Smoothing: A Neural Collapse Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#31070;&#32463;&#23849;&#28291;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#26631;&#31614;&#24179;&#28369;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22312;&#26631;&#31614;&#24179;&#28369;&#35757;&#32451;&#19979;&#26356;&#24555;&#22320;&#25910;&#25947;&#21040;&#31070;&#32463;&#23849;&#28291;&#35299;&#65292;&#24182;&#36798;&#21040;&#26356;&#24378;&#30340;&#31070;&#32463;&#23849;&#28291;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#26631;&#31614;&#24179;&#28369;&#25439;&#22833;&#19979;&#30340;&#27169;&#22411;&#22312;&#30456;&#21516;&#30340;NC1&#27700;&#24179;&#19979;&#34920;&#29616;&#20986;&#21152;&#24378;&#30340;NC2&#65292;&#24182;&#21487;&#22312;&#29702;&#35770;&#19978;&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24179;&#28369;&#25439;&#22833;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24191;&#27867;&#37319;&#29992;&#30340;&#19968;&#31181;&#25216;&#26415;&#65292;&#29992;&#20110;&#20943;&#36731;&#36807;&#25311;&#21512;&#12290;&#26412;&#25991;&#20174;&#31070;&#32463;&#23849;&#28291;&#65288;NC&#65289;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#26631;&#31614;&#24179;&#28369;&#65292;&#36825;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#32463;&#39564;&#21644;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#25551;&#36848;&#35757;&#32451;&#30340;&#26368;&#21518;&#38454;&#27573;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#26631;&#31614;&#24179;&#28369;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#24555;&#22320;&#25910;&#25947;&#21040;&#31070;&#32463;&#23849;&#28291;&#35299;&#65292;&#24182;&#36798;&#21040;&#26356;&#24378;&#30340;&#31070;&#32463;&#23849;&#28291;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#22312;&#30456;&#21516;&#30340;NC1&#27700;&#24179;&#19979;&#65292;&#26631;&#31614;&#24179;&#28369;&#25439;&#22833;&#19979;&#30340;&#27169;&#22411;&#26174;&#31034;&#20986;&#21152;&#24378;&#30340;NC2&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#29702;&#35299;&#26631;&#31614;&#24179;&#28369;&#25439;&#22833;&#19979;&#30340;&#24615;&#33021;&#20248;&#21183;&#21644;&#22686;&#24378;&#30340;&#27169;&#22411;&#26657;&#20934;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#26080;&#32422;&#26463;&#29305;&#24449;&#27169;&#22411;&#25512;&#23548;&#20986;&#20004;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#38381;&#24335;&#35299;&#65292;&#24182;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#26631;&#31614;&#24179;&#28369;&#19979;&#30340;&#27169;&#22411;&#20855;&#26377;&#36739;&#20302;&#30340;&#26465;&#20214;&#25968;&#65292;&#22240;&#27492;&#22312;&#29702;&#35770;&#19978;&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32508;&#21512;&#20102;&#32463;&#39564;&#21644;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#20026;&#29702;&#35299;&#26631;&#31614;&#24179;&#28369;&#30340;&#25928;&#26524;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label smoothing loss is a widely adopted technique to mitigate overfitting in deep neural networks. This paper studies label smoothing from the perspective of Neural Collapse (NC), a powerful empirical and theoretical framework which characterizes model behavior during the terminal phase of training. We first show empirically that models trained with label smoothing converge faster to neural collapse solutions and attain a stronger level of neural collapse. Additionally, we show that at the same level of NC1, models under label smoothing loss exhibit intensified NC2. These findings provide valuable insights into the performance benefits and enhanced model calibration under label smoothing loss. We then leverage the unconstrained feature model to derive closed-form solutions for the global minimizers for both loss functions and further demonstrate that models under label smoothing have a lower conditioning number and, therefore, theoretically converge faster. Our study, combining empiri
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#19978;&#34920;&#29616;&#20248;&#20110;&#28145;&#24230;&#32593;&#32476;&#65292;&#24403;&#32473;&#20104;&#36275;&#22815;&#26102;&#38388;&#26102;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22270;&#20687;&#26333;&#20809;&#26102;&#38388;&#30340;&#38480;&#21046;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#38477;&#33267;&#28145;&#24230;&#32593;&#32476;&#30340;&#27700;&#24179;&#65292;&#36825;&#26263;&#31034;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#26102;&#38656;&#35201;&#39069;&#22806;&#30340;&#24515;&#29702;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#19982;&#32593;&#32476;&#20043;&#38388;&#30340;&#38169;&#35823;&#27169;&#24335;&#20063;&#23384;&#22312;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.03973</link><description>&lt;p&gt;
&#32473;&#20104;&#36275;&#22815;&#26102;&#38388;&#65292;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#19978;&#20987;&#36133;&#20102;&#28145;&#24230;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Humans Beat Deep Networks at Recognizing Objects in Unusual Poses, Given Enough Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03973
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#19978;&#34920;&#29616;&#20248;&#20110;&#28145;&#24230;&#32593;&#32476;&#65292;&#24403;&#32473;&#20104;&#36275;&#22815;&#26102;&#38388;&#26102;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22270;&#20687;&#26333;&#20809;&#26102;&#38388;&#30340;&#38480;&#21046;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#38477;&#33267;&#28145;&#24230;&#32593;&#32476;&#30340;&#27700;&#24179;&#65292;&#36825;&#26263;&#31034;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#26102;&#38656;&#35201;&#39069;&#22806;&#30340;&#24515;&#29702;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#19982;&#32593;&#32476;&#20043;&#38388;&#30340;&#38169;&#35823;&#27169;&#24335;&#20063;&#23384;&#22312;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#20960;&#20010;&#29289;&#20307;&#35782;&#21035;&#22522;&#20934;&#19978;&#27491;&#22312;&#32553;&#23567;&#19982;&#20154;&#31867;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#22312;&#28041;&#21450;&#20174;&#19981;&#23547;&#24120;&#35270;&#35282;&#35266;&#23519;&#29289;&#20307;&#30340;&#25361;&#25112;&#24615;&#22270;&#20687;&#20013;&#23545;&#36825;&#19968;&#24046;&#36317;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#19982;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#65288;EfficientNet&#12289;SWAG&#12289;ViT&#12289;SWIN&#12289;BEiT&#12289;ConvNext&#65289;&#30456;&#27604;&#65292;&#36825;&#20123;&#32593;&#32476;&#22312;&#27492;&#24773;&#20917;&#19979;&#26222;&#36941;&#33030;&#24369;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#38543;&#30528;&#25105;&#20204;&#38480;&#21046;&#22270;&#20687;&#26333;&#20809;&#26102;&#38388;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#19979;&#38477;&#21040;&#28145;&#24230;&#32593;&#32476;&#30340;&#27700;&#24179;&#65292;&#36825;&#34920;&#26126;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#26102;&#38656;&#35201;&#39069;&#22806;&#30340;&#24515;&#29702;&#36807;&#31243;&#65288;&#38656;&#35201;&#39069;&#22806;&#30340;&#26102;&#38388;&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20154;&#31867;&#19982;&#32593;&#32476;&#30340;&#38169;&#35823;&#27169;&#24335;&#65292;&#21457;&#29616;&#21363;&#20351;&#22312;&#38480;&#21046;&#26102;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#20154;&#31867;&#19982;&#21069;&#39304;&#28145;&#24230;&#32593;&#32476;&#20063;&#26377;&#19981;&#21516;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#38656;&#35201;&#26356;&#22810;&#30340;&#24037;&#20316;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#24102;&#21040;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#27700;&#24179;&#12290;&#29702;&#35299;&#22312;&#22806;&#37096;&#24773;&#20917;&#19979;&#21457;&#29983;&#30340;&#24515;&#29702;&#36807;&#31243;&#30340;&#26412;&#36136;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning is closing the gap with humans on several object recognition benchmarks. Here we investigate this gap in the context of challenging images where objects are seen from unusual viewpoints. We find that humans excel at recognizing objects in unusual poses, in contrast with state-of-the-art pretrained networks (EfficientNet, SWAG, ViT, SWIN, BEiT, ConvNext) which are systematically brittle in this condition. Remarkably, as we limit image exposure time, human performance degrades to the level of deep networks, suggesting that additional mental processes (requiring additional time) take place when humans identify objects in unusual poses. Finally, our analysis of error patterns of humans vs. networks reveals that even time-limited humans are dissimilar to feed-forward deep networks. We conclude that more work is needed to bring computer vision systems to the level of robustness of the human visual system. Understanding the nature of the mental processes taking place during extr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#39033;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#36824;&#27604;&#36739;&#20102;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#21644;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#19982;&#27531;&#24046;&#36830;&#25509;&#30340;&#26550;&#26500;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#20915;&#31574;&#26641;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#32780;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#22312;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#24182;&#27809;&#26377;&#36229;&#36807;&#20256;&#32479;MLP&#26550;&#26500;&#30340;&#31616;&#21270;&#21464;&#20307;&#12290;</title><link>https://arxiv.org/abs/2402.03970</link><description>&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#65306;&#27880;&#24847;&#21147;&#26159;&#21807;&#19968;&#38656;&#35201;&#30340;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Tabular Data: Is Attention All You Need?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#39033;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#36824;&#27604;&#36739;&#20102;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#21644;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#19982;&#27531;&#24046;&#36830;&#25509;&#30340;&#26550;&#26500;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#20915;&#31574;&#26641;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#32780;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#22312;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#24182;&#27809;&#26377;&#36229;&#36807;&#20256;&#32479;MLP&#26550;&#26500;&#30340;&#31616;&#21270;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#24182;&#22312;&#28041;&#21450;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#22312;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#20248;&#21183;&#23384;&#22312;&#30528;&#19981;&#19968;&#33268;&#30340;&#35777;&#25454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#39033;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#36824;&#27604;&#36739;&#20102;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#21644;&#20256;&#32479;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#19982;&#27531;&#24046;&#36830;&#25509;&#30340;&#26550;&#26500;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#34920;&#26126;&#31070;&#32463;&#32593;&#32476;&#22312;&#20915;&#31574;&#26641;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#22312;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#24182;&#27809;&#26377;&#36229;&#36807;&#20256;&#32479;MLP&#26550;&#26500;&#30340;&#31616;&#21270;&#21464;&#20307;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24110;&#21161;&#30740;&#31350;&#21644;&#23454;&#36341;&#31038;&#21306;&#22312;&#26410;&#26469;&#30340;&#34920;&#26684;&#25968;&#25454;&#24212;&#29992;&#20013;&#20570;&#20986;&#26126;&#26234;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning has revolutionized the field of AI and led to remarkable achievements in applications involving image and text data. Unfortunately, there is inconclusive evidence on the merits of neural networks for structured tabular data. In this paper, we introduce a large-scale empirical study comparing neural networks against gradient-boosted decision trees on tabular data, but also transformer-based architectures against traditional multi-layer perceptrons (MLP) with residual connections. In contrast to prior work, our empirical findings indicate that neural networks are competitive against decision trees. Furthermore, we assess that transformer-based architectures do not outperform simpler variants of traditional MLP architectures on tabular datasets. As a result, this paper helps the research and practitioner communities make informed choices on deploying neural networks on future tabular data applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#19981;&#23545;&#31216;&#26041;&#24335;&#26356;&#26032;&#20854;&#20449;&#24565;&#65292;&#26356;&#22810;&#22320;&#20174;&#27604;&#39044;&#26399;&#22909;&#30340;&#32467;&#26524;&#20013;&#23398;&#21040;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#23398;&#20064;&#20851;&#20110;&#21453;&#20107;&#23454;&#21453;&#39304;&#26102;&#65292;&#36825;&#31181;&#25928;&#24212;&#20250;&#36870;&#36716;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24037;&#20316;&#26041;&#24335;&#65292;&#24182;&#25581;&#31034;&#20102;&#38382;&#39064;&#26694;&#26550;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.03969</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#26234;&#33021;&#20307;&#26159;&#19981;&#23545;&#31216;&#30340;&#20449;&#24565;&#26356;&#26032;&#32773;
&lt;/p&gt;
&lt;p&gt;
In-context learning agents are asymmetric belief updaters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#19981;&#23545;&#31216;&#26041;&#24335;&#26356;&#26032;&#20854;&#20449;&#24565;&#65292;&#26356;&#22810;&#22320;&#20174;&#27604;&#39044;&#26399;&#22909;&#30340;&#32467;&#26524;&#20013;&#23398;&#21040;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#23398;&#20064;&#20851;&#20110;&#21453;&#20107;&#23454;&#21453;&#39304;&#26102;&#65292;&#36825;&#31181;&#25928;&#24212;&#20250;&#36870;&#36716;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24037;&#20316;&#26041;&#24335;&#65292;&#24182;&#25581;&#31034;&#20102;&#38382;&#39064;&#26694;&#26550;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#21160;&#24577;&#65292;&#36890;&#36807;&#20174;&#35748;&#30693;&#24515;&#29702;&#23398;&#20013;&#24341;&#20837;&#30340;&#19977;&#20010;&#20202;&#22120;&#23398;&#20064;&#20219;&#21153;&#36827;&#34892;&#36866;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#20197;&#19981;&#23545;&#31216;&#30340;&#26041;&#24335;&#26356;&#26032;&#20854;&#20449;&#24565;&#65292;&#24182;&#19988;&#20174;&#27604;&#39044;&#26399;&#22909;&#30340;&#32467;&#26524;&#20013;&#23398;&#21040;&#30340;&#19996;&#35199;&#27604;&#20174;&#27604;&#39044;&#26399;&#24046;&#30340;&#32467;&#26524;&#20013;&#23398;&#21040;&#30340;&#19996;&#35199;&#26356;&#22810;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#24403;&#23398;&#20064;&#20851;&#20110;&#21453;&#20107;&#23454;&#21453;&#39304;&#26102;&#65292;&#36825;&#31181;&#25928;&#24212;&#20250;&#36870;&#36716;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#28041;&#21450;&#20195;&#29702;&#24615;&#26102;&#28040;&#22833;&#12290;&#36890;&#36807;&#30740;&#31350;&#36890;&#36807;&#20803;-&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#30340;&#29702;&#24819;&#21270;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#23454;&#20102;&#36825;&#20123;&#21457;&#29616;&#65292;&#21457;&#29616;&#31867;&#20284;&#30340;&#27169;&#24335;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24037;&#20316;&#26041;&#24335;&#65292;&#31361;&#20986;&#26174;&#31034;&#38382;&#39064;&#30340;&#26694;&#26550;&#26174;&#33879;&#24433;&#21709;&#23398;&#20064;&#30340;&#36807;&#31243;&#65292;&#36825;&#20063;&#26159;&#20154;&#31867;&#35748;&#30693;&#20013;&#35266;&#23519;&#21040;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the in-context learning dynamics of large language models (LLMs) using three instrumental learning tasks adapted from cognitive psychology. We find that LLMs update their beliefs in an asymmetric manner and learn more from better-than-expected outcomes than from worse-than-expected ones. Furthermore, we show that this effect reverses when learning about counterfactual feedback and disappears when no agency is implied. We corroborate these findings by investigating idealized in-context learning agents derived through meta-reinforcement learning, where we observe similar patterns. Taken together, our results contribute to our understanding of how in-context learning works by highlighting that the framing of a problem significantly influences how learning occurs, a phenomenon also observed in human cognition.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#32771;&#23519;&#20102;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#29305;&#24449;&#21521;&#37327;&#30340;&#32500;&#24230;&#38382;&#39064;&#65292;&#21457;&#29616;&#23454;&#38469;&#20351;&#29992;&#30340;&#26550;&#26500;&#19982;&#29702;&#35770;&#20445;&#35777;&#23384;&#22312;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.03966</link><description>&lt;p&gt;
&#20851;&#20110;MPNN&#20013;&#29305;&#24449;&#21521;&#37327;&#30340;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
On dimensionality of feature vectors in MPNNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03966
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#32771;&#23519;&#20102;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#29305;&#24449;&#21521;&#37327;&#30340;&#32500;&#24230;&#38382;&#39064;&#65292;&#21457;&#29616;&#23454;&#38469;&#20351;&#29992;&#30340;&#26550;&#26500;&#19982;&#29702;&#35770;&#20445;&#35777;&#23384;&#22312;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#32771;&#23519;&#20102;Morris&#31561;&#20154;&#65288;AAAI'19&#65289;&#20851;&#20110;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#19982;Weisfeiler-Leman&#65288;WL&#65289;&#21516;&#26500;&#27979;&#35797;&#22312;&#21306;&#20998;&#33021;&#21147;&#19978;&#30456;&#31561;&#30340;&#32463;&#20856;&#32467;&#26524;&#12290;Morris&#31561;&#20154;&#23637;&#31034;&#20102;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#21644;$O(n)$&#32500;&#29305;&#24449;&#21521;&#37327;&#30340;&#20223;&#30495;&#32467;&#26524;&#65292;&#20854;&#20013;$n$&#26159;&#22270;&#30340;&#33410;&#28857;&#25968;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#24615;&#24341;&#20837;&#21040;&#26550;&#26500;&#20013;&#65292;Aamand&#31561;&#20154;&#65288;NeurIPS'22&#65289;&#33021;&#22815;&#23558;&#29305;&#24449;&#21521;&#37327;&#30340;&#32500;&#24230;&#25552;&#39640;&#21040;$O(\log n)$&#65292;&#23613;&#31649;&#20197;&#39640;&#27010;&#29575;&#20445;&#35777;&#23436;&#20840;&#27169;&#25311;&#30340;&#24320;&#38144;&#20063;&#22686;&#21152;&#20102;&#12290;&#22312;&#25152;&#26377;&#36825;&#20123;&#26500;&#36896;&#20013;&#65292;&#20026;&#20102;&#20445;&#35777;&#19982;WL&#27979;&#35797;&#30340;&#31561;&#20215;&#24615;&#65292;MPNN&#20013;&#30340;&#29305;&#24449;&#21521;&#37327;&#32500;&#24230;&#24517;&#39035;&#38543;&#30528;&#22270;&#30340;&#22823;&#23567;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#20351;&#29992;&#30340;&#26550;&#26500;&#20855;&#26377;&#24658;&#23450;&#32500;&#24230;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#32467;&#26524;&#25552;&#20379;&#30340;&#20445;&#35777;&#19982;&#23454;&#38469;&#20351;&#29992;&#30340;&#26550;&#26500;&#29305;&#24615;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit the classical result of Morris et al.~(AAAI'19) that message-passing graphs neural networks (MPNNs) are equal in their distinguishing power to the Weisfeiler--Leman (WL) isomorphism test.   Morris et al.~show their simulation result with ReLU activation function and $O(n)$-dimensional feature vectors, where $n$ is the number of nodes of the graph. Recently, by introducing randomness into the architecture, Aamand et al.~(NeurIPS'22) were able to improve this bound to $O(\log n)$-dimensional feature vectors, although at the expense of guaranteeing perfect simulation only with high probability.   In all these constructions, to guarantee equivalence to the WL test, the dimension of feature vectors in the MPNN has to increase with the size of the graphs. However, architectures used in practice have feature vectors of constant dimension. Thus, there is a gap between the guarantees provided by these results and the actual characteristics of architectures used in practice. In this p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COAT&#65306;&#22240;&#26524;&#34920;&#31034;&#21161;&#25163;&#65292;&#35813;&#21161;&#25163;&#20174;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#30340;&#22240;&#26524;&#22240;&#23376;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20026;&#25506;&#32034;&#38544;&#34255;&#19990;&#30028;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2402.03941</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#38544;&#34255;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
Discovery of the Hidden World with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03941
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COAT&#65306;&#22240;&#26524;&#34920;&#31034;&#21161;&#25163;&#65292;&#35813;&#21161;&#25163;&#20174;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#30340;&#22240;&#26524;&#22240;&#23376;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20026;&#25506;&#32034;&#38544;&#34255;&#19990;&#30028;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#36215;&#28304;&#20110;&#20174;&#24050;&#30693;&#20107;&#23454;&#21644;&#35266;&#23519;&#20013;&#21457;&#29616;&#26032;&#30340;&#22240;&#26524;&#30693;&#35782;&#12290;&#20256;&#32479;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#27979;&#37327;&#21464;&#37327;&#65292;&#36890;&#24120;&#30001;&#20154;&#31867;&#19987;&#23478;&#25552;&#20379;&#65292;&#20197;&#25214;&#21040;&#22240;&#26524;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#22240;&#26524;&#21464;&#37327;&#36890;&#24120;&#26080;&#27861;&#33719;&#21462;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23835;&#36215;&#20026;&#20174;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#20013;&#21457;&#29616;&#39640;&#32423;&#38544;&#34255;&#21464;&#37327;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;COAT&#65306;&#22240;&#26524;&#34920;&#31034;&#21161;&#25163;&#12290;COAT&#23558;LLMs&#20316;&#20026;&#22240;&#32032;&#25552;&#20379;&#22120;&#24341;&#20837;&#65292;&#25552;&#21462;&#20986;&#26469;&#33258;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#28508;&#22312;&#22240;&#26524;&#22240;&#23376;&#12290;&#27492;&#22806;&#65292;LLMs&#36824;&#21487;&#20197;&#34987;&#25351;&#31034;&#25552;&#20379;&#29992;&#20110;&#25910;&#38598;&#25968;&#25454;&#20540;&#65288;&#20363;&#22914;&#27880;&#37322;&#26631;&#20934;&#65289;&#30340;&#39069;&#22806;&#20449;&#24687;&#65292;&#24182;&#23558;&#21407;&#22987;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#19968;&#27493;&#35299;&#26512;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#27880;&#37322;&#25968;&#25454;&#23558;&#34987;&#36755;&#20837;&#21040;...
&lt;/p&gt;
&lt;p&gt;
Science originates with discovering new causal knowledge from a combination of known facts and observations. Traditional causal discovery approaches mainly rely on high-quality measured variables, usually given by human experts, to find causal relations. However, the causal variables are usually unavailable in a wide range of real-world applications. The rise of large language models (LLMs) that are trained to learn rich knowledge from the massive observations of the world, provides a new opportunity to assist with discovering high-level hidden variables from the raw observational data. Therefore, we introduce COAT: Causal representatiOn AssistanT. COAT incorporates LLMs as a factor proposer that extracts the potential causal factors from unstructured data. Moreover, LLMs can also be instructed to provide additional information used to collect data values (e.g., annotation criteria) and to further parse the raw unstructured data into structured data. The annotated data will be fed to a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23454;&#29616;&#20102;&#21322;&#23548;&#20307;&#37327;&#23376;&#27604;&#29305;&#30340;&#20840;&#33258;&#21160;&#35843;&#35856;&#65292;&#36890;&#36807;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#12289;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#65292;&#22312;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#20174;&#25509;&#22320;&#22120;&#20214;&#21040;&#25289;&#27604;&#25391;&#33633;&#30340;&#23436;&#20840;&#33258;&#20027;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.03931</link><description>&lt;p&gt;
&#23436;&#20840;&#33258;&#20027;&#35843;&#35856;&#33258;&#26059;&#37327;&#23376;&#27604;&#29305;
&lt;/p&gt;
&lt;p&gt;
Fully autonomous tuning of a spin qubit
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23454;&#29616;&#20102;&#21322;&#23548;&#20307;&#37327;&#23376;&#27604;&#29305;&#30340;&#20840;&#33258;&#21160;&#35843;&#35856;&#65292;&#36890;&#36807;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#12289;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#65292;&#22312;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#20174;&#25509;&#22320;&#22120;&#20214;&#21040;&#25289;&#27604;&#25391;&#33633;&#30340;&#23436;&#20840;&#33258;&#20027;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#37324;&#65292;&#21322;&#23548;&#20307;&#19978;&#30340;&#37327;&#23376;&#27604;&#29305;&#30740;&#31350;&#20026;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#20570;&#20986;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#21322;&#23548;&#20307;&#37327;&#23376;&#30005;&#36335;&#30340;&#21457;&#23637;&#20173;&#21463;&#38480;&#20110;&#26377;&#25928;&#35843;&#35856;&#21644;&#25805;&#20316;&#36825;&#20123;&#30005;&#36335;&#30340;&#25361;&#25112;&#12290;&#30830;&#23450;&#36825;&#20123;&#27604;&#29305;&#30340;&#26368;&#20339;&#25805;&#20316;&#26465;&#20214;&#26159;&#22797;&#26434;&#30340;&#65292;&#28041;&#21450;&#21040;&#24191;&#27867;&#30340;&#21442;&#25968;&#31354;&#38388;&#30340;&#25506;&#32034;&#12290;&#36825;&#25552;&#20986;&#20102;&#19968;&#20010;&#29616;&#23454;&#20013;&#30340;&#8220;&#22823;&#28023;&#25438;&#38024;&#8221;&#38382;&#39064;&#65292;&#30452;&#21040;&#29616;&#22312;&#65292;&#30001;&#20110;&#22120;&#20214;&#21464;&#24322;&#24615;&#21644;&#21046;&#36896;&#29781;&#30133;&#65292;&#36825;&#19968;&#38382;&#39064;&#20173;&#26410;&#23436;&#20840;&#23454;&#29616;&#33258;&#21160;&#21270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25253;&#36947;&#20102;&#21322;&#23548;&#20307;&#37327;&#23376;&#27604;&#29305;&#30340;&#20840;&#33258;&#21160;&#35843;&#35856;&#65292;&#20174;&#25509;&#22320;&#30340;&#22120;&#20214;&#21040;&#25289;&#27604;&#25391;&#33633;&#65292;&#28165;&#26224;&#22320;&#34920;&#31034;&#27604;&#29305;&#25805;&#20316;&#25104;&#21151;&#12290;&#25105;&#20204;&#22312;Ge/Si&#26680;/&#22771;&#32435;&#31859;&#32447;&#22120;&#20214;&#20013;&#23637;&#31034;&#20102;&#36825;&#31181;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#23454;&#29616;&#30340;&#33258;&#21160;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#28145;&#24230;&#23398;&#20064;&#12289;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#36827;&#34892;&#20102;&#25972;&#21512;&#12290;&#25105;&#20204;&#26399;&#26395;&#36825;&#31181;&#33258;&#21160;&#21270;&#31639;&#27861;&#33021;&#22815;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#21322;&#23548;&#20307;&#37327;&#23376;&#30005;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spanning over two decades, the study of qubits in semiconductors for quantum computing has yielded significant breakthroughs. However, the development of large-scale semiconductor quantum circuits is still limited by challenges in efficiently tuning and operating these circuits. Identifying optimal operating conditions for these qubits is complex, involving the exploration of vast parameter spaces. This presents a real 'needle in the haystack' problem, which, until now, has resisted complete automation due to device variability and fabrication imperfections. In this study, we present the first fully autonomous tuning of a semiconductor qubit, from a grounded device to Rabi oscillations, a clear indication of successful qubit operation. We demonstrate this automation, achieved without human intervention, in a Ge/Si core/shell nanowire device. Our approach integrates deep learning, Bayesian optimization, and computer vision techniques. We expect this automation algorithm to apply to a wi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36820;&#22238;&#23545;&#40784;&#30340;&#20915;&#31574;Transformer&#65288;RADT&#65289;&#65292;&#36890;&#36807;&#20998;&#31163;&#22238;&#25253;&#19982;&#20256;&#32479;&#36755;&#20837;&#24207;&#21015;&#65292;&#23454;&#29616;&#26377;&#25928;&#22320;&#23558;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2402.03923</link><description>&lt;p&gt;
&#36820;&#22238;&#23545;&#40784;&#30340;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
Return-Aligned Decision Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36820;&#22238;&#23545;&#40784;&#30340;&#20915;&#31574;Transformer&#65288;RADT&#65289;&#65292;&#36890;&#36807;&#20998;&#31163;&#22238;&#25253;&#19982;&#20256;&#32479;&#36755;&#20837;&#24207;&#21015;&#65292;&#23454;&#29616;&#26377;&#25928;&#22320;&#23558;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#23398;&#20064;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#65288;&#21363;&#22238;&#25253;&#65289;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24212;&#29992;&#33539;&#22260;&#30340;&#25193;&#22823;&#65292;&#35757;&#32451;&#33021;&#22815;&#26368;&#22823;&#21270;&#22238;&#25253;&#24182;&#20351;&#23454;&#38469;&#22238;&#25253;&#19982;&#25351;&#23450;&#30446;&#26631;&#22238;&#25253;&#23545;&#40784;&#30340;&#26234;&#33021;&#20307;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20174;&#32780;&#25511;&#21046;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#12290;&#20915;&#31574;Transformer&#65288;DT&#65289;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#20248;&#21270;&#29983;&#25104;&#20197;&#30446;&#26631;&#22238;&#25253;&#20026;&#26465;&#20214;&#30340;&#21160;&#20316;&#30340;&#31574;&#30053;&#65292;&#24182;&#37197;&#22791;&#20102;&#20351;&#29992;&#30446;&#26631;&#22238;&#25253;&#25511;&#21046;&#26234;&#33021;&#20307;&#30340;&#26426;&#21046;&#12290;&#23613;&#31649;DT&#26088;&#22312;&#23545;&#40784;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#65292;&#20294;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#20102;DT&#20013;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36820;&#22238;&#23545;&#40784;&#30340;&#20915;&#31574;Transformer&#65288;RADT&#65289;&#65292;&#26088;&#22312;&#26377;&#25928;&#22320;&#23558;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#22238;&#25253;&#20174;&#20256;&#32479;&#30340;&#36755;&#20837;&#24207;&#21015;&#20013;&#20998;&#31163;&#20986;&#26469;&#65292;&#20256;&#32479;&#36755;&#20837;&#24207;&#21015;&#36890;&#24120;&#21253;&#21547;&#22238;&#25253;&#12289;&#29366;&#24577;&#21644;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional approaches in offline reinforcement learning aim to learn the optimal policy that maximizes the cumulative reward, also known as return. However, as applications broaden, it becomes increasingly crucial to train agents that not only maximize the returns, but align the actual return with a specified target return, giving control over the agent's performance. Decision Transformer (DT) optimizes a policy that generates actions conditioned on the target return through supervised learning and is equipped with a mechanism to control the agent using the target return. Despite being designed to align the actual return with the target return, we have empirically identified a discrepancy between the actual return and the target return in DT. In this paper, we propose Return-Aligned Decision Transformer (RADT), designed to effectively align the actual return with the target return. Our model decouples returns from the conventional input sequence, which typically consists of returns, s
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLAMBO&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#12290;&#36890;&#36807;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;BO&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;LLAMBO&#33021;&#22815;&#25552;&#20379;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#22312;&#38646;&#26679;&#26412;&#28909;&#21551;&#21160;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03921</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large Language Models to Enhance Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03921
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLAMBO&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#12290;&#36890;&#36807;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;BO&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;LLAMBO&#33021;&#22815;&#25552;&#20379;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#22312;&#38646;&#26679;&#26412;&#28909;&#21551;&#21160;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#26159;&#19968;&#31181;&#20248;&#21270;&#22797;&#26434;&#21644;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#23427;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#24615;&#24471;&#21040;&#20102;&#24378;&#35843;&#65292;&#29305;&#21035;&#26159;&#36229;&#21442;&#25968;&#35843;&#20248;&#65292;&#20294;&#20854;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#26377;&#25928;&#22320;&#24179;&#34913;&#21208;&#25506;&#21644;&#24320;&#21457;&#12290;&#23613;&#31649;&#22312;BO&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#24179;&#34913;&#36825;&#19968;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#24494;&#22937;&#30340;&#36807;&#31243;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;LLAMBO&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#19982;BO&#30456;&#32467;&#21512;&#12290;&#22312;&#39640;&#23618;&#27425;&#19978;&#65292;&#25105;&#20204;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#26041;&#24335;&#26469;&#25551;&#36848;BO&#38382;&#39064;&#65292;&#20351;LLM&#33021;&#22815;&#26681;&#25454;&#21382;&#21490;&#35780;&#20272;&#25552;&#20986;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#32467;&#21512;LLM&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;&#26469;&#22686;&#24378;&#22522;&#20110;&#27169;&#22411;&#30340;BO&#30340;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLAMBO&#22312;&#38646;&#26679;&#26412;&#28909;&#21551;&#21160;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#25913;&#21892;&#20195;&#29702;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is a powerful approach for optimizing complex and expensive-to-evaluate black-box functions. Its importance is underscored in many applications, notably including hyperparameter tuning, but its efficacy depends on efficiently balancing exploration and exploitation. While there has been substantial progress in BO methods, striking this balance still remains a delicate process. In this light, we present \texttt{LLAMBO}, a novel approach that integrates the capabilities of large language models (LLM) within BO. At a high level, we frame the BO problem in natural language terms, enabling LLMs to iteratively propose promising solutions conditioned on historical evaluations. More specifically, we explore how combining contextual understanding, few-shot learning proficiency, and domain knowledge of LLMs can enhance various components of model-based BO. Our findings illustrate that \texttt{LLAMBO} is effective at zero-shot warmstarting, and improves surrogate modelin
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#22330;&#26223;&#30340;&#26080;&#31034;&#20363;&#22686;&#37327;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#29305;&#24449;&#25972;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35268;&#33539;&#29305;&#24449;&#28418;&#31227;&#24182;&#21033;&#29992;&#21407;&#22411;&#26469;&#20943;&#23569;&#20219;&#21153;&#26032;&#40092;&#24230;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.03917</link><description>&lt;p&gt;
&#20919;&#21551;&#21160;&#26080;&#31034;&#20363;&#22686;&#37327;&#23398;&#20064;&#30340;&#24377;&#24615;&#29305;&#24449;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
Elastic Feature Consolidation for Cold Start Exemplar-free Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03917
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#20919;&#21551;&#21160;&#22330;&#26223;&#30340;&#26080;&#31034;&#20363;&#22686;&#37327;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#29305;&#24449;&#25972;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35268;&#33539;&#29305;&#24449;&#28418;&#31227;&#24182;&#21033;&#29992;&#21407;&#22411;&#26469;&#20943;&#23569;&#20219;&#21153;&#26032;&#40092;&#24230;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#31034;&#20363;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65288;EFCIL&#65289;&#26088;&#22312;&#20174;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#23398;&#20064;&#65292;&#32780;&#19981;&#38656;&#35201;&#35775;&#38382;&#20808;&#21069;&#20219;&#21153;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20919;&#21551;&#21160;&#22330;&#26223;&#65292;&#22312;&#31532;&#19968;&#20010;&#20219;&#21153;&#20013;&#27809;&#26377;&#36275;&#22815;&#30340;&#25968;&#25454;&#26469;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#39592;&#24178;&#32593;&#32476;&#12290;&#23545;&#20110;EFCIL&#26469;&#35828;&#65292;&#36825;&#26159;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#39640;&#24230;&#30340;&#21487;&#22609;&#24615;&#65292;&#36825;&#20250;&#23548;&#33268;&#29305;&#24449;&#28418;&#31227;&#65292;&#22312;&#26080;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#36827;&#34892;&#34917;&#20607;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35268;&#33539;&#22312;&#19982;&#20808;&#21069;&#20219;&#21153;&#39640;&#24230;&#30456;&#20851;&#30340;&#26041;&#21521;&#19978;&#30340;&#28418;&#31227;&#65292;&#24182;&#21033;&#29992;&#21407;&#22411;&#26469;&#20943;&#23569;&#20219;&#21153;&#26032;&#40092;&#24230;&#20559;&#24046;&#65292;&#20197;&#25972;&#21512;&#29305;&#24449;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#24377;&#24615;&#29305;&#24449;&#25972;&#21512;&#65288;EFC&#65289;&#65292;&#23427;&#21033;&#29992;&#22522;&#20110;&#32463;&#39564;&#29305;&#24449;&#30697;&#38453;&#65288;EFM&#65289;&#30340;&#21487;&#35299;&#20108;&#38454;&#36817;&#20284;&#26469;&#22788;&#29702;&#29305;&#24449;&#28418;&#31227;&#12290;EFM&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#24341;&#20837;&#20102;&#20266;&#24230;&#37327;&#65292;&#25105;&#20204;&#20351;&#29992;&#23427;&#26469;&#35268;&#33539;&#37325;&#35201;&#26041;&#21521;&#19978;&#30340;&#29305;&#24449;&#28418;&#31227;&#65292;&#24182;&#26356;&#26032;&#39640;&#26031;&#21407;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exemplar-Free Class Incremental Learning (EFCIL) aims to learn from a sequence of tasks without having access to previous task data. In this paper, we consider the challenging Cold Start scenario in which insufficient data is available in the first task to learn a high-quality backbone. This is especially challenging for EFCIL since it requires high plasticity, which results in feature drift which is difficult to compensate for in the exemplar-free setting. To address this problem, we propose a simple and effective approach that consolidates feature representations by regularizing drift in directions highly relevant to previous tasks and employs prototypes to reduce task-recency bias. Our method, called Elastic Feature Consolidation (EFC), exploits a tractable second-order approximation of feature drift based on an Empirical Feature Matrix (EFM). The EFM induces a pseudo-metric in feature space which we use to regularize feature drift in important directions and to update Gaussian prot
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30701;&#26399;&#20449;&#21495;&#20013;&#23398;&#20064;&#25351;&#26631;&#65292;&#30452;&#25509;&#26368;&#22823;&#21270;&#25351;&#26631;&#19982;&#21271;&#26497;&#24230;&#37327;&#26631;&#20934;&#20043;&#38388;&#30340;&#32479;&#35745;&#33021;&#21147;&#65292;&#20174;&#32780;&#20943;&#23569;&#22312;&#32447;&#25511;&#21046;&#23454;&#39564;&#30340;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.03915</link><description>&lt;p&gt;
&#23398;&#20064;&#26368;&#22823;&#21270;&#21152;&#36895;A/B&#27979;&#35797;&#30340;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Learning Metrics that Maximise Power for Accelerated A/B-Tests
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#30701;&#26399;&#20449;&#21495;&#20013;&#23398;&#20064;&#25351;&#26631;&#65292;&#30452;&#25509;&#26368;&#22823;&#21270;&#25351;&#26631;&#19982;&#21271;&#26497;&#24230;&#37327;&#26631;&#20934;&#20043;&#38388;&#30340;&#32479;&#35745;&#33021;&#21147;&#65292;&#20174;&#32780;&#20943;&#23569;&#22312;&#32447;&#25511;&#21046;&#23454;&#39564;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25216;&#26415;&#20844;&#21496;&#20013;&#65292;&#22312;&#32447;&#25511;&#21046;&#23454;&#39564;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#23454;&#29616;&#33258;&#20449;&#30340;&#20915;&#31574;&#12290;&#23450;&#20041;&#20102;&#19968;&#20010;&#21271;&#26497;&#24230;&#37327;&#26631;&#20934;&#65288;&#22914;&#38271;&#26399;&#25910;&#20837;&#25110;&#29992;&#25143;&#20445;&#30041;&#65289;&#65292;&#22312;A/B&#27979;&#35797;&#20013;&#65292;&#33021;&#22815;&#22312;&#36825;&#20010;&#25351;&#26631;&#19978;&#26377;&#32479;&#35745;&#26174;&#33879;&#25552;&#21319;&#30340;&#31995;&#32479;&#21464;&#20307;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20248;&#36234;&#30340;&#12290;&#28982;&#32780;&#65292;&#21271;&#26497;&#24230;&#37327;&#26631;&#20934;&#36890;&#24120;&#20855;&#26377;&#26102;&#24310;&#21644;&#19981;&#25935;&#24863;&#24615;&#12290;&#22240;&#27492;&#65292;&#23454;&#39564;&#30340;&#25104;&#26412;&#24456;&#39640;&#65306;&#23454;&#39564;&#38656;&#35201;&#38271;&#26102;&#38388;&#36816;&#34892;&#65292;&#21363;&#20351;&#22914;&#27492;&#65292;&#20108;&#31867;&#38169;&#35823;&#65288;&#21363;&#20551;&#38452;&#24615;&#65289;&#20173;&#28982;&#26222;&#36941;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#30701;&#26399;&#20449;&#21495;&#20013;&#23398;&#20064;&#25351;&#26631;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#25351;&#26631;&#30452;&#25509;&#26368;&#22823;&#21270;&#23427;&#20204;&#30456;&#23545;&#20110;&#21271;&#26497;&#24230;&#37327;&#26631;&#20934;&#25152;&#20855;&#26377;&#30340;&#32479;&#35745;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#23481;&#26131;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#65292;&#21363;&#26356;&#39640;&#30340;&#24179;&#22343;&#24230;&#37327;&#25935;&#24863;&#24615;&#24182;&#19981;&#24847;&#21619;&#30528;&#25913;&#36827;&#20102;&#20108;&#31867;&#38169;&#35823;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#26368;&#23567;&#21270;&#25351;&#26631;&#22312;&#36807;&#21435;&#23454;&#39564;&#30340;$log$&#19978;&#20135;&#29983;&#30340;$p$-value&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#20174;&#20004;&#20010;&#31038;&#20132;&#23186;&#20307;&#24212;&#29992;&#31243;&#24207;&#20013;&#25910;&#38598;&#20102;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online controlled experiments are a crucial tool to allow for confident decision-making in technology companies. A North Star metric is defined (such as long-term revenue or user retention), and system variants that statistically significantly improve on this metric in an A/B-test can be considered superior. North Star metrics are typically delayed and insensitive. As a result, the cost of experimentation is high: experiments need to run for a long time, and even then, type-II errors (i.e. false negatives) are prevalent.   We propose to tackle this by learning metrics from short-term signals that directly maximise the statistical power they harness with respect to the North Star. We show that existing approaches are prone to overfitting, in that higher average metric sensitivity does not imply improved type-II errors, and propose to instead minimise the $p$-values a metric would have produced on a log of past experiments. We collect such datasets from two social media applications with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20998;&#26512;&#21592;&#24037;&#27969;&#22833;&#24773;&#20917;&#65292;&#24182;&#37319;&#29992;AdaBoost&#65292;SVM&#21644;RandomForest&#19977;&#31181;&#31639;&#27861;&#36827;&#34892;&#21592;&#24037;&#31163;&#32844;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.03905</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#21592;&#24037;&#27969;&#22833;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Employee Turnover Analysis Using Machine Learning Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20998;&#26512;&#21592;&#24037;&#27969;&#22833;&#24773;&#20917;&#65292;&#24182;&#37319;&#29992;AdaBoost&#65292;SVM&#21644;RandomForest&#19977;&#31181;&#31639;&#27861;&#36827;&#34892;&#21592;&#24037;&#31163;&#32844;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21592;&#24037;&#30340;&#30693;&#35782;&#26159;&#32452;&#32455;&#30340;&#36164;&#20135;&#12290;&#27969;&#22833;&#21487;&#33021;&#20250;&#20135;&#29983;&#26126;&#26174;&#21644;&#38544;&#34255;&#30340;&#25104;&#26412;&#65292;&#24182;&#36896;&#25104;&#19981;&#21487;&#20462;&#22797;&#30340;&#25439;&#23475;&#12290;&#20026;&#20102;&#20811;&#26381;&#21644;&#20943;&#36731;&#36825;&#31181;&#39118;&#38505;&#65292;&#24212;&#35813;&#30417;&#27979;&#21592;&#24037;&#30340;&#29366;&#20917;&#12290;&#30001;&#20110;&#20998;&#26512;&#21592;&#24037;&#31119;&#21033;&#29305;&#24449;&#30340;&#22797;&#26434;&#24615;&#36739;&#39640;&#65292;&#21487;&#20197;&#23558;&#21592;&#24037;&#30340;&#31163;&#32844;&#39044;&#27979;&#20132;&#32473;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#21592;&#24037;&#30340;&#31163;&#32844;&#29575;&#12290;&#20351;&#29992;AdaBoost&#65292;SVM&#21644;RandomForest&#19977;&#31181;&#19981;&#21516;&#30340;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#26469;&#35780;&#20272;&#21592;&#24037;&#31163;&#32844;&#30340;&#20934;&#30830;&#24615;&#12290;&#24471;&#21040;&#30340;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#24314;&#31435;&#39044;&#27979;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Employee's knowledge is an organization asset. Turnover may impose apparent and hidden costs and irreparable damages. To overcome and mitigate this risk, employee's condition should be monitored. Due to high complexity of analyzing well-being features, employee's turnover predicting can be delegated to machine learning techniques. In this paper, we discuss employee's attrition rate. Three different supervised learning algorithms comprising AdaBoost, SVM and RandomForest are used to benchmark employee attrition accuracy. Attained models can help out at establishing predictive analytics.
&lt;/p&gt;</description></item><item><title>&#22797;&#21512;&#22238;&#25253;&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#38477;&#20302;&#26041;&#24046;&#21644;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#30340;&#36129;&#29486;&#21644;&#21019;&#26032;&#12290;</title><link>https://arxiv.org/abs/2402.03903</link><description>&lt;p&gt;
&#22797;&#21512;&#22238;&#25253;&#38477;&#20302;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
Compound Returns Reduce Variance in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03903
&lt;/p&gt;
&lt;p&gt;
&#22797;&#21512;&#22238;&#25253;&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#38477;&#20302;&#26041;&#24046;&#21644;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#30340;&#36129;&#29486;&#21644;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27493;&#22238;&#25253;&#65292;&#20363;&#22914;$n$&#27493;&#22238;&#25253;&#21644;$\lambda$&#22238;&#25253;&#65292;&#36890;&#24120;&#29992;&#20110;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#22810;&#27493;&#22238;&#25253;&#30340;&#26041;&#24046;&#25104;&#20026;&#20854;&#38271;&#24230;&#30340;&#38480;&#21046;&#22240;&#32032;&#65292;&#36807;&#24230;&#36828;&#26395;&#26410;&#26469;&#20250;&#22686;&#21152;&#26041;&#24046;&#24182;&#36870;&#36716;&#22810;&#27493;&#23398;&#20064;&#30340;&#22909;&#22788;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22797;&#21512;&#22238;&#25253;&#65288;$n$&#27493;&#22238;&#25253;&#30340;&#21152;&#26435;&#24179;&#22343;&#65289;&#38477;&#20302;&#26041;&#24046;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;&#20219;&#20309;&#19982;&#32473;&#23450;$n$&#27493;&#22238;&#25253;&#20855;&#26377;&#30456;&#21516;&#25910;&#32553;&#27169;&#25968;&#30340;&#22797;&#21512;&#22238;&#25253;&#30340;&#26041;&#24046;&#20005;&#26684;&#36739;&#20302;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36825;&#31181;&#38477;&#20302;&#26041;&#24046;&#30340;&#29305;&#24615;&#25913;&#21892;&#20102;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#19979;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#30340;&#26377;&#38480;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#30001;&#20110;&#19968;&#33324;&#22797;&#21512;&#22238;&#25253;&#30340;&#23454;&#26045;&#21487;&#33021;&#20195;&#20215;&#39640;&#26114;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#33258;&#21161;&#22238;&#25253;&#65292;&#23427;&#20204;&#22312;&#20445;&#25345;&#39640;&#25928;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#20102;&#26041;&#24046;&#65292;&#21363;&#20351;&#22312;&#20351;&#29992;&#23567;&#25209;&#37327;&#32463;&#39564;&#22238;&#25918;&#26102;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#26174;&#31034;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Multistep returns, such as $n$-step returns and $\lambda$-returns, are commonly used to improve the sample efficiency of reinforcement learning (RL) methods. The variance of the multistep returns becomes the limiting factor in their length; looking too far into the future increases variance and reverses the benefits of multistep learning. In our work, we demonstrate the ability of compound returns -- weighted averages of $n$-step returns -- to reduce variance. We prove for the first time that any compound return with the same contraction modulus as a given $n$-step return has strictly lower variance. We additionally prove that this variance-reduction property improves the finite-sample complexity of temporal-difference learning under linear function approximation. Because general compound returns can be expensive to implement, we introduce two-bootstrap returns which reduce variance while remaining efficient, even when using minibatched experience replay. We conduct experiments showing
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#28857;&#31215;&#27880;&#24847;&#21147;&#23618;&#22914;&#20309;&#21516;&#26102;&#23398;&#20064;&#20301;&#32622;&#21644;&#35821;&#20041;&#20851;&#27880;&#65292;&#21457;&#29616;&#22312;&#39640;&#32500;&#25968;&#25454;&#21644;&#22823;&#37327;&#35757;&#32451;&#26679;&#26412;&#26465;&#20214;&#19979;&#65292;&#23384;&#22312;&#20174;&#20301;&#32622;&#26426;&#21046;&#21040;&#35821;&#20041;&#26426;&#21046;&#30340;&#30456;&#21464;&#65292;&#24182;&#25552;&#20379;&#20102;&#38750;&#20984;&#32463;&#39564;&#25439;&#22833;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#38381;&#24335;&#34920;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.03902</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#27714;&#35299;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#27169;&#22411;&#20013;&#20301;&#32622;&#23398;&#20064;&#21644;&#35821;&#20041;&#23398;&#20064;&#20043;&#38388;&#30340;&#30456;&#21464;
&lt;/p&gt;
&lt;p&gt;
A phase transition between positional and semantic learning in a solvable model of dot-product attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03902
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#28857;&#31215;&#27880;&#24847;&#21147;&#23618;&#22914;&#20309;&#21516;&#26102;&#23398;&#20064;&#20301;&#32622;&#21644;&#35821;&#20041;&#20851;&#27880;&#65292;&#21457;&#29616;&#22312;&#39640;&#32500;&#25968;&#25454;&#21644;&#22823;&#37327;&#35757;&#32451;&#26679;&#26412;&#26465;&#20214;&#19979;&#65292;&#23384;&#22312;&#20174;&#20301;&#32622;&#26426;&#21046;&#21040;&#35821;&#20041;&#26426;&#21046;&#30340;&#30456;&#21464;&#65292;&#24182;&#25552;&#20379;&#20102;&#38750;&#20984;&#32463;&#39564;&#25439;&#22833;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#38381;&#24335;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28857;&#31215;&#27880;&#24847;&#21147;&#23618;&#22914;&#20309;&#23398;&#20064;&#20301;&#32622;&#27880;&#24847;&#21147;&#30697;&#38453;&#65288;&#36890;&#36807;&#21508;&#33258;&#30340;&#20301;&#32622;&#20915;&#23450;&#20196;&#29260;&#20043;&#38388;&#30340;&#20851;&#27880;&#65289;&#21644;&#35821;&#20041;&#27880;&#24847;&#21147;&#30697;&#38453;&#65288;&#36890;&#36807;&#24847;&#20041;&#20915;&#23450;&#20196;&#29260;&#20043;&#38388;&#30340;&#20851;&#27880;&#65289;&#12290;&#36890;&#36807;&#31639;&#27861;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21516;&#26679;&#31616;&#21333;&#30340;&#26550;&#26500;&#22914;&#20309;&#20351;&#29992;&#20301;&#32622;&#26426;&#21046;&#25110;&#35821;&#20041;&#26426;&#21046;&#26469;&#23454;&#29616;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#21487;&#35757;&#32451;&#30340;&#32465;&#23450;&#21644;&#20302;&#31209;&#26597;&#35810;&#21644;&#38190;&#30697;&#38453;&#30340;&#38750;&#32447;&#24615;&#33258;&#27880;&#24847;&#23618;&#30340;&#23398;&#20064;&#12290;&#22312;&#39640;&#32500;&#25968;&#25454;&#21644;&#30456;&#23545;&#36739;&#22823;&#25968;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#30340;&#28176;&#36817;&#26497;&#38480;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#38750;&#20984;&#32463;&#39564;&#25439;&#22833;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#38381;&#24335;&#34920;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#26368;&#23567;&#20540;&#23545;&#24212;&#20110;&#20301;&#32622;&#26426;&#21046;&#25110;&#35821;&#20041;&#26426;&#21046;&#65292;&#35777;&#26126;&#20102;&#38543;&#30528;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#20174;&#20301;&#32622;&#26426;&#21046;&#21521;&#35821;&#20041;&#26426;&#21046;&#30340;&#33258;&#21457;&#30456;&#21464;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#28857;&#31215;&#27880;&#24847;&#21147;&#19982;&#20854;&#20182;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate how a dot-product attention layer learns a positional attention matrix (with tokens attending to each other based on their respective positions) and a semantic attention matrix (with tokens attending to each other based on their meaning). For an algorithmic task, we experimentally show how the same simple architecture can learn to implement a solution using either the positional or semantic mechanism. On the theoretical side, we study the learning of a non-linear self-attention layer with trainable tied and low-rank query and key matrices. In the asymptotic limit of high-dimensional data and a comparably large number of training samples, we provide a closed-form characterization of the global minimum of the non-convex empirical loss landscape. We show that this minimum corresponds to either a positional or a semantic mechanism and evidence an emergent phase transition from the former to the latter with increasing sample complexity. Finally, we compare the dot-product att
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36890;&#29992;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#24341;&#20837;&#20102;&#25209;&#37327;&#36951;&#25022;&#30340;&#27010;&#24565;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#26080;&#35760;&#24518;&#28304;&#21644;&#19968;&#38454;&#39532;&#23572;&#21487;&#22827;&#28304;&#30340;&#24773;&#20917;&#19979;&#30340;&#28176;&#36817;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.03901</link><description>&lt;p&gt;
&#25209;&#22788;&#29702;&#36890;&#29992;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Batch Universal Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03901
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36890;&#29992;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#24341;&#20837;&#20102;&#25209;&#37327;&#36951;&#25022;&#30340;&#27010;&#24565;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#26080;&#35760;&#24518;&#28304;&#21644;&#19968;&#38454;&#39532;&#23572;&#21487;&#22827;&#28304;&#30340;&#24773;&#20917;&#19979;&#30340;&#28176;&#36817;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22240;&#20854;&#24778;&#20154;&#30340;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#33521;&#35821;&#21477;&#23376;&#30340;&#33021;&#21147;&#32780;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;LLMs&#26412;&#36136;&#19978;&#26159;&#39044;&#27979;&#22120;&#65292;&#36890;&#36807;&#20272;&#35745;&#32473;&#23450;&#36807;&#21435;&#30340;&#21333;&#35789;&#24207;&#21015;&#30340;&#27010;&#29575;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#20174;&#36890;&#29992;&#39044;&#27979;&#30340;&#35282;&#24230;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#26159;&#33258;&#28982;&#32780;&#28982;&#30340;&#12290;&#20026;&#20102;&#20844;&#24179;&#22320;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25209;&#37327;&#36951;&#25022;&#30340;&#27010;&#24565;&#20316;&#20026;&#32463;&#20856;&#24179;&#22343;&#36951;&#25022;&#30340;&#20462;&#25913;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#26080;&#35760;&#24518;&#28304;&#21644;&#19968;&#38454;&#39532;&#23572;&#21487;&#22827;&#28304;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#22686;&#21152;&#24120;&#25968;&#39044;&#27979;&#22120;&#30340;&#28176;&#36817;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently gained much popularity due to their surprising ability at generating human-like English sentences. LLMs are essentially predictors, estimating the probability of a sequence of words given the past. Therefore, it is natural to evaluate their performance from a universal prediction perspective. In order to do that fairly, we introduce the notion of batch regret as a modification of the classical average regret, and we study its asymptotical value for add-constant predictors, in the case of memoryless sources and first-order Markov sources.
&lt;/p&gt;</description></item><item><title>DistiLLM&#26159;&#19968;&#20010;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#20559;&#26012;Kullback-Leibler&#25955;&#24230;&#25439;&#22833;&#21644;&#33258;&#36866;&#24212;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#32570;&#20047;&#26631;&#20934;&#21270;&#30446;&#26631;&#20989;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03898</link><description>&lt;p&gt;
DistiLLM: &#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DistiLLM: Towards Streamlined Distillation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03898
&lt;/p&gt;
&lt;p&gt;
DistiLLM&#26159;&#19968;&#20010;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#20559;&#26012;Kullback-Leibler&#25955;&#24230;&#25439;&#22833;&#21644;&#33258;&#36866;&#24212;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#32570;&#20047;&#26631;&#20934;&#21270;&#30446;&#26631;&#20989;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#23558;&#25945;&#24072;&#27169;&#22411;&#21387;&#32553;&#20026;&#26356;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#38024;&#23545;&#33258;&#22238;&#24402;&#24207;&#21015;&#27169;&#22411;&#65288;&#20363;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;KD&#26041;&#27861;&#23384;&#22312;&#32570;&#20047;&#26631;&#20934;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#20351;&#29992;&#23398;&#29983;&#29983;&#25104;&#30340;&#36755;&#20986;&#26469;&#35299;&#20915;&#35757;&#32451;-&#25512;&#29702;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#20570;&#27861;&#26174;&#33879;&#22686;&#21152;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DistiLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#26694;&#26550;&#12290;DistiLLM&#30001;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#65288;1&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#26012;Kullback-Leibler&#25955;&#24230;&#25439;&#22833;&#65292;&#25105;&#20204;&#25581;&#31034;&#24182;&#21033;&#29992;&#20102;&#23427;&#30340;&#29702;&#35770;&#23646;&#24615;&#65307;&#65288;2&#65289;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#21033;&#29992;&#23398;&#29983;&#29983;&#25104;&#30340;&#36755;&#20986;&#30340;&#25928;&#29575;&#12290;&#21253;&#25324;&#25351;&#20196;&#36319;&#38543;&#20219;&#21153;&#22312;&#20869;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;DistiLLM&#22312;&#26500;&#24314;&#39640;&#24615;&#33021;&#27169;&#22411;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) is widely used for compressing a teacher model to a smaller student model, reducing its inference cost and memory footprint while preserving model capabilities. However, current KD methods for auto-regressive sequence models (e.g., large language models) suffer from missing a standardized objective function. Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs. To tackle these issues, we introduce DistiLLM, a more effective and efficient KD framework for auto-regressive language models. DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties, and (2) an adaptive off-policy approach designed to enhance the efficiency in utilizing student-generated outputs. Extensive experiments, including instruction-following tasks, demonstrate the effectiveness of DistiLLM in building high-performing 
&lt;/p&gt;</description></item><item><title>&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#39044;&#27979;&#26102;&#22495;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#26681;&#25454;&#20854;&#29305;&#23450;&#38656;&#27714;&#26469;&#30830;&#23450;&#26368;&#20339;&#39044;&#27979;&#26102;&#22495;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.03893</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#30340;&#39044;&#27979;&#26102;&#22495;&#38656;&#27714;&#65306;&#20248;&#21270;&#23433;&#20840;&#12289;&#33298;&#36866;&#21644;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Prediction Horizon Requirements for Automated Driving: Optimizing Safety, Comfort, and Efficiency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03893
&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#39044;&#27979;&#26102;&#22495;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#26681;&#25454;&#20854;&#29305;&#23450;&#38656;&#27714;&#26469;&#30830;&#23450;&#26368;&#20339;&#39044;&#27979;&#26102;&#22495;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#20854;&#20182;&#36947;&#36335;&#20351;&#29992;&#32773;&#30340;&#31227;&#21160;&#23545;&#20110;&#25913;&#21892;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;(AV)&#30340;&#24615;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#19982;AV&#24615;&#33021;&#30456;&#20851;&#30340;&#26102;&#38388;&#33539;&#22260;&#30340;&#20851;&#31995;&#20173;&#19981;&#28165;&#26970;&#12290;&#23613;&#31649;&#23384;&#22312;&#22823;&#37327;&#30340;&#36712;&#36857;&#39044;&#27979;&#31639;&#27861;&#65292;&#20294;&#36824;&#27809;&#26377;&#30740;&#31350;&#25506;&#35752;&#19981;&#21516;&#39044;&#27979;&#38271;&#24230;&#22914;&#20309;&#24433;&#21709;AV&#23433;&#20840;&#21644;&#20854;&#20182;&#36710;&#36742;&#24615;&#33021;&#25351;&#26631;&#65292;&#23548;&#33268;&#39044;&#27979;&#26041;&#27861;&#30340;&#39044;&#27979;&#26102;&#22495;&#38656;&#27714;&#26410;&#23450;&#20041;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#22522;&#20110;&#39118;&#38505;&#30340;&#39044;&#27979;&#36712;&#36857;&#35268;&#21010;&#22120;&#36827;&#34892;&#22810;&#27425;&#23454;&#39564;&#65292;&#27169;&#25311;&#20102;&#38271;&#36798;20&#31186;&#30340;&#39044;&#27979;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#27169;&#25311;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#26681;&#25454;&#29305;&#23450;AV&#24615;&#33021;&#26631;&#20934;&#21644;&#24212;&#29992;&#38656;&#27714;&#25351;&#23450;&#26368;&#20302;&#35201;&#27714;&#21644;&#26368;&#20339;&#39044;&#27979;&#26102;&#22495;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Predicting the movement of other road users is beneficial for improving automated vehicle (AV) performance. However, the relationship between the time horizon associated with these predictions and AV performance remains unclear. Despite the existence of numerous trajectory prediction algorithms, no studies have been conducted on how varying prediction lengths affect AV safety and other vehicle performance metrics, resulting in undefined horizon requirements for prediction methods. Our study addresses this gap by examining the effects of different prediction horizons on AV performance, focusing on safety, comfort, and efficiency. Through multiple experiments using a state-of-the-art, risk-based predictive trajectory planner, we simulated predictions with horizons up to 20 seconds. Based on our simulations, we propose a framework for specifying the minimum required and optimal prediction horizons based on specific AV performance criteria and application needs. Our results indicate that a
&lt;/p&gt;</description></item><item><title>MOMENT&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;&#65292;&#36890;&#36807;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#32534;&#21046;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#26377;&#38480;&#30417;&#30563;&#22330;&#26223;&#19979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03885</link><description>&lt;p&gt;
MOMENT&#65306;&#19968;&#20010;&#24320;&#25918;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;
&lt;/p&gt;
&lt;p&gt;
MOMENT: A Family of Open Time-series Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03885
&lt;/p&gt;
&lt;p&gt;
MOMENT&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;&#65292;&#36890;&#36807;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#32534;&#21046;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#26377;&#38480;&#30417;&#30563;&#22330;&#26223;&#19979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;MOMENT&#65292;&#19968;&#20010;&#24320;&#28304;&#30340;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;&#12290;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#22823;&#27169;&#22411;&#26041;&#38754;&#23384;&#22312;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#65306;&#65288;1&#65289;&#32570;&#20047;&#19968;&#20010;&#22823;&#32780;&#26377;&#20957;&#32858;&#21147;&#30340;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#23384;&#20648;&#24211;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22810;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#20351;&#24471;&#22810;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#21464;&#24471;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#39564;&#35780;&#20272;&#26631;&#20934;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#12289;&#26102;&#38388;&#21644;&#30417;&#30563;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#65288;3&#65289;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#22823;&#32780;&#22810;&#26679;&#30340;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;&#26102;&#38388;&#24207;&#21015;&#22534;&#65292;&#20197;&#31995;&#32479;&#22320;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#29305;&#23450;&#30340;&#25361;&#25112;&#65292;&#20197;&#35299;&#38145;&#22823;&#35268;&#27169;&#30340;&#22810;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20511;&#37492;&#26368;&#36817;&#30340;&#24037;&#20316;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#26377;&#38480;&#30417;&#30563;&#22330;&#26223;&#19979;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MOMENT, a family of open-source foundation models for general-purpose time-series analysis. Pre-training large models on time-series data is challenging due to (1) the absence of a large and cohesive public time-series repository, and (2) diverse time-series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time-series, called the Time-series Pile, and systematically tackle time-series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time-series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#35299;&#20915;&#32422;&#26463;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#36229;&#26799;&#24230;&#20272;&#35745;&#31574;&#30053;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#36866;&#29992;&#20110;&#30830;&#23450;&#24615;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#36824;&#36866;&#29992;&#20110;&#38543;&#26426;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#19968;&#33324;&#30340;&#22238;&#36864;&#12290;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65292;&#35813;&#26694;&#26550;&#37117;&#20855;&#26377;&#24456;&#39640;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03883</link><description>&lt;p&gt;
&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#36827;&#34892;&#21452;&#23618;&#20248;&#21270;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Bilevel Optimization on Riemannian Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#35299;&#20915;&#32422;&#26463;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#36229;&#26799;&#24230;&#20272;&#35745;&#31574;&#30053;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#36866;&#29992;&#20110;&#30830;&#23450;&#24615;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#36824;&#36866;&#29992;&#20110;&#38543;&#26426;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#19968;&#33324;&#30340;&#22238;&#36864;&#12290;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65292;&#35813;&#26694;&#26550;&#37117;&#20855;&#26377;&#24456;&#39640;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#32422;&#26463;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#21464;&#37327;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#31181;&#22312;&#27969;&#24418;&#19978;&#30340;&#36229;&#26799;&#24230;&#20272;&#35745;&#31574;&#30053;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;&#25105;&#20204;&#23545;&#27969;&#24418;&#19978;&#30340;&#36229;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#21644;&#22797;&#26434;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#23558;&#36825;&#20123;&#30740;&#31350;&#25193;&#23637;&#21040;&#38543;&#26426;&#21452;&#23618;&#20248;&#21270;&#21644;&#20351;&#29992;&#19968;&#33324;&#30340;&#22238;&#36864;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization has seen an increasing presence in various domains of applications. In this work, we propose a framework for solving bilevel optimization problems where variables of both lower and upper level problems are constrained on Riemannian manifolds. We provide several hypergradient estimation strategies on manifolds and study their estimation error. We provide convergence and complexity analysis for the proposed hypergradient descent algorithm on manifolds. We also extend the developments to stochastic bilevel optimization and to the use of general retraction. We showcase the utility of the proposed framework on various applications.
&lt;/p&gt;</description></item><item><title>&#20960;&#20309;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#20855;&#26377;&#32463;&#20856;&#27169;&#25311;&#22120;&#26080;&#27861;&#36798;&#21040;&#30340;&#25351;&#25968;&#24046;&#24322;&#30340;&#21327;&#35758;&#65292;&#24182;&#19988;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;Simon&#31639;&#27861;&#21457;&#29616;&#20102;&#19968;&#20010;&#20363;&#23376;&#65292;&#36825;&#26159;BQP$^A\neq$BPP&#21327;&#35758;&#30340;&#31034;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.03871</link><description>&lt;p&gt;
&#20960;&#20309;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;BQP$^A$&#21327;&#35758;&#21644;&#28508;&#22312;&#22270;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Geometric quantum machine learning of BQP$^A$ protocols and latent graph classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03871
&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#20855;&#26377;&#32463;&#20856;&#27169;&#25311;&#22120;&#26080;&#27861;&#36798;&#21040;&#30340;&#25351;&#25968;&#24046;&#24322;&#30340;&#21327;&#35758;&#65292;&#24182;&#19988;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;Simon&#31639;&#27861;&#21457;&#29616;&#20102;&#19968;&#20010;&#20363;&#23376;&#65292;&#36825;&#26159;BQP$^A\neq$BPP&#21327;&#35758;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;GQML&#65289;&#26088;&#22312;&#23884;&#20837;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#65292;&#20197;&#23398;&#20064;&#39640;&#25928;&#30340;&#27714;&#35299;&#21327;&#35758;&#12290;&#28982;&#32780;&#65292;&#20851;&#38190;&#38382;&#39064;&#26159;&#26159;&#21542;&#21487;&#20197;&#23558;(G)QML&#24120;&#35268;&#22320;&#29992;&#20110;&#26500;&#24314;&#19982;&#32463;&#20856;&#27169;&#25311;&#22120;&#30456;&#27604;&#30340;&#25351;&#25968;&#24046;&#24322;&#30340;&#21327;&#35758;&#12290;&#26412;&#25991;&#32771;&#34385;&#29992;&#20110;&#23398;&#20064;&#24067;&#23572;&#20989;&#25968;&#23646;&#24615;&#30340;Simon&#38382;&#39064;&#65292;&#24182;&#26174;&#31034;&#36825;&#21487;&#20197;&#19982;&#38750;&#30417;&#30563;&#30005;&#36335;&#20998;&#31867;&#38382;&#39064;&#30456;&#20851;&#32852;&#12290;&#20351;&#29992;&#20960;&#20309;QML&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#25105;&#20204;&#20174;&#22522;&#26412;&#21407;&#29702;&#23398;&#20064;Simon&#31639;&#27861;&#65292;&#20174;&#32780;&#21457;&#29616;&#20102;&#20851;&#20110;&#26576;&#20010;&#25968;&#25454;&#38598;&#65288;oracle $A$&#65289;&#30340;BQP$^A\neq$BPP&#21327;&#35758;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21457;&#29616;&#21253;&#25324;&#24320;&#21457;&#20986;&#29992;&#20110;&#23884;&#20837;&#24067;&#23572;&#20989;&#25968;&#30340;&#31561;&#21464;&#29305;&#24449;&#26144;&#23556;&#65292;&#22522;&#20110;&#35782;&#21035;&#30340;&#20301;&#32763;&#36716;&#21644;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#25197;&#36716;&#65292;&#20197;&#21450;&#22522;&#20110;&#19981;&#21464;&#37327;&#35266;&#27979;&#37327;&#30340;&#27979;&#37327;&#65292;&#20855;&#26377;&#37319;&#26679;&#20248;&#21183;&#12290;&#25152;&#25552;&#20986;&#30340;&#24037;&#20316;&#27969;&#31243;&#25351;&#21521;&#20102;&#25968;&#25454;&#23884;&#20837;&#21644;&#32463;&#20856;&#21518;&#22788;&#29702;&#30340;&#37325;&#35201;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geometric quantum machine learning (GQML) aims to embed problem symmetries for learning efficient solving protocols. However, the question remains if (G)QML can be routinely used for constructing protocols with an exponential separation from classical analogs. In this Letter we consider Simon's problem for learning properties of Boolean functions, and show that this can be related to an unsupervised circuit classification problem. Using the workflow of geometric QML, we learn from first principles Simon's algorithm, thus discovering an example of BQP$^A\neq$BPP protocol with respect to some dataset (oracle $A$). Our key findings include the development of an equivariant feature map for embedding Boolean functions, based on twirling with respect to identified bitflip and permutational symmetries, and measurement based on invariant observables with a sampling advantage. The proposed workflow points to the importance of data embeddings and classical post-processing, while keeping the vari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#27714;&#35299;&#20013;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;NTK&#22312;&#19981;&#21516;&#32447;&#24615;&#24494;&#20998;&#31639;&#23376;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#24378;&#35843;&#20102;&#37319;&#29992;&#20108;&#38454;&#26041;&#27861;&#35757;&#32451;PINNs&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20108;&#38454;&#26041;&#27861;&#30340;&#25910;&#25947;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#35889;&#20559;&#24046;&#21644;&#25910;&#25947;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#21644;&#22522;&#20934;&#27979;&#35797;&#29992;&#20363;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03864</link><description>&lt;p&gt;
&#38024;&#23545;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#38454;&#27573;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The Challenges of the Nonlinear Regime for Physics-Informed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#27714;&#35299;&#20013;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;NTK&#22312;&#19981;&#21516;&#32447;&#24615;&#24494;&#20998;&#31639;&#23376;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#24378;&#35843;&#20102;&#37319;&#29992;&#20108;&#38454;&#26041;&#27861;&#35757;&#32451;PINNs&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20108;&#38454;&#26041;&#27861;&#30340;&#25910;&#25947;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#35889;&#20559;&#24046;&#21644;&#25910;&#25947;&#32531;&#24930;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#21644;&#22522;&#20934;&#27979;&#35797;&#29992;&#20363;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#35270;&#35282;&#26159;&#30740;&#31350;&#26080;&#38480;&#23485;&#24230;&#24773;&#20917;&#19979;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#35757;&#32451;&#21160;&#24577;&#30340;&#26377;&#20215;&#20540;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#35270;&#35282;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;PINNs&#27714;&#35299;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;NTK&#22312;&#19981;&#21516;&#32447;&#24615;&#24494;&#20998;&#31639;&#23376;&#19979;&#30340;&#19981;&#21516;&#34892;&#20026;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#26681;&#25454;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#24378;&#35843;&#37319;&#29992;&#20108;&#38454;&#26041;&#27861;&#35757;&#32451;PINNs&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20108;&#38454;&#26041;&#27861;&#30340;&#25910;&#25947;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#35889;&#20559;&#24046;&#21644;&#25910;&#25947;&#32531;&#24930;&#30340;&#25361;&#25112;&#12290;&#25152;&#26377;&#30340;&#29702;&#35770;&#32467;&#26524;&#37117;&#24471;&#21040;&#20102;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;PDEs&#30340;&#25968;&#20540;&#31034;&#20363;&#30340;&#25903;&#25345;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#29992;&#20363;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Neural Tangent Kernel (NTK) viewpoint represents a valuable approach to examine the training dynamics of Physics-Informed Neural Networks (PINNs) in the infinite width limit. We leverage this perspective and focus on the case of nonlinear Partial Differential Equations (PDEs) solved by PINNs. We provide theoretical results on the different behaviors of the NTK depending on the linearity of the differential operator. Moreover, inspired by our theoretical results, we emphasize the advantage of employing second-order methods for training PINNs. Additionally, we explore the convergence capabilities of second-order methods and address the challenges of spectral bias and slow convergence. Every theoretical result is supported by numerical examples with both linear and nonlinear PDEs, and we validate our training method on benchmark test cases.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;AI&#27169;&#22411;&#30340;&#30830;&#20999;&#31639;&#27861;&#65292;&#26426;&#21046;&#35299;&#37322;&#24615;&#65288;MI&#65289;&#26088;&#22312;&#29702;&#35299;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#24494;&#19981;&#36275;&#36947;&#30340;&#34892;&#20026;&#21644;&#33021;&#21147;&#65292;&#32780;&#24573;&#35270;&#20102;&#38544;&#34255;&#22312;&#32593;&#32476;&#20869;&#37096;&#30340;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21628;&#21505;&#30740;&#31350;&#30028;&#26397;&#30528;&#26032;&#30340;&#26694;&#26550;&#21162;&#21147;&#65292;&#30740;&#31350;&#36825;&#20123;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.03855</link><description>&lt;p&gt;
&#12298;&#23450;&#20301;&#35770;&#25991;&#65306;&#25506;&#32034;&#30740;&#31350;&#27169;&#22411;&#34920;&#31034;&#30340;&#26032;&#26694;&#26550;&#12299;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Toward New Frameworks for Studying Model Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03855
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;AI&#27169;&#22411;&#30340;&#30830;&#20999;&#31639;&#27861;&#65292;&#26426;&#21046;&#35299;&#37322;&#24615;&#65288;MI&#65289;&#26088;&#22312;&#29702;&#35299;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#24494;&#19981;&#36275;&#36947;&#30340;&#34892;&#20026;&#21644;&#33021;&#21147;&#65292;&#32780;&#24573;&#35270;&#20102;&#38544;&#34255;&#22312;&#32593;&#32476;&#20869;&#37096;&#30340;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21628;&#21505;&#30740;&#31350;&#30028;&#26397;&#30528;&#26032;&#30340;&#26694;&#26550;&#21162;&#21147;&#65292;&#30740;&#31350;&#36825;&#20123;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#21046;&#35299;&#37322;&#24615;&#65288;MI&#65289;&#26088;&#22312;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;AI&#27169;&#22411;&#23398;&#20064;&#30340;&#30830;&#20999;&#31639;&#27861;&#26469;&#29702;&#35299;&#27169;&#22411;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;MI&#30740;&#31350;&#30340;&#34892;&#20026;&#21644;&#33021;&#21147;&#37117;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#21644;&#31526;&#21495;&#23545;&#40784;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#33021;&#21147;&#24182;&#19981;&#37027;&#20040;&#24494;&#19981;&#36275;&#36947;&#65292;&#36825;&#20026;&#30740;&#31350;&#32593;&#32476;&#20869;&#37096;&#30340;&#38544;&#34255;&#34920;&#31034;&#20316;&#20026;&#20998;&#26512;&#21333;&#20301;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25991;&#29486;&#22238;&#39038;&#65292;&#23545;&#29305;&#24449;&#21644;&#34892;&#20026;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#30340;&#34920;&#31034;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#21644;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20123;&#22522;&#26412;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#36890;&#36807;&#35752;&#35770;&#21644;&#25506;&#32034;&#24615;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30740;&#31350;&#34920;&#31034;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#39046;&#22495;&#65292;&#24182;&#19988;&#30446;&#21069;MI&#20013;&#24314;&#31435;&#30340;&#26041;&#27861;&#19981;&#36275;&#20197;&#29702;&#35299;&#34920;&#31034;&#65292;&#22240;&#27492;&#25512;&#21160;&#30740;&#31350;&#30028;&#26397;&#30528;&#30740;&#31350;&#34920;&#31034;&#30340;&#26032;&#26694;&#26550;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mechanistic interpretability (MI) aims to understand AI models by reverse-engineering the exact algorithms neural networks learn. Most works in MI so far have studied behaviors and capabilities that are trivial and token-aligned. However, most capabilities are not that trivial, which advocates for the study of hidden representations inside these networks as the unit of analysis. We do a literature review, formalize representations for features and behaviors, highlight their importance and evaluation, and perform some basic exploration in the mechanistic interpretability of representations. With discussion and exploratory results, we justify our position that studying representations is an important and under-studied field, and that currently established methods in MI are not sufficient to understand representations, thus pushing for the research community to work toward new frameworks for studying representations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24322;&#24120;&#28857;&#29983;&#25104;&#26041;&#27861;BISECT&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#21487;&#20197;&#21019;&#24314;&#20855;&#26377;&#30495;&#23454;&#34892;&#20026;&#30340;&#24322;&#24120;&#28857;&#12290;&#20351;&#29992;BISECT&#29983;&#25104;&#30340;&#21512;&#25104;&#24322;&#24120;&#28857;&#21487;&#26377;&#25928;&#22686;&#24378;&#22810;&#31181;&#25968;&#25454;&#38598;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#20363;&#22914;&#65292;&#22312;&#19982;&#22522;&#32447;&#27604;&#36739;&#26102;&#65292;&#20351;&#29992;BISECT&#36827;&#34892;&#36807;&#37319;&#26679;&#23558;&#38169;&#35823;&#29575;&#38477;&#20302;&#20102;&#26368;&#22810;3&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.03846</link><description>&lt;p&gt;
&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#30340;&#39640;&#25928;&#38544;&#34255;&#24322;&#24120;&#28857;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Generation of Hidden Outliers for Improved Outlier Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03846
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24322;&#24120;&#28857;&#29983;&#25104;&#26041;&#27861;BISECT&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#65292;&#21487;&#20197;&#21019;&#24314;&#20855;&#26377;&#30495;&#23454;&#34892;&#20026;&#30340;&#24322;&#24120;&#28857;&#12290;&#20351;&#29992;BISECT&#29983;&#25104;&#30340;&#21512;&#25104;&#24322;&#24120;&#28857;&#21487;&#26377;&#25928;&#22686;&#24378;&#22810;&#31181;&#25968;&#25454;&#38598;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#20363;&#22914;&#65292;&#22312;&#19982;&#22522;&#32447;&#27604;&#36739;&#26102;&#65292;&#20351;&#29992;BISECT&#36827;&#34892;&#36807;&#37319;&#26679;&#23558;&#38169;&#35823;&#29575;&#38477;&#20302;&#20102;&#26368;&#22810;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#28857;&#29983;&#25104;&#26159;&#35299;&#20915;&#37325;&#35201;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#30340;&#24120;&#29992;&#25216;&#26415;&#12290;&#29983;&#25104;&#20855;&#26377;&#30495;&#23454;&#34892;&#20026;&#30340;&#24322;&#24120;&#28857;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#27969;&#34892;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#39640;&#32500;&#31354;&#38388;&#20013;&#24322;&#24120;&#28857;&#30340;&#8220;&#22810;&#35270;&#22270;&#8221;&#23646;&#24615;&#12290;&#21807;&#19968;&#32771;&#34385;&#21040;&#27492;&#23646;&#24615;&#30340;&#29616;&#26377;&#26041;&#27861;&#22312;&#25928;&#29575;&#21644;&#25928;&#26524;&#19978;&#23384;&#22312;&#19981;&#36275;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BISECT&#30340;&#26032;&#30340;&#24322;&#24120;&#28857;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21019;&#24314;&#20855;&#26377;&#30495;&#23454;&#34892;&#20026;&#19988;&#27169;&#20223;&#35813;&#23646;&#24615;&#30340;&#24322;&#24120;&#28857;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;BISECT&#37319;&#29992;&#20102;&#26412;&#25991;&#20171;&#32461;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#21629;&#39064;&#65292;&#35828;&#26126;&#22914;&#20309;&#39640;&#25928;&#22320;&#29983;&#25104;&#36825;&#20123;&#30495;&#23454;&#24322;&#24120;&#28857;&#12290;&#19982;&#24403;&#21069;&#37325;&#26032;&#21019;&#24314;&#8220;&#22810;&#35270;&#22270;&#8221;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#20445;&#35777;&#21644;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;BISECT&#29983;&#25104;&#30340;&#21512;&#25104;&#24322;&#24120;&#28857;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#21508;&#31181;&#25968;&#25454;&#38598;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#29992;&#20363;&#12290;&#20363;&#22914;&#65292;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#20351;&#29992;BISECT&#36827;&#34892;&#36807;&#37319;&#26679;&#23558;&#38169;&#35823;&#29575;&#38477;&#20302;&#20102;&#26368;&#22810;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Outlier generation is a popular technique used for solving important outlier detection tasks. Generating outliers with realistic behavior is challenging. Popular existing methods tend to disregard the 'multiple views' property of outliers in high-dimensional spaces. The only existing method accounting for this property falls short in efficiency and effectiveness. We propose BISECT, a new outlier generation method that creates realistic outliers mimicking said property. To do so, BISECT employs a novel proposition introduced in this article stating how to efficiently generate said realistic outliers. Our method has better guarantees and complexity than the current methodology for recreating 'multiple views'. We use the synthetic outliers generated by BISECT to effectively enhance outlier detection in diverse datasets, for multiple use cases. For instance, oversampling with BISECT reduced the error by up to 3 times when compared with the baselines.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#28041;&#21450;&#21040;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#35268;&#33539;&#33258;&#30001;&#24230;&#12289;&#20445;&#23432;&#24615;&#21644;&#20869;&#22312;&#32500;&#24230;&#20272;&#35745;&#12290;&#29616;&#26377;&#30740;&#31350;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24120;&#23558;&#21521;&#37327;&#22330;&#23454;&#29616;&#20026;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#65292;&#32780;&#19981;&#26159;&#38480;&#21046;&#20026;&#33021;&#37327;&#20989;&#25968;&#30340;&#26799;&#24230;&#65292;&#23548;&#33268;&#20851;&#20110;&#27492;&#32422;&#26463;&#26159;&#21542;&#25552;&#39640;&#24615;&#33021;&#30340;&#30740;&#31350;&#32467;&#26524;&#30683;&#30462;&#12290;</title><link>https://arxiv.org/abs/2402.03845</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#35268;&#33539;&#33258;&#30001;&#24230;&#12289;&#20445;&#23432;&#24615;&#21644;&#20869;&#22312;&#32500;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
On gauge freedom, conservativity and intrinsic dimensionality estimation in diffusion models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03845
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#28041;&#21450;&#21040;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#35268;&#33539;&#33258;&#30001;&#24230;&#12289;&#20445;&#23432;&#24615;&#21644;&#20869;&#22312;&#32500;&#24230;&#20272;&#35745;&#12290;&#29616;&#26377;&#30740;&#31350;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24120;&#23558;&#21521;&#37327;&#22330;&#23454;&#29616;&#20026;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#65292;&#32780;&#19981;&#26159;&#38480;&#21046;&#20026;&#33021;&#37327;&#20989;&#25968;&#30340;&#26799;&#24230;&#65292;&#23548;&#33268;&#20851;&#20110;&#27492;&#32422;&#26463;&#26159;&#21542;&#25552;&#39640;&#24615;&#33021;&#30340;&#30740;&#31350;&#32467;&#26524;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#29983;&#25104;&#27169;&#22411;&#65292;&#26368;&#36817;&#22312;&#39640;&#32500;&#24230;&#30340;&#37319;&#26679;&#36136;&#37327;&#21644;&#23494;&#24230;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#23427;&#20204;&#20381;&#36182;&#20110;&#19968;&#20010;&#27491;&#21521;&#36830;&#32493;&#25193;&#25955;&#36807;&#31243;&#21644;&#19968;&#20010;&#21453;&#21521;&#36830;&#32493;&#21435;&#22122;&#36807;&#31243;&#65292;&#21487;&#20197;&#29992;&#19968;&#20010;&#26102;&#21464;&#21521;&#37327;&#22330;&#26469;&#25551;&#36848;&#65292;&#24182;&#29992;&#20316;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#21407;&#22987;&#23450;&#20041;&#20013;&#65292;&#35813;&#21521;&#37327;&#22330;&#34987;&#20551;&#35774;&#20026;&#20998;&#25968;&#20989;&#25968;&#65288;&#21363;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#26576;&#19968;&#29305;&#23450;&#26102;&#38388;&#30340;&#27010;&#29575;&#23545;&#25968;&#30340;&#26799;&#24230;&#65289;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#23558;&#36825;&#20010;&#21521;&#37327;&#22330;&#23454;&#29616;&#20026;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#65292;&#24182;&#19988;&#19981;&#38480;&#21046;&#20854;&#20026;&#26576;&#31181;&#33021;&#37327;&#20989;&#25968;&#30340;&#26799;&#24230;&#65288;&#20063;&#23601;&#26159;&#35828;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#24182;&#19981;&#32422;&#26463;&#21521;&#37327;&#22330;&#26159;&#20445;&#23432;&#30340;&#65289;&#12290;&#23613;&#31649;&#19968;&#20123;&#30740;&#31350;&#20174;&#23454;&#35777;&#35282;&#24230;&#32771;&#23519;&#36825;&#31181;&#32422;&#26463;&#26159;&#21542;&#20250;&#24102;&#26469;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#20294;&#23427;&#20204;&#24471;&#20986;&#20102;&#30683;&#30462;&#30340;&#32467;&#26524;&#65292;&#24182;&#26410;&#25552;&#20379;&#20998;&#26512;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are generative models that have recently demonstrated impressive performances in terms of sampling quality and density estimation in high dimensions. They rely on a forward continuous diffusion process and a backward continuous denoising process, which can be described by a time-dependent vector field and is used as a generative model. In the original formulation of the diffusion model, this vector field is assumed to be the score function (i.e. it is the gradient of the log-probability at a given time in the diffusion process). Curiously, on the practical side, most studies on diffusion models implement this vector field as a neural network function and do not constrain it be the gradient of some energy function (that is, most studies do not constrain the vector field to be conservative). Even though some studies investigated empirically whether such a constraint will lead to a performance gain, they lead to contradicting results and failed to provide analytical resul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#20999;&#29255;Wasserstein Weisfeiler-Lehman&#22270;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#31232;&#30095;&#22270;&#24418;&#25968;&#25454;&#38598;&#26102;&#20855;&#26377;&#27491;&#23450;&#24615;&#21644;&#26174;&#33879;&#30340;&#22797;&#26434;&#24230;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.03838</link><description>&lt;p&gt;
&#24102;&#26377;&#20999;&#29255;Wasserstein Weisfeiler-Lehman&#22270;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Gaussian process regression with Sliced Wasserstein Weisfeiler-Lehman graph kernels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#20999;&#29255;Wasserstein Weisfeiler-Lehman&#22270;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#31232;&#30095;&#22270;&#24418;&#25968;&#25454;&#38598;&#26102;&#20855;&#26377;&#27491;&#23450;&#24615;&#21644;&#26174;&#33879;&#30340;&#22797;&#26434;&#24230;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#22312;&#35745;&#31639;&#29289;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#21462;&#22797;&#26434;&#27169;&#24335;&#65292;&#29992;&#20110;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#25110;&#39044;&#27979;&#26448;&#26009;&#24615;&#36136;&#31561;&#20219;&#21153;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#31867;&#25968;&#25454;&#38598;&#30001;&#20855;&#26377;&#22823;&#37327;&#33410;&#28857;&#30340;&#32593;&#26684;&#34920;&#31034;&#30340;&#36755;&#20837;&#65288;&#35270;&#20026;&#22270;&#24418;&#65289;&#21644;&#20351;&#29992;&#25968;&#20540;&#27714;&#35299;&#22120;&#33719;&#24471;&#30340;&#30456;&#24212;&#36755;&#20986;&#32452;&#25104;&#12290;&#36825;&#24847;&#21619;&#30528;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#24517;&#39035;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#36830;&#32493;&#33410;&#28857;&#23646;&#24615;&#30340;&#22823;&#35268;&#27169;&#31232;&#30095;&#22270;&#24418;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65292;&#24341;&#20837;&#20102;&#20999;&#29255;Wasserstein Weisfeiler-Lehman&#65288;SWWL&#65289;&#22270;&#26680;&#12290;&#19982;&#29616;&#26377;&#30340;&#22270;&#26680;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;SWWL&#26680;&#20855;&#26377;&#27491;&#23450;&#24615;&#21644;&#26174;&#33879;&#30340;&#22797;&#26434;&#24230;&#38477;&#20302;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#27492;&#21069;&#19981;&#21487;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;&#12290;&#26032;&#30340;&#26680;&#39318;&#20808;&#22312;&#20998;&#23376;&#22270;&#20998;&#31867;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised learning has recently garnered significant attention in the field of computational physics due to its ability to effectively extract complex patterns for tasks like solving partial differential equations, or predicting material properties. Traditionally, such datasets consist of inputs given as meshes with a large number of nodes representing the problem geometry (seen as graphs), and corresponding outputs obtained with a numerical solver. This means the supervised learning model must be able to handle large and sparse graphs with continuous node attributes. In this work, we focus on Gaussian process regression, for which we introduce the Sliced Wasserstein Weisfeiler-Lehman (SWWL) graph kernel. In contrast to existing graph kernels, the proposed SWWL kernel enjoys positive definiteness and a drastic complexity reduction, which  makes it possible to process datasets that were previously impossible to handle. The new kernel is first validated on graph classification for molec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20511;&#21161;&#26368;&#20248;&#20256;&#36755;&#30340;&#23545;&#20598;&#24418;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#20998;&#24067;&#30340;&#37325;&#24515;&#12290;&#36890;&#36807;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#27714;&#35299;&#22120;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#21452;&#23618;&#23545;&#25239;&#23398;&#20064;&#24182;&#36866;&#29992;&#20110;&#19968;&#33324;&#25104;&#26412;&#20989;&#25968;&#12290;&#36825;&#20010;&#26041;&#27861;&#30340;&#20851;&#38190;&#20248;&#21183;&#22312;&#20110;&#19981;&#38656;&#35201;&#20351;&#29992;&#19977;&#23618;&#20248;&#21270;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#25104;&#26412;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#29702;&#35770;&#35823;&#24046;&#30028;&#65292;&#24182;&#22312;&#31034;&#20363;&#22330;&#26223;&#21644;&#22270;&#20687;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#24212;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03828</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#20272;&#35745;&#20998;&#24067;&#30340;&#37325;&#24515;
&lt;/p&gt;
&lt;p&gt;
Estimating Barycenters of Distributions with Neural Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20511;&#21161;&#26368;&#20248;&#20256;&#36755;&#30340;&#23545;&#20598;&#24418;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#20998;&#24067;&#30340;&#37325;&#24515;&#12290;&#36890;&#36807;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#27714;&#35299;&#22120;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#21452;&#23618;&#23545;&#25239;&#23398;&#20064;&#24182;&#36866;&#29992;&#20110;&#19968;&#33324;&#25104;&#26412;&#20989;&#25968;&#12290;&#36825;&#20010;&#26041;&#27861;&#30340;&#20851;&#38190;&#20248;&#21183;&#22312;&#20110;&#19981;&#38656;&#35201;&#20351;&#29992;&#19977;&#23618;&#20248;&#21270;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#25104;&#26412;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#29702;&#35770;&#35823;&#24046;&#30028;&#65292;&#24182;&#22312;&#31034;&#20363;&#22330;&#26223;&#21644;&#22270;&#20687;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#24212;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#19968;&#32452;&#27010;&#29575;&#20998;&#24067;&#65292;&#20174;&#19994;&#32773;&#26377;&#26102;&#38656;&#35201;&#25214;&#21040;&#19968;&#31181;&#8220;&#24179;&#22343;&#8221;&#20998;&#24067;&#65292;&#20197;&#20805;&#20998;&#32858;&#21512;&#21442;&#32771;&#20998;&#24067;&#12290;&#19968;&#20010;&#29702;&#35770;&#19978;&#26377;&#21560;&#24341;&#21147;&#30340;&#24179;&#22343;&#20998;&#24067;&#27010;&#24565;&#26159;Wasserstein&#37325;&#24515;&#65292;&#32780;&#25105;&#20204;&#30340;&#24037;&#20316;&#37325;&#28857;&#23601;&#26159;&#23427;&#12290;&#36890;&#36807;&#24314;&#31435;&#22312;&#26368;&#20248;&#20256;&#36755;&#30340;&#23545;&#20598;&#24418;&#24335;&#20043;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#26041;&#27861;&#26469;&#35299;&#20915;Wasserstein&#37325;&#24515;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#26368;&#36817;&#30340;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#27714;&#35299;&#22120;&#65306;&#23427;&#20855;&#26377;&#21452;&#23618;&#23545;&#25239;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#36866;&#29992;&#20110;&#19968;&#33324;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;&#36825;&#20123;&#26159;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#20248;&#21183;&#65292;&#22240;&#20026;&#20856;&#22411;&#30340;&#21033;&#29992;&#37325;&#24515;&#20219;&#21153;&#30340;&#23545;&#25239;&#24615;&#31639;&#27861;&#24448;&#24448;&#21033;&#29992;&#19977;&#23618;&#20248;&#21270;&#65292;&#24182;&#19988;&#20027;&#35201;&#38598;&#20013;&#22312;&#20108;&#27425;&#25104;&#26412;&#19978;&#12290;&#25105;&#20204;&#36824;&#20026;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#29702;&#35770;&#35823;&#24046;&#30028;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#31034;&#20363;&#22330;&#26223;&#21644;&#22270;&#20687;&#25968;&#25454;&#35774;&#32622;&#19978;&#30340;&#36866;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a collection of probability measures, a practitioner sometimes needs to find an "average" distribution which adequately aggregates reference distributions. A theoretically appealing notion of such an average is the Wasserstein barycenter, which is the primal focus of our work. By building upon the dual formulation of Optimal Transport (OT), we propose a new scalable approach for solving the Wasserstein barycenter problem. Our methodology is based on the recent Neural OT solver: it has bi-level adversarial learning objective and works for general cost functions. These are key advantages of our method, since the typical adversarial algorithms leveraging barycenter tasks utilize tri-level optimization and focus mostly on quadratic cost. We also establish theoretical error bounds for our proposed approach and showcase its applicability and effectiveness on illustrative scenarios and image data setups.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RevOrder&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32763;&#36716;&#21152;&#27861;&#12289;&#20943;&#27861;&#21644;nD&#20056;&#20197;1D&#30340;&#36755;&#20986;&#25968;&#23383;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31639;&#26415;&#36816;&#31639;&#12290;&#32463;&#36807;&#20840;&#38754;&#27979;&#35797;&#65292;RevOrder&#22312;&#22522;&#26412;&#31639;&#26415;&#36816;&#31639;&#20013;&#36798;&#21040;&#20102;&#23436;&#32654;&#20934;&#30830;&#24230;&#65292;&#24182;&#22312;&#38500;&#27861;&#20219;&#21153;&#20013;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22823;&#25968;&#26102;&#12290;&#22312;GSM8K&#25968;&#23398;&#20219;&#21153;&#20013;&#24212;&#29992;RevOrder&#36827;&#34892;&#24494;&#35843;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#38169;&#35823;&#29575;&#24182;&#25552;&#39640;&#20102;&#24635;&#20307;&#24471;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.03822</link><description>&lt;p&gt;
RevOrder&#65306;&#19968;&#31181;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20013;&#31639;&#26415;&#36816;&#31639;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RevOrder: A Novel Method for Enhanced Arithmetic in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RevOrder&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32763;&#36716;&#21152;&#27861;&#12289;&#20943;&#27861;&#21644;nD&#20056;&#20197;1D&#30340;&#36755;&#20986;&#25968;&#23383;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31639;&#26415;&#36816;&#31639;&#12290;&#32463;&#36807;&#20840;&#38754;&#27979;&#35797;&#65292;RevOrder&#22312;&#22522;&#26412;&#31639;&#26415;&#36816;&#31639;&#20013;&#36798;&#21040;&#20102;&#23436;&#32654;&#20934;&#30830;&#24230;&#65292;&#24182;&#22312;&#38500;&#27861;&#20219;&#21153;&#20013;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22823;&#25968;&#26102;&#12290;&#22312;GSM8K&#25968;&#23398;&#20219;&#21153;&#20013;&#24212;&#29992;RevOrder&#36827;&#34892;&#24494;&#35843;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#38169;&#35823;&#29575;&#24182;&#25552;&#39640;&#20102;&#24635;&#20307;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RevOrder&#65292;&#19968;&#31181;&#26088;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31639;&#26415;&#36816;&#31639;&#30340;&#26032;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32763;&#36716;&#21152;&#27861;&#12289;&#20943;&#27861;&#21644;n&#20301;&#25968;&#20056;&#20197;1&#20301;&#25968;&#65288;nD&#20056;&#20197;1D&#65289;&#30340;&#36755;&#20986;&#25968;&#23383;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#39034;&#24207;&#20013;&#38388;&#25968;&#23383;&#30340;&#25968;&#37327; (CSID)&#65292;&#36825;&#26159;&#25105;&#20204;&#24341;&#20837;&#30340;&#19968;&#31181;&#35780;&#20272;&#26041;&#31243;&#22797;&#26434;&#24615;&#30340;&#26032;&#24230;&#37327;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#27979;&#35797;&#65292;RevOrder&#19981;&#20165;&#22312;&#22522;&#26412;&#30340;&#31639;&#26415;&#36816;&#31639;&#20013;&#36798;&#21040;&#20102;&#23436;&#32654;&#30340;&#20934;&#30830;&#24230;&#65292;&#32780;&#19988;&#22312;&#38500;&#27861;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20256;&#32479;&#27169;&#22411;&#38590;&#20197;&#22788;&#29702;&#30340;&#22823;&#25968;&#24773;&#20917;&#19979;&#12290;RevOrder&#30340;&#23454;&#29616;&#23545;&#20110;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#37117;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#12290;&#27492;&#22806;&#65292;&#23558;RevOrder&#24212;&#29992;&#20110;&#23545;GSM8K&#25968;&#23398;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;LLaMA2-7B&#27169;&#22411;&#20013;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#65292;&#23558;&#26041;&#31243;&#35745;&#31639;&#38169;&#35823;&#29575;&#38477;&#20302;&#20102;46%&#65292;&#23558;&#24635;&#20307;&#24471;&#20998;&#20174;41.6&#25552;&#21319;&#21040;44.4&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents RevOrder, a novel technique aimed at improving arithmetic operations in large language models (LLMs) by reversing the output digits in addition, subtraction, and n-digit by 1-digit (nD by 1D) multiplication tasks. Our method significantly reduces the Count of Sequential Intermediate Digits (CSID) to $\mathcal{O}(1)$, a new metric we introduce to assess equation complexity. Through comprehensive testing, RevOrder not only achieves perfect accuracy in basic arithmetic operations but also substantially boosts LLM performance in division tasks, particularly with large numbers where traditional models struggle. Implementation of RevOrder is cost-effective for both training and inference phases. Moreover, applying RevOrder to fine-tune the LLaMA2-7B model on the GSM8K math task results in a considerable improvement, reducing equation calculation errors by 46% and increasing overall scores from 41.6 to 44.4.
&lt;/p&gt;</description></item><item><title>SMOTE&#26159;&#19968;&#31181;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#24120;&#29992;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#22797;&#21046;&#21407;&#22987;&#23569;&#25968;&#26679;&#26412;&#26469;&#37325;&#26032;&#29983;&#25104;&#21407;&#22987;&#20998;&#24067;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;SMOTE&#30340;&#23494;&#24230;&#22312;&#23569;&#25968;&#26679;&#26412;&#20998;&#24067;&#30340;&#36793;&#30028;&#38468;&#36817;&#36880;&#28176;&#20943;&#23567;&#65292;&#20174;&#32780;&#39564;&#35777;&#20102;BorderLine SMOTE&#31574;&#30053;&#30340;&#21512;&#29702;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;SMOTE&#30456;&#20851;&#31574;&#30053;&#65292;&#24182;&#19982;&#20854;&#20182;&#37325;&#26032;&#24179;&#34913;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#26368;&#32456;&#21457;&#29616;&#65292;&#22312;&#25968;&#25454;&#38598;&#26497;&#24230;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#65292;SMOTE&#12289;&#25552;&#20986;&#30340;&#26041;&#27861;&#25110;&#27424;&#37319;&#26679;&#31243;&#24207;&#26159;&#26368;&#20339;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.03819</link><description>&lt;p&gt;
SMOTE&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#65306;&#20851;&#20110;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#30340;&#38480;&#21046;&#21644;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Theoretical and experimental study of SMOTE: limitations and comparisons of rebalancing strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03819
&lt;/p&gt;
&lt;p&gt;
SMOTE&#26159;&#19968;&#31181;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#24120;&#29992;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#22797;&#21046;&#21407;&#22987;&#23569;&#25968;&#26679;&#26412;&#26469;&#37325;&#26032;&#29983;&#25104;&#21407;&#22987;&#20998;&#24067;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;SMOTE&#30340;&#23494;&#24230;&#22312;&#23569;&#25968;&#26679;&#26412;&#20998;&#24067;&#30340;&#36793;&#30028;&#38468;&#36817;&#36880;&#28176;&#20943;&#23567;&#65292;&#20174;&#32780;&#39564;&#35777;&#20102;BorderLine SMOTE&#31574;&#30053;&#30340;&#21512;&#29702;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;SMOTE&#30456;&#20851;&#31574;&#30053;&#65292;&#24182;&#19982;&#20854;&#20182;&#37325;&#26032;&#24179;&#34913;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#26368;&#32456;&#21457;&#29616;&#65292;&#22312;&#25968;&#25454;&#38598;&#26497;&#24230;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#65292;SMOTE&#12289;&#25552;&#20986;&#30340;&#26041;&#27861;&#25110;&#27424;&#37319;&#26679;&#31243;&#24207;&#26159;&#26368;&#20339;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SMOTE&#65288;Synthetic Minority Oversampling Technique&#65289;&#26159;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#24120;&#29992;&#30340;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#28176;&#36827;&#24773;&#20917;&#19979;&#65292;SMOTE&#65288;&#40664;&#35748;&#21442;&#25968;&#65289;&#36890;&#36807;&#31616;&#21333;&#22797;&#21046;&#21407;&#22987;&#23569;&#25968;&#26679;&#26412;&#26469;&#37325;&#26032;&#29983;&#25104;&#21407;&#22987;&#20998;&#24067;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#23569;&#25968;&#26679;&#26412;&#20998;&#24067;&#30340;&#25903;&#25345;&#36793;&#30028;&#38468;&#36817;&#65292;SMOTE&#30340;&#23494;&#24230;&#20250;&#20943;&#23567;&#65292;&#20174;&#32780;&#39564;&#35777;&#20102;&#24120;&#35265;&#30340;BorderLine SMOTE&#31574;&#30053;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;SMOTE&#30456;&#20851;&#31574;&#30053;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#29616;&#26377;&#30340;&#37325;&#26032;&#24179;&#34913;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21482;&#26377;&#24403;&#25968;&#25454;&#38598;&#26497;&#24230;&#19981;&#24179;&#34913;&#26102;&#25165;&#38656;&#35201;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#12290;&#23545;&#20110;&#36825;&#31181;&#25968;&#25454;&#38598;&#65292;SMOTE&#12289;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#25110;&#27424;&#37319;&#26679;&#31243;&#24207;&#26159;&#26368;&#20339;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic Minority Oversampling Technique (SMOTE) is a common rebalancing strategy for handling imbalanced data sets. Asymptotically, we prove that SMOTE (with default parameter) regenerates the original distribution by simply copying the original minority samples. We also prove that SMOTE density vanishes near the boundary of the support of the minority distribution, therefore justifying the common BorderLine SMOTE strategy. Then we introduce two new SMOTE-related strategies, and compare them with state-of-the-art rebalancing procedures. We show that rebalancing strategies are only required when the data set is highly imbalanced. For such data sets, SMOTE, our proposals, or undersampling procedures are the best strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21333;&#23618;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#22312;&#39640;&#32500;&#26497;&#38480;&#19979;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#39044;&#27979;&#65292;&#24182;&#25512;&#24191;&#20102;&#23545;&#22810;&#31181;&#25968;&#25454;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#23613;&#31649;GCN&#22312;&#25910;&#25947;&#36895;&#24230;&#19978;&#26159;&#19968;&#33268;&#30340;&#65292;&#20294;&#22312;&#20219;&#20309;&#24773;&#20917;&#19979;&#37117;&#19981;&#33021;&#36798;&#21040;&#36125;&#21494;&#26031;&#26368;&#20248;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.03818</link><description>&lt;p&gt;
&#21333;&#23618;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#28176;&#36827;&#27867;&#21270;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Asymptotic generalization error of a single-layer graph convolutional network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21333;&#23618;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#22312;&#39640;&#32500;&#26497;&#38480;&#19979;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#39044;&#27979;&#65292;&#24182;&#25512;&#24191;&#20102;&#23545;&#22810;&#31181;&#25968;&#25454;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#23613;&#31649;GCN&#22312;&#25910;&#25947;&#36895;&#24230;&#19978;&#26159;&#19968;&#33268;&#30340;&#65292;&#20294;&#22312;&#20219;&#20309;&#24773;&#20917;&#19979;&#37117;&#19981;&#33021;&#36798;&#21040;&#36125;&#21494;&#26031;&#26368;&#20248;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#23454;&#36341;&#20013;&#23637;&#29616;&#20986;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#26159;&#30456;&#23545;&#20110;&#24191;&#27867;&#30740;&#31350;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65292;&#20851;&#20110;&#20854;&#27867;&#21270;&#29305;&#24615;&#19982;&#26679;&#26412;&#25968;&#37327;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39044;&#27979;&#20102;&#22312;&#39640;&#32500;&#26497;&#38480;&#19979;&#65292;&#22522;&#20110;&#23646;&#24615;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;SBM&#65289;&#29983;&#25104;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#21333;&#23618;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#30340;&#24615;&#33021;&#12290;&#20043;&#21069;&#65292;&#20165;&#22312;Shi&#31561;&#20154;&#30340;&#25991;&#31456;&#20013;&#32771;&#34385;&#20102;&#19978;&#19979;&#25991;-SBM&#65288;CSBM&#65289;&#19978;&#30340;&#23725;&#22238;&#24402;&#20998;&#26512;&#65307;&#25105;&#20204;&#23558;&#20998;&#26512;&#25512;&#24191;&#21040;CSBM&#30340;&#20219;&#24847;&#20984;&#25439;&#22833;&#21644;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#24182;&#28155;&#21152;&#20102;&#23545;&#21478;&#19968;&#20010;&#25968;&#25454;&#27169;&#22411;&#8212;&#8212;&#31070;&#32463;&#20248;&#20808;SBM&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#39640;&#20449;&#22122;&#27604;&#26497;&#38480;&#65292;&#24182;&#35814;&#32454;&#20171;&#32461;&#20102;GCN&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#23613;&#31649;&#19968;&#33268;&#65292;&#20294;&#23545;&#20110;&#20219;&#20309;&#32771;&#34385;&#30340;&#24773;&#20917;&#37117;&#19981;&#33021;&#36798;&#21040;&#36125;&#21494;&#26031;&#26368;&#20248;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While graph convolutional networks show great practical promises, the theoretical understanding of their generalization properties as a function of the number of samples is still in its infancy compared to the more broadly studied case of supervised fully connected neural networks. In this article, we predict the performances of a single-layer graph convolutional network (GCN) trained on data produced by attributed stochastic block models (SBMs) in the high-dimensional limit. Previously, only ridge regression on contextual-SBM (CSBM) has been considered in Shi et al. 2022; we generalize the analysis to arbitrary convex loss and regularization for the CSBM and add the analysis for another data model, the neural-prior SBM. We also study the high signal-to-noise ratio limit, detail the convergence rates of the GCN and show that, while consistent, it does not reach the Bayes-optimal rate for any of the considered cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#19968;&#33268;&#24615;&#27169;&#22411;&#21387;&#32553;&#30340;&#20419;&#36827;&#32593;&#32476;&#20869;&#32852;&#21512;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23458;&#25143;&#31471;&#25237;&#31080;&#21644;&#27169;&#22411;&#32858;&#21512;&#26469;&#20943;&#23569;&#20869;&#23384;&#31354;&#38388;&#21644;&#36890;&#20449;&#27969;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.03815</link><description>&lt;p&gt;
&#22522;&#20110;&#25237;&#31080;&#19968;&#33268;&#24615;&#27169;&#22411;&#21387;&#32553;&#30340;&#20419;&#36827;&#32593;&#32476;&#20869;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Expediting In-Network Federated Learning by Voting-Based Consensus Model Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#31080;&#19968;&#33268;&#24615;&#27169;&#22411;&#21387;&#32553;&#30340;&#20419;&#36827;&#32593;&#32476;&#20869;&#32852;&#21512;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23458;&#25143;&#31471;&#25237;&#31080;&#21644;&#27169;&#22411;&#32858;&#21512;&#26469;&#20943;&#23569;&#20869;&#23384;&#31354;&#38388;&#21644;&#36890;&#20449;&#27969;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#33021;&#21147;&#65292;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#20026;&#20102;&#36890;&#36807;FL&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#36890;&#36807;&#20114;&#32852;&#32593;&#19982;&#21442;&#25968;&#26381;&#21153;&#22120;&#20132;&#25442;&#27169;&#22411;&#26356;&#26032;&#12290;&#20026;&#20102;&#21152;&#36895;&#36890;&#20449;&#36895;&#24230;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#20102;&#22312;&#21442;&#25968;&#26381;&#21153;&#22120;&#30340;&#20301;&#32622;&#37096;&#32626;&#21487;&#32534;&#31243;&#20132;&#25442;&#26426;&#65288;PS&#65289;&#20197;&#21327;&#35843;&#23458;&#25143;&#31471;&#12290;&#22312;FL&#20013;&#37096;&#32626;PS&#30340;&#25361;&#25112;&#22312;&#20110;&#20854;&#20869;&#23384;&#31354;&#38388;&#31232;&#32570;&#65292;&#26080;&#27861;&#22312;PS&#19978;&#36816;&#34892;&#28040;&#32791;&#22823;&#37327;&#20869;&#23384;&#30340;&#32858;&#21512;&#31639;&#27861;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;&#23398;&#20064;&#32593;&#32476;&#20869;&#32858;&#21512;&#21387;&#32553;&#65288;FediAC&#65289;&#31639;&#27861;&#65292;&#23427;&#21253;&#21547;&#20004;&#20010;&#38454;&#27573;&#65306;&#23458;&#25143;&#31471;&#25237;&#31080;&#21644;&#27169;&#22411;&#32858;&#21512;&#12290;&#22312;&#21069;&#19968;&#38454;&#27573;&#65292;&#23458;&#25143;&#31471;&#21521;PS&#25253;&#21578;&#20854;&#37325;&#35201;&#27169;&#22411;&#26356;&#26032;&#30340;&#32034;&#24341;&#65292;&#20197;&#20272;&#35745;&#20840;&#23616;&#37325;&#35201;&#27169;&#22411;&#26356;&#26032;&#12290;&#22312;&#21518;&#19968;&#38454;&#27573;&#65292;&#23458;&#25143;&#31471;&#23558;&#20840;&#23616;&#37325;&#35201;&#27169;&#22411;&#26356;&#26032;&#19978;&#20256;&#21040;PS&#36827;&#34892;&#32858;&#21512;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;FediAC&#28040;&#32791;&#30340;&#20869;&#23384;&#31354;&#38388;&#21644;&#36890;&#20449;&#27969;&#37327;&#36739;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, federated learning (FL) has gained momentum because of its capability in preserving data privacy. To conduct model training by FL, multiple clients exchange model updates with a parameter server via Internet. To accelerate the communication speed, it has been explored to deploy a programmable switch (PS) in lieu of the parameter server to coordinate clients. The challenge to deploy the PS in FL lies in its scarce memory space, prohibiting running memory consuming aggregation algorithms on the PS. To overcome this challenge, we propose Federated Learning in-network Aggregation with Compression (FediAC) algorithm, consisting of two phases: client voting and model aggregating. In the former phase, clients report their significant model update indices to the PS to estimate global significant model updates. In the latter phase, clients upload global significant model updates to the PS for aggregation. FediAC consumes much less memory space and communication traffic than existing w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#38750;&#31163;&#25955;&#36793;&#33945;&#38754;&#21644;&#36880;&#23618;&#24102;&#23485;&#39044;&#27979;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#20016;&#23500;&#19988;&#26377;&#25928;&#30340;&#25299;&#25169;&#33945;&#38754;&#22270;&#33258;&#32534;&#30721;&#22120;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#23398;&#20064;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#20449;&#24687;&#34920;&#31034;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.03814</link><description>&lt;p&gt;
&#24102;&#38750;&#31163;&#25955;&#24102;&#23485;&#30340;&#33945;&#38754;&#22270;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Masked Graph Autoencoder with Non-discrete Bandwidths
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#38750;&#31163;&#25955;&#36793;&#33945;&#38754;&#21644;&#36880;&#23618;&#24102;&#23485;&#39044;&#27979;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#20016;&#23500;&#19988;&#26377;&#25928;&#30340;&#25299;&#25169;&#33945;&#38754;&#22270;&#33258;&#32534;&#30721;&#22120;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#23398;&#20064;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#20449;&#24687;&#34920;&#31034;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33945;&#38754;&#22270;&#33258;&#32534;&#30721;&#22120;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#22270;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#20986;&#29616;&#65292;&#20294;&#23578;&#26410;&#23436;&#20840;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#29616;&#26377;&#30340;&#31163;&#25955;&#36793;&#33945;&#38754;&#21644;&#20108;&#36827;&#21046;&#38142;&#25509;&#37325;&#26500;&#31574;&#30053;&#22312;&#23398;&#20064;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#20449;&#24687;&#34920;&#31034;&#26041;&#38754;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#36825;&#20123;&#38480;&#21046;&#21253;&#25324;&#38459;&#22622;&#28040;&#24687;&#27969;&#12289;&#23481;&#26131;&#36807;&#24230;&#24179;&#28369;&#20197;&#21450;&#27425;&#20248;&#30340;&#37051;&#22495;&#21306;&#20998;&#24615;&#12290;&#21463;&#36825;&#20123;&#35748;&#35782;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#38750;&#31163;&#25955;&#36793;&#33945;&#38754;&#65292;&#20854;&#20174;&#36830;&#32493;&#21644;&#20998;&#25955;&#27010;&#29575;&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#32780;&#19981;&#26159;&#31163;&#25955;&#20271;&#21162;&#21033;&#20998;&#24067;&#12290;&#36825;&#20123;&#33945;&#38754;&#38480;&#21046;&#20102;&#27599;&#20010;&#36793;&#30340;&#36755;&#20986;&#28040;&#24687;&#37327;&#65292;&#31216;&#20026;&#8220;&#24102;&#23485;&#8221;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#20449;&#24687;&#20016;&#23500;&#19988;&#26377;&#25928;&#30340;&#25299;&#25169;&#33945;&#38754;&#22270;&#33258;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#24102;&#23485;&#33945;&#38754;&#21644;&#36880;&#23618;&#24102;&#23485;&#39044;&#27979;&#30446;&#26631;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#22270;&#25299;&#25169;&#23398;&#20064;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked graph autoencoders have emerged as a powerful graph self-supervised learning method that has yet to be fully explored. In this paper, we unveil that the existing discrete edge masking and binary link reconstruction strategies are insufficient to learn topologically informative representations, from the perspective of message propagation on graph neural networks. These limitations include blocking message flows, vulnerability to over-smoothness, and suboptimal neighborhood discriminability. Inspired by these understandings, we explore non-discrete edge masks, which are sampled from a continuous and dispersive probability distribution instead of the discrete Bernoulli distribution. These masks restrict the amount of output messages for each edge, referred to as "bandwidths". We propose a novel, informative, and effective topological masked graph autoencoder using bandwidth masking and a layer-wise bandwidth prediction objective. We demonstrate its powerful graph topological learni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#32858;&#31867;&#30340;NK&#28151;&#21512;&#36951;&#20256;&#31639;&#27861;&#65292;&#21033;&#29992;NK&#32858;&#31867;&#39564;&#35777;&#20934;&#21017;2&#65288;NKCV2&#65289;&#26469;&#35780;&#20272;&#35299;&#20915;&#26041;&#26696;&#24182;&#35782;&#21035;&#22522;&#20110;&#23494;&#24230;&#30340;&#21306;&#22495;&#12290;&#31639;&#27861;&#21033;&#29992;&#20915;&#31574;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#28784;&#30418;&#20248;&#21270;&#65292;&#24182;&#20351;&#29992;&#21464;&#24322;&#31639;&#23376;&#12289;&#20998;&#21306;&#20132;&#21449;&#21644;&#23616;&#37096;&#25628;&#32034;&#31574;&#30053;&#36827;&#34892;&#20248;&#21270;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#33021;&#26377;&#25928;&#26816;&#27979;&#32858;&#31867;&#21306;&#22495;&#12290;</title><link>https://arxiv.org/abs/2402.03813</link><description>&lt;p&gt;
NK&#28151;&#21512;&#36951;&#20256;&#31639;&#27861;&#29992;&#20110;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
NK Hybrid Genetic Algorithm for Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#32858;&#31867;&#30340;NK&#28151;&#21512;&#36951;&#20256;&#31639;&#27861;&#65292;&#21033;&#29992;NK&#32858;&#31867;&#39564;&#35777;&#20934;&#21017;2&#65288;NKCV2&#65289;&#26469;&#35780;&#20272;&#35299;&#20915;&#26041;&#26696;&#24182;&#35782;&#21035;&#22522;&#20110;&#23494;&#24230;&#30340;&#21306;&#22495;&#12290;&#31639;&#27861;&#21033;&#29992;&#20915;&#31574;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#28784;&#30418;&#20248;&#21270;&#65292;&#24182;&#20351;&#29992;&#21464;&#24322;&#31639;&#23376;&#12289;&#20998;&#21306;&#20132;&#21449;&#21644;&#23616;&#37096;&#25628;&#32034;&#31574;&#30053;&#36827;&#34892;&#20248;&#21270;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#33021;&#26377;&#25928;&#26816;&#27979;&#32858;&#31867;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32858;&#31867;&#30340;NK&#28151;&#21512;&#36951;&#20256;&#31639;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#28151;&#21512;&#31639;&#27861;&#20351;&#29992;NK&#32858;&#31867;&#39564;&#35777;&#20934;&#21017;2&#65288;NKCV2&#65289;&#12290;NKCV2&#21033;&#29992;&#20851;&#20110;N&#20010;&#23567;&#23545;&#35937;&#32452;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;&#27599;&#20010;&#32452;&#30001;&#25968;&#25454;&#38598;&#20013;&#30340;K+1&#20010;&#23545;&#35937;&#32452;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#20855;&#26377;&#22266;&#23450;&#23567;K&#30340;NKCV2&#21487;&#20197;&#35782;&#21035;&#22522;&#20110;&#23494;&#24230;&#30340;&#21306;&#22495;&#12290;&#22312;NKCV2&#20013;&#65292;&#20915;&#31574;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#24050;&#30693;&#65292;&#36825;&#20801;&#35768;&#25105;&#20204;&#24212;&#29992;&#28784;&#30418;&#20248;&#21270;&#12290;&#25552;&#20986;&#20102;&#21464;&#24322;&#31639;&#23376;&#12289;&#20998;&#21306;&#20132;&#21449;&#21644;&#23616;&#37096;&#25628;&#32034;&#31574;&#30053;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#20351;&#29992;&#20102;&#20915;&#31574;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#20449;&#24687;&#12290;&#22312;&#20998;&#21306;&#20132;&#21449;&#20013;&#65292;&#35780;&#20272;&#20989;&#25968;&#34987;&#20998;&#35299;&#20026;q&#20010;&#29420;&#31435;&#30340;&#37096;&#20998;&#65307;&#20998;&#21306;&#20132;&#21449;&#20197;&#35745;&#31639;&#22797;&#26434;&#24230;&#20026;O(N)&#36820;&#22238;&#22312;2^q&#20010;&#21487;&#33021;&#30340;&#21518;&#20195;&#20013;&#26368;&#22909;&#30340;&#21518;&#20195;&#12290;NK&#28151;&#21512;&#36951;&#20256;&#31639;&#27861;&#33021;&#22815;&#26816;&#27979;&#21040;&#20855;&#26377;&#23494;&#24230;&#30340;&#21306;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The NK hybrid genetic algorithm for clustering is proposed in this paper. In order to evaluate the solutions, the hybrid algorithm uses the NK clustering validation criterion 2 (NKCV2). NKCV2 uses information about the disposition of $N$ small groups of objects. Each group is composed of $K+1$ objects of the dataset. Experimental results show that density-based regions can be identified by using NKCV2 with fixed small $K$. In NKCV2, the relationship between decision variables is known, which in turn allows us to apply gray box optimization. Mutation operators, a partition crossover, and a local search strategy are proposed, all using information about the relationship between decision variables. In partition crossover, the evaluation function is decomposed into $q$ independent components; partition crossover then deterministically returns the best among $2^q$ possible offspring with computational complexity $O(N)$. The NK hybrid genetic algorithm allows the detection of clusters with a
&lt;/p&gt;</description></item><item><title>SDEMG&#26159;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;&#21435;&#22122;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SDEMG&#32988;&#36807;&#20102;&#20854;&#20182;&#27604;&#36739;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03808</link><description>&lt;p&gt;
SDEMG: &#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
SDEMG: Score-based Diffusion Model for Surface Electromyographic Signal Denoising
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03808
&lt;/p&gt;
&lt;p&gt;
SDEMG&#26159;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#34920;&#38754;&#32908;&#30005;&#20449;&#21495;&#21435;&#22122;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;SDEMG&#32988;&#36807;&#20102;&#20854;&#20182;&#27604;&#36739;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#34987;&#30417;&#27979;&#30340;&#32908;&#32905;&#38752;&#36817;&#24515;&#33039;&#26102;&#65292;&#34920;&#38754;&#32908;&#30005;&#22270;&#65288;sEMG&#65289;&#35760;&#24405;&#21487;&#33021;&#21463;&#21040;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20449;&#21495;&#30340;&#24433;&#21709;&#12290;&#19968;&#20123;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#20449;&#21495;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#22914;&#39640;&#36890;&#28388;&#27874;&#22120;&#21644;&#27169;&#26495;&#20943;&#27861;&#65292;&#32780;&#19968;&#20123;&#26041;&#27861;&#21017;&#36890;&#36807;&#27714;&#21462;&#26144;&#23556;&#20989;&#25968;&#20174;&#24102;&#26377;ECG&#24178;&#25200;&#30340;sEMG&#65288;&#22122;&#22768;sEMG&#65289;&#20013;&#24674;&#22797;&#20986;&#24178;&#20928;&#30340;sEMG&#20449;&#21495;&#12290;&#26368;&#36817;&#24341;&#20837;&#20102;&#19968;&#31181;&#33879;&#21517;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#36890;&#36807;&#22122;&#22768;&#36755;&#20837;&#25968;&#25454;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#20934;&#30830;&#30340;&#26679;&#26412;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;SDEMG&#65292;&#29992;&#20316;sEMG&#20449;&#21495;&#21435;&#22122;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;SDEMG&#26041;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#24320;&#25918;&#25509;&#28304;&#30340;Non-Invasive Adaptive Prosthetics&#25968;&#25454;&#24211;&#30340;&#25968;&#25454;&#20197;&#21450;&#26469;&#33258;MIT-BIH Normal Sinus Rhythm&#25968;&#25454;&#24211;&#30340;ECG&#20449;&#21495;&#36827;&#34892;&#20102;&#38477;&#22122;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SDEMG&#32988;&#36807;&#20102;&#27604;&#36739;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surface electromyography (sEMG) recordings can be influenced by electrocardiogram (ECG) signals when the muscle being monitored is close to the heart. Several existing methods use signal-processing-based approaches, such as high-pass filter and template subtraction, while some derive mapping functions to restore clean sEMG signals from noisy sEMG (sEMG with ECG interference). Recently, the score-based diffusion model, a renowned generative model, has been introduced to generate high-quality and accurate samples with noisy input data. In this study, we proposed a novel approach, termed SDEMG, as a score-based diffusion model for sEMG signal denoising. To evaluate the proposed SDEMG approach, we conduct experiments to reduce noise in sEMG signals, employing data from an openly accessible source, the Non-Invasive Adaptive Prosthetics database, along with ECG signals from the MIT-BIH Normal Sinus Rhythm Database. The experiment result indicates that SDEMG outperformed comparative methods a
&lt;/p&gt;</description></item><item><title>SEABO&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#20197;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#26681;&#25454;&#19987;&#23478;&#25968;&#25454;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#24471;&#21040;&#22870;&#21169;&#20989;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#24615;&#33021;&#19982;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.03807</link><description>&lt;p&gt;
SEABO: &#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SEABO: A Simple Search-Based Method for Offline Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03807
&lt;/p&gt;
&lt;p&gt;
SEABO&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#20197;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#26681;&#25454;&#19987;&#23478;&#25968;&#25454;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#24471;&#21040;&#22870;&#21169;&#20989;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#24615;&#33021;&#19982;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30001;&#20110;&#33021;&#22815;&#20174;&#38745;&#24577;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#24182;&#28040;&#38500;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#38656;&#27714;&#65292;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#31163;&#32447;RL&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#26631;&#26377;&#22870;&#21169;&#26631;&#31614;&#30340;&#31163;&#32447;&#36716;&#25442;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#32463;&#24120;&#38656;&#35201;&#25163;&#24037;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#36825;&#26377;&#26102;&#26159;&#22256;&#38590;&#30340;&#12289;&#21171;&#21160;&#23494;&#38598;&#30340;&#25110;&#20302;&#25928;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25226;&#37325;&#28857;&#25918;&#22312;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#35774;&#32622;&#19978;&#65292;&#26088;&#22312;&#22522;&#20110;&#19987;&#23478;&#25968;&#25454;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#24471;&#21040;&#19968;&#20010;&#22870;&#21169;&#20989;&#25968;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#31163;&#32447;IL&#26041;&#27861;&#65292;&#31216;&#20026;SEABO&#12290;SEABO&#20197;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#23558;&#36739;&#22823;&#30340;&#22870;&#21169;&#20998;&#37197;&#32473;&#19982;&#19987;&#23478;&#28436;&#31034;&#20013;&#26368;&#25509;&#36817;&#30340;&#36716;&#25442;&#65292;&#21542;&#21017;&#20998;&#37197;&#36739;&#23567;&#30340;&#22870;&#21169;&#12290;&#22312;&#22810;&#20010;D4RL&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SEABO&#33021;&#22815;&#36798;&#21040;&#19982;&#31163;&#32447;RL&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) has attracted much attention due to its ability in learning from static offline datasets and eliminating the need of interacting with the environment. Nevertheless, the success of offline RL relies heavily on the offline transitions annotated with reward labels. In practice, we often need to hand-craft the reward function, which is sometimes difficult, labor-intensive, or inefficient. To tackle this challenge, we set our focus on the offline imitation learning (IL) setting, and aim at getting a reward function based on the expert data and unlabeled data. To that end, we propose a simple yet effective search-based offline IL method, tagged SEABO. SEABO allocates a larger reward to the transition that is close to its closest neighbor in the expert demonstration, and a smaller reward otherwise, all in an unsupervised learning manner. Experimental results on a variety of D4RL datasets indicate that SEABO can achieve competitive performance to offline RL 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#35299;&#37322;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#22312;&#20449;&#36151;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#65292;&#32467;&#21512;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;SHapley Additive exPlanations&#65288;SHAP&#65289;&#12290;&#23427;&#25552;&#39640;&#20102;&#20449;&#36151;&#20915;&#31574;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#22686;&#36827;&#20102;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20043;&#38388;&#30340;&#20449;&#20219;&#21644;&#21512;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.03806</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#22312;&#20449;&#36151;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#65306;&#22686;&#24378;&#37329;&#34701;&#24037;&#31243;&#20013;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Explainable Automated Machine Learning for Credit Decisions: Enhancing Human Artificial Intelligence Collaboration in Financial Engineering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#35299;&#37322;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#22312;&#20449;&#36151;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#65292;&#32467;&#21512;&#20102;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;SHapley Additive exPlanations&#65288;SHAP&#65289;&#12290;&#23427;&#25552;&#39640;&#20102;&#20449;&#36151;&#20915;&#31574;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#22686;&#36827;&#20102;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20043;&#38388;&#30340;&#20449;&#20219;&#21644;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#22312;&#37329;&#34701;&#24037;&#31243;&#39046;&#22495;&#30340;&#25972;&#21512;&#65292;&#29305;&#21035;&#20851;&#27880;&#20854;&#22312;&#20449;&#36151;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#12290;&#37329;&#34701;&#39046;&#22495;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#38656;&#35201;&#22312;&#36825;&#20123;&#31995;&#32479;&#20013;&#25214;&#21040;&#22797;&#26434;&#31639;&#27861;&#20915;&#31574;&#19982;&#36879;&#26126;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#37325;&#28857;&#26159;AutoML&#22914;&#20309;&#31616;&#21270;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#20197;&#36827;&#34892;&#20449;&#29992;&#35780;&#20998;&#65292;&#32780;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;SHapley Additive exPlanations&#65288;SHAP&#65289;&#65292;&#21017;&#20026;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#20102;&#27934;&#23519;&#21147;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;AutoML&#21644;XAI&#30340;&#32467;&#21512;&#19981;&#20165;&#25552;&#39640;&#20102;&#20449;&#36151;&#20915;&#31574;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#36824;&#20419;&#36827;&#20102;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20043;&#38388;&#30340;&#20449;&#20219;&#21644;&#21512;&#20316;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#21487;&#35299;&#37322;&#30340;AutoML&#22312;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#37329;&#34701;&#20915;&#31574;&#30340;&#36879;&#26126;&#24230;&#21644;&#36131;&#20219;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#19982;&#35201;&#27714;&#20026;&#29992;&#25143;&#25552;&#20379;&#35299;&#37322;&#30340;&#36235;&#21183;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the integration of Explainable Automated Machine Learning (AutoML) in the realm of financial engineering, specifically focusing on its application in credit decision-making. The rapid evolution of Artificial Intelligence (AI) in finance has necessitated a balance between sophisticated algorithmic decision-making and the need for transparency in these systems. The focus is on how AutoML can streamline the development of robust machine learning models for credit scoring, while Explainable AI (XAI) methods, particularly SHapley Additive exPlanations (SHAP), provide insights into the models' decision-making processes. This study demonstrates how the combination of AutoML and XAI not only enhances the efficiency and accuracy of credit decisions but also fosters trust and collaboration between humans and AI systems. The findings underscore the potential of explainable AutoML in improving the transparency and accountability of AI-driven financial decisions, aligning with r
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21160;&#24577;&#36339;&#36807;&#38750;&#27963;&#36291;&#31070;&#32463;&#20803;&#30340;&#35745;&#31639;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;LLM&#30340;&#39640;&#25928;&#28608;&#27963;&#20989;&#25968;ReLU^2&#65292;&#23427;&#22312;&#31232;&#30095;&#24615;&#19982;&#24615;&#33021;&#12289;&#31232;&#30095;&#24615;&#30340;&#39044;&#27979;&#24615;&#21644;&#30828;&#20214;&#20146;&#21644;&#24615;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.03804</link><description>&lt;p&gt;
ReLU^2&#33719;&#32988;&#65306;&#21457;&#29616;&#31232;&#30095;LLM&#30340;&#39640;&#25928;&#28608;&#27963;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03804
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#36339;&#36807;&#38750;&#27963;&#36291;&#31070;&#32463;&#20803;&#30340;&#35745;&#31639;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;LLM&#30340;&#39640;&#25928;&#28608;&#27963;&#20989;&#25968;ReLU^2&#65292;&#23427;&#22312;&#31232;&#30095;&#24615;&#19982;&#24615;&#33021;&#12289;&#31232;&#30095;&#24615;&#30340;&#39044;&#27979;&#24615;&#21644;&#30828;&#20214;&#20146;&#21644;&#24615;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#36339;&#36807;&#38750;&#27963;&#36291;&#31070;&#32463;&#20803;&#30340;&#35745;&#31639;&#65292;&#31232;&#30095;&#35745;&#31639;&#20026;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#26029;&#25552;&#20379;&#20102;&#19968;&#31181;&#24341;&#20154;&#27880;&#30446;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#34429;&#28982;&#20256;&#32479;&#26041;&#27861;&#27880;&#37325;&#22522;&#20110;ReLU&#30340;LLM&#65292;&#21033;&#29992;&#28608;&#27963;&#20540;&#20013;&#30340;&#38646;&#65292;&#20294;&#25105;&#20204;&#23558;&#31232;&#30095;LLM&#30340;&#33539;&#22260;&#25193;&#22823;&#21040;&#38646;&#28608;&#27963;&#20540;&#20043;&#22806;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#20803;&#36755;&#20986;&#24133;&#24230;&#21644;&#23450;&#21046;&#30340;&#24133;&#24230;&#38408;&#20540;&#26469;&#23450;&#20041;&#31070;&#32463;&#20803;&#28608;&#27963;&#65292;&#24182;&#35777;&#26126;&#38750;ReLU LLM&#20063;&#34920;&#29616;&#20986;&#31232;&#30095;&#28608;&#27963;&#12290;&#20026;&#20102;&#25214;&#21040;&#26368;&#39640;&#25928;&#30340;&#31232;&#30095;&#35745;&#31639;&#28608;&#27963;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65292;&#20174;&#19977;&#20010;&#26041;&#38754;&#32771;&#23519;LLM&#30340;&#31232;&#30095;&#24615;&#65306;&#31232;&#30095;&#24615;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12289;&#31232;&#30095;&#24615;&#30340;&#39044;&#27979;&#24615;&#21644;&#30828;&#20214;&#20146;&#21644;&#24615;&#12290;&#25105;&#20204;&#23545;&#20351;&#29992;&#19981;&#21516;&#28608;&#27963;&#20989;&#25968;&#30340;LLM&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;ReLU, SwiGLU, ReGLU &#21644; ReLU^2&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#37319;&#29992;ReLU^2&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse computation offers a compelling solution for the inference of Large Language Models (LLMs) in low-resource scenarios by dynamically skipping the computation of inactive neurons. While traditional approaches focus on ReLU-based LLMs, leveraging zeros in activation values, we broaden the scope of sparse LLMs beyond zero activation values. We introduce a general method that defines neuron activation through neuron output magnitudes and a tailored magnitude threshold, demonstrating that non-ReLU LLMs also exhibit sparse activation. To find the most efficient activation function for sparse computation, we propose a systematic framework to examine the sparsity of LLMs from three aspects: the trade-off between sparsity and performance, the predictivity of sparsity, and the hardware affinity. We conduct thorough experiments on LLMs utilizing different activation functions, including ReLU, SwiGLU, ReGLU, and ReLU$^2$. The results indicate that models employing ReLU$^2$ excel across all t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20154;&#33080;&#26816;&#27979;&#39046;&#22495;&#30340;&#29616;&#29366;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25351;&#20986;&#20102;&#20854;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#30740;&#31350;&#26041;&#21521;&#65292;&#20197;&#25552;&#39640;&#20154;&#33080;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.03796</link><description>&lt;p&gt;
&#20154;&#33080;&#26816;&#27979;&#65306;&#29616;&#29366;&#19982;&#30740;&#31350;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Face Detection: Present State and Research Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20154;&#33080;&#26816;&#27979;&#39046;&#22495;&#30340;&#29616;&#29366;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25351;&#20986;&#20102;&#20854;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#30740;&#31350;&#26041;&#21521;&#65292;&#20197;&#25552;&#39640;&#20154;&#33080;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#22788;&#29702;&#21253;&#21547;&#20154;&#31867;&#22270;&#20687;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#37117;&#20351;&#29992;&#20154;&#33080;&#26816;&#27979;&#20316;&#20026;&#26680;&#24515;&#32452;&#20214;&#12290;&#23613;&#31649;&#23545;&#35813;&#20027;&#39064;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#20154;&#33080;&#26816;&#27979;&#20173;&#28982;&#23384;&#22312;&#38382;&#39064;&#12290;&#20154;&#33080;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#20173;&#26377;&#24453;&#25552;&#39640;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#23637;&#31034;&#20102;&#22312;&#36825;&#19968;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#20197;&#21450;&#20173;&#38656;&#35299;&#20915;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#21487;&#20197;&#20316;&#20026;&#20154;&#33080;&#26816;&#27979;&#39046;&#22495;&#30740;&#31350;&#39033;&#30446;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The majority of computer vision applications that handle images featuring humans use face detection as a core component. Face detection still has issues, despite much research on the topic. Face detection's accuracy and speed might yet be increased. This review paper shows the progress made in this area as well as the substantial issues that still need to be tackled. The paper provides research directions that can be taken up as research projects in the field of face detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;/&#25110;&#21160;&#20316;&#31354;&#38388;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24724;&#20445;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#22312;Legendre&#22810;&#39033;&#24335;&#22522;&#30784;&#19978;&#26500;&#24314;MDP&#34920;&#31034;&#26469;&#35299;&#20915;$\nu-$&#24179;&#28369; MDPs&#65292;&#22312;&#36739;&#24369;&#30340;&#20551;&#35774;&#19979;&#21487;&#20197;&#36798;&#21040;&#26080;&#24724;&#29305;&#24615;&#65292;&#21516;&#26102;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.03792</link><description>&lt;p&gt;
&#36830;&#32493;&#29366;&#24577;&#21644;/&#25110;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#26080;&#24724;&#24378;&#21270;&#23398;&#20064;&#22312;&#24179;&#28369;MDPs&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
No-Regret Reinforcement Learning in Smooth MDPs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;/&#25110;&#21160;&#20316;&#31354;&#38388;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24724;&#20445;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#22312;Legendre&#22810;&#39033;&#24335;&#22522;&#30784;&#19978;&#26500;&#24314;MDP&#34920;&#31034;&#26469;&#35299;&#20915;$\nu-$&#24179;&#28369; MDPs&#65292;&#22312;&#36739;&#24369;&#30340;&#20551;&#35774;&#19979;&#21487;&#20197;&#36798;&#21040;&#26080;&#24724;&#29305;&#24615;&#65292;&#21516;&#26102;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;/&#25110;&#21160;&#20316;&#31354;&#38388;&#30340;&#38382;&#39064;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#22914;&#20309;&#33719;&#24471;&#26080;&#24724;&#20445;&#35777;&#20173;&#28982;&#26159;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#38500;&#38750;&#26159;&#38750;&#24120;&#29305;&#23450;&#30340;&#24773;&#26223;&#65292;&#21542;&#21017;&#36825;&#20010;&#26222;&#36941;&#38382;&#39064;&#20173;&#26410;&#35299;&#20915;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#36827;&#34892;&#32467;&#26500;&#20551;&#35774;&#30340;&#26041;&#27861;&#65292;&#21363;$\nu-$&#24179;&#28369;&#24615;&#65292;&#35813;&#26041;&#27861;&#25512;&#24191;&#20102;&#30446;&#21069;&#25552;&#20986;&#30340;&#22810;&#31181;&#35774;&#32622;&#65288;&#22914;&#32447;&#24615;MDPs&#21644;Lipschitz MDPs&#65289;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#26469;&#22312;$\nu-$&#24179;&#28369; MDPs&#20013;&#36827;&#34892;&#24724;&#24680;&#26368;&#23567;&#21270;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#37117;&#22522;&#20110;&#36890;&#36807;&#22522;&#20110;Legendre&#22810;&#39033;&#24335;&#30340;&#27491;&#20132;&#29305;&#24449;&#26144;&#23556;&#26500;&#24314;MDP&#34920;&#31034;&#30340;&#24605;&#24819;&#12290;&#31532;&#19968;&#20010;&#31639;&#27861;\textsc{Legendre-Eleanor}&#22312;&#36739;&#24369;&#30340;&#20551;&#35774;&#19979;&#21487;&#20197;&#36798;&#21040;&#26080;&#24724;&#29305;&#24615;&#65292;&#20294;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#65292;&#32780;&#31532;&#20108;&#20010;\textsc{Legendre-LSVI}&#31639;&#27861;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#65292;
&lt;/p&gt;
&lt;p&gt;
Obtaining no-regret guarantees for reinforcement learning (RL) in the case of problems with continuous state and/or action spaces is still one of the major open challenges in the field. Recently, a variety of solutions have been proposed, but besides very specific settings, the general problem remains unsolved. In this paper, we introduce a novel structural assumption on the Markov decision processes (MDPs), namely $\nu-$smoothness, that generalizes most of the settings proposed so far (e.g., linear MDPs and Lipschitz MDPs). To face this challenging scenario, we propose two algorithms for regret minimization in $\nu-$smooth MDPs. Both algorithms build upon the idea of constructing an MDP representation through an orthogonal feature map based on Legendre polynomials. The first algorithm, \textsc{Legendre-Eleanor}, archives the no-regret property under weaker assumptions but is computationally inefficient, whereas the second one, \textsc{Legendre-LSVI}, runs in polynomial time, although 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#30693;&#35782;-&#25968;&#25454;&#23545;&#40784;&#30340;&#26694;&#26550;KDAlign&#65292;&#29992;&#20110;&#24369;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#12290;&#36825;&#20010;&#26694;&#26550;&#23558;&#20154;&#24037;&#19987;&#23478;&#24635;&#32467;&#30340;&#35268;&#21017;&#30693;&#35782;&#19982;&#26377;&#38480;&#30340;&#24102;&#26631;&#31614;&#25968;&#25454;&#38598;&#25104;&#65292;&#36890;&#36807;&#23545;&#40784;&#30693;&#35782;&#21644;&#25968;&#25454;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03785</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;-&#25968;&#25454;&#23545;&#40784;&#36827;&#34892;&#24369;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Anomaly Detection via Knowledge-Data Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#30693;&#35782;-&#25968;&#25454;&#23545;&#40784;&#30340;&#26694;&#26550;KDAlign&#65292;&#29992;&#20110;&#24369;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#12290;&#36825;&#20010;&#26694;&#26550;&#23558;&#20154;&#24037;&#19987;&#23478;&#24635;&#32467;&#30340;&#35268;&#21017;&#30693;&#35782;&#19982;&#26377;&#38480;&#30340;&#24102;&#26631;&#31614;&#25968;&#25454;&#38598;&#25104;&#65292;&#36890;&#36807;&#23545;&#40784;&#30693;&#35782;&#21644;&#25968;&#25454;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#35768;&#22810;&#22522;&#20110;web&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#21253;&#25324;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#12289;&#21453;&#27927;&#38065;&#12289;&#35774;&#22791;&#25925;&#38556;&#26816;&#27979;&#21644;&#32593;&#32476;&#25925;&#38556;&#20998;&#26512;&#12290;&#22823;&#22810;&#25968;&#20381;&#36182;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#30001;&#20110;&#32570;&#20047;&#26631;&#31614;&#32780;&#38590;&#20197;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#26816;&#27979;&#31934;&#24230;&#12290;&#24369;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979; (WSAD) &#24341;&#20837;&#20102;&#26377;&#38480;&#25968;&#37327;&#30340;&#24102;&#26631;&#31614;&#24322;&#24120;&#26679;&#26412;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22312;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#27867;&#21270;&#21040;&#26410;&#35265;&#24322;&#24120;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;Knowledge-Data Alignment (KDAlign)&#65292;&#23558;&#36890;&#24120;&#30001;&#20154;&#24037;&#19987;&#23478;&#24635;&#32467;&#30340;&#35268;&#21017;&#30693;&#35782;&#38598;&#25104;&#21040;&#26377;&#38480;&#30340;&#24102;&#26631;&#31614;&#25968;&#25454;&#20013;&#20197;&#34917;&#20805;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#35268;&#21017;&#36716;&#21270;&#20026;&#30693;&#35782;&#31354;&#38388;&#65292;&#24182;&#23558;&#30693;&#35782;&#19982;&#25968;&#25454;&#30340;&#34701;&#21512;&#37325;&#26032;&#26144;&#23556;&#20026;&#30693;&#35782;&#21644;&#25968;&#25454;&#30340;&#23545;&#40784;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#31181;&#23545;&#40784;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection (AD) plays a pivotal role in numerous web-based applications, including malware detection, anti-money laundering, device failure detection, and network fault analysis. Most methods, which rely on unsupervised learning, are hard to reach satisfactory detection accuracy due to the lack of labels. Weakly Supervised Anomaly Detection (WSAD) has been introduced with a limited number of labeled anomaly samples to enhance model performance. Nevertheless, it is still challenging for models, trained on an inadequate amount of labeled data, to generalize to unseen anomalies. In this paper, we introduce a novel framework Knowledge-Data Alignment (KDAlign) to integrate rule knowledge, typically summarized by human experts, to supplement the limited labeled data. Specifically, we transpose these rules into the knowledge space and subsequently recast the incorporation of knowledge as the alignment of knowledge and data. To facilitate this alignment, we employ the Optimal Transport 
&lt;/p&gt;</description></item><item><title>AirPhyNet&#26159;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31354;&#27668;&#39063;&#31890;&#36816;&#21160;&#30340;&#29289;&#29702;&#21407;&#29702;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#23545;&#31354;&#27668;&#36136;&#37327;&#30340;&#39044;&#27979;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03784</link><description>&lt;p&gt;
AirPhyNet: &#21033;&#29992;&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#31354;&#27668;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
AirPhyNet: Harnessing Physics-Guided Neural Networks for Air Quality Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03784
&lt;/p&gt;
&lt;p&gt;
AirPhyNet&#26159;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#24341;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31354;&#27668;&#39063;&#31890;&#36816;&#21160;&#30340;&#29289;&#29702;&#21407;&#29702;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#23545;&#31354;&#27668;&#36136;&#37327;&#30340;&#39044;&#27979;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#21644;&#24314;&#27169;&#22312;&#20844;&#20849;&#21355;&#29983;&#21644;&#29615;&#22659;&#31649;&#29702;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#24110;&#21161;&#20010;&#20154;&#21644;&#24403;&#23616;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#38271;&#26399;&#39044;&#27979;&#31934;&#24230;&#19978;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#31232;&#30095;&#25110;&#19981;&#23436;&#25972;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#24448;&#24448;&#20381;&#36182;&#20110;&#32570;&#20047;&#22362;&#23454;&#29289;&#29702;&#22522;&#30784;&#30340;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#65292;&#23548;&#33268;&#39044;&#27979;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#38477;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Physics guided Neural Network for Air Quality Prediction&#65288;AirPhyNet&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#31354;&#27668;&#39063;&#31890;&#36816;&#21160;&#30340;&#20004;&#20010;&#25104;&#29087;&#30340;&#29289;&#29702;&#21407;&#29702;&#65288;&#25193;&#25955;&#21644;&#24179;&#27969;&#65289;&#23558;&#20854;&#34920;&#31034;&#20026;&#24494;&#20998;&#26041;&#31243;&#32593;&#32476;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22270;&#32467;&#26500;&#23558;&#29289;&#29702;&#30693;&#35782;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#21033;&#29992;&#28508;&#22312;&#34920;&#31034;&#26469;&#25429;&#25417;&#26102;&#31354;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Air quality prediction and modelling plays a pivotal role in public health and environment management, for individuals and authorities to make informed decisions. Although traditional data-driven models have shown promise in this domain, their long-term prediction accuracy can be limited, especially in scenarios with sparse or incomplete data and they often rely on black-box deep learning structures that lack solid physical foundation leading to reduced transparency and interpretability in predictions. To address these limitations, this paper presents a novel approach named Physics guided Neural Network for Air Quality Prediction (AirPhyNet). Specifically, we leverage two well-established physics principles of air particle movement (diffusion and advection) by representing them as differential equation networks. Then, we utilize a graph structure to integrate physics knowledge into a neural network architecture and exploit latent representations to capture spatio-temporal relationships
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;MolTC&#65292;&#29992;&#20110;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#38142;&#29702;&#35770;&#23454;&#29616;&#32479;&#19968;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.03781</link><description>&lt;p&gt;
MolTC: &#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#20998;&#23376;&#20851;&#31995;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MolTC: Towards Molecular Relational Modeling In Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;MolTC&#65292;&#29992;&#20110;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#38142;&#29702;&#35770;&#23454;&#29616;&#32479;&#19968;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#65288;MRL&#65289;&#26088;&#22312;&#29702;&#35299;&#20998;&#23376;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22312;&#25512;&#36827;&#29983;&#29289;&#21270;&#23398;&#30740;&#31350;&#26041;&#38754;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37319;&#29992;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;MRL&#26041;&#27861;&#65292;&#36825;&#20123;&#27169;&#22411;&#20197;&#20854;&#24222;&#22823;&#30340;&#30693;&#35782;&#23384;&#20648;&#24211;&#21644;&#20808;&#36827;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#25991;&#26412;&#25968;&#25454;&#65292;&#22240;&#27492;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20998;&#23376;&#22270;&#20013;&#22266;&#26377;&#30340;&#20016;&#23500;&#32467;&#26500;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#32479;&#19968;&#30340;&#26694;&#26550;&#21152;&#21095;&#20102;&#20449;&#24687;&#30340;&#28010;&#36153;&#65292;&#22240;&#20026;&#23427;&#38459;&#30861;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#20849;&#20139;&#23398;&#20064;&#21040;&#30340;&#30456;&#20114;&#20316;&#29992;&#29702;&#30001;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#29992;&#20110;&#26681;&#25454;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#29702;&#35770;&#23545;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#39044;&#27979;&#65292;&#31216;&#20026;MolTC&#65292;&#23427;&#21487;&#20197;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research. Recently, the adoption of large language models (LLMs), known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL. Despite their potential, these methods predominantly rely on the textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs. Moreover, the absence of a unified framework exacerbates the information underutilization, as it hinders the sharing of interaction rationale learned across diverse datasets. To address these challenges, this work proposes a novel LLM-based multi-modal framework for Molecular inTeraction prediction following Chain-of-Thought (CoT) theory, termed MolTC, which can efficiently integrate rich graphical information of molecular pairs. For achieving a unified MRL, MolT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23459;&#20256;&#35821;&#35328;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;PPN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#23459;&#20256;&#26032;&#38395;&#21644;&#24120;&#35268;&#26032;&#38395;&#12290;&#30740;&#31350;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#26377;&#20851;&#25991;&#20307;&#32447;&#32034;&#30340;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.03780</link><description>&lt;p&gt;
&#25581;&#31034;&#23459;&#20256;&#65306;&#22522;&#20110;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23459;&#20256;&#35821;&#35328;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;PPN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#23459;&#20256;&#26032;&#38395;&#21644;&#24120;&#35268;&#26032;&#38395;&#12290;&#30740;&#31350;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#26377;&#20851;&#25991;&#20307;&#32447;&#32034;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23459;&#20256;&#35821;&#35328;&#21450;&#20854;&#25991;&#20307;&#29305;&#24449;&#12290;&#25552;&#20986;&#20102;PPN&#25968;&#25454;&#38598;&#65292;&#21363;&#23459;&#20256;&#24615;&#20266;&#26032;&#38395;&#25968;&#25454;&#38598;&#65292;&#23427;&#26159;&#19968;&#31181;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#65292;&#30001;&#19987;&#23478;&#26426;&#26500;&#30830;&#23450;&#30340;&#23459;&#20256;&#26469;&#28304;&#32593;&#31449;&#19978;&#30340;&#26032;&#38395;&#25991;&#31456;&#32452;&#25104;&#12290;&#20174;&#35813;&#25968;&#25454;&#38598;&#20013;&#38543;&#26426;&#36873;&#25321;&#20102;&#19968;&#37096;&#20998;&#26679;&#26412;&#19982;&#26469;&#33258;&#24120;&#35268;&#27861;&#22269;&#26032;&#38395;&#30340;&#25991;&#31456;&#28151;&#21512;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;URL&#36827;&#34892;&#20102;&#25513;&#30422;&#65292;&#20197;&#36827;&#34892;&#20154;&#31867;&#27880;&#37322;&#23454;&#39564;&#65292;&#20351;&#29992;11&#20010;&#19981;&#21516;&#30340;&#26631;&#31614;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#20004;&#31181;&#31867;&#22411;&#30340;&#26032;&#38395;&#32440;&#23545;&#27599;&#20010;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26469;&#35782;&#21035;&#27880;&#37322;&#32773;&#20351;&#29992;&#30340;&#32447;&#32034;&#65292;&#24182;&#19982;&#26426;&#22120;&#20998;&#31867;&#36827;&#34892;&#27604;&#36739;&#12290;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;VAGO&#20998;&#26512;&#22120;&#36827;&#34892;&#36766;&#36848;&#27169;&#31946;&#21644;&#20027;&#35266;&#24615;&#30340;&#27979;&#37327;&#65292;&#20351;&#29992;TF-IDF&#20316;&#20026;&#22522;&#20934;&#65292;&#20197;&#21450;&#22235;&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#65306;&#20004;&#20010;&#22522;&#20110;RoBERTa&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#21477;&#27861;&#30340;CATS&#65292;&#20197;&#21450;&#32467;&#21512;&#21477;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;&#30340;&#19968;&#20010;XGBoost&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the language of propaganda and its stylistic features. It presents the PPN dataset, standing for Propagandist Pseudo-News, a multisource, multilingual, multimodal dataset composed of news articles extracted from websites identified as propaganda sources by expert agencies. A limited sample from this set was randomly mixed with papers from the regular French press, and their URL masked, to conduct an annotation-experiment by humans, using 11 distinct labels. The results show that human annotators were able to reliably discriminate between the two types of press across each of the labels. We propose different NLP techniques to identify the cues used by the annotators, and to compare them with machine classification. They include the analyzer VAGO to measure discourse vagueness and subjectivity, a TF-IDF to serve as a baseline, and four different classifiers: two RoBERTa-based models, CATS using syntax, and one XGBoost combining syntactic and semantic features.   K
&lt;/p&gt;</description></item><item><title>EERO &#26159;&#19968;&#31181;&#26089;&#26399;&#36864;&#20986;&#19982;&#25298;&#32477;&#36873;&#39033;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#20998;&#31867;&#22120;&#26469;&#36873;&#25321;&#27599;&#20010;&#23454;&#20363;&#30340;&#36864;&#20986;&#22836;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#31649;&#29702;&#39044;&#31639;&#20998;&#37197;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03779</link><description>&lt;p&gt;
EERO: &#26089;&#26399;&#36864;&#20986;&#19982;&#25298;&#32477;&#36873;&#39033;&#29992;&#20110;&#26377;&#38480;&#39044;&#31639;&#19979;&#30340;&#39640;&#25928;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
EERO: Early Exit with Reject Option for Efficient Classification with limited budget
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03779
&lt;/p&gt;
&lt;p&gt;
EERO &#26159;&#19968;&#31181;&#26089;&#26399;&#36864;&#20986;&#19982;&#25298;&#32477;&#36873;&#39033;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#20998;&#31867;&#22120;&#26469;&#36873;&#25321;&#27599;&#20010;&#23454;&#20363;&#30340;&#36864;&#20986;&#22836;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#31649;&#29702;&#39044;&#31639;&#20998;&#37197;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19981;&#26029;&#22797;&#26434;&#21270;&#35201;&#27714;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#26377;&#25928;&#31649;&#29702;&#35745;&#31639;&#36164;&#28304;&#12290;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#26089;&#26399;&#36864;&#20986;&#31574;&#30053;&#65292;&#36890;&#36807;&#25552;&#20379;&#32553;&#30701;&#31616;&#21333;&#25968;&#25454;&#23454;&#20363;&#22788;&#29702;&#36335;&#24452;&#30340;&#26426;&#21046;&#65292;&#23454;&#29616;&#33258;&#36866;&#24212;&#35745;&#31639;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EERO&#65292;&#19968;&#31181;&#23558;&#26089;&#26399;&#36864;&#20986;&#38382;&#39064;&#36716;&#21270;&#20026;&#20351;&#29992;&#20855;&#26377;&#25298;&#32477;&#36873;&#39033;&#30340;&#22810;&#20010;&#20998;&#31867;&#22120;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#36873;&#25321;&#27599;&#20010;&#23454;&#20363;&#30340;&#36864;&#20986;&#22836;&#12290;&#25105;&#20204;&#20351;&#29992;&#25351;&#25968;&#26435;&#37325;&#32858;&#21512;&#26469;&#26657;&#20934;&#19981;&#21516;&#22836;&#37096;&#36864;&#20986;&#30340;&#27010;&#29575;&#65292;&#20197;&#20445;&#35777;&#19968;&#20010;&#22266;&#23450;&#30340;&#39044;&#31639;&#12290;&#25105;&#20204;&#32771;&#34385;&#36125;&#21494;&#26031;&#39118;&#38505;&#12289;&#39044;&#31639;&#32422;&#26463;&#21644;&#22836;&#37096;&#29305;&#23450;&#39044;&#31639;&#28040;&#32791;&#31561;&#22240;&#32032;&#12290;&#36890;&#36807;&#22312;Cifar&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;ResNet-18&#27169;&#22411;&#21644;ConvNext&#26550;&#26500;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#33021;&#26377;&#25928;&#31649;&#29702;&#39044;&#31639;&#20998;&#37197;&#65292;&#36824;&#33021;&#25552;&#39640;&#36807;&#24230;&#32771;&#34385;&#22330;&#26223;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing complexity of advanced machine learning models requires innovative approaches to manage computational resources effectively. One such method is the Early Exit strategy, which allows for adaptive computation by providing a mechanism to shorten the processing path for simpler data instances. In this paper, we propose EERO, a new methodology to translate the problem of early exiting to a problem of using multiple classifiers with reject option in order to better select the exiting head for each instance. We calibrate the probabilities of exiting at the different heads using aggregation with exponential weights to guarantee a fixed budget .We consider factors such as Bayesian risk, budget constraints, and head-specific budget consumption. Experimental results, conducted using a ResNet-18 model and a ConvNext architecture on Cifar and ImageNet datasets, demonstrate that our method not only effectively manages budget allocation but also enhances accuracy in overthinking scenar
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;MetaTree&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#32463;&#20856;&#31639;&#27861;&#30340;&#36755;&#20986;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#20855;&#26377;&#24378;&#22823;&#27010;&#25324;&#24615;&#33021;&#30340;&#20915;&#31574;&#26641;&#12290;</title><link>https://arxiv.org/abs/2402.03774</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#23398;&#20064;&#20915;&#31574;&#26641;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning a Decision Tree Algorithm with Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03774
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;MetaTree&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#32463;&#20856;&#31639;&#27861;&#30340;&#36755;&#20986;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#20855;&#26377;&#24378;&#22823;&#27010;&#25324;&#24615;&#33021;&#30340;&#20915;&#31574;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#22240;&#20854;&#21487;&#35299;&#37322;&#24615;&#21644;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#23454;&#29616;&#39640;&#39044;&#27979;&#24615;&#33021;&#32780;&#38395;&#21517;&#12290;&#20256;&#32479;&#19978;&#65292;&#20915;&#31574;&#26641;&#26159;&#36890;&#36807;&#36882;&#24402;&#31639;&#27861;&#26500;&#24314;&#30340;&#65292;&#22312;&#26641;&#30340;&#27599;&#20010;&#33410;&#28857;&#19978;&#23558;&#25968;&#25454;&#36827;&#34892;&#20998;&#21306;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#26368;&#20339;&#20998;&#21306;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#38024;&#23545;&#23616;&#37096;&#27573;&#20248;&#21270;&#30340;&#20915;&#31574;&#26641;&#21487;&#33021;&#26080;&#27861;&#24102;&#26469;&#20840;&#23616;&#27010;&#25324;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MetaTree&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#32463;&#20856;&#31639;&#27861;&#30340;&#36807;&#28388;&#36755;&#20986;&#26469;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#24378;&#22823;&#30340;&#20998;&#31867;&#20915;&#31574;&#26641;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#25311;&#21512;&#36138;&#23146;&#20915;&#31574;&#26641;&#21644;&#20248;&#21270;&#20915;&#31574;&#26641;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;MetaTree&#20135;&#29983;&#20855;&#26377;&#24378;&#22823;&#27010;&#25324;&#24615;&#33021;&#30340;&#20915;&#31574;&#26641;&#12290;&#36825;&#31181;&#35757;&#32451;&#20351;MetaTree&#19981;&#20165;&#21487;&#20197;&#27169;&#25311;&#36825;&#20123;&#31639;&#27861;&#65292;&#36824;&#21487;&#20197;&#26681;&#25454;&#19978;&#19979;&#25991;&#26234;&#33021;&#22320;&#35843;&#25972;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24378;&#30340;&#27010;&#25324;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees are renowned for their interpretability capability to achieve high predictive performance, especially on tabular data. Traditionally, they are constructed through recursive algorithms, where they partition the data at every node in a tree. However, identifying the best partition is challenging, as decision trees optimized for local segments may not bring global generalization. To address this, we introduce MetaTree, which trains a transformer-based model on filtered outputs from classical algorithms to produce strong decision trees for classification. Specifically, we fit both greedy decision trees and optimized decision trees on a large number of datasets. We then train MetaTree to produce the trees that achieve strong generalization performance. This training enables MetaTree to not only emulate these algorithms, but also to intelligently adapt its strategy according to the context, thereby achieving superior generalization performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24378;&#21270;&#23398;&#20064;&#26469;&#33258;&#25171;&#21253;&#22870;&#21169;&#30340;&#26032;&#38382;&#39064;&#65292;&#20854;&#20013;&#23398;&#20064;&#22120;&#21482;&#33021;&#33719;&#21462;&#24207;&#21015;&#30340;&#25171;&#21253;&#22870;&#21169;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25506;&#32034;&#26410;&#30693;&#30340;&#21363;&#26102;&#22870;&#21169;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03771</link><description>&lt;p&gt;
&#26469;&#33258;&#25171;&#21253;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#23454;&#20363;&#32423;&#22870;&#21169;&#37325;&#26032;&#20998;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Bagged Reward: A Transformer-based Approach for Instance-Level Reward Redistribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03771
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24378;&#21270;&#23398;&#20064;&#26469;&#33258;&#25171;&#21253;&#22870;&#21169;&#30340;&#26032;&#38382;&#39064;&#65292;&#20854;&#20013;&#23398;&#20064;&#22120;&#21482;&#33021;&#33719;&#21462;&#24207;&#21015;&#30340;&#25171;&#21253;&#22870;&#21169;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25506;&#32034;&#26410;&#30693;&#30340;&#21363;&#26102;&#22870;&#21169;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#27599;&#20010;&#21160;&#20316;&#30340;&#21363;&#26102;&#22870;&#21169;&#20449;&#21495;&#23558;&#20026;&#20195;&#29702;&#29983;&#25104;&#65292;&#20197;&#20415;&#20195;&#29702;&#23398;&#20064;&#22914;&#20309;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#20197;&#33719;&#21462;&#26368;&#20248;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20195;&#29702;&#26080;&#27861;&#33719;&#21462;&#21363;&#26102;&#22870;&#21169;&#20449;&#21495;&#12290;&#30456;&#21453;&#65292;&#23398;&#20064;&#22120;&#21482;&#22312;&#36335;&#24452;&#30340;&#32467;&#26463;&#22788;&#33719;&#21462;&#22870;&#21169;&#65292;&#20854;&#20013;&#36335;&#24452;&#30340;&#37096;&#20998;&#24207;&#21015;&#34987;&#23450;&#20041;&#20026;&#19968;&#20010;&#21253;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#22120;&#24517;&#39035;&#38754;&#23545;&#25506;&#32034;&#21253;&#20013;&#26410;&#30693;&#21363;&#26102;&#22870;&#21169;&#30340;&#26174;&#33879;&#22256;&#38590;&#65292;&#36825;&#19981;&#33021;&#36890;&#36807;&#29616;&#26377;&#26041;&#27861;&#35299;&#20915;&#65292;&#21253;&#25324;&#20165;&#32771;&#34385;&#23436;&#25972;&#36335;&#24452;&#24182;&#24573;&#30053;&#20869;&#37096;&#22870;&#21169;&#20998;&#24067;&#30340;&#36712;&#36857;&#26041;&#27861;&#12290;&#20026;&#20102;&#27491;&#24335;&#30740;&#31350;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#65292;&#31216;&#20026;&#26469;&#33258;&#25171;&#21253;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;Reinforcement Learning from Bagged Rewards&#65292;RLBR&#65289;&#65292;&#21482;&#33021;&#33719;&#21462;&#24207;&#21015;&#30340;&#25171;&#21253;&#22870;&#21169;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#65292;&#24314;&#31435;&#20102;RLBR&#19982;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement Learning (RL), an instant reward signal is generated for each action of the agent, such that the agent learns to maximize the cumulative reward to obtain the optimal policy. However, in many real-world applications, the instant reward signals are not obtainable by the agent. Instead, the learner only obtains rewards at the ends of bags, where a bag is defined as a partial sequence of a complete trajectory. In this situation, the learner has to face the significant difficulty of exploring the unknown instant rewards in the bags, which could not be addressed by existing approaches, including those trajectory-based approaches that consider only complete trajectories and ignore the inner reward distributions. To formally study this situation, we introduce a novel RL setting termed Reinforcement Learning from Bagged Rewards (RLBR), where only the bagged rewards of sequences can be obtained. We provide the theoretical study to establish the connection between RLBR and standa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#21487;&#21464;&#38271;&#24230;&#32534;&#30721;&#21487;&#20197;&#26377;&#25928;&#21387;&#32553;&#36890;&#20449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Fed-CVLC&#65292;&#21487;&#20197;&#26681;&#25454;&#27169;&#22411;&#26356;&#26032;&#30340;&#21160;&#24577;&#36827;&#34892;&#20195;&#30721;&#38271;&#24230;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2402.03770</link><description>&lt;p&gt;
Fed-CVLC:&#20351;&#29992;&#21487;&#21464;&#38271;&#24230;&#32534;&#30721;&#21387;&#32553;&#32852;&#37030;&#23398;&#20064;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Fed-CVLC: Compressing Federated Learning Communications with Variable-Length Codes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03770
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#21487;&#21464;&#38271;&#24230;&#32534;&#30721;&#21487;&#20197;&#26377;&#25928;&#21387;&#32553;&#36890;&#20449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Fed-CVLC&#65292;&#21487;&#20197;&#26681;&#25454;&#27169;&#22411;&#26356;&#26032;&#30340;&#21160;&#24577;&#36827;&#34892;&#20195;&#30721;&#38271;&#24230;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#33539; paradigm &#19979;&#65292;&#19968;&#20010;&#21442;&#25968;&#26381;&#21153;&#22120;&#65288;PS&#65289;&#21516;&#26102;&#19982;&#20998;&#24067;&#24335;&#21442;&#19982;&#30340;&#23458;&#25143;&#31471;&#36827;&#34892;&#36890;&#20449;&#65292;&#36827;&#34892;&#27169;&#22411;&#25910;&#38598;&#12289;&#26356;&#26032;&#32858;&#21512;&#21644;&#27169;&#22411;&#20998;&#21457;&#65292;&#21516;&#26102;&#19981;&#25509;&#35302;&#20010;&#21035;&#23458;&#25143;&#31471;&#25317;&#26377;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;FL &#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#26041;&#38754;&#20855;&#26377;&#21560;&#24341;&#21147;&#65307;&#28982;&#32780;&#65292;PS &#19982;&#20998;&#25955;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#36890;&#20449;&#21487;&#33021;&#25104;&#20026;&#20005;&#37325;&#30340;&#29942;&#39048;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#27169;&#22411;&#21387;&#32553;&#31639;&#27861;&#65292;&#22914;&#37327;&#21270;&#21644;&#31232;&#30095;&#21270;&#65292;&#20294;&#23427;&#20204;&#19968;&#33324;&#20551;&#35774;&#20102;&#22266;&#23450;&#30340;&#20195;&#30721;&#38271;&#24230;&#65292;&#36825;&#19981;&#21453;&#26144;&#27169;&#22411;&#26356;&#26032;&#30340;&#24322;&#36136;&#24615;&#21644;&#21487;&#21464;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312; FL &#20013;&#21487;&#21464;&#38271;&#24230;&#23545;&#20110;&#21387;&#32553;&#26159;&#26377;&#30410;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Fed-CVLC&#65288;&#24102;&#26377;&#21487;&#21464;&#38271;&#24230;&#32534;&#30721;&#30340;&#32852;&#37030;&#23398;&#20064;&#21387;&#32553;&#65289;&#65292;&#23427;&#26681;&#25454;&#27169;&#22411;&#26356;&#26032;&#30340;&#21160;&#24577;&#23545;&#20195;&#30721;&#38271;&#24230;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26368;&#20248;&#30340;&#35843;&#25972;&#31574;&#30053;&#65292;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#65288;&#31561;&#20215;&#20110; ...
&lt;/p&gt;
&lt;p&gt;
In Federated Learning (FL) paradigm, a parameter server (PS) concurrently communicates with distributed participating clients for model collection, update aggregation, and model distribution over multiple rounds, without touching private data owned by individual clients. FL is appealing in preserving data privacy; yet the communication between the PS and scattered clients can be a severe bottleneck. Model compression algorithms, such as quantization and sparsification, have been suggested but they generally assume a fixed code length, which does not reflect the heterogeneity and variability of model updates. In this paper, through both analysis and experiments, we show strong evidences that variable-length is beneficial for compression in FL. We accordingly present Fed-CVLC (Federated Learning Compression with Variable-Length Codes), which fine-tunes the code length in response of the dynamics of model updates. We develop optimal tuning strategy that minimizes the loss function (equiva
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#20004;&#31181;&#27169;&#22411;&#65292;&#29992;&#20110;&#26657;&#27491;&#21644;&#20998;&#31163;&#39640;&#20809;&#35889;&#22270;&#20687;&#20013;&#30340;&#33639;&#20809;&#29289;&#36136;&#12290;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#32771;&#34385;&#38750;&#32447;&#24615;&#25928;&#24212;&#65292;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#20016;&#24230;&#20272;&#35745;&#65292;&#20026;&#33041;&#32959;&#30244;&#25163;&#26415;&#25552;&#20379;&#25913;&#36827;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03761</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39640;&#20809;&#35889;&#22270;&#20687;&#26657;&#27491;&#19982;&#20998;&#31163;&#22312;&#33041;&#32959;&#30244;&#25163;&#26415;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-Based Correction and Unmixing of Hyperspectral Images for Brain Tumor Surgery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#20004;&#31181;&#27169;&#22411;&#65292;&#29992;&#20110;&#26657;&#27491;&#21644;&#20998;&#31163;&#39640;&#20809;&#35889;&#22270;&#20687;&#20013;&#30340;&#33639;&#20809;&#29289;&#36136;&#12290;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#32771;&#34385;&#38750;&#32447;&#24615;&#25928;&#24212;&#65292;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#20016;&#24230;&#20272;&#35745;&#65292;&#20026;&#33041;&#32959;&#30244;&#25163;&#26415;&#25552;&#20379;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20809;&#35889;&#25104;&#20687;&#65288;HSI&#65289;&#29992;&#20110;&#33639;&#20809;&#24341;&#23548;&#30340;&#33041;&#32959;&#30244;&#20999;&#38500;&#33021;&#22815;&#20351;&#20154;&#20204;&#33021;&#22815;&#30475;&#21040;&#19981;&#21516;&#32452;&#32455;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#32780;&#36825;&#20123;&#24046;&#24322;&#23545;&#20154;&#30524;&#26469;&#35828;&#26159;&#26080;&#27861;&#36776;&#21035;&#30340;&#12290;&#36825;&#31181;&#22686;&#24378;&#25216;&#26415;&#21487;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#20999;&#38500;&#33041;&#32959;&#30244;&#65292;&#25552;&#39640;&#24739;&#32773;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;HSI&#20013;&#30340;&#35768;&#22810;&#22788;&#29702;&#26041;&#27861;&#20351;&#29992;&#20102;&#31616;&#21270;&#30340;&#32447;&#24615;&#26041;&#27861;&#65292;&#26080;&#27861;&#25429;&#25417;&#21040;&#38750;&#32447;&#24615;&#12289;&#27874;&#38271;&#20381;&#36182;&#24615;&#30340;&#29616;&#35937;&#65292;&#36825;&#20123;&#29616;&#35937;&#22312;&#20934;&#30830;&#24674;&#22797;&#33639;&#20809;&#29289;&#36136;&#20016;&#24230;&#26102;&#24517;&#39035;&#36827;&#34892;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#26657;&#27491;&#21644;&#20998;&#31163;&#65292;&#21487;&#20197;&#32771;&#34385;&#38750;&#32447;&#24615;&#25928;&#24212;&#65292;&#24182;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#20016;&#24230;&#20272;&#35745;&#12290;&#36825;&#20004;&#20010;&#27169;&#22411;&#37117;&#20351;&#29992;&#31867;&#20284;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#32467;&#26500;&#26469;&#22788;&#29702;&#33719;&#21462;&#30340;&#20809;&#35889;&#12290;&#20854;&#20013;&#19968;&#20010;&#27169;&#22411;&#20351;&#29992;&#30416;&#37240;&#21343;&#21833;&#65288;PpIX&#65289;&#27987;&#24230;&#26631;&#35760;&#36827;&#34892;&#35757;&#32451;&#65292;&#21478;&#19968;&#20010;&#27169;&#22411;&#32463;&#21382;&#21322;&#30417;&#30563;&#35757;&#32451;&#65292;&#39318;&#20808;&#26080;&#30417;&#30563;&#23398;&#20064;&#39640;&#20809;&#35889;&#20998;&#31163;&#65292;&#28982;&#21518;&#23398;&#20064;&#20351;&#29992;&#20855;&#26377;&#24322;&#36136;&#20809;&#23398;&#21644;&#20960;&#20309;&#29305;&#24615;&#30340;&#33639;&#20809;&#21457;&#23556;&#20809;&#35889;&#36827;&#34892;&#26657;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperspectral Imaging (HSI) for fluorescence-guided brain tumor resection enables visualization of differences between tissues that are not distinguishable to humans. This augmentation can maximize brain tumor resection, improving patient outcomes. However, much of the processing in HSI uses simplified linear methods that are unable to capture the non-linear, wavelength-dependent phenomena that must be modeled for accurate recovery of fluorophore abundances. We therefore propose two deep learning models for correction and unmixing, which can account for the nonlinear effects and produce more accurate estimates of abundances. Both models use an autoencoder-like architecture to process the captured spectra. One is trained with protoporphyrin IX (PpIX) concentration labels. The other undergoes semi-supervised training, first learning hyperspectral unmixing self-supervised and then learning to correct fluorescence emission spectra for heterogeneous optical and geometric properties using a 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#34394;&#20551;&#22270;&#20687;&#20250;&#23548;&#33268;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35780;&#20272;&#24187;&#35273;&#31243;&#24230;&#30340;&#22522;&#20934;CorrelationQA&#65292;&#24182;&#21457;&#29616;&#20027;&#27969;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26222;&#36941;&#21463;&#21040;&#36825;&#31181;&#26412;&#33021;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.03757</link><description>&lt;p&gt;
&#26412;&#33021;&#20559;&#35265;&#65306;&#34394;&#20551;&#22270;&#20687;&#23548;&#33268;MLLMs&#20135;&#29983;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#34394;&#20551;&#22270;&#20687;&#20250;&#23548;&#33268;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35780;&#20272;&#24187;&#35273;&#31243;&#24230;&#30340;&#22522;&#20934;CorrelationQA&#65292;&#24182;&#21457;&#29616;&#20027;&#27969;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26222;&#36941;&#21463;&#21040;&#36825;&#31181;&#26412;&#33021;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#20986;&#29616;&#20351;LLMs&#20855;&#22791;&#20102;&#35270;&#35273;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20687;GPT-4V&#36825;&#26679;&#24378;&#22823;&#30340;MLLMs&#22312;&#38754;&#23545;&#26576;&#20123;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#26102;&#20173;&#28982;&#20197;&#24778;&#20154;&#30340;&#26041;&#24335;&#22833;&#36133;&#20102;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31867;&#20856;&#22411;&#36755;&#20837;&#65292;&#36825;&#20123;&#36755;&#20837;&#20196;MLLMs&#22256;&#24785;&#65292;&#23427;&#20204;&#30001;&#39640;&#24230;&#30456;&#20851;&#20294;&#19982;&#31572;&#26696;&#19981;&#19968;&#33268;&#30340;&#22270;&#20687;&#32452;&#25104;&#65292;&#23548;&#33268;MLLMs&#20135;&#29983;&#24187;&#35273;&#12290;&#20026;&#20102;&#37327;&#21270;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CorrelationQA&#65292;&#36825;&#26159;&#39318;&#20010;&#35780;&#20272;&#32473;&#23450;&#34394;&#20551;&#22270;&#20687;&#30340;&#24187;&#35273;&#31243;&#24230;&#30340;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#21253;&#21547;13&#20010;&#31867;&#21035;&#30340;7,308&#20010;&#25991;&#26412;-&#22270;&#20687;&#23545;&#12290;&#22522;&#20110;&#25552;&#20986;&#30340;CorrelationQA&#65292;&#25105;&#20204;&#23545;9&#20010;&#20027;&#27969;MLLMs&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#34920;&#26126;&#23427;&#20204;&#26222;&#36941;&#21463;&#21040;&#36825;&#31181;&#26412;&#33021;&#20559;&#35265;&#30340;&#19981;&#21516;&#31243;&#24230;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#31934;&#36873;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#32467;&#26524;&#33021;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently experienced remarkable progress, where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities, leading to impressive performances in various multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs. In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from hallucination. To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the hallucination level given spurious images. This benchmark contains 7,308 text-image pairs across 13 categories. Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation result
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#38598;&#20307;&#21464;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#37319;&#38598;&#21270;&#23398;&#30456;&#20851;&#25968;&#25454;&#28857;&#12289;&#20851;&#27880;&#27169;&#22411;&#39044;&#27979;&#26368;&#19981;&#30830;&#23450;&#30340;&#21306;&#22495;&#65292;&#36827;&#34892;&#20102;&#31283;&#20581;&#20998;&#23376;&#25968;&#25454;&#38598;&#30340;&#22686;&#24378;&#37319;&#26679;&#12290;</title><link>https://arxiv.org/abs/2402.03753</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#38598;&#20307;&#21464;&#37327;&#22686;&#24378;&#37319;&#26679;&#31283;&#20581;&#20998;&#23376;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Enhanced sampling of robust molecular datasets with uncertainty-based collective variables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#38598;&#20307;&#21464;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#37319;&#38598;&#21270;&#23398;&#30456;&#20851;&#25968;&#25454;&#28857;&#12289;&#20851;&#27880;&#27169;&#22411;&#39044;&#27979;&#26368;&#19981;&#30830;&#23450;&#30340;&#21306;&#22495;&#65292;&#36827;&#34892;&#20102;&#31283;&#20581;&#20998;&#23376;&#25968;&#25454;&#38598;&#30340;&#22686;&#24378;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#19968;&#20010;&#20195;&#34920;&#20998;&#23376;&#31995;&#32479;&#21487;&#35775;&#38382;&#26500;&#22411;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#24471;&#21040;&#30340;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#21183;&#20989;&#25968;(MLIP)&#30340;&#31283;&#20581;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20998;&#23376;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#65292;&#20854;&#28508;&#22312;&#33021;&#38754;(PESs)&#21576;&#29616;&#20986;&#20247;&#22810;&#23616;&#37096;&#26497;&#23567;&#20540;&#21644;&#33021;&#22418;&#65292;&#23548;&#33268;&#20102;&#19968;&#20010;&#24456;&#22823;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#22914;&#38543;&#26426;&#37319;&#26679;&#25110;&#20840;&#38754;&#25506;&#32034;&#65292;&#35201;&#20040;&#38590;&#20197;&#22788;&#29702;&#65292;&#35201;&#20040;&#26080;&#27861;&#25429;&#25417;&#21040;&#31232;&#26377;&#20294;&#39640;&#24230;&#20449;&#24687;&#21270;&#30340;&#26500;&#22411;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#20316;&#20026;&#38598;&#20307;&#21464;&#37327;(CV)&#24341;&#23548;&#37319;&#38598;&#21270;&#23398;&#30456;&#20851;&#25968;&#25454;&#28857;&#12289;&#20851;&#27880;ML&#27169;&#22411;&#39044;&#27979;&#26368;&#19981;&#30830;&#23450;&#21306;&#22495;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#20316;&#20026;&#26377;&#20559;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;CV&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20811;&#26381;&#33021;&#37327;&#30340;&#25928;&#26524;&#19978;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating a data set that is representative of the accessible configuration space of a molecular system is crucial for the robustness of machine learned interatomic potentials (MLIP). However, the complexity of molecular systems, characterized by intricate potential energy surfaces (PESs) with numerous local minima and energy barriers, presents a significant challenge. Traditional methods of data generation, such as random sampling or exhaustive exploration, are either intractable or may not capture rare, but highly informative configurations. In this study, we propose a method that leverages uncertainty as the collective variable (CV) to guide the acquisition of chemically-relevant data points, focusing on regions of the configuration space where ML model predictions are most uncertain. This approach employs a Gaussian Mixture Model-based uncertainty metric from a single model as the CV for biased molecular dynamics simulations. The effectiveness of our approach in overcoming energy 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#36731;&#37327;&#32423;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#26368;&#23567;&#32553;&#25918;&#22270;&#20687;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#25216;&#26415;&#23454;&#29616;&#20248;&#31168;&#24615;&#33021;&#65292;&#26080;&#38656;&#26174;&#33879;&#22686;&#21152;&#22270;&#20687;&#23610;&#23544;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#39640;&#25928;&#22788;&#29702;&#23567;&#25968;&#25454;&#38598;&#65292;&#36824;&#33021;&#26377;&#25928;&#22788;&#29702;&#25509;&#36817;&#21407;&#22987;&#23610;&#23544;&#30340;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2402.03752</link><description>&lt;p&gt;
&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#26368;&#23567;&#32553;&#25918;&#22270;&#20687;&#23545;&#36731;&#37327;&#32423;&#35270;&#35273;Transformer&#36827;&#34892;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Pre-training of Lightweight Vision Transformers on Small Datasets with Minimally Scaled Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#36731;&#37327;&#32423;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#26368;&#23567;&#32553;&#25918;&#22270;&#20687;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#25216;&#26415;&#23454;&#29616;&#20248;&#31168;&#24615;&#33021;&#65292;&#26080;&#38656;&#26174;&#33879;&#22686;&#21152;&#22270;&#20687;&#23610;&#23544;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#39640;&#25928;&#22788;&#29702;&#23567;&#25968;&#25454;&#38598;&#65292;&#36824;&#33021;&#26377;&#25928;&#22788;&#29702;&#25509;&#36817;&#21407;&#22987;&#23610;&#23544;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36731;&#37327;&#32423;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#33021;&#21542;&#22312;&#20998;&#36776;&#29575;&#36739;&#23567;&#30340;&#23567;&#25968;&#25454;&#38598;&#19978;&#19982;ResNet&#31561;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#24615;&#33021;&#30456;&#21305;&#25932;&#25110;&#36229;&#36234;&#65311;&#26412;&#25253;&#21578;&#36890;&#36807;&#20351;&#29992;&#26368;&#23567;&#22270;&#20687;&#32553;&#25918;&#30340;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#25216;&#26415;&#65292;&#35777;&#26126;&#20102;&#32431;ViT&#30830;&#23454;&#21487;&#20197;&#36890;&#36807;&#39044;&#35757;&#32451;&#23454;&#29616;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;CIFAR-10&#21644;CIFAR-100&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#25152;&#20351;&#29992;&#30340;ViT&#27169;&#22411;&#21442;&#25968;&#19981;&#36229;&#36807;365&#19975;&#20010;&#65292;&#24182;&#19988;&#20056;&#32047;&#21152;&#65288;MAC&#65289;&#35745;&#25968;&#20302;&#20110;0.27G&#65292;&#21487;&#20197;&#31216;&#20043;&#20026;&#8220;&#36731;&#37327;&#32423;&#8221;&#27169;&#22411;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#26174;&#33879;&#22686;&#21152;CIFAR-10&#21644;CIFAR-100&#22270;&#20687;&#23610;&#23544;&#30340;&#24773;&#20917;&#19979;&#65292;&#36798;&#21040;&#20102;&#31867;&#20284;&#36731;&#37327;&#32423;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#36825;&#19968;&#25104;&#23601;&#20984;&#26174;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#23427;&#19981;&#20165;&#21487;&#20197;&#22788;&#29702;&#23567;&#25968;&#25454;&#38598;&#65292;&#32780;&#19988;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#25509;&#36817;&#21407;&#22987;&#23610;&#23544;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can a lightweight Vision Transformer (ViT) match or exceed the performance of Convolutional Neural Networks (CNNs) like ResNet on small datasets with small image resolutions? This report demonstrates that a pure ViT can indeed achieve superior performance through pre-training, using a masked auto-encoder technique with minimal image scaling. Our experiments on the CIFAR-10 and CIFAR-100 datasets involved ViT models with fewer than 3.65 million parameters and a multiply-accumulate (MAC) count below 0.27G, qualifying them as 'lightweight' models. Unlike previous approaches, our method attains state-of-the-art performance among similar lightweight transformer-based architectures without significantly scaling up images from CIFAR-10 and CIFAR-100. This achievement underscores the efficiency of our model, not only in handling small datasets but also in effectively processing images close to their original scale.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#31227;&#21160;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#23545;&#40784;&#22270;&#21644;&#35774;&#35745;&#25193;&#24352;&#23545;&#40784;&#21367;&#31215;&#32593;&#32476;&#26469;&#25429;&#25417;&#20132;&#36890;&#22330;&#26223;&#20013;&#22797;&#26434;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#20026;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03750</link><description>&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#31227;&#21160;&#24615;&#24314;&#27169;&#65306;&#19968;&#31181;&#26102;&#31354;&#22270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Digital Twin Mobility Profiling: A Spatio-Temporal Graph Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#31227;&#21160;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#23545;&#40784;&#22270;&#21644;&#35774;&#35745;&#25193;&#24352;&#23545;&#40784;&#21367;&#31215;&#32593;&#32476;&#26469;&#25429;&#25417;&#20132;&#36890;&#22330;&#26223;&#20013;&#22797;&#26434;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#20026;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#25968;&#25454;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#31227;&#21160;&#24615;&#24314;&#27169;&#25104;&#20026;&#21033;&#29992;&#22823;&#37327;&#31227;&#21160;&#24615;&#25968;&#25454;&#21019;&#24314;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#21487;&#34892;&#26041;&#27861;&#12290;&#31227;&#21160;&#24615;&#24314;&#27169;&#21487;&#20197;&#20174;&#31227;&#21160;&#24615;&#25968;&#25454;&#20013;&#25552;&#21462;&#22478;&#24066;&#20132;&#36890;&#20013;&#30340;&#28508;&#22312;&#27169;&#24335;&#65292;&#26159;&#21508;&#31181;&#19982;&#20132;&#36890;&#30456;&#20851;&#30340;&#24212;&#29992;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#24615;&#39640;&#21644;&#25968;&#25454;&#37327;&#24040;&#22823;&#65292;&#31227;&#21160;&#24615;&#24314;&#27169;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#36890;&#36807;&#21019;&#24314;&#32593;&#32476;&#30340;&#34394;&#25311;&#34920;&#31034;&#26469;&#27169;&#25311;&#20854;&#34892;&#20026;&#65292;&#20026;&#25104;&#26412;&#26377;&#25928;&#21644;&#24615;&#33021;&#20248;&#21270;&#30340;&#31649;&#29702;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#20026;&#20102;&#25429;&#25417;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#22797;&#26434;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#25105;&#20204;&#26500;&#24314;&#23545;&#40784;&#22270;&#26469;&#23436;&#25104;&#26102;&#31354;&#30456;&#20851;&#34920;&#31034;&#65292;&#24182;&#35774;&#35745;&#25193;&#24352;&#23545;&#40784;&#21367;&#31215;&#32593;&#32476;&#65288;DACN&#65289;&#26469;&#23398;&#20064;&#31934;&#32454;&#30340;&#30456;&#20851;&#24615;&#65292;&#21363;&#26102;&#31354;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#31227;&#21160;&#24615;&#24314;&#27169;&#65288;DTMP&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the arrival of the big data era, mobility profiling has become a viable method of utilizing enormous amounts of mobility data to create an intelligent transportation system. Mobility profiling can extract potential patterns in urban traffic from mobility data and is critical for a variety of traffic-related applications. However, due to the high level of complexity and the huge amount of data, mobility profiling faces huge challenges. Digital Twin (DT) technology paves the way for cost-effective and performance-optimised management by digitally creating a virtual representation of the network to simulate its behaviour. In order to capture the complex spatio-temporal features in traffic scenario, we construct alignment diagrams to assist in completing the spatio-temporal correlation representation and design dilated alignment convolution network (DACN) to learn the fine-grained correlations, i.e., spatio-temporal interactions. We propose a digital twin mobility profiling (DTMP) fra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#19981;&#21464;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65288;ICNet&#65289;&#29992;&#20110;PDE&#30340;&#21457;&#29616;&#65292;&#36890;&#36807;&#36807;&#28388;&#25481;&#19981;&#33021;&#28385;&#36275;Galilean&#21464;&#25442;&#35201;&#27714;&#30340;&#20505;&#36873;&#39033;&#65292;&#23884;&#20837;&#22266;&#23450;&#21644;&#21487;&#33021;&#30340;&#39033;&#21040;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#26174;&#33879;&#25269;&#28040;&#20102;&#31232;&#30095;&#39640;&#22122;&#22768;&#25968;&#25454;&#30340;&#24433;&#21709;&#24182;&#36807;&#28388;&#25481;&#20887;&#20313;&#39033;&#12290;</title><link>https://arxiv.org/abs/2402.03747</link><description>&lt;p&gt;
PDE&#21457;&#29616;&#30340;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
An invariance constrained deep learning network for PDE discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#19981;&#21464;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65288;ICNet&#65289;&#29992;&#20110;PDE&#30340;&#21457;&#29616;&#65292;&#36890;&#36807;&#36807;&#28388;&#25481;&#19981;&#33021;&#28385;&#36275;Galilean&#21464;&#25442;&#35201;&#27714;&#30340;&#20505;&#36873;&#39033;&#65292;&#23884;&#20837;&#22266;&#23450;&#21644;&#21487;&#33021;&#30340;&#39033;&#21040;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#26174;&#33879;&#25269;&#28040;&#20102;&#31232;&#30095;&#39640;&#22122;&#22768;&#25968;&#25454;&#30340;&#24433;&#21709;&#24182;&#36807;&#28388;&#25481;&#20887;&#20313;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20174;&#31232;&#30095;&#25968;&#25454;&#21644;&#39640;&#22122;&#22768;&#20013;&#21457;&#29616;&#25511;&#21046;&#26041;&#31243;&#20173;&#28982;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#26159;&#30001;&#20110;&#23548;&#25968;&#35745;&#31639;&#30340;&#22256;&#38590;&#21644;&#22122;&#22768;&#30340;&#24178;&#25200;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#28385;&#36275;&#29289;&#29702;&#23450;&#24459;&#65292;&#20505;&#36873;&#24211;&#30340;&#36873;&#25321;&#21407;&#21017;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#19981;&#21464;&#24615;&#26159;&#25511;&#21046;&#26041;&#31243;&#30340;&#22522;&#26412;&#23450;&#24459;&#20043;&#19968;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32422;&#26463;&#19981;&#21464;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#65288;ICNet&#65289;&#29992;&#20110;PDE&#30340;&#21457;&#29616;&#12290;&#32771;&#34385;&#21040;&#26102;&#31354;&#24179;&#31227;&#19981;&#21464;&#24615;&#65288;Galilean&#19981;&#21464;&#24615;&#65289;&#26159;&#29289;&#29702;&#23450;&#24459;&#30340;&#22522;&#26412;&#23646;&#24615;&#65292;&#25105;&#20204;&#36807;&#28388;&#25481;&#19981;&#33021;&#28385;&#36275;Galilean&#21464;&#25442;&#35201;&#27714;&#30340;&#20505;&#36873;&#39033;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#22266;&#23450;&#21644;&#21487;&#33021;&#30340;&#39033;&#23884;&#20837;&#21040;&#31070;&#32463;&#32593;&#32476;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#65292;&#26174;&#33879;&#25269;&#28040;&#20102;&#31232;&#30095;&#39640;&#22122;&#22768;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#36807;&#28388;&#25481;&#20887;&#20313;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of partial differential equations (PDEs) from datasets has attracted increased attention. However, the discovery of governing equations from sparse data with high noise is still very challenging due to the difficulty of derivatives computation and the disturbance of noise. Moreover, the selection principles for the candidate library to meet physical laws need to be further studied. The invariance is one of the fundamental laws for governing equations. In this study, we propose an invariance constrained deep learning network (ICNet) for the discovery of PDEs. Considering that temporal and spatial translation invariance (Galilean invariance) is a fundamental property of physical laws, we filter the candidates that cannot meet the requirement of the Galilean transformations. Subsequently, we embedded the fixed and possible terms into the loss function of neural network, significantly countering the effect of sparse data with high noise. Then, by filtering out redundant terms
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#39318;&#27425;&#25581;&#31034;&#20102;&#25915;&#20987;&#32773;&#22312;&#22810;&#26234;&#33021;&#20307;&#31454;&#20105;&#29615;&#22659;&#20013;&#21363;&#20351;&#21463;&#38480;&#20110;&#21463;&#23475;&#32773;&#30340;&#37096;&#20998;&#35266;&#27979;&#20063;&#33021;&#29983;&#25104;&#23545;&#25239;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03741</link><description>&lt;p&gt;
SUB-PLAY&#65306;&#38024;&#23545;&#37096;&#20998;&#35266;&#27979;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#30340;&#23545;&#25239;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03741
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#39318;&#27425;&#25581;&#31034;&#20102;&#25915;&#20987;&#32773;&#22312;&#22810;&#26234;&#33021;&#20307;&#31454;&#20105;&#29615;&#22659;&#20013;&#21363;&#20351;&#21463;&#38480;&#20110;&#21463;&#23475;&#32773;&#30340;&#37096;&#20998;&#35266;&#27979;&#20063;&#33021;&#29983;&#25104;&#23545;&#25239;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#20026;&#26080;&#20154;&#26426;&#30340;&#32676;&#20307;&#25511;&#21046;&#12289;&#26426;&#26800;&#33218;&#30340;&#21327;&#20316;&#25805;&#32437;&#20197;&#21450;&#22810;&#30446;&#26631;&#21253;&#22260;&#31561;&#24320;&#36767;&#20102;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#22312;MARL&#37096;&#32626;&#36807;&#31243;&#20013;&#23384;&#22312;&#28508;&#22312;&#30340;&#23433;&#20840;&#23041;&#32961;&#38656;&#35201;&#26356;&#22810;&#20851;&#27880;&#21644;&#28145;&#20837;&#35843;&#26597;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36805;&#36895;&#21033;&#29992;&#21463;&#23475;&#32773;&#30340;&#28431;&#27934;&#29983;&#25104;&#23545;&#25239;&#31574;&#30053;&#65292;&#23548;&#33268;&#21463;&#23475;&#32773;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#22833;&#36133;&#12290;&#20363;&#22914;&#65292;&#23558;&#36229;&#20154;&#32423;&#21035;&#30340;&#22260;&#26827;AI&#30340;&#33719;&#32988;&#29575;&#38477;&#20302;&#21040;&#32422;20%&#12290;&#36825;&#20123;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20004;&#20154;&#31454;&#20105;&#29615;&#22659;&#65292;&#24182;&#20551;&#35774;&#25915;&#20987;&#32773;&#20855;&#26377;&#23436;&#25972;&#30340;&#20840;&#23616;&#29366;&#24577;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in multi-agent reinforcement learning (MARL) have opened up vast application prospects, including swarm control of drones, collaborative manipulation by robotic arms, and multi-target encirclement. However, potential security threats during the MARL deployment need more attention and thorough investigation. Recent researches reveal that an attacker can rapidly exploit the victim's vulnerabilities and generate adversarial policies, leading to the victim's failure in specific tasks. For example, reducing the winning rate of a superhuman-level Go AI to around 20%. They predominantly focus on two-player competitive environments, assuming attackers possess complete global state observation.   In this study, we unveil, for the first time, the capability of attackers to generate adversarial policies even when restricted to partial observations of the victims in multi-agent competitive environments. Specifically, we propose a novel black-box attack (SUB-PLAY), which incorporate
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BotSSCL&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#31038;&#20132;&#26426;&#22120;&#20154;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#26816;&#27979;&#22797;&#26434;&#26426;&#22120;&#20154;&#21644;&#20381;&#36182;&#31616;&#26131;&#29305;&#24449;&#19978;&#30340;&#23616;&#38480;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03740</link><description>&lt;p&gt;
BotSSCL: &#22522;&#20110;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
BotSSCL: Social Bot Detection with Self-Supervised Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03740
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BotSSCL&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#31038;&#20132;&#26426;&#22120;&#20154;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#26816;&#27979;&#22797;&#26434;&#26426;&#22120;&#20154;&#21644;&#20381;&#36182;&#31616;&#26131;&#29305;&#24449;&#19978;&#30340;&#23616;&#38480;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#65288;OSNs&#65289;&#26469;&#35828;&#65292;&#33258;&#21160;&#21270;&#36134;&#25143;&#65288;&#20063;&#31216;&#20026;&#8220;&#31038;&#20132;&#26426;&#22120;&#20154;&#8221;&#65289;&#30340;&#26816;&#27979;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26085;&#30410;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#31038;&#20132;&#26426;&#22120;&#20154;&#65292;&#20294;&#20173;&#23384;&#22312;&#26174;&#33879;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#39318;&#20808;&#65292;&#24403;&#21069;&#27169;&#22411;&#22312;&#26816;&#27979;&#26088;&#22312;&#27169;&#20223;&#30495;&#23454;OSN&#29992;&#25143;&#30340;&#22797;&#26434;&#26426;&#22120;&#20154;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20854;&#27425;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#23481;&#26131;&#21463;&#21040;&#25805;&#32437;&#30340;&#31616;&#26131;&#20010;&#20154;&#36164;&#26009;&#29305;&#24449;&#12290;&#38500;&#20102;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25805;&#32437;&#30340;&#33030;&#24369;&#24615;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#27867;&#21270;&#24615;&#65292;&#23548;&#33268;&#22312;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#21448;&#22312;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#26816;&#27979;&#30340;&#26032;&#26694;&#26550;&#65288;BotSSCL&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#21306;&#20998;&#31038;&#20132;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#65292;&#20174;&#32780;&#25552;&#39640;&#32447;&#24615;&#21487;&#20998;&#24615;&#12290;BotSSCL&#20135;&#29983;&#30340;&#39640;&#32423;&#34920;&#31034;&#22686;&#24378;&#20102;&#20854;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The detection of automated accounts, also known as "social bots", has been an increasingly important concern for online social networks (OSNs). While several methods have been proposed for detecting social bots, significant research gaps remain. First, current models exhibit limitations in detecting sophisticated bots that aim to mimic genuine OSN users. Second, these methods often rely on simplistic profile features, which are susceptible to manipulation. In addition to their vulnerability to adversarial manipulations, these models lack generalizability, resulting in subpar performance when trained on one dataset and tested on another.   To address these challenges, we propose a novel framework for social Bot detection with Self-Supervised Contrastive Learning (BotSSCL). Our framework leverages contrastive learning to distinguish between social bots and humans in the embedding space to improve linear separability. The high-level representations derived by BotSSCL enhance its resilienc
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;PrivateLASSO&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#38543;&#26426;&#19978;&#19979;&#25991;&#32447;&#24615;&#36172;&#33218;&#38382;&#39064;&#65292;&#24182;&#22312;&#24046;&#20998;&#38544;&#31169;&#30340;&#32422;&#26463;&#19979;&#35777;&#26126;&#20102;&#20854;&#38544;&#31169;&#21644;&#23454;&#29992;&#24615;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.03737</link><description>&lt;p&gt;
&#38754;&#21521;&#39640;&#32500;&#19978;&#19979;&#25991;&#32447;&#24615;&#36172;&#33218;&#38382;&#39064;&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private High Dimensional Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03737
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;PrivateLASSO&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#38543;&#26426;&#19978;&#19979;&#25991;&#32447;&#24615;&#36172;&#33218;&#38382;&#39064;&#65292;&#24182;&#22312;&#24046;&#20998;&#38544;&#31169;&#30340;&#32422;&#26463;&#19979;&#35777;&#26126;&#20102;&#20854;&#38544;&#31169;&#21644;&#23454;&#29992;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#21442;&#25968;&#21521;&#37327;&#20026;$s_{0}$-&#31232;&#30095;&#19988;&#20915;&#31574;&#21046;&#23450;&#32773;&#21463;&#21040;&#24046;&#20998;&#38544;&#31169;&#30340;&#20013;&#22830;&#21644;&#26412;&#22320;&#27169;&#22411;&#30340;&#32422;&#26463;&#19979;&#32771;&#34385;&#39640;&#32500;&#38543;&#26426;&#19978;&#19979;&#25991;&#32447;&#24615;&#36172;&#33218;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PrivateLASSO&#65292;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;LASSO&#36172;&#33218;&#31639;&#27861;&#12290;PrivateLASSO&#22522;&#20110;&#20004;&#20010;&#23376;&#31243;&#24207;&#65306;(i)&#22522;&#20110;&#31232;&#30095;&#30828;&#38408;&#20540;&#30340;&#38544;&#31169;&#26426;&#21046;&#21644;(ii)&#29992;&#20110;&#35782;&#21035;&#21442;&#25968;$\theta$&#25903;&#25345;&#38598;&#30340;&#38408;&#20540;&#35268;&#21017;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;PrivateLASSO&#30340;&#26368;&#23567;&#26368;&#22823;&#31169;&#26377;&#19979;&#30028;&#65292;&#24182;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#24314;&#31435;&#20102;PrivateLASSO&#22312;&#20013;&#22830;&#27169;&#22411;&#19979;&#30340;&#38544;&#31169;&#21644;&#23454;&#29992;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a high-dimensional stochastic contextual linear bandit problem when the parameter vector is $s_{0}$-sparse and the decision maker is subject to privacy constraints under both central and local models of differential privacy. We present PrivateLASSO, a differentially private LASSO bandit algorithm. PrivateLASSO is based on two sub-routines: (i) a sparse hard-thresholding-based privacy mechanism and (ii) an episodic thresholding rule for identifying the support of the parameter $\theta$. We prove minimax private lower bounds and establish privacy and utility guarantees for PrivateLASSO for the central model under standard assumptions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20998;&#21106;&#30340;&#19978;&#30028;(PUB)&#21644;&#21033;&#29992;&#30701;&#38543;&#26426;&#34892;&#36208;&#29983;&#25104;&#26356;&#22823;&#21021;&#22987;&#35299;&#30340;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26368;&#22823;s-&#26463;&#38382;&#39064;(MBP)&#12290;</title><link>https://arxiv.org/abs/2402.03736</link><description>&lt;p&gt;
&#19968;&#31181;&#26377;&#25928;&#30340;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#21450;&#26368;&#22823;s-&#26463;&#38382;&#39064;&#30340;&#26032;&#36793;&#30028;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Effective Branch-and-Bound Algorithm with New Bounding Methods for the Maximum $s$-Bundle Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20998;&#21106;&#30340;&#19978;&#30028;(PUB)&#21644;&#21033;&#29992;&#30701;&#38543;&#26426;&#34892;&#36208;&#29983;&#25104;&#26356;&#22823;&#21021;&#22987;&#35299;&#30340;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26368;&#22823;s-&#26463;&#38382;&#39064;(MBP)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;s-&#26463;&#38382;&#39064;&#65288;MBP&#65289;&#28041;&#21450;&#22312;&#32473;&#23450;&#22270;&#20013;&#35782;&#21035;&#26368;&#22823;s-&#26463;&#30340;&#20219;&#21153;&#12290;&#22270;G=(V, E)&#34987;&#31216;&#20026;s-&#26463;&#65292;&#22914;&#26524;&#20854;&#39030;&#28857;&#36830;&#36890;&#24230;&#33267;&#23569;&#20026;|V|-s&#65292;&#20854;&#20013;&#39030;&#28857;&#36830;&#36890;&#24230;&#31561;&#20110;&#21024;&#38500;&#26368;&#23569;&#20010;&#39030;&#28857;&#20351;&#24471;&#22270;&#21464;&#20026;&#38750;&#36830;&#36890;&#25110;&#24179;&#20961;&#22270;&#30340;&#26368;&#23567;&#25968;&#37327;&#12290;MBP&#26159;NP&#38590;&#30340;&#65292;&#23545;&#20110;&#24378;&#35843;&#39030;&#28857;&#36830;&#36890;&#24230;&#30340;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20855;&#26377;&#30456;&#20851;&#24615;&#12290;MBP&#30340;&#31934;&#30830;&#31639;&#27861;&#20027;&#35201;&#37319;&#29992;&#20998;&#25903;&#23450;&#30028;&#65288;BnB&#65289;&#26694;&#26550;&#65292;&#20854;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#23545;&#26368;&#22823;s-&#26463;&#30340;&#22522;&#25968;&#30340;&#19978;&#30028;&#36136;&#37327;&#21644;&#22270;&#20943;&#23569;&#30340;&#21021;&#22987;&#19979;&#30028;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#20998;&#21106;&#25216;&#26415;&#23454;&#29616;&#26356;&#32039;&#23494;&#19978;&#30028;&#30340;&#26032;&#22411;&#22522;&#20110;&#20998;&#21106;&#30340;&#19978;&#30028;&#65288;PUB&#65289;&#12290;&#20026;&#20102;&#22686;&#21152;&#19979;&#30028;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#22242;&#19978;&#36827;&#34892;&#30701;&#38543;&#26426;&#34892;&#36208;&#26469;&#29983;&#25104;&#26356;&#22823;&#30340;&#21021;&#22987;&#35299;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26032;&#30340;&#21021;&#22987;&#35299;&#30340;BnB&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Maximum s-Bundle Problem (MBP) addresses the task of identifying a maximum s-bundle in a given graph. A graph G=(V, E) is called an s-bundle if its vertex connectivity is at least |V|-s, where the vertex connectivity equals the minimum number of vertices whose deletion yields a disconnected or trivial graph. MBP is NP-hard and holds relevance in numerous realworld scenarios emphasizing the vertex connectivity. Exact algorithms for MBP mainly follow the branch-and-bound (BnB) framework, whose performance heavily depends on the quality of the upper bound on the cardinality of a maximum s-bundle and the initial lower bound with graph reduction. In this work, we introduce a novel Partition-based Upper Bound (PUB) that leverages the graph partitioning technique to achieve a tighter upper bound compared to existing ones. To increase the lower bound, we propose to do short random walks on a clique to generate larger initial solutions. Then, we propose a new BnB algorithm that uses the ini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DEAN&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#35782;&#21035;&#36807;&#26102;&#20107;&#23454;&#12290;DEAN&#36890;&#36807;&#20840;&#38754;&#24314;&#27169;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30340;&#38544;&#21547;&#32467;&#26500;&#20449;&#24687;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#30340;&#26041;&#27861;&#26469;&#25581;&#31034;&#28508;&#22312;&#30340;&#36807;&#26102;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;DEAN&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.03732</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20013;&#28145;&#24230;&#36807;&#26102;&#20107;&#23454;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Outdated Fact Detection in Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DEAN&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#35782;&#21035;&#36807;&#26102;&#20107;&#23454;&#12290;DEAN&#36890;&#36807;&#20840;&#38754;&#24314;&#27169;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30340;&#38544;&#21547;&#32467;&#26500;&#20449;&#24687;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#30340;&#26041;&#27861;&#26469;&#25581;&#31034;&#28508;&#22312;&#30340;&#36807;&#26102;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;DEAN&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#22240;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#28508;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36807;&#26102;&#20107;&#23454;&#30340;&#38382;&#39064;&#32473;KG&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#24433;&#21709;&#20102;&#20854;&#20316;&#20026;&#29616;&#23454;&#19990;&#30028;&#20449;&#24687;&#30340;&#25972;&#20307;&#36136;&#37327;&#12290;&#29616;&#26377;&#30340;&#36807;&#26102;&#20107;&#23454;&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#20381;&#36182;&#20110;&#25163;&#21160;&#35782;&#21035;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DEAN&#65288;&#28145;&#24230;&#36807;&#26102;&#20107;&#23454;&#26816;&#27979;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;KG&#20013;&#35782;&#21035;&#36807;&#26102;&#20107;&#23454;&#12290;DEAN&#36890;&#36807;&#20840;&#38754;&#24314;&#27169;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30340;&#38544;&#21547;&#32467;&#26500;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#21306;&#20998;&#20107;&#23454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33258;&#24049;&#30340;&#29420;&#29305;&#20043;&#22788;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#25581;&#31034;&#28508;&#22312;&#30340;&#36807;&#26102;&#20449;&#24687;&#65292;DEAN&#37319;&#29992;&#20102;&#22522;&#20110;&#39044;&#23450;&#20041;&#30340;&#20851;&#31995;&#21040;&#33410;&#28857;&#65288;R2N&#65289;&#22270;&#30340;&#23545;&#27604;&#26041;&#27861;&#65292;&#35813;&#22270;&#30001;&#23454;&#20307;&#25968;&#37327;&#21152;&#26435;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;DEAN&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) have garnered significant attention for their vast potential across diverse domains. However, the issue of outdated facts poses a challenge to KGs, affecting their overall quality as real-world information evolves. Existing solutions for outdated fact detection often rely on manual recognition. In response, this paper presents DEAN (Deep outdatEd fAct detectioN), a novel deep learning-based framework designed to identify outdated facts within KGs. DEAN distinguishes itself by capturing implicit structural information among facts through comprehensive modeling of both entities and relations. To effectively uncover latent out-of-date information, DEAN employs a contrastive approach based on a pre-defined Relations-to-Nodes (R2N) graph, weighted by the number of entities. Experimental results demonstrate the effectiveness and superiority of DEAN over state-of-the-art baseline methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#27169;&#22411;&#30340;&#39044;&#27979;&#21644;&#22806;&#37096;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#20915;&#31574;&#30340;&#19968;&#33268;&#24615;&#12290;&#32463;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03728</link><description>&lt;p&gt;
&#24322;&#26500;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#33268;&#32852;&#21512;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Consistent Joint Decision-Making with Heterogeneous Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#27169;&#22411;&#30340;&#39044;&#27979;&#21644;&#22806;&#37096;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#20915;&#31574;&#30340;&#19968;&#33268;&#24615;&#12290;&#32463;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#20419;&#36827;&#20102;&#30001;&#19981;&#21516;&#27169;&#22411;&#20570;&#20986;&#30340;&#20915;&#31574;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#12290;&#36890;&#36807;&#21033;&#29992;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;ILP&#65289;&#26694;&#26550;&#65292;&#25105;&#20204;&#23558;&#21508;&#31181;&#27169;&#22411;&#30340;&#39044;&#27979;&#26144;&#23556;&#21040;&#20840;&#23616;&#24402;&#19968;&#21270;&#21644;&#21487;&#27604;&#36739;&#30340;&#20540;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#20915;&#31574;&#30340;&#20808;&#39564;&#27010;&#29575;&#12289;&#32622;&#20449;&#24230;&#65288;&#19981;&#30830;&#23450;&#24615;&#65289;&#21644;&#27169;&#22411;&#30340;&#39044;&#26399;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20256;&#32479;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel decision-making framework that promotes consistency among decisions made by diverse models while utilizing external knowledge. Leveraging the Integer Linear Programming (ILP) framework, we map predictions from various models into globally normalized and comparable values by incorporating information about decisions' prior probability, confidence (uncertainty), and the models' expected accuracy. Our empirical study demonstrates the superiority of our approach over conventional baselines on multiple datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ISAHP&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#24322;&#27493;&#12289;&#30456;&#20114;&#20381;&#36182;&#30340;&#22810;&#31867;&#22411;&#20107;&#20214;&#24207;&#21015;&#20013;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#23454;&#20363;&#32423;&#30340;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#28385;&#36275;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#35201;&#27714;&#30340;&#31070;&#32463;&#28857;&#36807;&#31243;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#21464;&#21387;&#22120;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#23454;&#29616;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#30340;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2402.03726</link><description>&lt;p&gt;
&#20174;&#23454;&#20363;&#32423;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;Hawkes&#36807;&#31243;&#20013;&#23398;&#20064;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Learning Granger Causality from Instance-wise Self-attentive Hawkes Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ISAHP&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#24322;&#27493;&#12289;&#30456;&#20114;&#20381;&#36182;&#30340;&#22810;&#31867;&#22411;&#20107;&#20214;&#24207;&#21015;&#20013;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#23454;&#20363;&#32423;&#30340;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#28385;&#36275;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#35201;&#27714;&#30340;&#31070;&#32463;&#28857;&#36807;&#31243;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#21464;&#21387;&#22120;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#26469;&#23454;&#29616;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20174;&#24322;&#27493;&#12289;&#30456;&#20114;&#20381;&#36182;&#30340;&#22810;&#31867;&#22411;&#20107;&#20214;&#24207;&#21015;&#20013;&#23398;&#20064;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23545;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#21457;&#29616;&#23454;&#20363;&#32423;&#30340;&#22240;&#26524;&#32467;&#26500;&#24863;&#20852;&#36259;&#12290;&#23454;&#20363;&#32423;&#22240;&#26524;&#20851;&#31995;&#35782;&#21035;&#21333;&#20010;&#20107;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20026;&#20915;&#31574;&#25552;&#20379;&#20102;&#26356;&#31934;&#32454;&#21270;&#30340;&#20449;&#24687;&#12290;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#24037;&#20316;&#35201;&#20040;&#38656;&#35201;&#24378;&#21152;&#19968;&#20123;&#20551;&#35774;&#65292;&#27604;&#22914;&#24378;&#21152;&#21040;&#24378;&#24230;&#20989;&#25968;&#20013;&#30340;&#32447;&#24615;&#20551;&#35774;&#65292;&#35201;&#20040;&#21551;&#21457;&#24335;&#22320;&#23450;&#20041;&#27169;&#22411;&#21442;&#25968;&#65292;&#36825;&#20123;&#19981;&#19968;&#23450;&#28385;&#36275;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;&#23454;&#20363;&#32423;&#33258;&#25105;&#27880;&#24847;&#21147;Hawkes&#36807;&#31243;&#65288;ISAHP&#65289;&#65292;&#21487;&#20197;&#30452;&#25509;&#25512;&#26029;&#20107;&#20214;&#23454;&#20363;&#32423;&#30340;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#12290;ISAHP&#26159;&#31532;&#19968;&#20010;&#28385;&#36275;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#35201;&#27714;&#30340;&#31070;&#32463;&#28857;&#36807;&#31243;&#27169;&#22411;&#12290;&#23427;&#21033;&#29992;&#20102;&#21464;&#21387;&#22120;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#19982;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#30340;&#21407;&#29702;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;ISAHP&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of learning Granger causality from asynchronous, interdependent, multi-type event sequences. In particular, we are interested in discovering instance-level causal structures in an unsupervised manner. Instance-level causality identifies causal relationships among individual events, providing more fine-grained information for decision-making. Existing work in the literature either requires strong assumptions, such as linearity in the intensity function, or heuristically defined model parameters that do not necessarily meet the requirements of Granger causality. We propose Instance-wise Self-Attentive Hawkes Processes (ISAHP), a novel deep learning framework that can directly infer the Granger causality at the event instance level. ISAHP is the first neural point process model that meets the requirements of Granger causality. It leverages the self-attention mechanism of the transformer to align with the principles of Granger causality. We empirically demonstrate th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#32479;&#35745;&#27979;&#35797;&#26041;&#27861;&#65288;VAE-AD&#27979;&#35797;&#65289;&#65292;&#36890;&#36807;&#37327;&#21270;&#24322;&#24120;&#21306;&#22495;&#30340;&#21487;&#38752;&#24615;&#65292;&#21487;&#20197;&#25511;&#21046;&#35823;&#26816;&#30340;&#27010;&#29575;&#21040;&#25152;&#26399;&#26395;&#30340;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.03724</link><description>&lt;p&gt;
&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#32479;&#35745;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Statistical Test for Anomaly Detections by Variational Auto-Encoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#32479;&#35745;&#27979;&#35797;&#26041;&#27861;&#65288;VAE-AD&#27979;&#35797;&#65289;&#65292;&#36890;&#36807;&#37327;&#21270;&#24322;&#24120;&#21306;&#22495;&#30340;&#21487;&#38752;&#24615;&#65292;&#21487;&#20197;&#25511;&#21046;&#35823;&#26816;&#30340;&#27010;&#29575;&#21040;&#25152;&#26399;&#26395;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#30340;&#21487;&#38752;&#24615;&#35780;&#20272;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#22522;&#20110;VAE&#30340;AD&#24050;&#32463;&#22312;&#21508;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#31215;&#26497;&#30340;&#30740;&#31350;&#65292;&#20174;&#26041;&#27861;&#24320;&#21457;&#21040;&#24212;&#29992;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#24403;AD&#30340;&#32467;&#26524;&#29992;&#20110;&#39640;&#39118;&#38505;&#30340;&#20915;&#31574;&#26102;&#65292;&#22914;&#21307;&#23398;&#35786;&#26029;&#65292;&#38656;&#35201;&#30830;&#20445;&#26816;&#27979;&#21040;&#30340;&#24322;&#24120;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VAE-AD&#27979;&#35797;&#20316;&#20026;&#22312;&#32479;&#35745;&#26816;&#39564;&#26694;&#26550;&#19979;&#37327;&#21270;&#22522;&#20110;VAE&#30340;AD&#30340;&#32479;&#35745;&#21487;&#38752;&#24615;&#30340;&#26041;&#27861;&#12290;&#21033;&#29992;VAE-AD&#27979;&#35797;&#65292;&#21487;&#20197;&#20197;p&#20540;&#30340;&#24418;&#24335;&#37327;&#21270;VAE&#26816;&#27979;&#21040;&#30340;&#24322;&#24120;&#21306;&#22495;&#30340;&#21487;&#38752;&#24615;&#12290;&#36825;&#24847;&#21619;&#30528;&#65292;&#22914;&#26524;&#22312;p&#20540;&#20302;&#20110;&#26576;&#20010;&#38408;&#20540;&#26102;&#23459;&#24067;&#20026;&#24322;&#24120;&#65292;&#21017;&#21487;&#20197;&#23558;&#35823;&#26816;&#30340;&#27010;&#29575;&#25511;&#21046;&#22312;&#25152;&#26399;&#26395;&#30340;&#27700;&#24179;&#12290;&#30001;&#20110;VAE-AD&#27979;&#35797;&#26159;&#22522;&#20110;&#19968;&#31181;&#31216;&#20026;&#36873;&#25321;&#24615;&#25512;&#29702;&#30340;&#26032;&#32479;&#35745;&#25512;&#26029;&#26694;&#26550;&#26500;&#24314;&#30340;&#65292;&#20854;&#26377;&#25928;&#24615;&#26159;&#30830;&#20445;&#34987;&#35777;&#26126;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we consider the reliability assessment of anomaly detection (AD) using Variational Autoencoder (VAE). Over the last decade, VAE-based AD has been actively studied in various perspective, from method development to applied research. However, when the results of ADs are used in high-stakes decision-making, such as in medical diagnosis, it is necessary to ensure the reliability of the detected anomalies. In this study, we propose the VAE-AD Test as a method for quantifying the statistical reliability of VAE-based AD within the framework of statistical testing. Using the VAE-AD Test, the reliability of the anomaly regions detected by a VAE can be quantified in the form of p-values. This means that if an anomaly is declared when the p-value is below a certain threshold, it is possible to control the probability of false detection to a desired level. Since the VAE-AD Test is constructed based on a new statistical inference framework called selective inference, its validity is 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#37051;&#23621;&#36873;&#25321;&#65288;SNS&#65289;&#36890;&#36807;&#25913;&#21892;&#25152;&#36873;&#37051;&#23621;&#30340;&#36136;&#37327;&#65292;&#25913;&#21892;&#20102;&#22270;&#24418;&#34920;&#31034;&#65292;&#24182;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03720</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#37051;&#23621;&#36873;&#25321;&#29992;&#20110;&#22270;&#24418;LLMs
&lt;/p&gt;
&lt;p&gt;
Similarity-based Neighbor Selection for Graph LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03720
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#37051;&#23621;&#36873;&#25321;&#65288;SNS&#65289;&#36890;&#36807;&#25913;&#21892;&#25152;&#36873;&#37051;&#23621;&#30340;&#36136;&#37327;&#65292;&#25913;&#21892;&#20102;&#22270;&#24418;&#34920;&#31034;&#65292;&#24182;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#22312;&#30452;&#25509;&#22788;&#29702;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#65288;LLMs&#65289;&#26102;&#38754;&#20020;&#29420;&#29305;&#25361;&#25112;&#65292;&#20294;&#23427;&#20204;&#30340;&#24191;&#27867;&#24120;&#35782;&#30693;&#35782;&#21644;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#20026;TAGs&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#25552;&#20379;&#20102;&#26497;&#22823;&#30340;&#24076;&#26395;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#24050;&#32463;&#35299;&#20915;&#20102;&#36807;&#24230;&#21387;&#32553;&#12289;&#24322;&#36136;&#24615;&#21644;&#20449;&#24687;&#38598;&#25104;&#19981;&#24403;&#31561;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#21463;&#21040;&#25968;&#25454;&#38598;&#20998;&#21306;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#39640;&#32423;LLMs&#30340;&#20302;&#21033;&#29992;&#29575;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#37051;&#23621;&#36873;&#25321;&#65288;SNS&#65289;&#12290;&#20351;&#29992;SimCSE&#21644;&#39640;&#32423;&#37051;&#23621;&#36873;&#25321;&#25216;&#26415;&#65292;SNS&#26377;&#25928;&#25552;&#39640;&#20102;&#25152;&#36873;&#37051;&#23621;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#22270;&#24418;&#34920;&#31034;&#24182;&#20943;&#36731;&#20102;&#36807;&#24230;&#21387;&#32553;&#21644;&#24322;&#36136;&#24615;&#31561;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#20316;&#20026;&#19968;&#31181;&#24402;&#32435;&#21644;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;SNS&#22312;&#20256;&#32479;GNN&#26041;&#27861;&#19978;&#23637;&#31034;&#20102;&#26356;&#24378;&#30340;&#27867;&#21270;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#23454;&#39564;&#31526;&#21512;&#26631;&#20934;&#30340;&#25968;&#25454;&#38598;&#20998;&#21306;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-attributed graphs (TAGs) present unique challenges for direct processing by Language Learning Models (LLMs), yet their extensive commonsense knowledge and robust reasoning capabilities offer great promise for node classification in TAGs. Prior research in this field has grappled with issues such as over-squashing, heterophily, and ineffective graph information integration, further compounded by inconsistencies in dataset partitioning and underutilization of advanced LLMs. To address these challenges, we introduce Similarity-based Neighbor Selection (SNS). Using SimCSE and advanced neighbor selection techniques, SNS effectively improves the quality of selected neighbors, thereby improving graph representation and alleviating issues like over-squashing and heterophily. Besides, as an inductive and training-free approach, SNS demonstrates superior generalization and scalability over traditional GNN methods. Our comprehensive experiments, adhering to standard dataset partitioning prac
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;Clarify&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32416;&#27491;&#27169;&#22411;&#38169;&#35823;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#31616;&#30701;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#32416;&#27491;&#27169;&#22411;&#30340;&#19968;&#33268;&#22833;&#36133;&#27169;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03715</link><description>&lt;p&gt;
&#28548;&#28165;&#65306;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32416;&#27491;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Clarify: Improving Model Robustness With Natural Language Corrections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03715
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;Clarify&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32416;&#27491;&#27169;&#22411;&#38169;&#35823;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#31616;&#30701;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#32416;&#27491;&#27169;&#22411;&#30340;&#19968;&#33268;&#22833;&#36133;&#27169;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#34987;&#35757;&#32451;&#20174;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30456;&#20851;&#24615;&#12290;&#36825;&#36890;&#24120;&#20250;&#23548;&#33268;&#27169;&#22411;&#20381;&#36182;&#20110;&#39640;&#32423;&#38169;&#35823;&#27010;&#24565;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#31181;&#38169;&#35823;&#27010;&#24565;&#65292;&#25105;&#20204;&#24517;&#39035;&#25552;&#20379;&#39069;&#22806;&#30340;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20123;&#39069;&#22806;&#30340;&#23454;&#20363;&#32423;&#30417;&#30563;&#24418;&#24335;&#65292;&#20363;&#22914;&#26631;&#35760;&#34394;&#20551;&#29305;&#24449;&#25110;&#26469;&#33258;&#24179;&#34913;&#20998;&#24067;&#30340;&#39069;&#22806;&#26631;&#35760;&#25968;&#25454;&#12290;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26469;&#35828;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#33021;&#20250;&#21464;&#24471;&#26114;&#36149;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20197;&#25509;&#36817;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#35268;&#27169;&#36827;&#34892;&#39069;&#22806;&#27880;&#37322;&#12290;&#25105;&#20204;&#20551;&#35774;&#26377;&#38024;&#23545;&#24615;&#30340;&#20851;&#20110;&#27169;&#22411;&#38169;&#35823;&#27010;&#24565;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26159;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#39069;&#22806;&#30417;&#30563;&#24418;&#24335;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Clarify&#65292;&#19968;&#31181;&#26032;&#22411;&#30028;&#38754;&#21644;&#26041;&#27861;&#26469;&#20132;&#20114;&#24335;&#22320;&#32416;&#27491;&#27169;&#22411;&#30340;&#38169;&#35823;&#27010;&#24565;&#12290;&#36890;&#36807;Clarify&#65292;&#29992;&#25143;&#21482;&#38656;&#35201;&#25552;&#20379;&#19968;&#20010;&#31616;&#30701;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#25551;&#36848;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#22833;&#36133;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23436;&#20840;&#33258;&#21160;&#21270;&#22320;&#20351;&#29992;s
&lt;/p&gt;
&lt;p&gt;
In supervised learning, models are trained to extract correlations from a static dataset. This often leads to models that rely on high-level misconceptions. To prevent such misconceptions, we must necessarily provide additional information beyond the training data. Existing methods incorporate forms of additional instance-level supervision, such as labels for spurious features or additional labeled data from a balanced distribution. Such strategies can become prohibitively costly for large-scale datasets since they require additional annotation at a scale close to the original training data. We hypothesize that targeted natural language feedback about a model's misconceptions is a more efficient form of additional supervision. We introduce Clarify, a novel interface and method for interactively correcting model misconceptions. Through Clarify, users need only provide a short text description to describe a model's consistent failure patterns. Then, in an entirely automated way, we use s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#36328;&#20256;&#24863;&#22120;&#20301;&#32622;&#30340;&#21160;&#20316;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#30830;&#23450;&#20102;&#29992;&#20110;&#26500;&#24314;&#20301;&#32622;&#26080;&#20851;&#27169;&#22411;&#30340;&#20851;&#38190;&#36523;&#20307;&#37096;&#20301;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#21160;&#20316;&#27169;&#22411;&#65292;&#21333;&#20010;&#27169;&#22411;&#30340;&#24103;&#32423;&#21035;F1&#24471;&#20998;&#36798;&#21040;91.41&#65285;&#65292;&#26080;&#35770;&#20256;&#24863;&#22120;&#25918;&#32622;&#22914;&#20309;&#12290;</title><link>https://arxiv.org/abs/2402.03714</link><description>&lt;p&gt;
&#22312;&#21487;&#31359;&#25140;&#35774;&#22791;&#19978;&#25512;&#36827;&#20301;&#32622;&#26080;&#20851;&#21644;&#35774;&#22791;&#26080;&#20851;&#30340;&#21160;&#20316;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Advancing Location-Invariant and Device-Agnostic Motion Activity Recognition on Wearable Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#36328;&#20256;&#24863;&#22120;&#20301;&#32622;&#30340;&#21160;&#20316;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#30830;&#23450;&#20102;&#29992;&#20110;&#26500;&#24314;&#20301;&#32622;&#26080;&#20851;&#27169;&#22411;&#30340;&#20851;&#38190;&#36523;&#20307;&#37096;&#20301;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#21160;&#20316;&#27169;&#22411;&#65292;&#21333;&#20010;&#27169;&#22411;&#30340;&#24103;&#32423;&#21035;F1&#24471;&#20998;&#36798;&#21040;91.41&#65285;&#65292;&#26080;&#35770;&#20256;&#24863;&#22120;&#25918;&#32622;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#24050;&#32463;&#28183;&#36879;&#21040;&#20154;&#20204;&#30340;&#29983;&#27963;&#20013;&#65292;&#22312;&#20132;&#20114;&#31995;&#32479;&#21644;&#27963;&#21160;&#35782;&#21035;&#26041;&#38754;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#20174;&#19994;&#32773;&#22312;&#22788;&#29702;&#24863;&#30693;&#24322;&#26500;&#24615;&#26102;&#38754;&#20020;&#30528;&#37325;&#22823;&#38556;&#30861;&#65292;&#38656;&#35201;&#20026;&#19981;&#21516;&#30340;&#24179;&#21488;&#23450;&#21046;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36328;&#20256;&#24863;&#22120;&#20301;&#32622;&#30340;&#21160;&#20316;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#26174;&#20102;&#36825;&#20010;&#25361;&#25112;&#65292;&#24182;&#30830;&#23450;&#20102;&#29992;&#20110;&#26500;&#24314;&#20301;&#32622;&#26080;&#20851;&#27169;&#22411;&#30340;&#20851;&#38190;&#36523;&#20307;&#37096;&#20301;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#38598;&#25104;&#21040;&#20219;&#20309;&#35774;&#22791;&#19978;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26368;&#22823;&#30340;&#22810;&#20301;&#32622;&#27963;&#21160;&#25968;&#25454;&#38598;&#65288;N=50&#65292;&#32047;&#35745;200&#23567;&#26102;&#65289;&#65292;&#24182;&#23558;&#20854;&#20844;&#24320;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22312;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#21160;&#20316;&#27169;&#22411;&#65292;&#21333;&#20010;&#27169;&#22411;&#30340;&#24103;&#32423;&#21035;F1&#24471;&#20998;&#36798;&#21040;91.41&#65285;&#65292;&#26080;&#35770;&#20256;&#24863;&#22120;&#25918;&#32622;&#22914;&#20309;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36328;&#20301;&#32622;&#25968;&#25454;&#21512;&#25104;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#19968;&#20010;&#20301;&#32622;&#21512;&#25104;&#21478;&#19968;&#20010;&#20301;&#32622;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#20943;&#36731;&#32321;&#29712;&#30340;&#25968;&#25454;&#25910;&#38598;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wearable sensors have permeated into people's lives, ushering impactful applications in interactive systems and activity recognition. However, practitioners face significant obstacles when dealing with sensing heterogeneities, requiring custom models for different platforms. In this paper, we conduct a comprehensive evaluation of the generalizability of motion models across sensor locations. Our analysis highlights this challenge and identifies key on-body locations for building location-invariant models that can be integrated on any device. For this, we introduce the largest multi-location activity dataset (N=50, 200 cumulative hours), which we make publicly available. We also present deployable on-device motion models reaching 91.41% frame-level F1-score from a single model irrespective of sensor placements. Lastly, we investigate cross-location data synthesis, aiming to alleviate the laborious data collection tasks by synthesizing data in one location given data from another. These 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#21644;&#32479;&#19968;&#31163;&#25955;&#21644;&#36830;&#32493;&#26102;&#38388;&#31163;&#25955;&#21435;&#22122;&#25193;&#25955;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25968;&#23398;&#31616;&#21270;&#21644;&#25512;&#23548;&#65292;&#20351;&#24471;&#31163;&#25955;&#25193;&#25955;&#30340;&#35757;&#32451;&#26356;&#20934;&#30830;&#26131;&#20248;&#21270;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#31934;&#30830;&#21644;&#21152;&#36895;&#30340;&#37319;&#26679;&#12290;&#21516;&#26102;&#65292;&#25104;&#21151;&#22320;&#32479;&#19968;&#20102;&#31163;&#25955;&#26102;&#38388;&#21644;&#36830;&#32493;&#26102;&#38388;&#31163;&#25955;&#25193;&#25955;&#12290;</title><link>https://arxiv.org/abs/2402.03701</link><description>&lt;p&gt;
&#25913;&#36827;&#21644;&#32479;&#19968;&#31163;&#25955;&#21644;&#36830;&#32493;&#26102;&#38388;&#31163;&#25955;&#21435;&#22122;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Improving and Unifying Discrete&amp;Continuous-time Discrete Denoising Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#21644;&#32479;&#19968;&#31163;&#25955;&#21644;&#36830;&#32493;&#26102;&#38388;&#31163;&#25955;&#21435;&#22122;&#25193;&#25955;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25968;&#23398;&#31616;&#21270;&#21644;&#25512;&#23548;&#65292;&#20351;&#24471;&#31163;&#25955;&#25193;&#25955;&#30340;&#35757;&#32451;&#26356;&#20934;&#30830;&#26131;&#20248;&#21270;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#31934;&#30830;&#21644;&#21152;&#36895;&#30340;&#37319;&#26679;&#12290;&#21516;&#26102;&#65292;&#25104;&#21151;&#22320;&#32479;&#19968;&#20102;&#31163;&#25955;&#26102;&#38388;&#21644;&#36830;&#32493;&#26102;&#38388;&#31163;&#25955;&#25193;&#25955;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#22312;&#33258;&#28982;&#31163;&#25955;&#25968;&#25454;&#22914;&#35821;&#35328;&#21644;&#22270;&#24418;&#19978;&#24471;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#34429;&#28982;&#31163;&#25955;&#26102;&#38388;&#31163;&#25955;&#25193;&#25955;&#24050;&#32463;&#24314;&#31435;&#20102;&#19968;&#27573;&#26102;&#38388;&#65292;&#20294;&#30452;&#21040;&#26368;&#36817;Campbell&#31561;&#20154;&#65288;2022&#65289;&#25165;&#24341;&#20837;&#20102;&#36830;&#32493;&#26102;&#38388;&#31163;&#25955;&#25193;&#25955;&#30340;&#31532;&#19968;&#20010;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#36807;&#31243;&#19982;&#31163;&#25955;&#26102;&#38388;&#29256;&#26412;&#26377;&#24456;&#22823;&#24046;&#24322;&#65292;&#38656;&#35201;&#38750;&#24179;&#20961;&#30340;&#36817;&#20284;&#25165;&#33021;&#36827;&#34892;&#21487;&#34892;&#24615;&#20998;&#26512;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#23545;&#21464;&#20998;&#19979;&#30028;&#30340;&#25968;&#23398;&#31616;&#21270;&#65292;&#36825;&#20123;&#31616;&#21270;&#20351;&#31163;&#25955;&#25193;&#25955;&#30340;&#35757;&#32451;&#26356;&#21152;&#20934;&#30830;&#21644;&#26131;&#20110;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21453;&#21521;&#21435;&#22122;&#20844;&#24335;&#65292;&#33021;&#22815;&#23454;&#29616;&#31934;&#30830;&#21644;&#21152;&#36895;&#30340;&#37319;&#26679;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#33021;&#22815;&#20248;&#38597;&#22320;&#32479;&#19968;&#31163;&#25955;&#26102;&#38388;&#21644;&#36830;&#32493;&#26102;&#38388;&#31163;&#25955;&#25193;&#25955;&#12290;&#36890;&#36807;&#26356;&#31616;&#21333;&#30340;&#20998;&#26512;&#20844;&#24335;&#65292;&#21069;&#21521;&#21644;&#29616;&#22312;&#20063;&#21253;&#25324;&#20102;&#21518;&#21521;&#27010;&#29575;&#21487;&#20197;&#28789;&#27963;&#22320;&#36866;&#24212;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete diffusion models have seen a surge of attention with applications on naturally discrete data such as language and graphs. Although discrete-time discrete diffusion has been established for a while, only recently Campbell et al. (2022) introduced the first framework for continuous-time discrete diffusion. However, their training and sampling processes differ significantly from the discrete-time version, necessitating nontrivial approximations for tractability. In this paper, we first present a series of mathematical simplifications of the variational lower bound that enable more accurate and easy-to-optimize training for discrete diffusion. In addition, we derive a simple formulation for backward denoising that enables exact and accelerated sampling, and importantly, an elegant unification of discrete-time and continuous-time discrete diffusion. Thanks to simpler analytical formulations, both forward and now also backward probabilities can flexibly accommodate any noise distrib
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20934;&#30830;&#22320;&#27979;&#37327;&#39640;&#36798;1&#20159;&#21442;&#25968;&#30340;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;(LLC)&#65292;&#24182;&#35777;&#26126;&#20102;&#20272;&#35745;&#24471;&#21040;&#30340;LLC&#20855;&#26377;&#37325;&#32553;&#25918;&#19981;&#21464;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03698</link><description>&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#20272;&#35745;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;
&lt;/p&gt;
&lt;p&gt;
Estimating the Local Learning Coefficient at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#20013;&#20934;&#30830;&#22320;&#27979;&#37327;&#39640;&#36798;1&#20159;&#21442;&#25968;&#30340;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;(LLC)&#65292;&#24182;&#35777;&#26126;&#20102;&#20272;&#35745;&#24471;&#21040;&#30340;LLC&#20855;&#26377;&#37325;&#32553;&#25918;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;(LLC)&#26159;&#19968;&#31181;&#37327;&#21270;&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#65292;&#26368;&#21021;&#26159;&#22312;&#36125;&#21494;&#26031;&#32479;&#35745;&#20013;&#20351;&#29992;&#22855;&#24322;&#23398;&#20064;&#29702;&#35770;(SLT)&#25512;&#23548;&#20986;&#26469;&#30340;&#12290;&#24050;&#30693;&#26377;&#20960;&#31181;&#25968;&#20540;&#20272;&#35745;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;&#30340;&#26041;&#27861;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#36825;&#20123;&#26041;&#27861;&#23578;&#26410;&#25193;&#23637;&#21040;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#25110;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#12290;&#36890;&#36807;&#22312;arXiv:2308.12108 [stat.ML]&#20013;&#24320;&#21457;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#21487;&#20197;&#20934;&#30830;&#21644;&#33258;&#27965;&#22320;&#27979;&#37327;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;(DLN)&#20013;&#39640;&#36798;1&#20159;&#21442;&#25968;&#30340;&#23616;&#37096;&#23398;&#20064;&#31995;&#25968;(LLC)&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#20272;&#35745;&#24471;&#21040;&#30340;LLC&#20855;&#26377;&#29702;&#35770;&#25968;&#37327;&#25152;&#20855;&#22791;&#30340;&#37325;&#32553;&#25918;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The \textit{local learning coefficient} (LLC) is a principled way of quantifying model complexity, originally derived in the context of Bayesian statistics using singular learning theory (SLT). Several methods are known for numerically estimating the local learning coefficient, but so far these methods have not been extended to the scale of modern deep learning architectures or data sets. Using a method developed in {\tt arXiv:2308.12108 [stat.ML]} we empirically show how the LLC may be measured accurately and self-consistently for deep linear networks (DLNs) up to 100M parameters. We also show that the estimated LLC has the rescaling invariance that holds for the theoretical quantity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#38544;&#31169;&#23041;&#32961;&#21644;&#38450;&#24481;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#29983;&#21629;&#21608;&#26399;&#30340;&#35270;&#35282;&#36827;&#34892;&#35752;&#35770;&#65292;&#25552;&#20379;&#20102;&#34892;&#19994;&#21644;&#23454;&#36341;&#32773;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#26041;&#38754;&#30340;&#25351;&#23548;&#21644;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.03688</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#29983;&#21629;&#21608;&#26399;&#35270;&#35282;&#30340;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#23041;&#32961;&#21644;&#38450;&#24481;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Privacy Threats and Defense in Vertical Federated Learning: From Model Life Cycle Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#38544;&#31169;&#23041;&#32961;&#21644;&#38450;&#24481;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#29983;&#21629;&#21608;&#26399;&#30340;&#35270;&#35282;&#36827;&#34892;&#35752;&#35770;&#65292;&#25552;&#20379;&#20102;&#34892;&#19994;&#21644;&#23454;&#36341;&#32773;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#26041;&#38754;&#30340;&#25351;&#23548;&#21644;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;Vertical Federated Learning&#65292;VFL&#65289;&#26159;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#65292;&#22810;&#20010;&#21442;&#19982;&#32773;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#20123;&#21442;&#19982;&#32773;&#20849;&#20139;&#30456;&#21516;&#30340;&#26679;&#26412;&#38598;&#65292;&#20294;&#25345;&#26377;&#19981;&#21516;&#30340;&#29305;&#24449;&#12290;&#34429;&#28982;VFL&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#21327;&#21516;&#26426;&#22120;&#23398;&#20064;&#65292;&#20294;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#38544;&#31169;&#23041;&#32961;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#39318;&#27425;&#23545;VFL&#20013;&#30340;&#38544;&#31169;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#26681;&#25454;&#29305;&#24449;&#23545;&#25915;&#20987;&#21644;&#38450;&#24481;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#24320;&#25918;&#24615;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#35752;&#35770;&#22260;&#32469;&#27169;&#22411;&#30340;&#29983;&#21629;&#21608;&#26399;&#23637;&#24320;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#19981;&#21516;&#38454;&#27573;&#36935;&#21040;&#30340;&#38544;&#31169;&#23041;&#32961;&#21450;&#30456;&#24212;&#30340;&#23545;&#31574;&#12290;&#36825;&#39033;&#35843;&#26597;&#26082;&#20026;&#30740;&#31350;&#30028;&#25552;&#20379;&#20102;&#36164;&#28304;&#65292;&#20063;&#20026;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#25351;&#23548;&#21644;&#21487;&#34892;&#30340;&#35265;&#35299;&#65292;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning (VFL) is a federated learning paradigm where multiple participants, who share the same set of samples but hold different features, jointly train machine learning models. Although VFL enables collaborative machine learning without sharing raw data, it is still susceptible to various privacy threats. In this paper, we conduct the first comprehensive survey of the state-of-the-art in privacy attacks and defenses in VFL. We provide taxonomies for both attacks and defenses, based on their characterizations, and discuss open challenges and future research directions. Specifically, our discussion is structured around the model's life cycle, by delving into the privacy threats encountered during different stages of machine learning and their corresponding countermeasures. This survey not only serves as a resource for the research community but also offers clear guidance and actionable insights for practitioners to safeguard data privacy throughout the model's life c
&lt;/p&gt;</description></item><item><title>PARD&#26159;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#33258;&#22238;&#24402;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#20013;&#30340;&#37096;&#20998;&#39034;&#24207;&#20197;&#22359;&#36880;&#22359;&#30340;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.03687</link><description>&lt;p&gt;
Pard: &#20855;&#26377;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#33258;&#22238;&#24402;&#25193;&#25955;&#29992;&#20110;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03687
&lt;/p&gt;
&lt;p&gt;
PARD&#26159;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#33258;&#22238;&#24402;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#20013;&#30340;&#37096;&#20998;&#39034;&#24207;&#20197;&#22359;&#36880;&#22359;&#30340;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#22238;&#24402;&#27169;&#22411;&#23545;&#20110;&#22270;&#30340;&#39034;&#24207;&#25935;&#24863;&#65292;&#20294;&#20854;&#31616;&#21333;&#26377;&#25928;&#65292;&#22312;&#22270;&#29983;&#25104;&#39046;&#22495;&#19968;&#30452;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#32622;&#25442;&#19981;&#21464;&#24615;&#32780;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#22270;&#25193;&#25955;&#27169;&#22411;&#19968;&#27425;&#24615;&#29983;&#25104;&#22270;&#65292;&#20294;&#38656;&#35201;&#39069;&#22806;&#30340;&#29305;&#24449;&#21644;&#25104;&#21315;&#19978;&#19975;&#27493;&#30340;&#21435;&#22122;&#25165;&#33021;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;PARD&#65292;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#33258;&#22238;&#24402;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#12290;PARD&#21033;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#32622;&#25442;&#19981;&#21464;&#24615;&#65292;&#26080;&#38656;&#20851;&#27880;&#22270;&#30340;&#39034;&#24207;&#25935;&#24863;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#38598;&#21512;&#19981;&#21516;&#65292;&#22270;&#20013;&#30340;&#20803;&#32032;&#24182;&#19981;&#26159;&#23436;&#20840;&#26080;&#24207;&#30340;&#65292;&#33410;&#28857;&#21644;&#36793;&#26377;&#19968;&#20010;&#29420;&#29305;&#30340;&#37096;&#20998;&#39034;&#24207;&#12290;&#21033;&#29992;&#36825;&#20010;&#37096;&#20998;&#39034;&#24207;&#65292;PARD&#20197;&#22359;&#36880;&#22359;&#30340;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#22270;&#65292;&#20854;&#20013;&#27599;&#20010;&#22359;&#30340;&#27010;&#29575;&#20026;c&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to ordering. Yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant. Current graph diffusion models generate graphs in a one-shot fashion, but they require extra features and thousands of denoising steps to achieve optimal performance. We introduce PARD, a Permutation-invariant Auto Regressive Diffusion model that integrates diffusion models with autoregressive methods. PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity. Specifically, we show that contrary to sets, elements in a graph are not entirely unordered and there is a unique partial order for nodes and edges. With this partial order, PARD generates a graph in a block-by-block, autoregressive fashion, where each block's probability is c
&lt;/p&gt;</description></item><item><title>RL-VLM-F&#26159;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#35774;&#35745;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03681</link><description>&lt;p&gt;
RL-VLM-F: &#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03681
&lt;/p&gt;
&lt;p&gt;
RL-VLM-F&#26159;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#35774;&#35745;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#22870;&#21169;&#35774;&#35745;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#21644;&#21453;&#22797;&#35797;&#38169;&#30340;&#36807;&#31243;&#26469;&#35774;&#35745;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20195;&#29702;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#21482;&#20351;&#29992;&#20219;&#21153;&#30446;&#26631;&#30340;&#25991;&#26412;&#25551;&#36848;&#21644;&#20195;&#29702;&#30340;&#35270;&#35273;&#35266;&#27979;&#65292;&#24182;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#36890;&#36807;&#26597;&#35810;&#36825;&#20123;&#27169;&#22411;&#65292;&#22522;&#20110;&#20219;&#21153;&#30446;&#26631;&#30340;&#25991;&#26412;&#25551;&#36848;&#32473;&#20986;&#23545;&#20195;&#29702;&#30340;&#22270;&#20687;&#35266;&#27979;&#30340;&#20559;&#22909;&#65292;&#24182;&#20174;&#20559;&#22909;&#26631;&#31614;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#35201;&#27714;&#36825;&#20123;&#27169;&#22411;&#36755;&#20986;&#21407;&#22987;&#22870;&#21169;&#20998;&#25968;&#65292;&#36825;&#21487;&#33021;&#23384;&#22312;&#22122;&#38899;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RL-VLM-F&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#25104;&#21151;&#22320;&#20135;&#29983;&#20102;&#26377;&#25928;&#30340;&#22870;&#21169;&#21644;&#31574;&#30053;&#65292;&#21253;&#25324;&#32463;&#20856;&#25511;&#21046;&#20197;&#21450;&#21018;&#24615;&#21644;&#28789;&#27963;&#25805;&#32437;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains - including classic control, as well as manipulation of rigid, articulate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#19979;&#30340;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;&#65288;LSTS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#26681;&#25454;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#25351;&#23548;&#26234;&#33021;&#20307;&#22312;&#26368;&#23567;&#21270;&#29615;&#22659;&#20132;&#20114;&#27425;&#25968;&#30340;&#21516;&#26102;&#23454;&#29616;&#20174;&#21021;&#22987;&#29366;&#24577;&#21040;&#30446;&#26631;&#29366;&#24577;&#30340;&#24341;&#23548;&#12290;&#22312;&#32593;&#26684;&#19990;&#30028;&#23454;&#39564;&#20013;&#65292;LSTS&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#26102;&#38388;&#21040;&#38408;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.03678</link><description>&lt;p&gt;
&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Logical Specifications-guided Dynamic Task Sampling for Reinforcement Learning Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#19979;&#30340;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;&#65288;LSTS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#26681;&#25454;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#25351;&#23548;&#26234;&#33021;&#20307;&#22312;&#26368;&#23567;&#21270;&#29615;&#22659;&#20132;&#20114;&#27425;&#25968;&#30340;&#21516;&#26102;&#23454;&#29616;&#20174;&#21021;&#22987;&#29366;&#24577;&#21040;&#30446;&#26631;&#29366;&#24577;&#30340;&#24341;&#23548;&#12290;&#22312;&#32593;&#26684;&#19990;&#30028;&#23454;&#39564;&#20013;&#65292;LSTS&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#26102;&#38388;&#21040;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#20351;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#23398;&#20064;&#22810;&#26679;&#21270;&#34892;&#20026;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#26377;&#25928;&#30340;&#31574;&#30053;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#29615;&#22659;&#20132;&#20114;&#12290;&#20026;&#20102;&#20943;&#23569;&#26679;&#26412;&#22797;&#26434;&#24615;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#65292;&#22914;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL$_f$&#65289;&#20844;&#24335;&#25110;&#22870;&#21169;&#26426;&#22120;&#65288;RM&#65289;&#65292;&#26469;&#25351;&#23548;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#19979;&#30340;&#21160;&#24577;&#20219;&#21153;&#37319;&#26679;&#65288;LSTS&#65289;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#26681;&#25454;&#39640;&#32423;&#20219;&#21153;&#35268;&#33539;&#25351;&#23548;&#26234;&#33021;&#20307;&#20174;&#21021;&#22987;&#29366;&#24577;&#21040;&#30446;&#26631;&#29366;&#24577;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#29615;&#22659;&#20132;&#20114;&#27425;&#25968;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;LSTS&#19981;&#20551;&#35774;&#29615;&#22659;&#21160;&#21147;&#23398;&#25110;&#22870;&#21169;&#26426;&#22120;&#30340;&#20449;&#24687;&#65292;&#24182;&#21160;&#24577;&#37319;&#26679;&#23548;&#33268;&#25104;&#21151;&#30446;&#26631;&#31574;&#30053;&#30340;&#26377;&#24076;&#26395;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#32593;&#26684;&#19990;&#30028;&#19978;&#35780;&#20272;&#20102;LSTS&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#26102;&#38388;&#21040;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has made significant strides in enabling artificial agents to learn diverse behaviors. However, learning an effective policy often requires a large number of environment interactions. To mitigate sample complexity issues, recent approaches have used high-level task specifications, such as Linear Temporal Logic (LTL$_f$) formulas or Reward Machines (RM), to guide the learning progress of the agent. In this work, we propose a novel approach, called Logical Specifications-guided Dynamic Task Sampling (LSTS), that learns a set of RL policies to guide an agent from an initial state to a goal state based on a high-level task specification, while minimizing the number of environmental interactions. Unlike previous work, LSTS does not assume information about the environment dynamics or the Reward Machine, and dynamically samples promising tasks that lead to successful goal policies. We evaluate LSTS on a gridworld and show that it achieves improved time-to-threshol
&lt;/p&gt;</description></item><item><title>PPIretrieval&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#26377;&#25928;&#25628;&#32034;&#28508;&#22312;PPIs&#65292;&#24182;&#25429;&#25417;&#34507;&#30333;&#36136;&#34920;&#38754;&#30340;&#20016;&#23500;&#20960;&#20309;&#21644;&#21270;&#23398;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.03675</link><description>&lt;p&gt;
&#20351;&#29992;PPIretrieval&#36827;&#34892;&#26377;&#25928;&#30340;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Effective Protein-Protein Interaction Exploration with PPIretrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03675
&lt;/p&gt;
&lt;p&gt;
PPIretrieval&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#26377;&#25928;&#25628;&#32034;&#28508;&#22312;PPIs&#65292;&#24182;&#25429;&#25417;&#34507;&#30333;&#36136;&#34920;&#38754;&#30340;&#20016;&#23500;&#20960;&#20309;&#21644;&#21270;&#23398;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#65288;PPIs&#65289;&#22312;&#35843;&#25511;&#35768;&#22810;&#32454;&#32990;&#21151;&#33021;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#20449;&#21495;&#20256;&#23548;&#12289;&#36816;&#36755;&#21644;&#20813;&#30123;&#38450;&#24481;&#12290;&#38543;&#30528;&#22810;&#38142;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#32467;&#26500;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#25552;&#39640;&#65292;&#25361;&#25112;&#24050;&#32463;&#36716;&#21521;&#26377;&#25928;&#22320;&#23548;&#33322;&#24222;&#22823;&#30340;&#22797;&#26434;&#23431;&#23449;&#20197;&#35782;&#21035;&#28508;&#22312;&#30340;PPIs&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PPIretrieval&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#25506;&#32034;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#29616;&#26377;&#30340;PPI&#25968;&#25454;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#26377;&#25928;&#25628;&#32034;&#28508;&#22312;&#30340;PPIs&#65292;&#25429;&#25417;&#34507;&#30333;&#36136;&#34920;&#38754;&#30340;&#20016;&#23500;&#20960;&#20309;&#21644;&#21270;&#23398;&#20449;&#24687;&#12290;&#24403;&#25552;&#20379;&#20102;&#19968;&#20010;&#26410;&#35265;&#36807;&#30340;&#26597;&#35810;&#34507;&#30333;&#36136;&#21450;&#20854;&#30456;&#20851;&#30340;&#32467;&#21512;&#20301;&#28857;&#26102;&#65292;PPIretrieval&#33021;&#22815;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#26377;&#25928;&#35782;&#21035;&#28508;&#22312;&#30340;&#32467;&#21512;&#20276;&#20387;&#21450;&#20854;&#30456;&#24212;&#30340;&#32467;&#21512;&#20301;&#28857;&#65292;&#20419;&#36827;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#22797;&#21512;&#29289;&#30340;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein-protein interactions (PPIs) are crucial in regulating numerous cellular functions, including signal transduction, transportation, and immune defense. As the accuracy of multi-chain protein complex structure prediction improves, the challenge has shifted towards effectively navigating the vast complex universe to identify potential PPIs. Herein, we propose PPIretrieval, the first deep learning-based model for protein-protein interaction exploration, which leverages existing PPI data to effectively search for potential PPIs in an embedding space, capturing rich geometric and chemical information of protein surfaces. When provided with an unseen query protein with its associated binding site, PPIretrieval effectively identifies a potential binding partner along with its corresponding binding site in an embedding space, facilitating the formation of protein-protein complexes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;Frank-Wolfe&#31639;&#27861;&#30340;&#26032;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#26469;&#35299;&#20915;&#20559;&#24046;Gromov-Wasserstein&#38382;&#39064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;PGW&#38382;&#39064;&#26500;&#25104;&#20102;&#24230;&#37327;&#27979;&#24230;&#31354;&#38388;&#30340;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.03664</link><description>&lt;p&gt;
&#39640;&#25928;&#27714;&#35299;&#20559;&#24046;Gromov-Wasserstein&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Efficient Solvers for Partial Gromov-Wasserstein
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;Frank-Wolfe&#31639;&#27861;&#30340;&#26032;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#26469;&#35299;&#20915;&#20559;&#24046;Gromov-Wasserstein&#38382;&#39064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;PGW&#38382;&#39064;&#26500;&#25104;&#20102;&#24230;&#37327;&#27979;&#24230;&#31354;&#38388;&#30340;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24046;Gromov-Wasserstein&#65288;PGW&#65289;&#38382;&#39064;&#21487;&#20197;&#27604;&#36739;&#20855;&#26377;&#19981;&#22343;&#21248;&#36136;&#37327;&#30340;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#27979;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#36825;&#20123;&#31354;&#38388;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#21644;&#37096;&#20998;&#21305;&#37197;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;PGW&#38382;&#39064;&#21487;&#20197;&#36716;&#21270;&#20026;Gromov-Wasserstein&#38382;&#39064;&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#31867;&#20284;&#20110;&#25226;&#20559;&#24046;&#26368;&#20248;&#36816;&#36755;&#38382;&#39064;&#36716;&#21270;&#20026;&#26368;&#20248;&#36816;&#36755;&#38382;&#39064;&#12290;&#36825;&#20010;&#36716;&#21270;&#23548;&#33268;&#20102;&#20004;&#20010;&#26032;&#30340;&#27714;&#35299;&#22120;&#65292;&#22522;&#20110;Frank-Wolfe&#31639;&#27861;&#65292;&#25968;&#23398;&#21644;&#35745;&#31639;&#19978;&#31561;&#20215;&#65292;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;PGW&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;PGW&#38382;&#39064;&#26500;&#25104;&#20102;&#24230;&#37327;&#27979;&#24230;&#31354;&#38388;&#30340;&#24230;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#22312;&#24418;&#29366;&#21305;&#37197;&#21644;&#27491;&#26679;&#26412;&#26410;&#26631;&#35760;&#23398;&#20064;&#38382;&#39064;&#19978;&#30340;&#35745;&#31639;&#26102;&#38388;&#21644;&#24615;&#33021;&#27604;&#36739;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27714;&#35299;&#22120;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The partial Gromov-Wasserstein (PGW) problem facilitates the comparison of measures with unequal masses residing in potentially distinct metric spaces, thereby enabling unbalanced and partial matching across these spaces. In this paper, we demonstrate that the PGW problem can be transformed into a variant of the Gromov-Wasserstein problem, akin to the conversion of the partial optimal transport problem into an optimal transport problem. This transformation leads to two new solvers, mathematically and computationally equivalent, based on the Frank-Wolfe algorithm, that provide efficient solutions to the PGW problem. We further establish that the PGW problem constitutes a metric for metric measure spaces. Finally, we validate the effectiveness of our proposed solvers in terms of computation time and performance on shape-matching and positive-unlabeled learning problems, comparing them against existing baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#31526;&#21495;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;NS-DNNs&#65289;&#20013;&#30340;&#31526;&#21495;&#27491;&#30830;&#24615;&#21407;&#21017;&#65292;&#21363;&#29992;&#20110;&#25512;&#29702;&#30340;&#31070;&#32463;&#23618;&#23545;&#20013;&#38388;&#31526;&#21495;&#30340;&#39044;&#27979;&#24517;&#39035;&#19982;&#36755;&#20837;&#25968;&#25454;&#30340;&#31526;&#21495;&#34920;&#31034;&#30456;&#21305;&#37197;&#12290;&#31526;&#21495;&#27491;&#30830;&#24615;&#26159;NS-DNN&#21487;&#35299;&#37322;&#24615;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#24517;&#35201;&#29305;&#24615;&#65292;&#24182;&#20026;&#25512;&#29702;&#21644;&#20132;&#27969;&#27169;&#22411;&#34892;&#20026;&#25552;&#20379;&#20102;&#31934;&#30830;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03663</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#21253;&#21547;&#31526;&#21495;&#23618;&#30340;&#31526;&#21495;&#27491;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Symbol Correctness in Deep Neural Networks Containing Symbolic Layers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#31526;&#21495;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;NS-DNNs&#65289;&#20013;&#30340;&#31526;&#21495;&#27491;&#30830;&#24615;&#21407;&#21017;&#65292;&#21363;&#29992;&#20110;&#25512;&#29702;&#30340;&#31070;&#32463;&#23618;&#23545;&#20013;&#38388;&#31526;&#21495;&#30340;&#39044;&#27979;&#24517;&#39035;&#19982;&#36755;&#20837;&#25968;&#25454;&#30340;&#31526;&#21495;&#34920;&#31034;&#30456;&#21305;&#37197;&#12290;&#31526;&#21495;&#27491;&#30830;&#24615;&#26159;NS-DNN&#21487;&#35299;&#37322;&#24615;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#24517;&#35201;&#29305;&#24615;&#65292;&#24182;&#20026;&#25512;&#29702;&#21644;&#20132;&#27969;&#27169;&#22411;&#34892;&#20026;&#25552;&#20379;&#20102;&#31934;&#30830;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22788;&#29702;&#20197;&#24863;&#30693;&#21644;&#36923;&#36753;&#25512;&#29702;&#30456;&#32467;&#21512;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#31070;&#32463;&#31526;&#21495;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;NS-DNNs&#65289;&#65292;&#23427;&#20204;&#38500;&#20102;&#20256;&#32479;&#30340;&#31070;&#32463;&#23618;&#20043;&#22806;&#65292;&#36824;&#21253;&#21547;&#31526;&#21495;&#23618;&#65306;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30001;&#31526;&#21495;&#27714;&#35299;&#22120;&#35780;&#20272;&#30340;&#31526;&#21495;&#34920;&#36798;&#24335;&#65288;&#20363;&#22914;&#65292;SAT&#20844;&#24335;&#65292;&#36923;&#36753;&#31243;&#24207;&#65289;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#24418;&#24335;&#21270;&#20102;&#19968;&#31181;&#30452;&#35266;&#12289;&#39640;&#23618;&#27425;&#30340;&#21407;&#21017;&#65292;&#21487;&#20197;&#25351;&#23548;NS-DNNs&#30340;&#35774;&#35745;&#21644;&#20998;&#26512;&#65306;&#31526;&#21495;&#27491;&#30830;&#24615;&#65292;&#21363;&#31070;&#32463;&#23618;&#23545;&#20013;&#38388;&#31526;&#21495;&#30340;&#27491;&#30830;&#24615;&#65292;&#30456;&#23545;&#20110;&#36755;&#20837;&#25968;&#25454;&#30340;&#65288;&#36890;&#24120;&#26410;&#30693;&#30340;&#65289;&#22522;&#26412;&#31526;&#21495;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31526;&#21495;&#27491;&#30830;&#24615;&#26159;NS-DNN&#21487;&#35299;&#37322;&#24615;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#24517;&#35201;&#29305;&#24615;&#65288;&#23613;&#31649;&#36890;&#24120;&#26080;&#27861;&#36827;&#34892;&#35757;&#32451;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31526;&#21495;&#27491;&#30830;&#24615;&#26694;&#26550;&#22312;&#31070;&#32463;&#31526;&#21495;&#36793;&#30028;&#22788;&#25512;&#29702;&#21644;&#20132;&#27969;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#25552;&#20379;&#20102;&#19968;&#31181;&#31934;&#30830;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#22522;&#26412;&#26435;&#34913;&#26377;&#20102;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
To handle AI tasks that combine perception and logical reasoning, recent work introduces Neurosymbolic Deep Neural Networks (NS-DNNs), which contain -- in addition to traditional neural layers -- symbolic layers: symbolic expressions (e.g., SAT formulas, logic programs) that are evaluated by symbolic solvers during inference. We identify and formalize an intuitive, high-level principle that can guide the design and analysis of NS-DNNs: symbol correctness, the correctness of the intermediate symbols predicted by the neural layers with respect to a (generally unknown) ground-truth symbolic representation of the input data. We demonstrate that symbol correctness is a necessary property for NS-DNN explainability and transfer learning (despite being in general impossible to train for). Moreover, we show that the framework of symbol correctness provides a precise way to reason and communicate about model behavior at neural-symbolic boundaries, and gives insight into the fundamental tradeoffs
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#19978;&#36827;&#34892;&#20256;&#36882;&#24335;&#22870;&#21169;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20272;&#35745;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#22870;&#21169;&#12290;&#36890;&#36807;&#21033;&#29992;&#26377;&#38480;&#30340;&#20154;&#24037;&#22870;&#21169;&#27880;&#37322;&#21644;&#21487;&#29992;&#25968;&#25454;&#26500;&#24314;&#22870;&#21169;&#20256;&#25773;&#22270;&#65292;&#24182;&#21033;&#29992;&#22270;&#36827;&#34892;&#22870;&#21169;&#25512;&#26029;&#65292;&#20174;&#32780;&#25512;&#26029;&#20986;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#22870;&#21169;&#12290;</title><link>https://arxiv.org/abs/2402.03661</link><description>&lt;p&gt;
&#22312;&#22270;&#19978;&#36827;&#34892;&#20256;&#36882;&#24335;&#22870;&#21169;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Transductive Reward Inference on Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03661
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#19978;&#36827;&#34892;&#20256;&#36882;&#24335;&#22870;&#21169;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20272;&#35745;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#22870;&#21169;&#12290;&#36890;&#36807;&#21033;&#29992;&#26377;&#38480;&#30340;&#20154;&#24037;&#22870;&#21169;&#27880;&#37322;&#21644;&#21487;&#29992;&#25968;&#25454;&#26500;&#24314;&#22870;&#21169;&#20256;&#25773;&#22270;&#65292;&#24182;&#21033;&#29992;&#22270;&#36827;&#34892;&#22870;&#21169;&#25512;&#26029;&#65292;&#20174;&#32780;&#25512;&#26029;&#20986;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22870;&#21169;&#20449;&#24687;&#20256;&#25773;&#22270;&#19978;&#36827;&#34892;&#20256;&#36882;&#24335;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#33021;&#22815;&#26377;&#25928;&#22320;&#20272;&#35745;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#22870;&#21169;&#12290;&#22870;&#21169;&#25512;&#26029;&#26159;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#23398;&#20064;&#26377;&#25928;&#31574;&#30053;&#30340;&#20851;&#38190;&#65292;&#32780;&#30452;&#25509;&#30340;&#29615;&#22659;&#20132;&#20114;&#35201;&#20040;&#25104;&#26412;&#22826;&#39640;&#65292;&#35201;&#20040;&#26159;&#19981;&#36947;&#24503;&#30340;&#65292;&#24182;&#19988;&#24456;&#23569;&#26377;&#21487;&#35775;&#38382;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20363;&#22914;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#22270;&#19978;&#20449;&#24687;&#20256;&#25773;&#30340;&#19978;&#19979;&#25991;&#29305;&#24615;&#30340;&#22870;&#21169;&#25512;&#26029;&#26041;&#27861;&#65292;&#21033;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#20154;&#24037;&#22870;&#21169;&#27880;&#37322;&#26469;&#25512;&#26029;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#21644;&#26377;&#38480;&#30340;&#22870;&#21169;&#27880;&#37322;&#26500;&#24314;&#22870;&#21169;&#20256;&#25773;&#22270;&#65292;&#20854;&#20013;&#36793;&#26435;&#37325;&#21253;&#21547;&#19982;&#22870;&#21169;&#30456;&#20851;&#30340;&#21508;&#31181;&#24433;&#21709;&#22240;&#32032;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26500;&#24314;&#30340;&#22270;&#36827;&#34892;&#20256;&#36882;&#24335;&#22870;&#21169;&#25512;&#26029;&#65292;&#20174;&#32780;&#20272;&#35745;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we present a transductive inference approach on that reward information propagation graph, which enables the effective estimation of rewards for unlabelled data in offline reinforcement learning. Reward inference is the key to learning effective policies in practical scenarios, while direct environmental interactions are either too costly or unethical and the reward functions are rarely accessible, such as in healthcare and robotics. Our research focuses on developing a reward inference method based on the contextual properties of information propagation on graphs that capitalizes on a constrained number of human reward annotations to infer rewards for unlabelled data. We leverage both the available data and limited reward annotations to construct a reward propagation graph, wherein the edge weights incorporate various influential factors pertaining to the rewards. Subsequently, we employ the constructed graph for transductive reward inference, thereby estimating rewards
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#20102;&#22312;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20013;&#65292;&#20351;&#29992;&#30456;&#21516;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#21021;&#22987;&#21270;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20250;&#20986;&#29616;&#19968;&#20010;&#26377;&#36259;&#30340;&#32447;&#24615;&#29616;&#35937;&#65292;&#31216;&#20026;&#36328;&#20219;&#21153;&#32447;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#24182;&#25512;&#27979;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#19968;&#33539;&#24335;&#20013;&#26412;&#36136;&#19978;&#31867;&#20284;&#32447;&#24615;&#26144;&#23556;&#65292;&#20174;&#21442;&#25968;&#31354;&#38388;&#21040;&#29305;&#24449;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#36825;&#19968;&#21457;&#29616;&#25581;&#31034;&#20102;&#20851;&#20110;&#27169;&#22411;&#21512;&#24182;/&#32534;&#36753;&#21644;&#21442;&#25968;&#20849;&#20139;&#31561;&#26041;&#38754;&#30340;&#26032;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.03660</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20013;&#20986;&#29616;&#20102;&#36328;&#20219;&#21153;&#32447;&#24615;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Cross-Task Linearity Emerges in the Pretraining-Finetuning Paradigm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#20102;&#22312;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20013;&#65292;&#20351;&#29992;&#30456;&#21516;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#21021;&#22987;&#21270;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20250;&#20986;&#29616;&#19968;&#20010;&#26377;&#36259;&#30340;&#32447;&#24615;&#29616;&#35937;&#65292;&#31216;&#20026;&#36328;&#20219;&#21153;&#32447;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#24182;&#25512;&#27979;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#19968;&#33539;&#24335;&#20013;&#26412;&#36136;&#19978;&#31867;&#20284;&#32447;&#24615;&#26144;&#23556;&#65292;&#20174;&#21442;&#25968;&#31354;&#38388;&#21040;&#29305;&#24449;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#36825;&#19968;&#21457;&#29616;&#25581;&#31034;&#20102;&#20851;&#20110;&#27169;&#22411;&#21512;&#24182;/&#32534;&#36753;&#21644;&#21442;&#25968;&#20849;&#20139;&#31561;&#26041;&#38754;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#24050;&#25104;&#20026;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#30340;&#20027;&#27969;&#36235;&#21183;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#20174;&#20844;&#20849;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#21021;&#22987;&#21270;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20013;&#20986;&#29616;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#32447;&#24615;&#29616;&#35937;&#65292;&#31216;&#20026;&#36328;&#20219;&#21153;&#32447;&#24615;&#65288;CTL&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22914;&#26524;&#25105;&#20204;&#32447;&#24615;&#25554;&#20540;&#20004;&#20010;&#24494;&#35843;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#26435;&#37325;&#25554;&#20540;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#22823;&#33268;&#31561;&#20110;&#27599;&#23618;&#20013;&#20004;&#20010;&#24494;&#35843;&#27169;&#22411;&#29305;&#24449;&#30340;&#32447;&#24615;&#25554;&#20540;&#12290;&#36825;&#26679;&#30340;&#36328;&#20219;&#21153;&#32447;&#24615;&#22312;&#21516;&#34892;&#25991;&#29486;&#20013;&#23578;&#26410;&#34987;&#27880;&#24847;&#21040;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#35777;&#25454;&#65292;&#25903;&#25345;&#20174;&#30456;&#21516;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#24320;&#22987;&#30340;&#24494;&#35843;&#27169;&#22411;&#19968;&#33268;&#20986;&#29616;CTL&#12290;&#25105;&#20204;&#25512;&#27979;&#22312;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#26412;&#36136;&#19978;&#26159;&#32447;&#24615;&#26144;&#23556;&#65292;&#20174;&#21442;&#25968;&#31354;&#38388;&#21040;&#29305;&#24449;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#28857;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#20851;&#20110;&#27169;&#22411;&#21512;&#24182;/&#32534;&#36753;&#12289;&#21442;&#25968;&#20849;&#20139;&#31561;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pretraining-finetuning paradigm has become the prevailing trend in modern deep learning. In this work, we discover an intriguing linear phenomenon in models that are initialized from a common pretrained checkpoint and finetuned on different tasks, termed as Cross-Task Linearity (CTL). Specifically, if we linearly interpolate the weights of two finetuned models, the features in the weight-interpolated model are approximately equal to the linear interpolation of features in two finetuned models at each layer. Such cross-task linearity has not been noted in peer literature. We provide comprehensive empirical evidence supporting that CTL consistently occurs for finetuned models that start from the same pretrained checkpoint. We conjecture that in the pretraining-finetuning paradigm, neural networks essentially function as linear maps, mapping from the parameter space to the feature space. Based on this viewpoint, our study unveils novel insights into explaining model merging/editing, p
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32929;&#31080;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;Summarize-Explain-Predict&#65288;SEP&#65289;&#27169;&#22411;&#26469;&#35299;&#20915;&#32929;&#31080;&#39044;&#27979;&#20013;&#30340;&#35299;&#37322;&#38382;&#39064;&#21644;&#25968;&#25454;&#26631;&#27880;&#25104;&#26412;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03659</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#21453;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32929;&#31080;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03659
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32929;&#31080;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;Summarize-Explain-Predict&#65288;SEP&#65289;&#27169;&#22411;&#26469;&#35299;&#20915;&#32929;&#31080;&#39044;&#27979;&#20013;&#30340;&#35299;&#37322;&#38382;&#39064;&#21644;&#25968;&#25454;&#26631;&#27880;&#25104;&#26412;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20256;&#32479;&#30340;&#38750;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35828;&#65292;&#35299;&#37322;&#32929;&#31080;&#39044;&#27979;&#36890;&#24120;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013;&#35299;&#37322;&#20165;&#38480;&#20110;&#21487;&#35270;&#21270;&#37325;&#35201;&#25991;&#26412;&#19978;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#29983;&#25104;&#20154;&#31867;&#21487;&#35835;&#35299;&#37322;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#32929;&#31080;&#39044;&#27979;&#23545;LLM&#26469;&#35828;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#33021;&#22815;&#26435;&#34913;&#28151;&#20081;&#31038;&#20250;&#25991;&#26412;&#23545;&#32929;&#31080;&#20215;&#26684;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;&#38543;&#30528;&#24341;&#20837;&#35299;&#37322;&#32452;&#20214;&#65292;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#65292;&#38656;&#35201;LLM&#33021;&#22815;&#29992;&#21475;&#22836;&#26041;&#24335;&#35299;&#37322;&#20026;&#20160;&#20040;&#26576;&#20123;&#22240;&#32032;&#27604;&#20854;&#20182;&#22240;&#32032;&#26356;&#37325;&#35201;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35201;&#20026;&#36825;&#26679;&#30340;&#20219;&#21153;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#38656;&#35201;&#19987;&#23478;&#26631;&#27880;&#30340;&#26679;&#26412;&#26469;&#35299;&#37322;&#35757;&#32451;&#38598;&#20013;&#30340;&#27599;&#27425;&#32929;&#31080;&#27874;&#21160;&#65292;&#36825;&#22312;&#25104;&#26412;&#21644;&#23454;&#38469;&#21487;&#25193;&#23637;&#24615;&#19978;&#26159;&#26114;&#36149;&#19988;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;Summarize-Explain-Predict&#65288;SEP&#65289;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining stock predictions is generally a difficult task for traditional non-generative deep learning models, where explanations are limited to visualizing the attention weights on important texts. Today, Large Language Models (LLMs) present a solution to this problem, given their known capabilities to generate human-readable explanations for their decision-making process. However, the task of stock prediction remains challenging for LLMs, as it requires the ability to weigh the varying impacts of chaotic social texts on stock prices. The problem gets progressively harder with the introduction of the explanation component, which requires LLMs to explain verbally why certain factors are more important than the others. On the other hand, to fine-tune LLMs for such a task, one would need expert-annotated samples of explanation for every stock movement in the training set, which is expensive and impractical to scale. To tackle these issues, we propose our Summarize-Explain-Predict (SEP) 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#23884;&#22871;&#30340;&#20302;&#31209;&#36817;&#20284;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#36816;&#31639;&#31526;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26080;&#32422;&#26463;&#20248;&#21270;&#20844;&#24335;&#38544;&#24335;&#39640;&#25928;&#22320;&#20445;&#25345;&#23398;&#20064;&#20989;&#25968;&#30340;&#27491;&#20132;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03655</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23884;&#22871;&#20302;&#31209;&#36817;&#20284;&#23454;&#29616;&#36816;&#31639;&#31526;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Operator SVD with Neural Networks via Nested Low-Rank Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#23884;&#22871;&#30340;&#20302;&#31209;&#36817;&#20284;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#36816;&#31639;&#31526;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26080;&#32422;&#26463;&#20248;&#21270;&#20844;&#24335;&#38544;&#24335;&#39640;&#25928;&#22320;&#20445;&#25345;&#23398;&#20064;&#20989;&#25968;&#30340;&#27491;&#20132;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#21644;&#31185;&#23398;&#35745;&#31639;&#38382;&#39064;&#20013;&#65292;&#35745;&#31639;&#32473;&#23450;&#32447;&#24615;&#31639;&#23376;&#30340;&#29305;&#24449;&#20540;&#20998;&#35299;&#65288;EVD&#65289;&#25110;&#25214;&#21040;&#20854;&#20027;&#35201;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#20989;&#25968;&#26159;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#12290;&#23545;&#20110;&#39640;&#32500;&#29305;&#24449;&#20540;&#38382;&#39064;&#65292;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#29305;&#24449;&#20989;&#25968;&#34987;&#35748;&#20026;&#26159;&#20256;&#32479;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#25216;&#26415;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#22522;&#20110;&#25130;&#26029;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#20302;&#31209;&#36817;&#20284;&#34920;&#24449;&#65292;&#24182;&#20276;&#38543;&#30528;&#31216;&#20026;&#23884;&#22871;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#27491;&#30830;&#30340;&#39034;&#24207;&#23398;&#20064;&#21069;L&#20010;&#22855;&#24322;&#20540;&#21644;&#22855;&#24322;&#20989;&#25968;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#26080;&#32422;&#26463;&#20248;&#21270;&#20844;&#24335;&#38544;&#24335;&#39640;&#25928;&#22320;&#20419;&#36827;&#20102;&#23398;&#20064;&#20989;&#25968;&#30340;&#27491;&#20132;&#24615;&#65292;&#36825;&#20010;&#20844;&#24335;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36890;&#36807;&#29616;&#25104;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#31639;&#27861;&#27714;&#35299;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#20248;&#21270;&#26694;&#26550;&#22312;&#20351;&#29992;&#26696;&#20363;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computing eigenvalue decomposition (EVD) of a given linear operator, or finding its leading eigenvalues and eigenfunctions, is a fundamental task in many machine learning and scientific computing problems. For high-dimensional eigenvalue problems, training neural networks to parameterize the eigenfunctions is considered as a promising alternative to the classical numerical linear algebra techniques. This paper proposes a new optimization framework based on the low-rank approximation characterization of a truncated singular value decomposition, accompanied by new techniques called nesting for learning the top-$L$ singular values and singular functions in the correct order. The proposed method promotes the desired orthogonality in the learned functions implicitly and efficiently via an unconstrained optimization formulation, which is easy to solve with off-the-shelf gradient-based optimization algorithms. We demonstrate the effectiveness of the proposed optimization framework for use cas
&lt;/p&gt;</description></item><item><title>TGX&#26159;&#19968;&#20010;&#19987;&#20026;&#20998;&#26512;&#26102;&#38388;&#32593;&#32476;&#32780;&#35774;&#35745;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#25552;&#20379;&#20102;&#33258;&#21160;&#21270;&#30340;&#25968;&#25454;&#21152;&#36733;&#12289;&#25968;&#25454;&#22788;&#29702;&#21644;&#28436;&#21270;&#22270;&#20998;&#26512;&#12290;&#23427;&#25903;&#25345;&#22810;&#20010;&#20869;&#32622;&#25968;&#25454;&#38598;&#21644;&#22806;&#37096;&#25968;&#25454;&#38598;&#30340;&#35775;&#38382;&#65292;&#24182;&#25552;&#20379;&#25968;&#25454;&#22788;&#29702;&#21644;&#32593;&#32476;&#20998;&#26512;&#21151;&#33021;&#65292;&#20351;&#24471;&#22788;&#29702;&#21644;&#30740;&#31350;&#26102;&#38388;&#32593;&#32476;&#21464;&#24471;&#26356;&#21152;&#20415;&#25463;&#12290;</title><link>https://arxiv.org/abs/2402.03651</link><description>&lt;p&gt;
&#20351;&#29992;TGX&#36827;&#34892;&#26102;&#38388;&#22270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Temporal Graph Analysis with TGX
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03651
&lt;/p&gt;
&lt;p&gt;
TGX&#26159;&#19968;&#20010;&#19987;&#20026;&#20998;&#26512;&#26102;&#38388;&#32593;&#32476;&#32780;&#35774;&#35745;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#25552;&#20379;&#20102;&#33258;&#21160;&#21270;&#30340;&#25968;&#25454;&#21152;&#36733;&#12289;&#25968;&#25454;&#22788;&#29702;&#21644;&#28436;&#21270;&#22270;&#20998;&#26512;&#12290;&#23427;&#25903;&#25345;&#22810;&#20010;&#20869;&#32622;&#25968;&#25454;&#38598;&#21644;&#22806;&#37096;&#25968;&#25454;&#38598;&#30340;&#35775;&#38382;&#65292;&#24182;&#25552;&#20379;&#25968;&#25454;&#22788;&#29702;&#21644;&#32593;&#32476;&#20998;&#26512;&#21151;&#33021;&#65292;&#20351;&#24471;&#22788;&#29702;&#21644;&#30740;&#31350;&#26102;&#38388;&#32593;&#32476;&#21464;&#24471;&#26356;&#21152;&#20415;&#25463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#32593;&#32476;&#65292;&#38543;&#30528;&#20854;&#19981;&#26029;&#21464;&#21270;&#30340;&#20851;&#31995;&#65292;&#26368;&#33021;&#20197;&#26102;&#38388;&#22270;&#24418;&#24335;&#25429;&#25417;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36719;&#20214;&#24211;&#20027;&#35201;&#26159;&#38024;&#23545;&#38745;&#24577;&#22270;&#35774;&#35745;&#30340;&#65292;&#24573;&#30053;&#20102;&#26102;&#38388;&#22270;&#30340;&#21160;&#24577;&#24615;&#36136;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TGX&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#20998;&#26512;&#26102;&#38388;&#32593;&#32476;&#32780;&#35774;&#35745;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#21253;&#21547;&#20102;&#33258;&#21160;&#21270;&#30340;&#25968;&#25454;&#21152;&#36733;&#12289;&#25968;&#25454;&#22788;&#29702;&#21644;&#28436;&#21270;&#22270;&#20998;&#26512;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;TGX&#25552;&#20379;&#20102;11&#20010;&#20869;&#32622;&#25968;&#25454;&#38598;&#21644;8&#20010;&#22806;&#37096;&#26102;&#38388;&#22270;&#22522;&#20934;&#65288;TGB&#65289;&#25968;&#25454;&#38598;&#30340;&#35775;&#38382;&#65292;&#20197;&#21450;&#20219;&#20309;&#20197;.csv&#26684;&#24335;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#38500;&#20102;&#25968;&#25454;&#21152;&#36733;&#22806;&#65292;TGX&#36824;&#25552;&#20379;&#20102;&#25968;&#25454;&#22788;&#29702;&#21151;&#33021;&#65292;&#20363;&#22914;&#23545;&#26102;&#38388;&#22270;&#30340;&#31163;&#25955;&#21270;&#21644;&#33410;&#28857;&#23376;&#37319;&#26679;&#65292;&#20197;&#21152;&#36895;&#22788;&#29702;&#36739;&#22823;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;TGX&#36890;&#36807;&#25552;&#20379;&#22810;&#26679;&#21270;&#30340;&#24230;&#37327;&#25351;&#26631;&#65288;&#22914;&#24179;&#22343;&#33410;&#28857;&#24230;&#21644;&#27599;&#20010;&#26102;&#38388;&#25139;&#30340;&#33410;&#28857;&#21644;&#36793;&#30340;&#28436;&#21270;&#25968;&#37327;&#65289;&#20197;&#36827;&#34892;&#32593;&#32476;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#35813;&#36719;&#20214;&#21253;&#36824;&#25972;&#21512;&#20102;&#24230;&#37327;&#25351;&#26631;&#30340;&#35745;&#31639;&#21450;&#23545;&#22270;&#24418;&#36827;&#34892;&#21487;&#35270;&#21270;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world networks, with their evolving relations, are best captured as temporal graphs. However, existing software libraries are largely designed for static graphs where the dynamic nature of temporal graphs is ignored. Bridging this gap, we introduce TGX, a Python package specially designed for analysis of temporal networks that encompasses an automated pipeline for data loading, data processing, and analysis of evolving graphs. TGX provides access to eleven built-in datasets and eight external Temporal Graph Benchmark (TGB) datasets as well as any novel datasets in the .csv format. Beyond data loading, TGX facilitates data processing functionalities such as discretization of temporal graphs and node subsampling to accelerate working with larger datasets. For comprehensive investigation, TGX offers network analysis by providing a diverse set of measures, including average node degree and the evolving number of nodes and edges per timestamp. Additionally, the package consolidates mea
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#32447;&#24615;&#26680;&#22238;&#24402;&#21644;&#27969;&#24418;&#23398;&#20064;&#30340;&#38750;&#21442;&#25968;&#25968;&#25454;&#25554;&#34917;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#24314;&#27169;&#21644;&#35745;&#31639;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03648</link><description>&lt;p&gt;
&#22810;&#32447;&#24615;&#26680;&#22238;&#24402;&#21644;&#27969;&#24418;&#23398;&#20064;&#30340;&#25968;&#25454;&#25554;&#34917;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multilinear Kernel Regression and Imputation via Manifold Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#32447;&#24615;&#26680;&#22238;&#24402;&#21644;&#27969;&#24418;&#23398;&#20064;&#30340;&#38750;&#21442;&#25968;&#25968;&#25454;&#25554;&#34917;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#24314;&#27169;&#21644;&#35745;&#31639;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#25968;&#25454;&#25554;&#34917;&#26041;&#27861;&#65292;&#34987;&#31216;&#20026;&#22810;&#32447;&#24615;&#26680;&#22238;&#24402;&#21644;&#27969;&#24418;&#23398;&#20064;&#25554;&#34917;&#26041;&#27861;&#65288;MultiL-KRIM&#65289;&#12290;&#21463;&#21040;&#27969;&#24418;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;MultiL-KRIM&#25226;&#25968;&#25454;&#29305;&#24449;&#24314;&#27169;&#20026;&#20301;&#20110;&#25110;&#25509;&#36817;&#23884;&#20837;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#26410;&#30693;&#24179;&#28369;&#27969;&#24418;&#30340;&#28857;&#20113;&#12290;&#19982;&#20856;&#22411;&#30340;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#21518;&#32773;&#36890;&#36807;&#22522;&#20110;&#22270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#27491;&#21017;&#21270;&#22120;&#23547;&#25214;&#20302;&#32500;&#27169;&#24335;&#65292;MultiL-KRIM&#21017;&#24314;&#31435;&#22312;&#23545;&#27969;&#24418;&#30340;&#20999;&#31354;&#38388;&#30340;&#30452;&#35266;&#27010;&#24565;&#19978;&#65292;&#24182;&#23558;&#28857;&#20113;&#37051;&#23621;&#20043;&#38388;&#30340;&#21327;&#20316;&#65288;&#22238;&#24402;&#22120;&#65289;&#30452;&#25509;&#32435;&#20837;&#25439;&#22833;&#20989;&#25968;&#30340;&#25968;&#25454;&#24314;&#27169;&#39033;&#20013;&#12290;&#22810;&#20010;&#26680;&#20989;&#25968;&#30340;&#20351;&#29992;&#25552;&#20379;&#20102;&#40065;&#26834;&#24615;&#21644;&#20016;&#23500;&#30340;&#36924;&#36817;&#24615;&#36136;&#65292;&#32780;&#22810;&#20010;&#30697;&#38453;&#22240;&#23376;&#25552;&#20379;&#20102;&#20302;&#31209;&#24314;&#27169;&#65292;&#25972;&#21512;&#20102;&#38477;&#32500;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#31616;&#21270;&#20102;&#35745;&#31639;&#12290;&#20004;&#20010;&#37325;&#35201;&#30340;&#24212;&#29992;&#39046;&#22495;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel nonparametric framework for data imputation, coined multilinear kernel regression and imputation via the manifold assumption (MultiL-KRIM). Motivated by manifold learning, MultiL-KRIM models data features as a point cloud located in or close to a user-unknown smooth manifold embedded in a reproducing kernel Hilbert space. Unlike typical manifold-learning routes, which seek low-dimensional patterns via regularizers based on graph-Laplacian matrices, MultiL-KRIM builds instead on the intuitive concept of tangent spaces to manifolds and incorporates collaboration among point-cloud neighbors (regressors) directly into the data-modeling term of the loss function. Multiple kernel functions are allowed to offer robustness and rich approximation properties, while multiple matrix factors offer low-rank modeling, integrate dimensionality reduction, and streamline computations with no need of training data. Two important application domains showcase the functionality
&lt;/p&gt;</description></item><item><title>CAMBranch&#26159;&#19968;&#20010;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#22686;&#24378;MILP&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#20998;&#25903;&#31574;&#30053;&#12290;&#36890;&#36807;&#29983;&#25104;&#22686;&#24378;MILP&#24182;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#65292;CAMBranch&#33021;&#22815;&#33719;&#21462;&#22823;&#37327;&#26631;&#35760;&#30340;&#19987;&#23478;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#25903;&#20915;&#31574;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.03647</link><description>&lt;p&gt;
CAMBranch: &#22522;&#20110;&#22686;&#24378;MILP&#30340;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#20998;&#25903;
&lt;/p&gt;
&lt;p&gt;
CAMBranch: Contrastive Learning with Augmented MILPs for Branching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03647
&lt;/p&gt;
&lt;p&gt;
CAMBranch&#26159;&#19968;&#20010;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#22686;&#24378;MILP&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#20998;&#25903;&#31574;&#30053;&#12290;&#36890;&#36807;&#29983;&#25104;&#22686;&#24378;MILP&#24182;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#65292;CAMBranch&#33021;&#22815;&#33719;&#21462;&#22823;&#37327;&#26631;&#35760;&#30340;&#19987;&#23478;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#25903;&#20915;&#31574;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#24341;&#20837;&#20102;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#26469;&#22686;&#24378;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;(B\&amp;B)&#30340;&#20998;&#25903;&#31574;&#30053;&#12290;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#24378;&#20998;&#25903;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#24182;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#29992;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#19987;&#23478;&#26679;&#26412;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#24378;&#20998;&#25903;&#30340;&#26679;&#26412;&#65292;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CAMBranch: &#22522;&#20110;&#22686;&#24378;MILP&#30340;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#20998;&#25903;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#26377;&#38480;&#25968;&#37327;&#30340;&#26469;&#33258;&#21407;&#22987;MILP&#30340;&#19987;&#23478;&#25968;&#25454;&#24212;&#29992;&#21464;&#37327;&#36716;&#31227;&#26469;&#29983;&#25104;&#22686;&#24378;MILP (AMILP)&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#33719;&#21462;&#22823;&#37327;&#26631;&#35760;&#30340;&#19987;&#23478;&#26679;&#26412;&#12290;CAMBranch&#21033;&#29992;MILP&#21644;AMILP&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#27169;&#22411;&#25429;&#25417;MILP&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#25903;&#20915;&#31574;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements have introduced machine learning frameworks to enhance the Branch and Bound (B\&amp;B) branching policies for solving Mixed Integer Linear Programming (MILP). These methods, primarily relying on imitation learning of Strong Branching, have shown superior performance. However, collecting expert samples for imitation learning, particularly for Strong Branching, is a time-consuming endeavor. To address this challenge, we propose \textbf{C}ontrastive Learning with \textbf{A}ugmented \textbf{M}ILPs for \textbf{Branch}ing (CAMBranch), a framework that generates Augmented MILPs (AMILPs) by applying variable shifting to limited expert data from their original MILPs. This approach enables the acquisition of a considerable number of labeled expert samples. CAMBranch leverages both MILPs and AMILPs for imitation learning and employs contrastive learning to enhance the model's ability to capture MILP features, thereby improving the quality of branching decisions. Experimental resul
&lt;/p&gt;</description></item><item><title>"Lens"&#26159;&#19968;&#20010;&#22522;&#20110;T5&#26550;&#26500;&#30340;&#22522;&#30784;&#32593;&#32476;&#27969;&#37327;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#34920;&#31034;&#65292;&#33021;&#22815;&#22312;&#27969;&#37327;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#31934;&#30830;&#30340;&#39044;&#27979;&#21644;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.03646</link><description>&lt;p&gt;
Lens: &#32593;&#32476;&#27969;&#37327;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lens: A Foundation Model for Network Traffic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03646
&lt;/p&gt;
&lt;p&gt;
"Lens"&#26159;&#19968;&#20010;&#22522;&#20110;T5&#26550;&#26500;&#30340;&#22522;&#30784;&#32593;&#32476;&#27969;&#37327;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#34920;&#31034;&#65292;&#33021;&#22815;&#22312;&#27969;&#37327;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#31934;&#30830;&#30340;&#39044;&#27979;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#27969;&#37327;&#26159;&#25351;&#36890;&#36807;&#20114;&#32852;&#32593;&#25110;&#36830;&#25509;&#35745;&#31639;&#26426;&#30340;&#20219;&#20309;&#31995;&#32479;&#21457;&#36865;&#21644;&#25509;&#25910;&#30340;&#20449;&#24687;&#37327;&#12290;&#20998;&#26512;&#21644;&#29702;&#35299;&#32593;&#32476;&#27969;&#37327;&#23545;&#20110;&#25552;&#39640;&#32593;&#32476;&#23433;&#20840;&#21644;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#21253;&#30340;&#29305;&#27530;&#29305;&#24615;&#65292;&#22914;&#24322;&#26500;&#26631;&#22836;&#21644;&#32570;&#20047;&#35821;&#20041;&#30340;&#21152;&#23494;&#36127;&#36733;&#65292;&#32593;&#32476;&#27969;&#37327;&#30340;&#20998;&#26512;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#25429;&#25417;&#27969;&#37327;&#30340;&#28508;&#22312;&#35821;&#20041;&#65292;&#19968;&#20123;&#30740;&#31350;&#37319;&#29992;&#20102;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#25110;&#35299;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#20174;&#22823;&#35268;&#27169;&#30340;&#27969;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#21482;&#22312;&#27969;&#37327;&#29702;&#35299;&#65288;&#20998;&#31867;&#65289;&#25110;&#27969;&#37327;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Lens&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#30784;&#30340;&#32593;&#32476;&#27969;&#37327;&#27169;&#22411;&#65292;&#21033;&#29992;T5&#26550;&#26500;&#20174;&#22823;&#35268;&#27169;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#39044;&#35757;&#32451;&#34920;&#31034;&#12290;&#20511;&#21161;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#30340;&#20248;&#21183;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#23454;&#29616;&#31934;&#30830;&#30340;&#27969;&#37327;&#39044;&#27979;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network traffic refers to the amount of information being sent and received over the internet or any system that connects computers. Analyzing and understanding network traffic is vital for improving network security and management. However, the analysis of network traffic poses great challenges due to the unique characteristics of data packets, such as heterogeneous headers and encrypted payload lacking semantics. To capture the latent semantics of traffic, a few studies have adopted pre-training techniques based on the Transformer encoder or decoder to learn the representations from large-scale traffic data. However, these methods typically excel only in traffic understanding (classification) or traffic generation tasks. To address this issue, we develop Lens, a foundational network traffic model that leverages the T5 architecture to learn the pre-trained representations from large-scale unlabeled data. Harnessing the strength of the encoder-decoder framework, which captures the glob
&lt;/p&gt;</description></item><item><title>Stanceosaurus 2.0&#25193;&#23637;&#20102;&#21407;&#22987;&#26694;&#26550;&#65292;&#26032;&#22686;&#23545;&#20420;&#32599;&#26031;&#21644;&#35199;&#29677;&#29273;&#30340;&#20998;&#31867;&#65292;&#26088;&#22312;&#25903;&#25345;&#20998;&#26512;&#36328;&#25991;&#21270;&#21644;&#36328;&#35821;&#35328;&#30340;&#34394;&#20551;&#20449;&#24687;&#12290;&#36890;&#36807;&#19982;&#21021;&#22987;&#30740;&#31350;&#30340;&#32467;&#26524;&#30456;&#24403;&#30340;&#38646;-shot&#36328;&#35821;&#35328;&#36801;&#31227;&#65292;&#39564;&#35777;&#20102;&#25968;&#25454;&#30340;&#20215;&#20540;&#21644;&#31435;&#22330;&#20998;&#31867;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03642</link><description>&lt;p&gt;
Stanceosaurus 2.0: &#23545;&#20420;&#32599;&#26031;&#21644;&#35199;&#29677;&#29273;&#30340;&#34394;&#20551;&#20449;&#24687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Stanceosaurus 2.0: Classifying Stance Towards Russian and Spanish Misinformation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03642
&lt;/p&gt;
&lt;p&gt;
Stanceosaurus 2.0&#25193;&#23637;&#20102;&#21407;&#22987;&#26694;&#26550;&#65292;&#26032;&#22686;&#23545;&#20420;&#32599;&#26031;&#21644;&#35199;&#29677;&#29273;&#30340;&#20998;&#31867;&#65292;&#26088;&#22312;&#25903;&#25345;&#20998;&#26512;&#36328;&#25991;&#21270;&#21644;&#36328;&#35821;&#35328;&#30340;&#34394;&#20551;&#20449;&#24687;&#12290;&#36890;&#36807;&#19982;&#21021;&#22987;&#30740;&#31350;&#30340;&#32467;&#26524;&#30456;&#24403;&#30340;&#38646;-shot&#36328;&#35821;&#35328;&#36801;&#31227;&#65292;&#39564;&#35777;&#20102;&#25968;&#25454;&#30340;&#20215;&#20540;&#21644;&#31435;&#22330;&#20998;&#31867;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stanceosaurus &#35821;&#26009;&#24211;&#65288;Zheng&#31561;&#65292;2022&#65289;&#26088;&#22312;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#12289;&#26631;&#27880;&#30340;&#12289;&#20174;Twitter&#20013;&#25552;&#21462;&#30340;&#20116;&#20998;&#31867;&#31435;&#22330;&#25968;&#25454;&#65292;&#36866;&#29992;&#20110;&#20998;&#26512;&#36328;&#25991;&#21270;&#21644;&#36328;&#35821;&#35328;&#30340;&#34394;&#20551;&#20449;&#24687;&#12290;&#22312;Stanceosaurus 2.0&#29256;&#26412;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#20102;&#20420;&#32599;&#26031;&#21644;&#35199;&#29677;&#29273;&#12290;&#21069;&#32773;&#30001;&#20110;&#19982;&#35199;&#26041;&#32039;&#24352;&#23616;&#21183;&#21644;&#23545;&#20044;&#20811;&#20848;&#30340;&#26292;&#21147;&#20837;&#20405;&#32780;&#21464;&#24471;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#32780;&#21518;&#32773;&#21017;&#20195;&#34920;&#30528;&#19968;&#20010;&#34987;&#20027;&#35201;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#24573;&#35270;&#30340;&#24222;&#22823;&#31038;&#32676;&#12290;&#36890;&#36807;&#28155;&#21152;3,874&#26465;&#35199;&#29677;&#29273;&#21644;&#20420;&#35821;&#25512;&#25991;&#65292;&#28041;&#21450;41&#21017;&#34394;&#20551;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25903;&#25345;&#20851;&#27880;&#36825;&#20123;&#38382;&#39064;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#20123;&#25968;&#25454;&#30340;&#20215;&#20540;&#65292;&#25105;&#20204;&#22312;&#22810;&#35821;&#35328;BERT&#19978;&#20351;&#29992;&#38646;-shot&#36328;&#35821;&#35328;&#36801;&#31227;&#65292;&#24471;&#21040;&#20102;&#19982;&#26368;&#21021;&#30340;Stanceosaurus&#30740;&#31350;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#20004;&#31181;&#35821;&#35328;&#30340;macro F1&#24471;&#20998;&#22343;&#20026;43&#12290;&#36825;&#26174;&#31034;&#20102;&#31435;&#22330;&#20998;&#31867;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Stanceosaurus corpus (Zheng et al., 2022) was designed to provide high-quality, annotated, 5-way stance data extracted from Twitter, suitable for analyzing cross-cultural and cross-lingual misinformation. In the Stanceosaurus 2.0 iteration, we extend this framework to encompass Russian and Spanish. The former is of current significance due to prevalent misinformation amid escalating tensions with the West and the violent incursion into Ukraine. The latter, meanwhile, represents an enormous community that has been largely overlooked on major social media platforms. By incorporating an additional 3,874 Spanish and Russian tweets over 41 misinformation claims, our objective is to support research focused on these issues. To demonstrate the value of this data, we employed zero-shot cross-lingual transfer on multilingual BERT, yielding results on par with the initial Stanceosaurus study with a macro F1 score of 43 for both languages. This underlines the viability of stance classificatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#21270;&#23545;&#38544;&#31169;&#25512;&#26029;&#20013;&#32676;&#20307;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20943;&#23569;ReLU&#28608;&#27963;&#20989;&#25968;&#25968;&#37327;&#20250;&#19981;&#25104;&#27604;&#20363;&#22320;&#38477;&#20302;&#23569;&#25968;&#32676;&#20307;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#23545;&#20110;&#22810;&#25968;&#32676;&#20307;&#21017;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#37319;&#29992;&#31616;&#21333;&#30340;&#24494;&#35843;&#27493;&#39588;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03629</link><description>&lt;p&gt;
&#31169;&#26377;&#25512;&#26029;&#30340;&#32447;&#24615;&#21270;&#23545;&#32676;&#20307;&#20934;&#30830;&#24615;&#30340;&#19981;&#23545;&#31216;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Disparate Impact on Group Accuracy of Linearization for Private Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#21270;&#23545;&#38544;&#31169;&#25512;&#26029;&#20013;&#32676;&#20307;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20943;&#23569;ReLU&#28608;&#27963;&#20989;&#25968;&#25968;&#37327;&#20250;&#19981;&#25104;&#27604;&#20363;&#22320;&#38477;&#20302;&#23569;&#25968;&#32676;&#20307;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#23545;&#20110;&#22810;&#25968;&#32676;&#20307;&#21017;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#37319;&#29992;&#31616;&#21333;&#30340;&#24494;&#35843;&#27493;&#39588;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#23545;&#20855;&#26377;&#23494;&#30721;&#23433;&#20840;&#24615;&#30340;&#25968;&#25454;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#30340;&#25512;&#26029;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#20026;&#20102;&#20943;&#36731;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#20013;&#26114;&#36149;&#30340;&#21152;&#23494;&#35745;&#31639;&#30340;&#29942;&#39048;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#24314;&#35758;&#22312;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#20013;&#32447;&#24615;&#21270;&#30446;&#26631;&#37096;&#20998;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#36816;&#34892;&#26102;&#38388;&#65292;&#23545;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#24448;&#24448;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#35745;&#31639;&#20248;&#21183;&#21487;&#33021;&#23548;&#33268;&#20844;&#24179;&#24615;&#25104;&#26412;&#22686;&#21152;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#20943;&#23569;ReLU&#28608;&#27963;&#20989;&#25968;&#25968;&#37327;&#20250;&#19981;&#25104;&#27604;&#20363;&#22320;&#38477;&#20302;&#23569;&#25968;&#32676;&#20307;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#23545;&#20110;&#22810;&#25968;&#32676;&#20307;&#21017;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#22312;&#23545;&#20915;&#31574;&#36793;&#30028;&#24615;&#36136;&#36827;&#34892;&#38480;&#21046;&#24615;&#20551;&#35774;&#30340;&#22522;&#30784;&#19978;&#25552;&#20379;&#20102;&#25968;&#23398;&#35299;&#37322;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#26222;&#36941;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#31616;&#21333;&#30340;&#31243;&#24207;&#25913;&#21464;&#32447;&#24615;&#27169;&#22411;&#30340;&#24494;&#35843;&#27493;&#39588;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring privacy-preserving inference on cryptographically secure data is a well-known computational challenge. To alleviate the bottleneck of costly cryptographic computations in non-linear activations, recent methods have suggested linearizing a targeted portion of these activations in neural networks. This technique results in significantly reduced runtimes with often negligible impacts on accuracy. In this paper, we demonstrate that such computational benefits may lead to increased fairness costs. Specifically, we find that reducing the number of ReLU activations disproportionately decreases the accuracy for minority groups compared to majority groups. To explain these observations, we provide a mathematical interpretation under restricted assumptions about the nature of the decision boundary, while also showing the prevalence of this problem across widely used datasets and architectures. Finally, we show how a simple procedure altering the fine-tuning step for linearized models ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;ReLU&#32593;&#32476;&#22312;&#21152;&#26435;&#34928;&#20943;&#27491;&#21017;&#21270;&#19979;&#21450;&#20854;&#20984;&#26494;&#24347;&#20043;&#38388;&#30340;&#26368;&#20248;&#24615;&#24046;&#36317;&#65292;&#35777;&#26126;&#20102;&#24403;&#35757;&#32451;&#25968;&#25454;&#26159;&#38543;&#26426;&#30340;&#26102;&#20505;&#65292;&#30456;&#23545;&#26368;&#20248;&#24615;&#24046;&#36317;&#21487;&#20197;&#34987;&#19968;&#20010;$O(\sqrt{\log n})$&#30340;&#22240;&#23376;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#23616;&#37096;&#26799;&#24230;&#26041;&#27861;&#20960;&#20046;&#32943;&#23450;&#20250;&#25910;&#25947;&#21040;&#35757;&#32451;&#25439;&#22833;&#36739;&#20302;&#30340;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.03625</link><description>&lt;p&gt;
ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#20984;&#26494;&#24347;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36924;&#36817;&#20840;&#23616;&#26368;&#20248;&#35299;
&lt;/p&gt;
&lt;p&gt;
Convex Relaxations of ReLU Neural Networks Approximate Global Optima in Polynomial Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;ReLU&#32593;&#32476;&#22312;&#21152;&#26435;&#34928;&#20943;&#27491;&#21017;&#21270;&#19979;&#21450;&#20854;&#20984;&#26494;&#24347;&#20043;&#38388;&#30340;&#26368;&#20248;&#24615;&#24046;&#36317;&#65292;&#35777;&#26126;&#20102;&#24403;&#35757;&#32451;&#25968;&#25454;&#26159;&#38543;&#26426;&#30340;&#26102;&#20505;&#65292;&#30456;&#23545;&#26368;&#20248;&#24615;&#24046;&#36317;&#21487;&#20197;&#34987;&#19968;&#20010;$O(\sqrt{\log n})$&#30340;&#22240;&#23376;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#23616;&#37096;&#26799;&#24230;&#26041;&#27861;&#20960;&#20046;&#32943;&#23450;&#20250;&#25910;&#25947;&#21040;&#35757;&#32451;&#25439;&#22833;&#36739;&#20302;&#30340;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;ReLU&#32593;&#32476;&#22312;&#21152;&#26435;&#34928;&#20943;&#27491;&#21017;&#21270;&#19979;&#21450;&#20854;&#20984;&#26494;&#24347;&#20043;&#38388;&#30340;&#26368;&#20248;&#24615;&#24046;&#36317;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#35757;&#32451;&#25968;&#25454;&#26159;&#38543;&#26426;&#30340;&#26102;&#20505;&#65292;&#21407;&#22987;&#38382;&#39064;&#19982;&#20854;&#20984;&#26494;&#24347;&#20043;&#38388;&#30340;&#30456;&#23545;&#26368;&#20248;&#24615;&#24046;&#36317;&#21487;&#20197;&#34987;&#19968;&#20010;$O(\sqrt{\log n})$&#30340;&#22240;&#23376;&#30028;&#38480;&#65292;&#20854;&#20013;$n$&#26159;&#35757;&#32451;&#26679;&#26412;&#30340;&#25968;&#37327;&#12290;&#19968;&#20010;&#31616;&#21333;&#30340;&#24212;&#29992;&#21487;&#20197;&#23548;&#20986;&#19968;&#20010;&#21487;&#34892;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20445;&#35777;&#22312;&#23545;&#25968;&#22240;&#23376;&#33539;&#22260;&#20869;&#35299;&#20915;&#21407;&#22987;&#30340;&#38750;&#20984;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#21442;&#25968;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#19979;&#65292;&#23616;&#37096;&#26799;&#24230;&#26041;&#27861;&#20960;&#20046;&#32943;&#23450;&#20250;&#25910;&#25947;&#21040;&#35757;&#32451;&#25439;&#22833;&#36739;&#20302;&#30340;&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#30456;&#23545;&#20110;&#29616;&#26377;&#32467;&#26524;&#32780;&#35328;&#26159;&#25351;&#25968;&#32423;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#25581;&#31034;&#20102;&#20026;&#20160;&#20040;&#23616;&#37096;&#26799;&#24230;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the optimality gap between two-layer ReLU networks regularized with weight decay and their convex relaxations. We show that when the training data is random, the relative optimality gap between the original problem and its relaxation can be bounded by a factor of $O(\sqrt{\log n})$, where $n$ is the number of training samples. A simple application leads to a tractable polynomial-time algorithm that is guaranteed to solve the original non-convex problem up to a logarithmic factor. Moreover, under mild assumptions, we show that with random initialization on the parameters local gradient methods almost surely converge to a point that has low training loss. Our result is an exponential improvement compared to existing results and sheds new light on understanding why local gradient methods work well.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#27010;&#29575;&#30005;&#36335;&#20013;&#36793;&#38469;MAP&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#36830;&#32493;&#22810;&#32447;&#24615;&#20989;&#25968;&#26469;&#20272;&#35745;&#26597;&#35810;&#21464;&#37327;&#30340;&#36171;&#20540;&#25104;&#26412;&#24182;&#23558;&#20854;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#20855;&#26377;&#33258;&#25105;&#30417;&#30563;&#21644;&#39640;&#25928;&#24615;&#30340;&#20248;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.03621</link><description>&lt;p&gt;
&#27010;&#29575;&#30005;&#36335;&#20013;&#29992;&#20110;&#36793;&#38469;MAP&#30340;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#22120;
&lt;/p&gt;
&lt;p&gt;
Neural Network Approximators for Marginal MAP in Probabilistic Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#27010;&#29575;&#30005;&#36335;&#20013;&#36793;&#38469;MAP&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#36830;&#32493;&#22810;&#32447;&#24615;&#20989;&#25968;&#26469;&#20272;&#35745;&#26597;&#35810;&#21464;&#37327;&#30340;&#36171;&#20540;&#25104;&#26412;&#24182;&#23558;&#20854;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#20855;&#26377;&#33258;&#25105;&#30417;&#30563;&#21644;&#39640;&#25928;&#24615;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#30005;&#36335;&#65288;PCs&#65289;&#22914;&#21644;&#31215;&#32593;&#32476;&#20197;&#39640;&#25928;&#22320;&#34920;&#31034;&#22823;&#22411;&#22810;&#21464;&#37327;&#27010;&#29575;&#20998;&#24067;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#19982;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#39532;&#23572;&#21487;&#22827;&#32593;&#32476;&#31561;&#20854;&#20182;&#27010;&#29575;&#34920;&#31034;&#30456;&#27604;&#65292;PCs&#26356;&#21463;&#38738;&#30544;&#65292;&#22240;&#20026;PCs&#21487;&#20197;&#22312;&#32593;&#32476;&#22823;&#23567;&#32447;&#24615;&#25193;&#23637;&#30340;&#26102;&#38388;&#20869;&#35299;&#20915;&#36793;&#38469;&#25512;&#29702;&#65288;MAR&#65289;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26368;&#22823;&#21518;&#39564;&#27010;&#29575;&#65288;MAP&#65289;&#21644;&#36793;&#38469;MAP&#65288;MMAP&#65289;&#20219;&#21153;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#20173;&#28982;&#26159;NP&#22256;&#38590;&#30340;&#12290;&#21463;&#26368;&#36817;&#20851;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#25509;&#36817;&#26368;&#20248;&#35299;&#30340;&#20248;&#21270;&#38382;&#39064;&#65288;&#22914;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65289;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;PCs&#20013;&#30340;(M)MAP&#25512;&#29702;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#29992;&#36830;&#32493;&#22810;&#32447;&#24615;&#20989;&#25968;&#26469;&#36817;&#20284;&#26597;&#35810;&#21464;&#37327;&#30340;&#36171;&#20540;&#25104;&#26412;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20316;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#26377;&#20004;&#20010;&#20027;&#35201;&#20248;&#28857;&#65292;&#21363;&#33258;&#25105;&#30417;&#30563;&#21644;&#22312;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20043;&#21518;&#65292;&#23427;&#21482;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
Probabilistic circuits (PCs) such as sum-product networks efficiently represent large multi-variate probability distributions. They are preferred in practice over other probabilistic representations such as Bayesian and Markov networks because PCs can solve marginal inference (MAR) tasks in time that scales linearly in the size of the network. Unfortunately, the maximum-a-posteriori (MAP) and marginal MAP (MMAP) tasks remain NP-hard in these models. Inspired by the recent work on using neural networks for generating near-optimal solutions to optimization problems such as integer linear programming, we propose an approach that uses neural networks to approximate (M)MAP inference in PCs. The key idea in our approach is to approximate the cost of an assignment to the query variables using a continuous multilinear function, and then use the latter as a loss function. The two main benefits of our new method are that it is self-supervised and after the neural network is learned, it requires 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;VAR&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#23618;&#22270;&#20808;&#39564;&#25512;&#26029;&#20108;&#20803;&#26684;&#20848;&#26480;&#22240;&#26524;&#22270;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#30456;&#27604;&#31454;&#20105;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12289;&#36229;&#21442;&#25968;&#25968;&#37327;&#21644;&#31232;&#30095;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#37117;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.03614</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#20998;&#35299;&#26684;&#20848;&#26480;&#22240;&#26524;&#22270;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Bayesian Factorised Granger-Causal Graphs For Multivariate Time-series Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;VAR&#27169;&#22411;&#65292;&#21033;&#29992;&#20998;&#23618;&#22270;&#20808;&#39564;&#25512;&#26029;&#20108;&#20803;&#26684;&#20848;&#26480;&#22240;&#26524;&#22270;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#30456;&#27604;&#31454;&#20105;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12289;&#36229;&#21442;&#25968;&#25968;&#37327;&#21644;&#31232;&#30095;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#37117;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#21160;&#21457;&#29616;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#26684;&#20848;&#26480;&#22240;&#26524;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#30690;&#37327;&#33258;&#22238;&#24402;(VAR)&#27169;&#22411;&#24050;&#32463;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#19978;&#32463;&#36807;&#20102;&#26102;&#38388;&#30340;&#32771;&#39564;&#65292;&#21253;&#25324;&#36125;&#21494;&#26031;&#21464;&#31181;&#21644;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;VAR&#26684;&#20848;&#26480;&#22240;&#26524;&#26041;&#27861;&#20351;&#29992;&#31232;&#30095;&#24615;&#35825;&#23548;&#24809;&#32602;/&#20808;&#39564;&#25110;&#20107;&#21518;&#38408;&#20540;&#26469;&#35299;&#37322;&#23427;&#20204;&#30340;&#31995;&#25968;&#20316;&#20026;&#26684;&#20848;&#26480;&#22240;&#26524;&#22270;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36125;&#21494;&#26031;VAR&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#20010;&#20998;&#23618;&#22270;&#20808;&#39564;&#26469;&#34920;&#31034;&#20108;&#20803;&#26684;&#20848;&#26480;&#22240;&#26524;&#22270;&#65292;&#19982;VAR&#31995;&#25968;&#20998;&#24320;&#32771;&#34385;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#25512;&#26029;&#20108;&#20803;&#26684;&#20848;&#26480;&#22240;&#26524;&#22270;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#36739;&#23569;&#30340;&#36229;&#21442;&#25968;&#65292;&#24182;&#22312;&#31232;&#30095;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of automatically discovering Granger causal relations from observational multivariate time-series data. Vector autoregressive (VAR) models have been time-tested for this problem, including Bayesian variants and more recent developments using deep neural networks. Most existing VAR methods for Granger causality use sparsity-inducing penalties/priors or post-hoc thresholds to interpret their coefficients as Granger causal graphs. Instead, we propose a new Bayesian VAR model with a hierarchical graph prior over binary Granger causal graphs, separately from the VAR coefficients. We develop an efficient algorithm to infer the posterior over binary Granger causal graphs. Our method provides better uncertainty quantification, has less hyperparameters, and achieves better performance than competing approaches, especially on sparse multivariate time-series data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RAP&#30340;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#20197;&#21160;&#24577;&#26041;&#24335;&#21033;&#29992;&#36807;&#21435;&#30340;&#32463;&#39564;&#26469;&#22686;&#24378;&#20195;&#29702;&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#24182;&#22312;&#32431;&#25991;&#26412;&#21644;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;RAP&#22312;&#25991;&#26412;&#22330;&#26223;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;LLM&#20195;&#29702;&#22312;&#20855;&#36523;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03610</link><description>&lt;p&gt;
RAP&#65306;&#20855;&#26377;&#19978;&#19979;&#25991;&#35760;&#24518;&#30340;&#26816;&#32034;&#22686;&#24378;&#35268;&#21010;&#29992;&#20110;&#22810;&#27169;&#24577;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RAP&#30340;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#20197;&#21160;&#24577;&#26041;&#24335;&#21033;&#29992;&#36807;&#21435;&#30340;&#32463;&#39564;&#26469;&#22686;&#24378;&#20195;&#29702;&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#24182;&#22312;&#32431;&#25991;&#26412;&#21644;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;RAP&#22312;&#25991;&#26412;&#22330;&#26223;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;LLM&#20195;&#29702;&#22312;&#20855;&#36523;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21487;&#20197;&#34987;&#37096;&#32626;&#20026;&#29992;&#20110;&#26426;&#22120;&#20154;&#12289;&#28216;&#25103;&#21644;API&#38598;&#25104;&#31561;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#20915;&#31574;&#24212;&#29992;&#30340;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#36807;&#21435;&#30340;&#32463;&#39564;&#26469;&#25351;&#23548;&#24403;&#21069;&#30340;&#20915;&#31574;&#36807;&#31243;&#20173;&#28982;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26816;&#32034;&#22686;&#24378;&#35268;&#21010;&#65288;RAP&#65289;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21160;&#24577;&#22320;&#21033;&#29992;&#36807;&#21435;&#19982;&#24403;&#21069;&#24773;&#20917;&#21644;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#32463;&#39564;&#65292;&#20174;&#32780;&#25552;&#21319;&#20195;&#29702;&#30340;&#35268;&#21010;&#33021;&#21147;&#12290;RAP&#30340;&#29305;&#28857;&#22312;&#20110;&#23427;&#30340;&#22810;&#21151;&#33021;&#24615;&#65306;&#23427;&#22312;&#32431;&#25991;&#26412;&#21644;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#20219;&#21153;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;RAP&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#25991;&#26412;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;SOTA&#30340;&#34920;&#29616;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#22810;&#27169;&#24577;LLM&#20195;&#29702;&#22312;&#20855;&#36523;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#26174;&#20102;RAP&#22312;&#25512;&#36827;LLM&#30340;&#21151;&#33021;&#21644;&#36866;&#29992;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration. However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges. Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities. RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks. Empirical evaluations demonstrate RAP's effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents' performance for embodied tasks. These results highlight RAP's potential in advancing the functionality and applicability of LLM 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#22312;&#35782;&#21035;&#36991;&#23381;&#33647;&#20999;&#25442;&#21407;&#22240;&#19978;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-4&#21487;&#20197;&#20934;&#30830;&#22320;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#36991;&#23381;&#33647;&#20999;&#25442;&#30340;&#21407;&#22240;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;BERT&#27169;&#22411;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.03597</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#23454;&#38469;&#25968;&#25454;&#20013;&#35782;&#21035;&#36991;&#23381;&#33647;&#20999;&#25442;&#30340;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Identifying Reasons for Contraceptive Switching from Real-World Data Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#22312;&#35782;&#21035;&#36991;&#23381;&#33647;&#20999;&#25442;&#21407;&#22240;&#19978;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-4&#21487;&#20197;&#20934;&#30830;&#22320;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#36991;&#23381;&#33647;&#20999;&#25442;&#30340;&#21407;&#22240;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;BERT&#27169;&#22411;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#26041;&#36991;&#23381;&#33647;&#22312;&#25903;&#25345;&#22919;&#22899;&#29983;&#27542;&#20581;&#24247;&#26041;&#38754;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#12290;&#22312;&#32654;&#22269;&#26377;&#23558;&#36817;5000&#19975;&#22899;&#24615;&#20351;&#29992;&#36991;&#23381;&#33647;&#65292;&#20102;&#35299;&#23548;&#33268;&#36991;&#23381;&#33647;&#36873;&#25321;&#21644;&#20999;&#25442;&#30340;&#22240;&#32032;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#19982;&#33647;&#29289;&#20999;&#25442;&#30456;&#20851;&#30340;&#35768;&#22810;&#22240;&#32032;&#36890;&#24120;&#21482;&#22312;&#26080;&#32467;&#26500;&#30340;&#20020;&#24202;&#35760;&#24405;&#20013;&#24471;&#21040;&#25429;&#33719;&#65292;&#24182;&#19988;&#24456;&#38590;&#25552;&#21462;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#36817;&#24320;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#65288;&#36890;&#36807;&#31526;&#21512;HIPAA&#30340;Microsoft Azure API&#65289;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#20197;&#20174;UCSF&#20449;&#24687;&#20849;&#20139;&#24179;&#21488;&#30340;&#20020;&#24202;&#35760;&#24405;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#36991;&#23381;&#33647;&#31867;&#21035;&#20999;&#25442;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GPT-4&#21487;&#20197;&#20934;&#30830;&#22320;&#25552;&#21462;&#36991;&#23381;&#33647;&#20999;&#25442;&#30340;&#21407;&#22240;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;BERT&#27169;&#22411;&#65292;&#22312;&#36991;&#23381;&#33647;&#24320;&#22987;&#21644;&#20572;&#27490;&#25552;&#21462;&#26041;&#38754;&#30340;microF1&#20998;&#25968;&#20998;&#21035;&#20026;0.849&#21644;0.881&#12290;&#23545;&#20110;GPT-4&#25552;&#21462;&#30340;&#20999;&#25442;&#21407;&#22240;&#30340;&#20154;&#24037;&#35780;&#20272;&#26174;&#31034;&#20986;91.4%&#30340;&#20934;&#30830;&#24230;&#65292;&#20986;&#29616;&#24187;&#35273;&#30340;&#24773;&#20917;&#24456;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prescription contraceptives play a critical role in supporting women's reproductive health. With nearly 50 million women in the United States using contraceptives, understanding the factors that drive contraceptives selection and switching is of significant interest. However, many factors related to medication switching are often only captured in unstructured clinical notes and can be difficult to extract. Here, we evaluate the zero-shot abilities of a recently developed large language model, GPT-4 (via HIPAA-compliant Microsoft Azure API), to identify reasons for switching between classes of contraceptives from the UCSF Information Commons clinical notes dataset. We demonstrate that GPT-4 can accurately extract reasons for contraceptive switching, outperforming baseline BERT-based models with microF1 scores of 0.849 and 0.881 for contraceptive start and stop extraction, respectively. Human evaluation of GPT-4-extracted reasons for switching showed 91.4% accuracy, with minimal hallucin
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#26102;&#38656;&#35201;&#32771;&#34385;&#20998;&#24067;&#36716;&#21464;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#25512;&#33616;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#36827;&#34892;&#35266;&#27979;RL&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.03590</link><description>&lt;p&gt;
&#35780;&#20272;&#20998;&#24067;&#36716;&#21464;&#23545;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Assessing the Impact of Distribution Shift on Reinforcement Learning Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03590
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#26102;&#38656;&#35201;&#32771;&#34385;&#20998;&#24067;&#36716;&#21464;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#25512;&#33616;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#36827;&#34892;&#35266;&#27979;RL&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#27491;&#22312;&#35299;&#20915;&#33258;&#36523;&#30340;&#21487;&#37325;&#22797;&#24615;&#21361;&#26426;&#12290;&#29305;&#21035;&#26159;&#65292;&#24378;&#21270;&#23398;&#20064;(RL)&#38754;&#20020;&#30528;&#19968;&#31995;&#21015;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#27604;&#36739;&#28857;&#20272;&#35745;&#21644;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26174;&#31034;&#25104;&#21151;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#22270;&#34920;&#21487;&#33021;&#20250;&#25513;&#30422;&#36807;&#25311;&#21512;&#25110;&#23545;&#23454;&#39564;&#35774;&#32622;&#30340;&#20381;&#36182;&#24615;&#12290;&#34429;&#28982;RL&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#21487;&#38752;&#24615;&#25351;&#26631;&#20197;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#27599;&#20010;&#31639;&#27861;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#20294;&#36807;&#21435;&#30340;&#24037;&#20316;&#24314;&#35758;&#24182;&#19981;&#20551;&#35774;&#23384;&#22312;&#36229;&#20986;&#20998;&#24067;&#30340;&#35266;&#27979;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#35780;&#20272;&#26041;&#27861;&#65292;&#34913;&#37327;&#20102;&#22312;&#20998;&#24067;&#36716;&#21464;&#19979;RL&#31639;&#27861;&#30340;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#30340;&#24037;&#20855;&#25903;&#25345;&#22312;&#20195;&#29702;&#22312;&#20854;&#29615;&#22659;&#20013;&#34892;&#21160;&#26102;&#32771;&#34385;&#24615;&#33021;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#23588;&#20854;&#25512;&#33616;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20316;&#20026;&#35266;&#27979;RL&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;RL&#21644;&#27169;&#25311;&#21160;&#21147;&#23398;&#30340;&#29420;&#29305;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research in machine learning is making progress in fixing its own reproducibility crisis. Reinforcement learning (RL), in particular, faces its own set of unique challenges. Comparison of point estimates, and plots that show successful convergence to the optimal policy during training, may obfuscate overfitting or dependence on the experimental setup. Although researchers in RL have proposed reliability metrics that account for uncertainty to better understand each algorithm's strengths and weaknesses, the recommendations of past work do not assume the presence of out-of-distribution observations. We propose a set of evaluation methods that measure the robustness of RL algorithms under distribution shifts. The tools presented here argue for the need to account for performance over time while the agent is acting in its environment. In particular, we recommend time series analysis as a method of observational RL evaluation. We also show that the unique properties of RL and simulated dyna
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#33258;&#34892;&#36710;&#20849;&#20139;&#31995;&#32479;&#20013;&#21160;&#24577;&#20877;&#24179;&#34913;&#38382;&#39064;&#30340;&#26102;&#31354;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#23454;&#29616;&#29420;&#31435;&#21644;&#21327;&#20316;&#30340;&#36710;&#36742;&#20877;&#24179;&#34913;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25968;&#23398;&#20248;&#21270;&#26041;&#27861;&#30340;&#19981;&#23454;&#38469;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.03589</link><description>&lt;p&gt;
&#33258;&#34892;&#36710;&#20849;&#20139;&#31995;&#32479;&#20013;&#21160;&#24577;&#20877;&#24179;&#34913;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Reinforcement Learning Approach for Dynamic Rebalancing in Bike-Sharing System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#33258;&#34892;&#36710;&#20849;&#20139;&#31995;&#32479;&#20013;&#21160;&#24577;&#20877;&#24179;&#34913;&#38382;&#39064;&#30340;&#26102;&#31354;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#23454;&#29616;&#29420;&#31435;&#21644;&#21327;&#20316;&#30340;&#36710;&#36742;&#20877;&#24179;&#34913;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25968;&#23398;&#20248;&#21270;&#26041;&#27861;&#30340;&#19981;&#23454;&#38469;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#34892;&#36710;&#20849;&#20139;&#31995;&#32479;&#25552;&#20379;&#29615;&#20445;&#30340;&#22478;&#24066;&#20986;&#34892;&#26041;&#24335;&#65292;&#26377;&#21161;&#20110;&#32531;&#35299;&#20132;&#36890;&#25317;&#22581;&#65292;&#20419;&#36827;&#20581;&#24247;&#29983;&#27963;&#26041;&#24335;&#12290;&#30001;&#20110;&#34892;&#31243;&#38656;&#27714;&#30340;&#38543;&#26426;&#24615;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#26377;&#25928;&#36816;&#33829;&#21644;&#20445;&#25345;&#39640;&#23458;&#25143;&#28385;&#24847;&#24230;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24120;&#24120;&#20986;&#29616;&#28385;&#31449;&#25110;&#31354;&#31449;&#29616;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#29992;&#36710;&#36742;&#37325;&#26032;&#20998;&#37197;&#33258;&#34892;&#36710;&#21040;&#19981;&#21516;&#31449;&#28857;&#30340;&#20877;&#24179;&#34913;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26102;&#31354;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24102;&#26377;&#22810;&#36742;&#36710;&#36742;&#30340;&#21160;&#24577;&#20877;&#24179;&#34913;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#22312;&#36830;&#32493;&#26102;&#38388;&#26694;&#26550;&#20013;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#36825;&#20801;&#35768;&#29420;&#31435;&#21644;&#21327;&#20316;&#30340;&#36710;&#36742;&#20877;&#24179;&#34913;&#65292;&#28040;&#38500;&#20102;&#22522;&#20110;&#26102;&#38388;&#31163;&#25955;&#21270;&#27169;&#22411;&#30340;&#19981;&#20999;&#23454;&#38469;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bike-Sharing Systems provide eco-friendly urban mobility, contributing to the alleviation of traffic congestion and to healthier lifestyles. Efficiently operating such systems and maintaining high customer satisfaction is challenging due to the stochastic nature of trip demand, leading to full or empty stations. Devising effective rebalancing strategies using vehicles to redistribute bikes among stations is therefore of uttermost importance for operators. As a promising alternative to classical mathematical optimization, reinforcement learning is gaining ground to solve sequential decision-making problems. This paper introduces a spatio-temporal reinforcement learning algorithm for the dynamic rebalancing problem with multiple vehicles. We first formulate the problem as a Multi-agent Markov Decision Process in a continuous time framework. This allows for independent and cooperative vehicle rebalancing, eliminating the impractical restriction of time-discretized models where vehicle dep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21452;&#22836;&#21028;&#21035;&#22120;&#36827;&#34892;&#36830;&#32493;&#39046;&#22495;&#23545;&#25239;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#22312;&#28304;&#23398;&#20064;&#38454;&#27573;&#24341;&#20837;&#20102;&#19968;&#20010;&#20165;&#22312;&#28304;&#22495;&#35757;&#32451;&#30340;&#28304;&#22495;&#21028;&#21035;&#22120;&#65292;&#20943;&#23569;&#20102;&#23545;&#25239;&#25439;&#22833;&#30340;&#32463;&#39564;&#20272;&#35745;&#35823;&#24046;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#31639;&#27861;&#23454;&#29616;&#20102;&#36229;&#36807;2%&#30340;&#20934;&#30830;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.03588</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#22836;&#21028;&#21035;&#22120;&#36827;&#34892;&#36830;&#32493;&#39046;&#22495;&#23545;&#25239;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Continual Domain Adversarial Adaptation via Double-Head Discriminators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21452;&#22836;&#21028;&#21035;&#22120;&#36827;&#34892;&#36830;&#32493;&#39046;&#22495;&#23545;&#25239;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#22312;&#28304;&#23398;&#20064;&#38454;&#27573;&#24341;&#20837;&#20102;&#19968;&#20010;&#20165;&#22312;&#28304;&#22495;&#35757;&#32451;&#30340;&#28304;&#22495;&#21028;&#21035;&#22120;&#65292;&#20943;&#23569;&#20102;&#23545;&#25239;&#25439;&#22833;&#30340;&#32463;&#39564;&#20272;&#35745;&#35823;&#24046;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#31639;&#27861;&#23454;&#29616;&#20102;&#36229;&#36807;2%&#30340;&#20934;&#30830;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#35774;&#32622;&#19979;&#30340;&#39046;&#22495;&#23545;&#25239;&#36866;&#24212;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#23384;&#22312;&#23545;&#20043;&#21069;&#30340;&#28304;&#22495;&#25968;&#25454;&#30340;&#35775;&#38382;&#38480;&#21046;&#12290;&#23613;&#31649;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#20165;&#20165;&#20351;&#29992;&#23569;&#37327;&#23384;&#20648;&#30340;&#28304;&#22495;&#25968;&#25454;&#65288;&#36825;&#26159;&#35760;&#24518;&#37325;&#25773;&#26041;&#27861;&#20013;&#30340;&#26631;&#20934;&#35774;&#32622;&#65289;&#26080;&#27861;&#26377;&#25928;&#23436;&#25104;&#23545;&#25239;&#36866;&#24212;&#20219;&#21153;&#12290;&#36825;&#20010;&#38480;&#21046;&#26469;&#33258;&#20110;&#20351;&#29992;&#23569;&#37327;&#28304;&#22495;&#26679;&#26412;&#23545;$\gH$-divergence&#36827;&#34892;&#32463;&#39564;&#20272;&#35745;&#30340;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#22836;&#21028;&#21035;&#22120;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20165;&#22312;&#28304;&#23398;&#20064;&#38454;&#27573;&#35757;&#32451;&#30340;&#28304;&#22495;&#21028;&#21035;&#22120;&#65292;&#20174;&#28304;&#22495;&#19968;&#20391;&#20943;&#23569;&#20102;$\gH$-divergence&#30456;&#20851;&#23545;&#25239;&#25439;&#22833;&#30340;&#32463;&#39564;&#20272;&#35745;&#35823;&#24046;&#12290;&#36827;&#19968;&#27493;&#22312;&#29616;&#26377;&#30340;&#39046;&#22495;&#36866;&#24212;&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#36229;&#36807;2%&#30340;&#20934;&#30830;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adversarial adaptation in a continual setting poses a significant challenge due to the limitations on accessing previous source domain data. Despite extensive research in continual learning, the task of adversarial adaptation cannot be effectively accomplished using only a small number of stored source domain data, which is a standard setting in memory replay approaches. This limitation arises from the erroneous empirical estimation of $\gH$-divergence with few source domain samples. To tackle this problem, we propose a double-head discriminator algorithm, by introducing an addition source-only domain discriminator that are trained solely on source learning phase. We prove that with the introduction of a pre-trained source-only domain discriminator, the empirical estimation error of $\gH$-divergence related adversarial loss is reduced from the source domain side. Further experiments on existing domain adaptation benchmark show that our proposed algorithm achieves more than 2$\%$
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26377;&#25928;&#30340;&#33719;&#21462;&#20989;&#25968;&#29992;&#20110;&#20027;&#21160;&#30456;&#20851;&#32858;&#31867;&#65292;&#20998;&#21035;&#22522;&#20110;&#19981;&#19968;&#33268;&#24615;&#27010;&#24565;&#21644;&#20449;&#24687;&#35770;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.03587</link><description>&lt;p&gt;
&#20027;&#21160;&#30456;&#20851;&#32858;&#31867;&#30340;&#26377;&#25928;&#33719;&#21462;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Effective Acquisition Functions for Active Correlation Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26377;&#25928;&#30340;&#33719;&#21462;&#20989;&#25968;&#29992;&#20110;&#20027;&#21160;&#30456;&#20851;&#32858;&#31867;&#65292;&#20998;&#21035;&#22522;&#20110;&#19981;&#19968;&#33268;&#24615;&#27010;&#24565;&#21644;&#20449;&#24687;&#35770;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#32858;&#31867;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#33539;&#20363;&#65292;&#25903;&#25345;&#27491;&#21644;&#36127;&#30340;&#30456;&#20284;&#24615;&#12290;&#26412;&#25991;&#20551;&#35774;&#30456;&#20284;&#24615;&#20107;&#20808;&#26410;&#30693;&#65292;&#32780;&#26159;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;&#20197;&#19968;&#31181;&#25104;&#26412;&#26377;&#25928;&#30340;&#26041;&#24335;&#36845;&#20195;&#22320;&#26597;&#35810;&#30456;&#20284;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#26377;&#25928;&#30340;&#33719;&#21462;&#20989;&#25968;&#29992;&#20110;&#22312;&#27492;&#35774;&#32622;&#19979;&#20351;&#29992;&#12290;&#20854;&#20013;&#19968;&#31181;&#22522;&#20110;&#19981;&#19968;&#33268;&#24615;&#27010;&#24565;&#65288;&#21363;&#24403;&#30456;&#20284;&#24615;&#36829;&#21453;&#20256;&#36882;&#24615;&#26102;&#65289;&#12290;&#20854;&#20313;&#20004;&#20010;&#22522;&#20110;&#20449;&#24687;&#35770;&#37327;&#65292;&#21363;&#29109;&#21644;&#20449;&#24687;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Correlation clustering is a powerful unsupervised learning paradigm that supports positive and negative similarities. In this paper, we assume the similarities are not known in advance. Instead, we employ active learning to iteratively query similarities in a cost-efficient way. In particular, we develop three effective acquisition functions to be used in this setting. One is based on the notion of inconsistency (i.e., when similarities violate the transitive property). The remaining two are based on information-theoretic quantities, i.e., entropy and information gain.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#21457;&#29616;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#23384;&#22312;&#30340;&#8220;Z-&#24726;&#35770;&#8221;&#38480;&#21046;&#20102;&#20854;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MQuinE&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#65292;MQuinE&#25104;&#21151;&#35299;&#20915;&#20102;Z-&#24726;&#35770;&#65292;&#24182;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.03583</link><description>&lt;p&gt;
MQuinE:&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#20013;&#8220;Z-&#24726;&#35770;&#8221;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
MQuinE: a cure for "Z-paradox'' in knowledge graph embedding models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03583
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#21457;&#29616;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#23384;&#22312;&#30340;&#8220;Z-&#24726;&#35770;&#8221;&#38480;&#21046;&#20102;&#20854;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MQuinE&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#65292;MQuinE&#25104;&#21151;&#35299;&#20915;&#20102;Z-&#24726;&#35770;&#65292;&#24182;&#22312;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#27169;&#22411;&#22312;&#35768;&#22810;&#30693;&#35782;&#22270;&#35889;&#20219;&#21153;&#65292;&#21253;&#25324;&#38142;&#25509;&#39044;&#27979;&#21644;&#20449;&#24687;&#26816;&#32034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;KGE&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#19968;&#20123;&#27969;&#34892;&#30340;&#29616;&#26377;KGE&#27169;&#22411;&#23384;&#22312;&#34920;&#36798;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#31216;&#20026;&#8220;Z-&#24726;&#35770;&#8221;&#12290;&#21463;&#21040;Z-&#24726;&#35770;&#30340;&#23384;&#22312;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KGE&#27169;&#22411;&#65292;&#31216;&#20026;MQuinE&#65292;&#22312;&#19981;&#21463;Z-&#24726;&#35770;&#30340;&#22256;&#25200;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#26469;&#27169;&#25311;&#21508;&#31181;&#20851;&#31995;&#27169;&#24335;&#65292;&#21253;&#25324;&#23545;&#31216;/&#38750;&#23545;&#31216;&#65292;&#36870;&#21521;&#65292;1-N/N-1/N-N&#21644;&#32452;&#21512;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#12290;&#23545;&#23454;&#38469;&#30693;&#35782;&#24211;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Z-&#24726;&#35770;&#30830;&#23454;&#38477;&#20302;&#20102;&#29616;&#26377;KGE&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#33021;&#23548;&#33268;&#26576;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27979;&#35797;&#26679;&#26412;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#36229;&#36807;20&#65285;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;MQuinE&#21487;&#20197;&#20943;&#36731;Z-&#24726;&#35770;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#22312;&#38142;&#25509;&#39044;&#27979;&#26041;&#38754;&#20197;&#26126;&#26174;&#20248;&#21183;&#36229;&#36234;&#29616;&#26377;&#30340;KGE&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) models achieved state-of-the-art results on many knowledge graph tasks including link prediction and information retrieval. Despite the superior performance of KGE models in practice, we discover a deficiency in the expressiveness of some popular existing KGE models called \emph{Z-paradox}. Motivated by the existence of Z-paradox, we propose a new KGE model called \emph{MQuinE} that does not suffer from Z-paradox while preserves strong expressiveness to model various relation patterns including symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations with theoretical justification. Experiments on real-world knowledge bases indicate that Z-paradox indeed degrades the performance of existing KGE models, and can cause more than 20\% accuracy drop on some challenging test samples. Our experiments further demonstrate that MQuinE can mitigate the negative impact of Z-paradox and outperform existing KGE models by a visible margin on link prediction
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#21021;&#22987;&#21270;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#25439;&#22833;&#28023;&#26862;&#30697;&#38453;&#30340;&#39640;&#27491;&#26354;&#29575;&#19982;&#21487;&#35757;&#32451;&#24615;&#24378;&#30340;&#21021;&#22987;&#28857;&#30456;&#20851;&#12290;&#19982;&#20808;&#21069;&#30340;&#35266;&#24565;&#30456;&#21453;&#65292;&#27491;&#26354;&#29575;&#24182;&#19981;&#20165;&#20165;&#19982;&#21021;&#22987;&#21270;&#33539;&#25968;&#30456;&#20851;&#65292;&#32780;&#19982;&#27169;&#22411;&#32622;&#20449;&#24230;&#12289;&#21021;&#22987;&#25439;&#22833;&#36739;&#20302;&#20197;&#21450;&#19968;&#31181;&#20197;&#21069;&#26410;&#30693;&#30340;&#25439;&#22833;&#26799;&#24230;&#28040;&#22833;&#30456;&#20851;&#12290;</title><link>https://arxiv.org/abs/2402.03579</link><description>&lt;p&gt;
&#35299;&#26500;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#30340;&#8220;&#37329;&#21457;&#22899;&#23401;&#21306;&#8221;
&lt;/p&gt;
&lt;p&gt;
Deconstructing the Goldilocks Zone of Neural Network Initialization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03579
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#21021;&#22987;&#21270;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#25439;&#22833;&#28023;&#26862;&#30697;&#38453;&#30340;&#39640;&#27491;&#26354;&#29575;&#19982;&#21487;&#35757;&#32451;&#24615;&#24378;&#30340;&#21021;&#22987;&#28857;&#30456;&#20851;&#12290;&#19982;&#20808;&#21069;&#30340;&#35266;&#24565;&#30456;&#21453;&#65292;&#27491;&#26354;&#29575;&#24182;&#19981;&#20165;&#20165;&#19982;&#21021;&#22987;&#21270;&#33539;&#25968;&#30456;&#20851;&#65292;&#32780;&#19982;&#27169;&#22411;&#32622;&#20449;&#24230;&#12289;&#21021;&#22987;&#25439;&#22833;&#36739;&#20302;&#20197;&#21450;&#19968;&#31181;&#20197;&#21069;&#26410;&#30693;&#30340;&#25439;&#22833;&#26799;&#24230;&#28040;&#22833;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#25439;&#22833;&#30340;&#20108;&#38454;&#24615;&#36136;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20248;&#21270;&#21160;&#21147;&#23398;&#26377;&#30528;&#24040;&#22823;&#24433;&#21709;&#12290;Fort&#65286;Scherlis&#65288;2019&#65289;&#21457;&#29616;&#65292;&#25439;&#22833;&#28023;&#26862;&#30697;&#38453;&#30340;&#39640;&#27491;&#26354;&#29575;&#21644;&#23616;&#37096;&#20984;&#24615;&#19982;&#20301;&#20110;&#34987;&#31216;&#20026;&#8220;&#37329;&#21457;&#22899;&#23401;&#21306;&#8221;&#30340;&#39640;&#24230;&#21487;&#35757;&#32451;&#30340;&#21021;&#22987;&#28857;&#30456;&#20851;&#12290;&#21482;&#26377;&#23569;&#25968;&#20960;&#39033;&#21518;&#32493;&#30740;&#31350;&#28041;&#21450;&#35813;&#20851;&#31995;&#65292;&#22240;&#27492;&#20854;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#22343;&#36136;&#31070;&#32463;&#32593;&#32476;&#30340;&#8220;&#37329;&#21457;&#22899;&#23401;&#21306;&#8221;&#36827;&#34892;&#20102;&#20005;&#26684;&#32780;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#23548;&#33268;&#25439;&#22833;&#28023;&#26862;&#30697;&#38453;&#38750;&#38646;&#27491;&#26354;&#29575;&#30340;&#22522;&#26412;&#26465;&#20214;&#65292;&#24182;&#35748;&#20026;&#23427;&#19982;&#21021;&#22987;&#21270;&#33539;&#25968;&#21482;&#26159;&#20598;&#28982;&#30456;&#20851;&#65292;&#19982;&#20808;&#21069;&#30340;&#20449;&#24565;&#30456;&#21453;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#39640;&#27491;&#26354;&#29575;&#19982;&#27169;&#22411;&#32622;&#20449;&#24230;&#12289;&#21021;&#22987;&#25439;&#22833;&#36739;&#20302;&#20197;&#21450;&#19968;&#31181;&#20197;&#21069;&#26410;&#30693;&#30340;&#28040;&#22833;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#26799;&#24230;&#30456;&#20851;&#32852;&#12290;&#20026;&#20102;&#20102;&#35299;&#27491;&#26354;&#29575;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35757;&#32451;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#30740;&#31350;&#23398;&#20064;&#30340;&#33021;&#21147;&#21644;&#25928;&#29575;&#65292;&#39318;&#20808;&#26159;&#36890;&#36807;&#27867;&#21270;&#35823;&#24046;&#21644;&#39640;&#25928;&#29575;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#26631;&#20934;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The second-order properties of the training loss have a massive impact on the optimization dynamics of deep learning models. Fort &amp; Scherlis (2019) discovered that a high positive curvature and local convexity of the loss Hessian are associated with highly trainable initial points located in a region coined the "Goldilocks zone". Only a handful of subsequent studies touched upon this relationship, so it remains largely unexplained. In this paper, we present a rigorous and comprehensive analysis of the Goldilocks zone for homogeneous neural networks. In particular, we derive the fundamental condition resulting in non-zero positive curvature of the loss Hessian and argue that it is only incidentally related to the initialization norm, contrary to prior beliefs. Further, we relate high positive curvature to model confidence, low initial loss, and a previously unknown type of vanishing cross-entropy loss gradient. To understand the importance of positive curvature for trainability of deep 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#32479;&#35745;&#23398;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;&#8220;&#25968;&#25454;&#38598;&#20559;&#24046;&#8221;&#38382;&#39064;&#65292;&#21457;&#29616;&#20854;&#20027;&#35201;&#21407;&#22240;&#26159;&#36755;&#20837;&#30340;&#31867;&#23646;&#24615;&#21644;&#38750;&#31867;&#23646;&#24615;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32771;&#34385;&#36825;&#31181;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#25968;&#25454;&#38598;&#20559;&#24046;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27599;&#20010;&#26679;&#26412;&#30340;&#30446;&#26631;&#21152;&#26435;&#25110;&#20197;&#26435;&#37325;&#27604;&#20363;&#37319;&#26679;&#26469;&#23454;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#26356;&#21152;&#31283;&#23450;&#21644;&#26377;&#25928;&#65292;&#24182;&#19988;&#19982;&#22240;&#26524;&#25512;&#29702;&#26377;&#19968;&#23450;&#30340;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2402.03577</link><description>&lt;p&gt;
&#20174;&#32479;&#35745;&#23398;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#25968;&#25454;&#38598;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Dataset Bias Problem from a Statistical Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#32479;&#35745;&#23398;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;&#8220;&#25968;&#25454;&#38598;&#20559;&#24046;&#8221;&#38382;&#39064;&#65292;&#21457;&#29616;&#20854;&#20027;&#35201;&#21407;&#22240;&#26159;&#36755;&#20837;&#30340;&#31867;&#23646;&#24615;&#21644;&#38750;&#31867;&#23646;&#24615;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32771;&#34385;&#36825;&#31181;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32531;&#35299;&#25968;&#25454;&#38598;&#20559;&#24046;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27599;&#20010;&#26679;&#26412;&#30340;&#30446;&#26631;&#21152;&#26435;&#25110;&#20197;&#26435;&#37325;&#27604;&#20363;&#37319;&#26679;&#26469;&#23454;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#26356;&#21152;&#31283;&#23450;&#21644;&#26377;&#25928;&#65292;&#24182;&#19988;&#19982;&#22240;&#26524;&#25512;&#29702;&#26377;&#19968;&#23450;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#32479;&#35745;&#23398;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#8220;&#25968;&#25454;&#38598;&#20559;&#24046;&#8221;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#35813;&#38382;&#39064;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#36755;&#20837;x&#20013;&#31867;&#23646;&#24615;u&#19982;&#38750;&#31867;&#23646;&#24615;b&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#65292;&#30001;p(u|b)&#19982;p(u)&#26174;&#33879;&#19981;&#21516;&#25152;&#34920;&#31034;&#12290;&#30001;&#20110;p(u|b)&#20986;&#29616;&#22312;&#26631;&#20934;&#30340;&#26368;&#22823;&#23545;&#25968;&#20284;&#28982;&#65288;MLL&#65289;&#30446;&#26631;&#30340;&#25277;&#26679;&#20998;&#24067;&#20013;&#65292;&#36890;&#36807;MLL&#22312;&#20559;&#24046;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22825;&#28982;&#22320;&#23558;&#36825;&#31181;&#30456;&#20851;&#24615;&#32435;&#20837;&#20854;&#21442;&#25968;&#20013;&#65292;&#23548;&#33268;&#23545;&#26080;&#20559;&#27979;&#35797;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#23558;&#27599;&#20010;&#26679;&#26412;n&#30340;&#30446;&#26631;&#21152;&#26435;&#20026;\frac{1}{p(u_{n}|b_{n})}&#25110;&#32773;&#20197;&#19982;\frac{1}{p(u_{n}|b_{n})}&#25104;&#27604;&#20363;&#30340;&#26435;&#37325;&#23545;&#26679;&#26412;&#36827;&#34892;&#37319;&#26679;&#26469;&#32531;&#35299;&#25968;&#25454;&#38598;&#20559;&#24046;&#12290;&#34429;&#28982;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#32479;&#35745;&#19978;&#26159;&#31561;&#20215;&#30340;&#65292;&#20294;&#21069;&#32773;&#22312;&#23454;&#36341;&#20013;&#35777;&#26126;&#26356;&#31283;&#23450;&#21644;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#25105;&#20204;&#30340;&#21435;&#20559;&#26041;&#27861;&#19982;&#22240;&#26524;&#25512;&#29702;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#21152;&#24378;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the "dataset bias" problem from a statistical standpoint, and identify the main cause of the problem as the strong correlation between a class attribute u and a non-class attribute b in the input x, represented by p(u|b) differing significantly from p(u). Since p(u|b) appears as part of the sampling distributions in the standard maximum log-likelihood (MLL) objective, a model trained on a biased dataset via MLL inherently incorporates such correlation into its parameters, leading to poor generalization to unbiased test data. From this observation, we propose to mitigate dataset bias via either weighting the objective of each sample n by \frac{1}{p(u_{n}|b_{n})} or sampling that sample with a weight proportional to \frac{1}{p(u_{n}|b_{n})}. While both methods are statistically equivalent, the former proves more stable and effective in practice. Additionally, we establish a connection between our debiasing approach and causal reasoning, reinforcing our method's th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#36890;&#36807;&#30740;&#31350;$\ell_0$-&#26377;&#30028;&#23545;&#25239;&#25915;&#20987;&#65292;&#35777;&#26126;&#20102;&#19968;&#20010;&#20998;&#24067;&#26080;&#20851;&#30340;&#23545;&#25239;&#35757;&#32451;&#27867;&#21270;&#30028;&#38480;&#65292;&#35299;&#20915;&#20102;&#25130;&#26029;&#20869;&#31215;&#30340;&#38750;&#32447;&#24615;&#21644;$\ell_0$&#31354;&#38388;&#19978;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03576</link><description>&lt;p&gt;
$\ell_0$-&#26377;&#30028;&#23545;&#25239;&#25915;&#20987;&#30340;&#23545;&#25239;&#35757;&#32451;&#30340;&#27867;&#21270;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Generalization Properties of Adversarial Training for $\ell_0$-Bounded Adversarial Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#36890;&#36807;&#30740;&#31350;$\ell_0$-&#26377;&#30028;&#23545;&#25239;&#25915;&#20987;&#65292;&#35777;&#26126;&#20102;&#19968;&#20010;&#20998;&#24067;&#26080;&#20851;&#30340;&#23545;&#25239;&#35757;&#32451;&#27867;&#21270;&#30028;&#38480;&#65292;&#35299;&#20915;&#20102;&#25130;&#26029;&#20869;&#31215;&#30340;&#38750;&#32447;&#24615;&#21644;$\ell_0$&#31354;&#38388;&#19978;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24191;&#27867;&#35266;&#23519;&#21040;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23567;&#30340;&#21152;&#24615;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#38169;&#35823;&#20998;&#31867;&#12290;&#26412;&#25991;&#20851;&#27880;$\ell_0$-&#26377;&#30028;&#23545;&#25239;&#25915;&#20987;&#65292;&#24182;&#26088;&#22312;&#29702;&#35770;&#19978;&#34920;&#24449;&#25130;&#26029;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#35757;&#32451;&#24615;&#33021;&#12290;&#27492;&#31867;&#20998;&#31867;&#22120;&#22312;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;$\ell_0$-&#23545;&#25239;&#35774;&#32622;&#20013;&#32463;&#39564;&#19978;&#21644;&#29702;&#35770;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#35777;&#26126;&#20102;&#19968;&#20010;&#20998;&#24067;&#26080;&#20851;&#30340;$\ell_0$-&#26377;&#30028;&#23545;&#25239;&#25200;&#21160;&#30340;&#20108;&#20998;&#31867;&#35774;&#32622;&#30340;&#26032;&#22411;&#27867;&#21270;&#30028;&#38480;&#12290;&#22312;&#36825;&#19968;&#35774;&#32622;&#20013;&#25512;&#23548;&#27867;&#21270;&#30028;&#38480;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#65288;i&#65289;&#25130;&#26029;&#20869;&#31215;&#39640;&#24230;&#38750;&#32447;&#24615;&#65307;&#65288;ii&#65289;&#30001;&#20110;&#23545;&#25239;&#35757;&#32451;&#20351;&#24471;&#22312;$\ell_0$&#31354;&#38388;&#19978;&#30340;&#26368;&#22823;&#21270;&#38382;&#39064;&#26159;&#38750;&#20984;&#21644;&#39640;&#24230;&#38750;&#20809;&#28369;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26032;&#30340;&#32534;&#30721;&#25216;&#26415;&#26469;&#30028;&#23450;...
&lt;/p&gt;
&lt;p&gt;
We have widely observed that neural networks are vulnerable to small additive perturbations to the input causing misclassification. In this paper, we focus on the $\ell_0$-bounded adversarial attacks, and aim to theoretically characterize the performance of adversarial training for an important class of truncated classifiers. Such classifiers are shown to have strong performance empirically, as well as theoretically in the Gaussian mixture model, in the $\ell_0$-adversarial setting. The main contribution of this paper is to prove a novel generalization bound for the binary classification setting with $\ell_0$-bounded adversarial perturbation that is distribution-independent. Deriving a generalization bound in this setting has two main challenges: (i) the truncated inner product which is highly non-linear; and (ii) maximization over the $\ell_0$ ball due to adversarial training is non-convex and highly non-smooth. To tackle these challenges, we develop new coding techniques for bounding
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;&#26159;&#19968;&#20010;&#33021;&#22815;&#39044;&#27979;&#22810;&#27493;&#26410;&#26469;&#29366;&#24577;&#21644;&#22870;&#21169;&#30340;&#26465;&#20214;&#24615;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#27169;&#22411;&#25928;&#26524;&#21644;&#24615;&#33021;&#26041;&#38754;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.03570</link><description>&lt;p&gt;
&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion World Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03570
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;&#26159;&#19968;&#20010;&#33021;&#22815;&#39044;&#27979;&#22810;&#27493;&#26410;&#26469;&#29366;&#24577;&#21644;&#22870;&#21169;&#30340;&#26465;&#20214;&#24615;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#27169;&#22411;&#25928;&#26524;&#21644;&#24615;&#33021;&#26041;&#38754;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;&#65288;DWM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#39044;&#27979;&#22810;&#27493;&#30340;&#26410;&#26469;&#29366;&#24577;&#21644;&#22870;&#21169;&#12290;&#19982;&#20256;&#32479;&#30340;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#30456;&#21453;&#65292;DWM&#36890;&#36807;&#21333;&#20010;&#21069;&#21521;&#20256;&#36882;&#25552;&#20379;&#20102;&#38271;&#26102;&#31243;&#30340;&#39044;&#27979;&#65292;&#28040;&#38500;&#20102;&#36882;&#24402;&#26597;&#35810;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#23558;DWM&#25972;&#21512;&#21040;&#22522;&#20110;&#27169;&#22411;&#30340;&#20215;&#20540;&#20272;&#35745;&#20013;&#65292;&#20854;&#20013;&#30701;&#26399;&#22238;&#25253;&#36890;&#36807;&#20174;DWM&#20013;&#37319;&#26679;&#30340;&#26410;&#26469;&#36712;&#36857;&#36827;&#34892;&#27169;&#25311;&#12290;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;DWM&#21487;&#20197;&#34987;&#35270;&#20026;&#36890;&#36807;&#29983;&#25104;&#24314;&#27169;&#26469;&#23454;&#29616;&#20445;&#23432;&#30340;&#20540;&#27491;&#21017;&#21270;&#12290;&#21478;&#22806;&#65292;&#23427;&#20063;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#25968;&#25454;&#28304;&#65292;&#20351;&#31163;&#32447;Q&#23398;&#20064;&#33021;&#22815;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;D4RL&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;DWM&#23545;&#38271;&#26102;&#31243;&#27169;&#25311;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#32477;&#23545;&#24615;&#33021;&#26041;&#38754;&#65292;DWM&#26174;&#33879;&#36229;&#36807;&#20102;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;44%&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Diffusion World Model (DWM), a conditional diffusion model capable of predicting multistep future states and rewards concurrently. As opposed to traditional one-step dynamics models, DWM offers long-horizon predictions in a single forward pass, eliminating the need for recursive quires. We integrate DWM into model-based value estimation, where the short-term return is simulated by future trajectories sampled from DWM. In the context of offline reinforcement learning, DWM can be viewed as a conservative value regularization through generative modeling. Alternatively, it can be seen as a data source that enables offline Q-learning with synthetic data. Our experiments on the D4RL dataset confirm the robustness of DWM to long-horizon simulation. In terms of absolute performance, DWM significantly surpasses one-step dynamics models with a $44\%$ performance gain, and achieves state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>SkipPredict&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35843;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39044;&#27979;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;&#23427;&#26681;&#25454;&#20316;&#19994;&#30340;&#39044;&#27979;&#35201;&#27714;&#23545;&#20316;&#19994;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#36890;&#36807;&#20248;&#20808;&#22788;&#29702;&#39044;&#27979;&#20026;&#30701;&#20316;&#19994;&#30340;&#20316;&#19994;&#20197;&#21450;&#24212;&#29992;&#35814;&#32454;&#39044;&#27979;&#26469;&#36817;&#20284;&#38271;&#20316;&#19994;&#30340;&#26368;&#30701;&#21097;&#20313;&#22788;&#29702;&#26102;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#25490;&#38431;&#31995;&#32479;&#20013;&#26377;&#25928;&#22320;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03564</link><description>&lt;p&gt;
SkipPredict&#65306;&#20309;&#26102;&#25237;&#36164;&#20110;&#20316;&#19994;&#35843;&#24230;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SkipPredict: When to Invest in Predictions for Scheduling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03564
&lt;/p&gt;
&lt;p&gt;
SkipPredict&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35843;&#24230;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39044;&#27979;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;&#23427;&#26681;&#25454;&#20316;&#19994;&#30340;&#39044;&#27979;&#35201;&#27714;&#23545;&#20316;&#19994;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#36890;&#36807;&#20248;&#20808;&#22788;&#29702;&#39044;&#27979;&#20026;&#30701;&#20316;&#19994;&#30340;&#20316;&#19994;&#20197;&#21450;&#24212;&#29992;&#35814;&#32454;&#39044;&#27979;&#26469;&#36817;&#20284;&#38271;&#20316;&#19994;&#30340;&#26368;&#30701;&#21097;&#20313;&#22788;&#29702;&#26102;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#25490;&#38431;&#31995;&#32479;&#20013;&#26377;&#25928;&#22320;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#26368;&#36817;&#23545;&#20855;&#26377;&#39044;&#27979;&#20316;&#19994;&#22823;&#23567;&#30340;&#35843;&#24230;&#24037;&#20316;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#39044;&#27979;&#22312;&#25490;&#38431;&#31995;&#32479;&#20013;&#30340;&#25104;&#26412;&#24433;&#21709;&#65292;&#28040;&#38500;&#20102;&#20808;&#21069;&#30740;&#31350;&#20013;&#39044;&#27979;&#26159;&#22806;&#37096;&#36164;&#28304;&#21644;/&#25110;&#20813;&#36153;&#30340;&#20551;&#35774;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21033;&#29992;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;SkipPredict&#65292;&#26088;&#22312;&#35299;&#20915;&#20854;&#22266;&#26377;&#30340;&#25104;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#24314;&#35758;&#30340;&#26041;&#27861;&#19981;&#26159;&#22343;&#21248;&#22320;&#23558;&#39044;&#27979;&#24212;&#29992;&#20110;&#25152;&#26377;&#20316;&#19994;&#65292;&#32780;&#26159;&#26681;&#25454;&#39044;&#27979;&#35201;&#27714;&#23558;&#20316;&#19994;&#20998;&#20026;&#19981;&#21516;&#31867;&#21035;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20301;&#8220;&#24265;&#20215;&#39044;&#27979;&#8221;&#26469;&#23545;&#20316;&#19994;&#36827;&#34892;&#20998;&#31867;&#65292;&#21028;&#26029;&#20854;&#26159;&#30701;&#20316;&#19994;&#36824;&#26159;&#38271;&#20316;&#19994;&#12290;SkipPredict&#23558;&#20248;&#20808;&#22788;&#29702;&#39044;&#27979;&#20026;&#30701;&#20316;&#19994;&#30340;&#20316;&#19994;&#65292;&#23545;&#20110;&#39044;&#27979;&#20026;&#38271;&#20316;&#19994;&#30340;&#20316;&#19994;&#65292;SkipPredict&#23558;&#24212;&#29992;&#31532;&#20108;&#36718;&#26356;&#35814;&#32454;&#30340;&#8220;&#26114;&#36149;&#39044;&#27979;&#8221;&#26469;&#36817;&#20284;&#36825;&#20123;&#20316;&#19994;&#30340;&#26368;&#30701;&#21097;&#20313;&#22788;&#29702;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#32771;&#34385;&#20102;&#39044;&#27979;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#25104;&#26412;&#23545;&#20004;&#20010;&#19981;&#21516;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#22312;&#22806;&#37096;&#25104;&#26412;&#27169;&#22411;&#20013;&#65292;&#39044;&#27979;&#30340;&#25104;&#26412;&#19982;&#20316;&#19994;&#30340;&#22823;&#23567;&#21644;&#35843;&#24230;&#26102;&#38388;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
In light of recent work on scheduling with predicted job sizes, we consider the effect of the cost of predictions in queueing systems, removing the assumption in prior research that predictions are external to the system's resources and/or cost-free. In particular, we introduce a novel approach to utilizing predictions, SkipPredict, designed to address their inherent cost. Rather than uniformly applying predictions to all jobs, we propose a tailored approach that categorizes jobs based on their prediction requirements. To achieve this, we employ one-bit "cheap predictions" to classify jobs as either short or long. SkipPredict prioritizes predicted short jobs over long jobs, and for the latter, SkipPredict applies a second round of more detailed "expensive predictions" to approximate Shortest Remaining Processing Time for these jobs. Our analysis takes into account the cost of prediction. We examine the effect of this cost for two distinct models. In the external cost model, predictions
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#33258;&#30001;&#25991;&#26412;&#20013;&#35782;&#21035;&#20316;&#20026;&#20195;&#29702;&#30340;&#27169;&#22411;&#21644;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#30340;&#23567;&#22411;&#32447;&#24615;&#25506;&#27979;&#22120;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26356;&#22823;&#27169;&#22411;&#20196;&#29260;&#32423;&#21035;&#19978;&#30340;&#33258;&#20449;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#38750;&#24179;&#20961;&#30340;&#20934;&#30830;&#24230;&#65292;&#36825;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.03563</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#27169;&#22411;&#21306;&#20998;&#21487;&#30693;&#19982;&#19981;&#21487;&#30693;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Distinguishing the Knowable from the Unknowable with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03563
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#33258;&#30001;&#25991;&#26412;&#20013;&#35782;&#21035;&#20316;&#20026;&#20195;&#29702;&#30340;&#27169;&#22411;&#21644;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#30340;&#23567;&#22411;&#32447;&#24615;&#25506;&#27979;&#22120;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26356;&#22823;&#27169;&#22411;&#20196;&#29260;&#32423;&#21035;&#19978;&#30340;&#33258;&#20449;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#38750;&#24179;&#20961;&#30340;&#20934;&#30830;&#24230;&#65292;&#36825;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#33258;&#30001;&#25991;&#26412;&#36755;&#20986;&#20013;&#65292;&#26159;&#21542;&#21487;&#20197;&#37492;&#21035;&#20986;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65288;&#21453;&#26144;&#32570;&#20047;&#30693;&#35782;&#30340;&#19981;&#30830;&#23450;&#24615;&#65289;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#65288;&#21453;&#26144;&#22522;&#30784;&#20998;&#24067;&#20013;&#30340;&#29109;&#65289;&#12290;&#22312;&#27809;&#26377;&#30495;&#23454;&#27010;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20010;&#35774;&#32622;&#65292;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#20026;&#20102;&#65288;&#36817;&#20284;&#22320;&#65289;&#20998;&#35299;&#32473;&#23450;LLM&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#19968;&#20010;&#26126;&#26174;&#26356;&#22823;&#30340;&#27169;&#22411;&#20805;&#24403;&#22320;&#38754;&#30495;&#30456;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22522;&#20110;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#30340;&#23567;&#22411;&#32447;&#24615;&#25506;&#27979;&#22120;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#22312;&#20196;&#29260;&#32423;&#21035;&#19978;&#26356;&#22823;&#27169;&#22411;&#23558;&#26356;&#33258;&#20449;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#22312;&#19968;&#20010;&#25991;&#26412;&#39046;&#22495;&#19978;&#35757;&#32451;&#30340;&#25506;&#27979;&#22120;&#21487;&#20197;&#27867;&#21270;&#21040;&#20854;&#20182;&#39046;&#22495;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#38750;&#24179;&#20961;&#30340;&#20934;&#30830;&#24230;&#12290;&#32508;&#21512;&#32771;&#34385;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#35299;&#37322;&#36825;&#20123;&#32467;&#26524;&#20316;&#20026;LLMs&#20869;&#37096;&#33258;&#28982;&#22320;&#21253;&#21547;&#20102;&#19981;&#21516;&#31867;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#34920;&#31034;&#65292;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#21046;&#23450;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text. In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given LLM's uncertainty, a significantly larger model stands in as a proxy for the ground truth. We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others. Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task. Taken together, we interpret these results as evidence that LLMs naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25237;&#24433;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;PGDM&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#23558;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36866;&#29992;&#20110;&#23545;&#29305;&#23450;&#26465;&#20214;&#26377;&#20005;&#26684;&#35201;&#27714;&#30340;&#22330;&#26223;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#25237;&#24433;&#26041;&#27861;&#30830;&#20445;&#29983;&#25104;&#30340;&#25968;&#25454;&#31526;&#21512;&#25351;&#23450;&#30340;&#32422;&#26463;&#25110;&#29289;&#29702;&#21407;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;PGDM&#22312;&#22797;&#26434;&#30340;&#32422;&#26463;&#21644;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#21512;&#25104;&#20986;&#31526;&#21512;&#35201;&#27714;&#30340;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.03559</link><description>&lt;p&gt;
&#29992;&#20110;&#32422;&#26463;&#28385;&#36275;&#30340;&#25237;&#24433;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Projected Generative Diffusion Models for Constraint Satisfaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25237;&#24433;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;PGDM&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#23558;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36866;&#29992;&#20110;&#23545;&#29305;&#23450;&#26465;&#20214;&#26377;&#20005;&#26684;&#35201;&#27714;&#30340;&#22330;&#26223;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#25237;&#24433;&#26041;&#27861;&#30830;&#20445;&#29983;&#25104;&#30340;&#25968;&#25454;&#31526;&#21512;&#25351;&#23450;&#30340;&#32422;&#26463;&#25110;&#29289;&#29702;&#21407;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;PGDM&#22312;&#22797;&#26434;&#30340;&#32422;&#26463;&#21644;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#21512;&#25104;&#20986;&#31526;&#21512;&#35201;&#27714;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#19968;&#20010;&#39034;&#24207;&#36807;&#31243;&#65292;&#33021;&#22815;&#20174;&#21407;&#22987;&#22122;&#22768;&#20013;&#21512;&#25104;&#20986;&#36830;&#36143;&#30340;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#36755;&#20986;&#31526;&#21512;&#29305;&#23450;&#20005;&#26684;&#26465;&#20214;&#30340;&#22330;&#26223;&#20013;&#30452;&#25509;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#38754;&#20020;&#30528;&#20005;&#37325;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#20171;&#32461;&#20102;&#25237;&#24433;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;PGDM&#65289;&#65292;&#35813;&#26041;&#27861;&#23558;&#20256;&#32479;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#20010;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#24212;&#29992;&#36845;&#20195;&#25237;&#24433;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#25968;&#25454;&#24544;&#23454;&#22320;&#36981;&#24490;&#25351;&#23450;&#30340;&#32422;&#26463;&#25110;&#29289;&#29702;&#21407;&#29702;&#12290;&#26412;&#25991;&#22312;&#21463;&#38480;&#21046;&#30340;&#32422;&#26463;&#31867;&#21035;&#19979;&#65292;&#23545;PGDM&#33021;&#22815;&#20174;&#21487;&#34892;&#23376;&#20998;&#24067;&#20013;&#21512;&#25104;&#36755;&#20986;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#65292;&#24182;&#22312;&#22797;&#26434;&#30340;&#38750;&#20984;&#32422;&#26463;&#21644;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#26696;&#20363;&#20013;&#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290;&#36825;&#20123;&#33021;&#21147;&#36890;&#36807;&#22312;&#35270;&#39057;&#29983;&#25104;&#20013;&#20307;&#29616;&#20102;&#20855;&#26377;&#29289;&#29702;&#23398;&#20449;&#24687;&#30340;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative diffusion models excel at robustly synthesizing coherent content from raw noise through a sequential process. However, their direct application in scenarios requiring outputs to adhere to specific, stringent criteria faces several severe challenges. This paper aims at overcome these challenges and introduces Projected Generative Diffusion Models (PGDM), an approach that recast traditional diffusion models sampling into a constrained-optimization problem. This enables the application of an iterative projections method to ensure that generated data faithfully adheres to specified constraints or physical principles. This paper provides theoretical support for the ability of PGDM to synthesize outputs from a feasible subdistribution under a restricted class of constraints while also providing large empirical evidence in the case of complex non-convex constraints and ordinary differential equations. These capabilities are demonstrated by physics-informed motion in video generatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#36335;&#24452;&#31614;&#21517;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#24930;&#22320;&#38663;&#20998;&#26512;&#12290;&#36890;&#36807;&#21033;&#29992;&#36335;&#24452;&#31614;&#21517;&#30340;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#31354;&#38388;&#20132;&#20114;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;GPS&#20256;&#24863;&#22120;&#32593;&#32476;&#23545;&#24930;&#28369;&#20107;&#20214;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#22312;&#27169;&#25311;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#24314;&#31435;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.03558</link><description>&lt;p&gt;
&#36335;&#24452;&#31614;&#21517;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24930;&#22320;&#38663;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65306;&#26356;&#22909;&#30340;&#32467;&#21512;&#26041;&#24335;&#65311;
&lt;/p&gt;
&lt;p&gt;
Path Signatures and Graph Neural Networks for Slow Earthquake Analysis: Better Together?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#36335;&#24452;&#31614;&#21517;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#24930;&#22320;&#38663;&#20998;&#26512;&#12290;&#36890;&#36807;&#21033;&#29992;&#36335;&#24452;&#31614;&#21517;&#30340;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#31354;&#38388;&#20132;&#20114;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;GPS&#20256;&#24863;&#22120;&#32593;&#32476;&#23545;&#24930;&#28369;&#20107;&#20214;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#22312;&#27169;&#25311;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#24314;&#31435;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36335;&#24452;&#31614;&#21517;&#20316;&#20026;&#19968;&#31181;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#25104;&#21151;&#30340;&#29702;&#35770;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#19981;&#35268;&#21017;&#36335;&#24452;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#22270;&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#20256;&#24863;&#22120;&#32593;&#32476;&#31561;&#19981;&#35268;&#21017;&#39046;&#22495;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36335;&#24452;&#31614;&#21517;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PS-GCNN&#65289;&#65292;&#23558;&#36335;&#24452;&#31614;&#21517;&#19982;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCNN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#21457;&#25381;&#20102;&#36335;&#24452;&#31614;&#21517;&#30340;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#21644;GCNN&#22788;&#29702;&#31354;&#38388;&#20114;&#21160;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#24930;&#22320;&#38663;&#24207;&#21015;&#65288;&#20063;&#31216;&#20026;&#24930;&#28369;&#20107;&#20214;&#65289;&#30340;&#20998;&#26512;&#65292;&#21033;&#29992;&#26469;&#33258;GPS&#26102;&#38388;&#24207;&#21015;&#30340;&#25968;&#25454;&#65292;&#22312;&#26032;&#35199;&#20848;&#21271;&#23707;&#19996;&#28023;&#23736;&#30340;GPS&#20256;&#24863;&#22120;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#22312;&#27169;&#25311;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#24314;&#31435;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#22522;&#20934;&#65292;&#36825;&#31181;&#27169;&#22411;&#27169;&#25311;&#20102;&#31867;&#20284;&#30340;&#21453;&#24212;&#25193;&#25955;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
The path signature, having enjoyed recent success in the machine learning community, is a theoretically-driven method for engineering features from irregular paths. On the other hand, graph neural networks (GNN), neural architectures for processing data on graphs, excel on tasks with irregular domains, such as sensor networks. In this paper, we introduce a novel approach, Path Signature Graph Convolutional Neural Networks (PS-GCNN), integrating path signatures into graph convolutional neural networks (GCNN), and leveraging the strengths of both path signatures, for feature extraction, and GCNNs, for handling spatial interactions. We apply our method to analyze slow earthquake sequences, also called slow slip events (SSE), utilizing data from GPS timeseries, with a case study on a GPS sensor network on the east coast of New Zealand's north island. We also establish benchmarks for our method on simulated stochastic differential equations, which model similar reaction-diffusion phenomenon
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;&#24403;&#21069;&#21333;GPU GNN&#31995;&#32479;&#30340;&#38519;&#38449;&#19982;&#27880;&#24847;&#20107;&#39033;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#24320;&#21457;&#26032;&#30340;&#21442;&#32771;&#31995;&#32479;&#65292;&#23454;&#29616;&#39640;&#25928;&#23454;&#29992;&#22320;&#35299;&#20915;&#31995;&#32479;&#35774;&#35745;&#38519;&#38449;&#30340;&#19968;&#31995;&#21015;&#20248;&#21270;&#65292;&#24182;&#25512;&#21160;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.03548</link><description>&lt;p&gt;
&#21333;GPU GNN&#31995;&#32479;&#65306;&#38519;&#38449;&#19982;&#27880;&#24847;&#20107;&#39033;
&lt;/p&gt;
&lt;p&gt;
Single-GPU GNN Systems: Traps and Pitfalls
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;&#24403;&#21069;&#21333;GPU GNN&#31995;&#32479;&#30340;&#38519;&#38449;&#19982;&#27880;&#24847;&#20107;&#39033;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#24320;&#21457;&#26032;&#30340;&#21442;&#32771;&#31995;&#32479;&#65292;&#23454;&#29616;&#39640;&#25928;&#23454;&#29992;&#22320;&#35299;&#20915;&#31995;&#32479;&#35774;&#35745;&#38519;&#38449;&#30340;&#19968;&#31995;&#21015;&#20248;&#21270;&#65292;&#24182;&#25512;&#21160;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#31995;&#32479;&#26222;&#36941;&#19981;&#23637;&#31034;&#35757;&#32451;&#20934;&#30830;&#24615;&#32467;&#26524;&#65292;&#24182;&#30452;&#25509;&#25110;&#38388;&#25509;&#20381;&#36182;&#20110;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#28145;&#20837;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#32473;&#31995;&#32479;&#35774;&#35745;&#21644;&#35780;&#20272;&#36807;&#31243;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#30340;&#38519;&#38449;&#65292;&#36136;&#30097;&#20102;&#35768;&#22810;&#25552;&#35758;&#30340;&#31995;&#32479;&#20248;&#21270;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#24433;&#21709;&#20102;&#32467;&#35770;&#21644;&#32463;&#39564;&#25945;&#35757;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#35768;&#22810;&#21333;GPU&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#38519;&#38449;&#30340;&#26681;&#26412;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20551;&#35774;&#12289;&#24314;&#35758;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#21442;&#32771;&#31995;&#32479;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#23454;&#29992;&#22320;&#35299;&#20915;&#31995;&#32479;&#35774;&#35745;&#38519;&#38449;&#30340;&#19968;&#31995;&#21015;&#20248;&#21270;&#12290;&#35813;&#35774;&#35745;&#21487;&#20197;&#26377;&#25928;&#22320;&#34701;&#20837;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#30495;&#27491;&#25512;&#21160;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current graph neural network (GNN) systems have established a clear trend of not showing training accuracy results, and directly or indirectly relying on smaller datasets for evaluations majorly. Our in-depth analysis shows that it leads to a chain of pitfalls in the system design and evaluation process, questioning the practicality of many of the proposed system optimizations, and affecting conclusions and lessons learned. We analyze many single-GPU systems and show the fundamental impact of these pitfalls. We further develop hypotheses, recommendations, and evaluation methodologies, and provide future directions. Finally, a new reference system is developed to establish a new line of optimizations rooted in solving the system-design pitfalls efficiently and practically. The proposed design can productively be integrated into prior works, thereby truly advancing the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OLS-OFU&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#29305;&#24449;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#22312;&#32447;&#35774;&#32622;&#20013;&#30340;&#26631;&#31614;&#36716;&#31227;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;OLS-OFU&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#39046;&#22495;&#36716;&#31227;&#26465;&#20214;&#19979;&#37117;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03545</link><description>&lt;p&gt;
&#22312;&#32447;&#29305;&#24449;&#26356;&#26032;&#25913;&#21892;&#22312;&#32447;&#65288;&#24191;&#20041;&#65289;&#26631;&#31614;&#36716;&#31227;&#36866;&#24212;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Online Feature Updates Improve Online (Generalized) Label Shift Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OLS-OFU&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#29305;&#24449;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#22312;&#32447;&#35774;&#32622;&#20013;&#30340;&#26631;&#31614;&#36716;&#31227;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;OLS-OFU&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#39046;&#22495;&#36716;&#31227;&#26465;&#20214;&#19979;&#37117;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#32447;&#35774;&#32622;&#20013;&#26631;&#31614;&#36716;&#31227;&#30340;&#26222;&#36941;&#38382;&#39064;&#65292;&#20854;&#20013;&#23384;&#22312;&#32570;&#22833;&#30340;&#26631;&#31614;&#65292;&#25968;&#25454;&#20998;&#24067;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#21450;&#26102;&#33719;&#24471;&#26631;&#31614;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#35843;&#25972;&#25110;&#26356;&#26032;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#26368;&#21518;&#19968;&#23618;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#25913;&#36827;&#29305;&#24449;&#34920;&#31034;&#30340;&#26410;&#34987;&#21457;&#25496;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#32447;&#26631;&#31614;&#36716;&#31227;&#33258;&#36866;&#24212;&#19982;&#22312;&#32447;&#29305;&#24449;&#26356;&#26032;&#65288;OLS-OFU&#65289;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#20248;&#21270;&#29305;&#24449;&#25552;&#21462;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#36827;&#39044;&#27979;&#27169;&#22411;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#23454;&#65292;OLS-OFU&#36890;&#36807;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#29305;&#24449;&#20248;&#21270;&#65292;&#20943;&#23569;&#20102;&#31639;&#27861;&#36951;&#25022;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#22312;&#22312;&#32447;&#26631;&#31614;&#36716;&#31227;&#21644;&#24191;&#20041;&#26631;&#31614;&#36716;&#31227;&#26465;&#20214;&#19979;&#65292;&#24378;&#35843;&#20102;OLS-OFU&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#39046;&#22495;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the prevalent issue of label shift in an online setting with missing labels, where data distributions change over time and obtaining timely labels is challenging. While existing methods primarily focus on adjusting or updating the final layer of a pre-trained classifier, we explore the untapped potential of enhancing feature representations using unlabeled data at test-time. Our novel method, Online Label Shift adaptation with Online Feature Updates (OLS-OFU), leverages self-supervised learning to refine the feature extraction process, thereby improving the prediction model. Theoretical analyses confirm that OLS-OFU reduces algorithmic regret by capitalizing on self-supervised learning for feature refinement. Empirical studies on various datasets, under both online label shift and generalized label shift conditions, underscore the effectiveness and robustness of OLS-OFU, especially in cases of domain shifts.
&lt;/p&gt;</description></item><item><title>HAMLET&#26159;&#19968;&#20010;&#22270;&#21464;&#25442;&#31070;&#32463;&#31639;&#23376;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#22359;&#21270;&#36755;&#20837;&#32534;&#30721;&#22120;&#23558;&#24494;&#20998;&#26041;&#31243;&#20449;&#24687;&#30452;&#25509;&#34701;&#20837;&#35299;&#20915;&#36807;&#31243;&#20013;&#65292;&#24182;&#23637;&#31034;&#20986;&#22312;&#22788;&#29702;&#22797;&#26434;&#25968;&#25454;&#21644;&#22122;&#22768;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20219;&#24847;&#20960;&#20309;&#24418;&#29366;&#21644;&#21508;&#31181;&#36755;&#20837;&#26684;&#24335;&#30340;PDE&#38382;&#39064;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;HAMLET&#33021;&#22815;&#36229;&#36234;&#24403;&#21069;&#30340;PDE&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.03541</link><description>&lt;p&gt;
HAMLET&#65306;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#22270;&#21464;&#25442;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
HAMLET: Graph Transformer Neural Operator for Partial Differential Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03541
&lt;/p&gt;
&lt;p&gt;
HAMLET&#26159;&#19968;&#20010;&#22270;&#21464;&#25442;&#31070;&#32463;&#31639;&#23376;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#22359;&#21270;&#36755;&#20837;&#32534;&#30721;&#22120;&#23558;&#24494;&#20998;&#26041;&#31243;&#20449;&#24687;&#30452;&#25509;&#34701;&#20837;&#35299;&#20915;&#36807;&#31243;&#20013;&#65292;&#24182;&#23637;&#31034;&#20986;&#22312;&#22788;&#29702;&#22797;&#26434;&#25968;&#25454;&#21644;&#22122;&#22768;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20219;&#24847;&#20960;&#20309;&#24418;&#29366;&#21644;&#21508;&#31181;&#36755;&#20837;&#26684;&#24335;&#30340;PDE&#38382;&#39064;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;HAMLET&#33021;&#22815;&#36229;&#36234;&#24403;&#21069;&#30340;PDE&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#21464;&#25442;&#26694;&#26550;HAMLET&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26102;&#30340;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#20855;&#26377;&#27169;&#22359;&#21270;&#36755;&#20837;&#32534;&#30721;&#22120;&#30340;&#22270;&#21464;&#25442;&#22120;&#65292;&#23558;&#24494;&#20998;&#26041;&#31243;&#20449;&#24687;&#30452;&#25509;&#34701;&#20837;&#35299;&#20915;&#36807;&#31243;&#20013;&#12290;&#36825;&#31181;&#27169;&#22359;&#21270;&#22686;&#24378;&#20102;&#21442;&#25968;&#23545;&#24212;&#25511;&#21046;&#65292;&#20351;&#24471;HAMLET&#33021;&#22815;&#36866;&#24212;&#20219;&#24847;&#20960;&#20309;&#24418;&#29366;&#21644;&#21508;&#31181;&#36755;&#20837;&#26684;&#24335;&#30340;PDE&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;HAMLET&#33021;&#22815;&#26377;&#25928;&#25193;&#23637;&#21040;&#22788;&#29702;&#22797;&#26434;&#25968;&#25454;&#21644;&#22122;&#22768;&#65292;&#23637;&#31034;&#20986;&#20854;&#40065;&#26834;&#24615;&#12290;HAMLET&#19981;&#20165;&#36866;&#29992;&#20110;&#21333;&#19968;&#31867;&#22411;&#30340;&#29289;&#29702;&#27169;&#25311;&#65292;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#27492;&#22806;&#65292;&#23427;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24377;&#24615;&#21644;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#22330;&#26223;&#19979;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#36229;&#36234;&#24403;&#21069;&#30340;PDE&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel graph transformer framework, HAMLET, designed to address the challenges in solving partial differential equations (PDEs) using neural networks. The framework uses graph transformers with modular input encoders to directly incorporate differential equation information into the solution process. This modularity enhances parameter correspondence control, making HAMLET adaptable to PDEs of arbitrary geometries and varied input formats. Notably, HAMLET scales effectively with increasing data complexity and noise, showcasing its robustness. HAMLET is not just tailored to a single type of physical simulation, but can be applied across various domains. Moreover, it boosts model resilience and performance, especially in scenarios with limited data. We demonstrate, through extensive experiments, that our framework is capable of outperforming current techniques for PDEs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#35843;&#33410;&#28216;&#25103;&#20026;&#22522;&#30784;&#30340;&#26694;&#26550;&#65292;&#23558;&#21487;&#20449;&#26426;&#22120;&#23398;&#20064;&#35270;&#20026;&#22810;&#30446;&#26631;&#22810;&#20027;&#20307;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;ParetoPlay&#31639;&#27861;&#65292;&#23547;&#25214;&#31038;&#20250;&#26368;&#20248;&#30340;&#28216;&#25103;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#30830;&#20445;&#20195;&#29702;&#26041;&#22987;&#32456;&#20445;&#25345;&#22312;Pareto&#21069;&#27839;&#19978;&#12290;</title><link>https://arxiv.org/abs/2402.03540</link><description>&lt;p&gt;
&#12298;&#21487;&#20449;&#26426;&#22120;&#23398;&#20064;&#30340;&#35843;&#33410;&#28216;&#25103;&#12299;
&lt;/p&gt;
&lt;p&gt;
Regulation Games for Trustworthy Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#35843;&#33410;&#28216;&#25103;&#20026;&#22522;&#30784;&#30340;&#26694;&#26550;&#65292;&#23558;&#21487;&#20449;&#26426;&#22120;&#23398;&#20064;&#35270;&#20026;&#22810;&#30446;&#26631;&#22810;&#20027;&#20307;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;ParetoPlay&#31639;&#27861;&#65292;&#23547;&#25214;&#31038;&#20250;&#26368;&#20248;&#30340;&#28216;&#25103;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#30830;&#20445;&#20195;&#29702;&#26041;&#22987;&#32456;&#20445;&#25345;&#22312;Pareto&#21069;&#27839;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20851;&#20110;&#21487;&#20449;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#24448;&#24448;&#38598;&#20013;&#22312;&#20449;&#20219;&#30340;&#20010;&#21035;&#26041;&#38754;&#65292;&#22914;&#20844;&#24179;&#24615;&#25110;&#38544;&#31169;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#25216;&#26415;&#24573;&#35270;&#20102;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20154;&#21644;&#36127;&#36131;&#35780;&#20272;&#20854;&#21487;&#20449;&#24230;&#30340;&#20154;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#21487;&#20449;&#26426;&#22120;&#23398;&#20064;&#35270;&#20026;&#22810;&#30446;&#26631;&#22810;&#20027;&#20307;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#33258;&#28982;&#22320;&#23548;&#33268;&#20102;&#19968;&#20010;&#31216;&#20026;&#35843;&#33410;&#28216;&#25103;&#30340;&#21338;&#24328;&#35770;&#24418;&#24335;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#28216;&#25103;&#23454;&#20363;&#8212;&#8212;SpecGame&#65292;&#20854;&#20013;&#25105;&#20204;&#24314;&#27169;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26500;&#24314;&#32773;&#19982;&#20844;&#24179;&#24615;&#21644;&#38544;&#31169;&#30417;&#31649;&#32773;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30417;&#31649;&#32773;&#24076;&#26395;&#35774;&#35745;&#22788;&#32602;&#25514;&#26045;&#26469;&#24378;&#21046;&#25191;&#34892;&#20182;&#20204;&#30340;&#35268;&#33539;&#65292;&#20294;&#19981;&#24076;&#26395;&#38459;&#27490;&#26500;&#24314;&#32773;&#30340;&#21442;&#19982;&#12290;&#20026;&#20102;&#23547;&#25214;&#36825;&#31181;&#31038;&#20250;&#26368;&#20248;&#65288;&#21363;&#23545;&#25152;&#26377;&#20195;&#29702;&#26041;&#37117;&#26377;&#25928;&#65289;&#30340;&#28216;&#25103;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ParetoPlay&#12290;&#36825;&#31181;&#26032;&#22411;&#22343;&#34913;&#25628;&#32034;&#31639;&#27861;&#30830;&#20445;&#20195;&#29702;&#26041;&#22987;&#32456;&#20445;&#25345;&#22312;Pareto&#21069;&#27839;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing work on trustworthy machine learning (ML) often concentrates on individual aspects of trust, such as fairness or privacy. Additionally, many techniques overlook the distinction between those who train ML models and those responsible for assessing their trustworthiness. To address these issues, we propose a framework that views trustworthy ML as a multi-objective multi-agent optimization problem. This naturally lends itself to a game-theoretic formulation we call regulation games. We illustrate a particular game instance, the SpecGame in which we model the relationship between an ML model builder and fairness and privacy regulators. Regulators wish to design penalties that enforce compliance with their specification, but do not want to discourage builders from participation. Seeking such socially optimal (i.e., efficient for all agents) solutions to the game, we introduce ParetoPlay. This novel equilibrium search algorithm ensures that agents remain on the Pareto frontier of th
&lt;/p&gt;</description></item><item><title>&#26080;&#20256;&#24863;&#22120;&#20301;&#32622;&#21644;&#36895;&#24230;&#20272;&#35745;&#26041;&#27861;&#22312;BLDC&#30005;&#26426;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03534</link><description>&lt;p&gt;
&#22522;&#20110;ANN&#30340;&#26080;&#20256;&#24863;&#22120;&#20301;&#32622;&#21644;&#36895;&#24230;&#20272;&#35745;&#26041;&#27861;&#24212;&#29992;&#20110;BLDC&#30005;&#26426;
&lt;/p&gt;
&lt;p&gt;
ANN-based position and speed sensorless estimation for BLDC motors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03534
&lt;/p&gt;
&lt;p&gt;
&#26080;&#20256;&#24863;&#22120;&#20301;&#32622;&#21644;&#36895;&#24230;&#20272;&#35745;&#26041;&#27861;&#22312;BLDC&#30005;&#26426;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BLDC&#30005;&#26426;&#24212;&#29992;&#38656;&#35201;&#31934;&#30830;&#30340;&#20301;&#32622;&#21644;&#36895;&#24230;&#27979;&#37327;&#65292;&#20256;&#32479;&#19978;&#38656;&#35201;&#20256;&#24863;&#22120;&#26469;&#33719;&#21462;&#36825;&#20123;&#27979;&#37327;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24102;&#26377;&#34928;&#20943;&#26434;&#25955;&#30340;&#32456;&#31471;&#30456;&#30005;&#21387;&#26469;&#20272;&#35745;&#36825;&#20123;&#27979;&#37327;&#32467;&#26524;&#30340;&#26041;&#27861;&#65292;&#35813;&#30005;&#21387;&#26159;&#36890;&#36807;FPGA&#33719;&#21462;&#30340;&#65292;&#35813;FPGA&#36824;&#25805;&#20316;&#19968;&#20010;PWM&#25511;&#21046;&#30340;&#36870;&#21464;&#22120;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#32534;&#30721;&#22120;&#65292;&#23558;&#30005;&#21387;&#26631;&#35760;&#20026;&#30005;&#27668;&#21644;&#34394;&#25311;&#36716;&#23376;&#29366;&#24577;&#65292;&#20026;&#20004;&#20010;&#20855;&#26377;&#24863;&#30693;&#22120;&#32423;&#32852;&#25299;&#25169;&#32467;&#26500;&#30340;&#19977;&#23618;ANN&#25552;&#20379;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#12290;&#31532;&#19968;&#20010;ANN&#26681;&#25454;&#24102;&#26377;&#22686;&#37327;&#26102;&#38388;&#25139;&#30340;&#30005;&#21387;&#29305;&#24449;&#26469;&#20272;&#35745;&#20301;&#32622;&#65292;&#31532;&#20108;&#20010;ANN&#26681;&#25454;&#32771;&#34385;&#37319;&#38598;&#31383;&#21475;&#20013;&#30340;&#26102;&#38388;&#25139;&#30340;&#20301;&#32622;&#24046;&#24322;&#29305;&#24449;&#26469;&#20272;&#35745;&#36895;&#24230;&#12290;&#22312;125&#21040;1,500&#36716;/&#20998;&#38047;&#30340;&#36127;&#36733;8&#26497;&#23545;&#30005;&#26426;&#19978;&#65292;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#35757;&#32451;&#21644;&#26080;&#20256;&#24863;&#22120;&#30340;&#27979;&#35797;&#24471;&#21040;&#30340;&#32477;&#23545;&#35823;&#24046;&#20998;&#21035;&#20026;0.8&#30005;&#27668;&#24230;&#21644;22&#36716;/&#20998;&#38047;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25972;&#20307;&#20301;&#32622;&#20272;&#35745;&#26126;&#26174;&#25913;&#21892;&#20102;&#20256;&#32479;&#21644;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
BLDC motor applications require precise position and speed measurements, traditionally obtained with sensors. This article presents a method for estimating those measurements without position sensors using terminal phase voltages with attenuated spurious, acquired with a FPGA that also operates a PWM-controlled inverter. Voltages are labelled with electrical and virtual rotor states using an encoder that provides training and testing data for two three-layer ANNs with perceptron-based cascade topology. The first ANN estimates the position from features of voltages with incremental timestamps, and the second ANN estimates the speed from features of position differentials considering timestamps in an acquisition window. Sensor-based training and sensorless testing at 125 to 1,500 rpm with a loaded 8-pole-pair motor obtained absolute errors of 0.8 electrical degrees and 22 rpm. Results conclude that the overall position estimation significantly improved conventional and advanced methods, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#29615;&#22659;&#20013;&#30340;&#19978;&#19979;&#25991;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#20316;&#24335;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;Fed-FairX-LinUCB&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#20445;&#35777;&#20102;&#20844;&#24179;&#24615;&#21644;&#38544;&#31169;&#24615;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#31639;&#27861;&#22312;&#20195;&#29702;&#25968;&#37327;&#19978;&#20135;&#29983;&#30340;&#20844;&#24179;&#24615;&#25439;&#22833;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03531</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#29615;&#22659;&#20013;&#30340;&#19978;&#19979;&#25991;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#38544;&#31169;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Fairness and Privacy Guarantees in Federated Contextual Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#29615;&#22659;&#20013;&#30340;&#19978;&#19979;&#25991;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#20316;&#24335;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;Fed-FairX-LinUCB&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#20445;&#35777;&#20102;&#20844;&#24179;&#24615;&#21644;&#38544;&#31169;&#24615;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#31639;&#27861;&#22312;&#20195;&#29702;&#25968;&#37327;&#19978;&#20135;&#29983;&#30340;&#20844;&#24179;&#24615;&#25439;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#32852;&#37030;&#29615;&#22659;&#20013;&#20855;&#26377;&#20844;&#24179;&#24615;&#21644;&#38544;&#31169;&#20445;&#35777;&#30340;&#19978;&#19979;&#25991;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;CMAB&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#22522;&#20110;&#20248;&#21183;&#30340;&#26333;&#20809;&#35270;&#20026;&#26399;&#26395;&#30340;&#20844;&#24179;&#32467;&#26524;&#65292;&#35813;&#32467;&#26524;&#26681;&#25454;&#30456;&#20851;&#22870;&#21169;&#30340;&#27604;&#20363;&#32473;&#20104;&#27599;&#20010;&#21160;&#20316;&#26333;&#20809;&#12290;&#25105;&#20204;&#20351;&#29992;&#20844;&#24179;&#25439;&#22833;&#26469;&#27169;&#25311;&#31639;&#27861;&#30340;&#25928;&#26524;&#65292;&#20844;&#24179;&#25439;&#22833;&#25429;&#25417;&#20102;&#20844;&#24179;&#26368;&#20248;&#31574;&#30053;&#21644;&#31639;&#27861;&#36755;&#20986;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23558;&#20844;&#24179;CMAB&#31639;&#27861;&#24212;&#29992;&#20110;&#27599;&#20010;&#29420;&#31435;&#30340;&#20195;&#29702;&#20250;&#23548;&#33268;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#30340;&#20844;&#24179;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#21512;&#20316;&#24335;--&#32852;&#37030;&#23398;&#20064;&#21487;&#26356;&#21152;&#26377;&#25928;&#65292;&#24182;&#25552;&#20379;&#30830;&#20445;&#24046;&#20998;&#38544;&#31169;&#30340;&#31639;&#27861;Fed-FairX-LinUCB&#12290;&#25193;&#23637;&#29616;&#26377;&#38544;&#31169;&#26694;&#26550;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#35774;&#35745;&#36328;&#20195;&#29702;&#36890;&#20449;&#25152;&#38656;&#20449;&#24687;&#30340;&#36890;&#20449;&#21327;&#35758;&#12290;&#19968;&#20010;&#31616;&#21333;&#30340;&#21327;&#35758;&#21487;&#33021;&#20250;&#23548;&#33268;&#36739;&#24369;&#30340;&#38544;&#31169;&#20445;&#35777;&#25110;&#26356;&#39640;&#30340;&#25439;&#22833;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#20801;&#35768;f
&lt;/p&gt;
&lt;p&gt;
This paper considers the contextual multi-armed bandit (CMAB) problem with fairness and privacy guarantees in a federated environment. We consider merit-based exposure as the desired fair outcome, which provides exposure to each action in proportion to the reward associated. We model the algorithm's effectiveness using fairness regret, which captures the difference between fair optimal policy and the policy output by the algorithm. Applying fair CMAB algorithm to each agent individually leads to fairness regret linear in the number of agents. We propose that collaborative -- federated learning can be more effective and provide the algorithm Fed-FairX-LinUCB that also ensures differential privacy. The primary challenge in extending the existing privacy framework is designing the communication protocol for communicating required information across agents. A naive protocol can either lead to weaker privacy guarantees or higher regret. We design a novel communication protocol that allows f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#31354;&#38388;&#29615;&#22659;&#20013;&#39564;&#35777;&#39044;&#27979;&#26041;&#27861;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22788;&#29702;&#19981;&#21305;&#37197;&#24773;&#20917;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03527</link><description>&lt;p&gt;
&#22312;&#31354;&#38388;&#29615;&#22659;&#20013;&#19968;&#33268;&#39564;&#35777;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Consistent Validation for Predictive Methods in Spatial Settings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#31354;&#38388;&#29615;&#22659;&#20013;&#39564;&#35777;&#39044;&#27979;&#26041;&#27861;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22788;&#29702;&#19981;&#21305;&#37197;&#24773;&#20917;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#39044;&#27979;&#20219;&#21153;&#23545;&#20110;&#22825;&#27668;&#39044;&#25253;&#12289;&#31354;&#27668;&#27745;&#26579;&#30740;&#31350;&#21644;&#20854;&#20182;&#31185;&#23398;&#24037;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;&#30830;&#23450;&#25105;&#20204;&#23545;&#32479;&#35745;&#25110;&#29289;&#29702;&#26041;&#27861;&#25152;&#20316;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#26159;&#31185;&#23398;&#32467;&#35770;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20256;&#32479;&#30340;&#39564;&#35777;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#39564;&#35777;&#20301;&#32622;&#21644;&#25105;&#20204;&#24076;&#26395;&#36827;&#34892;&#39044;&#27979;&#30340;&#65288;&#27979;&#35797;&#65289;&#20301;&#32622;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#36825;&#31181;&#19981;&#21305;&#37197;&#36890;&#24120;&#19981;&#26159;&#21327;&#21464;&#37327;&#20559;&#31227;&#30340;&#19968;&#20010;&#23454;&#20363;&#65288;&#24120;&#24120;&#34987;&#24418;&#24335;&#21270;&#65289;&#65292;&#22240;&#20026;&#39564;&#35777;&#21644;&#27979;&#35797;&#20301;&#32622;&#26159;&#22266;&#23450;&#30340;&#65288;&#20363;&#22914;&#65292;&#22312;&#32593;&#26684;&#19978;&#25110;&#36873;&#23450;&#30340;&#28857;&#19978;&#65289;&#65292;&#32780;&#19981;&#26159;&#20174;&#20004;&#20010;&#20998;&#24067;&#20013;&#29420;&#31435;&#21516;&#20998;&#24067;&#22320;&#37319;&#26679;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#23545;&#39564;&#35777;&#26041;&#27861;&#30340;&#26816;&#26597;&#65306;&#38543;&#30528;&#39564;&#35777;&#25968;&#25454;&#30340;&#23494;&#24230;&#36234;&#26469;&#36234;&#22823;&#65292;&#23427;&#20204;&#33021;&#22815;&#21464;&#24471;&#20219;&#24847;&#31934;&#30830;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20256;&#32479;&#26041;&#27861;&#21644;&#21327;&#21464;&#37327;&#20559;&#31227;&#26041;&#27861;&#21487;&#33021;&#19981;&#28385;&#36275;&#36825;&#20010;&#26816;&#26597;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23427;&#20511;&#37492;&#20102;&#21327;&#21464;&#37327;&#20559;&#31227;&#25991;&#29486;&#20013;&#30340;&#29616;&#26377;&#24605;&#24819;&#65292;&#20294;&#23545;&#39564;&#35777;&#25968;&#25454;&#36827;&#34892;&#20102;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial prediction tasks are key to weather forecasting, studying air pollution, and other scientific endeavors. Determining how much to trust predictions made by statistical or physical methods is essential for the credibility of scientific conclusions. Unfortunately, classical approaches for validation fail to handle mismatch between locations available for validation and (test) locations where we want to make predictions. This mismatch is often not an instance of covariate shift (as commonly formalized) because the validation and test locations are fixed (e.g., on a grid or at select points) rather than i.i.d. from two distributions. In the present work, we formalize a check on validation methods: that they become arbitrarily accurate as validation data becomes arbitrarily dense. We show that classical and covariate-shift methods can fail this check. We instead propose a method that builds from existing ideas in the covariate-shift literature, but adapts them to the validation data 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20179;&#20648;&#20013;&#30340;&#25315;&#36873;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#12290;&#19982;&#20256;&#32479;&#21551;&#21457;&#24335;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#24555;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#65292;&#24182;&#33021;&#38477;&#20302;&#36335;&#24452;&#30340;&#24863;&#30693;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.03525</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#20179;&#20648;&#20013;&#30340;&#25315;&#36873;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Picker Routing Problem in Warehousing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20179;&#20648;&#20013;&#30340;&#25315;&#36873;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#12290;&#19982;&#20256;&#32479;&#21551;&#21457;&#24335;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#24555;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#65292;&#24182;&#33021;&#38477;&#20302;&#36335;&#24452;&#30340;&#24863;&#30693;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35746;&#21333;&#25315;&#36873;&#36710;&#36742;&#36335;&#24452;&#26159;&#20179;&#24211;&#36816;&#33829;&#31649;&#29702;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#30001;&#20110;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#21644;&#38656;&#35201;&#24555;&#36895;&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#65292;&#23454;&#36341;&#20013;&#24120;&#24120;&#20351;&#29992;&#27425;&#20248;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#26367;&#20195;&#20256;&#32479;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#36873;&#25321;&#65292;&#21487;&#33021;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#25315;&#36873;&#36710;&#36742;&#36335;&#24452;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#38382;&#39064;&#21442;&#25968;&#19978;&#19982;&#29616;&#26377;&#21551;&#21457;&#24335;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#20854;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#20248;&#28857;&#26159;&#21487;&#20197;&#38477;&#20302;&#36335;&#24452;&#30340;&#24863;&#30693;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Order Picker Routing is a critical issue in Warehouse Operations Management. Due to the complexity of the problem and the need for quick solutions, suboptimal algorithms are frequently employed in practice. However, Reinforcement Learning offers an appealing alternative to traditional heuristics, potentially outperforming existing methods in terms of speed and accuracy. We introduce an attention based neural network for modeling picker tours, which is trained using Reinforcement Learning. Our method is evaluated against existing heuristics across a range of problem parameters to demonstrate its efficacy. A key advantage of our proposed method is its ability to offer an option to reduce the perceived complexity of routes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36328;&#39046;&#22495;&#35780;&#20272;&#20102;&#38646;&#26679;&#26412;&#25688;&#35201;&#29983;&#25104;&#22120;&#30340;&#30495;&#23454;&#24615;&#12290;&#36890;&#36807;&#23545;&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#21644;&#27861;&#24459;&#27861;&#26696;&#31561;&#19987;&#19994;&#39046;&#22495;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#29983;&#25104;&#25688;&#35201;&#30340;&#30495;&#23454;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32473;&#23450;&#39046;&#22495;&#30340;&#26222;&#36941;&#24615;&#23545;&#25688;&#35201;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.03509</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#35780;&#20272;&#38646;&#26679;&#26412;&#25688;&#35201;&#29983;&#25104;&#22120;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36328;&#39046;&#22495;&#35780;&#20272;&#20102;&#38646;&#26679;&#26412;&#25688;&#35201;&#29983;&#25104;&#22120;&#30340;&#30495;&#23454;&#24615;&#12290;&#36890;&#36807;&#23545;&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#21644;&#27861;&#24459;&#27861;&#26696;&#31561;&#19987;&#19994;&#39046;&#22495;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#29983;&#25104;&#25688;&#35201;&#30340;&#30495;&#23454;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32473;&#23450;&#39046;&#22495;&#30340;&#26222;&#36941;&#24615;&#23545;&#25688;&#35201;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#38646;&#26679;&#26412;&#65288;&#21363;&#22312;&#27809;&#26377;&#26126;&#30830;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65289;&#29983;&#25104;&#25688;&#35201;&#65292;&#32463;&#36807;&#20154;&#24037;&#35780;&#20272;&#65292;&#36825;&#20123;&#25688;&#35201;&#24448;&#24448;&#19982;&#25163;&#24037;&#32534;&#20889;&#30340;&#21442;&#32771;&#25688;&#35201;&#30456;&#27604;&#65292;&#29978;&#33267;&#26356;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26089;&#26399;&#30340;&#30740;&#31350;&#20960;&#20046;&#19987;&#27880;&#20110;&#35780;&#20272;&#26032;&#38395;&#25991;&#31456;&#30340;&#25688;&#35201;&#12290;&#38646;&#26679;&#26412;&#25688;&#35201;&#29983;&#25104;&#22120;&#22312;&#20854;&#20182;&#65288;&#21487;&#33021;&#26356;&#19987;&#19994;&#65289;&#39046;&#22495;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#36328;&#19987;&#19994;&#39046;&#22495;&#20013;&#38646;&#26679;&#26412;&#29983;&#25104;&#30340;&#25688;&#35201;&#65292;&#21253;&#25324;&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#21644;&#27861;&#24459;&#27861;&#26696;&#65288;&#38500;&#20102;&#26631;&#20934;&#26032;&#38395;&#25688;&#35201;&#30340;&#21442;&#32771;&#65289;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#36755;&#20986;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#20174;&#39046;&#22495;&#19987;&#23478;&#22788;&#33719;&#21462;&#27880;&#37322;&#65292;&#20197;&#35782;&#21035;&#25688;&#35201;&#20013;&#30340;&#19981;&#19968;&#33268;&#20043;&#22788;&#65292;&#24182;&#23545;&#36825;&#20123;&#38169;&#35823;&#36827;&#34892;&#31995;&#32479;&#20998;&#31867;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32473;&#23450;&#39046;&#22495;&#30340;&#26222;&#36941;&#24615;&#26159;&#21542;&#20250;&#24433;&#21709;&#22312;&#35813;&#39046;&#22495;&#30340;&#25991;&#31456;&#30340;&#25688;&#35201;&#30340;&#25552;&#21462;&#21644;&#24544;&#23454;&#24230;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25152;&#26377;&#25910;&#38598;&#21040;&#30340;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that large language models (LLMs) are capable of generating summaries zero-shot (i.e., without explicit supervision) that, under human assessment, are often comparable or even preferred to manually composed reference summaries. However, this prior work has focussed almost exclusively on evaluating news article summarization. How do zero-shot summarizers perform in other (potentially more specialized) domains? In this work we evaluate zero-shot generated summaries across specialized domains including biomedical articles, and legal bills (in addition to standard news benchmarks for reference). We focus especially on the factuality of outputs. We acquire annotations from domain experts to identify inconsistencies in summaries and systematically categorize these errors. We analyze whether the prevalence of a given domain in the pretraining corpus affects extractiveness and faithfulness of generated summaries of articles in this domain. We release all collected annotat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#24191;&#27867;&#27867;&#21270;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#30740;&#31350;&#35299;&#20915;&#25277;&#35937;&#21644;&#25512;&#29702;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#35797;&#22270;&#25552;&#39640;&#35745;&#31639;&#26426;&#31995;&#32479;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03507</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25277;&#35937;&#21644;&#25512;&#29702;&#65306;&#36808;&#21521;&#26426;&#22120;&#30340;&#24191;&#27867;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neural networks for abstraction and reasoning: Towards broad generalization in machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03507
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#24191;&#27867;&#27867;&#21270;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#30740;&#31350;&#35299;&#20915;&#25277;&#35937;&#21644;&#25512;&#29702;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#35797;&#22270;&#25552;&#39640;&#35745;&#31639;&#26426;&#31995;&#32479;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#20010;&#19990;&#32426;&#20197;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#19968;&#30452;&#35797;&#22270;&#22797;&#21046;&#20154;&#31867;&#30340;&#25277;&#35937;&#21644;&#25512;&#29702;&#33021;&#21147;-&#21019;&#36896;&#20986;&#33021;&#22815;&#20174;&#19968;&#23567;&#32452;&#31034;&#20363;&#20013;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#35745;&#31639;&#26426;&#31995;&#32479;&#65292;&#22312;&#20154;&#20204;&#21457;&#29616;&#36825;&#24456;&#23481;&#26131;&#30340;&#24773;&#20917;&#19979;&#12290;&#34429;&#28982;&#29305;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#35299;&#20915;&#21508;&#31181;&#21508;&#26679;&#30340;&#38382;&#39064;&#65292;&#20294;&#23545;&#20110;&#36229;&#36234;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#30340;&#24191;&#27867;&#27867;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#31181;&#35299;&#20915;&#25277;&#35937;&#19982;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;ARC&#26159;&#19968;&#20010;&#29992;&#20110;&#27979;&#35797;&#31639;&#27861;&#22312;&#24191;&#27867;&#27867;&#21270;&#26041;&#38754;&#30340;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#26377;&#19977;&#20010;&#22269;&#38469;&#31454;&#36187;&#25552;&#20379;&#20102;10&#19975;&#32654;&#20803;&#30340;&#22870;&#37329;&#65292;&#26368;&#22909;&#30340;&#31639;&#27861;&#20173;&#28982;&#26080;&#27861;&#35299;&#20915;&#22823;&#22810;&#25968;ARC&#20219;&#21153;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#25163;&#24037;&#35268;&#21017;&#65292;&#27809;&#26377;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#20102;&#26368;&#36817;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#23637;&#26159;&#21542;&#33021;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;DreamCoder&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#27714;&#35299;&#22120;&#24212;&#29992;&#21040;ARC&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
For half a century, artificial intelligence research has attempted to reproduce the human qualities of abstraction and reasoning - creating computer systems that can learn new concepts from a minimal set of examples, in settings where humans find this easy. While specific neural networks are able to solve an impressive range of problems, broad generalisation to situations outside their training data has proved elusive.In this work, we look at several novel approaches for solving the Abstraction &amp; Reasoning Corpus (ARC), a dataset of abstract visual reasoning tasks introduced to test algorithms on broad generalization. Despite three international competitions with $100,000 in prizes, the best algorithms still fail to solve a majority of ARC tasks and rely on complex hand-crafted rules, without using machine learning at all. We revisit whether recent advances in neural networks allow progress on this task.   First, we adapt the DreamCoder neurosymbolic reasoning solver to ARC. DreamCoder
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;SAL&#65288;Separate And Learn&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#26631;&#35760;&#25968;&#25454;&#65292;&#20998;&#31163;&#24182;&#35757;&#32451;&#24322;&#24120;&#28857;&#21644;OOD&#20998;&#31867;&#22120;&#65292;&#29702;&#35770;&#19978;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#20445;&#35777;&#21644;&#20005;&#26684;&#30340;&#38169;&#35823;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.03502</link><description>&lt;p&gt;
&#26410;&#26631;&#35760;&#25968;&#25454;&#22914;&#20309;&#22312;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#20013;&#21457;&#25381;&#21487;&#35777;&#26126;&#30340;&#20316;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Does Unlabeled Data Provably Help Out-of-Distribution Detection?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;SAL&#65288;Separate And Learn&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#26631;&#35760;&#25968;&#25454;&#65292;&#20998;&#31163;&#24182;&#35757;&#32451;&#24322;&#24120;&#28857;&#21644;OOD&#20998;&#31867;&#22120;&#65292;&#29702;&#35770;&#19978;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#20445;&#35777;&#21644;&#20005;&#26684;&#30340;&#38169;&#35823;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#26816;&#27979;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#26041;&#38754;&#25913;&#36827;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#28508;&#21147;&#12290;&#21033;&#29992;&#37326;&#22806;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#33021;&#21147;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#20869;&#20998;&#24067;&#65288;ID&#65289;&#21644;OOD&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12290;&#32570;&#20047;&#28165;&#27905;&#30340;OOD&#26679;&#26412;&#38598;&#21512;&#22312;&#23398;&#20064;&#26368;&#20248;OOD&#20998;&#31867;&#22120;&#26041;&#38754;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;&#30446;&#21069;&#65292;&#32570;&#20047;&#23545;&#26410;&#26631;&#35760;&#25968;&#25454;&#22914;&#20309;&#24110;&#21161;OOD&#26816;&#27979;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;SAL&#65288;Separate And Learn&#65289;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#35813;&#26694;&#26550;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#24378;&#22823;&#30340;&#20445;&#35777;&#21644;&#23454;&#35777;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26694;&#26550;&#23558;&#20505;&#36873;&#24322;&#24120;&#28857;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#20998;&#31163;&#20986;&#26469;&#65292;&#24182;&#20351;&#29992;&#20505;&#36873;&#24322;&#24120;&#28857;&#21644;&#26631;&#35760;&#30340;ID&#25968;&#25454;&#35757;&#32451;OOD&#20998;&#31867;&#22120;&#12290;&#20174;&#21487;&#20998;&#31163;&#24615;&#21644;&#21487;&#23398;&#20064;&#24615;&#30340;&#35282;&#24230;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#38169;&#35823;&#30028;&#38480;&#65292;&#27491;&#24335;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#20013;&#30340;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using unlabeled data to regularize the machine learning models has demonstrated promise for improving safety and reliability in detecting out-of-distribution (OOD) data. Harnessing the power of unlabeled in-the-wild data is non-trivial due to the heterogeneity of both in-distribution (ID) and OOD data. This lack of a clean set of OOD samples poses significant challenges in learning an optimal OOD classifier. Currently, there is a lack of research on formally understanding how unlabeled data helps OOD detection. This paper bridges the gap by introducing a new learning framework SAL (Separate And Learn) that offers both strong theoretical guarantees and empirical effectiveness. The framework separates candidate outliers from the unlabeled data and then trains an OOD classifier using the candidate outliers and the labeled ID data. Theoretically, we provide rigorous error bounds from the lens of separability and learnability, formally justifying the two components in our algorithm. Our the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#30340;&#24378;&#21270;&#23398;&#20064;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65288;CRLQAS&#65289;&#65292;&#35299;&#20915;&#20102;&#22312;&#22122;&#22768;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#26102;&#20195;&#20013;&#65292;&#23545;&#20110;&#26550;&#26500;&#25628;&#32034;&#30340;&#22122;&#22768;&#25928;&#24212;&#30340;&#19981;&#29702;&#35299;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03500</link><description>&lt;p&gt;
&#37327;&#23376;&#30828;&#20214;&#38169;&#35823;&#19979;&#30340;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Curriculum reinforcement learning for quantum architecture search under hardware errors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#30340;&#24378;&#21270;&#23398;&#20064;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65288;CRLQAS&#65289;&#65292;&#35299;&#20915;&#20102;&#22312;&#22122;&#22768;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#26102;&#20195;&#20013;&#65292;&#23545;&#20110;&#26550;&#26500;&#25628;&#32034;&#30340;&#22122;&#22768;&#25928;&#24212;&#30340;&#19981;&#29702;&#35299;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22122;&#22768;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#26102;&#20195;&#65292;&#25214;&#21040;&#19982;&#24403;&#21069;&#35774;&#22791;&#38480;&#21046;&#20860;&#23481;&#30340;&#26377;&#29992;&#30005;&#36335;&#26159;&#20851;&#38190;&#25361;&#25112;&#12290;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#65288;VQAs&#65289;&#36890;&#36807;&#22266;&#23450;&#30005;&#36335;&#26550;&#26500;&#21644;&#20248;&#21270;&#22806;&#37096;&#24490;&#29615;&#20013;&#30340;&#20010;&#21035;&#38376;&#21442;&#25968;&#26469;&#25552;&#20379;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#21442;&#25968;&#20248;&#21270;&#21487;&#33021;&#21464;&#24471;&#38590;&#20197;&#22788;&#29702;&#65292;&#24182;&#19988;&#31639;&#27861;&#30340;&#25972;&#20307;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#21021;&#22987;&#36873;&#25321;&#30340;&#30005;&#36335;&#26550;&#26500;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;&#65288;QAS&#65289;&#31639;&#27861;&#26469;&#33258;&#21160;&#35774;&#35745;&#26377;&#29992;&#30340;&#30005;&#36335;&#26550;&#26500;&#12290;&#22312;&#20165;&#38480;&#21442;&#25968;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#35266;&#23519;&#21040;&#22122;&#22768;&#25928;&#24212;&#20005;&#37325;&#24433;&#21709;&#20248;&#21270;&#22120;&#21644;&#26368;&#32456;&#32467;&#26524;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#19968;&#26465;&#37325;&#35201;&#30340;&#30740;&#31350;&#32447;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26550;&#26500;&#25628;&#32034;&#30340;&#22122;&#22768;&#25928;&#24212;&#65292;&#21487;&#33021;&#21516;&#26679;&#37325;&#35201;&#30340;&#38382;&#39064;&#30446;&#21069;&#36824;&#19981;&#22826;&#29702;&#35299;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#35838;&#31243;&#30340;&#24378;&#21270;&#23398;&#20064;QAS&#65288;CRLQAS&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The key challenge in the noisy intermediate-scale quantum era is finding useful circuits compatible with current device limitations. Variational quantum algorithms (VQAs) offer a potential solution by fixing the circuit architecture and optimizing individual gate parameters in an external loop. However, parameter optimization can become intractable, and the overall performance of the algorithm depends heavily on the initially chosen circuit architecture. Several quantum architecture search (QAS) algorithms have been developed to design useful circuit architectures automatically. In the case of parameter optimization alone, noise effects have been observed to dramatically influence the performance of the optimizer and final outcomes, which is a key line of study. However, the effects of noise on the architecture search, which could be just as critical, are poorly understood. This work addresses this gap by introducing a curriculum-based reinforcement learning QAS (CRLQAS) algorithm desi
&lt;/p&gt;</description></item><item><title>&#31227;&#38500;&#33258;&#36866;&#24212;&#26041;&#27861;&#20013;&#30340;&#24179;&#26041;&#26681;&#21487;&#20197;&#22312;&#21367;&#31215;&#32467;&#26500;&#19978;&#20943;&#23567;&#19982;SGD&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;transformers&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03496</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21435;&#25481;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#20013;&#30340;&#24179;&#26041;&#26681;&#21527;&#65311;&#19968;&#20010;&#20108;&#38454;&#35282;&#24230;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03496
&lt;/p&gt;
&lt;p&gt;
&#31227;&#38500;&#33258;&#36866;&#24212;&#26041;&#27861;&#20013;&#30340;&#24179;&#26041;&#26681;&#21487;&#20197;&#22312;&#21367;&#31215;&#32467;&#26500;&#19978;&#20943;&#23567;&#19982;SGD&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;transformers&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26799;&#24230;&#20248;&#21270;&#22120;&#22914;Adam(W)&#26159;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#65288;&#22914;transformers&#65289;&#30340;&#40664;&#35748;&#35757;&#32451;&#31639;&#27861;&#12290;&#23427;&#20204;&#30340;&#23545;&#35282;&#20808;&#39564;&#22522;&#20110;&#26799;&#24230;&#22806;&#31215;&#65292;&#36890;&#36807;&#24179;&#26041;&#26681;&#21152;&#20837;&#21040;&#21442;&#25968;&#26356;&#26032;&#20013;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#34987;&#31216;&#20026;&#36817;&#20284;&#30340;&#20108;&#38454;&#26041;&#27861;&#65292;&#20294;&#24179;&#26041;&#26681;&#34920;&#31034;&#20102;&#19968;&#20010;&#26681;&#26412;&#24615;&#30340;&#21306;&#21035;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21435;&#25481;&#24179;&#26041;&#26681;&#21518;&#33258;&#36866;&#24212;&#26041;&#27861;&#30340;&#34892;&#20026;&#22914;&#20309;&#21464;&#21270;&#65292;&#21363;&#21152;&#24378;&#23427;&#20204;&#30340;&#20108;&#38454;&#21160;&#26426;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#21435;&#25481;&#24179;&#26041;&#26681;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#33021;&#22815;&#22312;&#21367;&#31215;&#32467;&#26500;&#19978;&#32553;&#23567;&#19982;SGD&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22312;transformers&#19978;&#22522;&#20110;&#24179;&#26041;&#26681;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20108;&#38454;&#35282;&#24230;&#23545;&#20110;&#24320;&#21457;&#20855;&#26377;&#38750;&#23545;&#35282;&#20808;&#39564;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#20063;&#20855;&#26377;&#23454;&#38469;&#22909;&#22788;&#12290;&#19982;&#20687;Shampoo&#36825;&#26679;&#22522;&#20110;&#24179;&#26041;&#26681;&#30340;&#23545;&#24212;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#20204;&#19981;&#38656;&#35201;&#25968;&#20540;&#19981;&#31283;&#23450;&#30340;&#30697;&#38453;&#24179;&#26041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive gradient optimizers like Adam(W) are the default training algorithms for many deep learning architectures, such as transformers. Their diagonal preconditioner is based on the gradient outer product which is incorporated into the parameter update via a square root. While these methods are often motivated as approximate second-order methods, the square root represents a fundamental difference. In this work, we investigate how the behavior of adaptive methods changes when we remove the root, i.e. strengthen their second-order motivation. Surprisingly, we find that such square-root-free adaptive methods close the generalization gap to SGD on convolutional architectures, while maintaining their root-based counterpart's performance on transformers. The second-order perspective also has practical benefits for the development of adaptive methods with non-diagonal preconditioner. In contrast to root-based counterparts like Shampoo, they do not require numerically unstable matrix square
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#38543;&#26426;&#24615;&#30340;&#26080;&#38480;&#28145;&#24230;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#26694;&#26550;&#20013;&#25972;&#21512;&#37096;&#20998;&#38543;&#26426;&#24615;&#65292;&#25913;&#21892;&#29616;&#26377;&#26550;&#26500;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#19978;&#30340;&#35745;&#31639;&#25928;&#29575;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#28789;&#27963;&#30340;&#32593;&#32476;&#35774;&#35745;&#37197;&#32622;&#65292;&#21516;&#26102;&#36890;&#36807;&#25968;&#23398;&#35777;&#26126;&#30830;&#20445;&#20102;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03495</link><description>&lt;p&gt;
&#37096;&#20998;&#38543;&#26426;&#30340;&#26080;&#38480;&#28145;&#24230;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Partially Stochastic Infinitely Deep Bayesian Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#38543;&#26426;&#24615;&#30340;&#26080;&#38480;&#28145;&#24230;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#26694;&#26550;&#20013;&#25972;&#21512;&#37096;&#20998;&#38543;&#26426;&#24615;&#65292;&#25913;&#21892;&#29616;&#26377;&#26550;&#26500;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#19978;&#30340;&#35745;&#31639;&#25928;&#29575;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#28789;&#27963;&#30340;&#32593;&#32476;&#35774;&#35745;&#37197;&#32622;&#65292;&#21516;&#26102;&#36890;&#36807;&#25968;&#23398;&#35777;&#26126;&#30830;&#20445;&#20102;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#38543;&#26426;&#30340;&#26080;&#38480;&#28145;&#24230;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#37096;&#20998;&#38543;&#26426;&#24615;&#25972;&#21512;&#21040;&#26080;&#38480;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#30340;&#26032;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26032;&#22411;&#26550;&#26500;&#26088;&#22312;&#25913;&#21892;&#29616;&#26377;&#26550;&#26500;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#19978;&#30340;&#35745;&#31639;&#25928;&#29575;&#38480;&#21046;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#37096;&#20998;&#38543;&#26426;&#24615;&#22312;&#26080;&#38480;&#28145;&#24230;&#26497;&#38480;&#19979;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#20840;&#38543;&#26426;&#24615;&#30340;&#22909;&#22788;&#65292;&#22914;&#40065;&#26834;&#24615;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#20869;&#23384;&#25928;&#29575;&#65292;&#21516;&#26102;&#25913;&#21892;&#20102;&#23427;&#20204;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#19978;&#30340;&#35745;&#31639;&#25928;&#29575;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#31181;&#26550;&#26500;&#37197;&#32622;&#65292;&#25552;&#20379;&#20102;&#32593;&#32476;&#35774;&#35745;&#30340;&#28789;&#27963;&#24615;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#26435;&#37325;&#21010;&#20998;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#30830;&#31435;&#25105;&#20204;&#30340;&#32593;&#32476;&#23478;&#26063;&#31526;&#21512;&#36890;&#29992;&#26465;&#20214;&#20998;&#24067;&#36817;&#20284;&#22120;&#30340;&#25968;&#23398;&#20445;&#35777;&#65292;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present Partially Stochastic Infinitely Deep Bayesian Neural Networks, a novel family of architectures that integrates partial stochasticity into the framework of infinitely deep neural networks. Our new class of architectures is designed to improve the limitations of existing architectures around computational efficiency at training and inference time. To do this, we leverage the advantages of partial stochasticity in the infinite-depth limit which include the benefits of full stochasticity e.g. robustness, uncertainty quantification, and memory efficiency, whilst improving their limitations around computational efficiency at training and inference time. We present a variety of architectural configurations, offering flexibility in network design including different methods for weight partition. We also provide mathematical guarantees on the expressivity of our models by establishing that our network family qualifies as Universal Conditional Distribution Approximators
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20020;&#24202;&#25968;&#25454;&#20013;&#33043;&#27602;&#30151;&#30340;&#26089;&#26399;&#21457;&#20316;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;XGBoost&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#35268;&#33539;&#21270;&#25928;&#29992;&#20998;&#25968;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03486</link><description>&lt;p&gt;
&#20020;&#24202;&#29615;&#22659;&#20013;&#26089;&#26399;&#39044;&#27979;&#33043;&#27602;&#30151;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Early prediction of onset of sepsis in Clinical Setting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20020;&#24202;&#25968;&#25454;&#20013;&#33043;&#27602;&#30151;&#30340;&#26089;&#26399;&#21457;&#20316;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;XGBoost&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#35268;&#33539;&#21270;&#25928;&#29992;&#20998;&#25968;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#26469;&#33258;&#32445;&#32422;&#24067;&#26391;&#20811;&#26031;Montefiore&#21307;&#30103;&#20013;&#24515;&#30340;&#21435;&#26631;&#35782;&#21270;&#20020;&#24202;&#25968;&#25454;&#65292;&#39044;&#27979;&#33043;&#27602;&#30151;&#30340;&#26089;&#26399;&#21457;&#20316;&#12290;&#37319;&#29992;&#20102;&#26377;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;XGBoost&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;80%&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;107&#20010;&#29305;&#24449;&#65288;&#21253;&#25324;&#21407;&#22987;&#21644;&#34893;&#29983;&#29305;&#24449;&#65289;&#12290;&#38543;&#21518;&#65292;&#35813;&#27169;&#22411;&#22312;&#21097;&#20313;&#30340;20%&#30340;&#27979;&#35797;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#27169;&#22411;&#22312;&#35757;&#32451;&#38454;&#27573;&#23436;&#20840;&#26410;&#30693;&#30340;&#21069;&#30651;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#22312;&#20010;&#20307;&#24739;&#32773;&#27700;&#24179;&#19978;&#30340;&#24615;&#33021;&#21644;&#39044;&#27979;&#30340;&#21450;&#26102;&#24615;&#65292;&#20351;&#29992;&#20102;&#35268;&#33539;&#21270;&#25928;&#29992;&#20998;&#25968;&#65292;&#36825;&#26159;&#33043;&#27602;&#30151;&#26816;&#27979;&#20013;&#24191;&#27867;&#35748;&#21487;&#30340;&#35780;&#20998;&#26041;&#27861;&#65292;&#22914;PhysioNet Sepsis Challenge&#35770;&#25991;&#20013;&#25152;&#36848;&#12290;&#36824;&#35774;&#35745;&#20102;F1&#20540;&#12289;&#25935;&#24863;&#24615;&#12289;&#29305;&#24322;&#24615;&#21644;&#26631;&#24535;&#29575;&#31561;&#25351;&#26631;&#12290;&#35813;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#35268;&#33539;&#21270;&#25928;&#29992;&#20998;&#25968;&#20026;0.494&#65292;&#22312;&#21069;&#30651;&#25968;&#25454;&#19978;&#30340;&#35268;&#33539;&#21270;&#25928;&#29992;&#20998;&#25968;&#20026;0.378&#65288;&#38408;&#20540;&#20026;0.3&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes the use of Machine Learning models to predict the early onset of sepsis using deidentified clinical data from Montefiore Medical Center in Bronx, NY, USA. A supervised learning approach was adopted, wherein an XGBoost model was trained utilizing 80\% of the train dataset, encompassing 107 features (including the original and derived features). Subsequently, the model was evaluated on the remaining 20\% of the test data. The model was validated on prospective data that was entirely unseen during the training phase. To assess the model's performance at the individual patient level and timeliness of the prediction, a normalized utility score was employed, a widely recognized scoring methodology for sepsis detection, as outlined in the PhysioNet Sepsis Challenge paper. Metrics such as F1 Score, Sensitivity, Specificity, and Flag Rate were also devised. The model achieved a normalized utility score of 0.494 on test data and 0.378 on prospective data at threshold 0.3. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#26041;&#24335;&#30740;&#31350;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26550;&#26500;&#65292;&#27604;&#36739;&#20102;&#20107;&#21518;&#35299;&#37322;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35299;&#37322;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#23613;&#31649;&#26377;&#23616;&#38480;&#24615;&#65292;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#33021;&#22815;&#25429;&#33719;&#27604;&#20165;&#20165;&#26816;&#26597;&#27880;&#24847;&#21147;&#26435;&#37325;&#26356;&#26377;&#29992;&#30340;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2402.03485</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#19982;&#20107;&#21518;&#21487;&#35299;&#37322;&#24615;&#30456;&#36935;&#65306;&#25968;&#23398;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Attention Meets Post-hoc Interpretability: A Mathematical Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#26041;&#24335;&#30740;&#31350;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26550;&#26500;&#65292;&#27604;&#36739;&#20102;&#20107;&#21518;&#35299;&#37322;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35299;&#37322;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#23613;&#31649;&#26377;&#23616;&#38480;&#24615;&#65292;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#33021;&#22815;&#25429;&#33719;&#27604;&#20165;&#20165;&#26816;&#26597;&#27880;&#24847;&#21147;&#26435;&#37325;&#26356;&#26377;&#29992;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#22522;&#20110;transformer&#31561;&#26550;&#26500;&#65292;&#25104;&#20026;&#20102;&#25216;&#26415;&#38761;&#21629;&#30340;&#26680;&#24515;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#38500;&#20102;&#24110;&#21161;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#20043;&#22806;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#26412;&#36523;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#20869;&#37096;&#34892;&#20026;&#30340;&#26377;&#24847;&#20041;&#27934;&#23519;&#12290;&#36825;&#20123;&#27934;&#23519;&#26159;&#21542;&#21487;&#20197;&#29992;&#20316;&#35299;&#37322;&#65311;&#20851;&#20110;&#27492;&#20105;&#35770;&#19981;&#26029;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#26041;&#24335;&#30740;&#31350;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26550;&#26500;&#65292;&#24182;&#20934;&#30830;&#23450;&#20301;&#20102;&#20107;&#21518;&#35299;&#37322;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35299;&#37322;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#25105;&#20204;&#34920;&#26126;&#23427;&#20204;&#25552;&#20379;&#20102;&#30456;&#24403;&#19981;&#21516;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#23613;&#31649;&#26377;&#20854;&#23616;&#38480;&#24615;&#65292;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#33021;&#22815;&#25429;&#33719;&#27604;&#20165;&#20165;&#26816;&#26597;&#27880;&#24847;&#21147;&#26435;&#37325;&#26356;&#26377;&#29992;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based architectures, in particular transformers, are at the heart of a technological revolution. Interestingly, in addition to helping obtain state-of-the-art results on a wide range of applications, the attention mechanism intrinsically provides meaningful insights on the internal behavior of the model. Can these insights be used as explanations? Debate rages on. In this paper, we mathematically study a simple attention-based architecture and pinpoint the differences between post-hoc and attention-based explanations. We show that they provide quite different results, and that, despite their limitations, post-hoc methods are capable of capturing more useful insights than merely examining the attention weights.
&lt;/p&gt;</description></item><item><title>FINEST&#26041;&#27861;&#36890;&#36807;&#20174;&#32473;&#23450;&#30340;&#25512;&#33616;&#27169;&#22411;&#33719;&#24471;&#21442;&#32771;&#25490;&#24207;&#21015;&#34920;&#65292;&#24182;&#22312;&#27169;&#25311;&#30340;&#25200;&#21160;&#22330;&#26223;&#19979;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#65292;&#20445;&#25345;&#25490;&#24207;&#65292;&#20197;&#31283;&#23450;&#21644;&#25913;&#21892;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03481</link><description>&lt;p&gt;
FINEST: &#36890;&#36807;&#20445;&#25345;&#25490;&#24207;&#36827;&#34892;&#24494;&#35843;&#20197;&#31283;&#23450;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
FINEST: Stabilizing Recommendations by Rank-Preserving Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03481
&lt;/p&gt;
&lt;p&gt;
FINEST&#26041;&#27861;&#36890;&#36807;&#20174;&#32473;&#23450;&#30340;&#25512;&#33616;&#27169;&#22411;&#33719;&#24471;&#21442;&#32771;&#25490;&#24207;&#21015;&#34920;&#65292;&#24182;&#22312;&#27169;&#25311;&#30340;&#25200;&#21160;&#22330;&#26223;&#19979;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#65292;&#20445;&#25345;&#25490;&#24207;&#65292;&#20197;&#31283;&#23450;&#21644;&#25913;&#21892;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#21487;&#33021;&#20250;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#30340;&#24494;&#23567;&#25200;&#21160;&#32780;&#36755;&#20986;&#22823;&#19981;&#30456;&#21516;&#30340;&#25512;&#33616;&#32467;&#26524;&#12290;&#21333;&#20010;&#29992;&#25143;&#25968;&#25454;&#30340;&#25913;&#21464;&#20250;&#25913;&#21464;&#20854;&#20182;&#29992;&#25143;&#30340;&#25512;&#33616;&#32467;&#26524;&#12290;&#22312;&#21307;&#30103;&#12289;&#20303;&#25151;&#21644;&#37329;&#34701;&#31561;&#24212;&#29992;&#20013;&#65292;&#36825;&#31181;&#25935;&#24863;&#24615;&#21487;&#33021;&#23545;&#29992;&#25143;&#20307;&#39564;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#31283;&#23450;&#32473;&#23450;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#25269;&#25239;&#36825;&#31181;&#25200;&#21160;&#12290;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;(1)&#32570;&#20047;&#21487;&#20197;&#29992;&#26469;&#38170;&#23450;&#36755;&#20986;&#30340;&#8220;&#21442;&#32771;&#8221;&#25490;&#24207;&#21015;&#34920;&#65307;(2)&#22312;&#20445;&#35777;&#27169;&#22411;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#25152;&#26377;&#21487;&#33021;&#25200;&#21160;&#30340;&#25490;&#24207;&#21015;&#34920;&#31283;&#23450;&#24615;&#26041;&#38754;&#23384;&#22312;&#35745;&#31639;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;FINEST&#36890;&#36807;&#20174;&#32473;&#23450;&#30340;&#25512;&#33616;&#27169;&#22411;&#33719;&#24471;&#21442;&#32771;&#25490;&#24207;&#21015;&#34920;&#65292;&#28982;&#21518;&#22312;&#27169;&#25311;&#30340;&#25200;&#21160;&#22330;&#26223;&#19979;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#65292;&#24182;&#20445;&#25345;&#25490;&#24207;&#30340;&#35268;&#21017;&#21270;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;FINEST&#21487;&#20197;&#31283;&#23450;&#25512;&#33616;&#31995;&#32479;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern recommender systems may output considerably different recommendations due to small perturbations in the training data. Changes in the data from a single user will alter the recommendations as well as the recommendations of other users. In applications like healthcare, housing, and finance, this sensitivity can have adverse effects on user experience. We propose a method to stabilize a given recommender system against such perturbations. This is a challenging task due to (1) the lack of a ``reference'' rank list that can be used to anchor the outputs; and (2) the computational challenges in ensuring the stability of rank lists with respect to all possible perturbations of training data. Our method, FINEST, overcomes these challenges by obtaining reference rank lists from a given recommendation model and then fine-tuning the model under simulated perturbation scenarios with rank-preserving regularization on sampled items. Our experiments on real-world datasets demonstrate that FIN
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#30740;&#20102;&#20026;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#25903;&#25345;&#30340;&#20806;&#32423;&#21442;&#25968;&#20154;&#24037;&#26234;&#33021;&#22522;&#30784;&#35774;&#26045;&#65292;&#24182;&#25551;&#36848;&#20102;&#22312;&#31995;&#32479;&#35774;&#35745;&#20013;&#25152;&#38754;&#20020;&#30340;&#37325;&#35201;&#25216;&#26415;&#25361;&#25112;&#21644;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03480</link><description>&lt;p&gt;
&#20026;&#31185;&#23398;&#21457;&#29616;&#26381;&#21153;&#30340;&#20806;&#32423;&#21442;&#25968;&#20154;&#24037;&#26234;&#33021;&#22522;&#30784;&#35774;&#26045;&#65306;&#19968;&#39033;&#35843;&#30740;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Trillion Parameter AI Serving Infrastructure for Scientific Discovery: A Survey and Vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03480
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#30740;&#20102;&#20026;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#25903;&#25345;&#30340;&#20806;&#32423;&#21442;&#25968;&#20154;&#24037;&#26234;&#33021;&#22522;&#30784;&#35774;&#26045;&#65292;&#24182;&#25551;&#36848;&#20102;&#22312;&#31995;&#32479;&#35774;&#35745;&#20013;&#25152;&#38754;&#20020;&#30340;&#37325;&#35201;&#25216;&#26415;&#25361;&#25112;&#21644;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#27491;&#22312;&#25913;&#21464;&#30740;&#31350;&#24037;&#20316;&#65292;&#23454;&#29616;&#26032;&#25216;&#26415;&#65292;&#24182;&#26368;&#32456;&#24102;&#26469;&#26032;&#30340;&#21457;&#29616;&#12290;&#38543;&#30528;&#23545;&#26356;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#65292;&#25105;&#20204;&#27491;&#36827;&#20837;&#19968;&#20010;&#20806;&#32423;&#21442;&#25968;&#27169;&#22411;&#65288;TPM&#65289;&#30340;&#26102;&#20195;&#65292;&#21363;&#20855;&#26377;&#36229;&#36807;&#19968;&#19975;&#20159;&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#22914;&#21326;&#20026;&#30340;PanGu-$ \ Sigma $&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#38024;&#23545;&#31185;&#23398;&#30028;&#29305;&#23450;&#38656;&#27714;&#30340;TPM&#29992;&#25143;&#21644;&#25552;&#20379;&#32773;&#29983;&#24577;&#31995;&#32479;&#30340;&#24895;&#26223;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#20026;&#25552;&#20379;TPM&#26381;&#21153;&#30340;&#31995;&#32479;&#35774;&#35745;&#20013;&#30340;&#37325;&#22823;&#25216;&#26415;&#25361;&#25112;&#21644;&#24320;&#25918;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#36719;&#20214;&#22534;&#26632;&#21644;&#25509;&#21475;&#35201;&#27714;&#65292;&#20197;&#25903;&#25345;&#30740;&#31350;&#32773;&#22810;&#26679;&#21270;&#21644;&#28789;&#27963;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning methods are transforming research, enabling new techniques, and ultimately leading to new discoveries. As the demand for more capable AI models continues to grow, we are now entering an era of Trillion Parameter Models (TPM), or models with more than a trillion parameters -- such as Huawei's PanGu-$\Sigma$. We describe a vision for the ecosystem of TPM users and providers that caters to the specific needs of the scientific community. We then outline the significant technical challenges and open problems in system design for serving TPMs to enable scientific research and discovery. Specifically, we describe the requirements of a comprehensive software stack and interfaces to support the diverse and flexible requirements of researchers.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#26681;&#25454;&#20540;&#25439;&#22833;&#20248;&#20808;&#36873;&#25321;&#32423;&#21035;&#65292;&#21487;&#20197;&#25913;&#21892;&#20195;&#29702;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26041;&#27861;&#23545;&#25913;&#21892;&#20195;&#29702;&#34920;&#29616;&#20063;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03479</link><description>&lt;p&gt;
ICED: &#36890;&#36807;&#19978;&#19979;&#25991;&#29615;&#22659;&#35774;&#35745;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
ICED: Zero-Shot Transfer in Reinforcement Learning via In-Context Environment Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#36801;&#31227;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#26681;&#25454;&#20540;&#25439;&#22833;&#20248;&#20808;&#36873;&#25321;&#32423;&#21035;&#65292;&#21487;&#20197;&#25913;&#21892;&#20195;&#29702;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#26041;&#27861;&#23545;&#25913;&#21892;&#20195;&#29702;&#34920;&#29616;&#20063;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#33258;&#20027;&#20195;&#29702;&#36890;&#24120;&#32570;&#20047;&#25104;&#21151;&#22320;&#25512;&#24191;&#21040;&#26032;&#29615;&#22659;&#30340;&#33021;&#21147;&#65292;&#21363;&#20351;&#36825;&#20123;&#29615;&#22659;&#19982;&#23427;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#29615;&#22659;&#20855;&#26377;&#30456;&#20284;&#30340;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#20307;&#29615;&#22659;&#23454;&#20363;&#65288;&#25110;&#32423;&#21035;&#65289;&#30340;&#37319;&#26679;&#23545;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#25512;&#24191;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#20849;&#20139;&#22522;&#26412;&#23618;&#30340;&#28145;&#24230;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26550;&#26500;&#65292;&#26681;&#25454;&#20854;&#20540;&#25439;&#22833;&#20248;&#20808;&#36873;&#25321;&#32423;&#21035;&#65292;&#21487;&#20197;&#26368;&#23567;&#21270;&#20195;&#29702;&#30340;&#20869;&#37096;&#34920;&#31034;&#19982;&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#35757;&#32451;&#32423;&#21035;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#36825;&#20026;&#26576;&#20123;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#23454;&#29616;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#27880;&#24847;&#21147;&#36716;&#21521;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;UED&#65289;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#20855;&#26377;&#26356;&#22810;&#25511;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;UED&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25913;&#21464;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#29615;&#22659;&#23454;&#20363;&#65292;&#20174;&#32780;&#24433;&#21709;&#20195;&#29702;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents trained using deep reinforcement learning (RL) often lack the ability to successfully generalise to new environments, even when they share characteristics with the environments they have encountered during training. In this work, we investigate how the sampling of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents. We discover that, for deep actor-critic architectures sharing their base layers, prioritising levels according to their value loss minimises the mutual information between the agent's internal representation and the set of training levels in the generated training data. This provides a novel theoretical justification for the implicit regularisation achieved by certain adaptive sampling strategies. We then turn our attention to unsupervised environment design (UED) methods, which have more control over the data generation mechanism. We find that existing UED methods can significantly shift the trainin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#21512;&#26041;&#27861;&#65292;&#36229;&#25193;&#25955;&#65292;&#21487;&#20197;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#20934;&#30830;&#20272;&#35745;&#35748;&#35782;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03478</link><description>&lt;p&gt;
&#36229;&#25193;&#25955;&#65306;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#20272;&#35745;&#35748;&#35782;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Hyper-Diffusion: Estimating Epistemic and Aleatoric Uncertainty with a Single Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#21512;&#26041;&#27861;&#65292;&#36229;&#25193;&#25955;&#65292;&#21487;&#20197;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#20934;&#30830;&#20272;&#35745;&#35748;&#35782;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#39640;&#39118;&#38505;&#24212;&#29992;&#39046;&#22495;&#65288;&#22914;&#21307;&#23398;&#24433;&#20687;&#21644;&#22825;&#27668;&#39044;&#25253;&#65289;&#26102;&#65292;&#20934;&#30830;&#20272;&#35745;&#21644;&#21306;&#20998;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65288;&#21487;&#20197;&#36890;&#36807;&#26356;&#22810;&#30340;&#35757;&#32451;&#25968;&#25454;&#38477;&#20302;&#30340;&#19981;&#30830;&#23450;&#24615;&#65289;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#65288;&#19982;&#24403;&#21069;&#20219;&#21153;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#20934;&#30830;&#26377;&#25928;&#22320;&#20174;&#25968;&#25454;&#38598;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#31361;&#30772;&#24615;&#33021;&#21147;&#65292;&#29616;&#22312;&#20351;&#24471;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20174;&#27010;&#24565;&#19978;&#21464;&#24471;&#31616;&#21333;&#26126;&#20102;&#65306;&#21482;&#38656;&#35201;&#35757;&#32451;&#21644;&#20174;&#19968;&#20010;&#22823;&#22411;&#25193;&#25955;&#27169;&#22411;&#38598;&#21512;&#20013;&#37319;&#26679;&#21363;&#21487;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27169;&#22411;&#26550;&#26500;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#35757;&#32451;&#36825;&#26679;&#19968;&#20010;&#38598;&#21512;&#21464;&#24471;&#38590;&#20197;&#35745;&#31639;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#21512;&#26041;&#27861;&#65292;&#36229;&#25193;&#25955;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#21333;&#19968;&#27169;&#22411;&#20934;&#30830;&#20272;&#35745;&#35748;&#35782;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating and disentangling epistemic uncertainty (uncertainty that can be reduced with more training data) and aleatoric uncertainty (uncertainty that is inherent to the task at hand) is critically important when applying machine learning (ML) to high-stakes applications such as medical imaging and weather forecasting. Conditional diffusion models' breakthrough ability to accurately and efficiently sample from the posterior distribution of a dataset now makes uncertainty estimation conceptually straightforward: One need only train and sample from a large ensemble of diffusion models. Unfortunately, training such an ensemble becomes computationally intractable as the complexity of the model architecture grows.   In this work we introduce a new approach to ensembling, hyper-diffusion, which allows one to accurately estimate epistemic and aleatoric uncertainty with a single model. Unlike existing Monte Carlo dropout based single-model ensembling methods, hyper-diffusion offers the same 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#21518;&#39564;&#25277;&#26679;&#65288;DPS&#65289;&#36827;&#34892;&#20809;&#35889;CT&#26448;&#26009;&#20998;&#35299;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26080;&#30417;&#30563;&#35757;&#32451;&#30340;&#20808;&#39564;&#30693;&#35782;&#21644;&#29289;&#29702;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#12289;&#20302;&#19981;&#30830;&#23450;&#24615;&#21644;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.03476</link><description>&lt;p&gt;
CT&#26448;&#26009;&#20998;&#35299;&#25216;&#26415;&#30340;&#20809;&#35889;&#25193;&#25955;&#21518;&#39564;&#25277;&#26679;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
CT Material Decomposition using Spectral Diffusion Posterior Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#21518;&#39564;&#25277;&#26679;&#65288;DPS&#65289;&#36827;&#34892;&#20809;&#35889;CT&#26448;&#26009;&#20998;&#35299;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26080;&#30417;&#30563;&#35757;&#32451;&#30340;&#20808;&#39564;&#30693;&#35782;&#21644;&#29289;&#29702;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#12289;&#20302;&#19981;&#30830;&#23450;&#24615;&#21644;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#21518;&#39564;&#25277;&#26679;&#65288;DPS&#65289;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#20809;&#35889;CT&#27979;&#37327;&#20013;&#36827;&#34892;&#26448;&#26009;&#20998;&#35299;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26469;&#33258;&#26080;&#30417;&#30563;&#35757;&#32451;&#30340;&#22797;&#26434;&#20808;&#39564;&#30693;&#35782;&#21644;&#20005;&#26684;&#30340;&#27979;&#37327;&#29289;&#29702;&#27169;&#22411;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#19988;&#26356;&#31283;&#23450;&#30340;&#21464;&#20307;&#65292;&#20351;&#29992;&#21551;&#21160;&#36807;&#31243;&#20943;&#23569;&#20102;&#21453;&#21521;&#36807;&#31243;&#25152;&#38656;&#30340;&#26102;&#38388;&#27493;&#25968;&#65292;&#24182;&#20351;&#29992;&#26799;&#24230;&#36817;&#20284;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#38024;&#23545;&#20004;&#31181;&#20809;&#35889;CT&#31995;&#32479;&#65288;&#21452;&#33021;&#37327;CT&#21644;&#21452;&#23618;&#25506;&#27979;&#22120;CT&#65289;&#65292;&#23545;&#24615;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#22312;&#36825;&#20004;&#20010;&#31995;&#32479;&#19978;&#65292;DPS&#20165;&#20351;&#29992;10%&#30340;&#36845;&#20195;&#27425;&#25968;&#65292;&#23601;&#21487;&#20197;&#33719;&#24471;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#26448;&#26009;&#20998;&#35299;&#65288;MBMD&#65289;&#20013;&#20351;&#29992;&#30340;&#30456;&#21516;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#24230;&#37327;&#65288;SSIM&#65289;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#21551;&#21160;&#36807;&#31243;&#30340;DPS&#65288;JSDPS&#65289;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#35745;&#31639;&#26102;&#38388;&#36229;&#36807;85%&#65292;&#24182;&#19988;&#19982;&#32463;&#20856;DPS&#21644;MBMD&#30456;&#27604;&#65292;&#20855;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#26368;&#20302;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#26368;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a new deep learning approach based on diffusion posterior sampling (DPS) to perform material decomposition from spectral CT measurements. This approach combines sophisticated prior knowledge from unsupervised training with a rigorous physical model of the measurements. A faster and more stable variant is proposed that uses a jumpstarted process to reduce the number of time steps required in the reverse process and a gradient approximation to reduce the computational cost. Performance is investigated for two spectral CT systems: dual-kVp and dual-layer detector CT. On both systems, DPS achieves high Structure Similarity Index Metric Measure(SSIM) with only 10% of iterations as used in the model-based material decomposition(MBMD). Jumpstarted DPS (JSDPS) further reduces computational time by over 85% and achieves the highest accuracy, the lowest uncertainty, and the lowest computational costs compared to classic DPS and MBMD. The results demonstrate the potenti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28369;&#21160;&#31383;&#21475;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#26862;&#26519;&#20998;&#31867;&#22120;&#30340;&#27963;&#21160;&#21306;&#22495;&#32768;&#26001;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#25490;&#24207;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#24573;&#30053;&#26102;&#38388;&#28436;&#21270;&#29305;&#24449;&#26102;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03474</link><description>&lt;p&gt;
&#22522;&#20110;&#28369;&#21160;&#31383;&#21475;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#26862;&#26519;&#20998;&#31867;&#22120;&#30340;&#27963;&#21160;&#21306;&#22495;&#32768;&#26001;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Active Region-based Flare Forecasting with Sliding Window Multivariate Time Series Forest Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28369;&#21160;&#31383;&#21475;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#26862;&#26519;&#20998;&#31867;&#22120;&#30340;&#27963;&#21160;&#21306;&#22495;&#32768;&#26001;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#25490;&#24207;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#24573;&#30053;&#26102;&#38388;&#28436;&#21270;&#29305;&#24449;&#26102;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#22522;&#20110;&#29289;&#29702;&#27169;&#25311;&#21644;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#65288;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#65289;&#30340;&#35768;&#22810;&#24212;&#29992;&#24050;&#32463;&#20986;&#29616;&#65292;&#29992;&#20110;&#20998;&#26512;&#21644;&#39044;&#27979;&#22826;&#38451;&#32768;&#26001;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#29702;&#35299;&#22826;&#38451;&#32768;&#26001;&#30340;&#21160;&#21147;&#23398;&#36807;&#31243;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20027;&#35201;&#30446;&#30340;&#26159;&#39044;&#27979;&#36825;&#20123;&#20107;&#20214;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23567;&#23427;&#20204;&#21487;&#33021;&#23545;&#22320;&#29699;&#36896;&#25104;&#30340;&#39118;&#38505;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#19968;&#20010;&#26174;&#33879;&#30340;&#32570;&#28857;&#26159;&#32570;&#20047;&#32771;&#34385;&#32768;&#26001;&#36215;&#28304;&#27963;&#21160;&#21306;&#22495;&#30340;&#26102;&#38388;&#28436;&#21270;&#29305;&#24449;&#12290;&#36825;&#20010;&#30095;&#24573;&#22952;&#30861;&#20102;&#36825;&#20123;&#26041;&#27861;&#25226;&#39640;&#32500;&#27963;&#21160;&#21306;&#22495;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#20840;&#37096;&#25235;&#20303;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#25805;&#20316;&#20013;&#30340;&#21487;&#29992;&#24615;&#12290;&#26412;&#30740;&#31350;&#38598;&#20013;&#20110;&#24320;&#21457;&#35299;&#37322;&#24615;&#20998;&#31867;&#22120;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28369;&#21160;&#31383;&#21475;&#30340;&#23376;&#38388;&#29305;&#24449;&#25490;&#24207;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few decades, many applications of physics-based simulations and data-driven techniques (including machine learning and deep learning) have emerged to analyze and predict solar flares. These approaches are pivotal in understanding the dynamics of solar flares, primarily aiming to forecast these events and minimize potential risks they may pose to Earth. Although current methods have made significant progress, there are still limitations to these data-driven approaches. One prominent drawback is the lack of consideration for the temporal evolution characteristics in the active regions from which these flares originate. This oversight hinders the ability of these methods to grasp the relationships between high-dimensional active region features, thereby limiting their usability in operations. This study centers on the development of interpretable classifiers for multivariate time series and the demonstration of a novel feature ranking method with sliding window-based sub-int
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#20449;&#24687;&#32534;&#30721;&#65292;&#24182;&#21457;&#29616;&#34920;&#31034;&#29109;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#24130;&#24459;&#20851;&#31995;&#12290;&#36890;&#36807;&#20449;&#24687;&#29702;&#35770;&#21644;&#22238;&#24402;&#25216;&#26415;&#65292;&#24314;&#31435;&#20102;&#26032;&#26631;&#35760;&#30340;&#20449;&#24687;&#22686;&#30410;&#19982;&#23725;&#22238;&#24402;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#65292;&#24182;&#25506;&#32034;&#20102;Lasso&#22238;&#24402;&#22312;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#26631;&#35760;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20449;&#24687;&#22312;&#26631;&#35760;&#20043;&#38388;&#20998;&#24067;&#65292;&#32780;&#19981;&#20165;&#20165;&#38598;&#20013;&#22312;&#29305;&#23450;&#30340;&#8220;&#26377;&#24847;&#20041;&#8221;&#30340;&#26631;&#35760;&#19978;&#12290;</title><link>https://arxiv.org/abs/2402.03471</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20960;&#20309;&#20449;&#24687;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Information of Large Language Model Geometry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03471
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#20449;&#24687;&#32534;&#30721;&#65292;&#24182;&#21457;&#29616;&#34920;&#31034;&#29109;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#24130;&#24459;&#20851;&#31995;&#12290;&#36890;&#36807;&#20449;&#24687;&#29702;&#35770;&#21644;&#22238;&#24402;&#25216;&#26415;&#65292;&#24314;&#31435;&#20102;&#26032;&#26631;&#35760;&#30340;&#20449;&#24687;&#22686;&#30410;&#19982;&#23725;&#22238;&#24402;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#65292;&#24182;&#25506;&#32034;&#20102;Lasso&#22238;&#24402;&#22312;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#26631;&#35760;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20449;&#24687;&#22312;&#26631;&#35760;&#20043;&#38388;&#20998;&#24067;&#65292;&#32780;&#19981;&#20165;&#20165;&#38598;&#20013;&#22312;&#29305;&#23450;&#30340;&#8220;&#26377;&#24847;&#20041;&#8221;&#30340;&#26631;&#35760;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#20449;&#24687;&#32534;&#30721;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#34920;&#31034;&#29109;&#65292;&#24182;&#21457;&#29616;&#20102;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#24130;&#24459;&#20851;&#31995;&#30340;&#29616;&#35937;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#65288;&#26465;&#20214;&#65289;&#29109;&#30340;&#29702;&#35770;&#26469;&#35299;&#37322;&#36825;&#31181;&#35268;&#27169;&#23450;&#24459;&#29616;&#35937;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;LLMs&#30340;&#33258;&#22238;&#24402;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#20449;&#24687;&#29702;&#35770;&#21644;&#22238;&#24402;&#25216;&#26415;&#26469;&#20998;&#26512;&#26368;&#21518;&#19968;&#20010;&#26631;&#35760;&#19982;&#20043;&#21069;&#19978;&#19979;&#25991;&#26631;&#35760;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#26032;&#26631;&#35760;&#30340;&#20449;&#24687;&#22686;&#30410;&#19982;&#23725;&#22238;&#24402;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;Lasso&#22238;&#24402;&#22312;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#26631;&#35760;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#26377;&#26102;&#34920;&#29616;&#20248;&#20110;&#32039;&#23494;&#30456;&#20851;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;&#27604;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#20449;&#24687;&#20998;&#24067;&#22312;&#26631;&#35760;&#20043;&#38388;&#65292;&#32780;&#19981;&#20165;&#20165;&#38598;&#20013;&#22312;&#29305;&#23450;&#30340;&#8220;&#26377;&#24847;&#20041;&#8221;&#30340;&#26631;&#35760;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the information encoded in the embeddings of large language models (LLMs). We conduct simulations to analyze the representation entropy and discover a power law relationship with model sizes. Building upon this observation, we propose a theory based on (conditional) entropy to elucidate the scaling law phenomenon. Furthermore, we delve into the auto-regressive structure of LLMs and examine the relationship between the last token and previous context tokens using information theory and regression techniques. Specifically, we establish a theoretical connection between the information gain of new tokens and ridge regression. Additionally, we explore the effectiveness of Lasso regression in selecting meaningful tokens, which sometimes outperforms the closely related attention weights. Finally, we conduct controlled experiments, and find that information is distributed across tokens, rather than being concentrated in specific "meaningful" tokens alone.
&lt;/p&gt;</description></item><item><title>&#26080;&#20559;&#22909;&#23545;&#40784;&#23398;&#20064;&#20351;&#29992;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;&#20316;&#20026;&#20851;&#38190;&#30446;&#26631;&#65292;&#22312;&#25552;&#20379;&#31283;&#20581;&#22870;&#21169;&#20449;&#21495;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20559;&#22909;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03469</link><description>&lt;p&gt;
&#26080;&#20559;&#22909;&#23545;&#40784;&#23398;&#20064;&#19982;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Preference-free Alignment Learning with Regularized Relevance Reward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03469
&lt;/p&gt;
&lt;p&gt;
&#26080;&#20559;&#22909;&#23545;&#40784;&#23398;&#20064;&#20351;&#29992;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;&#20316;&#20026;&#20851;&#38190;&#30446;&#26631;&#65292;&#22312;&#25552;&#20379;&#31283;&#20581;&#22870;&#21169;&#20449;&#21495;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20559;&#22909;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#34987;&#35748;&#20026;&#26159;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#19982;&#26222;&#36941;&#30340;&#35266;&#28857;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#20542;&#21521;&#20110;&#32473;&#38271;&#30340;&#19982;&#20027;&#39064;&#26080;&#20851;&#30340;&#22238;&#22797;&#26356;&#39640;&#30340;&#20998;&#25968;&#65292;&#32780;&#32473;&#30701;&#30340;&#19982;&#20027;&#39064;&#30456;&#20851;&#30340;&#22238;&#22797;&#36739;&#20302;&#20998;&#12290;&#22312;&#36825;&#19968;&#35266;&#23519;&#30340;&#39537;&#21160;&#19979;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26080;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#8220;&#30456;&#20851;&#24615;&#8221;&#20316;&#20026;&#23545;&#40784;&#30340;&#19968;&#20010;&#20851;&#38190;&#30446;&#26631;&#12290;&#22312;&#25105;&#20204;&#30340;&#31532;&#19968;&#27425;&#23581;&#35797;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#20165;&#36890;&#36807;&#26816;&#32034;&#24471;&#21040;&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#23481;&#26131;&#21463;&#21040;&#22870;&#21169;&#27450;&#39575;&#30340;&#24433;&#21709;&#65292;&#21363;&#36807;&#24230;&#20248;&#21270;&#21040;&#19981;&#26399;&#26395;&#30340;&#25463;&#24452;&#19978;&#65292;&#24403;&#25105;&#20204;&#23558;&#35813;&#24471;&#20998;&#20316;&#20026;&#22870;&#21169;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26377;&#25928;&#30340;&#24402;&#32435;&#20559;&#24046;&#25972;&#21512;&#21040;&#24120;&#35268;&#30340;&#30456;&#20851;&#24615;&#20013;&#65292;&#20114;&#30456;&#27491;&#21017;&#21270;&#65292;&#24418;&#25104;&#20102;&#19968;&#31181;&#28151;&#21512;&#22870;&#21169;&#20989;&#25968;&#65306;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;&#65288;$R^3$&#65289;&#12290;$R^3$&#36890;&#36807;&#25552;&#20379;&#31283;&#20581;&#30340;&#22870;&#21169;&#20449;&#21495;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#20559;&#22909;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;$R^3$&#19981;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
Learning from human preference has been considered key to aligning Large Language Models (LLMs) with human values. However, contrary to popular belief, our preliminary study reveals that reward models trained on human preference datasets tend to give higher scores to long off-topic responses than short on-topic ones. Motivated by this observation, we explore a preference-free approach utilizing `relevance' as a key objective for alignment. On our first attempt, we find that the relevance score obtained by a retriever alone is vulnerable to reward hacking, i.e., overoptimizing to undesired shortcuts, when we utilize the score as a reward for reinforcement learning. To mitigate it, we integrate effective inductive biases into the vanilla relevance to regularize each other, resulting in a mixture of reward functions: Regularized Relevance Reward ($R^3$). $R^3$ significantly improves performance on preference benchmarks by providing a robust reward signal. Notably, $R^3$ does not require a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20219;&#24847;&#32447;&#24615;&#21464;&#25442;&#36827;&#34892;&#31934;&#30830;&#24352;&#37327;&#34917;&#20840;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;-&#24352;&#37327;&#20056;&#31215;&#21644;&#24352;&#37327;&#26680;&#33539;&#25968;&#23450;&#20041;&#65292;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#24182;&#24471;&#21040;&#29702;&#35770;&#30028;&#38480;&#12290;&#36825;&#20123;&#24037;&#20316;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#24352;&#37327;&#34917;&#20840;&#30340;&#28789;&#27963;&#24615;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03468</link><description>&lt;p&gt;
&#30001;&#20219;&#24847;&#32447;&#24615;&#21464;&#25442;&#39537;&#21160;&#30340;&#31934;&#30830;&#24352;&#37327;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Exact Tensor Completion Powered by Arbitrary Linear Transforms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20219;&#24847;&#32447;&#24615;&#21464;&#25442;&#36827;&#34892;&#31934;&#30830;&#24352;&#37327;&#34917;&#20840;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;-&#24352;&#37327;&#20056;&#31215;&#21644;&#24352;&#37327;&#26680;&#33539;&#25968;&#23450;&#20041;&#65292;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#24182;&#24471;&#21040;&#29702;&#35770;&#30028;&#38480;&#12290;&#36825;&#20123;&#24037;&#20316;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#24352;&#37327;&#34917;&#20840;&#30340;&#28789;&#27963;&#24615;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#65292;&#26088;&#22312;&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#23436;&#32654;&#24674;&#22797;&#24352;&#37327;&#12290;&#29616;&#26377;&#30340;&#29702;&#35770;&#20445;&#35777;&#35201;&#27714;&#28041;&#21450;&#30340;&#21464;&#25442;&#26159;&#27491;&#20132;&#30340;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36339;&#20986;&#20102;&#21508;&#21521;&#21516;&#24615;&#25110;&#33258;&#20276;&#30340;&#32422;&#26463;&#65292;&#24314;&#31435;&#20102;&#20351;&#29992;&#20219;&#24847;&#32447;&#24615;&#21464;&#25442;&#36827;&#34892;&#31934;&#30830;&#24352;&#37327;&#34917;&#20840;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;-&#24352;&#37327;&#20056;&#31215;&#65292;&#23548;&#33268;&#20102;&#24352;&#37327;&#26680;&#33539;&#25968;&#30340;&#26032;&#23450;&#20041;&#12290;&#37197;&#22791;&#20102;&#36825;&#20123;&#24037;&#20855;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#25968;&#27861;&#30340;&#39640;&#25928;&#31639;&#27861;&#26469;&#35299;&#20915;&#21464;&#25442;&#21518;&#30340;&#24352;&#37327;&#34917;&#20840;&#38382;&#39064;&#65292;&#24182;&#24471;&#21040;&#29702;&#35770;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21644;&#35777;&#26126;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#24352;&#37327;&#34917;&#20840;&#30340;&#28789;&#27963;&#24615;&#65292;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, a tensor completion problem is studied, which aims to perfectly recover the tensor from partial observations. Existing theoretical guarantee requires the involved transform to be orthogonal, which hinders its applications. In this paper, jumping out of the constraints of isotropy or self-adjointness, the theoretical guarantee of exact tensor completion with arbitrary linear transforms is established. To that end, we define a new tensor-tensor product, which leads us to a new definition of the tensor nuclear norm. Equipped with these tools, an efficient algorithm based on alternating direction of multipliers is designed to solve the transformed tensor completion program and the theoretical bound is obtained. Our model and proof greatly enhance the flexibility of tensor completion and extensive experiments validate the superiority of the proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#40654;&#26364;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;RSGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#20462;&#25913;&#27969;&#65288;RSMF&#65289;&#30340;&#25193;&#25955;&#36924;&#36817;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23545;RSGD&#30340;&#36817;&#20284;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.03467</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#20462;&#25913;&#27969;&#30340;&#40654;&#26364;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Stochastic Modified Flows for Riemannian Stochastic Gradient Descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#40654;&#26364;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;RSGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#20462;&#25913;&#27969;&#65288;RSMF&#65289;&#30340;&#25193;&#25955;&#36924;&#36817;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23545;RSGD&#30340;&#36817;&#20284;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#40654;&#26364;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;RSGD&#65289;&#25910;&#25947;&#36895;&#24230;&#32473;&#20986;&#20102;&#23450;&#37327;&#20272;&#35745;&#65292;&#24182;&#23558;&#20854;&#19982;&#40654;&#26364;&#26799;&#24230;&#27969;&#21644;&#25193;&#25955;&#36807;&#31243;&#8212;&#8212;&#40654;&#26364;&#38543;&#26426;&#20462;&#25913;&#27969;&#65288;RSMF&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#21033;&#29992;&#38543;&#26426;&#24494;&#20998;&#20960;&#20309;&#24037;&#20855;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#23567;&#23398;&#20064;&#29575;&#33539;&#22260;&#20869;&#65292;RSGD&#21487;&#20197;&#36817;&#20284;&#20026;&#30001;&#26080;&#31351;&#32500;&#32500;&#32435;&#36807;&#31243;&#39537;&#21160;&#30340;RSMF&#30340;&#35299;&#12290;RSMF&#32771;&#34385;&#21040;&#20102;RSGD&#30340;&#38543;&#26426;&#27874;&#21160;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#19982;&#30830;&#23450;&#24615;&#40654;&#26364;&#26799;&#24230;&#27969;&#30340;&#36924;&#36817;&#39034;&#24207;&#12290;RSGD&#20351;&#29992;&#20102;&#37325;&#20256;&#36882;&#26144;&#23556;&#30340;&#27010;&#24565;&#65292;&#21363;&#23545;&#25351;&#25968;&#26144;&#23556;&#30340;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#36817;&#20284;&#65292;&#25105;&#20204;&#23545;&#25193;&#25955;&#36924;&#36817;&#30340;&#24369;&#35823;&#24046;&#36827;&#34892;&#20102;&#23450;&#37327;&#30028;&#23450;&#65292;&#22312;&#37325;&#20256;&#36882;&#26144;&#23556;&#12289;&#27969;&#24418;&#20960;&#20309;&#21644;&#26799;&#24230;&#30340;&#38543;&#26426;&#20272;&#35745;&#30340;&#20551;&#35774;&#19979;&#35777;&#26126;&#20102;&#36825;&#20123;&#30028;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give quantitative estimates for the rate of convergence of Riemannian stochastic gradient descent (RSGD) to Riemannian gradient flow and to a diffusion process, the so-called Riemannian stochastic modified flow (RSMF). Using tools from stochastic differential geometry we show that, in the small learning rate regime, RSGD can be approximated by the solution to the RSMF driven by an infinite-dimensional Wiener process. The RSMF accounts for the random fluctuations of RSGD and, thereby, increases the order of approximation compared to the deterministic Riemannian gradient flow. The RSGD is build using the concept of a retraction map, that is, a cost efficient approximation of the exponential map, and we prove quantitative bounds for the weak error of the diffusion approximation under assumptions on the retraction map, the geometry of the manifold, and the random estimators of the gradient.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#24067;&#24335;&#31070;&#32463;&#35745;&#31639;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#26469;&#20811;&#26381;&#32500;&#24230;&#28798;&#38590;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#20219;&#24847;&#31934;&#24230;&#19979;&#36924;&#36817;Lipschitz&#20989;&#25968;&#65292;&#22312;&#21442;&#25968;&#37327;&#21644;&#21069;&#21521;&#20256;&#25773;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.03460</link><description>&lt;p&gt;
&#29992;&#20998;&#24067;&#24335;&#31070;&#32463;&#35745;&#31639;&#31361;&#30772;&#32500;&#24230;&#28798;&#38590;
&lt;/p&gt;
&lt;p&gt;
Breaking the Curse of Dimensionality with Distributed Neural Computation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03460
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#24067;&#24335;&#31070;&#32463;&#35745;&#31639;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#26469;&#20811;&#26381;&#32500;&#24230;&#28798;&#38590;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#20219;&#24847;&#31934;&#24230;&#19979;&#36924;&#36817;Lipschitz&#20989;&#25968;&#65292;&#22312;&#21442;&#25968;&#37327;&#21644;&#21069;&#21521;&#20256;&#25773;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#31070;&#32463;&#35745;&#31639;&#31639;&#27861;&#26469;&#20811;&#26381;&#32500;&#24230;&#28798;&#38590;&#12290;&#25105;&#20204;&#30340;&#27169;&#22359;&#21270;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#65292;&#31216;&#20026;&#8220;&#31070;&#32463;&#36884;&#24452;&#8221;&#65292;&#21487;&#20197;&#22312;&#22810;&#21488;&#26426;&#22120;&#19978;&#23454;&#29616;&#20219;&#24847;&#31934;&#24230;&#65292;&#21516;&#26102;&#20165;&#21152;&#36733;&#23569;&#37327;&#21442;&#25968;&#21040;GPU VRAM&#20013;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#27599;&#20010;&#35823;&#24046;&#27700;&#24179;$\varepsilon&gt;0$&#21644;&#27599;&#20010;Lipschitz&#20989;&#25968;$f:[0,1]^n\to \mathbb{R}$&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#31070;&#32463;&#36884;&#24452;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;$[0,1]^n$&#19978;&#20197;$\varepsilon$&#31934;&#24230;&#22343;&#21248;&#36924;&#36817;$f$&#65292;&#24182;&#19988;&#20165;&#38656;&#35201;&#22312;&#20869;&#23384;&#20013;&#21152;&#36733;$\mathcal{O}(\varepsilon^{-1})$&#20010;&#32593;&#32476;&#21442;&#25968;&#20197;&#21450;&#22312;&#21069;&#21521;&#20256;&#25773;&#26399;&#38388;&#21152;&#36733;$\mathcal{O}(\varepsilon^{-1}\log(\varepsilon^{-1}))$&#20010;&#32593;&#32476;&#21442;&#25968;&#12290;&#36825;&#25913;&#36827;&#20102;&#20256;&#32479;&#38750;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#21363;ReLU&#22810;&#23618;&#24863;&#30693;&#26426;&#65289;&#30340;&#26368;&#20248;&#30028;&#38480;&#65292;&#21518;&#32773;&#38656;&#35201;$\mathcal{O}(\varepsilon^{-n/2})$&#20010;&#21442;&#25968;&#26469;&#36798;&#21040;&#30456;&#21516;&#30340;&#31934;&#24230;&#12290;&#30446;&#21069;&#21807;&#19968;&#30340;&#20854;&#20182;&#21487;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
We present a theoretical approach to overcome the curse of dimensionality using a neural computation algorithm which can be distributed across several machines. Our modular distributed deep learning paradigm, termed \textit{neural pathways}, can achieve arbitrary accuracy while only loading a small number of parameters into GPU VRAM. Formally, we prove that for every error level $\varepsilon&gt;0$ and every Lipschitz function $f:[0,1]^n\to \mathbb{R}$, one can construct a neural pathways model which uniformly approximates $f$ to $\varepsilon$ accuracy over $[0,1]^n$ while only requiring networks of $\mathcal{O}(\varepsilon^{-1})$ parameters to be loaded in memory and $\mathcal{O}(\varepsilon^{-1}\log(\varepsilon^{-1}))$ to be loaded during the forward pass. This improves the optimal bounds for traditional non-distributed deep learning models, namely ReLU MLPs, which need $\mathcal{O}(\varepsilon^{-n/2})$ parameters to achieve the same accuracy. The only other available deep learning model
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#20132;&#36890;&#30446;&#30340;&#22320;&#39044;&#27979;&#27169;&#22411;&#65288;EBM&#65289;&#65292;&#22312;&#22810;&#20010;&#28151;&#21512;&#20132;&#36890;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#25552;&#20379;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#20132;&#20114;&#20316;&#29992;&#30340;&#20998;&#26512;&#20197;&#21450;&#39044;&#27979;&#35299;&#37322;&#30340;&#36136;&#37327;&#31034;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.03457</link><description>&lt;p&gt;
&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#20132;&#36890;&#30446;&#30340;&#22320;&#39044;&#27979;&#26041;&#27861;&#8212;&#8212;&#21487;&#35299;&#37322;&#25311;&#21512;&#26426;&#22120;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient and Interpretable Traffic Destination Prediction using Explainable Boosting Machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#20132;&#36890;&#30446;&#30340;&#22320;&#39044;&#27979;&#27169;&#22411;&#65288;EBM&#65289;&#65292;&#22312;&#22810;&#20010;&#28151;&#21512;&#20132;&#36890;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#33021;&#22815;&#25552;&#20379;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#20132;&#20114;&#20316;&#29992;&#30340;&#20998;&#26512;&#20197;&#21450;&#39044;&#27979;&#35299;&#37322;&#30340;&#36136;&#37327;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#31934;&#30830;&#30340;&#20132;&#36890;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#23545;&#20110;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#39550;&#39542;&#33267;&#20851;&#37325;&#35201;&#12290;&#21508;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24050;&#34987;&#24212;&#29992;&#20110;&#35299;&#20915;&#27492;&#25361;&#25112;&#65292;&#20294;&#23427;&#20204;&#30340;&#40657;&#30418;&#29305;&#24615;&#38480;&#21046;&#20102;&#22312;&#37096;&#32626;&#31995;&#32479;&#20013;&#30340;&#36879;&#26126;&#24230;&#21644;&#35843;&#35797;&#33021;&#21147;&#12290;&#29627;&#29827;&#30418;&#27169;&#22411;&#36890;&#36807;&#31867;&#20284;&#20110;GAM&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21487;&#21152;&#24615;&#27169;&#22411;&#65292;&#31216;&#20026;EBM&#65292;&#29992;&#20110;&#19977;&#20010;&#27969;&#34892;&#30340;&#28151;&#21512;&#20132;&#36890;&#25968;&#25454;&#38598;&#65288;SDD&#12289;InD&#21644;Argoverse&#65289;&#30340;&#20132;&#36890;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;EBM&#27169;&#22411;&#22312;&#39044;&#27979;SDD&#21644;InD&#20013;&#30340;&#34892;&#20154;&#30446;&#30340;&#22320;&#26102;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#21516;&#26102;&#23545;&#20197;&#36710;&#36742;&#20026;&#20027;&#30340;Argoverse&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#36866;&#24230;&#30340;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36879;&#26126;&#30340;&#35757;&#32451;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#30340;&#36136;&#37327;&#31034;&#20363;&#12290;&#20840;&#38754;&#30340;&#35757;&#32451;&#20195;&#30721;&#23558;&#22312;&#35770;&#25991;&#21457;&#34920;&#21518;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing accurate models for traffic trajectory predictions is crucial for achieving fully autonomous driving. Various deep neural network models have been employed to address this challenge, but their black-box nature hinders transparency and debugging capabilities in a deployed system. Glass-box models offer a solution by providing full interpretability through methods like \ac{GAM}. In this study, we evaluate an efficient additive model called \ac{EBM} for traffic prediction on three popular mixed traffic datasets: \ac{SDD}, \ac{InD}, and Argoverse. Our results show that the \ac{EBM} models perform competitively in predicting pedestrian destinations within \ac{SDD} and \ac{InD} while providing modest predictions for vehicle-dominant Argoverse dataset. Additionally, our transparent trained models allow us to analyse feature importance and interactions, as well as provide qualitative examples of predictions explanation. The full training code will be made public upon publication.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#25955;&#24335;&#38388;&#27463;&#32852;&#37030;&#23398;&#20064;&#65288;DSpodFL&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#32479;&#19968;&#20102;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;DGD&#65289;&#12289;&#38543;&#26426;&#38386;&#35805;&#65288;RG&#65289;&#21644;&#20998;&#25955;&#24335;&#32852;&#37030;&#24179;&#22343;&#65288;DFedAvg&#65289;&#31561;&#33879;&#21517;&#30340;&#20998;&#25955;&#20248;&#21270;&#26041;&#27861;&#12290;&#26681;&#25454;&#20998;&#26512;&#32467;&#26524;&#65292;DSpodFL&#33021;&#22815;&#22312;&#26356;&#19968;&#33324;&#30340;&#20551;&#35774;&#19979;&#36798;&#21040;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#19982;&#26368;&#20339;&#24615;&#24046;&#36317;&#30340;&#21305;&#37197;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03448</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;&#38388;&#27463;&#32852;&#37030;&#23398;&#20064;&#65306;&#20855;&#26377;&#24191;&#20041;&#25910;&#25947;&#20445;&#35777;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decentralized Sporadic Federated Learning: A Unified Methodology with Generalized Convergence Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#25955;&#24335;&#38388;&#27463;&#32852;&#37030;&#23398;&#20064;&#65288;DSpodFL&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#32479;&#19968;&#20102;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;DGD&#65289;&#12289;&#38543;&#26426;&#38386;&#35805;&#65288;RG&#65289;&#21644;&#20998;&#25955;&#24335;&#32852;&#37030;&#24179;&#22343;&#65288;DFedAvg&#65289;&#31561;&#33879;&#21517;&#30340;&#20998;&#25955;&#20248;&#21270;&#26041;&#27861;&#12290;&#26681;&#25454;&#20998;&#26512;&#32467;&#26524;&#65292;DSpodFL&#33021;&#22815;&#22312;&#26356;&#19968;&#33324;&#30340;&#20551;&#35774;&#19979;&#36798;&#21040;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#19982;&#26368;&#20339;&#24615;&#24046;&#36317;&#30340;&#21305;&#37197;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#36817;&#26469;&#21463;&#21040;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#28085;&#30422;&#20102;&#27169;&#22411;&#26356;&#26032;&#21644;&#27169;&#22411;&#32858;&#21512;&#36825;&#20004;&#20010;&#20851;&#38190;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#37117;&#30001;&#23458;&#25143;&#31471;&#36827;&#34892;&#30340;&#35774;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#25955;&#24335;&#38388;&#27463;&#32852;&#37030;&#23398;&#20064;&#65288;DSpodFL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;DFL&#26041;&#27861;&#65292;&#23427;&#22312;&#36825;&#20004;&#20010;&#36807;&#31243;&#20013;&#24191;&#20041;&#21270;&#20102;&#38388;&#27463;&#24615;&#30340;&#27010;&#24565;&#65292;&#24314;&#27169;&#20102;&#22312;&#23454;&#38469;DFL&#35774;&#32622;&#20013;&#20986;&#29616;&#30340;&#19981;&#21516;&#24418;&#24335;&#30340;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;DSpodFL&#23558;&#35768;&#22810;&#30528;&#21517;&#30340;&#20998;&#25955;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;DGD&#65289;&#65292;&#38543;&#26426;&#38386;&#35805;&#65288;RG&#65289;&#21644;&#20998;&#25955;&#24335;&#32852;&#37030;&#24179;&#22343;&#65288;DFedAvg&#65289;&#65292;&#32479;&#19968;&#21040;&#19968;&#20010;&#24314;&#27169;&#26694;&#26550;&#19979;&#12290;&#25105;&#20204;&#23545;DSpodFL&#30340;&#25910;&#25947;&#34892;&#20026;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#26174;&#31034;&#20986;&#21487;&#20197;&#22312;&#26356;&#19968;&#33324;&#30340;&#20551;&#35774;&#19979;&#65292;&#23558;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#19982;&#26377;&#38480;&#30340;&#26368;&#20339;&#24615;&#24046;&#36317;&#30456;&#21305;&#37197;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65306;
&lt;/p&gt;
&lt;p&gt;
Decentralized Federated Learning (DFL) has received significant recent research attention, capturing settings where both model updates and model aggregations -- the two key FL processes -- are conducted by the clients. In this work, we propose Decentralized Sporadic Federated Learning ($\texttt{DSpodFL}$), a DFL methodology which generalizes the notion of sporadicity in both of these processes, modeling the impact of different forms of heterogeneity that manifest in realistic DFL settings. $\texttt{DSpodFL}$ unifies many of the prominent decentralized optimization methods, e.g., distributed gradient descent (DGD), randomized gossip (RG), and decentralized federated averaging (DFedAvg), under a single modeling framework. We analytically characterize the convergence behavior of $\texttt{DSpodFL}$, showing, among other insights, that we can match a geometric convergence rate to a finite optimality gap under more general assumptions than in existing works. Through experiments, we demonstra
&lt;/p&gt;</description></item><item><title>&#21464;&#37327;&#37325;&#35201;&#24615;&#25490;&#24207;&#22312;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#26159;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;&#29305;&#24449; Knockoffs &#30340;&#25913;&#36827;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#37325;&#28857;&#26159;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#21644;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.03447</link><description>&lt;p&gt;
&#21464;&#37327;&#37325;&#35201;&#24615;&#25490;&#24207;&#22312;&#30456;&#20851;&#24615;&#19979;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Challenges in Variable Importance Ranking Under Correlation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03447
&lt;/p&gt;
&lt;p&gt;
&#21464;&#37327;&#37325;&#35201;&#24615;&#25490;&#24207;&#22312;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#26159;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;&#29305;&#24449; Knockoffs &#30340;&#25913;&#36827;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#37325;&#28857;&#26159;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#37327;&#37325;&#35201;&#24615;&#22312;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#23427;&#24110;&#21161;&#34913;&#37327;&#22240;&#32032;&#23545;&#39044;&#27979;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#36890;&#36807;&#25490;&#21015;&#29983;&#25104;&#8220;&#31354;&#8221;&#29305;&#24449;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#27492;&#12290;&#36825;&#31181;&#20998;&#26512;&#36890;&#24120;&#22312;&#21046;&#33647;&#24212;&#29992;&#20013;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#33021;&#35299;&#37322;&#21253;&#25324;&#22522;&#20110;&#26641;&#30340;&#38598;&#25104;&#27169;&#22411;&#22312;&#20869;&#30340;&#40657;&#30418;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21464;&#37327;&#37325;&#35201;&#24615;&#20272;&#31639;&#20013;&#23384;&#22312;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#21644;&#26174;&#33879;&#24178;&#25200;&#22240;&#32032;&#26159;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#26368;&#36817;&#65292;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#20960;&#31181;&#22522;&#20110;&#29305;&#24449; Knockoffs &#30340;&#36793;&#38469;&#25490;&#21015;&#35843;&#25972;&#65292;&#22914;&#26465;&#20214;&#39044;&#27979;&#24433;&#21709;&#65288;CPI&#65289;&#31561;&#21464;&#37327;&#37325;&#35201;&#24615;&#24230;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#37325;&#28857;&#26159;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#39033;&#32508;&#21512;&#27169;&#25311;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#29305;&#24449;&#30456;&#20851;&#24615;&#23545;&#21464;&#37327;&#37325;&#35201;&#24615;&#35780;&#20272;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Variable importance plays a pivotal role in interpretable machine learning as it helps measure the impact of factors on the output of the prediction model. Model agnostic methods based on the generation of "null" features via permutation (or related approaches) can be applied. Such analysis is often utilized in pharmaceutical applications due to its ability to interpret black-box models, including tree-based ensembles. A major challenge and significant confounder in variable importance estimation however is the presence of between-feature correlation. Recently, several adjustments to marginal permutation utilizing feature knockoffs were proposed to address this issue, such as the variable importance measure known as conditional predictive impact (CPI). Assessment and evaluation of such approaches is the focus of our work. We first present a comprehensive simulation study investigating the impact of feature correlation on the assessment of variable importance. We then theoretically prov
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#23616;&#37096;&#20307;&#31215;&#26657;&#27491;&#21644;&#38750;&#20405;&#20837;&#24615;&#21160;&#33033;&#37319;&#26679;&#27169;&#22411;&#65292;&#33258;&#21160;&#25512;&#23548;&#34880;&#28082;&#36755;&#20837;&#20989;&#25968;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#21160;&#24577;FDG-PET&#23450;&#37327;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03414</link><description>&lt;p&gt;
&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#29992;&#20110;&#34880;&#28082;&#36755;&#20837;&#30340;&#35745;&#31639;&#65292;&#21253;&#25324;&#23616;&#37096;&#20307;&#31215;&#26657;&#27491;&#20197;&#20415;&#36827;&#34892;&#33258;&#21160;&#21270;&#30340;&#21442;&#25968;&#21270;&#33041;PET&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
An end-to-end deep learning pipeline to derive blood input with partial volume corrections for automated parametric brain PET mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03414
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;&#23616;&#37096;&#20307;&#31215;&#26657;&#27491;&#21644;&#38750;&#20405;&#20837;&#24615;&#21160;&#33033;&#37319;&#26679;&#27169;&#22411;&#65292;&#33258;&#21160;&#25512;&#23548;&#34880;&#28082;&#36755;&#20837;&#20989;&#25968;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#21160;&#24577;FDG-PET&#23450;&#37327;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;2-[18F]&#27679;-2-&#33073;&#27687;-D-&#33889;&#33796;&#31958;&#27491;&#30005;&#23376;&#21457;&#23556;&#26029;&#23618;&#25195;&#25551;&#65288;dFDG-PET&#65289;&#23545;&#20154;&#31867;&#33041;&#37096;&#25104;&#20687;&#20855;&#26377;&#30456;&#24403;&#22823;&#30340;&#20020;&#24202;&#28508;&#21147;&#65292;&#20294;&#20854;&#21033;&#29992;&#20173;&#21463;&#38480;&#21046;&#12290;&#22312;&#23545;dFDG-PET&#30340;&#23450;&#37327;&#20998;&#26512;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#34920;&#24449;&#29305;&#23450;&#20110;&#24739;&#32773;&#30340;&#34880;&#28082;&#36755;&#20837;&#21151;&#33021;&#65292;&#20256;&#32479;&#19978;&#20381;&#36182;&#20405;&#20837;&#24615;&#21160;&#33033;&#37319;&#34880;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#38750;&#20405;&#20837;&#24615;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35745;&#31639;&#26469;&#33258;&#39048;&#20869;&#21160;&#33033;&#30340;&#23616;&#37096;&#20307;&#31215;&#65288;PV&#65289;&#26657;&#27491;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#20405;&#20837;&#24615;&#21160;&#33033;&#37319;&#34880;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#27969;&#31243;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;3D U-Net&#30340;&#39048;&#20869;&#21160;&#33033;&#65288;ICA&#65289;&#20998;&#21106;&#21644;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;MCIF-net&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;PV&#26657;&#27491;&#30340;&#27169;&#22411;&#20462;&#27491;&#34880;&#28082;&#36755;&#20837;&#20989;&#25968;&#65288;MCIF&#65289;&#12290;&#25152;&#24320;&#21457;&#30340;3D U-Net&#21644;RNN&#26159;&#20351;&#29992;5&#20493;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#22312;50&#20010;&#20154;&#31867;&#33041;FDG PET&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#30340;&#12290;ICA-net&#22312;&#24179;&#22343;Dice&#31995;&#25968;&#19978;&#36798;&#21040;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Dynamic 2-[18F] fluoro-2-deoxy-D-glucose positron emission tomography (dFDG-PET) for human brain imaging has considerable clinical potential, yet its utilization remains limited. A key challenge in the quantitative analysis of dFDG-PET is characterizing a patient-specific blood input function, traditionally reliant on invasive arterial blood sampling. This research introduces a novel approach employing non-invasive deep learning model-based computations from the internal carotid arteries (ICA) with partial volume (PV) corrections, thereby eliminating the need for invasive arterial sampling. We present an end-to-end pipeline incorporating a 3D U-Net based ICA-net for ICA segmentation, alongside a Recurrent Neural Network (RNN) based MCIF-net for the derivation of a model-corrected blood input function (MCIF) with PV corrections. The developed 3D U-Net and RNN was trained and validated using a 5-fold cross-validation approach on 50 human brain FDG PET datasets. The ICA-net achieved an av
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#30417;&#30563;&#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21161;&#36716;&#25442;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#21487;&#20197;&#22686;&#24378;LLM&#22522;&#30784;&#30340;&#35821;&#38899;&#29983;&#25104;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#20811;&#26381;&#20102;&#22312;&#25512;&#29702;&#26102;&#20986;&#29616;&#30340;&#22810;&#20010;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;LLM&#21487;&#20197;&#20174;&#25991;&#26412;&#20013;&#20165;&#29983;&#25104;&#35821;&#38899;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#65292;&#32780;&#35828;&#35805;&#32773;&#36523;&#20221;&#30001;&#21478;&#19968;&#20010;&#27169;&#22411;&#25552;&#20379;&#12290;</title><link>https://arxiv.org/abs/2402.03407</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#34920;&#31034;&#22686;&#24378;LLM&#22522;&#30784;&#30340;&#35821;&#38899;&#29983;&#25104;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03407
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21161;&#36716;&#25442;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#21487;&#20197;&#22686;&#24378;LLM&#22522;&#30784;&#30340;&#35821;&#38899;&#29983;&#25104;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#20811;&#26381;&#20102;&#22312;&#25512;&#29702;&#26102;&#20986;&#29616;&#30340;&#22810;&#20010;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;LLM&#21487;&#20197;&#20174;&#25991;&#26412;&#20013;&#20165;&#29983;&#25104;&#35821;&#38899;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#65292;&#32780;&#35828;&#35805;&#32773;&#36523;&#20221;&#30001;&#21478;&#19968;&#20010;&#27169;&#22411;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#19979;&#19968;&#20195;&#35821;&#38899;&#29983;&#25104;&#31995;&#32479;&#26368;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#29702;&#26102;&#65292;&#23427;&#20204;&#36935;&#21040;&#20102;&#22810;&#20010;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#22914;&#24187;&#35273;&#12289;&#36339;&#36807;&#20869;&#23481;&#25110;&#35821;&#38899;&#37325;&#22797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#36716;&#25442;&#65288;VC&#65289;&#26550;&#26500;&#65292;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#23558;&#30636;&#24577;&#29305;&#24449;&#65288;&#22914;&#20869;&#23481;&#65289;&#19982;&#22266;&#23450;&#29305;&#24449;&#65288;&#22914;&#35828;&#35805;&#32773;ID&#25110;&#24405;&#21046;&#26465;&#20214;&#65289;&#20998;&#24320;&#32534;&#30721;&#65292;&#21019;&#24314;&#35828;&#35805;&#32773;&#35299;&#32806;&#34920;&#31034;&#12290;&#20351;&#29992;&#35828;&#35805;&#32773;&#35299;&#32806;&#32534;&#30721;&#26469;&#35757;&#32451;LLMs&#36827;&#34892;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#20801;&#35768;LLM&#20165;&#36890;&#36807;&#25991;&#26412;&#29983;&#25104;&#35821;&#38899;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#65292;&#32780;&#35828;&#35805;&#32773;&#36523;&#20221;&#30001;VC&#27169;&#22411;&#30340;&#35299;&#30721;&#22120;&#25552;&#20379;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35757;&#32451;&#36807;&#35828;&#35805;&#32773;&#35299;&#32806;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#30340;LLMs&#22312;&#35828;&#35805;&#32773;&#30456;&#20284;&#24615;&#26041;&#38754;&#25552;&#20379;&#20102;4.7pp&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are one of the most promising technologies for the next era of speech generation systems, due to their scalability and in-context learning capabilities. Nevertheless, they suffer from multiple stability issues at inference time, such as hallucinations, content skipping or speech repetitions. In this work, we introduce a new self-supervised Voice Conversion (VC) architecture which can be used to learn to encode transitory features, such as content, separately from stationary ones, such as speaker ID or recording conditions, creating speaker-disentangled representations. Using speaker-disentangled codes to train LLMs for text-to-speech (TTS) allows the LLM to generate the content and the style of the speech only from the text, similarly to humans, while the speaker identity is provided by the decoder of the VC model. Results show that LLMs trained over speaker-disentangled self-supervised representations provide an improvement of 4.7pp in speaker similarity o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#38750;&#32447;&#24615;&#39640;&#20809;&#35889;&#35299;&#28151;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#19982;&#26080;&#29305;&#27530;&#20551;&#35774;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#25552;&#39640;&#35299;&#28151;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03398</link><description>&lt;p&gt;
&#28145;&#24230;&#38750;&#32447;&#24615;&#39640;&#20809;&#35889;&#35299;&#28151;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Nonlinear Hyperspectral Unmixing Using Multi-task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#38750;&#32447;&#24615;&#39640;&#20809;&#35889;&#35299;&#28151;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#19982;&#26080;&#29305;&#27530;&#20551;&#35774;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#25552;&#39640;&#35299;&#28151;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32447;&#24615;&#39640;&#20809;&#35889;&#35299;&#28151;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#32447;&#24615;&#28151;&#21512;&#27169;&#22411;&#22312;&#19968;&#20123;&#38382;&#39064;&#19978;&#26080;&#27861;&#36798;&#21040;&#21487;&#25509;&#21463;&#30340;&#20998;&#36776;&#29575;&#12290;&#20107;&#23454;&#19978;&#65292;&#22823;&#22810;&#25968;&#38750;&#32447;&#24615;&#35299;&#28151;&#26041;&#27861;&#37117;&#26159;&#36890;&#36807;&#20551;&#35774;&#29305;&#23450;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#32780;&#35774;&#35745;&#30340;&#65292;&#36825;&#38480;&#21046;&#20102;&#35299;&#28151;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#38750;&#32447;&#24615;&#35299;&#28151;&#26041;&#27861;&#65292;&#23558;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#19982;&#27809;&#26377;&#29305;&#27530;&#20551;&#35774;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#35813;&#27169;&#22411;&#30001;&#20004;&#20010;&#20998;&#25903;&#32452;&#25104;&#12290;&#22312;&#31532;&#19968;&#20010;&#20998;&#25903;&#20013;&#65292;&#36890;&#36807;&#37325;&#26500;&#39640;&#20809;&#35889;&#22270;&#20687;&#30340;&#34892;&#26469;&#23398;&#20064;&#31471;&#20803;&#65292;&#20351;&#29992;&#19968;&#20123;&#38544;&#34255;&#23618;&#65292;&#28982;&#21518;&#22312;&#31532;&#20108;&#20010;&#20998;&#25903;&#20013;&#65292;&#26681;&#25454;&#30456;&#24212;&#22270;&#20687;&#30340;&#21015;&#26469;&#23398;&#20064;&#20016;&#24230;&#20540;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36741;&#21161;&#20219;&#21153;&#26469;&#20419;&#20351;&#36825;&#20004;&#20010;&#20998;&#25903;&#20849;&#21516;&#24037;&#20316;&#12290;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#27491;&#21017;&#21270;&#22120;&#65292;&#29992;&#20110;&#20943;&#36731;&#36807;&#25311;&#21512;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20010;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonlinear hyperspectral unmixing has recently received considerable attention, as linear mixture models do not lead to an acceptable resolution in some problems. In fact, most nonlinear unmixing methods are designed by assuming specific assumptions on the nonlinearity model which subsequently limits the unmixing performance. In this paper, we propose an unsupervised nonlinear unmixing approach based on deep learning by incorporating a general nonlinear model with no special assumptions. This model consists of two branches. In the first branch, endmembers are learned by reconstructing the rows of hyperspectral images using some hidden layers, and in the second branch, abundance values are learned based on the columns of respective images. Then, using multi-task learning, we introduce an auxiliary task to enforce the two branches to work together. This technique can be considered as a regularizer mitigating overfitting, which improves the performance of the total network. Extensive exper
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;UniTSyn&#65292;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31243;&#24207;&#27979;&#35797;&#20013;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20851;&#32852;&#27979;&#35797;&#21644;&#34987;&#27979;&#35797;&#20989;&#25968;&#65292;UniTSyn&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#22312;&#29983;&#25104;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#27979;&#35797;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.03396</link><description>&lt;p&gt;
UniTSyn&#65306;&#19968;&#20010;&#21487;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31243;&#24207;&#27979;&#35797;&#20013;&#30340;&#33021;&#21147;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large Language Models for Program Testing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;UniTSyn&#65292;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31243;&#24207;&#27979;&#35797;&#20013;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20851;&#32852;&#27979;&#35797;&#21644;&#34987;&#27979;&#35797;&#20989;&#25968;&#65292;UniTSyn&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#22312;&#29983;&#25104;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#27979;&#35797;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#20195;&#30721;&#26041;&#38754;&#30340;&#20986;&#33394;&#33021;&#21147;&#24050;&#32463;&#24341;&#36215;&#20102;&#36719;&#20214;&#27979;&#35797;&#31038;&#21306;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20195;&#30721;LLM&#22312;&#29983;&#25104;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#27979;&#35797;&#26041;&#38754;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#22312;&#19981;&#21306;&#20998;&#27979;&#35797;&#30446;&#30340;&#20195;&#30721;&#21644;&#20854;&#20182;&#20195;&#30721;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;UniTSyn&#65292;&#23427;&#33021;&#22815;&#22686;&#24378;LLM&#22312;&#21333;&#20803;&#27979;&#35797;&#21512;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#23558;&#27979;&#35797;&#19982;&#34987;&#27979;&#35797;&#20989;&#25968;&#36827;&#34892;&#20851;&#32852;&#23545;&#20110;LLM&#25512;&#26029;&#39044;&#26399;&#34892;&#20026;&#21644;&#35201;&#39564;&#35777;&#30340;&#36923;&#36753;&#36335;&#24452;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#26381;&#21153;&#22120;&#21327;&#35758;&#65292;UniTSyn&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#27599;&#20010;&#39033;&#30446;&#25191;&#34892;&#35774;&#32622;&#25110;&#26131;&#30862;&#19988;&#38590;&#20197;&#25193;&#23637;&#30340;&#27599;&#20010;&#35821;&#35328;&#21551;&#21457;&#24335;&#30340;&#24773;&#20917;&#19979;&#25910;&#38598;&#28966;&#28857;&#27979;&#35797;&#23545;&#30340;&#25361;&#25112;&#30446;&#26631;&#12290;&#23427;&#21253;&#21547;&#20102;&#20116;&#31181;&#20027;&#27969;&#32534;&#31243;&#35821;&#35328;&#30340;270&#19975;&#20010;&#28966;&#28857;&#27979;&#35797;&#23545;&#65292;&#20351;&#20854;&#33021;&#22815;&#34987;&#24212;&#29992;&#20110;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable capability of large language models (LLMs) in generating high-quality code has drawn increasing attention in the software testing community. However, existing code LLMs often demonstrate unsatisfactory capabilities in generating accurate and complete tests since they were trained on code snippets collected without differentiating between code for testing purposes and other code. In this paper, we present a large-scale dataset UniTSyn, which is capable of enhancing the prowess of LLMs for Unit Test Synthesis. Associating tests with the tested functions is crucial for LLMs to infer the expected behavior and the logic paths to be verified. By leveraging Language Server Protocol, UniTSyn achieves the challenging goal of collecting focal-test pairs without per-project execution setups or per-language heuristics that tend to be fragile and difficult to scale. It contains 2.7 million focal-test pairs across five mainstream programming languages, making it possible to be utilize
&lt;/p&gt;</description></item><item><title>&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#20256;&#36882;&#21457;&#29616;&#34892;&#20026;&#29992;&#25143;&#32454;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.03388</link><description>&lt;p&gt;
&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#34892;&#20026;&#29992;&#25143;&#20998;&#21106;&#20013;&#30340;&#20248;&#21270;&#20256;&#36882;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Delivery Optimized Discovery in Behavioral User Segmentation under Budget Constrain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03388
&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#20256;&#36882;&#21457;&#29616;&#34892;&#20026;&#29992;&#25143;&#32454;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#22312;&#32447;&#34892;&#20026;&#36275;&#36857;&#21487;&#20197;&#20351;&#20844;&#21496;&#21457;&#29616;&#22522;&#20110;&#34892;&#20026;&#30340;&#29992;&#25143;&#32454;&#20998;&#65292;&#24182;&#21521;&#29992;&#25143;&#21457;&#36865;&#29305;&#23450;&#32454;&#20998;&#30340;&#20449;&#24687;&#12290;&#22312;&#21457;&#29616;&#32454;&#20998;&#20043;&#21518;&#65292;&#36890;&#36807;&#20687;Facebook&#21644;Google&#36825;&#26679;&#30340;&#39318;&#36873;&#23186;&#20307;&#28192;&#36947;&#21521;&#29992;&#25143;&#21457;&#36865;&#20449;&#24687;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21482;&#26377;&#37096;&#20998;&#34892;&#20026;&#32454;&#20998;&#20013;&#30340;&#29992;&#25143;&#22312;&#23186;&#20307;&#19978;&#25214;&#21040;&#21305;&#37197;&#65292;&#24182;&#19988;&#21482;&#26377;&#20854;&#20013;&#19968;&#23567;&#37096;&#20998;&#30475;&#21040;&#28040;&#24687;&#65288;&#26333;&#20809;&#65289;&#12290;&#21363;&#20351;&#39640;&#36136;&#37327;&#30340;&#21457;&#29616;&#20063;&#20250;&#22312;&#20256;&#36882;&#22833;&#36133;&#26102;&#21464;&#24471;&#26080;&#29992;&#12290;&#35768;&#22810;&#22797;&#26434;&#30340;&#31639;&#27861;&#29992;&#20110;&#21457;&#29616;&#34892;&#20026;&#32454;&#20998;&#65292;&#28982;&#32780;&#36825;&#20123;&#31639;&#27861;&#24573;&#30053;&#20102;&#20256;&#36882;&#32452;&#20214;&#12290;&#38382;&#39064;&#21464;&#24471;&#22797;&#26434;&#26159;&#22240;&#20026;&#65288;i&#65289;&#21457;&#29616;&#26159;&#22312;&#20844;&#21496;&#25968;&#25454;&#65288;&#20363;&#22914;&#29992;&#25143;&#28857;&#20987;&#65289;&#30340;&#34892;&#20026;&#25968;&#25454;&#31354;&#38388;&#20013;&#36827;&#34892;&#30340;&#65292;&#32780;&#20256;&#36882;&#21017;&#26159;&#22522;&#20110;&#23186;&#20307;&#23450;&#20041;&#30340;&#38745;&#24577;&#25968;&#25454;&#31354;&#38388;&#65288;&#20363;&#22914;&#22320;&#29702;&#20301;&#32622;&#65292;&#24180;&#40836;&#65289;&#36827;&#34892;&#30340;&#65307;&#65288;ii&#65289;&#20844;&#21496;&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#36816;&#20316;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#39044;&#31639;&#38480;&#21046;&#19979;&#20248;&#21270;&#20256;&#36882;&#21457;&#29616;&#34892;&#20026;&#29992;&#25143;&#32454;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Users' behavioral footprints online enable firms to discover behavior-based user segments (or, segments) and deliver segment specific messages to users. Following the discovery of segments, delivery of messages to users through preferred media channels like Facebook and Google can be challenging, as only a portion of users in a behavior segment find match in a medium, and only a fraction of those matched actually see the message (exposure). Even high quality discovery becomes futile when delivery fails. Many sophisticated algorithms exist for discovering behavioral segments; however, these ignore the delivery component. The problem is compounded because (i) the discovery is performed on the behavior data space in firms' data (e.g., user clicks), while the delivery is predicated on the static data space (e.g., geo, age) as defined by media; and (ii) firms work under budget constraint. We introduce a stochastic optimization based algorithm for delivery optimized discovery of behavioral u
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#22312;&#22270;&#29983;&#25104;&#20013;&#20351;&#29992;RNN&#65292;&#24182;&#36890;&#36807;&#22686;&#21152;&#26080;&#24207;&#27491;&#21017;&#21270;&#39033;&#35299;&#20915;&#20102;&#39034;&#24207;&#38382;&#39064;&#65292;&#23545;&#20110;&#39034;&#24207;&#22270;&#29983;&#25104;&#27169;&#22411;&#23588;&#20854;&#22312;&#25968;&#25454;&#31232;&#32570;&#26102;&#26377;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.03387</link><description>&lt;p&gt;
&#20811;&#26381;&#33258;&#22238;&#24402;&#22270;&#29983;&#25104;&#20013;&#30340;&#39034;&#24207;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Overcoming Order in Autoregressive Graph Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03387
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#22312;&#22270;&#29983;&#25104;&#20013;&#20351;&#29992;RNN&#65292;&#24182;&#36890;&#36807;&#22686;&#21152;&#26080;&#24207;&#27491;&#21017;&#21270;&#39033;&#35299;&#20915;&#20102;&#39034;&#24207;&#38382;&#39064;&#65292;&#23545;&#20110;&#39034;&#24207;&#22270;&#29983;&#25104;&#27169;&#22411;&#23588;&#20854;&#22312;&#25968;&#25454;&#31232;&#32570;&#26102;&#26377;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#29983;&#25104;&#26159;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#21253;&#25324;&#21270;&#23398;&#21644;&#31038;&#20132;&#32593;&#32476;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#29983;&#25104;&#20998;&#23376;&#22270;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#29983;&#25104;&#26041;&#27861;&#26356;&#20855;&#20248;&#21183;&#65292;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#23558;&#36830;&#32493;&#28508;&#22312;&#34920;&#31034;&#36716;&#25442;&#20026;&#22270;&#12290;&#23558;&#22270;&#29983;&#25104;&#35270;&#20026;&#39034;&#24207;&#29983;&#25104;&#26102;&#20250;&#20986;&#29616;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#30001;&#20110;&#22270;&#25153;&#24179;&#21270;&#26041;&#27861;&#30340;&#20855;&#20307;&#36873;&#25321;&#32780;&#23548;&#33268;&#30340;&#24207;&#21015;&#30340;&#20219;&#24847;&#39034;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;RNN&#65292;&#36890;&#36807;&#22686;&#21152;&#26080;&#24207;&#27491;&#21017;&#21270;&#65288;OLR&#65289;&#39033;&#26469;&#32771;&#34385;&#22270;&#30340;&#38750;&#39034;&#24207;&#24615;&#65292;&#36825;&#40723;&#21169;&#36882;&#24402;&#27169;&#22411;&#30340;&#38544;&#34255;&#29366;&#24577;&#22312;&#35757;&#32451;&#20998;&#24067;&#20013;&#30340;&#19981;&#21516;&#26377;&#25928;&#25490;&#24207;&#19979;&#20445;&#25345;&#19981;&#21464;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#39034;&#24207;&#22270;&#29983;&#25104;&#27169;&#22411;&#21463;&#30410;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#23588;&#20854;&#22312;&#25968;&#25454;&#19981;&#36275;&#26102;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#22270;&#29983;&#25104;&#39046;&#22495;&#30340;&#30740;&#31350;&#22686;&#28155;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph generation is a fundamental problem in various domains, including chemistry and social networks. Recent work has shown that molecular graph generation using recurrent neural networks (RNNs) is advantageous compared to traditional generative approaches which require converting continuous latent representations into graphs. One issue which arises when treating graph generation as sequential generation is the arbitrary order of the sequence which results from a particular choice of graph flattening method. In this work we propose using RNNs, taking into account the non-sequential nature of graphs by adding an Orderless Regularization (OLR) term that encourages the hidden state of the recurrent model to be invariant to different valid orderings present under the training distribution. We demonstrate that sequential graph generation models benefit from our proposed regularization scheme, especially when data is scarce. Our findings contribute to the growing body of research on graph g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#36890;&#29992;&#20915;&#31574;&#26641;&#38598;&#25104;&#31639;&#27861;&#65292;&#20998;&#24067;&#24335;&#26799;&#24230;&#25552;&#21319;&#26862;&#26519;&#65288;DGBF&#65289;&#65292;&#36890;&#36807;&#23558;&#21253;&#21644;&#25552;&#21319;&#30340;&#25968;&#23398;&#20844;&#24335;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#26641;&#20043;&#38388;&#33258;&#28982;&#22320;&#36827;&#34892;&#20998;&#24067;&#24335;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#22788;&#29702;&#31163;&#25955;&#25110;&#34920;&#26684;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#24314;&#27169;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03386</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#36890;&#29992;&#20915;&#31574;&#26641;&#38598;&#25104;&#65306;&#20998;&#24067;&#24335;&#26799;&#24230;&#25552;&#21319;&#26862;&#26519;&#65288;DGBF&#65289;
&lt;/p&gt;
&lt;p&gt;
A generalized decision tree ensemble based on the NeuralNetworks architecture: Distributed Gradient Boosting Forest (DGBF)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#36890;&#29992;&#20915;&#31574;&#26641;&#38598;&#25104;&#31639;&#27861;&#65292;&#20998;&#24067;&#24335;&#26799;&#24230;&#25552;&#21319;&#26862;&#26519;&#65288;DGBF&#65289;&#65292;&#36890;&#36807;&#23558;&#21253;&#21644;&#25552;&#21319;&#30340;&#25968;&#23398;&#20844;&#24335;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#26641;&#20043;&#38388;&#33258;&#28982;&#22320;&#36827;&#34892;&#20998;&#24067;&#24335;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#22788;&#29702;&#31163;&#25955;&#25110;&#34920;&#26684;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#24314;&#27169;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#25552;&#21319;&#26159;&#24314;&#27169;&#31163;&#25955;&#25110;&#34920;&#26684;&#25968;&#25454;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#25968;&#23398;&#29305;&#24615;&#65292;&#26080;&#27861;&#20687;&#31070;&#32463;&#32593;&#32476;&#37027;&#26679;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#36827;&#34892;&#20998;&#23618;&#34920;&#31034;&#23398;&#20064;&#65292;&#36825;&#26159;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#21644;&#24314;&#27169;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21253;&#21644;&#25552;&#21319;&#30340;&#25968;&#23398;&#20844;&#24335;&#21487;&#20197;&#21512;&#24182;&#22312;&#19968;&#36215;&#65292;&#23450;&#20041;&#19968;&#20010;&#22270;&#32467;&#26500;&#30340;&#26641;&#38598;&#25104;&#31639;&#27861;&#65292;&#24182;&#22312;&#26641;&#20043;&#38388;&#33258;&#28982;&#22320;&#36827;&#34892;&#20998;&#24067;&#24335;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#65288;&#26080;&#38656;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#65289;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#31216;&#20026;&#20998;&#24067;&#24335;&#26799;&#24230;&#25552;&#21319;&#26862;&#26519;&#65288;DGBF&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#25552;&#21319;&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;DGBF&#30340;&#29305;&#23450;&#22270;&#32467;&#26500;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;...&#65288;&#25688;&#35201;&#25130;&#26029;&#65289;
&lt;/p&gt;
&lt;p&gt;
Tree ensemble algorithms as RandomForest and GradientBoosting are currently the dominant methods for modeling discrete or tabular data, however, they are unable to perform a hierarchical representation learning from raw data as NeuralNetworks does thanks to its multi-layered structure, which is a key feature for DeepLearning problems and modeling unstructured data. This limitation is due to the fact that tree algorithms can not be trained with back-propagation because of their mathematical nature. However, in this work, we demonstrate that the mathematical formulation of bagging and boosting can be combined together to define a graph-structured-tree-ensemble algorithm with a distributed representation learning process between trees naturally (without using back-propagation). We call this novel approach Distributed Gradient Boosting Forest (DGBF) and we demonstrate that both RandomForest and GradientBoosting can be expressed as particular graph architectures of DGBT. Finally, we see tha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#38738;&#23569;&#24180;&#20851;&#31995;&#34892;&#20026;&#19982;&#32933;&#32982;&#30123;&#24773;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#38738;&#23569;&#24180;&#30340;&#32676;&#20307;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#19988;&#19982;&#24615;&#21035;&#21644;&#39278;&#39135;&#21464;&#37327;&#26377;&#20851;&#12290;&#24635;&#32467;&#20986;&#30340;&#20851;&#38190;&#22240;&#32032;&#26377;&#21161;&#20110;&#20102;&#35299;&#21644;&#24178;&#39044;&#38738;&#23569;&#24180;&#32933;&#32982;&#30123;&#24773;&#12290;</title><link>https://arxiv.org/abs/2402.03385</link><description>&lt;p&gt;
&#38738;&#23569;&#24180;&#20851;&#31995;&#34892;&#20026;&#19982;&#32933;&#32982;&#30123;&#24773;&#65306;&#24212;&#29992;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#25551;&#36848;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adolescent relational behaviour and the obesity pandemic: A descriptive study applying social network analysis and machine learning techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#38738;&#23569;&#24180;&#20851;&#31995;&#34892;&#20026;&#19982;&#32933;&#32982;&#30123;&#24773;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#38738;&#23569;&#24180;&#30340;&#32676;&#20307;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#19988;&#19982;&#24615;&#21035;&#21644;&#39278;&#39135;&#21464;&#37327;&#26377;&#20851;&#12290;&#24635;&#32467;&#20986;&#30340;&#20851;&#38190;&#22240;&#32032;&#26377;&#21161;&#20110;&#20102;&#35299;&#21644;&#24178;&#39044;&#38738;&#23569;&#24180;&#32933;&#32982;&#30123;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#36890;&#36807;&#25506;&#32034;&#32676;&#20307;&#33410;&#28857;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#30740;&#31350;&#19982;&#39278;&#39135;&#21644;&#24615;&#21035;&#30456;&#20851;&#30340;&#20122;&#32676;&#23384;&#22312;&#65292;&#24182;&#36890;&#36807;SNA&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20998;&#26512;&#32676;&#20307;&#20043;&#38388;&#30340;&#36830;&#25509;&#24615;&#12290;&#26041;&#27861;&#65306;2015&#24180;3&#26376;&#33267;12&#26376;&#65292;&#20849;&#26377;5&#20010;&#19981;&#21516;&#25945;&#32946;&#20013;&#24515;&#30340;235&#21517;&#23398;&#29983;&#21442;&#19982;&#20102;&#26412;&#30740;&#31350;&#12290;&#25968;&#25454;&#20998;&#26512;&#20998;&#20026;&#20004;&#20010;&#37096;&#20998;&#65306;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#21644;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#33267;&#20110;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#65292;&#24212;&#29992;Girvan-Newman&#25216;&#26415;&#22312;&#19981;&#21516;&#29677;&#32423;&#30340;&#21451;&#35850;&#32593;&#32476;&#20013;&#25214;&#21040;&#26368;&#20339;&#30340;&#20869;&#32858;&#32676;&#20307;&#25968;&#37327;&#12290;&#32467;&#26524;&#65306;&#22312;&#19977;&#20010;&#29677;&#32423;&#20013;&#24212;&#29992;Girvan-Newman&#21518;&#65292;&#20998;&#21035;&#24471;&#21040;&#20102;2&#20010;&#12289;7&#20010;&#21644;6&#20010;&#26368;&#20339;&#30340;&#32676;&#20307;&#21010;&#20998;&#12290;&#32676;&#20307;&#20043;&#38388;&#19982;&#24615;&#21035;&#21644;&#39278;&#39135;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#22312;&#20998;&#26512;&#20102;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#21644;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#32467;&#26524;&#21518;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#20102;&#24433;&#21709;&#38738;&#23569;&#24180;&#32933;&#32982;&#30123;&#24773;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aim: To study the existence of subgroups by exploring the similarities between the attributes of the nodes of the groups, in relation to diet and gender and, to analyse the connectivity between groups based on aspects of similarities between them through SNA and artificial intelligence techniques.   Methods: 235 students from 5 different educational centres participate in this study between March and December 2015. Data analysis carried out is divided into two blocks: social network analysis and unsupervised machine learning techniques. As for the social network analysis, the Girvan-Newman technique was applied to find the best number of cohesive groups within each of the friendship networks of the different classes analysed.   Results: After applying Girvan-Newman in the three classes, the best division into clusters was respectively 2 for classroom A, 7 for classroom B and 6 for classroom C. There are significant differences between the groups and the gender and diet variables. After
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#29983;&#23384;&#21644;&#32959;&#30244;&#31561;&#32423;&#39044;&#27979;&#65292;&#29983;&#23384;&#39044;&#27979;&#20934;&#30830;&#29575;&#36798;&#21040;65%&#65292;&#32959;&#30244;&#31561;&#32423;&#39044;&#27979;&#20934;&#30830;&#29575;&#36798;&#21040;97%&#12290;</title><link>https://arxiv.org/abs/2402.03384</link><description>&lt;p&gt;
&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#39044;&#27979;&#33014;&#36136;&#30244;&#30340;&#29983;&#23384;&#21644;&#31561;&#32423;
&lt;/p&gt;
&lt;p&gt;
Survival and grade of the glioma prediction using transfer learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#29983;&#23384;&#21644;&#32959;&#30244;&#31561;&#32423;&#39044;&#27979;&#65292;&#29983;&#23384;&#39044;&#27979;&#20934;&#30830;&#29575;&#36798;&#21040;65%&#65292;&#32959;&#30244;&#31561;&#32423;&#39044;&#27979;&#20934;&#30830;&#29575;&#36798;&#21040;97%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#26159;&#19968;&#31181;&#39640;&#24230;&#24694;&#24615;&#30340;&#33041;&#32959;&#30244;&#65292;&#27809;&#26377;&#27835;&#30103;&#30340;&#24773;&#20917;&#19979;&#39044;&#26399;&#23551;&#21629;&#20165;&#20026;3&#33267;6&#20010;&#26376;&#12290;&#20934;&#30830;&#26816;&#27979;&#21644;&#39044;&#27979;&#20854;&#29983;&#23384;&#21644;&#31561;&#32423;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#35814;&#23613;&#30340;&#20248;&#21270;&#65292;&#27979;&#35797;&#20102;&#21508;&#31181;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#65292;&#21253;&#25324;EfficientNet&#12289;ResNet&#12289;VGG16&#21644;Inception&#65292;&#20197;&#25214;&#21040;&#26368;&#36866;&#21512;&#30340;&#26550;&#26500;&#12290;&#36801;&#31227;&#23398;&#20064;&#24212;&#29992;&#20110;&#23545;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#26088;&#22312;&#23454;&#29616;&#20004;&#20010;&#30446;&#26631;&#65306;&#29983;&#23384;&#39044;&#27979;&#21644;&#32959;&#30244;&#31561;&#32423;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#29983;&#23384;&#39044;&#27979;&#20934;&#30830;&#29575;&#36798;&#21040;65%&#65292;&#23558;&#24739;&#32773;&#20998;&#20026;&#30701;&#26399;&#12289;&#20013;&#26399;&#21644;&#38271;&#26399;&#29983;&#23384;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#32959;&#30244;&#31561;&#32423;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#36798;&#21040;97%&#65292;&#20934;&#30830;&#21306;&#20998;&#20302;&#31561;&#32423;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;(LGG)&#21644;&#39640;&#31561;&#32423;&#33014;&#36136;&#27597;&#32454;&#32990;&#30244;(HGG)&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#25104;&#21151;&#24402;&#22240;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Glioblastoma is a highly malignant brain tumor with a life expectancy of only 3 to 6 months without treatment. Detecting and predicting its survival and grade accurately are crucial. This study introduces a novel approach using transfer learning techniques. Various pre-trained networks, including EfficientNet, ResNet, VGG16, and Inception, were tested through exhaustive optimization to identify the most suitable architecture. Transfer learning was applied to fine-tune these models on a glioblastoma image dataset, aiming to achieve two objectives: survival and tumor grade prediction.The experimental results show 65% accuracy in survival prediction, classifying patients into short, medium, or long survival categories. Additionally, the prediction of tumor grade achieved an accuracy of 97%, accurately differentiating low-grade gliomas (LGG) and high-grade gliomas (HGG). The success of the approach is attributed to the effectiveness of transfer learning, surpassing the current state-of-the
&lt;/p&gt;</description></item><item><title>&#20840;&#38142;&#36335;&#19978;&#21319;&#24314;&#27169;&#26041;&#27861;ECUP&#26088;&#22312;&#35299;&#20915;&#38142;&#36335;&#20559;&#24046;&#21644;&#22788;&#29702;&#19981;&#36866;&#24212;&#38382;&#39064;&#65292;&#22312;&#32447;&#33829;&#38144;&#20013;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.03379</link><description>&lt;p&gt;
&#20840;&#38142;&#36335;&#19978;&#21319;&#24314;&#27169;&#19982;&#19978;&#19979;&#25991;&#22686;&#24378;&#23398;&#20064;&#29992;&#20110;&#26234;&#33021;&#33829;&#38144;
&lt;/p&gt;
&lt;p&gt;
Entire Chain Uplift Modeling with Context-Enhanced Learning for Intelligent Marketing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03379
&lt;/p&gt;
&lt;p&gt;
&#20840;&#38142;&#36335;&#19978;&#21319;&#24314;&#27169;&#26041;&#27861;ECUP&#26088;&#22312;&#35299;&#20915;&#38142;&#36335;&#20559;&#24046;&#21644;&#22788;&#29702;&#19981;&#36866;&#24212;&#38382;&#39064;&#65292;&#22312;&#32447;&#33829;&#38144;&#20013;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#21319;&#24314;&#27169;&#22312;&#22312;&#32447;&#33829;&#38144;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#23427;&#26088;&#22312;&#36890;&#36807;&#39044;&#27979;&#20010;&#20307;&#22788;&#29702;&#25928;&#26524;&#65288;ITE&#65289;&#26469;&#20934;&#30830;&#34913;&#37327;&#19981;&#21516;&#31574;&#30053;&#65288;&#22914;&#20248;&#24800;&#21048;&#25110;&#25240;&#25187;&#65289;&#23545;&#19981;&#21516;&#29992;&#25143;&#30340;&#24433;&#21709;&#12290;&#22312;&#30005;&#23376;&#21830;&#21153;&#29615;&#22659;&#20013;&#65292;&#29992;&#25143;&#34892;&#20026;&#36981;&#24490;&#30830;&#23450;&#30340;&#39034;&#24207;&#38142;&#36335;&#65292;&#21253;&#25324;&#23637;&#31034;&#12289;&#28857;&#20987;&#21644;&#36716;&#21270;&#12290;&#33829;&#38144;&#31574;&#30053;&#22312;&#36825;&#20010;&#38142;&#36335;&#20013;&#30340;&#27599;&#20010;&#38454;&#27573;&#37117;&#20250;&#20135;&#29983;&#19981;&#21516;&#30340;&#19978;&#21319;&#25928;&#24212;&#65292;&#24433;&#21709;&#30528;&#28857;&#20987;&#29575;&#21644;&#36716;&#21270;&#29575;&#31561;&#25351;&#26631;&#12290;&#23613;&#31649;&#20854;&#23454;&#29992;&#24615;&#65292;&#29616;&#26377;&#30740;&#31350;&#24573;&#35270;&#20102;&#29305;&#23450;&#22788;&#29702;&#20013;&#25152;&#26377;&#38454;&#27573;&#30340;&#30456;&#20114;&#24433;&#21709;&#65292;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;&#22788;&#29702;&#20449;&#24687;&#65292;&#21487;&#33021;&#32473;&#21518;&#32493;&#30340;&#33829;&#38144;&#20915;&#31574;&#24341;&#20837;&#20102;&#37325;&#22823;&#20559;&#24046;&#12290;&#26412;&#25991;&#23558;&#36825;&#20004;&#20010;&#38382;&#39064;&#31216;&#20026;&#38142;&#36335;&#20559;&#24046;&#38382;&#39064;&#21644;&#22788;&#29702;&#19981;&#36866;&#24212;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#20855;&#26377;&#19978;&#19979;&#25991;&#22686;&#24378;&#23398;&#20064;&#30340;&#20840;&#38142;&#36335;&#19978;&#21319;&#26041;&#27861;&#65288;ECUP&#65289;&#12290;ECUP&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;
&lt;/p&gt;
&lt;p&gt;
Uplift modeling, vital in online marketing, seeks to accurately measure the impact of various strategies, such as coupons or discounts, on different users by predicting the Individual Treatment Effect (ITE). In an e-commerce setting, user behavior follows a defined sequential chain, including impression, click, and conversion. Marketing strategies exert varied uplift effects at each stage within this chain, impacting metrics like click-through and conversion rate. Despite its utility, existing research has neglected to consider the inter-task across all stages impacts within a specific treatment and has insufficiently utilized the treatment information, potentially introducing substantial bias into subsequent marketing decisions. We identify these two issues as the chain-bias problem and the treatment-unadaptive problem. This paper introduces the Entire Chain UPlift method with context-enhanced learning (ECUP), devised to tackle these issues. ECUP consists of two primary components: 1)
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#35895;&#27468;&#30340;&#35821;&#38899;&#35782;&#21035;&#21644;&#21477;&#23376;&#20998;&#31867;&#22312;&#21307;&#30103;&#20581;&#24247;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23558;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#24212;&#29992;&#20110;&#22260;&#25163;&#26415;&#26399;&#26381;&#21153;&#65292;&#21487;&#20197;&#25913;&#21892;&#24739;&#32773;&#27969;&#31243;&#21644;&#21307;&#30103;&#36136;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;&#21518;&#22788;&#29702;&#20998;&#31867;&#22120;&#65292;&#26412;&#30740;&#31350;&#22686;&#24378;&#20102;&#35895;&#27468;&#30340;&#35821;&#38899;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03369</link><description>&lt;p&gt;
&#35780;&#20272;&#35895;&#27468;&#35821;&#38899;&#35782;&#21035;&#21644;&#21477;&#23376;&#20998;&#31867;&#22312;&#21307;&#30103;&#20581;&#24247;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Google's Voice Recognition and Sentence Classification for Health Care Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#35895;&#27468;&#30340;&#35821;&#38899;&#35782;&#21035;&#21644;&#21477;&#23376;&#20998;&#31867;&#22312;&#21307;&#30103;&#20581;&#24247;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23558;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#24212;&#29992;&#20110;&#22260;&#25163;&#26415;&#26399;&#26381;&#21153;&#65292;&#21487;&#20197;&#25913;&#21892;&#24739;&#32773;&#27969;&#31243;&#21644;&#21307;&#30103;&#36136;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;&#21518;&#22788;&#29702;&#20998;&#31867;&#22120;&#65292;&#26412;&#30740;&#31350;&#22686;&#24378;&#20102;&#35895;&#27468;&#30340;&#35821;&#38899;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#24212;&#29992;&#20110;&#22260;&#25163;&#26415;&#26399;&#26381;&#21153;&#65288;Periop&#65289;&#65292;&#20197;&#20351;Periop&#21592;&#24037;&#33021;&#22815;&#20351;&#29992;&#31227;&#21160;&#25216;&#26415;&#35760;&#24405;&#24037;&#20316;&#27969;&#31243;&#37324;&#31243;&#30865;&#12290;&#22914;&#26524;&#33021;&#22815;&#20351;&#36825;&#31181;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#21464;&#24471;&#24378;&#22823;&#21487;&#38752;&#65292;&#23601;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#31227;&#21160;&#25216;&#26415;&#25913;&#21892;&#24739;&#32773;&#27969;&#31243;&#21644;&#21307;&#30103;&#36136;&#37327;&#12290;&#27492;&#23454;&#39564;&#30340;&#30446;&#26631;&#26159;&#20351;Periop&#21592;&#24037;&#33021;&#22815;&#25552;&#20379;&#26080;&#24178;&#25200;&#30340;&#25252;&#29702;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#24405;&#20837;&#21644;&#26597;&#35810;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#35813;&#30740;&#31350;&#30340;&#32467;&#26524;&#20063;&#36866;&#29992;&#20110;&#20854;&#20182;&#24773;&#20917;&#65292;&#21363;&#24037;&#31243;&#32463;&#29702;&#23581;&#35797;&#20351;&#29992;&#31227;&#21160;&#25216;&#26415;&#25913;&#21892;&#36890;&#20449;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#21518;&#22788;&#29702;&#20998;&#31867;&#22120;&#65288;&#21363;&#21477;&#23376;&#21253;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#26368;&#22823;&#29109;&#65289;&#22686;&#24378;&#20102;&#35895;&#27468;&#30340;&#35821;&#38899;&#35782;&#21035;&#33021;&#21147;&#12290;&#23454;&#39564;&#30740;&#31350;&#20102;&#19977;&#20010;&#22240;&#32032;&#65288;&#21407;&#22987;&#25514;&#36766;&#12289;&#31616;&#21270;&#25514;&#36766;&#21644;&#20010;&#24615;&#21270;&#25514;&#36766;&#65289;&#22312;&#19977;&#20010;&#27700;&#24179;&#65288;&#38646;&#27425;&#35757;&#32451;&#37325;&#22797;&#12289;5&#27425;&#35757;&#32451;&#37325;&#22797;&#21644;10&#27425;&#35757;&#32451;&#37325;&#22797;&#65289;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study examined the use of voice recognition technology in perioperative services (Periop) to enable Periop staff to record workflow milestones using mobile technology. The use of mobile technology to improve patient flow and quality of care could be facilitated if such voice recognition technology could be made robust. The goal of this experiment was to allow the Periop staff to provide care without being interrupted with data entry and querying tasks. However, the results are generalizable to other situations where an engineering manager attempts to improve communication performance using mobile technology. This study enhanced Google's voice recognition capability by using post-processing classifiers (i.e., bag-of-sentences, support vector machine, and maximum entropy). The experiments investigated three factors (original phrasing, reduced phrasing, and personalized phrasing) at three levels (zero training repetition, 5 training repetitions, and 10 training repetitions). Results 
&lt;/p&gt;</description></item><item><title>RAG-Fusion&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#22810;&#20010;&#26597;&#35810;&#65292;&#24182;&#32467;&#21512;&#20114;&#24800;&#25490;&#21517;&#34701;&#21512;&#25216;&#26415;&#65292;&#33021;&#22815;&#20174;&#19981;&#21516;&#35282;&#24230;&#19978;&#19979;&#25991;&#21270;&#21407;&#22987;&#26597;&#35810;&#65292;&#25552;&#20379;&#20934;&#30830;&#21644;&#20840;&#38754;&#30340;&#20449;&#24687;&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#26041;&#38754;&#26377;&#37325;&#35201;&#36827;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#20840;&#29699;&#21644;&#21306;&#22495;&#20043;&#38388;&#30340;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2402.03367</link><description>&lt;p&gt;
RAG-Fusion: &#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#26032;&#36884;&#24452;
&lt;/p&gt;
&lt;p&gt;
RAG-Fusion: a New Take on Retrieval-Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03367
&lt;/p&gt;
&lt;p&gt;
RAG-Fusion&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#22810;&#20010;&#26597;&#35810;&#65292;&#24182;&#32467;&#21512;&#20114;&#24800;&#25490;&#21517;&#34701;&#21512;&#25216;&#26415;&#65292;&#33021;&#22815;&#20174;&#19981;&#21516;&#35282;&#24230;&#19978;&#19979;&#25991;&#21270;&#21407;&#22987;&#26597;&#35810;&#65292;&#25552;&#20379;&#20934;&#30830;&#21644;&#20840;&#38754;&#30340;&#20449;&#24687;&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#26041;&#38754;&#26377;&#37325;&#35201;&#36827;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#20840;&#29699;&#21644;&#21306;&#22495;&#20043;&#38388;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Infineon&#24050;&#32463;&#30830;&#23450;&#24037;&#31243;&#24072;&#12289;&#23458;&#25143;&#32463;&#29702;&#21644;&#23458;&#25143;&#36805;&#36895;&#33719;&#21462;&#20135;&#21697;&#20449;&#24687;&#30340;&#38656;&#27714;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20010;&#38382;&#39064;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#32842;&#22825;&#26426;&#22120;&#20154;&#26469;&#35299;&#20915;&#65292;&#20294;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#35780;&#20272;&#20102;&#26032;&#36817;&#27969;&#34892;&#30340;RAG-Fusion&#26041;&#27861;&#30340;&#20351;&#29992;&#12290;RAG-Fusion&#23558;RAG&#21644;&#20114;&#24800;&#25490;&#21517;&#34701;&#21512;&#65288;RRF&#65289;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#20010;&#26597;&#35810;&#65292;&#20351;&#29992;&#20114;&#24800;&#20998;&#25968;&#23545;&#20854;&#36827;&#34892;&#20877;&#25490;&#24207;&#65292;&#24182;&#34701;&#21512;&#25991;&#26723;&#21644;&#20998;&#25968;&#12290;&#36890;&#36807;&#23545;&#20934;&#30830;&#24615;&#12289;&#30456;&#20851;&#24615;&#21644;&#20840;&#38754;&#24615;&#36827;&#34892;&#25163;&#21160;&#35780;&#20272;&#65292;&#25105;&#21457;&#29616;RAG-Fusion&#33021;&#22815;&#36890;&#36807;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#23545;&#21407;&#22987;&#26597;&#35810;&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#65292;&#25552;&#20379;&#20934;&#30830;&#21644;&#20840;&#38754;&#30340;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#24403;&#29983;&#25104;&#30340;&#26597;&#35810;&#19982;&#21407;&#22987;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#19981;&#36275;&#26102;&#65292;&#26377;&#20123;&#31572;&#26696;&#20559;&#31163;&#20102;&#20027;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#20840;&#29699;&#21644;&#21306;&#22495;&#20043;&#38388;&#30340;&#21464;&#38761;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infineon has identified a need for engineers, account managers, and customers to rapidly obtain product information. This problem is traditionally addressed with retrieval-augmented generation (RAG) chatbots, but in this study, I evaluated the use of the newly popularized RAG-Fusion method. RAG-Fusion combines RAG and reciprocal rank fusion (RRF) by generating multiple queries, reranking them with reciprocal scores and fusing the documents and scores. Through manually evaluating answers on accuracy, relevance, and comprehensiveness, I found that RAG-Fusion was able to provide accurate and comprehensive answers due to the generated queries contextualizing the original query from various perspectives. However, some answers strayed off topic when the generated queries' relevance to the original query is insufficient. This research marks significant progress in artificial intelligence (AI) and natural language processing (NLP) applications and demonstrates transformations in a global and m
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#29992;&#25143;&#21644;&#39033;&#30446;&#36755;&#20837;&#30340;ID&#21521;&#37327;&#20316;&#20026;&#25552;&#31034;&#65292;&#21033;&#29992;GPT-2&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#21487;&#35299;&#37322;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#32852;&#21512;&#35757;&#32451;&#26426;&#21046;&#24182;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#25506;&#32034;&#29992;&#25143;&#30340;&#20852;&#36259;&#65292;&#25552;&#39640;&#25512;&#33616;&#25928;&#26524;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.03366</link><description>&lt;p&gt;
&#24102;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#21487;&#35299;&#37322;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Explainable Recommendation with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03366
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#29992;&#25143;&#21644;&#39033;&#30446;&#36755;&#20837;&#30340;ID&#21521;&#37327;&#20316;&#20026;&#25552;&#31034;&#65292;&#21033;&#29992;GPT-2&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#21487;&#35299;&#37322;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#32852;&#21512;&#35757;&#32451;&#26426;&#21046;&#24182;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#25506;&#32034;&#29992;&#25143;&#30340;&#20852;&#36259;&#65292;&#25552;&#39640;&#25512;&#33616;&#25928;&#26524;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20869;&#25552;&#20379;&#35299;&#37322;&#33021;&#22815;&#25552;&#21319;&#29992;&#25143;&#28385;&#24847;&#24230;&#24182;&#24314;&#31435;&#20449;&#20219;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#35814;&#32454;&#35828;&#26126;&#20026;&#29992;&#25143;&#23450;&#21046;&#25512;&#33616;&#39033;&#30446;&#30340;&#21407;&#22240;&#12290;&#24403;&#21069;&#39046;&#22495;&#20013;&#20027;&#35201;&#30340;&#26041;&#27861;&#26159;&#29983;&#25104;&#22522;&#20110;&#25991;&#26412;&#30340;&#35299;&#37322;&#65292;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24212;&#29992;&#23588;&#20026;&#31361;&#20986;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#65292;&#25913;&#36827;LLMs&#20197;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#22312;&#23454;&#36341;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#26159;&#35757;&#32451;&#25552;&#31034;&#32780;&#19981;&#26159;LLM&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21033;&#29992;&#29992;&#25143;&#21644;&#39033;&#30446;&#36755;&#20837;&#30340;ID&#21521;&#37327;&#20316;&#20026;GPT-2&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#20013;&#37319;&#29992;&#32852;&#21512;&#35757;&#32451;&#26426;&#21046;&#65292;&#20248;&#21270;&#25512;&#33616;&#20219;&#21153;&#21644;&#35299;&#37322;&#20219;&#21153;&#12290;&#36825;&#31181;&#31574;&#30053;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#25506;&#32034;&#29992;&#25143;&#30340;&#20852;&#36259;&#65292;&#25552;&#39640;&#25512;&#33616;&#25928;&#26524;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;...
&lt;/p&gt;
&lt;p&gt;
Providing explanations within the recommendation system would boost user satisfaction and foster trust, especially by elaborating on the reasons for selecting recommended items tailored to the user. The predominant approach in this domain revolves around generating text-based explanations, with a notable emphasis on applying large language models (LLMs). However, refining LLMs for explainable recommendations proves impractical due to time constraints and computing resource limitations. As an alternative, the current approach involves training the prompt rather than the LLM. In this study, we developed a model that utilizes the ID vectors of user and item inputs as prompts for GPT-2. We employed a joint training mechanism within a multi-task learning framework to optimize both the recommendation task and explanation task. This strategy enables a more effective exploration of users' interests, improving recommendation effectiveness and user satisfaction. Through the experiments, our meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#20844;&#24179;&#25512;&#33616;&#31995;&#32479;&#65292;&#21517;&#20026;HetroFair&#65292;&#26088;&#22312;&#25552;&#39640;&#39033;&#30446;&#20391;&#30340;&#20844;&#24179;&#24615;&#12290;HetroFair&#20351;&#29992;&#20844;&#24179;&#27880;&#24847;&#21147;&#21644;&#24322;&#36136;&#24615;&#29305;&#24449;&#21152;&#26435;&#20004;&#20010;&#32452;&#20214;&#26469;&#29983;&#25104;&#20855;&#26377;&#20844;&#24179;&#24615;&#24847;&#35782;&#30340;&#23884;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.03365</link><description>&lt;p&gt;
&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#30064;&#36136;&#21451;&#21892;&#25512;&#33616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Heterophily-Aware Fair Recommendation using Graph Convolutional Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#20844;&#24179;&#25512;&#33616;&#31995;&#32479;&#65292;&#21517;&#20026;HetroFair&#65292;&#26088;&#22312;&#25552;&#39640;&#39033;&#30446;&#20391;&#30340;&#20844;&#24179;&#24615;&#12290;HetroFair&#20351;&#29992;&#20844;&#24179;&#27880;&#24847;&#21147;&#21644;&#24322;&#36136;&#24615;&#29305;&#24449;&#21152;&#26435;&#20004;&#20010;&#32452;&#20214;&#26469;&#29983;&#25104;&#20855;&#26377;&#20844;&#24179;&#24615;&#24847;&#35782;&#30340;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#25104;&#20026;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#19981;&#20165;&#35774;&#35745;&#20026;&#20026;&#26368;&#32456;&#29992;&#25143;&#26381;&#21153;&#65292;&#36824;&#35201;&#35753;&#20854;&#20182;&#21442;&#19982;&#32773;&#65288;&#22914;&#39033;&#30446;&#21644;&#39033;&#30446;&#20379;&#24212;&#21830;&#65289;&#20174;&#20013;&#21463;&#30410;&#12290;&#36825;&#20123;&#21442;&#19982;&#32773;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#25110;&#20914;&#31361;&#30340;&#30446;&#26631;&#21644;&#21033;&#30410;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#20844;&#24179;&#24615;&#21644;&#27969;&#34892;&#24230;&#20559;&#24046;&#32771;&#34385;&#30340;&#38656;&#27714;&#12290;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#26041;&#27861;&#20063;&#38754;&#20020;&#19981;&#20844;&#24179;&#24615;&#21644;&#27969;&#34892;&#24230;&#20559;&#24046;&#30340;&#25361;&#25112;&#65292;&#20854;&#24402;&#19968;&#21270;&#21644;&#32858;&#21512;&#36807;&#31243;&#21463;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#30340;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#31216;&#20026;HetroFair&#65292;&#26088;&#22312;&#25552;&#39640;&#39033;&#30446;&#20391;&#30340;&#20844;&#24179;&#24615;&#12290;HetroFair&#20351;&#29992;&#20004;&#20010;&#29420;&#31435;&#30340;&#32452;&#20214;&#29983;&#25104;&#20855;&#26377;&#20844;&#24179;&#24615;&#24847;&#35782;&#30340;&#23884;&#20837;&#65306;i&#65289;&#20844;&#24179;&#27880;&#24847;&#21147;&#65292;&#23427;&#22312;GNN&#30340;&#24402;&#19968;&#21270;&#36807;&#31243;&#20013;&#32467;&#21512;&#20102;&#28857;&#31215;&#65292;&#20197;&#20943;&#23569;&#33410;&#28857;&#24230;&#25968;&#30340;&#24433;&#21709;&#65307;ii&#65289;&#24322;&#36136;&#24615;&#29305;&#24449;&#21152;&#26435;&#65292;&#20026;&#19981;&#21516;&#30340;&#29305;&#24449;&#20998;&#37197;&#19981;&#21516;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, graph neural networks (GNNs) have become a popular tool to improve the accuracy and performance of recommender systems. Modern recommender systems are not only designed to serve the end users, but also to benefit other participants, such as items and items providers. These participants may have different or conflicting goals and interests, which raise the need for fairness and popularity bias considerations. GNN-based recommendation methods also face the challenges of unfairness and popularity bias and their normalization and aggregation processes suffer from these challenges. In this paper, we propose a fair GNN-based recommender system, called HetroFair, to improve items' side fairness. HetroFair uses two separate components to generate fairness-aware embeddings: i) fairness-aware attention which incorporates dot product in the normalization process of GNNs, to decrease the effect of nodes' degrees, and ii) heterophily feature weighting to assign distinct weights to 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#32452;&#21512;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36136;&#25968;&#21644;&#38750;&#36136;&#25968;&#20998;&#31867;&#20013;&#23454;&#29616;&#39640;&#21484;&#22238;&#29575;&#21644;&#24555;&#36895;&#25910;&#25947;&#30340;&#26032;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03363</link><description>&lt;p&gt;
&#25506;&#32034;&#36136;&#25968;&#20998;&#31867;&#65306;&#20351;&#29992;&#31232;&#30095;&#32534;&#30721;&#23454;&#29616;&#39640;&#21484;&#22238;&#29575;&#21644;&#24555;&#36895;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Exploring Prime Number Classification: Achieving High Recall Rate and Rapid Convergence with Sparse Encoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03363
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#32452;&#21512;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36136;&#25968;&#21644;&#38750;&#36136;&#25968;&#20998;&#31867;&#20013;&#23454;&#29616;&#39640;&#21484;&#22238;&#29575;&#21644;&#24555;&#36895;&#25910;&#25947;&#30340;&#26032;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#35770;&#65292;&#22312;&#36136;&#25968;&#21644;&#38750;&#36136;&#25968;&#20998;&#31867;&#19978;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26680;&#24515;&#26159;&#24320;&#21457;&#19968;&#31181;&#39640;&#24230;&#31232;&#30095;&#30340;&#32534;&#30721;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#32452;&#21512;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#22312;&#35782;&#21035;&#36136;&#25968;&#26102;&#36798;&#21040;&#20102;&#36229;&#36807;99\%&#30340;&#21484;&#22238;&#29575;&#65292;&#22312;&#35782;&#21035;&#38750;&#36136;&#25968;&#26102;&#36798;&#21040;&#20102;79\%&#30340;&#21484;&#22238;&#29575;&#65292;&#36825;&#20123;&#25968;&#23383;&#26159;&#20174;&#26412;&#36136;&#19978;&#19981;&#24179;&#34913;&#30340;&#39034;&#24207;&#25972;&#25968;&#24207;&#21015;&#20013;&#24471;&#20986;&#30340;&#65292;&#24182;&#19988;&#22312;&#23436;&#25104;&#21333;&#20010;&#35757;&#32451;&#21608;&#26399;&#20043;&#21069;&#36805;&#36895;&#25910;&#25947;&#12290;&#25105;&#20204;&#20351;&#29992; $10^6$ &#20010;&#25972;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#25351;&#23450;&#30340;&#25972;&#25968;&#24320;&#22987;&#65292;&#28982;&#21518;&#22312;&#19968;&#20010;&#19981;&#21516;&#33539;&#22260;&#30340; $2 \times 10^6$ &#20010;&#25972;&#25968;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#33539;&#22260;&#20174; $10^6$ &#21040; $3 \times 10^6$&#65292;&#20559;&#31227;&#37327;&#30456;&#21516;&#12290;&#23613;&#31649;&#21463;&#38480;&#20110;&#36164;&#28304;&#30340;&#20869;&#23384;&#23481;&#37327;&#65292;&#38480;&#21046;&#25105;&#20204;&#30340;&#20998;&#26512;&#36328;&#36234;&#20102; $3\times10^6$&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#26426;&#22120;&#23398;&#20064;&#22312;......&#30340;&#24212;&#29992;&#20570;&#20986;&#20102;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach at the intersection of machine learning and number theory, focusing on the classification of prime and non-prime numbers. At the core of our research is the development of a highly sparse encoding method, integrated with conventional neural network architectures. This combination has shown promising results, achieving a recall of over 99\% in identifying prime numbers and 79\% for non-prime numbers from an inherently imbalanced sequential series of integers, while exhibiting rapid model convergence before the completion of a single training epoch. We performed training using $10^6$ integers starting from a specified integer and tested on a different range of $2 \times 10^6$ integers extending from $10^6$ to $3 \times 10^6$, offset by the same starting integer. While constrained by the memory capacity of our resources, which limited our analysis to a span of $3\times10^6$, we believe that our study contribute to the application of machine learning in
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#30740;&#20102;&#22270;&#32553;&#20943;&#26041;&#27861;&#65292;&#21253;&#25324;&#31232;&#30095;&#21270;&#12289;&#31895;&#21270;&#21644;&#27987;&#32553;&#65292;&#22312;&#35299;&#20915;&#22823;&#22411;&#22270;&#24418;&#25968;&#25454;&#20998;&#26512;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#26041;&#38754;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#35843;&#30740;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#22238;&#39038;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#24615;&#12290;&#21516;&#26102;&#65292;&#35843;&#30740;&#36824;&#25552;&#20986;&#20102;&#20445;&#35777;&#22270;&#32553;&#20943;&#25216;&#26415;&#25345;&#32493;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.03358</link><description>&lt;p&gt;
&#22270;&#32553;&#20943;&#30340;&#32508;&#21512;&#35843;&#30740;&#65306;&#31232;&#30095;&#21270;&#12289;&#31895;&#21270;&#21644;&#27987;&#32553;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03358
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#30740;&#20102;&#22270;&#32553;&#20943;&#26041;&#27861;&#65292;&#21253;&#25324;&#31232;&#30095;&#21270;&#12289;&#31895;&#21270;&#21644;&#27987;&#32553;&#65292;&#22312;&#35299;&#20915;&#22823;&#22411;&#22270;&#24418;&#25968;&#25454;&#20998;&#26512;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#26041;&#38754;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#35843;&#30740;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#22238;&#39038;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#24615;&#12290;&#21516;&#26102;&#65292;&#35843;&#30740;&#36824;&#25552;&#20986;&#20102;&#20445;&#35777;&#22270;&#32553;&#20943;&#25216;&#26415;&#25345;&#32493;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#33258;&#28982;&#22320;&#34920;&#31034;&#20026;&#22270;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22270;&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#24615;&#21644;&#35268;&#27169;&#30340;&#22686;&#21152;&#20026;&#20998;&#26512;&#21644;&#35745;&#31639;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#22270;&#32553;&#20943;&#25216;&#26415;&#22312;&#20445;&#30041;&#20851;&#38190;&#23646;&#24615;&#30340;&#21516;&#26102;&#31616;&#21270;&#22823;&#22411;&#22270;&#24418;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#22312;&#26412;&#35843;&#30740;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;&#22270;&#32553;&#20943;&#26041;&#27861;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#21253;&#25324;&#22270;&#31232;&#30095;&#21270;&#12289;&#22270;&#31895;&#21270;&#21644;&#22270;&#27987;&#32553;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#32479;&#19968;&#23450;&#20041;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20998;&#23618;&#20998;&#31867;&#27861;&#26469;&#20998;&#31867;&#36825;&#20123;&#26041;&#27861;&#25152;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#35843;&#30740;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#20445;&#35777;&#22270;&#32553;&#20943;&#25216;&#26415;&#25345;&#32493;&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#35770;&#25991;&#21015;&#34920;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world datasets can be naturally represented as graphs, spanning a wide range of domains. However, the increasing complexity and size of graph datasets present significant challenges for analysis and computation. In response, graph reduction techniques have gained prominence for simplifying large graphs while preserving essential properties. In this survey, we aim to provide a comprehensive understanding of graph reduction methods, including graph sparsification, graph coarsening, and graph condensation. Specifically, we establish a unified definition for these methods and introduce a hierarchical taxonomy to categorize the challenges they address. Our survey then systematically reviews the technical details of these methods and emphasizes their practical applications across diverse scenarios. Furthermore, we outline critical research directions to ensure the continued effectiveness of graph reduction techniques, as well as provide a comprehensive paper list at https://github.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#20551;&#26032;&#38395;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#37096;&#32626;&#25581;&#38706;&#32773;&#20256;&#25773;&#30495;&#23454;&#26032;&#38395;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#27169;&#20223;&#23398;&#20064;&#36873;&#25321;&#25581;&#38706;&#32773;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;NAGASIL&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#20551;&#26032;&#38395;&#20943;&#36731;&#20013;&#23398;&#20064;&#26356;&#26377;&#25928;&#30340;&#25581;&#38706;&#32773;&#36873;&#25321;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.03357</link><description>&lt;p&gt;
&#21033;&#29992;&#32593;&#32476;&#25928;&#24212;&#20943;&#36731;&#20551;&#26032;&#38395;&#20256;&#25773;&#65306;&#36890;&#36807;&#33258;&#25105;&#27169;&#20223;&#23398;&#20064;&#36873;&#25321;&#25581;&#38706;&#32773;
&lt;/p&gt;
&lt;p&gt;
Harnessing Network Effect for Fake News Mitigation: Selecting Debunkers via Self-Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#20551;&#26032;&#38395;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#37096;&#32626;&#25581;&#38706;&#32773;&#20256;&#25773;&#30495;&#23454;&#26032;&#38395;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#27169;&#20223;&#23398;&#20064;&#36873;&#25321;&#25581;&#38706;&#32773;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;NAGASIL&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#20551;&#26032;&#38395;&#20943;&#36731;&#20013;&#23398;&#20064;&#26356;&#26377;&#25928;&#30340;&#25581;&#38706;&#32773;&#36873;&#25321;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#37096;&#32626;&#25581;&#38706;&#32773;&#20256;&#25773;&#30495;&#23454;&#26032;&#38395;&#65292;&#26368;&#23567;&#21270;&#20551;&#26032;&#38395;&#23545;&#31038;&#20132;&#32593;&#32476;&#30340;&#24433;&#21709;&#12290;&#36825;&#34987;&#35774;&#23450;&#20026;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#27599;&#20010;&#38454;&#27573;&#36873;&#21462;&#19968;&#20010;&#29992;&#25143;&#20256;&#25773;&#30495;&#23454;&#30340;&#26032;&#38395;&#12290;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#26159;&#19968;&#27425;&#24615;&#22870;&#21169;&#65292;&#22312;&#31038;&#20132;&#32593;&#32476;&#19978;&#20132;&#32455;&#20449;&#24687;&#20256;&#25773;&#20013;&#65292;&#26080;&#27861;&#21306;&#20998;&#21333;&#20010;&#25581;&#38706;&#32773;&#30340;&#36873;&#25321;&#25152;&#24102;&#26469;&#30340;&#8220;&#20928;&#8221;&#25928;&#24212;&#65292;&#21482;&#33021;&#35266;&#23519;&#21040;&#26469;&#33258;&#20943;&#36731;&#21162;&#21147;&#30340;&#38598;&#20307;&#25928;&#26524;&#12290;&#29616;&#26377;&#30340;&#33258;&#25105;&#27169;&#20223;&#23398;&#20064;&#65288;SIL&#65289;&#26041;&#27861;&#22312;&#20174;&#19968;&#27425;&#24615;&#22870;&#21169;&#20013;&#23398;&#20064;&#26041;&#38754;&#26174;&#31034;&#20986;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#22312;&#23454;&#38469;&#30340;&#20551;&#26032;&#38395;&#20943;&#36731;&#24212;&#29992;&#20013;&#30001;&#20110;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#65292;&#19981;&#36866;&#29992;&#12290;&#20026;&#20102;&#23398;&#20064;&#26356;&#26377;&#25928;&#30340;&#20551;&#26032;&#38395;&#20943;&#36731;&#25581;&#38706;&#32773;&#36873;&#25321;&#31574;&#30053;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;NAGASIL - &#22522;&#20110;&#36127;&#37319;&#26679;&#21644;&#29366;&#24577;&#22686;&#24378;&#29983;&#25104;&#23545;&#25239;&#33258;&#25105;&#27169;&#20223;&#23398;&#20064;&#65292;&#23427;&#21253;&#25324;&#20004;&#20010;&#38024;&#23545;&#20551;&#26032;&#38395;&#20943;&#36731;&#30340;&#25913;&#36827;:&#20174;&#36127;&#26679;&#26412;&#20013;&#23398;&#20064;&#21644;&#29366;&#24577;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to minimize the influence of fake news on social networks by deploying debunkers to propagate true news. This is framed as a reinforcement learning problem, where, at each stage, one user is selected to propagate true news. A challenging issue is episodic reward where the "net" effect of selecting individual debunkers cannot be discerned from the interleaving information propagation on social networks, and only the collective effect from mitigation efforts can be observed. Existing Self-Imitation Learning (SIL) methods have shown promise in learning from episodic rewards, but are ill-suited to the real-world application of fake news mitigation because of their poor sample efficiency. To learn a more effective debunker selection policy for fake news mitigation, this study proposes NAGASIL - Negative sampling and state Augmented Generative Adversarial Self-Imitation Learning, which consists of two improvements geared towards fake news mitigation: learning from negative sa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#25512;&#29305;&#24773;&#32490;&#19982;&#29983;&#29289;&#25216;&#26415;&#32929;&#31080;&#24066;&#22330;&#34892;&#20026;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20102;&#31038;&#20132;&#23186;&#20307;&#35805;&#35821;&#23545;&#25237;&#36164;&#32773;&#24773;&#32490;&#21644;&#20915;&#31574;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#24773;&#32490;&#21327;&#21464;&#37327;&#26469;&#25913;&#21892;&#32929;&#24066;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03353</link><description>&lt;p&gt;
&#25512;&#29305;&#23545;&#24066;&#22330;&#36235;&#21183;&#30340;&#24433;&#21709;&#65306;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#24773;&#32490;&#23545;&#29983;&#29289;&#25216;&#26415;&#32929;&#31080;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Tweet Influence on Market Trends: Analyzing the Impact of Social Media Sentiment on Biotech Stocks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#25512;&#29305;&#24773;&#32490;&#19982;&#29983;&#29289;&#25216;&#26415;&#32929;&#31080;&#24066;&#22330;&#34892;&#20026;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20102;&#31038;&#20132;&#23186;&#20307;&#35805;&#35821;&#23545;&#25237;&#36164;&#32773;&#24773;&#32490;&#21644;&#20915;&#31574;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#24773;&#32490;&#21327;&#21464;&#37327;&#26469;&#25913;&#21892;&#32929;&#24066;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#25512;&#29305;&#24773;&#32490;&#22312;&#19981;&#21516;&#31867;&#21035;&#65288;&#26032;&#38395;&#12289;&#20844;&#21496;&#35266;&#28857;&#12289;CEO&#35266;&#28857;&#12289;&#31454;&#20105;&#23545;&#25163;&#35266;&#28857;&#65289;&#19982;&#29983;&#29289;&#25216;&#26415;&#39046;&#22495;&#32929;&#24066;&#34892;&#20026;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#37325;&#28857;&#26159;&#29702;&#35299;&#31038;&#20132;&#23186;&#20307;&#35805;&#35821;&#23545;&#25237;&#36164;&#32773;&#24773;&#32490;&#21644;&#20915;&#31574;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21313;&#23478;&#26368;&#22823;&#12289;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#21046;&#33647;&#20844;&#21496;&#30340;&#21382;&#21490;&#32929;&#24066;&#25968;&#25454;&#65292;&#21516;&#26102;&#32771;&#23519;&#20102;&#19982;COVID-19&#12289;&#30123;&#33495;&#12289;&#36825;&#20123;&#20844;&#21496;&#20197;&#21450;&#23427;&#20204;&#30340;CEO&#30456;&#20851;&#30340;&#25512;&#29305;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;VADER&#24773;&#32490;&#20998;&#26512;&#26041;&#27861;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#25512;&#25991;&#30340;&#24773;&#32490;&#24471;&#20998;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#19982;&#32929;&#24066;&#34920;&#29616;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#37319;&#29992;ARIMA&#65288;&#33258;&#22238;&#24402;&#28369;&#21160;&#24179;&#22343;&#65289;&#21644;VAR&#65288;&#21521;&#37327;&#33258;&#22238;&#24402;&#65289;&#27169;&#22411;&#26469;&#39044;&#27979;&#32929;&#24066;&#34920;&#29616;&#65292;&#24182;&#21152;&#20837;&#24773;&#32490;&#21327;&#21464;&#37327;&#26469;&#25552;&#21319;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#25512;&#29305;&#24773;&#32490;&#12289;&#26032;&#38395;&#12289;&#29983;&#29289;&#25216;&#26415;&#20844;&#21496;&#12289;&#20854;CEO&#21644;&#32929;&#24066;&#34920;&#29616;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the relationship between tweet sentiment across diverse categories: news, company opinions, CEO opinions, competitor opinions, and stock market behavior in the biotechnology sector, with a focus on understanding the impact of social media discourse on investor sentiment and decision-making processes. We analyzed historical stock market data for ten of the largest and most influential pharmaceutical companies alongside Twitter data related to COVID-19, vaccines, the companies, and their respective CEOs. Using VADER sentiment analysis, we examined the sentiment scores of tweets and assessed their relationships with stock market performance. We employed ARIMA (AutoRegressive Integrated Moving Average) and VAR (Vector AutoRegression) models to forecast stock market performance, incorporating sentiment covariates to improve predictions. Our findings revealed a complex interplay between tweet sentiment, news, biotech companies, their CEOs, and stock market performance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32806;&#21512;&#32447;&#24615;&#32422;&#26463;&#30340;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#38646;&#38454;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#21333;&#24490;&#29615;&#31639;&#27861;&#29992;&#20110;&#27714;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#21035;&#20026;O(&#949;^(-2))&#21644;O(&#949;^(-4))&#12290;</title><link>https://arxiv.org/abs/2402.03352</link><description>&lt;p&gt;
&#38754;&#21521;&#20855;&#26377;&#32806;&#21512;&#32447;&#24615;&#32422;&#26463;&#30340;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#38646;&#38454;&#21407;&#22987;&#23545;&#20598;&#20132;&#26367;&#25237;&#24433;&#26799;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zeroth-Order primal-dual Alternating Projection Gradient Algorithms for Nonconvex Minimax Problems with Coupled linear Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32806;&#21512;&#32447;&#24615;&#32422;&#26463;&#30340;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#38646;&#38454;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#21333;&#24490;&#29615;&#31639;&#27861;&#29992;&#20110;&#27714;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#21035;&#20026;O(&#949;^(-2))&#21644;O(&#949;^(-4))&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#35774;&#32622;&#19979;&#20855;&#26377;&#32806;&#21512;&#32447;&#24615;&#32422;&#26463;&#30340;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#38646;&#38454;&#31639;&#27861;&#65292;&#36825;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#20449;&#21495;&#22788;&#29702;&#21644;&#20854;&#20182;&#39046;&#22495;&#20013;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20363;&#22914;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#21644;&#32593;&#32476;&#27969;&#38382;&#39064;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#31561;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#21333;&#24490;&#29615;&#31639;&#27861;&#65292;&#20998;&#21035;&#26159;&#38646;&#38454;&#21407;&#22987;&#23545;&#20598;&#20132;&#26367;&#25237;&#24433;&#26799;&#24230;&#65288;ZO-PDAPG&#65289;&#31639;&#27861;&#21644;&#38646;&#38454;&#27491;&#21017;&#21160;&#37327;&#21407;&#22987;&#23545;&#20598;&#25237;&#24433;&#26799;&#24230;&#31639;&#27861;&#65288;ZO-RMPDPG&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#32806;&#21512;&#32447;&#24615;&#32422;&#26463;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#38750;&#20984;-(&#24378;)&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#12290;&#35777;&#26126;&#20102;&#36825;&#20004;&#20010;&#31639;&#27861;&#33719;&#24471;&#19968;&#20010;&#949;-&#31283;&#23450;&#28857;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#21035;&#20026;O(&#949;^(-2))&#65288;&#23545;&#20110;&#27714;&#35299;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65289;&#21644;O(&#949;^(-4))&#65288;&#23545;&#20110;&#27714;&#35299;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study zeroth-order algorithms for nonconvex minimax problems with coupled linear constraints under the deterministic and stochastic settings, which have attracted wide attention in machine learning, signal processing and many other fields in recent years, e.g., adversarial attacks in resource allocation problems and network flow problems etc. We propose two single-loop algorithms, namely the zero-order primal-dual alternating projected gradient (ZO-PDAPG) algorithm and the zero-order regularized momentum primal-dual projected gradient algorithm (ZO-RMPDPG), for solving deterministic and stochastic nonconvex-(strongly) concave minimax problems with coupled linear constraints. The iteration complexity of the two proposed algorithms to obtain an $\varepsilon$-stationary point are proved to be $\mathcal{O}(\varepsilon ^{-2})$ (resp. $\mathcal{O}(\varepsilon ^{-4})$) for solving nonconvex-strongly concave (resp. nonconvex-concave) minimax problems with coupled linear const
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;AI&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22320;&#29699;&#31185;&#23398;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#20960;&#31181;&#24050;&#22312;&#22320;&#29699;&#31185;&#23398;&#20013;&#20351;&#29992;&#30340;GAI&#27169;&#22411;&#65292;&#21253;&#25324;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#12289;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#31561;&#12290;</title><link>https://arxiv.org/abs/2402.03349</link><description>&lt;p&gt;
&#24403;&#22320;&#29699;&#31185;&#23398;&#36935;&#19978;&#29983;&#25104;AI&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#22522;&#30784;&#12289;&#36235;&#21183;&#21644;&#26410;&#26469;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
When Geoscience Meets Generative AI and Large Language Models: Foundations, Trends, and Future Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03349
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;AI&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22320;&#29699;&#31185;&#23398;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#20960;&#31181;&#24050;&#22312;&#22320;&#29699;&#31185;&#23398;&#20013;&#20351;&#29992;&#30340;GAI&#27169;&#22411;&#65292;&#21253;&#25324;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#12289;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GAI&#65289;&#20195;&#34920;&#30528;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#65292;&#25215;&#35834;&#22312;&#19981;&#21516;&#30340;&#27169;&#24577;&#20013;&#21019;&#36896;&#21512;&#25104;&#25968;&#25454;&#21644;&#36755;&#20986;&#12290; GAI&#26368;&#36817;&#22312;&#29983;&#29289;&#23398;&#12289;&#21307;&#23398;&#12289;&#25945;&#32946;&#12289;&#31435;&#27861;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#37329;&#34701;&#31561;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#12290; &#20026;&#20102;&#23454;&#29616;&#22686;&#24378;&#30340;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#21644;&#21487;&#25345;&#32493;&#24615;&#65292;&#29983;&#25104;AI&#30830;&#23454;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#24046;&#24322;&#21270;&#22240;&#32032;&#65292;&#24182;&#25215;&#35834;&#22312;&#35813;&#39046;&#22495;&#24341;&#36215;&#27169;&#24335;&#36716;&#21464;&#12290; &#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;AI&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22320;&#29699;&#31185;&#23398;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290; &#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#20351;&#24471;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#24212;&#23545;&#19982;&#22320;&#29699;&#31185;&#23398;&#21644;&#22320;&#29699;&#31995;&#32479;&#21160;&#21147;&#23398;&#30456;&#20851;&#30340;&#22810;&#26679;&#21270;&#39044;&#27979;&#38382;&#39064;&#12289;&#27169;&#25311;&#21644;&#22810;&#26631;&#20934;&#20915;&#31574;&#25361;&#25112;&#12290; &#26412;&#32508;&#36848;&#35752;&#35770;&#20102;&#22312;&#22320;&#29699;&#31185;&#23398;&#20013;&#20351;&#29992;&#30340;&#20960;&#31181;GAI&#27169;&#22411;&#65292;&#21253;&#25324;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#12289;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (GAI) represents an emerging field that promises the creation of synthetic data and outputs in different modalities. GAI has recently shown impressive results across a large spectrum of applications ranging from biology, medicine, education, legislation, computer science, and finance. As one strives for enhanced safety, efficiency, and sustainability, generative AI indeed emerges as a key differentiator and promises a paradigm shift in the field. This paper explores the potential applications of generative AI and large language models in geoscience. The recent developments in the field of machine learning and deep learning have enabled the generative model's utility for tackling diverse prediction problems, simulation, and multi-criteria decision-making challenges related to geoscience and Earth system dynamics. This survey discusses several GAI models that have been used in geoscience comprising generative adversarial networks (GANs), physics-informe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;MEG&#24212;&#29992;&#20013;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#39062;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#31216;&#20026;&#28151;&#21512;&#27169;&#22411;&#26031;&#33922;&#24343;&#36866;&#24212;&#65288;MSA&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#26631;&#35760;&#25968;&#25454;&#24314;&#31435;&#20102;&#31561;&#25928;&#20449;&#21495;&#26041;&#24046;&#30340;&#25104;&#23545;&#23545;&#24212;&#20851;&#31995;&#20197;&#30830;&#20445;&#26377;&#25928;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#22312;&#31070;&#32463;&#31185;&#23398;&#38382;&#39064;&#20013;&#65292;MSA&#22312;&#20351;&#29992;&#33041;&#30913;&#22270;&#36827;&#34892;&#33041;&#40836;&#22238;&#24402;&#26102;&#34920;&#29616;&#20248;&#20110;&#26368;&#36817;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03345</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#21327;&#26041;&#24046;&#30697;&#38453;&#23545;&#40784;&#36890;&#36807;&#26031;&#33922;&#24343;&#30697;&#38453;&#20272;&#35745;&#22312;MEG&#24212;&#29992;&#20013;
&lt;/p&gt;
&lt;p&gt;
Weakly supervised covariance matrices alignment through Stiefel matrices estimation for MEG applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;MEG&#24212;&#29992;&#20013;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26032;&#39062;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#31216;&#20026;&#28151;&#21512;&#27169;&#22411;&#26031;&#33922;&#24343;&#36866;&#24212;&#65288;MSA&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#26631;&#35760;&#25968;&#25454;&#24314;&#31435;&#20102;&#31561;&#25928;&#20449;&#21495;&#26041;&#24046;&#30340;&#25104;&#23545;&#23545;&#24212;&#20851;&#31995;&#20197;&#30830;&#20445;&#26377;&#25928;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#22312;&#31070;&#32463;&#31185;&#23398;&#38382;&#39064;&#20013;&#65292;MSA&#22312;&#20351;&#29992;&#33041;&#30913;&#22270;&#36827;&#34892;&#33041;&#40836;&#22238;&#24402;&#26102;&#34920;&#29616;&#20248;&#20110;&#26368;&#36817;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#28151;&#21512;&#27169;&#22411;&#26031;&#33922;&#24343;&#36866;&#24212;&#65288;MSA&#65289;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#24039;&#65292;&#29305;&#21035;&#35299;&#20915;&#20102;&#30446;&#26631;&#25968;&#25454;&#38598;&#20013;&#26377;&#38480;&#26631;&#35760;&#20449;&#21495;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#21033;&#29992;&#39046;&#22495;&#30456;&#20851;&#30340;&#28151;&#21512;&#27169;&#22411;&#21644;&#26368;&#20248;&#20256;&#36755;&#39046;&#22495;&#33258;&#36866;&#24212;&#20551;&#35774;&#65292;&#25105;&#20204;&#21033;&#29992;&#30446;&#26631;&#22495;&#20013;&#20016;&#23500;&#30340;&#26080;&#26631;&#35760;&#25968;&#25454;&#65292;&#36890;&#36807;&#24314;&#31435;&#31561;&#25928;&#20449;&#21495;&#26041;&#24046;&#20043;&#38388;&#30340;&#25104;&#23545;&#23545;&#24212;&#20851;&#31995;&#65292;&#30830;&#20445;&#20102;&#26377;&#25928;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#20026;&#20102;&#20174;&#35266;&#27979;&#20449;&#21495;&#21327;&#26041;&#24046;&#30340;&#40654;&#26364;&#34920;&#31034;&#20013;&#24674;&#22797;&#22522;&#30784;&#20449;&#21495;&#26041;&#24046;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#35782;&#21035;&#20851;&#38190;&#30340;&#26031;&#33922;&#24343;&#30697;&#38453;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#25104;&#26412;&#20989;&#25968;&#65292;&#21516;&#26102;&#23398;&#20064;&#36825;&#20123;&#30697;&#38453;&#12289;&#25104;&#23545;&#22495;&#20851;&#31995;&#20197;&#21450;&#26681;&#25454;&#20219;&#21153;&#30340;&#39044;&#27979;&#22120;&#12289;&#20998;&#31867;&#22120;&#25110;&#22238;&#24402;&#22120;&#12290;&#24212;&#29992;&#20110;&#31070;&#32463;&#31185;&#23398;&#38382;&#39064;&#20013;&#65292;MSA&#22312;&#20351;&#29992;&#33041;&#30913;&#22270;&#36827;&#34892;&#20219;&#21153;&#21464;&#21270;&#30340;&#33041;&#40836;&#22238;&#24402;&#20013;&#20248;&#20110;&#26368;&#36817;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel domain adaptation technique for time series data, called Mixing model Stiefel Adaptation (MSA), specifically addressing the challenge of limited labeled signals in the target dataset. Leveraging a domain-dependent mixing model and the optimal transport domain adaptation assumption, we exploit abundant unlabeled data in the target domain to ensure effective prediction by establishing pairwise correspondence with equivalent signal variances between domains. Theoretical foundations are laid for identifying crucial Stiefel matrices, essential for recovering underlying signal variances from a Riemannian representation of observed signal covariances. We propose an integrated cost function that simultaneously learns these matrices, pairwise domain relationships, and a predictor, classifier, or regressor, depending on the task. Applied to neuroscience problems, MSA outperforms recent methods in brain-age regression with task variations using magnetoencephalography
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MADRL&#30340;&#26080;&#20154;&#26426;&#36712;&#36857;&#35774;&#35745;&#26041;&#27861;&#65292;&#37319;&#29992;&#25490;&#21517;&#20108;&#36827;&#21046;&#25513;&#30721;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;&#26080;&#20154;&#26426;&#20043;&#38388;&#30340;&#26080;&#30896;&#25758;&#36335;&#24452;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.03342</link><description>&lt;p&gt;
&#22522;&#20110;MADRL&#30340;&#36710;&#32852;&#32593;&#20013;&#20855;&#26377;&#38450;&#30896;&#25758;&#26426;&#21046;&#30340;&#26080;&#20154;&#26426;&#36712;&#36857;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
MADRL-based UAVs Trajectory Design with Anti-Collision Mechanism in Vehicular Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MADRL&#30340;&#26080;&#20154;&#26426;&#36712;&#36857;&#35774;&#35745;&#26041;&#27861;&#65292;&#37319;&#29992;&#25490;&#21517;&#20108;&#36827;&#21046;&#25513;&#30721;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;&#26080;&#20154;&#26426;&#20043;&#38388;&#30340;&#26080;&#30896;&#25758;&#36335;&#24452;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21363;&#23558;&#21040;&#26469;&#30340;6G&#32593;&#32476;&#20013;&#65292;&#26080;&#20154;&#26426;&#20316;&#20026;&#31227;&#21160;&#22522;&#31449;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#36710;&#36742;&#21040;&#19968;&#20999;&#65288;V2X&#65289;&#24212;&#29992;&#65292;&#23558;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#22810;&#20010;&#26080;&#20154;&#26426;&#30340;&#36712;&#36857;&#35774;&#35745;&#65292;&#20849;&#21516;&#20026;&#21516;&#19968;&#21306;&#22495;&#25552;&#20379;&#26381;&#21153;&#12290;&#36825;&#31181;&#32852;&#21512;&#36712;&#36857;&#35774;&#35745;&#21487;&#20197;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;MADRL&#65289;&#31639;&#27861;&#36827;&#34892;&#65292;&#20294;&#30830;&#20445;&#26080;&#20154;&#26426;&#20043;&#38388;&#30340;&#26080;&#30896;&#25758;&#36335;&#24452;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20256;&#32479;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26045;&#21152;&#39640;&#24809;&#32602;&#20197;&#36991;&#20813;&#19981;&#23433;&#20840;&#24773;&#20917;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#35777;&#26126;&#26159;&#26080;&#25928;&#30340;&#65292;&#32780;&#20108;&#36827;&#21046;&#25513;&#30721;&#21487;&#20197;&#29992;&#26469;&#38480;&#21046;&#19981;&#23433;&#20840;&#30340;&#34892;&#21160;&#65292;&#20294;&#26159;&#31616;&#21333;&#24212;&#29992;&#20110;&#25152;&#26377;&#26234;&#33021;&#20307;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#35299;&#21644;&#20302;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31561;&#32423;&#30340;&#20108;&#36827;&#21046;&#25513;&#30721;&#26041;&#27861;&#12290;&#36739;&#39640;&#31561;&#32423;&#30340;&#26080;&#20154;&#26426;&#31227;&#21160;&#20248;&#21270;&#65292;&#36739;&#20302;&#31561;&#32423;&#30340;&#26080;&#20154;&#26426;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#23450;&#20041;&#20102;&#25913;&#36827;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
In upcoming 6G networks, unmanned aerial vehicles (UAVs) are expected to play a fundamental role by acting as mobile base stations, particularly for demanding vehicle-to-everything (V2X) applications. In this scenario, one of the most challenging problems is the design of trajectories for multiple UAVs, cooperatively serving the same area. Such joint trajectory design can be performed using multi-agent deep reinforcement learning (MADRL) algorithms, but ensuring collision-free paths among UAVs becomes a critical challenge. Traditional methods involve imposing high penalties during training to discourage unsafe conditions, but these can be proven to be ineffective, whereas binary masks can be used to restrict unsafe actions, but naively applying them to all agents can lead to suboptimal solutions and inefficiencies. To address these issues, we propose a rank-based binary masking approach. Higher-ranked UAVs move optimally, while lower-ranked UAVs use this information to define improved 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#29305;&#24449;&#21521;&#37327;&#36827;&#34892;&#29305;&#23450;&#25490;&#21015;&#65292;&#24212;&#29992;CNN-DRL&#26041;&#27861;&#20110;&#37329;&#34701;&#25968;&#25454;&#20013;&#65292;&#22312;&#22238;&#25253;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;</title><link>https://arxiv.org/abs/2402.03338</link><description>&lt;p&gt;
&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#65292;&#24212;&#29992;&#20855;&#26377;&#38543;&#26426;&#29305;&#24449;&#30340;CNN-DRL&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CNN-DRL with Shuffled Features in Finance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#29305;&#24449;&#21521;&#37327;&#36827;&#34892;&#29305;&#23450;&#25490;&#21015;&#65292;&#24212;&#29992;CNN-DRL&#26041;&#27861;&#20110;&#37329;&#34701;&#25968;&#25454;&#20013;&#65292;&#22312;&#22238;&#25253;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20808;&#21069;&#30340;&#26041;&#27861;&#20013;&#65292;&#35266;&#23519;&#21040;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#24212;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37329;&#34701;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#22238;&#25253;&#12290;&#26412;&#30740;&#31350;&#24212;&#29992;&#20102;&#29305;&#23450;&#30340;&#25490;&#21015;&#26041;&#24335;&#26469;&#23545;&#29305;&#24449;&#21521;&#37327;&#36827;&#34892;&#22788;&#29702;&#65292;&#20174;&#32780;&#29983;&#25104;&#19968;&#20010;CNN&#30697;&#38453;&#65292;&#23558;&#26356;&#30456;&#20851;&#30340;&#29305;&#24449;&#25918;&#32622;&#22312;&#38752;&#36817;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#23454;&#39564;&#35780;&#20272;&#26126;&#30830;&#22320;&#35777;&#26126;&#20102;&#22238;&#25253;&#30340;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
In prior methods, it was observed that the application of Convolutional Neural Networks agent in Deep Reinforcement Learning to financial data resulted in an enhanced reward. In this study, a specific permutation was applied to the feature vector, thereby generating a CNN matrix that strategically positions more pertinent features in close proximity. Our comprehensive experimental evaluations unequivocally demonstrate a substantial enhancement in reward attainment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#24320;&#21457;&#34394;&#25311;&#28023;&#27915;&#29615;&#22659;&#27169;&#25311;&#30495;&#23454;&#23454;&#39564;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36827;&#34892;&#33258;&#20027;&#23548;&#33322;&#21644;&#25511;&#21046;&#30340;&#20851;&#38190;&#29305;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#35752;&#35770;&#20102;&#21019;&#24314;&#22522;&#20110;&#30495;&#23454;&#26426;&#22120;&#20154;&#24070;&#33337;&#30340;&#21151;&#33021;&#24615;&#25968;&#23383;&#23402;&#29983;&#25152;&#38656;&#30340;&#24314;&#27169;&#21644;&#23454;&#26045;&#27493;&#39588;&#20197;&#21450;&#25361;&#25112;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#24320;&#21457;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#23548;&#33322;&#31639;&#27861;&#22312;&#30495;&#23454;&#33337;&#21482;&#19978;&#30340;&#24212;&#29992;&#20855;&#26377;&#30452;&#25509;&#30340;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.03337</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#24070;&#33337;&#65306;&#27169;&#25311;&#22120;&#21644;&#21021;&#27493;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Reinforcement-learning robotic sailboats: simulator and preliminary results
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24320;&#21457;&#34394;&#25311;&#28023;&#27915;&#29615;&#22659;&#27169;&#25311;&#30495;&#23454;&#23454;&#39564;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36827;&#34892;&#33258;&#20027;&#23548;&#33322;&#21644;&#25511;&#21046;&#30340;&#20851;&#38190;&#29305;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#35752;&#35770;&#20102;&#21019;&#24314;&#22522;&#20110;&#30495;&#23454;&#26426;&#22120;&#20154;&#24070;&#33337;&#30340;&#21151;&#33021;&#24615;&#25968;&#23383;&#23402;&#29983;&#25152;&#38656;&#30340;&#24314;&#27169;&#21644;&#23454;&#26045;&#27493;&#39588;&#20197;&#21450;&#25361;&#25112;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#24320;&#21457;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#23548;&#33322;&#31639;&#27861;&#22312;&#30495;&#23454;&#33337;&#21482;&#19978;&#30340;&#24212;&#29992;&#20855;&#26377;&#30452;&#25509;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30528;&#37325;&#35299;&#20915;&#22312;&#20351;&#29992;&#26080;&#20154;&#34920;&#38754;&#33337;&#21482;(Unmanned Surface Vehicles, USV)&#30340;&#25968;&#23383;&#23402;&#29983;&#24320;&#23637;&#34394;&#25311;&#28023;&#27915;&#29615;&#22659;&#26469;&#22797;&#21046;&#30495;&#23454;&#23454;&#39564;&#26102;&#25152;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#26500;&#24314;&#34394;&#25311;&#19990;&#30028;&#30340;&#20851;&#38190;&#29305;&#24615;&#65292;&#32771;&#34385;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;(RL)&#26234;&#33021;&#20307;&#36827;&#34892;&#33258;&#20027;&#23548;&#33322;&#21644;&#25511;&#21046;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#20027;&#35201;&#38382;&#39064;&#28041;&#21450;&#27169;&#25311;&#26041;&#31243;&#30340;&#23450;&#20041;(&#29289;&#29702;&#21644;&#25968;&#23398;)&#12289;&#23427;&#20204;&#30340;&#26377;&#25928;&#23454;&#26045;&#20197;&#21450;&#22914;&#20309;&#23558;&#29992;&#20110;RL&#30340;&#27169;&#25311;&#25511;&#21046;&#21644;&#24863;&#30693;&#31574;&#30053;(&#20256;&#24863;&#22120;)&#21253;&#25324;&#36827;&#26469;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#30495;&#23454;&#26426;&#22120;&#20154;&#24070;&#33337;&#21019;&#24314;&#21151;&#33021;&#24615;&#25968;&#23383;&#23402;&#29983;&#25152;&#38656;&#30340;&#24314;&#27169;&#12289;&#23454;&#26045;&#27493;&#39588;&#21644;&#25361;&#25112;&#12290;&#35813;&#24212;&#29992;&#23558;&#31435;&#21363;&#29992;&#20110;&#24320;&#21457;&#22522;&#20110;RL&#30340;&#23548;&#33322;&#31639;&#27861;&#65292;&#20197;&#24212;&#29992;&#20110;&#30495;&#23454;&#33337;&#21482;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work focuses on the main challenges and problems in developing a virtual oceanic environment reproducing real experiments using Unmanned Surface Vehicles (USV) digital twins. We introduce the key features for building virtual worlds, considering using Reinforcement Learning (RL) agents for autonomous navigation and control. With this in mind, the main problems concern the definition of the simulation equations (physics and mathematics), their effective implementation, and how to include strategies for simulated control and perception (sensors) to be used with RL. We present the modeling, implementation steps, and challenges required to create a functional digital twin based on a real robotic sailing vessel. The application is immediate for developing navigation algorithms based on RL to be applied on real boats.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#21019;&#26032;&#24615;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;Cyclic NNs&#65289;&#65292;&#21487;&#20197;&#27169;&#25311;&#29983;&#29289;&#26234;&#33021;&#31995;&#32479;&#30340;&#28789;&#27963;&#21160;&#24577;&#22270;&#24418;&#29305;&#24615;&#65292;&#24182;&#22312;&#24191;&#27867;&#27979;&#35797;&#30340;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03332</link><description>&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Cyclic Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#21019;&#26032;&#24615;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;Cyclic NNs&#65289;&#65292;&#21487;&#20197;&#27169;&#25311;&#29983;&#29289;&#26234;&#33021;&#31995;&#32479;&#30340;&#28789;&#27963;&#21160;&#24577;&#22270;&#24418;&#29305;&#24615;&#65292;&#24182;&#22312;&#24191;&#27867;&#27979;&#35797;&#30340;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#31572;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#35774;&#35745;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#25105;&#20204;&#19981;&#38656;&#35201;&#25353;&#39034;&#24207;&#36880;&#23618;&#26500;&#24314;ANN&#26469;&#20445;&#35777;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#23646;&#24615;&#12290;&#21463;&#29983;&#29289;&#26234;&#33021;&#65288;BI&#65289;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#31070;&#32463;&#20803;&#24418;&#25104;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#12289;&#22270;&#24418;&#32467;&#26500;&#30340;&#32593;&#32476;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20855;&#26377;&#24320;&#21019;&#24615;&#24847;&#20041;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;Cyclic NNs&#65289;&#12290;&#23427;&#27169;&#25311;&#20102;&#29983;&#29289;&#31070;&#32463;&#31995;&#32479;&#28789;&#27963;&#21160;&#24577;&#30340;&#22270;&#24418;&#29305;&#24615;&#65292;&#20801;&#35768;&#31070;&#32463;&#20803;&#22312;&#21253;&#25324;&#29615;&#36335;&#22312;&#20869;&#30340;&#20219;&#20309;&#22270;&#24418;&#32467;&#26500;&#20013;&#36827;&#34892;&#36830;&#25509;&#12290;&#19982;&#24403;&#21069;ANN&#30340;DAG&#32467;&#26500;&#30456;&#27604;&#65292;&#36825;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#23637;&#20102;&#22522;&#20110;&#36825;&#31181;&#26032;&#35774;&#35745;&#33539; paradigm&#30340;&#39318;&#20010;&#35814;&#32454;&#27169;&#22411;&#8212;&#8212;&#22270;&#35206;&#30422;&#22810;&#23618;&#24863;&#30693;&#26426;&#12290;&#36890;&#36807;&#22312;&#24191;&#27867;&#27979;&#35797;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#22823;&#22810;&#25968;&#19968;&#33324;&#24773;&#20917;&#19979;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#20351;&#29992;&#21069;&#21521;&#35757;&#32451;&#31639;&#27861;&#65288;FF&#65289;&#35777;&#26126;&#20102;&#23427;&#30456;&#23545;&#20110;&#24403;&#21069;&#30340;BP&#35757;&#32451;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#38416;&#36848;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper answers a fundamental question in artificial neural network (ANN) design: We do not need to build ANNs layer-by-layer sequentially to guarantee the Directed Acyclic Graph (DAG) property. Drawing inspiration from biological intelligence (BI), where neurons form a complex, graph-structured network, we introduce the groundbreaking Cyclic Neural Networks (Cyclic NNs). It emulates the flexible and dynamic graph nature of biological neural systems, allowing neuron connections in any graph-like structure, including cycles. This offers greater adaptability compared to the DAG structure of current ANNs. We further develop the Graph Over Multi-layer Perceptron, which is the first detailed model based on this new design paradigm. Experimental validation of the Cyclic NN's advantages on widely tested datasets in most generalized cases, demonstrating its superiority over current BP training methods through the use of a forward-forward (FF) training algorithm. This research illustrates a 
&lt;/p&gt;</description></item><item><title>Slot Structured World Models&#32467;&#21512;&#20102;&#22522;&#20110;Slot&#27880;&#24847;&#21147;&#30340;&#8220;&#23545;&#35937;&#20026;&#20013;&#24515;&#8221;&#32534;&#30721;&#22120;&#21644;&#22522;&#20110;&#28508;&#22312;&#22270;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22810;&#27493;&#39044;&#27979;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26080;&#27861;&#25552;&#21462;&#8220;&#23545;&#35937;&#20026;&#20013;&#24515;&#8221;&#34920;&#31034;&#21644;&#21306;&#20998;&#22806;&#35266;&#30456;&#20284;&#30340;&#22810;&#20010;&#23545;&#35937;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03326</link><description>&lt;p&gt;
Slot&#32467;&#26500;&#21270;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Slot Structured World Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03326
&lt;/p&gt;
&lt;p&gt;
Slot Structured World Models&#32467;&#21512;&#20102;&#22522;&#20110;Slot&#27880;&#24847;&#21147;&#30340;&#8220;&#23545;&#35937;&#20026;&#20013;&#24515;&#8221;&#32534;&#30721;&#22120;&#21644;&#22522;&#20110;&#28508;&#22312;&#22270;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22810;&#27493;&#39044;&#27979;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26080;&#27861;&#25552;&#21462;&#8220;&#23545;&#35937;&#20026;&#20013;&#24515;&#8221;&#34920;&#31034;&#21644;&#21306;&#20998;&#22806;&#35266;&#30456;&#20284;&#30340;&#22810;&#20010;&#23545;&#35937;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#21644;&#25512;&#29702;&#20010;&#20307;&#23545;&#35937;&#21450;&#20854;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#26159;&#26500;&#24314;&#26234;&#33021;&#20154;&#24037;&#31995;&#32479;&#30340;&#30446;&#26631;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20351;&#29992;&#21069;&#39304;&#32534;&#30721;&#22120;&#26469;&#25552;&#21462;&#23545;&#35937;&#23884;&#20837;&#65292;&#20351;&#29992;&#28508;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#36825;&#20123;&#23545;&#35937;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#21069;&#39304;&#32534;&#30721;&#22120;&#26080;&#27861;&#25552;&#21462;&#8220;&#23545;&#35937;&#20026;&#20013;&#24515;&#8221;&#30340;&#34920;&#31034;&#65292;&#20063;&#26080;&#27861;&#21306;&#20998;&#22806;&#35266;&#30456;&#20284;&#30340;&#22810;&#20010;&#23545;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;Slot&#32467;&#26500;&#21270;&#19990;&#30028;&#27169;&#22411;&#8221;&#65288;SSWM&#65289;&#30340;&#19990;&#30028;&#27169;&#22411;&#31867;&#65292;&#23427;&#23558;&#22522;&#20110;Slot&#27880;&#24847;&#21147;&#30340;&#8220;&#23545;&#35937;&#20026;&#20013;&#24515;&#8221;&#32534;&#30721;&#22120;&#19982;&#22522;&#20110;&#28508;&#22312;&#22270;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#22312;Spriteworld&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35813;&#27979;&#35797;&#20351;&#29992;&#31616;&#21333;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#35268;&#21017;&#65292;&#22312;&#25805;&#20316;&#26465;&#20214;&#19979;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#65288;&#22810;&#27493;&#65289;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25152;&#26377;&#22797;&#29616;&#35770;&#25991;&#23454;&#39564;&#30340;&#20195;&#30721;&#37117;&#21487;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to perceive and reason about individual objects and their interactions is a goal to be achieved for building intelligent artificial systems. State-of-the-art approaches use a feedforward encoder to extract object embeddings and a latent graph neural network to model the interaction between these object embeddings. However, the feedforward encoder can not extract {\it object-centric} representations, nor can it disentangle multiple objects with similar appearance. To solve these issues, we introduce {\it Slot Structured World Models} (SSWM), a class of world models that combines an {\it object-centric} encoder (based on Slot Attention) with a latent graph-based dynamics model. We evaluate our method in the Spriteworld benchmark with simple rules of physical interaction, where Slot Structured World Models consistently outperform baselines on a range of (multi-step) prediction tasks with action-conditional object interactions. All code to reproduce paper experiments is availab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26631;&#35760;&#30340;&#28304;&#39046;&#22495;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#37096;&#32626;&#21040;&#20998;&#24067;&#19981;&#21516;&#30340;&#30446;&#26631;&#39046;&#22495;&#26102;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36830;&#25509;&#24310;&#36831;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#23450;&#21521;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03325</link><description>&lt;p&gt;
&#36830;&#25509;&#24310;&#36831;&#65306;&#21033;&#29992;&#23450;&#21521;&#22686;&#24378;&#26041;&#27861;&#25552;&#39640;&#40065;&#26834;&#24615;&#30340;&#24494;&#35843;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Connect Later: Improving Fine-tuning for Robustness with Targeted Augmentations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26631;&#35760;&#30340;&#28304;&#39046;&#22495;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#37096;&#32626;&#21040;&#20998;&#24067;&#19981;&#21516;&#30340;&#30446;&#26631;&#39046;&#22495;&#26102;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36830;&#25509;&#24310;&#36831;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#23450;&#21521;&#22686;&#24378;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26631;&#35760;&#30340;&#28304;&#39046;&#22495;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;&#37326;&#29983;&#21160;&#29289;&#30456;&#26426;&#38519;&#38449;&#30340;&#26631;&#35760;&#22270;&#20687;&#65289;&#36890;&#24120;&#22312;&#37096;&#32626;&#21040;&#20998;&#24067;&#19981;&#21516;&#30340;&#30446;&#26631;&#39046;&#22495;&#65288;&#20363;&#22914;&#26032;&#30340;&#30456;&#26426;&#38519;&#38449;&#20301;&#32622;&#30340;&#22270;&#20687;&#65289;&#26102;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#22312;&#23384;&#22312;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#30340;&#22495;&#36866;&#24212;&#35774;&#32622;&#20013;&#65292;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65288;&#20363;&#22914;&#36974;&#34109;&#33258;&#32534;&#30721;&#25110;&#23545;&#27604;&#23398;&#20064;&#65289;&#26159;&#32531;&#35299;&#24615;&#33021;&#19979;&#38477;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#39044;&#35757;&#32451;&#21487;&#20197;&#36890;&#36807;&#23558;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#30456;&#36830;&#25509;&#30340;&#36890;&#29992;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65288;&#20363;&#22914;&#36974;&#34109;&#25110;&#21098;&#35009;&#65289;&#26469;&#25552;&#39640;&#20998;&#24067;&#19981;&#21516;&#30340;&#38169;&#35823;&#29575;&#65292;&#21363;&#20351;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#20004;&#20010;&#39046;&#22495;&#30456;&#24046;&#24456;&#36828;&#12290;&#26412;&#25991;&#36890;&#36807;&#30495;&#23454;&#20219;&#21153;&#23637;&#31034;&#20102;&#22312;&#39044;&#35757;&#32451;&#21518;&#36827;&#34892;&#26631;&#20934;&#24494;&#35843;&#24182;&#19981;&#33021;&#25345;&#32493;&#25913;&#21892;&#20998;&#24067;&#19981;&#21516;&#30340;&#38169;&#35823;&#29575;&#65292;&#30456;&#27604;&#22312;&#26631;&#35760;&#30340;&#28304;&#25968;&#25454;&#19978;&#20174;&#22836;&#35757;&#32451;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;&#39044;&#35757;&#32451;&#26469;&#24212;&#23545;&#20998;&#24067;&#36716;&#21464;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36830;&#25509;&#24310;&#36831;&#65288;Connect Later&#65289;&#65306;&#22312;&#20351;&#29992;&#36890;&#29992;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#39044;&#35757;&#32451;&#21518;&#65292;&#29992;&#22522;&#20110;&#23545;&#30446;&#26631;&#39046;&#22495;&#20102;&#35299;&#30340;&#23450;&#21521;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models trained on a labeled source domain (e.g., labeled images from wildlife camera traps) often generalize poorly when deployed on an out-of-distribution (OOD) target domain (e.g., images from new camera trap locations). In the domain adaptation setting where unlabeled target data is available, self-supervised pretraining (e.g., masked autoencoding or contrastive learning) is a promising method to mitigate this performance drop. Pretraining improves OOD error when the generic data augmentations used (e.g., masking or cropping) connect the source and target domains, which may be far apart in the input space. In this paper, we show on real-world tasks that standard fine-tuning after pretraining does not consistently improve OOD error over simply training from scratch on labeled source data. To better leverage pretraining for distribution shifts, we propose Connect Later: after pretraining with generic augmentations, fine-tune with targeted augmentations designed with knowledge of the d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SpecFormer&#65292;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#22855;&#24322;&#20540;&#24809;&#32602;&#26469;&#22686;&#24378;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#38887;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;Lipschitz&#36793;&#30028;&#21644;&#26368;&#22823;&#22855;&#24322;&#20540;&#24809;&#32602;&#26041;&#27861;&#65288;MSVP&#65289;&#65292;&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;&#27880;&#24847;&#21147;&#26435;&#37325;&#30697;&#38453;&#30340;&#35889;&#33539;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.03317</link><description>&lt;p&gt;
SpecFormer&#65306;&#36890;&#36807;&#26368;&#22823;&#22855;&#24322;&#20540;&#24809;&#32602;&#26469;&#20445;&#25252;&#35270;&#35273;Transformer&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
SpecFormer: Guarding Vision Transformer Robustness via Maximum Singular Value Penalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SpecFormer&#65292;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#22855;&#24322;&#20540;&#24809;&#32602;&#26469;&#22686;&#24378;&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#38887;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;Lipschitz&#36793;&#30028;&#21644;&#26368;&#22823;&#22855;&#24322;&#20540;&#24809;&#32602;&#26041;&#27861;&#65288;MSVP&#65289;&#65292;&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;&#27880;&#24847;&#21147;&#26435;&#37325;&#30697;&#38453;&#30340;&#35889;&#33539;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#22240;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#32780;&#25104;&#20026;&#24191;&#27867;&#20351;&#29992;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#39318;&#36873;&#12290;&#28982;&#32780;&#65292;&#20854;&#24191;&#27867;&#24212;&#29992;&#24341;&#36215;&#20102;&#23545;&#38754;&#23545;&#24694;&#24847;&#25915;&#20987;&#26102;&#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#32463;&#39564;&#35843;&#25972;&#65292;&#32570;&#20047;&#26126;&#30830;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;SpecFormer&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#35813;&#26041;&#27861;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22686;&#24378;ViTs&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#38887;&#24615;&#65292;&#24182;&#24471;&#21040;&#20102;&#20180;&#32454;&#25512;&#23548;&#30340;&#29702;&#35770;&#20445;&#35777;&#30340;&#25903;&#25345;&#12290;&#25105;&#20204;&#20026;&#33258;&#27880;&#24847;&#23618;&#24314;&#31435;&#20102;&#26412;&#22320;Lipschitz&#36793;&#30028;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#26368;&#22823;&#22855;&#24322;&#20540;&#24809;&#32602;&#65288;MSVP&#65289;&#65292;&#20197;&#31934;&#30830;&#25511;&#21046;&#36825;&#20123;&#36793;&#30028;&#12290;&#25105;&#20204;&#20351;&#29992;&#24130;&#36845;&#20195;&#26041;&#27861;&#23558;MSVP&#26080;&#32541;&#38598;&#25104;&#21040;ViTs&#30340;&#27880;&#24847;&#21147;&#23618;&#20013;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#20462;&#25913;&#21518;&#30340;&#27169;&#22411;SpecFormer&#26377;&#25928;&#22320;&#38477;&#20302;&#20102;&#27880;&#24847;&#21147;&#26435;&#37325;&#30697;&#38453;&#30340;&#35889;&#33539;&#25968;&#65292;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have gained prominence as a preferred choice for a wide range of computer vision tasks due to their exceptional performance. However, their widespread adoption has raised concerns about security in the face of malicious attacks. Most existing methods rely on empirical adjustments during the training process, lacking a clear theoretical foundation. In this study, we address this gap by introducing SpecFormer, specifically designed to enhance ViTs' resilience against adversarial attacks, with support from carefully derived theoretical guarantees. We establish local Lipschitz bounds for the self-attention layer and introduce a novel approach, Maximum Singular Value Penalization (MSVP), to attain precise control over these bounds. We seamlessly integrate MSVP into ViTs' attention layers, using the power iteration method for enhanced computational efficiency. The modified model, SpecFormer, effectively reduces the spectral norms of attention weight matrices, there
&lt;/p&gt;</description></item><item><title>DeepSeekMath&#26159;&#19968;&#31181;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#25552;&#21319;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#31454;&#36187;&#32423;&#21035;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03300</link><description>&lt;p&gt;
DeepSeekMath: &#23558;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#25512;&#21521;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03300
&lt;/p&gt;
&lt;p&gt;
DeepSeekMath&#26159;&#19968;&#31181;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#25552;&#21319;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#31454;&#36187;&#32423;&#21035;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#25512;&#29702;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#32467;&#26500;&#21270;&#30340;&#29305;&#24615;&#65292;&#23545;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DeepSeekMath 7B&#65292;&#23427;&#22312;Common Crawl&#20013;&#33719;&#21462;&#20102;120B&#20010;&#19982;&#25968;&#23398;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24182;&#32467;&#21512;&#20102;&#33258;&#28982;&#35821;&#35328;&#21644;&#20195;&#30721;&#25968;&#25454;&#26469;&#32487;&#32493;&#39044;&#35757;&#32451;DeepSeek-Coder-Base-v1.5 7B&#12290;DeepSeekMath 7B&#22312;&#31454;&#36187;&#32423;&#21035;&#30340;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;51.7%&#30340;&#20998;&#25968;&#65292;&#26080;&#38656;&#20381;&#36182;&#22806;&#37096;&#24037;&#20855;&#21253;&#21644;&#25237;&#31080;&#25216;&#26415;&#65292;&#25509;&#36817;&#20102;Gemini-Ultra&#21644;GPT-4&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;DeepSeekMath 7B&#30340;&#33258;&#19968;&#33268;&#24615;&#22312;MATH&#19978;&#30340;64&#20010;&#26679;&#26412;&#20013;&#36798;&#21040;&#20102;60.9%&#30340;&#20998;&#25968;&#12290;DeepSeekMath&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#24402;&#22240;&#20110;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25968;&#25454;&#36873;&#25321;&#31649;&#36947;&#20805;&#20998;&#21033;&#29992;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#32593;&#32476;&#25968;&#25454;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32676;&#20307;&#30456;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;GRPO&#65289;&#65292;&#36825;&#26159;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#30340;&#19968;&#20010;&#21464;&#20307;&#65292;&#21487;&#20197;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities w
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#23398;&#20064;&#35270;&#39057;&#32534;&#30721;&#22120;&#65292;&#20351;&#29992;800&#20010;&#21442;&#25968;&#21644;900&#27425;&#20056;&#27861;&#26469;&#23454;&#29616;&#20302;&#35299;&#30721;&#22797;&#26434;&#24230;&#12290;&#35813;&#32534;&#30721;&#22120;&#22312;&#21387;&#32553;&#35270;&#39057;&#26102;&#33021;&#22815;&#21033;&#29992;&#26102;&#38388;&#20887;&#20313;&#65292;&#24182;&#22312;&#25509;&#36817;AVC&#30340;&#36895;&#29575;&#22833;&#30495;&#26465;&#20214;&#19979;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#36807;&#25311;&#21512;&#32534;&#35299;&#30721;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.03179</link><description>&lt;p&gt;
Cool-chic&#35270;&#39057;&#65306;&#36890;&#36807;800&#20010;&#21442;&#25968;&#23398;&#20064;&#35270;&#39057;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Cool-chic video: Learned video coding with 800 parameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03179
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#23398;&#20064;&#35270;&#39057;&#32534;&#30721;&#22120;&#65292;&#20351;&#29992;800&#20010;&#21442;&#25968;&#21644;900&#27425;&#20056;&#27861;&#26469;&#23454;&#29616;&#20302;&#35299;&#30721;&#22797;&#26434;&#24230;&#12290;&#35813;&#32534;&#30721;&#22120;&#22312;&#21387;&#32553;&#35270;&#39057;&#26102;&#33021;&#22815;&#21033;&#29992;&#26102;&#38388;&#20887;&#20313;&#65292;&#24182;&#22312;&#25509;&#36817;AVC&#30340;&#36895;&#29575;&#22833;&#30495;&#26465;&#20214;&#19979;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#36807;&#25311;&#21512;&#32534;&#35299;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#36890;&#36807;&#23398;&#20064;&#30340;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#65292;&#27599;&#20010;&#35299;&#30721;&#20687;&#32032;&#26377;900&#27425;&#20056;&#27861;&#65292;&#24635;&#20849;&#26377;800&#20010;&#21442;&#25968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#19968;&#20010;&#35299;&#30721;&#22797;&#26434;&#24230;&#26368;&#20302;&#30340;&#31070;&#32463;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#20043;&#19968;&#12290;&#23427;&#22522;&#20110;&#36807;&#25311;&#21512;&#30340;&#22270;&#29255;&#32534;&#35299;&#30721;&#22120;Cool-chic&#65292;&#24182;&#36890;&#36807;&#22686;&#21152;&#19968;&#20010;&#26102;&#22495;&#32534;&#30721;&#27169;&#22359;&#26469;&#24378;&#21270;&#35270;&#39057;&#30340;&#26102;&#38388;&#20887;&#20313;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#33021;&#22815;&#21387;&#32553;&#35270;&#39057;&#20197;&#23454;&#29616;&#20302;&#24310;&#36831;&#21644;&#38543;&#26426;&#35775;&#38382;&#37197;&#32622;&#65292;&#24182;&#22312;&#25509;&#36817;AVC&#30340;&#36895;&#29575;&#22833;&#30495;&#26465;&#20214;&#19979;&#20248;&#20110;&#20854;&#20182;&#36807;&#25311;&#21512;&#32534;&#35299;&#30721;&#22120;&#65292;&#22914;FFNeRV&#12290;&#35813;&#31995;&#32479;&#26159;&#24320;&#28304;&#30340;&#65306;orange-opensource.github.io/Cool-Chic&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a lightweight learned video codec with 900 multiplications per decoded pixel and 800 parameters overall. To the best of our knowledge, this is one of the neural video codecs with the lowest decoding complexity. It is built upon the overfitted image codec Cool-chic and supplements it with an inter coding module to leverage the video's temporal redundancies. The proposed model is able to compress videos using both low-delay and random access configurations and achieves rate-distortion close to AVC while out-performing other overfitted codecs such as FFNeRV. The system is made open-source: orange-opensource.github.io/Cool-Chic.
&lt;/p&gt;</description></item><item><title>EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.03049</link><description>&lt;p&gt;
EasyInstruct&#65306;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03049
&lt;/p&gt;
&lt;p&gt;
EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#25104;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#12290;&#20026;&#20102;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25968;&#25454;&#25968;&#37327;&#21644;&#25968;&#25454;&#36136;&#37327;&#20043;&#38388;&#36798;&#21040;&#31934;&#24039;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#19981;&#19968;&#33268;&#65292;&#30446;&#21069;&#27809;&#26377;&#26631;&#20934;&#30340;&#24320;&#28304;&#25351;&#20196;&#22788;&#29702;&#23454;&#29616;&#26694;&#26550;&#21487;&#20379;&#31038;&#21306;&#20351;&#29992;&#65292;&#36825;&#20351;&#24471;&#20174;&#19994;&#32773;&#26080;&#27861;&#36827;&#19968;&#27493;&#24320;&#21457;&#21644;&#25512;&#36827;&#12290;&#20026;&#20102;&#20419;&#36827;&#25351;&#20196;&#22788;&#29702;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyInstruct&#65292;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;LLMs&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#23427;&#23558;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#27169;&#22359;&#21270;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#12290;EasyInstruct&#24050;&#32463;&#22312;https://github.com/zjunlp/EasyInstruct&#19978;&#20844;&#24320;&#21457;&#24067;&#65292;&#24182;&#24471;&#21040;&#20102;&#31215;&#26497;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#23610;&#24230;&#25968;&#25454;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#26032;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.03021</link><description>&lt;p&gt;
&#25968;&#25454;&#35825;&#23548;&#30340;&#22810;&#23610;&#24230;&#25439;&#22833;&#21644;&#39640;&#25928;&#22810;&#36895;&#29575;&#26799;&#24230;&#19979;&#38477;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Data-induced multiscale losses and efficient multirate gradient descent schemes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#23610;&#24230;&#25968;&#25454;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#26032;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22810;&#23610;&#24230;&#25968;&#25454;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#12290;&#22914;&#26524;&#19968;&#20010;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#22312;&#19981;&#21516;&#26041;&#21521;&#19978;&#20855;&#26377;&#23610;&#24230;&#30340;&#26174;&#33879;&#21464;&#21270;&#65292;&#21017;&#20854;&#34987;&#31216;&#20026;&#22810;&#23610;&#24230;&#25968;&#25454;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#25439;&#22833;&#26223;&#35266;&#20013;&#30340;&#22810;&#23610;&#24230;&#32467;&#26500;&#65292;&#21253;&#25324;&#20854;&#26799;&#24230;&#21644;&#26469;&#33258;&#25968;&#25454;&#30340;&#28023;&#26862;&#30697;&#38453;&#12290;&#30456;&#24212;&#22320;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#21463;&#31185;&#23398;&#35745;&#31639;&#20013;&#20351;&#29992;&#30340;&#22810;&#23610;&#24230;&#31639;&#27861;&#30340;&#21551;&#21457;&#12290;&#36825;&#31181;&#26041;&#27861;&#35797;&#22270;&#36229;&#36234;&#32463;&#39564;&#24615;&#23398;&#20064;&#29575;&#36873;&#25321;&#65292;&#25552;&#20379;&#19968;&#31181;&#26356;&#31995;&#32479;&#12289;&#25968;&#25454;&#39537;&#21160;&#30340;&#31574;&#30053;&#26469;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#65292;&#29305;&#21035;&#26159;&#22312;&#21518;&#26399;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the impact of multiscale data on machine learning algorithms, particularly in the context of deep learning. A dataset is multiscale if its distribution shows large variations in scale across different directions. This paper reveals multiscale structures in the loss landscape, including its gradients and Hessians inherited from the data. Correspondingly, it introduces a novel gradient descent approach, drawing inspiration from multiscale algorithms used in scientific computing. This approach seeks to transcend empirical learning rate selection, offering a more systematic, data-informed strategy to enhance training efficiency, especially in the later stages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#20844;&#24179;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20808;&#22788;&#29702;&#12289;&#20013;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#26469;&#26368;&#23567;&#21270;&#20998;&#31867;&#38169;&#35823;&#65292;&#24182;&#22312;&#32473;&#23450;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#32447;&#24615;&#21644;&#21452;&#32447;&#24615;&#24046;&#24322;&#24230;&#37327;&#30340;&#27010;&#24565;&#65292;&#24182;&#25214;&#21040;&#20102;&#36125;&#21494;&#26031;&#26368;&#20248;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#24418;&#24335;&#12290;&#26412;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#22810;&#20010;&#20844;&#24179;&#24615;&#32422;&#26463;&#21644;&#24120;&#35265;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.02817</link><description>&lt;p&gt;
&#22522;&#20110;&#20808;&#22788;&#29702;&#12289;&#20013;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#30340;&#32447;&#24615;&#24046;&#24322;&#32422;&#26463;&#19979;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#20844;&#24179;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Bayes-Optimal Fair Classification with Linear Disparity Constraints via Pre-, In-, and Post-processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#20844;&#24179;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20808;&#22788;&#29702;&#12289;&#20013;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#26469;&#26368;&#23567;&#21270;&#20998;&#31867;&#38169;&#35823;&#65292;&#24182;&#22312;&#32473;&#23450;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#32447;&#24615;&#21644;&#21452;&#32447;&#24615;&#24046;&#24322;&#24230;&#37327;&#30340;&#27010;&#24565;&#65292;&#24182;&#25214;&#21040;&#20102;&#36125;&#21494;&#26031;&#26368;&#20248;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#24418;&#24335;&#12290;&#26412;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#22810;&#20010;&#20844;&#24179;&#24615;&#32422;&#26463;&#21644;&#24120;&#35265;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#33021;&#23545;&#21463;&#20445;&#25252;&#30340;&#32676;&#20307;&#20135;&#29983;&#19981;&#20844;&#24179;&#30340;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#20844;&#24179;&#20998;&#31867;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#32473;&#23450;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#20998;&#31867;&#38169;&#35823;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32447;&#24615;&#24046;&#24322;&#24230;&#37327;&#30340;&#27010;&#24565;&#65292;&#23427;&#20204;&#26159;&#27010;&#29575;&#20998;&#31867;&#22120;&#30340;&#32447;&#24615;&#20989;&#25968;&#65307;&#20197;&#21450;&#21452;&#32447;&#24615;&#24046;&#24322;&#24230;&#37327;&#65292;&#23427;&#20204;&#22312;&#32676;&#20307;&#22238;&#24402;&#20989;&#25968;&#26041;&#38754;&#20063;&#26159;&#32447;&#24615;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20960;&#31181;&#24120;&#35265;&#30340;&#24046;&#24322;&#24230;&#37327;&#65288;&#22914;&#20154;&#21475;&#24179;&#31561;&#12289;&#26426;&#20250;&#24179;&#31561;&#21644;&#39044;&#27979;&#24179;&#31561;&#65289;&#37117;&#26159;&#21452;&#32447;&#24615;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#25581;&#31034;&#19982;Neyman-Pearson&#24341;&#29702;&#30340;&#36830;&#25509;&#65292;&#25214;&#21040;&#20102;&#22312;&#21333;&#19968;&#32447;&#24615;&#24046;&#24322;&#24230;&#37327;&#19979;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#24418;&#24335;&#12290;&#23545;&#20110;&#21452;&#32447;&#24615;&#24046;&#24322;&#24230;&#37327;&#65292;&#36125;&#21494;&#26031;&#26368;&#20248;&#20844;&#24179;&#20998;&#31867;&#22120;&#21464;&#25104;&#20102;&#32676;&#20307;&#38408;&#20540;&#35268;&#21017;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;&#20844;&#24179;&#24615;&#32422;&#26463;&#65288;&#22914;&#24179;&#31561;&#30340;&#20960;&#29575;&#65289;&#21644;&#21463;&#20445;&#25252;&#23646;&#24615;&#24120;&#35265;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms may have disparate impacts on protected groups. To address this, we develop methods for Bayes-optimal fair classification, aiming to minimize classification error subject to given group fairness constraints. We introduce the notion of \emph{linear disparity measures}, which are linear functions of a probabilistic classifier; and \emph{bilinear disparity measures}, which are also linear in the group-wise regression functions. We show that several popular disparity measures -- the deviations from demographic parity, equality of opportunity, and predictive equality -- are bilinear.   We find the form of Bayes-optimal fair classifiers under a single linear disparity measure, by uncovering a connection with the Neyman-Pearson lemma. For bilinear disparity measures, Bayes-optimal fair classifiers become group-wise thresholding rules. Our approach can also handle multiple fairness constraints (such as equalized odds), and the common scenario when the protected attr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;&#65292;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29305;&#21035;&#26377;&#25928;&#30340;&#35774;&#35745;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02791</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Rethinking Optimization and Architecture for Tiny Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;&#65292;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29305;&#21035;&#26377;&#25928;&#30340;&#35774;&#35745;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23041;&#21147;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#28982;&#32780;&#65292;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#24212;&#29992;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#30340;&#24040;&#22823;&#25361;&#25112;&#65292;&#36843;&#20999;&#38656;&#35201;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#21463;&#22797;&#26434;&#35757;&#32451;&#36807;&#31243;&#30340;&#38480;&#21046;&#65292;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#35768;&#22810;&#32454;&#33410;&#24456;&#23569;&#24471;&#21040;&#20180;&#32454;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#19968;&#20010;&#20855;&#26377;10&#20159;&#21442;&#25968;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#20180;&#32454;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#32463;&#39564;&#30740;&#31350;&#26469;&#20998;&#26512;&#27599;&#20010;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#20010;&#26041;&#38754;&#65292;&#21363;&#31070;&#32463;&#26550;&#26500;&#12289;&#21442;&#25968;&#21021;&#22987;&#21270;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;&#22810;&#20010;&#35774;&#35745;&#20844;&#24335;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32463;&#39564;&#24615;&#22320;&#34987;&#35777;&#26126;&#29305;&#21035;&#26377;&#25928;&#65292;&#21253;&#25324;&#20998;&#35789;&#22120;&#21387;&#32553;&#12289;&#26550;&#26500;&#35843;&#25972;&#12289;&#21442;&#25968;&#32487;&#25215;&#21644;&#22810;&#36718;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;1.6T&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;PanGu-$\pi$-1B Pro&#21644;PanGu-$\pi$-1.5B Pro&#12290;
&lt;/p&gt;
&lt;p&gt;
The power of large language models (LLMs) has been demonstrated through numerous data and computing resources. However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required. Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully. In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component. Three perspectives are mainly discussed, i.e., neural architecture, parameter initialization, and optimization strategy. Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training. Then we train PanGu-$\pi$-1B Pro and PanGu-$\pi$-1.5B Pro on 1.6T multilingu
&lt;/p&gt;</description></item><item><title>&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35268;&#21010;&#24120;&#21463;&#38480;&#20110;&#22522;&#30784;&#20998;&#24067;&#21644;&#26679;&#26412;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;CDiffuser&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#21040;&#36798;&#39640;&#22238;&#25253;&#29366;&#24577;&#30340;&#27010;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02772</link><description>&lt;p&gt;
&#23545;&#27604;&#25193;&#25955;&#22120;&#65306;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35268;&#21010;&#39640;&#22238;&#25253;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Contrastive Diffuser: Planning Towards High Return States via Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02772
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35268;&#21010;&#24120;&#21463;&#38480;&#20110;&#22522;&#30784;&#20998;&#24067;&#21644;&#26679;&#26412;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;CDiffuser&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#21040;&#36798;&#39640;&#22238;&#25253;&#29366;&#24577;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38271;&#26399;&#35268;&#21010;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20960;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#21033;&#29992;&#20102;&#25193;&#25955;&#30340;&#24314;&#27169;&#33021;&#21147;&#36827;&#34892;&#20219;&#24847;&#20998;&#24067;&#30340;&#35268;&#21010;&#12290;&#36825;&#20123;&#26041;&#27861;&#20026;&#35268;&#21010;&#29983;&#25104;&#20102;&#21518;&#32493;&#36712;&#36857;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21463;&#21040;&#22522;&#30784;&#20998;&#24067;&#30340;&#38480;&#21046;&#65292;&#24182;&#24573;&#35270;&#20102;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#19981;&#21516;&#29366;&#24577;&#20855;&#26377;&#19981;&#21516;&#30340;&#22238;&#25253;&#12290;&#23427;&#20204;&#20165;&#20165;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#65292;&#24182;&#29983;&#25104;&#19982;&#31163;&#32447;&#25968;&#25454;&#38598;&#20855;&#26377;&#30456;&#21516;&#20998;&#24067;&#30340;&#36712;&#36857;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#21040;&#36798;&#39640;&#22238;&#25253;&#29366;&#24577;&#30340;&#27010;&#29575;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#12290;&#21363;&#20351;&#37197;&#22791;&#20102;&#24341;&#23548;&#27169;&#22411;&#65292;&#24615;&#33021;&#20173;&#28982;&#21463;&#21040;&#21387;&#21046;&#12290;&#38024;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CDiffuser&#30340;&#26032;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#36820;&#22238;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Applying diffusion models in reinforcement learning for long-term planning has gained much attention recently. Several diffusion-based methods have successfully leveraged the modeling capabilities of diffusion for arbitrary distributions. These methods generate subsequent trajectories for planning and have demonstrated significant improvement. However, these methods are limited by their plain base distributions and their overlooking of the diversity of samples, in which different states have different returns. They simply leverage diffusion to learn the distribution of offline dataset, generate the trajectories whose states share the same distribution with the offline dataset. As a result, the probability of these models reaching the high-return states is largely dependent on the dataset distribution. Even equipped with the guidance model, the performance is still suppressed. To address these limitations, in this paper, we propose a novel method called CDiffuser, which devises a return
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#40065;&#26834;&#20302;&#32423;&#25511;&#21046;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#22120;&#30340;&#38556;&#30861;&#29289;&#36991;&#38556;&#36712;&#36857;&#35268;&#21010;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#31215;&#26497;&#21442;&#19982;&#23398;&#20064;&#65292;&#32469;&#36807;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38750;&#37325;&#22797;&#21644;&#38543;&#26426;&#30340;&#36991;&#38556;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.02551</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#38556;&#30861;&#29289;&#36991;&#38556;&#36712;&#36857;&#35268;&#21010;&#22120;&#19982;&#40065;&#26834;&#20302;&#32423;&#25511;&#21046;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#22120;
&lt;/p&gt;
&lt;p&gt;
Obstacle Avoidance Deep Reinforcement Learning-Based Trajectory Planner with Robust Low-Level Control for Robotic Manipulators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02551
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#40065;&#26834;&#20302;&#32423;&#25511;&#21046;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#22120;&#30340;&#38556;&#30861;&#29289;&#36991;&#38556;&#36712;&#36857;&#35268;&#21010;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#31215;&#26497;&#21442;&#19982;&#23398;&#20064;&#65292;&#32469;&#36807;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38750;&#37325;&#22797;&#21644;&#38543;&#26426;&#30340;&#36991;&#38556;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#29616;&#20195;&#31574;&#30053;&#24448;&#24448;&#26159;&#22522;&#20110;&#23398;&#20064;&#30340;&#65292;&#20854;&#29305;&#28857;&#26159;&#40657;&#30418;&#24615;&#36136;&#22797;&#26434;&#65292;&#32570;&#20047;&#35299;&#37322;&#24615;&#65292;&#21487;&#33021;&#22312;&#30830;&#20445;&#31283;&#23450;&#24615;&#21644;&#23433;&#20840;&#24615;&#26041;&#38754;&#24102;&#26469;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#26080;&#38556;&#30861;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#36712;&#36857;&#35268;&#21010;&#22120;&#19982;&#26032;&#39062;&#30340;&#33258;&#21160;&#35843;&#35856;&#20302;&#32423;&#21644;&#20851;&#33410;&#32423;&#25511;&#21046;&#31574;&#30053;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#24182;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#31215;&#26497;&#21442;&#19982;&#23398;&#20064;&#38454;&#27573;&#12290;&#36825;&#31181;&#26041;&#27861;&#32469;&#36807;&#20102;&#19982;&#35745;&#31639;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#38750;&#37325;&#22797;&#21644;&#38543;&#26426;&#30340;&#36991;&#38556;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#21033;&#29992;&#26080;&#27169;&#22411;DRL&#20195;&#29702;&#22312;&#20851;&#33410;&#32423;&#25512;&#29702;&#20219;&#21153;&#31354;&#38388;&#20013;&#36827;&#34892;&#36895;&#24230;&#38480;&#21046;&#21644;&#26080;&#38556;&#30861;&#36816;&#21160;&#35268;&#21010;&#65292;&#28982;&#21518;&#23558;&#35813;&#35268;&#21010;&#36755;&#20837;&#21040;&#31283;&#20581;&#30340;&#23376;&#31995;&#32479;&#33258;&#36866;&#24212;&#25511;&#21046;&#22120;&#20013;&#65292;&#20135;&#29983;&#25152;&#38656;&#30340;&#25197;&#30697;&#65292;&#32780;&#26460;&#40515;&#25628;&#32034;&#20248;&#21270;&#65288;CSO&#65289;&#31639;&#27861;&#22686;&#24378;&#20102;&#25511;&#21046;&#22686;&#30410;&#20197;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In robotics, contemporary strategies are learning-based, characterized by a complex black-box nature and a lack of interpretability, which may pose challenges in ensuring stability and safety. To address these issues, we propose integrating an obstacle-free deep reinforcement learning (DRL) trajectory planner with a novel auto-tuning low- and joint-level control strategy, all while actively engaging in the learning phase through interactions with the environment. This approach circumvents the complexities associated with computations while also addressing nonrepetitive and random obstacle avoidance tasks. First, a model-free DRL agent to plan velocity-bounded and obstacle-free motion is employed for a manipulator with 'n' degrees of freedom (DoF) in task space through joint-level reasoning. This plan is then input into a robust subsystem-based adaptive controller, which produces the necessary torques, while the Cuckoo Search Optimization (CSO) algorithm enhances control gains to minimi
&lt;/p&gt;</description></item><item><title>TopoX&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#21253;&#21547;&#20102;&#26500;&#24314;&#12289;&#35745;&#31639;&#21644;&#23884;&#20837;&#25299;&#25169;&#22495;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;</title><link>https://arxiv.org/abs/2402.02441</link><description>&lt;p&gt;
TopoX: &#19968;&#20010;&#29992;&#20110;&#25299;&#25169;&#22495;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
TopoX: A Suite of Python Packages for Machine Learning on Topological Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02441
&lt;/p&gt;
&lt;p&gt;
TopoX&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#21253;&#21547;&#20102;&#26500;&#24314;&#12289;&#35745;&#31639;&#21644;&#23884;&#20837;&#25299;&#25169;&#22495;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;topox&#65292;&#19968;&#20010;&#25552;&#20379;&#21487;&#38752;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#65288;&#25193;&#23637;&#20102;&#22270;&#30340;&#39046;&#22495;&#65289;&#19978;&#36827;&#34892;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#65306;&#36229;&#22270;&#12289;&#21333;&#32431;&#12289;&#32990;&#33108;&#12289;&#36335;&#24452;&#21644;&#32452;&#21512;&#22797;&#21512;&#20307;&#12290;topox&#30001;&#19977;&#20010;&#36719;&#20214;&#21253;&#32452;&#25104;&#65306;toponetx&#29992;&#20110;&#26500;&#24314;&#21644;&#35745;&#31639;&#36825;&#20123;&#22495;&#65292;&#21253;&#25324;&#33410;&#28857;&#12289;&#36793;&#21644;&#39640;&#38454;&#21333;&#20803;&#30340;&#22788;&#29702;&#65307;topoembedx&#25552;&#20379;&#20102;&#23558;&#25299;&#25169;&#22495;&#23884;&#20837;&#21040;&#21521;&#37327;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;&#22522;&#20110;&#22270;&#30340;&#23884;&#20837;&#31639;&#27861;&#65292;&#22914;node2vec&#65307;topomodelx&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#20026;&#25299;&#25169;&#22495;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;topox&#30340;&#28304;&#20195;&#30721;&#32463;&#36807;&#24191;&#27867;&#30340;&#25991;&#26723;&#21270;&#21644;&#21333;&#20803;&#27979;&#35797;&#65292;&#24182;&#22312;https://github.com/pyt-team&#20197;MIT&#35768;&#21487;&#35777;&#30340;&#24418;&#24335;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce topox, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. topox consists of three packages: toponetx facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; topoembedx provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; topomodelx is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. The extensively documented and unit-tested source code of topox is available under MIT license at https://github.com/pyt-team.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20302;&#23618;&#27425;&#29305;&#24449;&#34920;&#31034;&#36827;&#34892;&#36947;&#36335;&#20998;&#21106;&#65292;&#22312;&#20027;&#27969;&#32593;&#32476;&#27169;&#22411;&#30340;&#20027;&#35201;&#38454;&#27573;&#23454;&#29616;&#20102;&#22823;&#37096;&#20998;&#36947;&#36335;&#20687;&#32032;&#30340;&#20934;&#30830;&#34920;&#31034;&#65292;&#25552;&#20986;&#20102;&#20197;&#20302;&#23618;&#27425;&#29305;&#24449;&#20026;&#20027;&#23548;&#30340;&#36947;&#36335;&#20998;&#21106;&#32593;&#32476;&#65288;LFD-RoadSeg&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.02430</link><description>&lt;p&gt;
&#21033;&#29992;&#20302;&#23618;&#27425;&#34920;&#31034;&#36827;&#34892;&#36229;&#24555;&#36895;&#36947;&#36335;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Exploiting Low-level Representations for Ultra-Fast Road Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20302;&#23618;&#27425;&#29305;&#24449;&#34920;&#31034;&#36827;&#34892;&#36947;&#36335;&#20998;&#21106;&#65292;&#22312;&#20027;&#27969;&#32593;&#32476;&#27169;&#22411;&#30340;&#20027;&#35201;&#38454;&#27573;&#23454;&#29616;&#20102;&#22823;&#37096;&#20998;&#36947;&#36335;&#20687;&#32032;&#30340;&#20934;&#30830;&#34920;&#31034;&#65292;&#25552;&#20986;&#20102;&#20197;&#20302;&#23618;&#27425;&#29305;&#24449;&#20026;&#20027;&#23548;&#30340;&#36947;&#36335;&#20998;&#21106;&#32593;&#32476;&#65288;LFD-RoadSeg&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#23884;&#20837;&#24335;&#24179;&#21488;&#19978;&#30340;&#23454;&#26102;&#24615;&#21644;&#20934;&#30830;&#24615;&#19968;&#30452;&#26159;&#36947;&#36335;&#20998;&#21106;&#26041;&#27861;&#30340;&#36861;&#27714;&#12290;&#20026;&#27492;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#35768;&#22810;&#36731;&#37327;&#32423;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#24573;&#35270;&#20102;&#36947;&#36335;&#26159;&#8220;&#29289;&#36136;&#8221;&#65288;&#32972;&#26223;&#25110;&#29615;&#22659;&#20803;&#32032;&#65289;&#32780;&#19981;&#26159;&#8220;&#19996;&#35199;&#8221;&#65288;&#29305;&#23450;&#21487;&#35782;&#21035;&#30340;&#23545;&#35937;&#65289;&#30340;&#20107;&#23454;&#65292;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#25506;&#32034;&#29992;&#20302;&#23618;&#27425;&#29305;&#24449;&#32780;&#19981;&#26159;&#39640;&#23618;&#27425;&#29305;&#24449;&#26469;&#34920;&#31034;&#36947;&#36335;&#30340;&#21487;&#34892;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20027;&#27969;&#32593;&#32476;&#27169;&#22411;&#30340;&#20027;&#35201;&#38454;&#27573;&#36275;&#20197;&#34920;&#31034;&#22823;&#37096;&#20998;&#20687;&#32032;&#30340;&#36947;&#36335;&#36827;&#34892;&#20998;&#21106;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20302;&#23618;&#27425;&#29305;&#24449;&#20026;&#20027;&#23548;&#30340;&#36947;&#36335;&#20998;&#21106;&#32593;&#32476;&#65288;LFD-RoadSeg&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LFD-RoadSeg&#37319;&#29992;&#20102;&#21452;&#36793;&#32467;&#26500;&#12290;&#39318;&#20808;&#35774;&#35745;&#20102;&#31354;&#38388;&#32454;&#33410;&#20998;&#25903;&#65292;&#36890;&#36807;ResNet-18&#30340;&#31532;&#19968;&#38454;&#27573;&#25552;&#21462;&#36947;&#36335;&#30340;&#20302;&#23618;&#27425;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#21518;&#35774;&#35745;&#20102;&#19978;&#19979;&#25991;&#35821;&#20041;&#20998;&#25903;&#65292;&#20197;&#25233;&#21046;&#20302;&#23618;&#27425;&#29305;&#24449;&#20013;&#38169;&#35823;&#22320;&#23558;&#26080;&#32441;&#29702;&#21306;&#22495;&#35823;&#35748;&#20026;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Achieving real-time and accuracy on embedded platforms has always been the pursuit of road segmentation methods. To this end, they have proposed many lightweight networks. However, they ignore the fact that roads are "stuff" (background or environmental elements) rather than "things" (specific identifiable objects), which inspires us to explore the feasibility of representing roads with low-level instead of high-level features. Surprisingly, we find that the primary stage of mainstream network models is sufficient to represent most pixels of the road for segmentation. Motivated by this, we propose a Low-level Feature Dominated Road Segmentation network (LFD-RoadSeg). Specifically, LFD-RoadSeg employs a bilateral structure. The spatial detail branch is firstly designed to extract low-level feature representation for the road by the first stage of ResNet-18. To suppress texture-less regions mistaken as the road in the low-level feature, the context semantic branch is then designed to ext
&lt;/p&gt;</description></item><item><title>Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02416</link><description>&lt;p&gt;
Aligner: &#36890;&#36807;&#24369;&#21040;&#24378;&#26657;&#27491;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02416
&lt;/p&gt;
&lt;p&gt;
Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#36827;&#34892;&#23545;&#40784;&#30340;&#21162;&#21147;&#20027;&#35201;&#26159;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30528;&#20027;&#35201;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#12289;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24037;&#31243;&#20197;&#21450;&#37325;&#35201;&#30340;&#26159;&#65292;&#38656;&#35201;&#35775;&#38382;LLM&#21442;&#25968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#23545;&#40784;&#33539;&#24335;Aligner&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#23545;&#40784;&#21644;&#26410;&#23545;&#40784;&#31572;&#26696;&#20043;&#38388;&#30340;&#26657;&#27491;&#27531;&#24046;&#26469;&#32469;&#36807;&#25972;&#20010;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;Aligner&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#21160;&#22238;&#24402;seq2seq&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#26597;&#35810;-&#31572;&#26696;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#23545;&#40784;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#23545;&#36164;&#28304;&#38656;&#27714;&#36739;&#23569;&#12290;&#20854;&#27425;&#65292;Aligner&#23454;&#29616;&#20102;&#20174;&#24369;&#21040;&#24378;&#30340;&#27867;&#21270;&#65307;&#36890;&#36807;Aligner&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;&#31532;&#19977;&#65292;Aligner&#20316;&#20026;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#8230;
&lt;/p&gt;
&lt;p&gt;
Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;$\ell_0$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#23545;&#20598;&#24418;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21407;&#23545;&#20598;&#31639;&#27861;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#23545;&#20598;&#33539;&#22260;&#20272;&#35745;&#21644;&#22686;&#37327;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#25928;&#29575;&#21644;&#32479;&#35745;&#24615;&#36136;&#12290;</title><link>https://arxiv.org/abs/2402.02322</link><description>&lt;p&gt;
&#21160;&#24577;&#22686;&#37327;&#20248;&#21270;&#29992;&#20110;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Dynamic Incremental Optimization for Best Subset Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;$\ell_0$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#23545;&#20598;&#24418;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21407;&#23545;&#20598;&#31639;&#27861;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#23545;&#20598;&#33539;&#22260;&#20272;&#35745;&#21644;&#22686;&#37327;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#25928;&#29575;&#21644;&#32479;&#35745;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#34987;&#35748;&#20026;&#26159;&#31232;&#30095;&#23398;&#20064;&#38382;&#39064;&#30340;&#8220;&#40644;&#37329;&#26631;&#20934;&#8221;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#25915;&#20987;&#36825;&#20010;&#38750;&#20809;&#28369;&#38750;&#20984;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;$\ell_0$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#23545;&#20598;&#24418;&#24335;&#12290;&#22522;&#20110;&#21407;&#22987;&#38382;&#39064;&#21644;&#23545;&#20598;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21407;&#23545;&#20598;&#31639;&#27861;&#12290;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#23545;&#20598;&#33539;&#22260;&#20272;&#35745;&#21644;&#22686;&#37327;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#28508;&#22312;&#22320;&#20943;&#23569;&#20102;&#20887;&#20313;&#35745;&#31639;&#24182;&#25913;&#36827;&#20102;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#30340;&#25928;&#29575;&#21644;&#32479;&#35745;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Best subset selection is considered the `gold standard' for many sparse learning problems. A variety of optimization techniques have been proposed to attack this non-smooth non-convex problem. In this paper, we investigate the dual forms of a family of $\ell_0$-regularized problems. An efficient primal-dual algorithm is developed based on the primal and dual problem structures. By leveraging the dual range estimation along with the incremental strategy, our algorithm potentially reduces redundant computation and improves the solutions of best subset selection. Theoretical analysis and experiments on synthetic and real-world datasets validate the efficiency and statistical properties of the proposed solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22806;&#28304;&#21464;&#37327;&#30340;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#32467;&#26500;&#21270;&#22240;&#26524;&#27169;&#22411;&#30340;&#36817;&#20284;&#31934;&#24230;&#65292;&#24182;&#23558;&#22240;&#26524;&#36125;&#21494;&#26031;&#20248;&#21270;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;&#22240;&#26524;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.02277</link><description>&lt;p&gt;
&#22240;&#26524;&#36125;&#21494;&#26031;&#20248;&#21270;&#36890;&#36807;&#22806;&#28304;&#20998;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Causal Bayesian Optimization via Exogenous Distribution Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22806;&#28304;&#21464;&#37327;&#30340;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#32467;&#26500;&#21270;&#22240;&#26524;&#27169;&#22411;&#30340;&#36817;&#20284;&#31934;&#24230;&#65292;&#24182;&#23558;&#22240;&#26524;&#36125;&#21494;&#26031;&#20248;&#21270;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;&#22240;&#26524;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32467;&#26500;&#21270;&#22240;&#26524;&#27169;&#22411;&#20013;&#65292;&#23558;&#30446;&#26631;&#21464;&#37327;&#26368;&#22823;&#21270;&#20316;&#20026;&#25805;&#20316;&#30446;&#26631;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22240;&#26524;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;CBO&#65289;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#25913;&#21464;&#22240;&#26524;&#32467;&#26500;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#30828;&#24178;&#39044;&#65292;&#35201;&#20040;&#24341;&#20837;&#21160;&#20316;&#33410;&#28857;&#21040;&#20869;&#29983;&#21464;&#37327;&#20013;&#65292;&#20197;&#35843;&#25972;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#20197;&#23454;&#29616;&#30446;&#26631;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#22806;&#28304;&#21464;&#37327;&#30340;&#20998;&#24067;&#65292;&#36825;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#36890;&#24120;&#34987;&#24573;&#30053;&#25110;&#36890;&#36807;&#26399;&#26395;&#36827;&#34892;&#36793;&#32536;&#21270;&#12290;&#22806;&#28304;&#20998;&#24067;&#23398;&#20064;&#25552;&#39640;&#20102;&#36890;&#24120;&#36890;&#36807;&#26377;&#38480;&#35266;&#27979;&#25968;&#25454;&#35757;&#32451;&#30340;&#20195;&#29702;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#21270;&#22240;&#26524;&#27169;&#22411;&#30340;&#36817;&#20284;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#23398;&#20064;&#21040;&#30340;&#22806;&#28304;&#20998;&#24067;&#23558;&#29616;&#26377;&#30340;CBO&#25193;&#23637;&#21040;&#36229;&#20986;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#65288;ANM&#65289;&#30340;&#19968;&#33324;&#22240;&#26524;&#26041;&#26696;&#12290;&#24674;&#22797;&#22806;&#28304;&#21464;&#37327;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#22122;&#22768;&#25110;&#26410;&#35266;&#27979;&#21040;&#30340;&#38544;&#34255;&#21464;&#37327;&#20351;&#29992;&#26356;&#28789;&#27963;&#30340;&#20808;&#39564;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;CBO&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximizing a target variable as an operational objective in a structured causal model is an important problem. Existing Causal Bayesian Optimization (CBO) methods either rely on hard interventions that alter the causal structure to maximize the reward; or introduce action nodes to endogenous variables so that the data generation mechanisms are adjusted to achieve the objective. In this paper, a novel method is introduced to learn the distribution of exogenous variables, which is typically ignored or marginalized through expectation by existing methods.   Exogenous distribution learning improves the approximation accuracy of structured causal models in a surrogate model that is usually trained with limited observational data. Moreover, the learned exogenous distribution extends existing CBO to general causal schemes beyond Additive Noise Models (ANM). The recovery of exogenous variables allows us to use a more flexible prior for noise or unobserved hidden variables. A new CBO method is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#24773;&#20917;&#19979;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20808;&#39564;&#20551;&#35774;&#36827;&#34892;&#31616;&#21333;&#30340;&#32553;&#25918;&#65292;&#20351;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.02229</link><description>&lt;p&gt;
&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#34920;&#29616;&#20986;&#33394;
&lt;/p&gt;
&lt;p&gt;
Vanilla Bayesian Optimization Performs Great in High Dimension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#24773;&#20917;&#19979;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20808;&#39564;&#20551;&#35774;&#36827;&#34892;&#31616;&#21333;&#30340;&#32553;&#25918;&#65292;&#20351;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#39640;&#32500;&#38382;&#39064;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#36719;&#32907;&#12290;&#21463;&#21040;&#32500;&#24230;&#22122;&#38899;&#30340;&#21050;&#28608;&#65292;&#35768;&#22810;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#23545;&#30446;&#26631;&#24212;&#29992;&#21508;&#31181;&#31616;&#21270;&#20551;&#35774;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#35782;&#21035;&#23548;&#33268;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#20219;&#21153;&#20013;&#19981;&#36866;&#29992;&#30340;&#36864;&#21270;&#29616;&#35937;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#29616;&#26377;&#31639;&#27861;&#22914;&#20309;&#36890;&#36807;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24230;&#26469;&#24212;&#23545;&#36825;&#20123;&#36864;&#21270;&#29616;&#35937;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#20013;&#20856;&#22411;&#20808;&#39564;&#20551;&#35774;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#23545;&#30446;&#26631;&#26045;&#21152;&#32467;&#26500;&#24615;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#23558;&#22797;&#26434;&#24615;&#38477;&#20302;&#21040;&#21487;&#31649;&#29702;&#30340;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#20462;&#25913;&#26041;&#27861;&#8212;&#8212;&#36890;&#36807;&#32500;&#24230;&#23545;&#39640;&#26031;&#36807;&#31243;&#38271;&#24230;&#20808;&#39564;&#36827;&#34892;&#31616;&#21333;&#30340;&#32553;&#25918;&#8212;&#8212;&#25581;&#31034;&#20102;&#26631;&#20934;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#26126;&#30830;&#34920;&#26126;&#20854;&#25928;&#26524;&#36828;&#36828;&#36229;&#20986;&#20197;&#24448;&#30340;&#39044;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional problems have long been considered the Achilles' heel of Bayesian optimization algorithms. Spurred by the curse of dimensionality, a large collection of algorithms aim to make it more performant in this setting, commonly by imposing various simplifying assumptions on the objective. In this paper, we identify the degeneracies that make vanilla Bayesian optimization poorly suited to high-dimensional tasks, and further show how existing algorithms address these degeneracies through the lens of lowering the model complexity. Moreover, we propose an enhancement to the prior assumptions that are typical to vanilla Bayesian optimization algorithms, which reduces the complexity to manageable levels without imposing structural restrictions on the objective. Our modification - a simple scaling of the Gaussian process lengthscale prior with the dimensionality - reveals that standard Bayesian optimization works drastically better than previously thought in high dimensions, clearly
&lt;/p&gt;</description></item><item><title>&#36816;&#29992;&#35821;&#35328;&#27169;&#22411;&#25216;&#26415;&#20110;&#39640;&#24615;&#33021;&#35745;&#31639;&#20219;&#21153;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;</title><link>https://arxiv.org/abs/2402.02018</link><description>&lt;p&gt;
&#20301;&#32622;&#35770;&#25991;&#65306;&#36229;&#32423;&#35745;&#31639;&#30740;&#31350;&#21644;LLMs&#30340;&#29616;&#29366;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Position Paper: The Landscape and Challenges of HPC Research and LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02018
&lt;/p&gt;
&lt;p&gt;
&#36816;&#29992;&#35821;&#35328;&#27169;&#22411;&#25216;&#26415;&#20110;&#39640;&#24615;&#33021;&#35745;&#31639;&#20219;&#21153;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#29305;&#21035;&#26159;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#12290;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;&#22522;&#20110;&#25552;&#31034;&#30340;&#25216;&#26415;&#22343;&#23637;&#29616;&#20986;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22522;&#20110;&#20195;&#30721;&#30340;&#20219;&#21153;&#20013;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#35768;&#22810;&#30740;&#31350;&#23454;&#39564;&#23460;&#21644;&#26426;&#26500;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#26041;&#38754;&#25237;&#20837;&#20102;&#22823;&#37327;&#36164;&#28304;&#65292;&#36798;&#21040;&#25110;&#31361;&#30772;&#20102;&#36229;&#32423;&#35745;&#31639;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#23558;&#36825;&#20123;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25216;&#26415;&#35843;&#25972;&#21644;&#24212;&#29992;&#20110;&#39640;&#24615;&#33021;&#35745;&#31639;&#20219;&#21153;&#20013;&#23558;&#20250;&#38750;&#24120;&#26377;&#30410;&#12290;&#26412;&#30740;&#31350;&#38416;&#36848;&#20102;&#25105;&#20204;&#19978;&#36848;&#35266;&#28857;&#30340;&#29702;&#30001;&#65292;&#24182;&#24378;&#35843;&#20102;&#29616;&#26377;&#24819;&#27861;&#22312;HPC&#20219;&#21153;&#20013;&#30340;&#25913;&#36827;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, language models (LMs), especially large language models (LLMs), have revolutionized the field of deep learning. Both encoder-decoder models and prompt-based techniques have shown immense potential for natural language processing and code-based tasks. Over the past several years, many research labs and institutions have invested heavily in high-performance computing, approaching or breaching exascale performance levels. In this paper, we posit that adapting and utilizing such language model-based techniques for tasks in high-performance computing (HPC) would be very beneficial. This study presents our reasoning behind the aforementioned position and highlights how existing ideas can be improved and adapted for HPC tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#22686;&#24378;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#36335;&#24452;&#25439;&#32791;&#39044;&#27979;&#20013;&#25968;&#25454;&#26377;&#38480;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#26469;&#33258;&#34562;&#31389;&#35206;&#30422;&#27169;&#25311;&#22120;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#21644;&#29420;&#31435;&#25910;&#38598;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20851;&#38190;&#30340;&#30495;&#23454;&#20540;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#24037;&#31243;&#21644;&#39640;&#25928;&#31283;&#20581;&#30340;&#26799;&#24230;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;CatBoost&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#36335;&#24452;&#25439;&#32791;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01969</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36335;&#24452;&#25439;&#32791;&#39044;&#27979;&#30340;&#27169;&#25311;&#22686;&#24378;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Simulation-Enhanced Data Augmentation for Machine Learning Pathloss Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#22686;&#24378;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#36335;&#24452;&#25439;&#32791;&#39044;&#27979;&#20013;&#25968;&#25454;&#26377;&#38480;&#30340;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#26469;&#33258;&#34562;&#31389;&#35206;&#30422;&#27169;&#25311;&#22120;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#21644;&#29420;&#31435;&#25910;&#38598;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#20851;&#38190;&#30340;&#30495;&#23454;&#20540;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#24037;&#31243;&#21644;&#39640;&#25928;&#31283;&#20581;&#30340;&#26799;&#24230;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;CatBoost&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#36335;&#24452;&#25439;&#32791;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#23545;&#20110;&#36335;&#24452;&#25439;&#32791;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#20854;&#26377;&#25928;&#24615;&#21487;&#33021;&#20250;&#38477;&#20302;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#25311;&#22686;&#24378;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#36335;&#24452;&#25439;&#32791;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20174;&#34562;&#31389;&#35206;&#30422;&#27169;&#25311;&#22120;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#19982;&#29420;&#31435;&#25910;&#38598;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#36807;&#22312;&#19981;&#21516;&#29615;&#22659;&#65288;&#21253;&#25324;&#20892;&#22330;&#12289;&#19992;&#38517;&#22320;&#24102;&#21644;&#20303;&#23429;&#21306;&#65289;&#24320;&#23637;&#24191;&#27867;&#30340;&#27979;&#37327;&#27963;&#21160;&#26469;&#25910;&#38598;&#12290;&#36825;&#31181;&#20840;&#38754;&#30340;&#25968;&#25454;&#25910;&#38598;&#20026;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#30495;&#23454;&#20540;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#32452;&#20449;&#36947;&#29305;&#24449;&#65292;&#21253;&#25324;&#20174;LiDAR&#25968;&#25454;&#38598;&#20013;&#23548;&#20986;&#30340;&#22320;&#29702;&#23646;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#39640;&#25928;&#19988;&#31283;&#20581;&#30340;&#26799;&#24230;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;CatBoost&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#27169;&#25311;&#25968;&#25454;&#30340;&#38598;&#25104;&#26174;&#33879;&#25913;&#21892;&#20102;&#36335;&#24452;&#25439;&#32791;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) offers a promising solution to pathloss prediction. However, its effectiveness can be degraded by the limited availability of data. To alleviate these challenges, this paper introduces a novel simulation-enhanced data augmentation method for ML pathloss prediction. Our method integrates synthetic data generated from a cellular coverage simulator and independently collected real-world datasets. These datasets were collected through an extensive measurement campaign in different environments, including farms, hilly terrains, and residential areas. This comprehensive data collection provides vital ground truth for model training. A set of channel features was engineered, including geographical attributes derived from LiDAR datasets. These features were then used to train our prediction model, incorporating the highly efficient and robust gradient boosting ML algorithm, CatBoost. The integration of synthetic data, as demonstrated in our study, significantly improves t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20984;&#20248;&#21270;&#26041;&#27861;&#20998;&#26512;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#38750;&#28176;&#36817;&#35774;&#32622;&#19979;&#30340;&#31934;&#30830;&#39044;&#27979;&#20998;&#25968;&#20989;&#25968;&#21644;&#25910;&#25947;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.01965</link><description>&lt;p&gt;
&#36890;&#36807;&#20984;&#20248;&#21270;&#23545;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analyzing Neural Network-Based Generative Diffusion Models through Convex Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20984;&#20248;&#21270;&#26041;&#27861;&#20998;&#26512;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#38750;&#28176;&#36817;&#35774;&#32622;&#19979;&#30340;&#31934;&#30830;&#39044;&#27979;&#20998;&#25968;&#20989;&#25968;&#21644;&#25910;&#25947;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#29983;&#25104;&#20013;&#21464;&#24471;&#24191;&#27867;&#20351;&#29992;&#12290;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#33073;&#39062;&#32780;&#20986;&#65292;&#38656;&#35201;&#20272;&#35745;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#30340;&#20998;&#25968;&#20989;&#25968;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20998;&#25968;&#21305;&#37197;&#21644;&#21435;&#22122;&#20998;&#25968;&#21305;&#37197;&#37325;&#26032;&#26500;&#24314;&#20026;&#20984;&#20248;&#21270;&#30340;&#24418;&#24335;&#65292;&#26469;&#20998;&#26512;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#25193;&#25955;&#29702;&#35770;&#20027;&#35201;&#26159;&#28176;&#36817;&#30340;&#65292;&#20294;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#25193;&#25955;&#27169;&#22411;&#32473;&#20986;&#20102;&#31934;&#30830;&#30340;&#39044;&#27979;&#20998;&#25968;&#20989;&#25968;&#65292;&#24182;&#24314;&#31435;&#20102;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;&#36825;&#39033;&#24037;&#20316;&#26377;&#21161;&#20110;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#28176;&#36817;&#35774;&#32622;&#20013;&#23398;&#20064;&#21040;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are becoming widely used in state-of-the-art image, video and audio generation. Score-based diffusion models stand out among these methods, necessitating the estimation of score function of the input data distribution. In this study, we present a theoretical framework to analyze two-layer neural network-based diffusion models by reframing score matching and denoising score matching as convex optimization. Though existing diffusion theory is mainly asymptotic, we characterize the exact predicted score function and establish the convergence result for neural network-based diffusion models with finite data. This work contributes to understanding what neural network-based diffusion model learns in non-asymptotic settings.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20998;&#20301;&#25968;&#31070;&#32463;&#36816;&#31639;&#22120;&#65292;&#33021;&#22815;&#22312;&#20989;&#25968;&#23450;&#20041;&#22495;&#19978;&#21516;&#26102;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#26080;&#38656;&#20998;&#24067;&#20551;&#35774;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;2D Darcy&#27969;&#21160;&#21644;3D&#36710;&#36742;&#34920;&#38754;&#21387;&#21147;&#39044;&#27979;&#20219;&#21153;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.01960</link><description>&lt;p&gt;
&#36890;&#36807;&#31526;&#21512;&#39044;&#27979;&#23454;&#29616;&#36816;&#31639;&#22120;&#23398;&#20064;&#30340;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Calibrated Uncertainty Quantification for Operator Learning via Conformal Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01960
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20998;&#20301;&#25968;&#31070;&#32463;&#36816;&#31639;&#22120;&#65292;&#33021;&#22815;&#22312;&#20989;&#25968;&#23450;&#20041;&#22495;&#19978;&#21516;&#26102;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#26080;&#38656;&#20998;&#24067;&#20551;&#35774;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;2D Darcy&#27969;&#21160;&#21644;3D&#36710;&#36742;&#34920;&#38754;&#21387;&#21147;&#39044;&#27979;&#20219;&#21153;&#19978;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#31639;&#22120;&#23398;&#20064;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#34987;&#37319;&#29992;&#65292;&#20854;&#20013;&#24456;&#22810;&#24212;&#29992;&#38656;&#35201;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#30001;&#20110;&#36816;&#31639;&#22120;&#23398;&#20064;&#30340;&#36755;&#20986;&#26159;&#36830;&#32493;&#20989;&#25968;&#65292;&#22312;&#25972;&#20010;&#23450;&#20041;&#22495;&#19978;&#21516;&#26102;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#21482;&#32771;&#34385;&#19968;&#20010;&#28857;&#30340;&#26657;&#20934;&#65292;&#25110;&#32773;&#38024;&#23545;&#19968;&#20010;&#26631;&#37327;&#20989;&#25968;&#36827;&#34892;&#26657;&#20934;&#65292;&#25110;&#32773;&#20570;&#20986;&#24378;&#22823;&#30340;&#20551;&#35774;&#65292;&#27604;&#22914;&#20551;&#35774;&#39640;&#26031;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#25511;&#21046;&#30340;&#20998;&#20301;&#25968;&#31070;&#32463;&#36816;&#31639;&#22120;,&#19968;&#31181;&#26080;&#20998;&#24067;&#12289;&#26377;&#38480;&#26679;&#26412;&#30340;&#20989;&#25968;&#26657;&#20934;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#26657;&#20934;&#20445;&#35777;&#65292;&#21363;&#35206;&#30422;&#29575;&#65292;&#20854;&#23450;&#20041;&#20026;&#20989;&#25968;&#23450;&#20041;&#22495;&#20869;&#30495;&#23454;&#20540;&#20301;&#20110;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#29699;&#20869;&#30340;&#39044;&#26399;&#30334;&#20998;&#27604;&#12290;&#22312;&#19968;&#20010;2D Darcy&#27969;&#21160;&#21644;&#19968;&#20010;3D&#36710;&#36742;&#34920;&#38754;&#21387;&#21147;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#34920;&#26126;&#26657;&#20934;&#30340;&#35206;&#30422;&#29575;&#21644;&#26377;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#21306;&#38388;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Operator learning has been increasingly adopted in scientific and engineering applications, many of which require calibrated uncertainty quantification. Since the output of operator learning is a continuous function, quantifying uncertainty simultaneously at all points in the domain is challenging. Current methods consider calibration at a single point or over one scalar function or make strong assumptions such as Gaussianity. We propose a risk-controlling quantile neural operator, a distribution-free, finite-sample functional calibration conformal prediction method. We provide a theoretical calibration guarantee on the coverage rate, defined as the expected percentage of points on the function domain whose true value lies within the predicted uncertainty ball. Empirical results on a 2D Darcy flow and a 3D car surface pressure prediction tasks validate our theoretical results, demonstrating calibrated coverage and efficient uncertainty bands outperforming baseline methods. In particula
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;AgentHPO&#25216;&#26415;&#36890;&#36807;&#33258;&#21160;&#21270;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#22823;&#22823;&#20943;&#23569;&#20102;&#35797;&#39564;&#27425;&#25968;&#65292;&#31616;&#21270;&#20102;&#35774;&#32622;&#36807;&#31243;&#65292;&#25552;&#21319;&#20102;&#35299;&#37322;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;</title><link>https://arxiv.org/abs/2402.01881</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Agent for Hyper-Parameter Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01881
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;AgentHPO&#25216;&#26415;&#36890;&#36807;&#33258;&#21160;&#21270;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#22823;&#22823;&#20943;&#23569;&#20102;&#35797;&#39564;&#27425;&#25968;&#65292;&#31616;&#21270;&#20102;&#35774;&#32622;&#36807;&#31243;&#65292;&#25552;&#21319;&#20102;&#35299;&#37322;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#12289;&#22823;&#37327;&#23454;&#39564;&#20197;&#21450;&#39640;&#35745;&#31639;&#21644;&#20154;&#21147;&#36164;&#28304;&#12290;&#23613;&#31649;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#35797;&#39564;&#25928;&#29575;&#12289;&#35774;&#32622;&#22797;&#26434;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#33258;&#21160;&#21270;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#31216;&#20026;AgentHPO&#65288;LLM Agent-based Hyperparameter Optimization&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AgentHPO&#33258;&#20027;&#22788;&#29702;&#20219;&#21153;&#20449;&#24687;&#65292;&#26681;&#25454;&#21382;&#21490;&#35797;&#39564;&#23545;&#29305;&#23450;&#36229;&#21442;&#25968;&#65288;HPs&#65289;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#36827;&#34892;&#36845;&#20195;&#20248;&#21270;&#12290;&#19982;&#20256;&#32479;&#30340;AutoML&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#31867;&#20284;&#20154;&#31867;&#30340;&#20248;&#21270;&#36807;&#31243;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#35797;&#39564;&#27425;&#25968;&#65292;&#31616;&#21270;&#20102;&#35774;&#32622;&#36807;&#31243;&#65292;&#24182;&#25552;&#21319;&#20102;&#35299;&#37322;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization is critical in modern machine learning, requiring expert knowledge, numerous trials, and high computational and human resources. Despite the advancements in Automated Machine Learning (AutoML), challenges in terms of trial efficiency, setup complexity, and interoperability still persist. To address these issues, we introduce a novel paradigm leveraging Large Language Models (LLMs) to automate hyperparameter optimization across diverse machine learning tasks, which is named AgentHPO (short for LLM Agent-based Hyperparameter Optimization). Specifically, AgentHPO processes the task information autonomously, conducts experiments with specific hyperparameters (HPs), and iteratively optimizes them based on historical trials. This human-like optimization process largely reduces the number of required trials, simplifies the setup process, and enhances interpretability and user trust, compared to traditional AutoML methods. Extensive empirical experiments conducted o
&lt;/p&gt;</description></item><item><title>LLMs&#26080;&#27861;&#29420;&#33258;&#36827;&#34892;&#35268;&#21010;&#25110;&#33258;&#25105;&#39564;&#35777;&#65292;&#20294;&#22312;&#35268;&#21010;/&#25512;&#29702;&#20219;&#21153;&#20013;&#21487;&#20197;&#20316;&#20026;&#36890;&#29992;&#36817;&#20284;&#30693;&#35782;&#28304;&#21457;&#25381;&#26356;&#22823;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.01817</link><description>&lt;p&gt;
LLMs&#26080;&#27861;&#35268;&#21010;&#65292;&#20294;&#21487;&#20197;&#22312;LLM-Modulo&#26694;&#26550;&#20013;&#24110;&#21161;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01817
&lt;/p&gt;
&lt;p&gt;
LLMs&#26080;&#27861;&#29420;&#33258;&#36827;&#34892;&#35268;&#21010;&#25110;&#33258;&#25105;&#39564;&#35777;&#65292;&#20294;&#22312;&#35268;&#21010;/&#25512;&#29702;&#20219;&#21153;&#20013;&#21487;&#20197;&#20316;&#20026;&#36890;&#29992;&#36817;&#20284;&#30693;&#35782;&#28304;&#21457;&#25381;&#26356;&#22823;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35268;&#21010;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#35282;&#33394;&#23384;&#22312;&#24456;&#22823;&#30340;&#22256;&#24785;&#12290;&#19968;&#26041;&#38754;&#26377;&#20154;&#36807;&#20110;&#20048;&#35266;&#22320;&#22768;&#31216;&#21482;&#38656;&#27491;&#30830;&#25552;&#31034;&#25110;&#33258;&#25105;&#39564;&#35777;&#31574;&#30053;&#65292;LLMs&#23601;&#33021;&#23436;&#25104;&#36825;&#20123;&#20219;&#21153;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20063;&#26377;&#20154;&#36807;&#20110;&#24754;&#35266;&#22320;&#35748;&#20026;LLMs&#22312;&#35268;&#21010;/&#25512;&#29702;&#20219;&#21153;&#20013;&#20165;&#33021;&#20316;&#20026;&#38382;&#39064;&#35268;&#33539;&#30340;&#31616;&#21333;&#32763;&#35793;&#22120;&#65292;&#24182;&#23558;&#38382;&#39064;&#20132;&#32473;&#22806;&#37096;&#31526;&#21495;&#27714;&#35299;&#22120;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20004;&#31181;&#26497;&#31471;&#35266;&#28857;&#37117;&#26159;&#38169;&#35823;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#33258;&#22238;&#24402;LLMs&#26412;&#36523;&#19981;&#33021;&#36827;&#34892;&#35268;&#21010;&#25110;&#33258;&#25105;&#39564;&#35777;&#65288;&#27605;&#31455;&#36825;&#26159;&#19968;&#31181;&#25512;&#29702;&#24418;&#24335;&#65289;&#65292;&#24182;&#23545;&#25991;&#29486;&#20013;&#30340;&#35823;&#35299;&#21407;&#22240;&#36827;&#34892;&#20102;&#19968;&#20123;&#38416;&#36848;&#12290;&#25105;&#20204;&#36824;&#23558;&#36777;&#31216;LLMs&#24212;&#35813;&#34987;&#35270;&#20026;&#20855;&#26377;&#26356;&#26377;&#24847;&#20041;&#30340;&#35282;&#33394;&#30340;&#36890;&#29992;&#36817;&#20284;&#30693;&#35782;&#28304;&#65292;&#33021;&#22312;&#35268;&#21010;/&#25512;&#29702;&#20219;&#21153;&#20013;&#21457;&#25381;&#26356;&#22823;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is considerable confusion about the role of Large Language Models (LLMs) in planning and reasoning tasks. On one side are over-optimistic claims that LLMs can indeed do these tasks with just the right prompting or self-verification strategies. On the other side are perhaps over-pessimistic claims that all that LLMs are good for in planning/reasoning tasks are as mere translators of the problem specification from one syntactic format to another, and ship the problem off to external symbolic solvers. In this position paper, we take the view that both these extremes are misguided. We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature. We will also argue that LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end forma
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#35299;&#20915;LLM&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#25581;&#31034;&#20102;LLM&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#30452;&#25509;&#25552;&#31034;&#12289;&#37327;&#21270;&#12289;&#23545;&#40784;&#12289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#21644;&#32467;&#21512;&#24037;&#20855;&#31561;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.01801</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Time Series: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#35299;&#20915;LLM&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#25581;&#31034;&#20102;LLM&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#30452;&#25509;&#25552;&#31034;&#12289;&#37327;&#21270;&#12289;&#23545;&#40784;&#12289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#21644;&#32467;&#21512;&#24037;&#20855;&#31561;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;LLM&#19981;&#20165;&#20165;&#23616;&#38480;&#20110;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#24418;&#65292;&#36824;&#20855;&#26377;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#30340;&#37325;&#35201;&#28508;&#21147;&#65292;&#21487;&#20197;&#22312;&#27668;&#20505;&#12289;&#29289;&#32852;&#32593;&#12289;&#21307;&#30103;&#12289;&#20132;&#36890;&#12289;&#38899;&#39057;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#21463;&#30410;&#12290;&#26412;&#35843;&#30740;&#35770;&#25991;&#23545;&#21033;&#29992;LLM&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#21508;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#21644;&#35814;&#32454;&#20998;&#31867;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;LLM&#21407;&#22987;&#25991;&#26412;&#25968;&#25454;&#35757;&#32451;&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;LLM&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#25552;&#21462;&#21040;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#65288;1&#65289;&#30452;&#25509;&#25552;&#31034;LLM&#65292;&#65288;2&#65289;&#26102;&#38388;&#24207;&#21015;&#37327;&#21270;&#65292;&#65288;3&#65289;&#23545;&#40784;&#25216;&#26415;&#65292;&#65288;4&#65289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#20316;&#20026;&#26725;&#25509;&#26426;&#21046;&#65292;&#21644;&#65288;5&#65289;&#32467;&#21512;LLM&#19982;&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;&#26412;&#35843;&#30740;&#36824;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#28041;&#21450;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) alignment techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey off
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#32780;&#21521;&#37327;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#31649;&#29702;&#21644;&#21033;&#29992;&#22810;&#26679;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01763</link><description>&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36935;&#19978;&#21521;&#37327;&#25968;&#25454;&#24211;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
When Large Language Models Meet Vector Databases: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#32780;&#21521;&#37327;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#31649;&#29702;&#21644;&#21033;&#29992;&#22810;&#26679;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#22312;&#20154;&#31867;&#25991;&#23383;&#22788;&#29702;&#21644;&#29983;&#25104;&#26041;&#38754;&#24320;&#21551;&#20102;&#26032;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23427;&#20204;&#30340;&#26174;&#33879;&#22686;&#38271;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#21253;&#25324;&#24187;&#35273;&#12289;&#20559;&#35265;&#12289;&#23454;&#26102;&#30693;&#35782;&#26356;&#26032;&#20197;&#21450;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#23454;&#26045;&#21644;&#32500;&#25252;&#30340;&#39640;&#25104;&#26412;&#31561;&#37325;&#35201;&#25361;&#25112;&#12290;&#32780;&#21478;&#19968;&#31181;&#26085;&#30410;&#27969;&#34892;&#30340;&#24037;&#20855;&#65292;&#21521;&#37327;&#25968;&#25454;&#24211;&#21017;&#20026;&#36825;&#20123;&#25361;&#25112;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#25968;&#25454;&#24211;&#25797;&#38271;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#65292;&#24182;&#19988;&#23545;&#20110;&#39640;&#25928;&#30340;&#20449;&#24687;&#26816;&#32034;&#21644;&#35821;&#20041;&#25628;&#32034;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#21512;&#65292;&#23427;&#20204;&#26174;&#33879;&#22686;&#24378;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#31649;&#29702;&#21644;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#22810;&#26679;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#36827;&#34892;&#20102;&#28145;&#20837;&#32780;&#29420;&#29305;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent burst in Large Language Models has opened new frontiers in human-like text processing and generation. However, alongside their remarkable growth, Large Language Models have encountered critical challenges including issues of hallucination, bias, real-time knowledge updates, and the high costs of implementation and maintenance in commercial settings. Vector Databases, another increasingly popular tool, offer potential solutions to these challenges. These databases are adept at handling high-dimensional data and are crucial for tasks such as efficient information retrieval and semantic search. By integrating with Large Language Models, they significantly enhance AI systems' ability to manage and utilize diverse data more effectively. This survey paper provides an in-depth and unique analysis of the intersection between Large Language Models and Vector Databases.
&lt;/p&gt;</description></item><item><title>SymbolicAI&#26159;&#19968;&#20010;&#22522;&#20110;&#36923;&#36753;&#30340;&#26694;&#26550;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#22810;&#31181;&#27714;&#35299;&#22120;&#26080;&#32541;&#38598;&#25104;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#25512;&#29702;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#34701;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.00854</link><description>&lt;p&gt;
SymbolicAI: &#19968;&#20010;&#32467;&#21512;&#29983;&#25104;&#27169;&#22411;&#21644;&#27714;&#35299;&#22120;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SymbolicAI: A framework for logic-based approaches combining generative models and solvers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00854
&lt;/p&gt;
&lt;p&gt;
SymbolicAI&#26159;&#19968;&#20010;&#22522;&#20110;&#36923;&#36753;&#30340;&#26694;&#26550;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#19982;&#22810;&#31181;&#27714;&#35299;&#22120;&#26080;&#32541;&#38598;&#25104;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#25512;&#29702;&#19982;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SymbolicAI&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#19988;&#27169;&#22359;&#21270;&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#27010;&#24565;&#23398;&#20064;&#21644;&#27969;&#31243;&#31649;&#29702;&#12290;SymbolicAI&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#26469;&#25191;&#34892;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#25351;&#20196;&#30340;&#20219;&#21153;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#31526;&#21495;&#25512;&#29702;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20351;&#29983;&#25104;&#27169;&#22411;&#19982;&#21508;&#31181;&#27714;&#35299;&#22120;&#26080;&#32541;&#38598;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;&#27010;&#29575;&#32534;&#31243;&#21407;&#29702;&#26469;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#21487;&#24494;&#20998;&#21644;&#32463;&#20856;&#32534;&#31243;&#33539; paradigms &#30340;&#21508;&#33258;&#20248;&#21183;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#22810;&#24577;&#30340;&#12289;&#32452;&#21512;&#30340;&#21644;&#33258;&#25351;&#30340;&#25968;&#25454;&#27969;&#25805;&#20316;&#65292;&#23558;LLM&#30340;&#36755;&#20986;&#19982;&#29992;&#25143;&#30340;&#30446;&#26631;&#23545;&#40784;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#20855;&#26377;&#38646;&#27425;&#21644;&#23569;&#27425;&#23398;&#20064;&#33021;&#21147;&#30340;&#21508;&#31181;&#22522;&#30784;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#36807;&#28193;&#65292;&#24182;&#19982;&#25797;&#38271;&#35299;&#20915;&#29305;&#23450;&#38382;&#39064;&#30340;&#19987;&#19994;&#21270;&#35843;&#20248;&#27169;&#22411;&#25110;&#27714;&#35299;&#22120;&#37197;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for data stream manipulation, aligning LLM outputs with user objectives. As a result, we can transition between the capabilities of various foundation models endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models or solvers proficient in addres
&lt;/p&gt;</description></item><item><title>&#12298;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#12299;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#25506;&#35752;&#20102;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#31181;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#20248;&#21183;&#65292;&#24182;&#25351;&#20986;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#37325;&#28857;&#23558;&#25918;&#22312;&#22914;&#20309;&#23558;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#21457;&#25381;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.00809</link><description>&lt;p&gt;
&#12298;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#12299;&#30340;&#31435;&#22330;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00809
&lt;/p&gt;
&lt;p&gt;
&#12298;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#12299;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#25506;&#35752;&#20102;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#31181;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#20248;&#21183;&#65292;&#24182;&#25351;&#20986;&#20102;&#19982;&#20043;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#37325;&#28857;&#23558;&#25918;&#22312;&#22914;&#20309;&#23558;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#21457;&#25381;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#20154;&#20204;&#20027;&#35201;&#20851;&#27880;&#22312;&#28041;&#21450;&#22823;&#35268;&#27169;&#22270;&#20687;&#21644;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#30417;&#30563;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#26356;&#24191;&#27867;&#30340;&#35270;&#35282;&#25581;&#31034;&#20102;&#35768;&#22810;&#34987;&#24573;&#35270;&#30340;&#24230;&#37327;&#26631;&#20934;&#12289;&#20219;&#21153;&#21644;&#25968;&#25454;&#31867;&#22411;&#65292;&#22914;&#19981;&#30830;&#23450;&#24615;&#12289;&#20027;&#21160;&#21644;&#25345;&#32493;&#23398;&#20064;&#20197;&#21450;&#31185;&#23398;&#25968;&#25454;&#65292;&#36825;&#20123;&#26041;&#38754;&#38656;&#35201;&#20851;&#27880;&#12290;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#65288;BDL&#65289;&#26159;&#19968;&#26465;&#26377;&#21069;&#26223;&#30340;&#36947;&#36335;&#65292;&#21487;&#20197;&#22312;&#36825;&#20123;&#19981;&#21516;&#30340;&#35774;&#32622;&#20013;&#25552;&#20379;&#20248;&#21183;&#12290;&#26412;&#25991;&#35748;&#20026;BDL&#21487;&#20197;&#25552;&#21319;&#28145;&#24230;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#23427;&#37325;&#26032;&#23457;&#35270;&#20102;BDL&#30340;&#20248;&#21183;&#12289;&#25215;&#35748;&#20102;&#29616;&#26377;&#30340;&#25361;&#25112;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;&#19968;&#20123;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38556;&#30861;&#30340;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#23637;&#26395;&#26410;&#26469;&#65292;&#35752;&#35770;&#38598;&#20013;&#22312;&#21487;&#33021;&#30340;&#26041;&#24335;&#19978;&#65292;&#23558;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#19982;BDL&#30456;&#32467;&#21512;&#65292;&#20197;&#20805;&#20998;&#21457;&#25381;&#23427;&#20204;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings. This paper posits that BDL can elevate the capabilities of deep learning. It revisits the strengths of BDL, acknowledges existing challenges, and highlights some exciting research avenues aimed at addressing these obstacles. Looking ahead, the discussion focuses on possible ways to combine large-scale foundation models with BDL to unlock their full potential.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;Transformer&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#36817;&#20284;&#24615;&#36136;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20851;&#38190;&#32452;&#20214;&#23545;&#34920;&#36798;&#33021;&#21147;&#30340;&#24433;&#21709;&#26426;&#21046;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#20851;&#38190;&#21442;&#25968;&#23545;Transformer&#30340;&#20316;&#29992;&#65292;&#24182;&#20026;&#26367;&#20195;&#26550;&#26500;&#25552;&#20379;&#20102;&#33258;&#28982;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2402.00522</link><description>&lt;p&gt;
&#29702;&#35299;Transformer&#22312;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;Transformer&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#36817;&#20284;&#24615;&#36136;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20851;&#38190;&#32452;&#20214;&#23545;&#34920;&#36798;&#33021;&#21147;&#30340;&#24433;&#21709;&#26426;&#21046;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#20851;&#38190;&#21442;&#25968;&#23545;Transformer&#30340;&#20316;&#29992;&#65292;&#24182;&#20026;&#26367;&#20195;&#26550;&#26500;&#25552;&#20379;&#20102;&#33258;&#28982;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;Transformer&#22312;&#38271;&#12289;&#31232;&#30095;&#21644;&#22797;&#26434;&#35760;&#24518;&#30340;&#24207;&#21015;&#24314;&#27169;&#20013;&#30340;&#36817;&#20284;&#24615;&#36136;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;Transformer&#30340;&#19981;&#21516;&#32452;&#20214;&#65288;&#22914;&#28857;&#31215;&#33258;&#27880;&#24847;&#21147;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#21069;&#39304;&#23618;&#65289;&#26159;&#22914;&#20309;&#24433;&#21709;&#20854;&#34920;&#36798;&#33021;&#21147;&#30340;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#24314;&#31435;&#26126;&#30830;&#30340;&#36817;&#20284;&#29575;&#26469;&#30740;&#31350;&#23427;&#20204;&#30340;&#32508;&#21512;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;Transformer&#20013;&#20851;&#38190;&#21442;&#25968;&#65288;&#22914;&#23618;&#25968;&#21644;&#27880;&#24847;&#21147;&#22836;&#25968;&#65289;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#36825;&#20123;&#27934;&#23519;&#36824;&#20026;&#26367;&#20195;&#26550;&#26500;&#25552;&#20379;&#20102;&#33258;&#28982;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We conduct a systematic study of the approximation properties of Transformer for sequence modeling with long, sparse and complicated memory. We investigate the mechanisms through which different components of Transformer, such as the dot-product self-attention, positional encoding and feed-forward layer, affect its expressive power, and we study their combined effects through establishing explicit approximation rates. Our study reveals the roles of critical parameters in the Transformer, such as the number of layers and the number of attention heads, and these insights also provide natural suggestions for alternative architectures.
&lt;/p&gt;</description></item><item><title>PirateNets&#26159;&#19968;&#31181;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;&#32593;&#32476;&#22312;&#36739;&#22823;&#28145;&#24230;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#27531;&#24046;&#36830;&#25509;&#23454;&#29616;&#20102;&#31283;&#23450;&#21644;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#24182;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00326</link><description>&lt;p&gt;
PirateNets&#65306;&#37319;&#29992;&#27531;&#24046;&#33258;&#36866;&#24212;&#32593;&#32476;&#30340;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PirateNets: Physics-informed Deep Learning with Residual Adaptive Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00326
&lt;/p&gt;
&lt;p&gt;
PirateNets&#26159;&#19968;&#31181;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;&#32593;&#32476;&#22312;&#36739;&#22823;&#28145;&#24230;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#27531;&#24046;&#36830;&#25509;&#23454;&#29616;&#20102;&#31283;&#23450;&#21644;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#24182;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#31070;&#32463;&#32593;&#32476;(PINNs)&#24050;&#25104;&#20026;&#35299;&#20915;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#25511;&#21046;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#30340;&#27969;&#34892;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20294;&#22312;&#37319;&#29992;&#26356;&#22823;&#21644;&#26356;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#31181;&#21453;&#30452;&#35273;&#34892;&#20026;&#30340;&#26681;&#28304;&#22312;&#20110;&#20351;&#29992;&#19981;&#36866;&#21512;&#30340;&#21021;&#22987;&#21270;&#26041;&#26696;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;(MLP)&#32593;&#32476;&#32467;&#26500;&#65292;&#23548;&#33268;&#32593;&#32476;&#23548;&#25968;&#30340;&#21487;&#35757;&#32451;&#24615;&#36739;&#24046;&#65292;&#24182;&#26368;&#32456;&#23548;&#33268;PDE&#27531;&#24046;&#25439;&#22833;&#30340;&#19981;&#31283;&#23450;&#26368;&#23567;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#27531;&#24046;&#33258;&#36866;&#24212;&#32593;&#32476;(PirateNets)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#22411;&#26550;&#26500;&#65292;&#26088;&#22312;&#20419;&#36827;&#28145;&#24230;PINN&#27169;&#22411;&#30340;&#31283;&#23450;&#21644;&#39640;&#25928;&#35757;&#32451;&#12290;PirateNets&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#27531;&#24046;&#36830;&#25509;&#65292;&#20801;&#35768;&#32593;&#32476;&#20316;&#20026;&#27973;&#23618;&#32593;&#32476;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#21152;&#28145;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#21021;&#22987;&#21270;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;PINN&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#24182;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While physics-informed neural networks (PINNs) have become a popular deep learning framework for tackling forward and inverse problems governed by partial differential equations (PDEs), their performance is known to degrade when larger and deeper neural network architectures are employed. Our study identifies that the root of this counter-intuitive behavior lies in the use of multi-layer perceptron (MLP) architectures with non-suitable initialization schemes, which result in poor trainablity for the network derivatives, and ultimately lead to an unstable minimization of the PDE residual loss. To address this, we introduce Physics-informed Residual Adaptive Networks (PirateNets), a novel architecture that is designed to facilitate stable and efficient training of deep PINN models. PirateNets leverage a novel adaptive residual connection, which allows the networks to be initialized as shallow networks that progressively deepen during training. We also show that the proposed initializatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;ODE&#26041;&#27861;&#65292;&#23558;&#31283;&#23450;&#24615;&#30340;Borkar-Meyn&#23450;&#29702;&#20174;&#38789;&#24046;&#24322;&#22122;&#22768;&#35774;&#23450;&#25299;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#35774;&#23450;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.07844</link><description>&lt;p&gt;
&#20351;&#29992;ODE&#26041;&#27861;&#36827;&#34892;&#24102;&#26377;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#30340;&#38543;&#26426;&#36924;&#36817;&#21644;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
The ODE Method for Stochastic Approximation and Reinforcement Learning with Markovian Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;ODE&#26041;&#27861;&#65292;&#23558;&#31283;&#23450;&#24615;&#30340;Borkar-Meyn&#23450;&#29702;&#20174;&#38789;&#24046;&#24322;&#22122;&#22768;&#35774;&#23450;&#25299;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#35774;&#23450;&#65292;&#25552;&#39640;&#20102;&#20854;&#22312;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#36924;&#36817;&#26159;&#19968;&#31867;&#36890;&#36807;&#36845;&#20195;&#12289;&#22686;&#37327;&#21644;&#38543;&#26426;&#26356;&#26032;&#21521;&#37327;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#12290;&#20998;&#26512;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#30830;&#20445;&#20854;&#31283;&#23450;&#24615;&#65292;&#21363;&#35777;&#26126;&#38543;&#26426;&#21521;&#37327;&#36845;&#20195;&#20960;&#20046;&#24517;&#23450;&#26377;&#30028;&#12290;&#26412;&#25991;&#23558;&#31283;&#23450;&#24615;&#30340;Borkar-Meyn&#23450;&#29702;&#20174;&#38789;&#24046;&#24322;&#22122;&#22768;&#35774;&#23450;&#25299;&#23637;&#21040;&#39532;&#23572;&#21487;&#22827;&#22122;&#22768;&#35774;&#23450;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#20854;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#36164;&#26684;&#36857;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#30340;&#26680;&#24515;&#22312;&#20110;&#23569;&#25968;&#20989;&#25968;&#30340;&#28176;&#36827;&#21464;&#21270;&#36895;&#29575;&#19979;&#38477;&#65292;&#36825;&#19968;&#28857;&#30001;&#22823;&#25968;&#23450;&#24459;&#21644;&#24120;&#29992;&#30340;V4 Lyapunov&#28418;&#31227;&#26465;&#20214;&#38544;&#21547;&#65292;&#24182;&#22312;&#39532;&#23572;&#21487;&#22827;&#38142;&#26159;&#26377;&#38480;&#19988;&#19981;&#21487;&#32422;&#26102;&#26174;&#28982;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic approximation is a class of algorithms that update a vector iteratively, incrementally, and stochastically, including, e.g., stochastic gradient descent and temporal difference learning. One fundamental challenge in analyzing a stochastic approximation algorithm is to establish its stability, i.e., to show that the stochastic vector iterates are bounded almost surely. In this paper, we extend the celebrated Borkar-Meyn theorem for stability from the Martingale difference noise setting to the Markovian noise setting, which greatly improves its applicability in reinforcement learning, especially in those off-policy reinforcement learning algorithms with linear function approximation and eligibility traces. Central to our analysis is the diminishing asymptotic rate of change of a few functions, which is implied by both a form of strong law of large numbers and a commonly used V4 Lyapunov drift condition and trivially holds if the Markov chain is finite and irreducible.
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#39046;&#22495;&#20135;&#29983;&#20102;&#39072;&#35206;&#24615;&#30340;&#24433;&#21709;&#65292;&#32553;&#23567;&#20102;&#22270;&#20687;&#36136;&#37327;&#19982;&#20154;&#31867;&#24863;&#30693;&#20559;&#22909;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;DM&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#20998;&#26512;&#20102;&#20854;&#29420;&#29305;&#29305;&#28857;&#21644;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#26367;&#20195;&#36755;&#20837;&#39046;&#22495;&#31561;&#24403;&#21069;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2401.00736</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#12289;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#21644;&#19968;&#20999;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models, Image Super-Resolution And Everything: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00736
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#39046;&#22495;&#20135;&#29983;&#20102;&#39072;&#35206;&#24615;&#30340;&#24433;&#21709;&#65292;&#32553;&#23567;&#20102;&#22270;&#20687;&#36136;&#37327;&#19982;&#20154;&#31867;&#24863;&#30693;&#20559;&#22909;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;DM&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#20998;&#26512;&#20102;&#20854;&#29420;&#29305;&#29305;&#28857;&#21644;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#26367;&#20195;&#36755;&#20837;&#39046;&#22495;&#31561;&#24403;&#21069;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#39046;&#22495;&#20013;&#20135;&#29983;&#20102;&#39072;&#35206;&#24615;&#30340;&#24433;&#21709;&#65292;&#36827;&#19968;&#27493;&#32553;&#23567;&#20102;&#22270;&#20687;&#36136;&#37327;&#19982;&#20154;&#31867;&#24863;&#30693;&#20559;&#22909;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23427;&#20204;&#26131;&#20110;&#35757;&#32451;&#65292;&#24182;&#33021;&#29983;&#25104;&#27604;&#20197;&#21069;&#30340;&#29983;&#25104;&#26041;&#27861;&#20135;&#29983;&#30340;&#26679;&#26412;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65306;&#39640;&#35745;&#31639;&#38656;&#27714;&#12289;&#21487;&#27604;&#24615;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12289;&#33394;&#24425;&#20559;&#31227;&#31561;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#22823;&#37327;&#30340;&#20986;&#29256;&#29289;&#65292;&#36827;&#20837;&#36825;&#20010;&#39046;&#22495;&#20196;&#20154;&#38590;&#20197;&#24212;&#23545;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21465;&#36848;&#65292;&#38416;&#26126;&#20102;&#24212;&#29992;&#20110;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#30340;DM&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20221;&#35814;&#32454;&#30340;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#35813;&#39046;&#22495;&#20869;&#19982;&#20854;&#20182;&#32508;&#36848;&#25991;&#31456;&#19981;&#21516;&#30340;&#29420;&#29305;&#29305;&#28857;&#21644;&#26041;&#27861;&#12290;&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#23545;DM&#30340;&#21407;&#21017;&#36827;&#34892;&#20102;&#19968;&#20010;&#36830;&#36143;&#30340;&#29702;&#35299;&#65292;&#24182;&#25506;&#32034;&#20102;&#24403;&#21069;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#21253;&#25324;&#26367;&#20195;&#36755;&#20837;&#39046;&#22495;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models (DMs) have disrupted the image Super-Resolution (SR) field and further closed the gap between image quality and human perceptual preferences. They are easy to train and can produce very high-quality samples that exceed the realism of those produced by previous generative methods. Despite their promising results, they also come with new challenges that need further research: high computational demands, comparability, lack of explainability, color shifts, and more. Unfortunately, entry into this field is overwhelming because of the abundance of publications. To address this, we provide a unified recount of the theoretical foundations underlying DMs applied to image SR and offer a detailed analysis that underscores the unique characteristics and methodologies within this domain, distinct from broader existing reviews in the field. This survey articulates a cohesive understanding of DM principles and explores current research avenues, including alternative input domains, c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#20248;&#21270;&#22120;&#65292;&#29992;&#20110;&#23545;&#22810;&#30446;&#26631;&#36951;&#20256;&#31639;&#27861;&#20013;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#36873;&#25321;&#36827;&#34892;&#31995;&#32479;&#24615;&#36807;&#20272;&#35843;&#25972;&#12290;</title><link>https://arxiv.org/abs/2312.16624</link><description>&lt;p&gt;
&#21452;&#38454;&#27573;&#20248;&#21270;&#22120;&#29992;&#20110;&#31995;&#32479;&#24615;&#36807;&#20272;&#35843;&#25972;&#24212;&#29992;&#20110;&#22810;&#30446;&#26631;&#36951;&#20256;&#31639;&#27861;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Dual-stage optimizer for systematic overestimation adjustment applied to multi-objective genetic algorithms for biomarker selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16624
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#20248;&#21270;&#22120;&#65292;&#29992;&#20110;&#23545;&#22810;&#30446;&#26631;&#36951;&#20256;&#31639;&#27861;&#20013;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#36873;&#25321;&#36827;&#34892;&#31995;&#32479;&#24615;&#36807;&#20272;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#32452;&#23398;&#25968;&#25454;&#20013;&#21457;&#29616;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#25361;&#25112;&#22312;&#20110;&#20998;&#23376;&#29305;&#24449;&#30340;&#20016;&#23500;&#24615;&#20294;&#26679;&#26412;&#30340;&#31232;&#32570;&#24615;&#12290;&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#38656;&#35201;&#35780;&#20272;&#21508;&#31181;&#29305;&#24449;&#38598;&#65288;&#27169;&#22411;&#65289;&#20197;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#32452;&#21512;&#12290;&#36825;&#20010;&#36807;&#31243;&#36890;&#24120;&#20351;&#29992;&#39564;&#35777;&#25968;&#25454;&#38598;&#36827;&#34892;&#65292;&#28041;&#21450;&#27979;&#35797;&#19981;&#21516;&#30340;&#29305;&#24449;&#38598;&#20197;&#20248;&#21270;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#35780;&#20272;&#23384;&#22312;&#24615;&#33021;&#20272;&#35745;&#35823;&#24046;&#65292;&#24403;&#36873;&#25321;&#28041;&#21450;&#22810;&#20010;&#27169;&#22411;&#26102;&#65292;&#26368;&#22909;&#30340;&#27169;&#22411;&#20960;&#20046;&#32943;&#23450;&#34987;&#36807;&#20272;&#12290;&#20351;&#29992;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#36827;&#34892;&#29983;&#29289;&#26631;&#24535;&#29289;&#35782;&#21035;&#21487;&#20197;&#35270;&#20026;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#21644;&#29305;&#24449;&#25968;&#37327;&#20013;&#30340;&#31616;&#32422;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#22810;&#30446;&#26631;&#38382;&#39064;&#12290;&#36951;&#20256;&#31639;&#27861;&#26159;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#30340;&#19968;&#31181;&#27969;&#34892;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#36827;&#21270;&#20986;&#35768;&#22810;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#27492;&#23481;&#26131;&#20986;&#29616;&#36807;&#20272;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#22312;&#36873;&#25321;&#20102;&#27169;&#22411;&#21518;&#20943;&#23569;&#36807;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge in biomarker discovery using machine learning from omics data lies in the abundance of molecular features but scarcity of samples. Most feature selection methods in machine learning require evaluating various sets of features (models) to determine the most effective combination. This process, typically conducted using a validation dataset, involves testing different feature sets to optimize the model's performance. Evaluations have performance estimation error and when the selection involves many models the best ones are almost certainly overestimated. Biomarker identification with feature selection methods can be addressed as a multi-objective problem with trade-offs between predictive ability and parsimony in the number of features. Genetic algorithms are a popular tool for multi-objective optimization but they evolve numerous solutions thus are prone to overestimation. Methods have been proposed to reduce the overestimation after a model has already been selected in si
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#30340;Switchback&#23454;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25972;&#20010;&#26102;&#38388;&#22359;&#65292;&#20197; $\sqrt{\log T/T}$ &#30340;&#36895;&#29575;&#20272;&#35745;&#20840;&#23616;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#12290;</title><link>https://arxiv.org/abs/2312.15574</link><description>&lt;p&gt;
&#26356;&#24555;&#36895;&#30340;Switchback&#23454;&#39564;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Faster Rates for Switchback Experiments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#30340;Switchback&#23454;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25972;&#20010;&#26102;&#38388;&#22359;&#65292;&#20197; $\sqrt{\log T/T}$ &#30340;&#36895;&#29575;&#20272;&#35745;&#20840;&#23616;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Switchback&#23454;&#39564;&#35774;&#35745;&#20013;&#65292;&#19968;&#20010;&#21333;&#29420;&#30340;&#21333;&#20803;&#65288;&#20363;&#22914;&#25972;&#20010;&#31995;&#32479;&#65289;&#22312;&#20132;&#26367;&#30340;&#26102;&#38388;&#22359;&#20013;&#26292;&#38706;&#20110;&#19968;&#20010;&#38543;&#26426;&#22788;&#29702;&#65292;&#22788;&#29702;&#24182;&#34892;&#22788;&#29702;&#20102;&#36328;&#21333;&#20803;&#21644;&#26102;&#38388;&#24178;&#25200;&#38382;&#39064;&#12290;Hu&#21644;Wager&#65288;2022&#65289;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#25130;&#26029;&#22359;&#36215;&#22987;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#65292;&#24182;&#22312;Markov&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#29992;&#20110;&#20272;&#35745;&#20840;&#23616;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;GATE&#65289;&#30340;$T^{-1/3}$&#36895;&#29575;&#65292;&#20182;&#20204;&#22768;&#31216;&#36825;&#20010;&#36895;&#29575;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#24314;&#35758;&#23558;&#27880;&#24847;&#21147;&#36716;&#21521;&#19981;&#21516;&#65288;&#19988;&#20381;&#36182;&#35774;&#35745;&#65289;&#30340;&#20272;&#35745;&#37327;&#65292;&#20197;&#33719;&#24471;&#26356;&#24555;&#30340;&#36895;&#29575;&#12290;&#23545;&#20110;&#30456;&#21516;&#30340;&#35774;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#20272;&#35745;&#22120;&#65292;&#20351;&#29992;&#25972;&#20010;&#22359;&#65292;&#24182;&#24778;&#20154;&#22320;&#35777;&#26126;&#65292;&#22312;&#30456;&#21516;&#30340;&#20551;&#35774;&#19979;&#65292;&#23427;&#23454;&#38469;&#19978;&#36798;&#21040;&#20102;&#21407;&#22987;&#30340;&#35774;&#35745;&#29420;&#31435;GATE&#20272;&#35745;&#37327;&#30340;$\sqrt{\log T/T}$&#30340;&#20272;&#35745;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Switchback experimental design, wherein a single unit (e.g., a whole system) is exposed to a single random treatment for interspersed blocks of time, tackles both cross-unit and temporal interference. Hu and Wager (2022) recently proposed a treatment-effect estimator that truncates the beginnings of blocks and established a $T^{-1/3}$ rate for estimating the global average treatment effect (GATE) in a Markov setting with rapid mixing. They claim this rate is optimal and suggest focusing instead on a different (and design-dependent) estimand so as to enjoy a faster rate. For the same design we propose an alternative estimator that uses the whole block and surprisingly show that it in fact achieves an estimation rate of $\sqrt{\log T/T}$ for the original design-independent GATE estimand under the same assumptions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#33258;&#21160;&#39550;&#39542;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#38543;&#30528;&#35268;&#27169;&#22686;&#21152;&#65292;&#31574;&#30053;&#24615;&#33021;&#30340;&#25913;&#21892;&#12290;&#19982;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#31574;&#30053;&#23558;&#25925;&#38556;&#29575;&#38477;&#20302;&#20102;64&#65285;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;25&#65285;&#30340;&#39550;&#39542;&#36827;&#23637;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2312.15122</link><description>&lt;p&gt;
&#25193;&#23637;&#23601;&#26159;&#19968;&#20999;&#65306;&#20351;&#29992;JAX&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Scaling Is All You Need: Autonomous Driving with JAX-Accelerated Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#33258;&#21160;&#39550;&#39542;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#38543;&#30528;&#35268;&#27169;&#22686;&#21152;&#65292;&#31574;&#30053;&#24615;&#33021;&#30340;&#25913;&#21892;&#12290;&#19982;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#31574;&#30053;&#23558;&#25925;&#38556;&#29575;&#38477;&#20302;&#20102;64&#65285;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;25&#65285;&#30340;&#39550;&#39542;&#36827;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#22797;&#26434;&#39046;&#22495;&#22914;&#35270;&#39057;&#28216;&#25103;&#20013;&#23637;&#29616;&#20986;&#36229;&#36234;&#26368;&#20248;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#36816;&#34892;&#24517;&#35201;&#35268;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;&#23454;&#39564;&#38750;&#24120;&#22256;&#38590;&#12290;&#26500;&#24314;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#24182;&#22312;&#22810;&#20010;GPU&#19978;&#36827;&#34892;&#20998;&#24067;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22312;&#30495;&#23454;&#19990;&#30028;&#36710;&#36742;&#19978;&#25910;&#38598;&#32463;&#39564;&#20174;&#23433;&#20840;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#35282;&#24230;&#26469;&#30475;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20010;&#39640;&#25928;&#19988;&#30495;&#23454;&#30340;&#39550;&#39542;&#27169;&#25311;&#22120;&#65292;&#20351;&#29992;&#22823;&#37327;&#26469;&#33258;&#30495;&#23454;&#39550;&#39542;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#33021;&#21147;&#38598;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;&#23454;&#39564;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#34920;&#29616;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;&#25105;&#20204;&#26368;&#20339;&#31574;&#30053;&#23558;&#25925;&#38556;&#29575;&#38477;&#20302;&#20102;64&#65285;&#65292;&#21516;&#26102;&#27604;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#31574;&#30053;&#25552;&#39640;&#20102;25&#65285;&#30340;&#39550;&#39542;&#36827;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning has been demonstrated to outperform even the best humans in complex domains like video games. However, running reinforcement learning experiments on the required scale for autonomous driving is extremely difficult. Building a large scale reinforcement learning system and distributing it across many GPUs is challenging. Gathering experience during training on real world vehicles is prohibitive from a safety and scalability perspective. Therefore, an efficient and realistic driving simulator is required that uses a large amount of data from real-world driving. We bring these capabilities together and conduct large-scale reinforcement learning experiments for autonomous driving. We demonstrate that our policy performance improves with increasing scale. Our best performing policy reduces the failure rate by 64% while improving the rate of driving progress by 25% compared to the policies produced by state-of-the-art machine learning for autonomous driving.
&lt;/p&gt;</description></item><item><title>XLand-MiniGrid&#26159;&#19968;&#20010;&#22312;JAX&#20013;&#21487;&#25193;&#23637;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#24037;&#20855;&#22871;&#20214;&#65292;&#25552;&#20379;&#20102;&#21253;&#21547;&#25968;&#30334;&#19975;&#20010;&#19981;&#21516;&#38590;&#24230;&#30340;&#20219;&#21153;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#36164;&#28304;&#19979;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#27665;&#20027;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.12044</link><description>&lt;p&gt;
XLand-MiniGrid: &#22312;JAX&#20013;&#21487;&#25193;&#23637;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
XLand-MiniGrid: Scalable Meta-Reinforcement Learning Environments in JAX
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12044
&lt;/p&gt;
&lt;p&gt;
XLand-MiniGrid&#26159;&#19968;&#20010;&#22312;JAX&#20013;&#21487;&#25193;&#23637;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#24037;&#20855;&#22871;&#20214;&#65292;&#25552;&#20379;&#20102;&#21253;&#21547;&#25968;&#30334;&#19975;&#20010;&#19981;&#21516;&#38590;&#24230;&#30340;&#20219;&#21153;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#22522;&#32447;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#36164;&#28304;&#19979;&#30340;&#22823;&#35268;&#27169;&#23454;&#39564;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;XLand&#30340;&#22810;&#26679;&#24615;&#21644;&#28145;&#24230;&#20197;&#21450;MiniGrid&#30340;&#31616;&#21333;&#21644;&#31616;&#32422;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;XLand-MiniGrid&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20803;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#24037;&#20855;&#22871;&#20214;&#21644;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#12290;XLand-MiniGrid&#37319;&#29992;JAX&#32534;&#20889;&#65292;&#26088;&#22312;&#39640;&#24230;&#21487;&#25193;&#23637;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;GPU&#25110;TPU&#21152;&#36895;&#22120;&#19978;&#36816;&#34892;&#65292;&#29992;&#26377;&#38480;&#36164;&#28304;&#23454;&#29616;&#22823;&#35268;&#27169;&#23454;&#39564;&#30340;&#27665;&#20027;&#21270;&#12290;&#38500;&#20102;&#29615;&#22659;&#22806;&#65292;XLand-MiniGrid&#36824;&#25552;&#20379;&#20102;&#39044;&#37319;&#26679;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#30334;&#19975;&#20010;&#19981;&#21516;&#38590;&#24230;&#30340;&#29420;&#29305;&#20219;&#21153;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#22522;&#32447;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#24555;&#36895;&#24320;&#22987;&#35757;&#32451;&#33258;&#36866;&#24212;&#20195;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#35268;&#27169;&#21270;&#21644;&#27867;&#21270;&#30340;&#21021;&#27493;&#20998;&#26512;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#22522;&#32447;&#22312;&#35757;&#32451;&#20013;&#21487;&#20197;&#36798;&#21040;&#27599;&#31186;&#25968;&#30334;&#19975;&#27493;&#65292;&#24182;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#22522;&#20934;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the diversity and depth of XLand and the simplicity and minimalism of MiniGrid, we present XLand-MiniGrid, a suite of tools and grid-world environments for meta-reinforcement learning research. Written in JAX, XLand-MiniGrid is designed to be highly scalable and can potentially run on GPU or TPU accelerators, democratizing large-scale experimentation with limited resources. Along with the environments, XLand-MiniGrid provides pre-sampled benchmarks with millions of unique tasks of varying difficulty and easy-to-use baselines that allow users to quickly start training adaptive agents. In addition, we have conducted a preliminary analysis of scaling and generalization, showing that our baselines are capable of reaching millions of steps per second during training and validating that the proposed benchmarks are challenging.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23398;&#26694;&#26550;&#29992;&#20110;&#20998;&#26512;Transformers&#65292;&#24182;&#25581;&#31034;&#20102;&#22312;&#38271;&#26102;&#38388;&#19979;&#30340;&#38598;&#22242;&#24418;&#25104;&#12290;&#36825;&#19968;&#30740;&#31350;&#20026;&#25968;&#23398;&#23478;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2312.10794</link><description>&lt;p&gt;
Transformers&#30340;&#25968;&#23398;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A mathematical perspective on Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10794
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23398;&#26694;&#26550;&#29992;&#20110;&#20998;&#26512;Transformers&#65292;&#24182;&#25581;&#31034;&#20102;&#22312;&#38271;&#26102;&#38388;&#19979;&#30340;&#38598;&#22242;&#24418;&#25104;&#12290;&#36825;&#19968;&#30740;&#31350;&#20026;&#25968;&#23398;&#23478;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#25105;&#20204;&#22522;&#20110;&#23558;Transformers&#35299;&#37322;&#20026;&#30456;&#20114;&#20316;&#29992;&#30340;&#31890;&#23376;&#31995;&#32479;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#26469;&#20998;&#26512;Transformers&#65292;&#25581;&#31034;&#20102;&#38271;&#26102;&#38388;&#19979;&#30340;&#38598;&#22242;&#24418;&#25104;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#28508;&#22312;&#30340;&#29702;&#35770;&#65292;&#24182;&#20026;&#25968;&#23398;&#23478;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers play a central role in the inner workings of large language models. We develop a mathematical framework for analyzing Transformers based on their interpretation as interacting particle systems, which reveals that clusters emerge in long time. Our study explores the underlying theory and offers new perspectives for mathematicians as well as computer scientists.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#31995;&#32479;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21442;&#25968;&#21644;&#27010;&#29575;&#20998;&#24067;&#30340;&#25193;&#23637;&#31354;&#38388;&#19978;&#26368;&#23567;&#21270;&#33258;&#30001;&#33021;&#20989;&#25968;&#65292;&#35813;&#26041;&#27861;&#34701;&#21512;&#20102;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#12289;&#27424;&#38459;&#23612;Langevin&#25193;&#25955;&#21644;p&#12290;</title><link>https://arxiv.org/abs/2312.07335</link><description>&lt;p&gt;
&#21160;&#37327;&#31890;&#23376;&#26368;&#22823;&#20284;&#28982;
&lt;/p&gt;
&lt;p&gt;
Momentum Particle Maximum Likelihood
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07335
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#31995;&#32479;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#21442;&#25968;&#21644;&#27010;&#29575;&#20998;&#24067;&#30340;&#25193;&#23637;&#31354;&#38388;&#19978;&#26368;&#23567;&#21270;&#33258;&#30001;&#33021;&#20989;&#25968;&#65292;&#35813;&#26041;&#27861;&#34701;&#21512;&#20102;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#12289;&#27424;&#38459;&#23612;Langevin&#25193;&#25955;&#21644;p&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;(MLE)&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#36890;&#24120;&#34987;&#37325;&#26032;&#35299;&#37322;&#20026;&#22312;&#21442;&#25968;&#21644;&#27010;&#29575;&#20998;&#24067;&#30340;&#25193;&#23637;&#31354;&#38388;&#19978;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#20363;&#22914;&#65292;&#26399;&#26395;&#26368;&#22823;&#21270;(EM)&#31639;&#27861;&#21487;&#20197;&#35299;&#37322;&#20026;&#22312;&#36825;&#20010;&#31354;&#38388;&#19978;&#36866;&#29992;&#20110;&#21512;&#36866;&#30340;&#33258;&#30001;&#33021;&#20989;&#25968;&#30340;&#22352;&#26631;&#19979;&#38477;&#12290;&#26368;&#36817;&#65292;&#36825;&#20010;&#35266;&#28857;&#19982;&#20174;&#26368;&#20248;&#20256;&#36755;&#21644;Wasserstein&#26799;&#24230;&#27969;&#20013;&#33719;&#24471;&#30340;&#21551;&#31034;&#30456;&#32467;&#21512;&#65292;&#21457;&#23637;&#20986;&#20102;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#27169;&#22411;&#31867;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#26631;&#20934;&#30340;EM&#12290;&#21463;&#20808;&#21069;&#35770;&#25991;&#30340;&#21551;&#21457;&#65292;&#23558;&#23558;&#21160;&#37327;&#20016;&#23500;&#30340;&#20248;&#21270;&#31639;&#27861;&#35299;&#37322;&#20026;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#31163;&#25955;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#20284;&#30340;&#21160;&#24577;&#31995;&#32479;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#22312;&#21442;&#25968;&#21644;&#27010;&#29575;&#20998;&#24067;&#30340;&#25193;&#23637;&#31354;&#38388;&#19978;&#30340;&#33258;&#30001;&#33021;&#20989;&#25968;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#21160;&#24577;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#12289;&#27424;&#38459;&#23612;Langevin&#25193;&#25955;&#21644;p&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum likelihood estimation (MLE) of latent variable models is often recast as an optimization problem over the extended space of parameters and probability distributions. For example, the Expectation Maximization (EM) algorithm can be interpreted as coordinate descent applied to a suitable free energy functional over this space. Recently, this perspective has been combined with insights from optimal transport and Wasserstein gradient flows to develop particle-based algorithms applicable to wider classes of models than standard EM.   Drawing inspiration from prior works which interpret `momentum-enriched' optimisation algorithms as discretizations of ordinary differential equations, we propose an analogous dynamical systems-inspired approach to minimizing the free energy functional over the extended space of parameters and probability distributions. The result is a dynamic system that blends elements of Nesterov's Accelerated Gradient method, the underdamped Langevin diffusion, and p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#22330;&#26223;&#20013;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#19978;&#19979;&#25991;&#35760;&#24518;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#20869;&#23384;&#31354;&#38388;&#20013;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#65292;&#25552;&#39640;&#21534;&#21520;&#37327;&#65292;&#24182;&#36890;&#36807;&#20010;&#24615;&#21270;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.03414</link><description>&lt;p&gt;
&#22312;&#32447;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#20013;&#30340;&#21387;&#32553;&#19978;&#19979;&#25991;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Compressed Context Memory For Online Language Model Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#22330;&#26223;&#20013;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#19978;&#19979;&#25991;&#35760;&#24518;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#20869;&#23384;&#31354;&#38388;&#20013;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#65292;&#25552;&#39640;&#21534;&#21520;&#37327;&#65292;&#24182;&#36890;&#36807;&#20010;&#24615;&#21270;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#22330;&#26223;&#20013;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#38190;/&#20540;&#21387;&#32553;&#26041;&#27861;&#65292;&#20854;&#20013;&#19978;&#19979;&#25991;&#19981;&#26029;&#25193;&#23637;&#12290;&#38543;&#30528;&#19978;&#19979;&#25991;&#30340;&#22686;&#21152;&#65292;&#27880;&#24847;&#21147;&#36807;&#31243;&#38656;&#35201;&#26356;&#22810;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#65292;&#36827;&#32780;&#38477;&#20302;&#35821;&#35328;&#27169;&#22411;&#30340;&#21534;&#21520;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21387;&#32553;&#19978;&#19979;&#25991;&#35760;&#24518;&#31995;&#32479;&#65292;&#23558;&#32047;&#31215;&#30340;&#27880;&#24847;&#21147;&#38190;/&#20540;&#23545;&#19981;&#26029;&#21387;&#32553;&#21040;&#32039;&#20945;&#30340;&#20869;&#23384;&#31354;&#38388;&#20013;&#65292;&#20197;&#20415;&#22312;&#35745;&#31639;&#29615;&#22659;&#30340;&#26377;&#38480;&#20869;&#23384;&#31354;&#38388;&#20013;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#21387;&#32553;&#36807;&#31243;&#28041;&#21450;&#23558;&#36731;&#37327;&#32423;&#30340;&#26465;&#20214;LoRA&#25972;&#21512;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#21069;&#21521;&#20256;&#36882;&#20013;&#36827;&#34892;&#25512;&#26029;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#27169;&#22411;&#30340;&#25152;&#26377;&#26435;&#37325;&#12290;&#36890;&#36807;&#23558;&#36882;&#24402;&#21387;&#32553;&#36807;&#31243;&#24314;&#27169;&#20026;&#21333;&#20010;&#24182;&#34892;&#21270;&#30340;&#21069;&#21521;&#35745;&#31639;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#23545;&#23545;&#35805;&#12289;&#20010;&#24615;&#21270;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a context key/value compression method for Transformer language models in online scenarios, where the context continually expands. As the context lengthens, the attention process demands increasing memory and computations, which in turn reduces the throughput of the language model. To address this challenge, we propose a compressed context memory system that continually compresses the accumulating attention key/value pairs into a compact memory space, facilitating language model inference in a limited memory space of computing environments. Our compression process involves integrating a lightweight conditional LoRA into the language model's forward pass during inference, without the need for fine-tuning the model's entire set of weights. We achieve efficient training by modeling the recursive compression process as a single parallelized forward computation. Through evaluations on conversation, personalization, and multi-task learning, we demonstrate that our approac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#22871;&#8220;&#21476;&#24618;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35843;&#21462;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#23637;&#31034;&#20102;&#20174;&#21487;&#20449;&#24230;&#20302;&#30340;&#27169;&#22411;&#20013;&#35843;&#21462;&#21487;&#38752;&#30693;&#35782;&#30340;&#21069;&#26223;&#12290;</title><link>https://arxiv.org/abs/2312.01037</link><description>&lt;p&gt;
&#20174;&#21476;&#24618;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#35843;&#21462;&#28508;&#22312;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Eliciting Latent Knowledge from Quirky Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#22871;&#8220;&#21476;&#24618;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35843;&#21462;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#23637;&#31034;&#20102;&#20174;&#21487;&#20449;&#24230;&#20302;&#30340;&#27169;&#22411;&#20013;&#35843;&#21462;&#21487;&#38752;&#30693;&#35782;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#21462;&#28508;&#22312;&#30693;&#35782;&#65288;ELK&#65289;&#26088;&#22312;&#22312;&#19968;&#20010;&#33021;&#21147;&#24378;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#28608;&#27963;&#20013;&#25214;&#21040;&#27169;&#24335;&#65292;&#21363;&#20351;&#32593;&#32476;&#30340;&#26126;&#26174;&#36755;&#20986;&#26159;&#38169;&#35823;&#25110;&#35823;&#23548;&#24615;&#30340;&#65292;&#20063;&#33021;&#31283;&#23450;&#36319;&#36394;&#19990;&#30028;&#30340;&#30495;&#23454;&#29366;&#24577;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;ELK&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;12&#20010;&#25968;&#25454;&#38598;&#21644;&#19968;&#22871;&#30456;&#24212;&#30340;&#8220;&#21476;&#24618;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#65292;&#21482;&#26377;&#22312;&#25552;&#31034;&#20013;&#21253;&#21547;&#20851;&#38190;&#35789;&#8220;Bob&#8221;&#26102;&#25165;&#20250;&#36827;&#34892;&#31995;&#32479;&#24615;&#38169;&#35823;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;&#25506;&#27979;&#26041;&#27861;&#21487;&#20197;&#35843;&#21462;&#27169;&#22411;&#22312;&#36825;&#20123;&#19978;&#19979;&#25991;&#20013;&#23545;&#27491;&#30830;&#31572;&#26696;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#21363;&#20351;&#38382;&#39064;&#27604;&#25506;&#27979;&#22120;&#35757;&#32451;&#30340;&#38382;&#39064;&#26356;&#22256;&#38590;&#12290;&#36825;&#26159;&#30001;&#20110;&#20013;&#38388;&#23618;&#28608;&#27963;&#20013;&#30340;&#19978;&#19979;&#25991;&#26080;&#20851;&#30340;&#30693;&#35782;&#34920;&#31034;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#19968;&#31181;&#26426;&#26800;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#20197;94%&#30340;AUROC&#26631;&#35782;&#19981;&#30495;&#23454;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20174;&#33021;&#21147;&#24378;&#20294;&#19981;&#21463;&#20449;&#20219;&#30340;&#27169;&#22411;&#20013;&#35843;&#21462;&#21487;&#38752;&#30340;&#30693;&#35782;&#65292;&#24182;&#20419;&#36827;&#26410;&#26469;&#30740;&#31350;ELK&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;&#26159;&#26377;&#24076;&#26395;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Eliciting Latent Knowledge (ELK) aims to find patterns in a capable neural network's activations which robustly track the true state of the world, even when the network's overt output is false or misleading. To further ELK research, we introduce 12 datasets and a corresponding suite of "quirky" language models that are LoRA finetuned to make systematic errors when answering questions if and only if the keyword "Bob" is present in the prompt. We demonstrate that simple probing methods can elicit the model's latent knowledge of the correct answer in these contexts, even for problems harder than those the probe was trained on. This is enabled by context-independent knowledge representations located in middle layer activations. We also find that a mechanistic anomaly detection approach can flag untruthful behavior with 94% AUROC. Our results show promise for eliciting reliable knowledge from capable but untrusted models, and facilitates future research empirically investigating ELK methods
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#19978;&#35757;&#32451;&#33258;&#22238;&#24402;Transformer&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#23398;&#20064;&#32452;&#21512;&#32467;&#26500;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#29983;&#25104;&#20013;&#38388;&#36755;&#20986;&#26469;&#32452;&#21512;&#20989;&#25968;&#22312;&#27867;&#21270;&#21040;&#26032;&#30340;&#26410;&#35265;&#32452;&#21512;&#26102;&#27604;&#19981;&#29983;&#25104;&#20013;&#38388;&#36755;&#20986;&#26356;&#26377;&#25928;&#12290;&#21516;&#26102;&#65292;&#32452;&#21512;&#39034;&#24207;&#30340;&#20559;&#24046;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2311.12997</link><description>&lt;p&gt;
&#33258;&#22238;&#24402;Transformer&#30340;&#32452;&#21512;&#33021;&#21147;&#65306;&#23545;&#21512;&#25104;&#30340;&#21487;&#35299;&#37322;&#20219;&#21153;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#19978;&#35757;&#32451;&#33258;&#22238;&#24402;Transformer&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#20854;&#23398;&#20064;&#32452;&#21512;&#32467;&#26500;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#29983;&#25104;&#20013;&#38388;&#36755;&#20986;&#26469;&#32452;&#21512;&#20989;&#25968;&#22312;&#27867;&#21270;&#21040;&#26032;&#30340;&#26410;&#35265;&#32452;&#21512;&#26102;&#27604;&#19981;&#29983;&#25104;&#20013;&#38388;&#36755;&#20986;&#26356;&#26377;&#25928;&#12290;&#21516;&#26102;&#65292;&#32452;&#21512;&#39034;&#24207;&#30340;&#20559;&#24046;&#20250;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#37327;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;Transformer&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#25191;&#34892;&#22522;&#26412;&#31639;&#26415;&#36816;&#31639;&#12290;&#32771;&#34385;&#21040;&#35821;&#35328;&#30340;&#20869;&#22312;&#32452;&#21512;&#24615;&#36136;&#65292;&#25105;&#20204;&#21487;&#20197;&#26399;&#26395;&#27169;&#22411;&#23398;&#20064;&#23558;&#36825;&#20123;&#33021;&#21147;&#32452;&#21512;&#36215;&#26469;&#65292;&#20174;&#32780;&#28508;&#22312;&#22320;&#20135;&#29983;&#23545;&#36755;&#20837;&#36827;&#34892;&#25805;&#20316;&#30340;&#32452;&#21512;&#29190;&#28856;&#12290;&#22522;&#20110;&#20197;&#19978;&#21160;&#26426;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#28041;&#21450;&#19968;&#32452;&#26126;&#30830;&#23450;&#20041;&#30340;&#25972;&#20307;&#33021;&#21147;&#32452;&#21512;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#19978;&#35757;&#32451;&#33258;&#22238;&#24402;Transformer&#27169;&#22411;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#24191;&#27867;&#32780;&#31995;&#32479;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;(1) &#33258;&#22238;&#24402;Transformer&#21487;&#20197;&#20174;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#32452;&#21512;&#32467;&#26500;&#65292;&#24182;&#19988;&#27867;&#21270;&#21040;&#25351;&#25968;&#29978;&#33267;&#32452;&#21512;&#29190;&#28856;&#25968;&#37327;&#30340;&#20989;&#25968;&#65307;(2) &#22312;&#32452;&#21512;&#20989;&#25968;&#26102;&#29983;&#25104;&#20013;&#38388;&#36755;&#20986;&#27604;&#19981;&#29983;&#25104;&#20219;&#20309;&#20013;&#38388;&#36755;&#20986;&#26356;&#26377;&#25928;&#22320;&#27867;&#21270;&#21040;&#26032;&#30340;&#12289;&#26410;&#35265;&#36807;&#30340;&#32452;&#21512;&#65307;(3) &#22312;&#32452;&#21512;&#39034;&#24207;&#20013;&#23384;&#22312;&#20559;&#24046;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers trained on huge text corpora exhibit a remarkable set of capabilities, e.g., performing basic arithmetic. Given the inherent compositional nature of language, one can expect the model to learn to compose these capabilities, potentially yielding a combinatorial explosion of what operations it can perform on an input. Motivated by the above, we train autoregressive Transformer models on a synthetic data-generating process that involves compositions of a set of well-defined monolithic capabilities. Through a series of extensive and systematic experiments on this data-generating process, we show that: (1) autoregressive Transformers can learn compositional structures from small amounts of training data and generalize to exponentially or even combinatorially many functions; (2) generating intermediate outputs when composing functions is more effective for generalizing to new, unseen compositions than not generating any intermediate outputs (3) biases in the order of the composi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#24615;Gram-Schmidt&#26041;&#27861;&#26469;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#21644;&#21435;&#38500;&#38750;&#32447;&#24615;&#20381;&#36182;&#24615;&#65292;&#20174;&#32780;&#25552;&#21462;&#25968;&#25454;&#20013;&#30340;&#32447;&#24615;&#29305;&#24449;&#24182;&#21435;&#38500;&#38750;&#32447;&#24615;&#20887;&#20313;&#12290;</title><link>https://arxiv.org/abs/2311.09386</link><description>&lt;p&gt;
&#36229;&#36234;PCA&#65306;&#19968;&#31181;&#27010;&#29575;&#24615;Gram-Schmidt&#26041;&#27861;&#30340;&#29305;&#24449;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Beyond PCA: A Probabilistic Gram-Schmidt Approach to Feature Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#24615;Gram-Schmidt&#26041;&#27861;&#26469;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#21644;&#21435;&#38500;&#38750;&#32447;&#24615;&#20381;&#36182;&#24615;&#65292;&#20174;&#32780;&#25552;&#21462;&#25968;&#25454;&#20013;&#30340;&#32447;&#24615;&#29305;&#24449;&#24182;&#21435;&#38500;&#38750;&#32447;&#24615;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#32447;&#24615;&#29305;&#24449;&#25552;&#21462;&#22312;&#25968;&#25454;&#20013;&#23384;&#22312;&#38750;&#32447;&#24615;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#26159;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#27010;&#29575;&#24615;Gram-Schmidt (GS)&#31867;&#22411;&#30340;&#27491;&#20132;&#21270;&#36807;&#31243;&#26469;&#26816;&#27979;&#21644;&#26144;&#23556;&#20986;&#20887;&#20313;&#32500;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#19968;&#26063;&#20989;&#25968;&#19978;&#24212;&#29992;GS&#36807;&#31243;&#65292;&#35813;&#26063;&#20989;&#25968;&#39044;&#35745;&#25429;&#25417;&#21040;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#26032;&#30340;&#22823;&#26041;&#24046;&#26041;&#21521;&#65292;&#25110;&#32773;&#23558;&#36825;&#20123;&#20381;&#36182;&#24615;&#20174;&#20027;&#25104;&#20998;&#20013;&#21435;&#38500;&#12290;&#22312;&#21069;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29109;&#20943;&#23569;&#30340;&#20449;&#24687;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#21518;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#25152;&#24471;&#31639;&#27861;&#22312;&#25152;&#36873;&#25321;&#20989;&#25968;&#26063;&#30340;&#32447;&#24615;&#24352;&#25104;&#31354;&#38388;&#20013;&#21487;&#20197;&#26816;&#27979;&#21644;&#21435;&#38500;&#38750;&#32447;&#24615;&#20381;&#36182;&#24615;&#12290;&#20004;&#31181;&#25552;&#20986;&#30340;&#26041;&#27861;&#37117;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#32447;&#24615;&#29305;&#24449;&#24182;&#21435;&#38500;&#38750;&#32447;&#24615;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear feature extraction at the presence of nonlinear dependencies among the data is a fundamental challenge in unsupervised learning. We propose using a probabilistic Gram-Schmidt (GS) type orthogonalization process in order to detect and map out redundant dimensions. Specifically, by applying the GS process over a family of functions which presumably captures the nonlinear dependencies in the data, we construct a series of covariance matrices that can either be used to identify new large-variance directions, or to remove those dependencies from the principal components. In the former case, we provide information-theoretic guarantees in terms of entropy reduction. In the latter, we prove that under certain assumptions the resulting algorithms detect and remove nonlinear dependencies whenever those dependencies lie in the linear span of the chosen function family. Both proposed methods extract linear features from the data while removing nonlinear redundancies. We provide simulation r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CAFE&#30340;&#30899;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22320;&#29702;&#20998;&#24067;&#24335;&#25968;&#25454;&#20013;&#24515;&#20013;&#20248;&#21270;&#35757;&#32451;&#36807;&#31243;&#65292;&#24179;&#34913;&#23398;&#20064;&#24615;&#33021;&#21644;&#30899;&#36275;&#36857;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;coreset&#36873;&#25321;&#35780;&#20272;&#23398;&#20064;&#24615;&#33021;&#65292;&#21033;&#29992;Lyapunov&#28418;&#31227;&#21152;&#24809;&#32602;&#26694;&#26550;&#22788;&#29702;&#26410;&#26469;&#30899;&#24378;&#24230;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2311.03615</link><description>&lt;p&gt;
CAFE&#65306;&#22312;&#22320;&#29702;&#20998;&#24067;&#24335;&#25968;&#25454;&#20013;&#24515;&#20013;&#30340;&#30899;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CAFE: Carbon-Aware Federated Learning in Geographically Distributed Data Centers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CAFE&#30340;&#30899;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22320;&#29702;&#20998;&#24067;&#24335;&#25968;&#25454;&#20013;&#24515;&#20013;&#20248;&#21270;&#35757;&#32451;&#36807;&#31243;&#65292;&#24179;&#34913;&#23398;&#20064;&#24615;&#33021;&#21644;&#30899;&#36275;&#36857;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;coreset&#36873;&#25321;&#35780;&#20272;&#23398;&#20064;&#24615;&#33021;&#65292;&#21033;&#29992;Lyapunov&#28418;&#31227;&#21152;&#24809;&#32602;&#26694;&#26550;&#22788;&#29702;&#26410;&#26469;&#30899;&#24378;&#24230;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#35757;&#32451;&#38656;&#35201;&#24040;&#22823;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#33021;&#28304;&#65292;&#23548;&#33268;&#30899;&#36275;&#36857;&#22686;&#21152;&#65292;&#21487;&#33021;&#23545;&#29615;&#22659;&#20135;&#29983;&#28508;&#22312;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22320;&#29702;&#20998;&#24067;&#24335;&#65288;&#22320;&#29702;&#20998;&#24067;&#30340;&#65289;&#25968;&#25454;&#20013;&#24515;&#20013;&#35757;&#32451;AI&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#24378;&#35843;&#23398;&#20064;&#24615;&#33021;&#21644;&#30899;&#36275;&#36857;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#32771;&#34385;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#23558;&#27169;&#22411;&#21442;&#25968;&#20132;&#25442;&#32622;&#20110;&#21407;&#22987;&#25968;&#25454;&#20043;&#19978;&#65292;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#24182;&#31526;&#21512;&#24403;&#22320;&#27861;&#35268;&#12290;&#37492;&#20110;&#19981;&#21516;&#22320;&#21306;&#30340;&#30899;&#24378;&#24230;&#21464;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CAFE&#65288;&#30899;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#20248;&#21270;&#22312;&#22266;&#23450;&#30899;&#36275;&#36857;&#39044;&#31639;&#20869;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;coreset&#36873;&#25321;&#26469;&#35780;&#20272;&#23398;&#20064;&#24615;&#33021;&#65292;&#37319;&#29992;Lyapunov&#28418;&#31227;&#21152;&#24809;&#32602;&#26694;&#26550;&#26469;&#22788;&#29702;&#26410;&#26469;&#30899;&#24378;&#24230;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training large-scale artificial intelligence (AI) models demands significant computational power and energy, leading to increased carbon footprint with potential environmental repercussions. This paper delves into the challenges of training AI models across geographically distributed (geo-distributed) data centers, emphasizing the balance between learning performance and carbon footprint. We consider Federated Learning (FL) as a solution, which prioritizes model parameter exchange over raw data, ensuring data privacy and compliance with local regulations. Given the variability in carbon intensity across regions, we propose a new framework called CAFE (short for Carbon-Aware Federated Learning) to optimize training within a fixed carbon footprint budget. Our approach incorporates coreset selection to assess learning performance, employs the Lyapunov drift-plus-penalty framework to address the unpredictability of future carbon intensity, and devises an efficient algorithm to address the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeepInception&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#35282;&#33394;&#25198;&#28436;&#33021;&#21147;&#26500;&#24314;&#26032;&#39062;&#30340;&#23884;&#22871;&#22330;&#26223;&#65292;&#25104;&#21151;&#20652;&#30496;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#30772;&#35299;&#32773;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;DeepInception&#22312;&#30772;&#35299;&#25104;&#21151;&#29575;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#24369;&#28857;&#12290;</title><link>https://arxiv.org/abs/2311.03191</link><description>&lt;p&gt;
DeepInception: &#20652;&#30496;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#30772;&#35299;&#32773;
&lt;/p&gt;
&lt;p&gt;
DeepInception: Hypnotize Large Language Model to Be Jailbreaker
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeepInception&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#35282;&#33394;&#25198;&#28436;&#33021;&#21147;&#26500;&#24314;&#26032;&#39062;&#30340;&#23884;&#22871;&#22330;&#26223;&#65292;&#25104;&#21151;&#20652;&#30496;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#30772;&#35299;&#32773;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;DeepInception&#22312;&#30772;&#35299;&#25104;&#21151;&#29575;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#30772;&#35299;&#25915;&#20987;&#65292;&#20351;&#24471;&#23433;&#20840;&#25514;&#26045;&#26080;&#25928;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30772;&#35299;&#30740;&#31350;&#36890;&#24120;&#37319;&#29992;&#26292;&#21147;&#20248;&#21270;&#25110;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#22806;&#25512;&#26041;&#27861;&#65292;&#36825;&#21487;&#33021;&#24182;&#19981;&#23454;&#38469;&#25110;&#26377;&#25928;&#12290;&#26412;&#25991;&#21463;&#21040;&#20197;&#31859;&#23572;&#26684;&#25289;&#22982;&#23454;&#39564;&#20026;&#28789;&#24863;&#65292;&#20851;&#20110;&#26435;&#23041;&#21147;&#37327;&#23545;&#20110;&#24341;&#21457;&#26377;&#23475;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;DeepInception&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#20652;&#30496;LLM&#25104;&#20026;&#30772;&#35299;&#32773;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DeepInception&#21033;&#29992;LLM&#30340;&#35282;&#33394;&#25198;&#28436;&#33021;&#21147;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23884;&#22871;&#22330;&#26223;&#26469;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#22312;&#27491;&#24120;&#22330;&#26223;&#19979;&#36867;&#36991;&#20351;&#29992;&#25511;&#21046;&#30340;&#33258;&#36866;&#24212;&#26041;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;DeepInception&#22312;&#30772;&#35299;&#25104;&#21151;&#29575;&#26041;&#38754;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#31454;&#20105;&#21147;&#30456;&#24403;&#65292;&#24182;&#21487;&#20197;&#22312;&#21518;&#32493;&#20132;&#20114;&#20013;&#23454;&#29616;&#25345;&#32493;&#30340;&#30772;&#35299;&#65292;&#25581;&#31034;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;LLM&#30340;&#33258;&#22833;&#20851;&#38190;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite remarkable success in various applications, large language models (LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrails void. However, previous studies for jailbreaks usually resort to brute-force optimization or extrapolations of a high computation cost, which might not be practical or effective. In this paper, inspired by the Milgram experiment w.r.t. the authority power for inciting harmfulness, we disclose a lightweight method, termed DeepInception, which can easily hypnotize LLM to be a jailbreaker. Specifically, DeepInception leverages the personification ability of LLM to construct a novel nested scene to behave, which realizes an adaptive way to escape the usage control in a normal scenario. Empirically, our DeepInception can achieve competitive jailbreak success rates with previous counterparts and realize a continuous jailbreak in subsequent interactions, which reveals the critical weakness of self-losing on both open and closed-source LLMs l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22240;&#26524;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#32508;&#21512;&#32771;&#34385;&#22240;&#26524;&#24615;&#12289;&#20010;&#20307;&#20844;&#24179;&#24615;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#31561;&#22240;&#32032;&#12290;&#36890;&#36807;&#22522;&#20110;&#22240;&#26524;&#32467;&#26500;&#21644;&#20445;&#25252;&#24615;&#22240;&#26524;&#25200;&#21160;&#26469;&#29983;&#25104;&#21487;&#27604;&#36739;&#30340;&#36755;&#20837;&#25968;&#25454;&#23454;&#20363;&#65292;&#24182;&#20511;&#21161;&#24230;&#37327;&#23398;&#20064;&#36827;&#34892;&#24230;&#37327;&#20272;&#35745;&#21644;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2310.19391</link><description>&lt;p&gt;
&#22240;&#26524;&#20844;&#24179;&#24230;&#37327;&#65306;&#36830;&#25509;&#22240;&#26524;&#24615;&#12289;&#20010;&#20307;&#20844;&#24179;&#24615;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Causal Fair Metric: Bridging Causality, Individual Fairness, and Adversarial Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22240;&#26524;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#32508;&#21512;&#32771;&#34385;&#22240;&#26524;&#24615;&#12289;&#20010;&#20307;&#20844;&#24179;&#24615;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#31561;&#22240;&#32032;&#12290;&#36890;&#36807;&#22522;&#20110;&#22240;&#26524;&#32467;&#26500;&#21644;&#20445;&#25252;&#24615;&#22240;&#26524;&#25200;&#21160;&#26469;&#29983;&#25104;&#21487;&#27604;&#36739;&#30340;&#36755;&#20837;&#25968;&#25454;&#23454;&#20363;&#65292;&#24182;&#20511;&#21161;&#24230;&#37327;&#23398;&#20064;&#36827;&#34892;&#24230;&#37327;&#20272;&#35745;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36127;&#36131;&#20219;AI&#38656;&#35201;&#20840;&#38754;&#32771;&#34385;&#22240;&#32032;&#65292;&#22914;&#40065;&#26834;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#22240;&#26524;&#24615;&#65292;&#20294;&#36825;&#20123;&#22240;&#32032;&#24120;&#24120;&#34987;&#23396;&#31435;&#22320;&#30740;&#31350;&#12290;&#29992;&#20110;&#35782;&#21035;&#27169;&#22411;&#20013;&#30340;&#28431;&#27934;&#30340;&#23545;&#25239;&#25200;&#21160;&#21644;&#20026;&#30456;&#20284;&#20010;&#20307;&#25552;&#20379;&#20844;&#24179;&#22788;&#29702;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#65292;&#23613;&#31649;&#26368;&#21021;&#23384;&#22312;&#24046;&#24322;&#65292;&#20294;&#37117;&#20381;&#36182;&#20110;&#29983;&#25104;&#21487;&#27604;&#36739;&#30340;&#36755;&#20837;&#25968;&#25454;&#23454;&#20363;&#30340;&#24230;&#37327;&#12290;&#20197;&#24448;&#20851;&#20110;&#23450;&#20041;&#36825;&#31181;&#32852;&#21512;&#24230;&#37327;&#30340;&#23581;&#35797;&#24448;&#24448;&#32570;&#20047;&#20851;&#20110;&#25968;&#25454;&#25110;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#30340;&#19968;&#33324;&#20551;&#35774;&#65292;&#24182;&#19988;&#26080;&#27861;&#21453;&#26144;&#21453;&#20107;&#23454;&#30340;&#25509;&#36817;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#32467;&#26500;&#30340;&#22240;&#26524;&#20844;&#24179;&#24230;&#37327;&#65292;&#21253;&#21547;&#25935;&#24863;&#23646;&#24615;&#21644;&#21463;&#20445;&#25252;&#30340;&#22240;&#26524;&#25200;&#21160;&#12290;&#20026;&#20102;&#22686;&#24378;&#25105;&#20204;&#30340;&#24230;&#37327;&#30340;&#23454;&#29992;&#24615;&#65292;&#22312;&#27809;&#26377;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24230;&#37327;&#23398;&#20064;&#20316;&#20026;&#24230;&#37327;&#20272;&#35745;&#21644;&#22312;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#20013;&#37096;&#32626;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26032;&#24230;&#37327;&#22312;&#20998;&#31867;&#22120;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the essential need for comprehensive considerations in responsible AI, factors like robustness, fairness, and causality are often studied in isolation. Adversarial perturbation, used to identify vulnerabilities in models, and individual fairness, aiming for equitable treatment of similar individuals, despite initial differences, both depend on metrics to generate comparable input data instances. Previous attempts to define such joint metrics often lack general assumptions about data or structural causal models and were unable to reflect counterfactual proximity. To address this, our paper introduces a causal fair metric formulated based on causal structures encompassing sensitive attributes and protected causal perturbation. To enhance the practicality of our metric, we propose metric learning as a method for metric estimation and deployment in real-world problems in the absence of structural causal models. We also demonstrate the application of our novel metric in classifiers.
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#36951;&#24536;&#65288;FU&#65289;&#26159;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#30340;&#25112;&#30053;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#21457;&#23637;FU&#26041;&#27861;&#26102;&#38656;&#35201;&#24179;&#34913;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#31454;&#20105;&#24615;&#35201;&#27714;&#65292;&#20197;&#32500;&#25345;FL&#31995;&#32479;&#30340;&#25928;&#26524;&#21644;&#21487;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.19218</link><description>&lt;p&gt;
&#32852;&#37030;&#36951;&#24536;&#30340;&#32508;&#36848;&#65306;&#20998;&#31867;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Survey of Federated Unlearning: A Taxonomy, Challenges and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19218
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#36951;&#24536;&#65288;FU&#65289;&#26159;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#30340;&#25112;&#30053;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#21457;&#23637;FU&#26041;&#27861;&#26102;&#38656;&#35201;&#24179;&#34913;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#31454;&#20105;&#24615;&#35201;&#27714;&#65292;&#20197;&#32500;&#25345;FL&#31995;&#32479;&#30340;&#25928;&#26524;&#21644;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38544;&#31169;&#20445;&#25252;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#21457;&#23637;&#65292;&#23545;&#23454;&#29616;&#34987;&#36951;&#24536;&#26435;&#30340;&#38656;&#27714;&#36234;&#26469;&#36234;&#22823;&#12290;&#30001;&#20110;FL&#30340;&#20998;&#25955;&#24615;&#36136;&#65292;&#23454;&#26045;&#36873;&#25321;&#24615;&#36951;&#24536;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#31181;&#22797;&#26434;&#24615;&#20652;&#29983;&#20102;&#19968;&#20010;&#26032;&#30340;&#39046;&#22495;&#65292;&#21363;&#32852;&#37030;&#36951;&#24536;&#65288;FU&#65289;&#12290;FU&#20316;&#20026;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#38656;&#27714;&#30340;&#25112;&#30053;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#23454;&#26045;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#12290;&#24320;&#21457;FU&#26041;&#27861;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#22312;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#22240;&#20026;&#36825;&#20123;&#22240;&#32032;&#24448;&#24448;&#20855;&#26377;&#31454;&#20105;&#24615;&#35201;&#27714;&#12290;&#22312;&#20445;&#25345;FL&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#36825;&#20123;&#26041;&#38754;&#30340;&#26368;&#20339;&#24179;&#34913;&#23545;&#20110;&#36981;&#23432;&#38544;&#31169;&#21644;&#23433;&#20840;&#26631;&#20934;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#32508;&#36848;&#23545;&#29616;&#26377;&#30340;FU&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21253;&#25324;&#23545;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#30340;&#35814;&#32454;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evolution of privacy-preserving Federated Learning (FL) has led to an increasing demand for implementing the right to be forgotten. The implementation of selective forgetting is particularly challenging in FL due to its decentralized nature. This complexity has given rise to a new field, Federated Unlearning (FU). FU emerges as a strategic solution to address the increasing need for data privacy, including the implementation of the `right to be forgotten'. The primary challenge in developing FU approaches lies in balancing the trade-offs in privacy, security, utility, and efficiency, as these elements often have competing requirements. Achieving an optimal equilibrium among these facets is crucial for maintaining the effectiveness and usability of FL systems while adhering to privacy and security standards. This survey provides a comprehensive analysis of existing FU methods, incorporating a detailed review of the various evaluation metrics. Furthermore, we unify these diverse meth
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;AIS&#25968;&#25454;&#39044;&#27979;&#33337;&#33334;&#36712;&#36857;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#33337;&#33334;&#19982;&#40120;&#40060;&#30896;&#25758;&#26469;&#24314;&#31435;&#26356;&#23433;&#20840;&#30340;&#28023;&#27915;&#29615;&#22659;&#12290;</title><link>https://arxiv.org/abs/2310.18948</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#36335;&#24452;&#38271;&#26399;&#33337;&#33334;&#36712;&#36857;&#39044;&#27979;&#24314;&#31435;&#26356;&#23433;&#20840;&#30340;&#28023;&#27915;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Building a Safer Maritime Environment Through Multi-Path Long-Term Vessel Trajectory Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18948
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;AIS&#25968;&#25454;&#39044;&#27979;&#33337;&#33334;&#36712;&#36857;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#33337;&#33334;&#19982;&#40120;&#40060;&#30896;&#25758;&#26469;&#24314;&#31435;&#26356;&#23433;&#20840;&#30340;&#28023;&#27915;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#19978;&#20132;&#36890;&#23545;&#20110;&#23454;&#29616;&#20840;&#29699;&#32463;&#27982;&#22686;&#38271;&#33267;&#20851;&#37325;&#35201;&#65292;&#21516;&#26102;&#20063;&#38656;&#35201;&#22312;&#21487;&#25345;&#32493;&#24615;&#21644;&#20445;&#25252;&#28626;&#21361;&#28023;&#27915;&#29289;&#31181;&#26041;&#38754;&#23653;&#34892;&#29983;&#24577;&#20041;&#21153;&#65292;&#23588;&#20854;&#26159;&#20445;&#25252;&#22823;&#22411;&#40120;&#31867;&#31181;&#32676;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#33258;&#21160;&#35782;&#21035;&#31995;&#32479;(AIS)&#25968;&#25454;&#36890;&#36807;&#25552;&#20379;&#33337;&#33334;&#36816;&#21160;&#30340;&#23454;&#26102;&#27969;&#25968;&#25454;&#65292;&#21487;&#20197;&#23454;&#29616;&#24378;&#21270;&#30340;&#20132;&#36890;&#30417;&#25511;&#65292;&#20174;&#32780;&#36991;&#20813;&#33337;&#33334;&#19982;&#40120;&#40060;&#30896;&#25758;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#21033;&#29992;AIS&#25968;&#25454;&#39044;&#27979;&#38271;&#26399;&#33337;&#33334;&#36712;&#36857;&#65292;&#20174;&#32780;&#39044;&#38450;&#33337;&#33334;&#19982;&#40120;&#40060;&#30340;&#30896;&#25758;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;(Bi-LSTM)&#26500;&#24314;&#20102;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;1&#21040;3&#23567;&#26102;&#30340;AIS&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#39044;&#27979;&#25509;&#19979;&#26469;12&#23567;&#26102;&#30340;&#33337;&#33334;&#36712;&#36857;&#12290;&#25105;&#20204;&#20174;&#21382;&#21490;AIS&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#36335;&#32447;&#21644;&#30446;&#30340;&#22320;&#30340;&#27010;&#29575;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#27169;&#22411;&#38543;&#21518;&#39044;&#27979;&#33337;&#33334;&#30340;&#36712;&#36857;&#65292;&#32771;&#34385;&#21040;&#28508;&#22312;&#36335;&#32447;&#21644;&#30446;&#30340;&#22320;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maritime transportation is paramount in achieving global economic growth, entailing concurrent ecological obligations in sustainability and safeguarding endangered marine species, most notably preserving large whale populations. In this regard, the Automatic Identification System (AIS) data plays a significant role by offering real-time streaming data on vessel movement, allowing enhanced traffic monitoring. This study explores using AIS data to prevent vessel-to-whale collisions by forecasting long-term vessel trajectories from engineered AIS data sequences. For such a task, we have developed an encoder-decoder model architecture using Bidirectional Long Short-Term Memory Networks (Bi-LSTM) to predict the next 12 hours of vessel trajectories using 1 to 3 hours of AIS data as input. We feed the model with probabilistic features engineered from historical AIS data that refer to each trajectory's potential route and destination. The model then predicts the vessel's trajectory, considerin
&lt;/p&gt;</description></item><item><title>&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#35266;&#27979;&#24314;&#27169;&#21644;&#22870;&#21169;&#24314;&#27169;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#36890;&#36807;&#32531;&#35299;&#35266;&#27979;&#24314;&#27169;&#25110;&#22870;&#21169;&#24314;&#27169;&#30340;&#21344;&#29992;&#20248;&#21183;&#26469;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2310.00344</link><description>&lt;p&gt;
HarmonyDream: &#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#20219;&#21153;&#21327;&#35843;&#21270;
&lt;/p&gt;
&lt;p&gt;
HarmonyDream: Task Harmonization Inside World Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00344
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;&#35266;&#27979;&#24314;&#27169;&#21644;&#22870;&#21169;&#24314;&#27169;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#36890;&#36807;&#32531;&#35299;&#35266;&#27979;&#24314;&#27169;&#25110;&#22870;&#21169;&#24314;&#27169;&#30340;&#21344;&#29992;&#20248;&#21183;&#26469;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#36890;&#36807;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#26469;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#30340;&#30446;&#26631;&#65292;&#19990;&#30028;&#27169;&#22411;&#36890;&#24120;&#21253;&#21547;&#35266;&#27979;&#24314;&#27169;&#21644;&#22870;&#21169;&#24314;&#27169;&#20004;&#20010;&#20219;&#21153;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#28145;&#20837;&#29702;&#35299;&#20102;&#27599;&#20010;&#20219;&#21153;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#32531;&#35299;&#35266;&#27979;&#24314;&#27169;&#25110;&#22870;&#21169;&#24314;&#27169;&#30340;&#21344;&#29992;&#20248;&#21183;&#26469;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#65292;&#34429;&#28982;&#24403;&#21069;&#30340;&#26174;&#24335;MBRL&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#35266;&#27979;&#27169;&#22411;&#24674;&#22797;&#29615;&#22659;&#30340;&#20016;&#23500;&#32454;&#33410;&#65292;&#20294;&#30001;&#20110;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#21644;&#27169;&#22411;&#23481;&#37327;&#30340;&#38480;&#21046;&#65292;&#36825;&#26159;&#22256;&#38590;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#38544;&#24335;MBRL&#20013;&#22870;&#21169;&#27169;&#22411;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#25797;&#38271;&#23398;&#20064;&#32039;&#20945;&#30340;&#20219;&#21153;&#23548;&#21521;&#21160;&#24577;&#65292;&#20294;&#22312;&#27809;&#26377;&#26356;&#20016;&#23500;&#30340;&#23398;&#20064;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#19981;&#36866;&#21512;&#39640;&#25928;&#23398;&#20064;&#12290;&#22312;&#36825;&#20123;&#35266;&#28857;&#21644;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning (MBRL) holds the promise of sample-efficient learning by utilizing a world model, which models how the environment works and typically encompasses components for two tasks: observation modeling and reward modeling. In this paper, through a dedicated empirical investigation, we gain a deeper understanding of the role each task plays in world models and uncover the overlooked potential of sample-efficient MBRL by mitigating the domination of either observation or reward modeling. Our key insight is that while prevalent approaches of explicit MBRL attempt to restore abundant details of the environment via observation models, it is difficult due to the environment's complexity and limited model capacity. On the other hand, reward models, while dominating implicit MBRL and adept at learning compact task-centric dynamics, are inadequate for sample-efficient learning without richer learning signals. Motivated by these insights and discoveries, we propose a s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#37319;&#29992;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#26222;&#36890;&#39550;&#39542;&#22330;&#26223;&#36716;&#21270;&#20026;&#36793;&#30028;&#24773;&#20917;&#22330;&#26223;&#12290;&#36890;&#36807;&#29983;&#25104;&#31616;&#27905;&#30340;&#22330;&#26223;&#22270;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#21644;&#19977;&#20803;&#23884;&#20837;&#25200;&#21160;&#22270;&#24418;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#23398;&#20064;&#21040;&#29983;&#25104;&#36793;&#30028;&#24773;&#20917;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2309.09844</link><description>&lt;p&gt;
CC-SGG: &#20351;&#29992;&#23398;&#20064;&#30340;&#22330;&#26223;&#22270;&#29983;&#25104;&#36793;&#30028;&#24773;&#20917;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
CC-SGG: Corner Case Scenario Generation using Learned Scene Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.09844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#37319;&#29992;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#26222;&#36890;&#39550;&#39542;&#22330;&#26223;&#36716;&#21270;&#20026;&#36793;&#30028;&#24773;&#20917;&#22330;&#26223;&#12290;&#36890;&#36807;&#29983;&#25104;&#31616;&#27905;&#30340;&#22330;&#26223;&#22270;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#21644;&#19977;&#20803;&#23884;&#20837;&#25200;&#21160;&#22270;&#24418;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#23398;&#20064;&#21040;&#29983;&#25104;&#36793;&#30028;&#24773;&#20917;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#30028;&#24773;&#20917;&#22330;&#26223;&#23545;&#20110;&#27979;&#35797;&#21644;&#39564;&#35777;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#65288;AV&#65289;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#36825;&#20123;&#22330;&#26223;&#22312;&#33258;&#28982;&#39550;&#39542;&#25968;&#25454;&#38598;&#20013;&#36890;&#24120;&#19981;&#36275;&#65292;&#22240;&#27492;&#20351;&#29992;&#21512;&#25104;&#30340;&#36793;&#30028;&#24773;&#20917;&#25968;&#25454;&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#24378;AV&#22312;&#29420;&#29305;&#24773;&#20917;&#19979;&#30340;&#23433;&#20840;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#21512;&#25104;&#30340;&#12289;&#30495;&#23454;&#20294;&#21448;&#36924;&#30495;&#30340;&#36793;&#30028;&#24773;&#20917;&#22330;&#26223;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNNs&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#26222;&#36890;&#39550;&#39542;&#22330;&#26223;&#36716;&#25442;&#20026;&#36793;&#30028;&#24773;&#20917;&#22330;&#26223;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#26222;&#36890;&#39550;&#39542;&#22330;&#26223;&#29983;&#25104;&#31616;&#27905;&#30340;&#22330;&#26223;&#22270;&#24418;&#24335;&#65292;&#26368;&#23567;&#21270;&#23545;&#20854;&#32467;&#26500;&#21644;&#23646;&#24615;&#30340;&#25913;&#21464;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#27880;&#24847;&#21147;&#21644;&#19977;&#20803;&#23884;&#20837;&#26469;&#23398;&#20064;&#25200;&#21160;&#36825;&#20123;&#22270;&#24418;&#20197;&#29983;&#25104;&#36793;&#30028;&#24773;&#20917;&#12290;&#36755;&#20837;&#21644;&#25200;&#21160;&#21518;&#30340;&#22270;&#24418;&#20877;&#27425;&#23548;&#20837;&#27169;&#25311;&#22120;&#20197;&#29983;&#25104;&#36793;&#30028;&#24773;&#20917;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#23398;&#20064;&#21040;&#20174;&#26222;&#36890;&#22330;&#26223;&#29983;&#25104;&#36793;&#30028;&#24773;&#20917;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Corner case scenarios are an essential tool for testing and validating the safety of autonomous vehicles (AVs). As these scenarios are often insufficiently present in naturalistic driving datasets, augmenting the data with synthetic corner cases greatly enhances the safe operation of AVs in unique situations. However, the generation of synthetic, yet realistic, corner cases poses a significant challenge. In this work, we introduce a novel approach based on Heterogeneous Graph Neural Networks (HGNNs) to transform regular driving scenarios into corner cases. To achieve this, we first generate concise representations of regular driving scenes as scene graphs, minimally manipulating their structure and properties. Our model then learns to perturb those graphs to generate corner cases using attention and triple embeddings. The input and perturbed graphs are then imported back into the simulation to generate corner case scenarios. Our model successfully learned to produce corner cases from i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33455;&#29255;&#19978;&#36827;&#34892;&#30828;&#20214;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#26694;&#26550;&#65288;OHQ&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#37327;&#21270;&#24863;&#30693;&#27969;&#27700;&#32447;&#21644;&#24341;&#20837;&#25513;&#30721;&#24341;&#23548;&#30340;&#37327;&#21270;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#36827;&#34892;&#39640;&#25928;&#37327;&#21270;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#25628;&#32034;&#31354;&#38388;&#36807;&#22823;&#21644;&#23454;&#38469;&#37096;&#32626;&#24046;&#36317;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2309.01945</link><description>&lt;p&gt;
OHQ: &#33455;&#29255;&#19978;&#30340;&#30828;&#20214;&#24863;&#30693;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
OHQ: On-chip Hardware-aware Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33455;&#29255;&#19978;&#36827;&#34892;&#30828;&#20214;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#26694;&#26550;&#65288;OHQ&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#37327;&#21270;&#24863;&#30693;&#27969;&#27700;&#32447;&#21644;&#24341;&#20837;&#25513;&#30721;&#24341;&#23548;&#30340;&#37327;&#21270;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#36827;&#34892;&#39640;&#25928;&#37327;&#21270;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#25628;&#32034;&#31354;&#38388;&#36807;&#22823;&#21644;&#23454;&#38469;&#37096;&#32626;&#24046;&#36317;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#37096;&#32626;&#20808;&#36827;&#28145;&#24230;&#27169;&#22411;&#30340;&#26368;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#21033;&#29992;&#22810;&#20301;&#23485;&#26550;&#26500;&#26469;&#37322;&#25918;&#37327;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#23384;&#22312;&#25628;&#32034;&#31354;&#38388;&#36807;&#22823;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#22240;&#27492;&#65292;&#37327;&#21270;&#36807;&#31243;&#20381;&#36182;&#20110;&#29420;&#31435;&#30340;&#39640;&#24615;&#33021;&#35774;&#22791;&#65292;&#32780;&#19981;&#26159;&#26412;&#22320;&#36827;&#34892;&#65292;&#36825;&#20063;&#23548;&#33268;&#20102;&#32771;&#34385;&#30340;&#30828;&#20214;&#25351;&#26631;&#19982;&#23454;&#38469;&#37096;&#32626;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33455;&#29255;&#19978;&#36827;&#34892;&#30828;&#20214;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#26694;&#26550;&#65288;OHQ&#65289;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#22312;&#32447;&#35774;&#22791;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#33455;&#29255;&#19978;&#30340;&#37327;&#21270;&#24863;&#30693;&#65288;OQA&#65289;&#27969;&#27700;&#32447;&#65292;&#33021;&#22815;&#24863;&#30693;&#37327;&#21270;&#31639;&#23376;&#22312;&#30828;&#20214;&#19978;&#30340;&#23454;&#38469;&#25928;&#29575;&#25351;&#26631;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25513;&#30721;&#24341;&#23548;&#30340;&#37327;&#21270;&#20272;&#35745;&#65288;MQE&#65289;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization emerges as one of the most promising approaches for deploying advanced deep models on resource-constrained hardware. Mixed-precision quantization leverages multiple bit-width architectures to unleash the accuracy and efficiency potential of quantized models. However, existing mixed-precision quantization suffers exhaustive search space that causes immense computational overhead. The quantization process thus relies on separate high-performance devices rather than locally, which also leads to a significant gap between the considered hardware metrics and the real deployment.In this paper, we propose an On-chip Hardware-aware Quantization (OHQ) framework that performs hardware-aware mixed-precision quantization without accessing online devices. First, we construct the On-chip Quantization Awareness (OQA) pipeline, enabling perceive the actual efficiency metrics of the quantization operator on the hardware.Second, we propose Mask-guided Quantization Estimation (MQE) technique 
&lt;/p&gt;</description></item><item><title>CroSSL&#26159;&#19968;&#31181;&#36328;&#27169;&#24577;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#38544;&#34255;&#25513;&#30721;&#21644;&#36328;&#27169;&#24577;&#32858;&#21512;&#22120;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#23398;&#20064;&#65292;&#26080;&#38656;&#36127;&#26679;&#26412;&#23545;&#21644;&#25968;&#25454;&#39044;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2307.16847</link><description>&lt;p&gt;
CroSSL: &#36328;&#27169;&#24577;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#24212;&#29992;&#36890;&#36807;&#38544;&#34255;&#25513;&#30721;
&lt;/p&gt;
&lt;p&gt;
CroSSL: Cross-modal Self-Supervised Learning for Time-series through Latent Masking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.16847
&lt;/p&gt;
&lt;p&gt;
CroSSL&#26159;&#19968;&#31181;&#36328;&#27169;&#24577;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#38544;&#34255;&#25513;&#30721;&#21644;&#36328;&#27169;&#24577;&#32858;&#21512;&#22120;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#30340;&#23398;&#20064;&#65292;&#26080;&#38656;&#36127;&#26679;&#26412;&#23545;&#21644;&#25968;&#25454;&#39044;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22810;&#27169;&#24577;&#26102;&#38388;&#24207;&#21015;&#30340;&#26631;&#27880;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#20005;&#37325;&#38459;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26159;&#19968;&#31181;&#26080;&#38656;&#20381;&#36182;&#26631;&#31614;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#30340;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;SSL&#26041;&#27861;&#38656;&#35201;&#35745;&#31639;&#26114;&#36149;&#30340;&#36127;&#26679;&#26412;&#23545;&#65292;&#24182;&#19988;&#36890;&#24120;&#20165;&#36866;&#29992;&#20110;&#21333;&#27169;&#24577;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;CroSSL&#65288;&#36328;&#27169;&#24577;&#33258;&#30417;&#30563;&#23398;&#20064;&#65289;&#65292;&#23427;&#25552;&#20986;&#20102;&#20004;&#20010;&#21019;&#26032;&#27010;&#24565;&#65306;&#36890;&#36807;&#27169;&#24577;&#29305;&#23450;&#32534;&#30721;&#22120;&#20135;&#29983;&#30340;&#20013;&#38388;&#23884;&#20837;&#30340;&#38544;&#34255;&#25513;&#30721;&#65292;&#20197;&#21450;&#36890;&#36807;&#36328;&#27169;&#24577;&#32858;&#21512;&#22120;&#23558;&#20854;&#32858;&#21512;&#20026;&#20840;&#23616;&#23884;&#20837;&#65292;&#21487;&#20197;&#25552;&#20379;&#32473;&#19979;&#28216;&#20998;&#31867;&#22120;&#12290;CroSSL&#20801;&#35768;&#22788;&#29702;&#32570;&#22833;&#27169;&#24577;&#21644;&#31471;&#21040;&#31471;&#30340;&#36328;&#27169;&#24577;&#23398;&#20064;&#65292;&#26080;&#38656;&#36827;&#34892;&#39044;&#22788;&#29702;&#20197;&#22788;&#29702;&#32570;&#22833;&#36755;&#20837;&#25110;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#36127;&#26679;&#26412;&#37319;&#26679;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#25968;&#25454;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#21152;&#36895;&#24230;&#35745;&#25110;&#38464;&#34746;&#20202;&#31561;&#36816;&#21160;&#20256;&#24863;&#22120;&#21644;&#29983;&#29289;&#20256;&#24863;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Limited availability of labeled data for machine learning on multimodal time-series extensively hampers progress in the field. Self-supervised learning (SSL) is a promising approach to learning data representations without relying on labels. However, existing SSL methods require expensive computations of negative pairs and are typically designed for single modalities, which limits their versatility. We introduce CroSSL (Cross-modal SSL), which puts forward two novel concepts: masking intermediate embeddings produced by modality-specific encoders, and their aggregation into a global embedding through a cross-modal aggregator that can be fed to down-stream classifiers. CroSSL allows for handling missing modalities and end-to-end cross-modal learning without requiring prior data preprocessing for handling missing inputs or negative-pair sampling for contrastive learning. We evaluate our method on a wide range of data, including motion sensors such as accelerometers or gyroscopes and biosi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;ASCII-Art&#30340;&#36328;&#27169;&#24577;&#20219;&#21153;&#65292;&#25506;&#35752;&#20102;ChatGPT&#21644;GPT3.5&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#22312;&#22270;&#20687;&#35782;&#21035;&#12289;&#22270;&#20687;&#37096;&#20998;&#30693;&#35782;&#21644;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#24182;&#19981;&#23436;&#20840;&#32570;&#20047;&#12290;</title><link>https://arxiv.org/abs/2307.16806</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;ASCII-Art&#30340;&#36328;&#27169;&#24577;&#20219;&#21153;&#27979;&#35797;ChatGPT&#23545;&#20110;&#29702;&#35299;&#28145;&#24230;&#30340;&#33021;&#21147;&#65306;GPT3.5&#22312;&#35782;&#21035;&#21644;&#29983;&#25104;ASCII-Art&#26041;&#38754;&#30340;&#33021;&#21147;&#24182;&#19981;&#23436;&#20840;&#32570;&#20047;
&lt;/p&gt;
&lt;p&gt;
Testing the Depth of ChatGPT's Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5's Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.16806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;ASCII-Art&#30340;&#36328;&#27169;&#24577;&#20219;&#21153;&#65292;&#25506;&#35752;&#20102;ChatGPT&#21644;GPT3.5&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#22312;&#22270;&#20687;&#35782;&#21035;&#12289;&#22270;&#20687;&#37096;&#20998;&#30693;&#35782;&#21644;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#24182;&#19981;&#23436;&#20840;&#32570;&#20047;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21457;&#24067;&#21518;&#30340;&#20843;&#20010;&#26376;&#37324;&#65292;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#21644;&#26131;&#20110;&#20351;&#29992;&#65292;ChatGPT&#21450;&#20854;&#24213;&#23618;&#27169;&#22411;GPT3.5&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#34429;&#28982;&#20986;&#29616;&#20102;&#19968;&#25209;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#33021;&#21147;&#33539;&#22260;&#30340;&#35770;&#25991;&#65292;&#20294;&#36825;&#20123;&#32593;&#32476;&#25152;&#25509;&#25910;&#21644;&#25552;&#21462;&#30340;&#20449;&#24687;&#35201;&#20040;&#26159;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#65292;&#35201;&#20040;&#26159;&#31867;&#20284;&#20195;&#30721;&#30340;&#39118;&#26684;&#21270;&#35821;&#35328;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21463;&#21040;&#23545;&#19968;&#20010;&#30495;&#27491;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20195;&#29702;&#22312;&#22810;&#20010;&#20449;&#21495;&#27169;&#24577;&#19978;&#20855;&#22791;&#30340;&#33021;&#21147;&#30340;&#21551;&#31034;&#65292;&#32771;&#23519;&#20102;GPT3.5&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#36755;&#20837;&#20197;ASCII-Art&#24418;&#24335;&#25552;&#20379;&#20869;&#23481;&#65292;&#27809;&#26377;&#26126;&#26174;&#30340;&#35821;&#35328;&#21270;&#24635;&#32467;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#35813;&#27169;&#22411;&#22312;&#32463;&#36807;&#20856;&#22411;&#30340;&#35270;&#35273;&#35774;&#32622;&#19979;&#30340;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35843;&#26597;&#20102;&#20854;&#23545;&#22270;&#20687;&#37096;&#20998;&#30340;&#30693;&#35782;&#20197;&#21450;&#22270;&#20687;&#29983;&#25104;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the eight months since its release, ChatGPT and its underlying model, GPT3.5, have garnered massive attention, due to their potent mix of capability and accessibility. While a niche-industry of papers have emerged examining the scope of capabilities these models possess, the information fed to and extracted from these networks has been either natural language text or stylized, code-like language. Drawing inspiration from the prowess we expect a truly human-level intelligent agent to have across multiple signal modalities, in this work we examine GPT3.5's aptitude for visual tasks, where the inputs feature content provided as ASCII-art without overt distillation into a lingual summary. We conduct experiments analyzing the model's performance on image recognition tasks after various transforms typical in visual settings, trials investigating knowledge of image parts, and tasks covering image generation.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35780;&#20272;&#20102;&#26080;&#21442;&#32771;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;&#22312;&#39640;&#35789;&#27719;&#37325;&#21472;&#20294;&#21547;&#20041;&#24046;&#24322;&#24456;&#22823;&#30340;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#32467;&#26524;&#21457;&#29616;&#23613;&#31649;&#36825;&#20123;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#36739;&#39640;&#65292;&#20294;&#23545;&#32454;&#31890;&#24230;&#38169;&#35823;&#35782;&#21035;&#22256;&#38590;&#65292;&#24182;&#19988;&#22312;&#26631;&#39064;&#19981;&#21512;&#29702;&#24615;&#38169;&#35823;&#12289;&#22270;&#20687;&#30456;&#20851;&#23545;&#35937;&#22823;&#23567;&#21464;&#21270;&#20197;&#21450;&#26631;&#39064;&#23545;&#21542;&#23450;&#24847;&#20041;&#30340;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#25935;&#24863;&#24615;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2305.14998</link><description>&lt;p&gt;
&#26080;&#21442;&#32771;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;&#30340;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Examination of the Robustness of Reference-Free Image Captioning Evaluation Metrics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14998
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35780;&#20272;&#20102;&#26080;&#21442;&#32771;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;&#22312;&#39640;&#35789;&#27719;&#37325;&#21472;&#20294;&#21547;&#20041;&#24046;&#24322;&#24456;&#22823;&#30340;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#32467;&#26524;&#21457;&#29616;&#23613;&#31649;&#36825;&#20123;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#36739;&#39640;&#65292;&#20294;&#23545;&#32454;&#31890;&#24230;&#38169;&#35823;&#35782;&#21035;&#22256;&#38590;&#65292;&#24182;&#19988;&#22312;&#26631;&#39064;&#19981;&#21512;&#29702;&#24615;&#38169;&#35823;&#12289;&#22270;&#20687;&#30456;&#20851;&#23545;&#35937;&#22823;&#23567;&#21464;&#21270;&#20197;&#21450;&#26631;&#39064;&#23545;&#21542;&#23450;&#24847;&#20041;&#30340;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#25935;&#24863;&#24615;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#26080;&#21442;&#32771;&#25351;&#26631;&#65292;&#22914;CLIPScore&#65288;Hessel&#31561;&#65292;2021&#65289;&#65292;UMIC&#65288;Lee&#31561;&#65292;2021&#65289;&#21644;PAC-S&#65288;Sarto&#31561;&#65292;2023&#65289;&#65292;&#29992;&#20110;&#33258;&#21160;&#26080;&#21442;&#32771;&#35780;&#20272;&#22270;&#20687;&#26631;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#35780;&#20272;&#36825;&#20123;&#25351;&#26631;&#22312;&#38656;&#35201;&#21306;&#20998;&#20855;&#26377;&#39640;&#35789;&#27719;&#37325;&#21472;&#20294;&#21547;&#20041;&#24046;&#24322;&#24456;&#22823;&#30340;&#20004;&#20010;&#26631;&#39064;&#30340;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#36825;&#20123;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;CLIPScore&#12289;UMIC&#21644;PAC-S&#24456;&#38590;&#35782;&#21035;&#32454;&#31890;&#24230;&#38169;&#35823;&#12290;&#34429;&#28982;&#25152;&#26377;&#25351;&#26631;&#23545;&#35270;&#35273;&#38169;&#35823;&#25935;&#24863;&#65292;&#20294;&#23545;&#26631;&#39064;&#19981;&#21512;&#29702;&#24615;&#38169;&#35823;&#30340;&#25935;&#24863;&#24615;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#25152;&#26377;&#25351;&#26631;&#23545;&#26631;&#39064;&#20013;&#25552;&#21450;&#30340;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#23545;&#35937;&#30340;&#22823;&#23567;&#21464;&#21270;&#25935;&#24863;&#65292;&#32780;CLIPScore&#21644;PAC-S&#23545;&#26631;&#39064;&#20013;&#25552;&#21450;&#30340;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#23545;&#35937;&#30340;&#25968;&#37327;&#20063;&#25935;&#24863;&#12290;&#20851;&#20110;&#26631;&#39064;&#30340;&#35821;&#35328;&#26041;&#38754;&#65292;&#25152;&#26377;&#25351;&#26631;&#23545;&#21542;&#23450;&#24847;&#20041;&#30340;&#29702;&#35299;&#33021;&#21147;&#36739;&#24369;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, reference-free metrics such as CLIPScore (Hessel et al., 2021), UMIC (Lee et al., 2021), and PAC-S (Sarto et al., 2023) have been proposed for automatic reference-free evaluation of image captions. Our focus lies in evaluating the robustness of these metrics in scenarios that require distinguishing between two captions with high lexical overlap but very different meanings. Our findings reveal that despite their high correlation with human judgments, CLIPScore, UMIC, and PAC-S struggle to identify fine-grained errors. While all metrics exhibit strong sensitivity to visual grounding errors, their sensitivity to caption implausibility errors is limited. Furthermore, we found that all metrics are sensitive to variations in the size of image-relevant objects mentioned in the caption, while CLIPScore and PAC-S are also sensitive to the number of mentions of image-relevant objects in the caption. Regarding linguistic aspects of a caption, all metrics show weak comprehension of negat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#25240;&#25187;&#33258;&#36866;&#24212;&#20559;&#22909;&#30340;&#20195;&#29702;&#30340;&#22312;&#32447;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#27599;&#19968;&#36718;&#20013;&#23637;&#31034;&#19968;&#31995;&#21015;&#29289;&#21697;&#24182;&#32771;&#34385;&#20195;&#29702;&#30340;&#20559;&#22909;&#28436;&#21464;&#65292;&#20197;&#23454;&#29616;&#23545;&#20110;&#30446;&#26631;&#38598;&#21512;&#30340;&#26368;&#23567;&#21270;&#21518;&#24724;&#12290;&#22312;&#38271;&#26399;&#35760;&#24518;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#20110;&#33021;&#22815;&#22312;&#20219;&#20309;&#26102;&#21051;&#23454;&#29616;&#30340;&#20998;&#24067;&#38598;&#21512;&#30340;&#39640;&#25928;&#27425;&#32447;&#24615;&#21518;&#24724;&#12290;</title><link>https://arxiv.org/abs/2302.06014</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#25240;&#25187;&#33258;&#36866;&#24212;&#20559;&#22909;&#30340;&#20195;&#29702;&#20013;&#30340;&#22312;&#32447;&#25512;&#33616;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Online Recommendations for Agents with Discounted Adaptive Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.06014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#25240;&#25187;&#33258;&#36866;&#24212;&#20559;&#22909;&#30340;&#20195;&#29702;&#30340;&#22312;&#32447;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#27599;&#19968;&#36718;&#20013;&#23637;&#31034;&#19968;&#31995;&#21015;&#29289;&#21697;&#24182;&#32771;&#34385;&#20195;&#29702;&#30340;&#20559;&#22909;&#28436;&#21464;&#65292;&#20197;&#23454;&#29616;&#23545;&#20110;&#30446;&#26631;&#38598;&#21512;&#30340;&#26368;&#23567;&#21270;&#21518;&#24724;&#12290;&#22312;&#38271;&#26399;&#35760;&#24518;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#20110;&#33021;&#22815;&#22312;&#20219;&#20309;&#26102;&#21051;&#23454;&#29616;&#30340;&#20998;&#24067;&#38598;&#21512;&#30340;&#39640;&#25928;&#27425;&#32447;&#24615;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;Bandit&#25512;&#33616;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#30340;&#20559;&#22909;&#65288;&#20195;&#34920;&#23545;&#25512;&#33616;&#29289;&#21697;&#30340;&#36873;&#25321;&#27010;&#29575;&#65289;&#26681;&#25454;&#36807;&#21435;&#30340;&#36873;&#25321;&#20316;&#20026;&#26410;&#30693;"&#20559;&#22909;&#27169;&#22411;"&#30340;&#20989;&#25968;&#32780;&#28436;&#21464;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#25105;&#20204;&#21521;&#20195;&#29702;&#23637;&#31034;$k$&#20010;&#29289;&#21697;&#65288;&#20849;$n$&#20010;&#65289;&#65292;&#20195;&#29702;&#36873;&#25321;&#19968;&#20010;&#29289;&#21697;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#20195;&#29702;&#30340;&#36873;&#25321;&#19978;&#23545;&#20110;&#26576;&#20010;"&#30446;&#26631;&#38598;&#21512;"&#65288;&#29289;&#21697;&#31616;&#21333;&#24418;&#24335;&#30340;&#23376;&#38598;&#65289;&#30340;&#23545;&#25239;&#25439;&#22833;&#26368;&#23567;&#21270;&#21518;&#24724;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;Agarwal&#21644;Brown&#65288;2022&#65289;&#30340;&#35774;&#23450;&#65292;&#20182;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#22343;&#21248;&#35760;&#24518;&#30340;&#20195;&#29702;&#65292;&#32780;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20801;&#35768;&#38750;&#22343;&#21248;&#35760;&#24518;&#65292;&#22312;&#27599;&#19968;&#36718;&#20013;&#23545;&#20195;&#29702;&#30340;&#35760;&#24518;&#21521;&#37327;&#24212;&#29992;&#19968;&#20010;&#25240;&#25187;&#22240;&#23376;&#12290;&#22312;"&#38271;&#26399;&#35760;&#24518;"&#30340;&#24773;&#20917;&#19979;&#65288;&#24403;&#26377;&#25928;&#30340;&#35760;&#24518;&#26102;&#31243;&#19982;$T$&#30340;&#27425;&#32447;&#24615;&#21464;&#21270;&#65289;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#23454;&#29616;&#23545;&#20110;"&#33021;&#22815;&#22312;&#20219;&#20309;&#26102;&#21051;&#23454;&#29616;&#30340;&#20998;&#24067;&#38598;&#21512;"&#65288;&#21363;"&#21363;&#26102;&#23454;&#29616;&#20998;&#24067;&#38598;&#21512;"&#65289;&#30340;&#39640;&#25928;&#27425;&#32447;&#24615;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a bandit recommendations problem in which an agent's preferences (representing selection probabilities over recommended items) evolve as a function of past selections, according to an unknown $\textit{preference model}$. In each round, we show a menu of $k$ items (out of $n$ total) to the agent, who then chooses a single item, and we aim to minimize regret with respect to some $\textit{target set}$ (a subset of the item simplex) for adversarial losses over the agent's choices. Extending the setting from Agarwal and Brown (2022), where uniform-memory agents were considered, here we allow for non-uniform memory in which a discount factor is applied to the agent's memory vector at each subsequent round. In the "long-term memory" regime (when the effective memory horizon scales with $T$ sublinearly), we show that efficient sublinear regret is obtainable with respect to the set of $\textit{everywhere instantaneously realizable distributions}$ (the "EIRD set", as formulated in pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#24565;&#26799;&#24230;&#65288;CG&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#25193;&#23637;&#21040;&#20102;&#38750;&#32447;&#24615;&#27010;&#24565;&#20989;&#25968;&#65292;&#24182;&#35777;&#26126;&#22312;&#29609;&#20855;&#31034;&#20363;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#65292;&#27010;&#24565;&#26799;&#24230;&#65288;CG&#65289;&#20248;&#20110;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAV&#65289;&#12290;</title><link>https://arxiv.org/abs/2208.14966</link><description>&lt;p&gt;
&#27010;&#24565;&#26799;&#24230;&#65306;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#19981;&#20381;&#36182;&#20110;&#32447;&#24615;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Concept Gradient: Concept-based Interpretation Without Linear Assumption
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.14966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#24565;&#26799;&#24230;&#65288;CG&#65289;&#30340;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#25193;&#23637;&#21040;&#20102;&#38750;&#32447;&#24615;&#27010;&#24565;&#20989;&#25968;&#65292;&#24182;&#35777;&#26126;&#22312;&#29609;&#20855;&#31034;&#20363;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#65292;&#27010;&#24565;&#26799;&#24230;&#65288;CG&#65289;&#20248;&#20110;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAV&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#23545;&#20110;&#20154;&#31867;&#26356;&#26131;&#29702;&#35299;&#12290;&#30446;&#21069;&#26368;&#24120;&#29992;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#27861;&#26159;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAV&#65289;&#12290;CAV&#20381;&#36182;&#20110;&#23398;&#20064;&#32473;&#23450;&#27169;&#22411;&#30340;&#26576;&#31181;&#28508;&#22312;&#34920;&#31034;&#19982;&#27010;&#24565;&#20043;&#38388;&#30340;&#32447;&#24615;&#20851;&#31995;&#12290;&#36890;&#24120;&#38544;&#21547;&#22320;&#20551;&#35774;&#32447;&#24615;&#21487;&#20998;&#65292;&#20294;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#22522;&#20110;&#27010;&#24565;&#35299;&#37322;&#30340;&#21407;&#22987;&#30446;&#30340;&#20986;&#21457;&#65292;&#25552;&#20986;&#20102;&#27010;&#24565;&#26799;&#24230;&#65288;CG&#65289;&#65292;&#23558;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#25193;&#23637;&#21040;&#20102;&#38750;&#32447;&#24615;&#27010;&#24565;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#33324;&#65288;&#21487;&#33021;&#38750;&#32447;&#24615;&#65289;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#21487;&#20197;&#25968;&#23398;&#19978;&#35780;&#20272;&#27010;&#24565;&#30340;&#24494;&#23567;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#23558;&#26799;&#24230;&#35299;&#37322;&#25193;&#23637;&#21040;&#20102;&#27010;&#24565;&#31354;&#38388;&#12290;&#25105;&#20204;&#22312;&#29609;&#20855;&#31034;&#20363;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#65292;&#35777;&#26126;&#20102;&#27010;&#24565;&#26799;&#24230;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAV&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept-based interpretations of black-box models are often more intuitive for humans to understand. The most widely adopted approach for concept-based interpretation is Concept Activation Vector (CAV). CAV relies on learning a linear relation between some latent representation of a given model and concepts. The linear separability is usually implicitly assumed but does not hold true in general. In this work, we started from the original intent of concept-based interpretation and proposed Concept Gradient (CG), extending concept-based interpretation beyond linear concept functions. We showed that for a general (potentially non-linear) concept, we can mathematically evaluate how a small change of concept affecting the model's prediction, which leads to an extension of gradient-based interpretation to the concept space. We demonstrated empirically that CG outperforms CAV in both toy examples and real world datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#25239;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;ARIEL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#22270;&#35270;&#22270;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#25552;&#21462;&#20449;&#24687;&#20016;&#23500;&#30340;&#23545;&#27604;&#26679;&#26412;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2208.06956</link><description>&lt;p&gt;
ARIEL: &#23545;&#25239;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ARIEL: Adversarial Graph Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.06956
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#25239;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;ARIEL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#22270;&#35270;&#22270;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#25552;&#21462;&#20449;&#24687;&#20016;&#23500;&#30340;&#23545;&#27604;&#26679;&#26412;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#23545;&#27604;&#23398;&#20064;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#22312;&#20110;&#27491;&#36127;&#26679;&#26412;&#30340;&#26500;&#24314;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#25509;&#36817;&#24615;&#20316;&#20026;&#21407;&#21017;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#35270;&#35273;&#39046;&#22495;&#23637;&#29616;&#20102;&#24456;&#22823;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#19968;&#20123;&#24037;&#20316;&#23558;&#36825;&#31181;&#26041;&#27861;&#20174;&#22270;&#20687;&#25193;&#23637;&#21040;&#20102;&#22270;&#20013;&#12290;&#28982;&#32780;&#65292;&#19982;&#22270;&#20687;&#19978;&#30340;&#25968;&#25454;&#22686;&#24378;&#19981;&#21516;&#65292;&#22270;&#19978;&#30340;&#25968;&#25454;&#22686;&#24378;&#19981;&#22826;&#30452;&#35266;&#65292;&#26356;&#38590;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#23545;&#27604;&#26679;&#26412;&#65292;&#36825;&#32473;&#25913;&#36827;&#30041;&#19979;&#20102;&#24456;&#22823;&#30340;&#31354;&#38388;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#22270;&#35270;&#22270;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23545;&#25239;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;ARIEL&#65289;&#65292;&#22312;&#21512;&#29702;&#30340;&#32422;&#26463;&#26465;&#20214;&#19979;&#25552;&#21462;&#20449;&#24687;&#20016;&#23500;&#30340;&#23545;&#27604;&#26679;&#26412;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#20449;&#24687;&#27491;&#21017;&#21270;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#31283;&#23450;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;s
&lt;/p&gt;
&lt;p&gt;
Contrastive learning is an effective unsupervised method in graph representation learning, and the key component of contrastive learning lies in the construction of positive and negative samples. Previous methods usually utilize the proximity of nodes in the graph as the principle. Recently, the data-augmentation-based contrastive learning method has advanced to show great power in the visual domain, and some works extended this method from images to graphs. However, unlike the data augmentation on images, the data augmentation on graphs is far less intuitive and much harder to provide high-quality contrastive samples, which leaves much space for improvement. In this work, by introducing an adversarial graph view for data augmentation, we propose a simple but effective method, Adversarial Graph Contrastive Learning (ARIEL), to extract informative contrastive samples within reasonable constraints. We develop a new technique called information regularization for stable training and use s
&lt;/p&gt;</description></item><item><title>&#36882;&#24402;QAOA&#26159;&#19968;&#31181;&#38750;&#23616;&#37096;&#30340;QAOA&#21464;&#20307;&#65292;&#29992;&#20110;&#25913;&#21892;&#36817;&#20284;&#35299;&#30340;&#36136;&#37327;&#12290;&#26412;&#25991;&#36890;&#36807;&#35782;&#21035;&#21644;&#20998;&#26512;RQAOA&#22833;&#36133;&#30340;&#26696;&#20363;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2207.06294</link><description>&lt;p&gt;
&#21152;&#24378;&#23398;&#20064;&#36741;&#21161;&#30340;&#36882;&#24402;QAOA
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning Assisted Recursive QAOA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.06294
&lt;/p&gt;
&lt;p&gt;
&#36882;&#24402;QAOA&#26159;&#19968;&#31181;&#38750;&#23616;&#37096;&#30340;QAOA&#21464;&#20307;&#65292;&#29992;&#20110;&#25913;&#21892;&#36817;&#20284;&#35299;&#30340;&#36136;&#37327;&#12290;&#26412;&#25991;&#36890;&#36807;&#35782;&#21035;&#21644;&#20998;&#26512;RQAOA&#22833;&#36133;&#30340;&#26696;&#20363;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35832;&#22914;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;&#65288;QAOA&#65289;&#20043;&#31867;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#22240;&#20026;&#23427;&#20204;&#26377;&#26395;&#21033;&#29992;NISQ&#35774;&#22791;&#35299;&#20915;&#22256;&#38590;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#32780;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;&#22312;&#20302;&#28145;&#24230;&#19979;&#65292;QAOA&#30340;&#26576;&#20123;&#23616;&#37096;&#24615;&#32422;&#26463;&#38480;&#21046;&#20102;&#20854;&#24615;&#33021;&#12290;&#20026;&#20102;&#36229;&#36234;&#36825;&#20123;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23616;&#37096;&#30340;QAOA&#21464;&#20307;&#65292;&#21363;&#36882;&#24402;QAOA&#65288;RQAOA&#65289;&#65292;&#20197;&#25552;&#39640;&#36817;&#20284;&#35299;&#30340;&#36136;&#37327;&#12290;RQAOA&#30456;&#23545;&#20110;QAOA&#30340;&#30740;&#31350;&#36739;&#23569;&#65292;&#20154;&#20204;&#23545;&#20854;&#20102;&#35299;&#36739;&#23569;&#65292;&#20363;&#22914;&#22312;&#21738;&#20123;&#23454;&#20363;&#26063;&#19978;&#21487;&#33021;&#26080;&#27861;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25105;&#20204;&#22788;&#29702;&#30340;&#26159;NP&#38590;&#38382;&#39064;&#65288;&#20855;&#20307;&#32780;&#35328;&#26159;&#20234;&#36763;&#33258;&#26059;&#27169;&#22411;&#65289;&#65292;&#39044;&#35745;RQAOA&#20250;&#22833;&#36133;&#65292;&#36825;&#24341;&#21457;&#20102;&#35774;&#35745;&#26356;&#22909;&#30340;&#37327;&#23376;&#31639;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#31934;&#31070;&#19979;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#20998;&#26512;&#20102;RQAOA&#22833;&#36133;&#30340;&#26696;&#20363;&#65292;&#24182;&#22522;&#20110;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational quantum algorithms such as the Quantum Approximation Optimization Algorithm (QAOA) in recent years have gained popularity as they provide the hope of using NISQ devices to tackle hard combinatorial optimization problems. It is, however, known that at low depth, certain locality constraints of QAOA limit its performance. To go beyond these limitations, a non-local variant of QAOA, namely recursive QAOA (RQAOA), was proposed to improve the quality of approximate solutions. The RQAOA has been studied comparatively less than QAOA, and it is less understood, for instance, for what family of instances it may fail to provide high quality solutions. However, as we are tackling $\mathsf{NP}$-hard problems (specifically, the Ising spin model), it is expected that RQAOA does fail, raising the question of designing even better quantum algorithms for combinatorial optimization. In this spirit, we identify and analyze cases where RQAOA fails and, based on this, propose a reinforcement le
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;Hilbert&#26354;&#32447;&#25237;&#24433;(HCP)&#36317;&#31163;&#65292;&#29992;&#20110;&#27979;&#37327;&#20004;&#20010;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#20855;&#26377;&#20302;&#22797;&#26434;&#24230;&#12290;&#36890;&#36807;Hilbert&#26354;&#32447;&#25237;&#24433;&#21644;&#36816;&#36755;&#36317;&#31163;&#35745;&#31639;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#26377;&#30028;&#25903;&#25745;&#30340;&#27010;&#29575;&#27979;&#24230;&#65292;&#24182;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#32500;&#24230;&#28798;&#38590;&#65292;&#36824;&#24320;&#21457;&#20102;&#20004;&#20010;HCP&#36317;&#31163;&#30340;&#21464;&#20307;&#65292;&#20351;&#29992;&#23376;&#31354;&#38388;&#25237;&#24433;&#12290;</title><link>https://arxiv.org/abs/2205.15059</link><description>&lt;p&gt;
Hilbert&#26354;&#32447;&#25237;&#24433;&#36317;&#31163;&#29992;&#20110;&#20998;&#24067;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Hilbert Curve Projection Distance for Distribution Comparison
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.15059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;Hilbert&#26354;&#32447;&#25237;&#24433;(HCP)&#36317;&#31163;&#65292;&#29992;&#20110;&#27979;&#37327;&#20004;&#20010;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#20855;&#26377;&#20302;&#22797;&#26434;&#24230;&#12290;&#36890;&#36807;Hilbert&#26354;&#32447;&#25237;&#24433;&#21644;&#36816;&#36755;&#36317;&#31163;&#35745;&#31639;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#26377;&#30028;&#25903;&#25745;&#30340;&#27010;&#29575;&#27979;&#24230;&#65292;&#24182;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#32500;&#24230;&#28798;&#38590;&#65292;&#36824;&#24320;&#21457;&#20102;&#20004;&#20010;HCP&#36317;&#31163;&#30340;&#21464;&#20307;&#65292;&#20351;&#29992;&#23376;&#31354;&#38388;&#25237;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#27604;&#36739;&#22312;&#25968;&#25454;&#20998;&#31867;&#21644;&#29983;&#25104;&#24314;&#27169;&#31561;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;Hilbert&#26354;&#32447;&#25237;&#24433;(HCP)&#36317;&#31163;&#65292;&#29992;&#20110;&#27979;&#37327;&#20004;&#20010;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#20855;&#26377;&#20302;&#22797;&#26434;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;Hilbert&#26354;&#32447;&#23558;&#20004;&#20010;&#39640;&#32500;&#27010;&#29575;&#20998;&#24067;&#25237;&#24433;&#21040;&#19968;&#36215;&#65292;&#24471;&#21040;&#23427;&#20204;&#20043;&#38388;&#30340;&#32806;&#21512;&#65292;&#28982;&#21518;&#26681;&#25454;&#32806;&#21512;&#22312;&#21407;&#22987;&#31354;&#38388;&#20013;&#35745;&#31639;&#36825;&#20004;&#20010;&#20998;&#24067;&#20043;&#38388;&#30340;&#36816;&#36755;&#36317;&#31163;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;HCP&#36317;&#31163;&#26159;&#19968;&#20010;&#36866;&#24403;&#30340;&#24230;&#37327;&#65292;&#24182;&#19988;&#23545;&#20110;&#26377;&#30028;&#25903;&#25745;&#30340;&#27010;&#29575;&#27979;&#24230;&#26159;&#33391;&#23450;&#20041;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;$d$&#32500;&#31354;&#38388;&#20013;&#20855;&#26377;$L_p$&#25104;&#26412;&#30340;&#25913;&#36827;&#32463;&#39564;HCP&#36317;&#31163;&#20197;&#19981;&#36229;&#36807;$O(n^{-1/2\max\{d,p\}})$&#30340;&#36895;&#29575;&#25910;&#25947;&#21040;&#20854;&#24635;&#20307;&#23545;&#24212;&#39033;&#12290;&#20026;&#20102;&#25233;&#21046;&#32500;&#24230;&#28798;&#38590;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#20004;&#20010;HCP&#36317;&#31163;&#30340;&#21464;&#20307;&#65292;&#20351;&#29992;&#65288;&#21487;&#23398;&#20064;&#30340;&#65289;&#23376;&#31354;&#38388;&#25237;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distribution comparison plays a central role in many machine learning tasks like data classification and generative modeling. In this study, we propose a novel metric, called Hilbert curve projection (HCP) distance, to measure the distance between two probability distributions with low complexity. In particular, we first project two high-dimensional probability distributions using Hilbert curve to obtain a coupling between them, and then calculate the transport distance between these two distributions in the original space, according to the coupling. We show that HCP distance is a proper metric and is well-defined for probability measures with bounded supports. Furthermore, we demonstrate that the modified empirical HCP distance with the $L_p$ cost in the $d$-dimensional space converges to its population counterpart at a rate of no more than $O(n^{-1/2\max\{d,p\}})$. To suppress the curse-of-dimensionality, we also develop two variants of the HCP distance using (learnable) subspace pro
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#25512;&#23548;&#20986;&#22312;&#32452;&#20844;&#24179;&#24615;&#19979;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FairBayes&#30340;&#22522;&#20110;&#32452;&#30340;&#38408;&#20540;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#25511;&#21046;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#23454;&#29616;&#22522;&#26412;&#26368;&#20248;&#30340;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2202.09724</link><description>&lt;p&gt;
&#22522;&#20110;&#32452;&#20844;&#24179;&#24615;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Bayes-Optimal Classifiers under Group Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.09724
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#25512;&#23548;&#20986;&#22312;&#32452;&#20844;&#24179;&#24615;&#19979;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FairBayes&#30340;&#22522;&#20110;&#32452;&#30340;&#38408;&#20540;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#25511;&#21046;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#23454;&#29616;&#22522;&#26412;&#26368;&#20248;&#30340;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#27491;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#21040;&#39640;&#39118;&#38505;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#20363;&#22914;&#31038;&#20250;&#31119;&#21033;&#38382;&#39064;&#12290;&#30001;&#20110;&#38656;&#35201;&#20943;&#23569;&#31639;&#27861;&#39044;&#27979;&#21487;&#33021;&#36896;&#25104;&#30340;&#19981;&#24179;&#31561;&#24433;&#21709;&#65292;&#35768;&#22810;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#22312;&#21508;&#31181;&#32452;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#21051;&#30011;&#36125;&#21494;&#26031;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#22522;&#26412;&#38382;&#39064;&#20165;&#22312;&#19968;&#20123;&#29305;&#27530;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#26412;&#25991;&#22522;&#20110;&#32463;&#20856;&#30340;Neyman-Pearson&#20551;&#35774;&#26816;&#39564;&#29702;&#35770;&#65288;Neyman&#21644;Pearson&#65292;1933&#65307;Shao&#65292;2003&#65289;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#26469;&#25512;&#23548;&#22312;&#32452;&#20844;&#24179;&#24615;&#19979;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#20998;&#31867;&#22120;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#32452;&#30340;&#38408;&#20540;&#26041;&#27861;&#65292;&#31216;&#20026;FairBayes&#65292;&#21487;&#20197;&#30452;&#25509;&#25511;&#21046;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#24182;&#23454;&#29616;&#22522;&#26412;&#26368;&#20248;&#30340;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;&#36825;&#20123;&#20248;&#21183;&#36890;&#36807;&#20805;&#20998;&#30340;&#23454;&#39564;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms are becoming integrated into more and more high-stakes decision-making processes, such as in social welfare issues. Due to the need of mitigating the potentially disparate impacts from algorithmic predictions, many approaches have been proposed in the emerging area of fair machine learning. However, the fundamental problem of characterizing Bayes-optimal classifiers under various group fairness constraints has only been investigated in some special cases. Based on the classical Neyman-Pearson argument (Neyman and Pearson, 1933; Shao, 2003) for optimal hypothesis testing, this paper provides a unified framework for deriving Bayes-optimal classifiers under group fairness. This enables us to propose a group-based thresholding method we call FairBayes, that can directly control disparity, and achieve an essentially optimal fairness-accuracy tradeoff. These advantages are supported by thorough experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;&#20013;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#26368;&#20248;&#30028;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;(MRE-NC)&#65292;&#20854;&#26399;&#26395;&#25439;&#22833;&#19982;&#26368;&#20248;&#30028;&#38480;&#21305;&#37197;&#12290;</title><link>https://arxiv.org/abs/2108.08677</link><description>&lt;p&gt;
&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;&#20013;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#26368;&#20248;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Order Optimal Bounds for One-Shot Federated Learning over non-Convex Loss Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2108.08677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;&#20013;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#26368;&#20248;&#30028;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;(MRE-NC)&#65292;&#20854;&#26399;&#26395;&#25439;&#22833;&#19982;&#26368;&#20248;&#30028;&#38480;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#19968;&#27425;&#24615;&#35774;&#32622;&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#26377;m&#21488;&#26426;&#22120;&#65292;&#27599;&#21488;&#26426;&#22120;&#20174;&#26410;&#30693;&#20998;&#24067;&#19978;&#30340;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#20013;&#35266;&#27979;&#21040;n&#20010;&#26679;&#26412;&#20989;&#25968;&#12290;&#35774;F&#65306;[-1,1]^d &#8594; R&#26159;&#36825;&#20010;&#26410;&#30693;&#20998;&#24067;&#19979;&#30340;&#39044;&#26399;&#25439;&#22833;&#20989;&#25968;&#12290;&#30446;&#26631;&#26159;&#25214;&#21040;F&#30340;&#26368;&#23567;&#21270;&#20272;&#35745;&#12290;&#22522;&#20110;&#23427;&#30340;&#35266;&#27979;&#65292;&#27599;&#21488;&#26426;&#22120;&#20135;&#29983;&#19968;&#20010;&#38271;&#24230;&#26377;&#30028;&#20026;B&#30340;&#20449;&#21495;&#24182;&#23558;&#20854;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#12290;&#26381;&#21153;&#22120;&#25910;&#38598;&#25152;&#26377;&#26426;&#22120;&#30340;&#20449;&#21495;&#24182;&#36755;&#20986;F&#30340;&#26368;&#23567;&#21270;&#20272;&#35745;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20219;&#20309;&#31639;&#27861;&#30340;&#26399;&#26395;&#25439;&#22833;&#19979;&#30028;&#20026;max(1/(\sqrt{n}(mB)^{1/d}), 1/\sqrt{mn})&#65292;&#38500;&#21435;&#23545;&#25968;&#22240;&#23376;&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;&#35813;&#19979;&#30028;&#22312;m&#21644;n&#19978;&#26159;&#27425;&#20248;&#30340;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#22810;&#20998;&#36776;&#29575;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#20272;&#35745;&#22120;(MRE-NC)&#65292;&#20854;&#26399;&#26395;&#25439;&#22833;&#22312;&#22823;&#30340;mn&#26102;&#19982;&#19979;&#30028;&#21305;&#37197;&#65292;&#38500;&#20102;&#22810;&#23545;&#25968;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of federated learning in a one-shot setting in which there are $m$ machines, each observing $n$ sample functions from an unknown distribution on non-convex loss functions. Let $F:[-1,1]^d\to\mathbb{R}$ be the expected loss function with respect to this unknown distribution. The goal is to find an estimate of the minimizer of $F$. Based on its observations, each machine generates a signal of bounded length $B$ and sends it to a server. The server collects signals of all machines and outputs an estimate of the minimizer of $F$. We show that the expected loss of any algorithm is lower bounded by $\max\big(1/(\sqrt{n}(mB)^{1/d}), 1/\sqrt{mn}\big)$, up to a logarithmic factor. We then prove that this lower bound is order optimal in $m$ and $n$ by presenting a distributed learning algorithm, called Multi-Resolution Estimator for Non-Convex loss function (MRE-NC), whose expected loss matches the lower bound for large $mn$ up to polylogarithmic factors.
&lt;/p&gt;</description></item><item><title>NetOTC&#26159;&#19968;&#31181;&#29992;&#20110;&#27604;&#36739;&#21644;&#23545;&#40784;&#20004;&#20010;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#38543;&#26426;&#34892;&#36208;&#30340;&#36716;&#25442;&#32806;&#21512;&#30340;&#26399;&#26395;&#25104;&#26412;&#26469;&#37327;&#21270;&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20379;&#39030;&#28857;&#21644;&#36793;&#30340;&#23545;&#40784;&#12290;&#23427;&#25429;&#25417;&#21040;&#20102;&#20851;&#20110;&#32593;&#32476;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#24182;&#20445;&#30041;&#20102;&#36793;&#32536;&#12290;</title><link>https://arxiv.org/abs/2106.07106</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#34892;&#36208;&#30340;&#36716;&#25442;&#32806;&#21512;&#27604;&#23545;&#21644;&#23545;&#40784;&#26377;&#21521;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Alignment and Comparison of Directed Networks via Transition Couplings of Random Walks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2106.07106
&lt;/p&gt;
&lt;p&gt;
NetOTC&#26159;&#19968;&#31181;&#29992;&#20110;&#27604;&#36739;&#21644;&#23545;&#40784;&#20004;&#20010;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#38543;&#26426;&#34892;&#36208;&#30340;&#36716;&#25442;&#32806;&#21512;&#30340;&#26399;&#26395;&#25104;&#26412;&#26469;&#37327;&#21270;&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20379;&#39030;&#28857;&#21644;&#36793;&#30340;&#23545;&#40784;&#12290;&#23427;&#25429;&#25417;&#21040;&#20102;&#20851;&#20110;&#32593;&#32476;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#24182;&#20445;&#30041;&#20102;&#36793;&#32536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#20256;&#36755;&#30340;&#36807;&#31243;&#65292;&#31216;&#20026;NetOTC&#65288;&#32593;&#32476;&#20248;&#21270;&#36716;&#25442;&#32806;&#21512;&#65289;&#65292;&#29992;&#20110;&#27604;&#36739;&#21644;&#23545;&#40784;&#20004;&#20010;&#32593;&#32476;&#12290;&#25152;&#30740;&#31350;&#30340;&#32593;&#32476;&#21487;&#20197;&#26159;&#26377;&#21521;&#25110;&#26080;&#21521;&#30340;&#65292;&#24102;&#26435;&#37325;&#25110;&#19981;&#24102;&#26435;&#37325;&#65292;&#24182;&#19988;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#22823;&#23567;&#30340;&#19981;&#21516;&#39030;&#28857;&#38598;&#12290;&#32473;&#23450;&#20004;&#20010;&#32593;&#32476;&#21644;&#19968;&#20010;&#19982;&#23427;&#20204;&#30340;&#39030;&#28857;&#30456;&#20851;&#30340;&#25104;&#26412;&#20989;&#25968;&#65292;NetOTC&#25214;&#21040;&#20854;&#38543;&#26426;&#34892;&#36208;&#30340;&#36716;&#25442;&#32806;&#21512;&#65292;&#20351;&#20854;&#20855;&#26377;&#26368;&#23567;&#30340;&#26399;&#26395;&#25104;&#26412;&#12290;&#26368;&#23567;&#21270;&#30340;&#25104;&#26412;&#37327;&#21270;&#20102;&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#32780;&#26368;&#20339;&#20256;&#36755;&#35745;&#21010;&#26412;&#36523;&#25552;&#20379;&#20102;&#20004;&#20010;&#32593;&#32476;&#30340;&#39030;&#28857;&#21644;&#36793;&#30340;&#23545;&#40784;&#12290;&#36890;&#36807;&#32806;&#21512;&#23436;&#25972;&#30340;&#38543;&#26426;&#34892;&#36208;&#65292;&#32780;&#19981;&#26159;&#23427;&#20204;&#30340;&#36793;&#32536;&#20998;&#24067;&#65292;&#30830;&#20445;NetOTC&#25429;&#25417;&#21040;&#20102;&#20851;&#20110;&#32593;&#32476;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#24182;&#20445;&#30041;&#20102;&#36793;&#32536;&#12290;NetOTC&#27809;&#26377;&#33258;&#30001;&#21442;&#25968;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#38543;&#26426;&#21270;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;NetOTC&#30340;&#19968;&#20123;&#29702;&#35770;&#29305;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20123;&#23454;&#39564;&#35777;&#26126;&#20854;&#23454;&#38469;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe and study a transport based procedure called NetOTC (network optimal transition coupling) for the comparison and alignment of two networks. The networks of interest may be directed or undirected, weighted or unweighted, and may have distinct vertex sets of different sizes. Given two networks and a cost function relating their vertices, NetOTC finds a transition coupling of their associated random walks having minimum expected cost. The minimizing cost quantifies the difference between the networks, while the optimal transport plan itself provides alignments of both the vertices and the edges of the two networks. Coupling of the full random walks, rather than their marginal distributions, ensures that NetOTC captures local and global information about the networks, and preserves edges. NetOTC has no free parameters, and does not rely on randomization. We investigate a number of theoretical properties of NetOTC and present experiments establishing its empirical performance.
&lt;/p&gt;</description></item><item><title>IM-META&#26159;&#19968;&#31181;&#22312;&#26410;&#30693;&#25299;&#25169;&#32593;&#32476;&#20013;&#36827;&#34892;&#24433;&#21709;&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33410;&#28857;&#20803;&#25968;&#25454;&#21644;&#26597;&#35810;&#20449;&#24687;&#65292;&#36890;&#36807;&#23398;&#20064;&#20803;&#25968;&#25454;&#19982;&#36793;&#30340;&#20851;&#31995;&#12289;&#26500;&#24314;&#22686;&#24378;&#22270;&#20197;&#21450;&#20351;&#29992;&#25299;&#25169;&#24863;&#30693;&#25490;&#24207;&#31574;&#30053;&#26469;&#30830;&#23450;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#31181;&#23376;&#33410;&#28857;&#21644;&#26597;&#35810;&#33410;&#28857;&#12290;</title><link>https://arxiv.org/abs/2106.02926</link><description>&lt;p&gt;
IM-META: &#20351;&#29992;&#33410;&#28857;&#20803;&#25968;&#25454;&#22312;&#26410;&#30693;&#25299;&#25169;&#32593;&#32476;&#20013;&#36827;&#34892;&#24433;&#21709;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
IM-META: Influence Maximization Using Node Metadata in Networks With Unknown Topology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2106.02926
&lt;/p&gt;
&lt;p&gt;
IM-META&#26159;&#19968;&#31181;&#22312;&#26410;&#30693;&#25299;&#25169;&#32593;&#32476;&#20013;&#36827;&#34892;&#24433;&#21709;&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33410;&#28857;&#20803;&#25968;&#25454;&#21644;&#26597;&#35810;&#20449;&#24687;&#65292;&#36890;&#36807;&#23398;&#20064;&#20803;&#25968;&#25454;&#19982;&#36793;&#30340;&#20851;&#31995;&#12289;&#26500;&#24314;&#22686;&#24378;&#22270;&#20197;&#21450;&#20351;&#29992;&#25299;&#25169;&#24863;&#30693;&#25490;&#24207;&#31574;&#30053;&#26469;&#30830;&#23450;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#31181;&#23376;&#33410;&#28857;&#21644;&#26597;&#35810;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22797;&#26434;&#32593;&#32476;&#30340;&#32467;&#26500;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25506;&#32034;&#24213;&#23618;&#32593;&#32476;&#30340;&#19968;&#37096;&#20998;&#26469;&#30830;&#23450;&#20855;&#26377;&#26368;&#22823;&#24433;&#21709;&#21147;&#30340;&#31181;&#23376;&#33410;&#28857;&#65292;&#32473;&#23450;&#33410;&#28857;&#26597;&#35810;&#30340;&#23567;&#39044;&#31639;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;IM-META&#65292;&#19968;&#31181;&#35299;&#20915;&#22312;&#26410;&#30693;&#25299;&#25169;&#32593;&#32476;&#20013;&#24433;&#21709;&#26368;&#22823;&#21270;&#65288;IM&#65289;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#26597;&#35810;&#21644;&#33410;&#28857;&#20803;&#25968;&#25454;&#20013;&#26816;&#32034;&#20449;&#24687;&#12290;&#30001;&#20110;&#20351;&#29992;&#36825;&#26679;&#30340;&#20803;&#25968;&#25454;&#24182;&#19981;&#26159;&#27809;&#26377;&#39118;&#38505;&#65292;&#22240;&#20026;&#20803;&#25968;&#25454;&#30340;&#22122;&#22768;&#29305;&#24615;&#21644;&#36830;&#36890;&#24615;&#25512;&#26029;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#26032;&#30340;IM&#38382;&#39064;&#65292;&#26088;&#22312;&#25214;&#21040;&#31181;&#23376;&#33410;&#28857;&#21644;&#26597;&#35810;&#33410;&#28857;&#12290;&#22312;IM-META&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20197;&#19979;&#19977;&#20010;&#27493;&#39588;&#36845;&#20195;&#36827;&#34892;&#65306;1&#65289;&#25105;&#20204;&#36890;&#36807;&#23402;&#29983;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25910;&#38598;&#21040;&#30340;&#20803;&#25968;&#25454;&#19982;&#36793;&#30340;&#20851;&#31995;&#65292;2&#65289;&#25105;&#20204;&#36873;&#25321;&#19968;&#23450;&#25968;&#37327;&#30340;&#25512;&#26029;&#20986;&#30340;&#21487;&#20449;&#36793;&#26469;&#26500;&#24314;&#19968;&#20010;&#22686;&#24378;&#22270;&#65292;3&#65289;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#25299;&#25169;&#24863;&#30693;&#25490;&#24207;&#31574;&#30053;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25512;&#26029;&#30340;&#24433;&#21709;&#25193;&#25955;&#26469;&#30830;&#23450;&#19979;&#19968;&#20010;&#35201;&#26597;&#35810;&#30340;&#33410;&#28857;&#12290;&#36890;&#36807;&#23545;IM-META&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the structure of complex networks is often unknown, we may identify the most influential seed nodes by exploring only a part of the underlying network, given a small budget for node queries. We propose IM-META, a solution to influence maximization (IM) in networks with unknown topology by retrieving information from queries and node metadata. Since using such metadata is not without risk due to the noisy nature of metadata and uncertainties in connectivity inference, we formulate a new IM problem that aims to find both seed nodes and queried nodes. In IM-META, we develop an effective method that iteratively performs three steps: 1) we learn the relationship between collected metadata and edges via a Siamese neural network, 2) we select a number of inferred confident edges to construct a reinforced graph, and 3) we identify the next node to query by maximizing the inferred influence spread using our topology-aware ranking strategy. Through experimental evaluation of IM-META on fou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;InstaHide&#30340;&#26368;&#26032;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#29702;&#35299;&#21644;&#20998;&#26512;&#36825;&#20123;&#25915;&#20987;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#22312;InstaHide&#25361;&#25112;&#35774;&#32622;&#19979;&#65292;&#20197;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#21644;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#24674;&#22797;&#25152;&#26377;&#31169;&#20154;&#22270;&#29255;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#26816;&#32034;&#25152;&#26377;InstaHide&#22270;&#29255;&#30340;&#35745;&#31639;&#38590;&#24230;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2011.11877</link><description>&lt;p&gt;
InstaHide&#28151;&#21512;&#20004;&#20010;&#31169;&#20154;&#22270;&#29255;&#26102;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
InstaHide's Sample Complexity When Mixing Two Private Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2011.11877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;InstaHide&#30340;&#26368;&#26032;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#29702;&#35299;&#21644;&#20998;&#26512;&#36825;&#20123;&#25915;&#20987;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#22312;InstaHide&#25361;&#25112;&#35774;&#32622;&#19979;&#65292;&#20197;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#21644;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#24674;&#22797;&#25152;&#26377;&#31169;&#20154;&#22270;&#29255;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#26816;&#32034;&#25152;&#26377;InstaHide&#22270;&#29255;&#30340;&#35745;&#31639;&#38590;&#24230;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#25935;&#24863;&#35757;&#32451;&#25968;&#25454;&#65292;&#22914;&#20309;&#20445;&#25252;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#24050;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#35838;&#39064;&#12290;InstaHide&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#29992;&#20110;&#20445;&#25252;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#26696;&#65292;&#23545;&#27979;&#35797;&#20934;&#30830;&#24615;&#21482;&#26377;&#24494;&#23567;&#24433;&#21709;&#65292;&#24182;&#19988;&#20854;&#23433;&#20840;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#31361;&#20986;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#23545;InstaHide&#30340;&#26368;&#26032;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#29702;&#35299;&#21644;&#20998;&#26512;&#36825;&#20123;&#25915;&#20987;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#25915;&#20987;&#35201;&#20040;&#27809;&#26377;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#65292;&#35201;&#20040;&#21482;&#33021;&#24674;&#22797;&#19968;&#20010;&#31169;&#20154;&#22270;&#29255;&#12290;&#22312;&#24403;&#21069;&#30340;InstaHide&#25361;&#25112;&#35774;&#32622;&#19979;&#65292;&#27599;&#20010;InstaHide&#22270;&#29255;&#26159;&#20004;&#20010;&#31169;&#20154;&#22270;&#29255;&#30340;&#28151;&#21512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#26469;&#20197;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#21644;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#24674;&#22797;&#25152;&#26377;&#31169;&#20154;&#22270;&#29255;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#26816;&#32034;&#25152;&#26377;InstaHide&#22270;&#29255;&#30340;&#35745;&#31639;&#38590;&#24230;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;InstaHide&#22312;&#20449;&#24687;&#35770;&#19978;&#24182;&#38750;&#26159;&#23433;&#20840;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training neural networks usually require large numbers of sensitive training data, and how to protect the privacy of training data has thus become a critical topic in deep learning research. InstaHide is a state-of-the-art scheme to protect training data privacy with only minor effects on test accuracy, and its security has become a salient question. In this paper, we systematically study recent attacks on InstaHide and present a unified framework to understand and analyze these attacks. We find that existing attacks either do not have a provable guarantee or can only recover a single private image. On the current InstaHide challenge setup, where each InstaHide image is a mixture of two private images, we present a new algorithm to recover all the private images with a provable guarantee and optimal sample complexity. In addition, we also provide a computational hardness result on retrieving all InstaHide images. Our results demonstrate that InstaHide is not information-theoretically s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#39033;&#24335;&#36924;&#36817;&#38544;&#31169;&#20445;&#25252;&#21644;&#21487;&#39564;&#35777;ReLU&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#24179;&#26041;&#20989;&#25968;&#24182;&#19981;&#26159;&#26368;&#20339;&#26367;&#20195;ReLU&#20989;&#25968;&#30340;&#20108;&#27425;&#22810;&#39033;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;</title><link>https://arxiv.org/abs/2011.05530</link><description>&lt;p&gt;
&#20851;&#20110;&#38544;&#31169;&#20445;&#25252;&#21644;&#21487;&#39564;&#35777;ReLU&#32593;&#32476;&#30340;&#22810;&#39033;&#24335;&#36924;&#36817;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Polynomial Approximations for Privacy-Preserving and Verifiable ReLU Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2011.05530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#39033;&#24335;&#36924;&#36817;&#38544;&#31169;&#20445;&#25252;&#21644;&#21487;&#39564;&#35777;ReLU&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#24179;&#26041;&#20989;&#25968;&#24182;&#19981;&#26159;&#26368;&#20339;&#26367;&#20195;ReLU&#20989;&#25968;&#30340;&#20108;&#27425;&#22810;&#39033;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#25512;&#29702;&#20219;&#21153;&#22806;&#21253;&#32473;&#19981;&#21463;&#20449;&#20219;&#30340;&#20113;&#65292;&#24341;&#21457;&#20102;&#25968;&#25454;&#38544;&#31169;&#21644;&#23436;&#25972;&#24615;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;&#26377;&#35768;&#22810;&#25216;&#26415;&#21487;&#20197;&#30830;&#20445;&#22522;&#20110;&#22810;&#39033;&#24335;&#35745;&#31639;&#30340;&#38544;&#31169;&#21644;&#23436;&#25972;&#24615;&#65292;&#20294;DNNs&#28041;&#21450;&#38750;&#22810;&#39033;&#24335;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#22522;&#20110;&#23558;&#38750;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;ReLU&#20989;&#25968;&#65289;&#26367;&#25442;&#20026;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#21487;&#39564;&#35777;&#25512;&#29702;&#25216;&#26415;&#12290;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#38656;&#35201;&#20855;&#26377;&#25972;&#25968;&#31995;&#25968;&#25110;&#26377;&#38480;&#22495;&#19978;&#30340;&#22810;&#39033;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#23558;&#22810;&#39033;&#24335;&#38480;&#21046;&#20026;&#20855;&#26377;&#25972;&#25968;&#31995;&#25968;&#65292;&#24179;&#26041;&#20989;&#25968;&#20063;&#19981;&#26159;&#33021;&#26367;&#20195;ReLU&#20989;&#25968;&#30340;&#26368;&#20339;&#20108;&#27425;&#22810;&#39033;&#24335;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;
&lt;/p&gt;
&lt;p&gt;
Outsourcing deep neural networks (DNNs) inference tasks to an untrusted cloud raises data privacy and integrity concerns. While there are many techniques to ensure privacy and integrity for polynomial-based computations, DNNs involve non-polynomial computations. To address these challenges, several privacy-preserving and verifiable inference techniques have been proposed based on replacing the non-polynomial activation functions such as the rectified linear unit (ReLU) function with polynomial activation functions. Such techniques usually require polynomials with integer coefficients or polynomials over finite fields. Motivated by such requirements, several works proposed replacing the ReLU function with the square function. In this work, we empirically show that the square function is not the best degree-2 polynomial that can replace the ReLU function even when restricting the polynomials to have integer coefficients. We instead propose a degree-2 polynomial activation function with a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#22312;&#39046;&#22495;&#36716;&#31227;&#19979;&#26377;&#25928;&#24212;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#31561;&#20219;&#21153;&#12290;&#36890;&#36807;&#21487;&#24494;&#30340;&#23494;&#24230;&#27604;&#20272;&#35745;&#22120;&#21644;&#20998;&#24067;&#40065;&#26834;&#23398;&#20064;&#26694;&#26550;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20026;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;</title><link>https://arxiv.org/abs/2010.05784</link><description>&lt;p&gt;
&#23398;&#20064;&#39046;&#22495;&#36716;&#31227;&#30340;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#65306;&#19968;&#31181;&#20998;&#24067;&#40065;&#26834;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Calibrated Uncertainties for Domain Shift: A Distributionally Robust Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2010.05784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#22312;&#39046;&#22495;&#36716;&#31227;&#19979;&#26377;&#25928;&#24212;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#31561;&#20219;&#21153;&#12290;&#36890;&#36807;&#21487;&#24494;&#30340;&#23494;&#24230;&#27604;&#20272;&#35745;&#22120;&#21644;&#20998;&#24067;&#40065;&#26834;&#23398;&#20064;&#26694;&#26550;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20026;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39046;&#22495;&#36716;&#31227;&#19979;&#23398;&#20064;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#28304;&#65288;&#35757;&#32451;&#65289;&#20998;&#24067;&#19982;&#30446;&#26631;&#65288;&#27979;&#35797;&#65289;&#20998;&#24067;&#19981;&#21516;&#12290;&#25105;&#20204;&#36890;&#36807;&#21487;&#24494;&#30340;&#23494;&#24230;&#27604;&#20272;&#35745;&#22120;&#26816;&#27979;&#36825;&#31181;&#39046;&#22495;&#36716;&#31227;&#65292;&#24182;&#19982;&#20219;&#21153;&#32593;&#32476;&#19968;&#36215;&#35757;&#32451;&#65292;&#26500;&#25104;&#19968;&#20010;&#20851;&#20110;&#39046;&#22495;&#36716;&#31227;&#30340;&#35843;&#25972;&#21518;&#30340;softmax&#39044;&#27979;&#24418;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#23494;&#24230;&#27604;&#20272;&#35745;&#21453;&#26144;&#20102;&#30446;&#26631;&#65288;&#27979;&#35797;&#65289;&#26679;&#26412;&#19982;&#28304;&#65288;&#35757;&#32451;&#65289;&#20998;&#24067;&#30340;&#25509;&#36817;&#31243;&#24230;&#12290;&#25105;&#20204;&#21033;&#29992;&#23427;&#26469;&#35843;&#25972;&#20219;&#21153;&#32593;&#32476;&#20013;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#20351;&#29992;&#23494;&#24230;&#27604;&#30340;&#24819;&#27861;&#22522;&#20110;&#20998;&#24067;&#40065;&#26834;&#23398;&#20064;&#65288;DRL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#39118;&#38505;&#26368;&#23567;&#21270;&#32771;&#34385;&#39046;&#22495;&#36716;&#31227;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#26377;&#21161;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#20687;&#33258;&#25105;&#35757;&#32451;&#21644;FixMatch&#36825;&#26679;&#30340;&#26041;&#27861;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a framework for learning calibrated uncertainties under domain shifts, where the source (training) distribution differs from the target (test) distribution. We detect such domain shifts via a differentiable density ratio estimator and train it together with the task network, composing an adjusted softmax predictive form concerning domain shift. In particular, the density ratio estimation reflects the closeness of a target (test) sample to the source (training) distribution. We employ it to adjust the uncertainty of prediction in the task network. This idea of using the density ratio is based on the distributionally robust learning (DRL) framework, which accounts for the domain shift by adversarial risk minimization. We show that our proposed method generates calibrated uncertainties that benefit downstream tasks, such as unsupervised domain adaptation (UDA) and semi-supervised learning (SSL). On these tasks, methods like self-training and FixMatch use uncertainties to select
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#36793;&#38469;&#26426;&#21046;&#21644;&#20266;&#26597;&#35810;&#38598;&#30340;&#36328;&#22495;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20266;&#26597;&#35810;&#22270;&#20687;&#65292;&#24182;&#20511;&#37492;&#20154;&#33080;&#35782;&#21035;&#26041;&#27861;&#20013;&#30340;&#22823;&#36793;&#38469;&#26426;&#21046;&#23545;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#23637;&#31034;&#20102;&#20854;&#40065;&#26834;&#24615;&#21644;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#21040;&#26032;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2005.09218</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#36793;&#38469;&#26426;&#21046;&#21644;&#20266;&#26597;&#35810;&#38598;&#30340;&#36328;&#22495;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Large Margin Mechanism and Pseudo Query Set on Cross-Domain Few-Shot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2005.09218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#36793;&#38469;&#26426;&#21046;&#21644;&#20266;&#26597;&#35810;&#38598;&#30340;&#36328;&#22495;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20266;&#26597;&#35810;&#22270;&#20687;&#65292;&#24182;&#20511;&#37492;&#20154;&#33080;&#35782;&#21035;&#26041;&#27861;&#20013;&#30340;&#22823;&#36793;&#38469;&#26426;&#21046;&#23545;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#23637;&#31034;&#20102;&#20854;&#40065;&#26834;&#24615;&#21644;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#21040;&#26032;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23569;&#26679;&#26412;&#23398;&#20064;&#38382;&#39064;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#26041;&#27861;&#37117;&#26159;&#22312;&#21333;&#19968;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#36328;&#22495;&#23569;&#26679;&#26412;&#23398;&#20064;&#26159;&#23569;&#26679;&#26412;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#20010;&#20840;&#26032;&#20998;&#25903;&#65292;&#20854;&#20013;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#35299;&#20915;&#22312;&#21333;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65288;&#20803;&#35757;&#32451;&#65289;&#32780;&#22312;&#21253;&#25324;&#26222;&#36890;&#29289;&#20307;&#12289;&#21355;&#26143;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#22312;&#20869;&#30340;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22823;&#36793;&#38469;&#24494;&#35843;&#26041;&#27861;&#65288;LMM-PQS&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25903;&#25345;&#22270;&#20687;&#29983;&#25104;&#20266;&#26597;&#35810;&#22270;&#20687;&#65292;&#24182;&#20511;&#37492;&#20154;&#33080;&#35782;&#21035;&#26041;&#27861;&#20013;&#30340;&#22823;&#36793;&#38469;&#26426;&#21046;&#23545;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#36827;&#34892;&#24494;&#35843;&#12290;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#65292;LMM-PQS&#22312;&#27604;&#22522;&#20934;&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#33021;&#22815;&#36731;&#26494;&#22320;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#26032;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, few-shot learning problems have received a lot of attention. While methods in most previous works were trained and tested on datasets in one single domain, cross-domain few-shot learning is a brand-new branch of few-shot learning problems, where models handle datasets in different domains between training and testing phases. In this paper, to solve the problem that the model is pre-trained (meta-trained) on a single dataset while fine-tuned on datasets in four different domains, including common objects, satellite images, and medical images, we propose a novel large margin fine-tuning method (LMM-PQS), which generates pseudo query images from support images and fine-tunes the feature extraction modules with a large margin mechanism inspired by methods in face recognition. According to the experiment results, LMM-PQS surpasses the baseline models by a significant margin and demonstrates that our approach is robust and can easily adapt pre-trained models to new domains w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#21033;&#29992;&#26368;&#22823;&#21644;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;&#36827;&#34892;&#39640;&#32500;&#24230;&#29420;&#31435;&#24615;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21345;&#26041;&#26816;&#39564;&#30340;&#31243;&#24207;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27431;&#27663;&#36317;&#31163;&#21644;&#39640;&#26031;&#26680;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#23454;&#35777;&#34920;&#29616;&#21644;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2001.01095</link><description>&lt;p&gt;
&#39640;&#32500;&#24230;&#29420;&#31435;&#24615;&#26816;&#27979;: &#36890;&#36807;&#26368;&#22823;&#21644;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
High-Dimensional Independence Testing via Maximum and Average Distance Correlations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2001.01095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#21033;&#29992;&#26368;&#22823;&#21644;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;&#36827;&#34892;&#39640;&#32500;&#24230;&#29420;&#31435;&#24615;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21345;&#26041;&#26816;&#39564;&#30340;&#31243;&#24207;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27431;&#27663;&#36317;&#31163;&#21644;&#39640;&#26031;&#26680;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#23454;&#35777;&#34920;&#29616;&#21644;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#21033;&#29992;&#26368;&#22823;&#21644;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;&#36827;&#34892;&#22810;&#20803;&#29420;&#31435;&#24615;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#34920;&#24449;&#20102;&#23427;&#20204;&#30456;&#23545;&#20110;&#36793;&#38469;&#30456;&#20851;&#32500;&#24230;&#25968;&#37327;&#30340;&#19968;&#33268;&#24615;&#29305;&#24615;&#65292;&#35780;&#20272;&#20102;&#27599;&#20010;&#26816;&#39564;&#32479;&#35745;&#37327;&#30340;&#20248;&#21183;&#65292;&#26816;&#26597;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#38646;&#20998;&#24067;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24555;&#36895;&#21345;&#26041;&#26816;&#39564;&#30340;&#26816;&#27979;&#31243;&#24207;&#12290;&#24471;&#20986;&#30340;&#26816;&#39564;&#26159;&#38750;&#21442;&#25968;&#30340;&#65292;&#24182;&#36866;&#29992;&#20110;&#27431;&#27663;&#36317;&#31163;&#21644;&#39640;&#26031;&#26680;&#20316;&#20026;&#24213;&#23618;&#24230;&#37327;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#25152;&#25552;&#20986;&#30340;&#27979;&#35797;&#30340;&#23454;&#38469;&#20351;&#29992;&#24773;&#20917;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#22810;&#20803;&#30456;&#20851;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;&#26368;&#22823;&#36317;&#31163;&#30456;&#20851;&#24615;&#12289;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;&#21644;&#21407;&#22987;&#36317;&#31163;&#30456;&#20851;&#24615;&#30340;&#23454;&#35777;&#34920;&#29616;&#65292;&#21516;&#26102;&#36827;&#34892;&#20102;&#19968;&#20010;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#65292;&#20197;&#26816;&#27979;&#20154;&#31867;&#34880;&#27974;&#20013;&#19981;&#21516;&#30284;&#30151;&#31867;&#22411;&#21644;&#32957;&#27700;&#24179;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces and investigates the utilization of maximum and average distance correlations for multivariate independence testing. We characterize their consistency properties in high-dimensional settings with respect to the number of marginally dependent dimensions, assess the advantages of each test statistic, examine their respective null distributions, and present a fast chi-square-based testing procedure. The resulting tests are non-parametric and applicable to both Euclidean distance and the Gaussian kernel as the underlying metric. To better understand the practical use cases of the proposed tests, we evaluate the empirical performance of the maximum distance correlation, average distance correlation, and the original distance correlation across various multivariate dependence scenarios, as well as conduct a real data experiment to test the presence of various cancer types and peptide levels in human plasma.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#27979;&#35797;&#26102;&#24207;&#25968;&#25454;&#20043;&#38388;&#29420;&#31435;&#24615;&#30340;&#26102;&#24207;&#20381;&#36182;&#32479;&#35745;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#20272;&#35745;&#26368;&#20339;&#20381;&#36182;&#28382;&#21518;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#22312;&#27979;&#35797;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#26102;&#28176;&#36817;&#26377;&#25928;&#21644;&#26222;&#36941;&#19968;&#33268;&#65292;&#24182;&#19988;&#19982;&#22810;&#31181;&#20381;&#36182;&#24230;&#37327;&#26041;&#27861;&#20860;&#23481;&#12290;</title><link>https://arxiv.org/abs/1908.06486</link><description>&lt;p&gt;
&#26102;&#24207;&#25968;&#25454;&#30340;&#29420;&#31435;&#24615;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Independence Testing for Temporal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1908.06486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#27979;&#35797;&#26102;&#24207;&#25968;&#25454;&#20043;&#38388;&#29420;&#31435;&#24615;&#30340;&#26102;&#24207;&#20381;&#36182;&#32479;&#35745;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#20272;&#35745;&#26368;&#20339;&#20381;&#36182;&#28382;&#21518;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#22312;&#27979;&#35797;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#26102;&#28176;&#36817;&#26377;&#25928;&#21644;&#26222;&#36941;&#19968;&#33268;&#65292;&#24182;&#19988;&#19982;&#22810;&#31181;&#20381;&#36182;&#24230;&#37327;&#26041;&#27861;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#24207;&#25968;&#25454;&#22312;&#29616;&#20195;&#25968;&#25454;&#31185;&#23398;&#20013;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#21028;&#26029;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#26159;&#21542;&#30456;&#20851;&#12290;&#29616;&#26377;&#26041;&#27861;&#24120;&#24120;&#23384;&#22312;&#38480;&#21046;&#65292;&#22914;&#20381;&#36182;&#21442;&#25968;&#20551;&#35774;&#12289;&#20165;&#26816;&#27979;&#32447;&#24615;&#20851;&#32852;&#12289;&#38656;&#35201;&#22810;&#20010;&#27979;&#35797;&#21644;&#20462;&#27491;&#31561;&#12290;&#34429;&#28982;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;&#38750;&#21442;&#25968;&#21644;&#26222;&#36941;&#19968;&#33268;&#30340;&#20381;&#36182;&#24230;&#37327;&#26041;&#27861;&#65292;&#20294;&#30452;&#25509;&#24212;&#29992;&#20110;&#26102;&#24207;&#25968;&#25454;&#21487;&#33021;&#23548;&#33268;p&#20540;&#33192;&#32960;&#21644;&#26080;&#25928;&#30340;&#26816;&#39564;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#22359;&#32622;&#25442;&#30340;&#26102;&#24207;&#20381;&#36182;&#32479;&#35745;&#37327;&#26469;&#27979;&#35797;&#26102;&#24207;&#25968;&#25454;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#12290;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27979;&#35797;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#26102;&#26159;&#28176;&#36817;&#26377;&#25928;&#21644;&#26222;&#36941;&#19968;&#33268;&#30340;&#65292;&#24182;&#19988;&#33021;&#22815;&#20272;&#35745;&#26368;&#22823;&#21270;&#20381;&#36182;&#30340;&#26368;&#20339;&#20381;&#36182;&#28382;&#21518;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#19982;&#20016;&#23500;&#30340;&#36317;&#31163;&#21644;&#26680;&#24515;&#20381;&#36182;&#24230;&#37327;&#26041;&#27861;&#20860;&#23481;&#65292;&#28040;&#38500;&#20102;
&lt;/p&gt;
&lt;p&gt;
Temporal data are increasingly prevalent in modern data science. A fundamental question is whether two time-series are related or not. Existing approaches often have limitations, such as relying on parametric assumptions, detecting only linear associations, and requiring multiple tests and corrections. While many non-parametric and universally consistent dependence measures have recently been proposed, directly applying them to temporal data can inflate the p-value and result in invalid test. To address these challenges, this paper introduces the temporal dependence statistic with block permutation to test independence between temporal data. Under proper assumptions, the proposed procedure is asymptotically valid and universally consistent for testing independence between stationary time-series, and capable of estimating the optimal dependence lag that maximizes the dependence. Notably, it is compatible with a rich family of distance and kernel based dependence measures, eliminates the
&lt;/p&gt;</description></item><item><title>SimFair&#26159;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#20844;&#24179;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#27169;&#25311;&#21644;&#36870;&#21521;&#24314;&#27169;&#26469;&#35299;&#20915;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#26377;&#25928;&#20445;&#25345;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15270</link><description>&lt;p&gt;
SimFair&#65306;&#29289;&#29702;&#24341;&#23548;&#30340;&#20844;&#24179;&#24863;&#30693;&#23398;&#20064;&#19982;&#27169;&#25311;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SimFair: Physics-Guided Fairness-Aware Learning with Simulation Models. (arXiv:2401.15270v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15270
&lt;/p&gt;
&lt;p&gt;
SimFair&#26159;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#20844;&#24179;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#27169;&#25311;&#21644;&#36870;&#21521;&#24314;&#27169;&#26469;&#35299;&#20915;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#26377;&#25928;&#20445;&#25345;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#24863;&#30693;&#24050;&#32463;&#25104;&#20026;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#36127;&#36131;&#20219;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#22522;&#30784;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#24615;&#33021;&#19981;&#24179;&#31561;&#26159;&#30001;&#20110;&#19981;&#21516;&#21306;&#22495;&#20998;&#24067;&#30340;&#21464;&#21270;&#24341;&#36215;&#30340;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20986;&#25552;&#39640;&#20844;&#24179;&#21487;&#36801;&#31227;&#24615;&#30340;&#25216;&#26415;&#65292;&#20294;&#26159;&#22312;&#27809;&#26377;&#26469;&#33258;&#26032;&#21306;&#22495;&#30340;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#30340;&#65292;&#36825;&#23545;&#20110;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#23581;&#35797;&#26159;&#19968;&#20010;&#29942;&#39048;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#22522;&#20110;&#29289;&#29702;&#26426;&#21046;&#27169;&#22411;&#24050;&#32463;&#22312;&#35768;&#22810;&#20855;&#26377;&#37325;&#22823;&#31038;&#20250;&#24433;&#21709;&#30340;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SimFair&#65292;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#20844;&#24179;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#22522;&#20110;&#29289;&#29702;&#35268;&#21017;&#30340;&#27169;&#25311;&#21644;&#36870;&#21521;&#24314;&#27169;&#21040;&#35757;&#32451;&#35774;&#35745;&#20013;&#26469;&#24357;&#34917;&#25968;&#25454;&#38480;&#21046;&#12290;&#20197;&#28201;&#24230;&#39044;&#27979;&#20026;&#20363;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;SimFair&#22312;&#20445;&#25345;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness-awareness has emerged as an essential building block for the responsible use of artificial intelligence in real applications. In many cases, inequity in performance is due to the change in distribution over different regions. While techniques have been developed to improve the transferability of fairness, a solution to the problem is not always feasible with no samples from the new regions, which is a bottleneck for pure data-driven attempts. Fortunately, physics-based mechanistic models have been studied for many problems with major social impacts. We propose SimFair, a physics-guided fairness-aware learning framework, which bridges the data limitation by integrating physical-rule-based simulation and inverse modeling into the training design. Using temperature prediction as an example, we demonstrate the effectiveness of the proposed SimFair in fairness preservation.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#27668;&#20505;&#29305;&#24449;&#39044;&#27979;&#22826;&#38451;&#33021;&#21457;&#30005;&#37327;&#65292;&#22312;&#19981;&#21516;&#22320;&#21306;&#38388;&#20855;&#26377;&#19968;&#23450;&#30340;&#36890;&#29992;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14422</link><description>&lt;p&gt;
&#23450;&#20301;&#26080;&#20851;&#28304;&#20813;&#22495;&#33258;&#36866;&#24212;&#23398;&#20064;&#39044;&#27979;&#22826;&#38451;&#33021;&#21457;&#30005;
&lt;/p&gt;
&lt;p&gt;
Location Agnostic Source-Free Domain Adaptive Learning to Predict Solar Power Generation. (arXiv:2401.14422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14422
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#27668;&#20505;&#29305;&#24449;&#39044;&#27979;&#22826;&#38451;&#33021;&#21457;&#30005;&#37327;&#65292;&#22312;&#19981;&#21516;&#22320;&#21306;&#38388;&#20855;&#26377;&#19968;&#23450;&#30340;&#36890;&#29992;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22826;&#38451;&#33021;&#21457;&#30005;&#30340;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;&#21576;&#29616;&#31354;&#38388;&#21644;&#26102;&#38388;&#21464;&#24322;&#24615;&#30340;&#27668;&#20505;&#29305;&#24449;&#12290;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#22240;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#32780;&#22312;&#19981;&#21516;&#22320;&#21306;&#20135;&#29983;&#21464;&#21270;&#65292;&#23548;&#33268;&#19968;&#20010;&#22312;&#26576;&#20010;&#22320;&#21306;&#24037;&#20316;&#33391;&#22909;&#20294;&#22312;&#20854;&#20182;&#22320;&#21306;&#19981;&#36215;&#20316;&#29992;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20840;&#29699;&#21464;&#26262;&#65292;&#27599;&#24180;&#22825;&#27668;&#27169;&#24335;&#30340;&#21464;&#21270;&#22312;&#21152;&#36895;&#12290;&#36825;&#31181;&#29616;&#35937;&#21487;&#33021;&#23548;&#33268;&#29616;&#26377;&#27169;&#22411;&#22312;&#21516;&#19968;&#22320;&#29702;&#21306;&#22495;&#20869;&#38543;&#26102;&#38388;&#25512;&#31227;&#32780;&#25928;&#26524;&#20943;&#24369;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#20197;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#30340;&#27668;&#20505;&#29305;&#24449;&#26469;&#20272;&#35745;&#22826;&#38451;&#33021;&#21457;&#30005;&#37327;&#12290;&#37319;&#29992;&#21069;&#39304;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#27169;&#22411;&#26469;&#22312;&#24050;&#30693;&#20301;&#32622;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#26377;&#30417;&#30563;&#35757;&#32451;&#65292;&#24182;&#29992;&#20110;&#21518;&#32493;&#39044;&#27979;&#26410;&#30693;&#20301;&#32622;&#30340;&#22826;&#38451;&#33021;&#21457;&#30005;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prediction of solar power generation is a challenging task due to its dependence on climatic characteristics that exhibit spatial and temporal variability. The performance of a prediction model may vary across different places due to changes in data distribution, resulting in a model that works well in one region but not in others. Furthermore, as a consequence of global warming, there is a notable acceleration in the alteration of weather patterns on an annual basis. This phenomenon introduces the potential for diminished efficacy of existing models, even within the same geographical region, as time progresses. In this paper, a domain adaptive deep learning-based framework is proposed to estimate solar power generation using weather features that can solve the aforementioned challenges. A feed-forward deep convolutional network model is trained for a known location dataset in a supervised manner and utilized to predict the solar power of an unknown location later. This adaptive da
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#22810;&#27169;&#24577;&#26102;&#38388;&#20851;&#31995;&#22270;&#23398;&#20064;&#65288;MTRGL&#65289;&#65292;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#31163;&#25955;&#29305;&#24449;&#32467;&#21512;&#25104;&#19968;&#20010;&#26102;&#38388;&#22270;&#65292;&#24182;&#37319;&#29992;&#35760;&#24518;&#26426;&#21046;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#36776;&#35782;&#23454;&#20307;&#38388;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#21462;&#24471;&#20102;&#22312;&#23454;&#35777;&#23454;&#39564;&#20013;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;&#36825;&#19968;&#30740;&#31350;&#23545;&#20110;&#25552;&#21319;&#33258;&#21160;&#21270;&#37197;&#23545;&#20132;&#26131;&#31574;&#30053;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.14199</link><description>&lt;p&gt;
MTRGL&#65306;&#36890;&#36807;&#22810;&#27169;&#24577;&#26102;&#38388;&#20851;&#31995;&#22270;&#23398;&#20064;&#26377;&#25928;&#36776;&#35782;&#26102;&#38388;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
MTRGL:Effective Temporal Correlation Discerning through Multi-modal Temporal Relational Graph Learning. (arXiv:2401.14199v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14199
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#22810;&#27169;&#24577;&#26102;&#38388;&#20851;&#31995;&#22270;&#23398;&#20064;&#65288;MTRGL&#65289;&#65292;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#31163;&#25955;&#29305;&#24449;&#32467;&#21512;&#25104;&#19968;&#20010;&#26102;&#38388;&#22270;&#65292;&#24182;&#37319;&#29992;&#35760;&#24518;&#26426;&#21046;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#36776;&#35782;&#23454;&#20307;&#38388;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#21462;&#24471;&#20102;&#22312;&#23454;&#35777;&#23454;&#39564;&#20013;&#30340;&#20248;&#24322;&#34920;&#29616;&#12290;&#36825;&#19968;&#30740;&#31350;&#23545;&#20110;&#25552;&#21319;&#33258;&#21160;&#21270;&#37197;&#23545;&#20132;&#26131;&#31574;&#30053;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#19982;&#37329;&#34701;&#24066;&#22330;&#24212;&#29992;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#37197;&#23545;&#20132;&#26131;&#12290;&#36825;&#31181;&#24066;&#22330;&#20013;&#24615;&#31574;&#30053;&#23545;&#37327;&#21270;&#37329;&#34701;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12290;&#37197;&#23545;&#20132;&#26131;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#36776;&#35782;&#23454;&#20307;&#20043;&#38388;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#36825;&#35201;&#27714;&#25972;&#21512;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#27169;&#24577;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#22810;&#27169;&#24577;&#26102;&#38388;&#20851;&#31995;&#22270;&#23398;&#20064;&#65288;MTRGL&#65289;&#12290;MTRGL&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#31163;&#25955;&#29305;&#24449;&#32467;&#21512;&#21040;&#19968;&#20010;&#26102;&#38388;&#22270;&#20013;&#65292;&#24182;&#37319;&#29992;&#20102;&#22522;&#20110;&#35760;&#24518;&#30340;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#26102;&#38388;&#30456;&#20851;&#24615;&#35782;&#21035;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#20010;&#26102;&#38388;&#22270;&#38142;&#36335;&#39044;&#27979;&#20219;&#21153;&#65292;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;MTRGL&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#24378;&#35843;&#20854;&#22312;&#20248;&#21270;&#33258;&#21160;&#21270;&#37197;&#23545;&#20132;&#26131;&#31574;&#30053;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we explore the synergy of deep learning and financial market applications, focusing on pair trading. This market-neutral strategy is integral to quantitative finance and is apt for advanced deep-learning techniques. A pivotal challenge in pair trading is discerning temporal correlations among entities, necessitating the integration of diverse data modalities. Addressing this, we introduce a novel framework, Multi-modal Temporal Relation Graph Learning (MTRGL). MTRGL combines time series data and discrete features into a temporal graph and employs a memory-based temporal graph neural network. This approach reframes temporal correlation identification as a temporal graph link prediction task, which has shown empirical success. Our experiments on real-world datasets confirm the superior performance of MTRGL, emphasizing its promise in refining automated pair trading strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;LPNL&#26694;&#26550;&#29992;&#20110;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#35821;&#21644;&#37319;&#26679;&#27969;&#31243;&#65292;&#20197;&#21450;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22270;&#20013;&#30340;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13227</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38142;&#25509;&#39044;&#27979;&#30340;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models. (arXiv:2401.13227v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;LPNL&#26694;&#26550;&#29992;&#20110;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#35821;&#21644;&#37319;&#26679;&#27969;&#31243;&#65292;&#20197;&#21450;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22270;&#20013;&#30340;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22270;&#23398;&#20064;&#26159;&#19968;&#39033;&#26032;&#39062;&#30340;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22270;&#20013;&#34164;&#21547;&#30340;&#22823;&#37327;&#20449;&#24687;&#32473;&#36825;&#19968;&#36807;&#31243;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#20171;&#32461;&#20102;LPNL&#65288;Link Prediction via Natural Language&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#30340;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#33021;&#20197;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#22270;&#32454;&#33410;&#30340;&#21019;&#26032;&#25552;&#31034;&#35821;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#37319;&#26679;&#27969;&#31243;&#65292;&#20174;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#37319;&#29992;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#26469;&#25511;&#21046;&#36755;&#20837;&#20196;&#29260;&#25968;&#37327;&#22312;&#39044;&#23450;&#38480;&#21046;&#20869;&#65292;&#35299;&#20915;&#20102;&#20449;&#24687;&#36807;&#36733;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#30340;T5&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#22823;&#22411;&#20844;&#20849;&#24322;&#26500;&#22270;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;LPNL&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#21508;&#31181;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring the application of large-scale language models to graph learning is a novel endeavor. However, the vast amount of information inherent in large graphs poses significant challenges to this process. This paper focuses on the link prediction task and introduces LPNL (Link Prediction via Natural Language), a framework based on a large language model designed for scalable link prediction on large-scale heterogeneous graphs.We design novel prompts for link prediction that articulate graph details in natural language. We propose a two-stage sampling pipeline to extract crucial information from large-scale heterogeneous graphs, and a divide-and-conquer strategy to control the input token count within predefined limits, addressing the challenge of overwhelming information. We fine-tune a T5 model based on our self-supervised learning designed for for link prediction. Extensive experiments on a large public heterogeneous graphs demonstrate that LPNL outperforms various advanced baselin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#35777;&#26126;&#20102;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.10463</link><description>&lt;p&gt;
&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
Critical Data Size of Language Models from a Grokking Perspective. (arXiv:2401.10463v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#35777;&#26126;&#20102;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#36825;&#26159;&#20174;&#24555;&#36895;&#35760;&#24518;&#21040;&#32531;&#24930;&#27867;&#21270;&#30340;&#19968;&#20010;&#22522;&#26412;&#36716;&#21464;&#38408;&#20540;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#30456;&#21464;&#24418;&#24335;&#21270;&#20026;Grokking&#37197;&#32622;&#19979;&#30340;&#25968;&#25454;&#25928;&#29575;&#20551;&#35828;&#65292;&#24182;&#30830;&#23450;&#20102;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21160;&#21147;&#23398;&#20013;&#30340;&#25968;&#25454;&#19981;&#36275;&#12289;&#20805;&#36275;&#21644;&#36807;&#21097;&#38454;&#27573;&#12290;&#25105;&#20204;&#36890;&#36807;&#37325;&#26032;&#35843;&#25972;&#21021;&#22987;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#65292;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;Grokking&#37197;&#32622;&#65292;&#31283;&#23450;&#22320;&#22312;&#31616;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#37325;&#29616;&#20102;Grokking&#12290;&#25105;&#20204;&#34920;&#26126;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#26679;&#26412;&#32423;&#21644;&#27169;&#22411;&#32423;&#30340;Grokking&#65292;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#25968;&#25454;&#25928;&#29575;&#20551;&#35828;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#25968;&#25454;&#38598;&#22823;&#23567;&#22788;&#21457;&#29983;&#30340;&#26356;&#24179;&#28369;&#30340;&#30456;&#21464;&#12290;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#36825;&#20010;&#20020;&#30028;&#28857;&#20063;&#21464;&#24471;&#26356;&#22823;&#65292;&#36825;&#34920;&#26126;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#21152;&#28145;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#29702;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data. Our results deepen the understanding of language model training, offering a novel pers
&lt;/p&gt;</description></item><item><title>&#36793;&#32536;&#21464;&#25442;&#22120;&#26159;&#19968;&#20010;&#20840;&#23616;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#33267;&#23569;3-WL&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#36229;&#36807;&#20854;&#20182;&#26550;&#26500;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20301;&#32622;&#25110;&#32467;&#26500;&#32534;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.10119</link><description>&lt;p&gt;
&#36208;&#21521;&#22522;&#20110;&#21407;&#21017;&#30340;&#22270;&#24418;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Towards Principled Graph Transformers. (arXiv:2401.10119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10119
&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#21464;&#25442;&#22120;&#26159;&#19968;&#20010;&#20840;&#23616;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#33267;&#23569;3-WL&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#36229;&#36807;&#20854;&#20182;&#26550;&#26500;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20301;&#32622;&#25110;&#32467;&#26500;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;k&#32500;Weisfeiler-Leman&#65288;k-WL&#65289;&#23618;&#27425;&#32467;&#26500;&#30340;&#22270;&#24418;&#23398;&#20064;&#26550;&#26500;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#24456;&#22909;&#29702;&#35299;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26550;&#26500;&#22312;&#30495;&#23454;&#20219;&#21153;&#20013;&#24448;&#24448;&#26080;&#27861;&#25552;&#20379;&#21487;&#38752;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24433;&#21709;&#21147;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;&#20840;&#23616;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#22914;&#22270;&#24418;&#21464;&#25442;&#22120;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#23558;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#19982;k-WL&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#27604;&#36739;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23588;&#20854;&#26159;&#22240;&#20026;&#36825;&#20123;&#26550;&#26500;&#20381;&#36182;&#20110;&#20301;&#32622;&#25110;&#32467;&#26500;&#32534;&#30721;&#26469;&#23454;&#29616;&#20854;&#34920;&#36798;&#33021;&#21147;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#36793;&#32536;&#21464;&#25442;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#33410;&#28857;&#23545;&#32780;&#19981;&#26159;&#33410;&#28857;&#19978;&#36827;&#34892;&#25805;&#20316;&#30340;&#20840;&#23616;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#20855;&#26377;&#33267;&#23569;3-WL&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#32463;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36793;&#32536;&#21464;&#25442;&#22120;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#29702;&#35770;&#23545;&#40784;&#30340;&#26550;&#26500;&#65292;&#21516;&#26102;&#19981;&#20381;&#36182;&#20110;&#20301;&#32622;&#25110;&#32467;&#26500;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph learning architectures based on the k-dimensional Weisfeiler-Leman (k-WL) hierarchy offer a theoretically well-understood expressive power. However, such architectures often fail to deliver solid predictive performance on real-world tasks, limiting their practical impact. In contrast, global attention-based models such as graph transformers demonstrate strong performance in practice, but comparing their expressive power with the k-WL hierarchy remains challenging, particularly since these architectures rely on positional or structural encodings for their expressivity and predictive performance. To address this, we show that the recently proposed Edge Transformer, a global attention model operating on node pairs instead of nodes, has at least 3-WL expressive power. Empirically, we demonstrate that the Edge Transformer surpasses other theoretically aligned architectures regarding predictive performance while not relying on positional or structural encodings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;ODE&#25554;&#20540;&#20013;&#28145;&#24230;&#21644;&#23485;&#24230;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#22312;&#25968;&#25454;&#38598;&#25554;&#20540;&#20013;&#23384;&#22312;&#30528;$p$&#21644;$L$&#20043;&#38388;&#30340;&#24179;&#34913;&#25240;&#34935;&#20851;&#31995;&#65292;&#32780;&#22312;&#27979;&#24230;&#25554;&#20540;&#20013;&#65292;$L$&#30340;&#22686;&#38271;&#19982;$p$&#21644;$\varepsilon$&#30340;&#20851;&#31995;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2401.09902</link><description>&lt;p&gt;
&#28145;&#24230;&#21644;&#23485;&#24230;&#22312;&#31070;&#32463;ODE&#25554;&#20540;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Interplay between depth and width for interpolation in neural ODEs. (arXiv:2401.09902v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;ODE&#25554;&#20540;&#20013;&#28145;&#24230;&#21644;&#23485;&#24230;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#22312;&#25968;&#25454;&#38598;&#25554;&#20540;&#20013;&#23384;&#22312;&#30528;$p$&#21644;$L$&#20043;&#38388;&#30340;&#24179;&#34913;&#25240;&#34935;&#20851;&#31995;&#65292;&#32780;&#22312;&#27979;&#24230;&#25554;&#20540;&#20013;&#65292;$L$&#30340;&#22686;&#38271;&#19982;$p$&#21644;$\varepsilon$&#30340;&#20851;&#31995;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(neural ODEs)&#24050;&#32463;&#25104;&#20026;&#20174;&#25511;&#21046;&#35282;&#24230;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#28982;&#24037;&#20855;&#65292;&#28982;&#32780;&#23545;&#20854;&#26368;&#20339;&#32467;&#26500;&#30340;&#23436;&#20840;&#29702;&#35299;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23485;&#24230;$p$&#21644;&#23618;&#20043;&#38388;&#30340;&#36807;&#28193;&#27425;&#25968;$L$&#65288;&#23454;&#38469;&#19978;&#26159;&#28145;&#24230;$L+1$&#65289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20197;&#20854;&#33021;&#22815;&#22312;Wasserstein&#35823;&#24046;&#36793;&#30028;$\varepsilon&gt;0$&#20869;&#25554;&#20540;&#19968;&#20010;&#21253;&#21547;$N$&#23545;&#28857;&#30340;&#26377;&#38480;&#25968;&#25454;&#38598;$D$&#25110;&#20004;&#20010;&#27010;&#29575;&#27979;&#24230;&#22312;$\mathbb{R}^d$&#20013;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;$p$&#21644;$L$&#20043;&#38388;&#30340;&#24179;&#34913;&#25240;&#34935;&#20851;&#31995;&#65292;&#22312;&#25968;&#25454;&#38598;&#25554;&#20540;&#20013;&#65292;$L$&#38543;&#30528;$O(1+N/p)$&#30340;&#27604;&#20363;&#22686;&#38271;&#65292;&#32780;&#22312;&#27979;&#24230;&#25554;&#20540;&#20013;&#65292;$L=O\left(1+(p\varepsilon^d)^{-1}\right)$&#12290;&#22312;&#33258;&#20027;&#24773;&#20917;&#19979;&#65292;$L=0$&#65292;&#38656;&#35201;&#36827;&#34892;&#21333;&#29420;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#20851;&#27880;&#25968;&#25454;&#38598;&#25554;&#20540;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;$\varepsilon$-&#36817;&#20284;&#25511;&#21046;&#24615;&#30340;&#25918;&#26494;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Neural ordinary differential equations (neural ODEs) have emerged as a natural tool for supervised learning from a control perspective, yet a complete understanding of their optimal architecture remains elusive. In this work, we examine the interplay between their width $p$ and number of layer transitions $L$ (effectively the depth $L+1$). Specifically, we assess the model expressivity in terms of its capacity to interpolate either a finite dataset $D$ comprising $N$ pairs of points or two probability measures in $\mathbb{R}^d$ within a Wasserstein error margin $\varepsilon&gt;0$. Our findings reveal a balancing trade-off between $p$ and $L$, with $L$ scaling as $O(1+N/p)$ for dataset interpolation, and $L=O\left(1+(p\varepsilon^d)^{-1}\right)$ for measure interpolation.  In the autonomous case, where $L=0$, a separate study is required, which we undertake focusing on dataset interpolation. We address the relaxed problem of $\varepsilon$-approximate controllability and establish an error 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#31471;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#26368;&#26032;&#25216;&#26415;&#65292;&#22312;&#32473;&#23450;&#30340;&#21387;&#32553;&#39044;&#31639;&#19979;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.06118</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#31471;&#21387;&#32553;&#36890;&#36807;&#21152;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Extreme Compression of Large Language Models via Additive Quantization. (arXiv:2401.06118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#31471;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#26368;&#26032;&#25216;&#26415;&#65292;&#22312;&#32473;&#23450;&#30340;&#21387;&#32553;&#39044;&#31639;&#19979;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#24320;&#28304;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#37327;&#21270;&#25216;&#26415;&#30340;&#31454;&#36187;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#22815;&#22312;&#26368;&#32456;&#29992;&#25143;&#35774;&#22791;&#19978;&#25191;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#22810;&#30721;&#26412;&#37327;&#21270;(MCQ)&#30340;&#32463;&#20856;&#26041;&#27861;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#20102;&#8220;&#26497;&#31471;&#8221;LLM&#21387;&#32553;&#30340;&#38382;&#39064;&#65292;&#21363;&#38024;&#23545;&#38750;&#24120;&#20302;&#30340;&#20301;&#25968;&#65292;&#20363;&#22914;&#27599;&#20010;&#21442;&#25968;2&#21040;3&#20301;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24314;&#31435;&#22312;&#21152;&#24615;&#37327;&#21270;&#36825;&#19968;&#32463;&#20856;&#31639;&#27861;&#20043;&#19978;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#31639;&#27861;&#22312;LLM&#21387;&#32553;&#26041;&#38754;&#25512;&#36827;&#20102;&#26368;&#26032;&#25216;&#26415;&#65292;&#20197;&#32473;&#23450;&#21387;&#32553;&#39044;&#31639;&#30340;&#20934;&#30830;&#24615;&#32780;&#35328;&#65292;&#20248;&#20110;&#25152;&#26377;&#26368;&#36817;&#25552;&#20986;&#30340;&#25216;&#26415;&#12290;&#20363;&#22914;&#65292;&#24403;&#23558;Llama 2&#27169;&#22411;&#21387;&#32553;&#21040;&#27599;&#20010;&#21442;&#25968;2&#20301;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;7B&#27169;&#22411;&#37327;&#21270;&#20026;6.93&#22256;&#24785;&#24230;(&#30456;&#23545;&#20110;&#20043;&#21069;&#26368;&#20339;&#24037;&#20316;&#25913;&#36827;1.29&#65292;&#30456;&#23545;&#20110;FP16&#25913;&#36827;1.81)&#65292;13B&#27169;&#22411;&#37327;&#21270;&#20026;5.70&#22256;&#24785;&#24230;(&#25913;&#36827;0.36)&#65292;70B&#27169;&#22411;&#37327;&#21270;&#20026;3.94&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of accurate open large language models (LLMs) has led to a race towards quantization techniques for such models enabling execution on end-user devices. In this paper, we revisit the problem of "extreme" LLM compression--defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter, from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our work builds on top of Additive Quantization, a classic algorithm from the MCQ family, and adapts it to the quantization of language models. The resulting algorithm advances the state-of-the-art in LLM compression, outperforming all recently-proposed techniques in terms of accuracy at a given compression budget. For instance, when compressing Llama 2 models to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93 perplexity (a 1.29 improvement relative to the best prior work, and 1.81 points from FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70B model to 3.94 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;Agent-Based&#27169;&#22411;&#27169;&#25311;&#20102;&#33647;&#29289;&#12289;&#34892;&#20026;&#21644;&#25968;&#23383;&#24178;&#39044;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#24314;&#35758;&#32508;&#21512;&#36816;&#29992;&#36825;&#20123;&#24178;&#39044;&#25514;&#26045;&#24212;&#23545;&#22823;&#27969;&#34892;&#30123;&#24773;&#12290;&#36890;&#36807;&#20998;&#26512;&#21457;&#29616;&#65292;&#26368;&#21021;&#30340;100&#22825;&#23545;&#20915;&#23450;&#30123;&#24773;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#24378;&#35843;&#20102;&#36805;&#36895;&#20915;&#31574;&#21644;&#39640;&#25928;&#25919;&#31574;&#21046;&#23450;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04795</link><description>&lt;p&gt;
&#22823;&#27969;&#34892;&#30340;&#21069;100&#22825;&#65307;&#33647;&#29289;&#12289;&#34892;&#20026;&#21644;&#25968;&#23383;&#24178;&#39044;&#30340;&#30456;&#20114;&#20316;&#29992;&#8212;&#8212;&#22522;&#20110;Agent-Based&#27169;&#22411;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
First 100 days of pandemic; an interplay of pharmaceutical, behavioral and digital interventions -- A study using agent based modeling. (arXiv:2401.04795v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;Agent-Based&#27169;&#22411;&#27169;&#25311;&#20102;&#33647;&#29289;&#12289;&#34892;&#20026;&#21644;&#25968;&#23383;&#24178;&#39044;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#24314;&#35758;&#32508;&#21512;&#36816;&#29992;&#36825;&#20123;&#24178;&#39044;&#25514;&#26045;&#24212;&#23545;&#22823;&#27969;&#34892;&#30123;&#24773;&#12290;&#36890;&#36807;&#20998;&#26512;&#21457;&#29616;&#65292;&#26368;&#21021;&#30340;100&#22825;&#23545;&#20915;&#23450;&#30123;&#24773;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#24378;&#35843;&#20102;&#36805;&#36895;&#20915;&#31574;&#21644;&#39640;&#25928;&#25919;&#31574;&#21046;&#23450;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#27969;&#34892;&#65292;&#29305;&#21035;&#26159;&#26368;&#36817;&#30340;COVID-19&#30123;&#24773;&#29190;&#21457;&#65292;&#23545;&#20844;&#20849;&#21355;&#29983;&#21644;&#20840;&#29699;&#32463;&#27982;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#30142;&#30149;&#21457;&#23637;&#36235;&#21183;&#21644;&#39640;&#25928;&#30340;&#24212;&#23545;&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#28508;&#22312;&#30340;&#26410;&#26469;&#30123;&#24773;&#29190;&#21457;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#20195;&#29702;&#20154;&#27169;&#22411;&#65288;Agent-Based Models&#65292;ABM&#65289;&#22312;&#25429;&#25417;&#22797;&#26434;&#30340;&#24863;&#26579;&#21160;&#24577;&#21644;&#29702;&#35299;&#24178;&#39044;&#25514;&#26045;&#24433;&#21709;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#27169;&#25311;&#20102;&#21453;&#26144;&#29616;&#23454;&#25919;&#31574;&#37319;&#32435;&#20013;&#30340;&#25361;&#25112;&#30340;&#30495;&#23454;&#33647;&#29289;&#12289;&#34892;&#20026;&#21644;&#25968;&#23383;&#24178;&#39044;&#65292;&#24182;&#25552;&#20986;&#20102;&#36825;&#20123;&#24178;&#39044;&#30340;&#25972;&#20307;&#32452;&#21512;&#29992;&#20110;&#22823;&#27969;&#34892;&#30123;&#24773;&#24212;&#23545;&#12290;&#36890;&#36807;&#36825;&#20123;&#27169;&#25311;&#65292;&#25105;&#20204;&#26681;&#25454;&#21326;&#30427;&#39039;&#24030;&#37329;&#26031;&#21439;&#30340;&#30495;&#23454;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#21644;&#22320;&#29702;&#26222;&#26597;&#25968;&#25454;&#65292;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#20154;&#21475;&#30340;&#26032;&#20852;&#34892;&#20026;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#26368;&#21021;100&#22825;&#22312;&#20915;&#23450;&#22823;&#27969;&#34892;&#30123;&#24773;&#36208;&#21183;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24378;&#35843;&#20102;&#24555;&#36895;&#20915;&#31574;&#21644;&#39640;&#25928;&#25919;&#31574;&#21046;&#23450;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pandemics, notably the recent COVID-19 outbreak, have impacted both public health and the global economy. A profound understanding of disease progression and efficient response strategies is thus needed to prepare for potential future outbreaks. In this paper, we emphasize the potential of Agent-Based Models (ABM) in capturing complex infection dynamics and understanding the impact of interventions. We simulate realistic pharmaceutical, behavioral, and digital interventions that mirror challenges in real-world policy adoption and suggest a holistic combination of these interventions for pandemic response. Using these simulations, we study the trends of emergent behavior on a large-scale population based on real-world socio-demographic and geo-census data from Kings County in Washington. Our analysis reveals the pivotal role of the initial 100 days in dictating a pandemic's course, emphasizing the importance of quick decision-making and efficient policy development. Further, we highligh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26680;Fisher-Rao&#27969;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#21333;&#20301;&#26102;&#38388;&#20869;&#20174;&#38750;&#24402;&#19968;&#21270;&#30446;&#26631;&#23494;&#24230;&#25110;&#36125;&#21494;&#26031;&#21518;&#39564;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#26041;&#27861;&#20351;&#29992;&#20102;&#22343;&#22330;ODE&#21644;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#65292;&#26080;&#38656;&#26799;&#24230;&#65292;&#21482;&#38656;&#35201;&#33021;&#22815;&#20174;&#21442;&#32771;&#23494;&#24230;&#20013;&#37319;&#26679;&#24182;&#35745;&#31639;&#30446;&#26631;&#23545;&#21442;&#32771;&#23494;&#24230;&#30340;&#27604;&#29575;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#20960;&#20309;&#28151;&#21512;&#30340;&#36335;&#24452;&#19978;&#27839;&#36895;&#24230;&#22330;&#36816;&#36755;&#26679;&#26412;&#65292;&#24452;&#21521;&#36755;&#36816;&#26679;&#26412;&#12290;&#26041;&#27861;&#36890;&#36807;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#27714;&#35299;&#27850;&#26494;&#26041;&#31243;&#65292;&#20351;&#27850;&#26494;&#26041;&#31243;&#30340;&#27714;&#35299;&#21464;&#24471;&#21487;&#34892;&#65292;&#24182;&#23558;&#20854;&#31163;&#25955;&#21270;&#20026;&#26377;&#38480;&#26679;&#26412;&#30340;&#22343;&#22330;ODE&#65292;&#20316;&#20026;&#23454;&#29616;&#31616;&#21333;&#30340;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#12290;&#21516;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#20063;&#21487;&#20197;&#20174;&#31163;&#25955;&#26102;&#38388;&#30340;&#35282;&#24230;&#25512;&#23548;&#20986;&#22343;&#22330;ODE&#65292;&#20316;&#20026;&#33945;&#26480;-&#23433;&#26222;&#23572;&#26041;&#31243;&#36830;&#32493;&#32447;&#24615;&#21270;&#30340;&#26497;&#38480;&#12290;</title><link>http://arxiv.org/abs/2401.03892</link><description>&lt;p&gt;
&#20197;&#26680;Fisher-Rao&#27969;&#36827;&#34892;&#21333;&#20301;&#26102;&#38388;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Sampling in Unit Time with Kernel Fisher-Rao Flow. (arXiv:2401.03892v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26680;Fisher-Rao&#27969;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#21333;&#20301;&#26102;&#38388;&#20869;&#20174;&#38750;&#24402;&#19968;&#21270;&#30446;&#26631;&#23494;&#24230;&#25110;&#36125;&#21494;&#26031;&#21518;&#39564;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#26041;&#27861;&#20351;&#29992;&#20102;&#22343;&#22330;ODE&#21644;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#65292;&#26080;&#38656;&#26799;&#24230;&#65292;&#21482;&#38656;&#35201;&#33021;&#22815;&#20174;&#21442;&#32771;&#23494;&#24230;&#20013;&#37319;&#26679;&#24182;&#35745;&#31639;&#30446;&#26631;&#23545;&#21442;&#32771;&#23494;&#24230;&#30340;&#27604;&#29575;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#20960;&#20309;&#28151;&#21512;&#30340;&#36335;&#24452;&#19978;&#27839;&#36895;&#24230;&#22330;&#36816;&#36755;&#26679;&#26412;&#65292;&#24452;&#21521;&#36755;&#36816;&#26679;&#26412;&#12290;&#26041;&#27861;&#36890;&#36807;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#27714;&#35299;&#27850;&#26494;&#26041;&#31243;&#65292;&#20351;&#27850;&#26494;&#26041;&#31243;&#30340;&#27714;&#35299;&#21464;&#24471;&#21487;&#34892;&#65292;&#24182;&#23558;&#20854;&#31163;&#25955;&#21270;&#20026;&#26377;&#38480;&#26679;&#26412;&#30340;&#22343;&#22330;ODE&#65292;&#20316;&#20026;&#23454;&#29616;&#31616;&#21333;&#30340;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#12290;&#21516;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#20063;&#21487;&#20197;&#20174;&#31163;&#25955;&#26102;&#38388;&#30340;&#35282;&#24230;&#25512;&#23548;&#20986;&#22343;&#22330;ODE&#65292;&#20316;&#20026;&#33945;&#26480;-&#23433;&#26222;&#23572;&#26041;&#31243;&#36830;&#32493;&#32447;&#24615;&#21270;&#30340;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22343;&#22330;ODE&#21644;&#30456;&#24212;&#30340;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#65292;&#29992;&#20110;&#20174;&#38750;&#24402;&#19968;&#21270;&#30340;&#30446;&#26631;&#23494;&#24230;&#25110;&#36125;&#21494;&#26031;&#21518;&#39564;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#26080;&#38656;&#26799;&#24230;&#65292;&#21487;&#20197;&#38381;&#21512;&#24418;&#24335;&#33719;&#24471;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#33021;&#22815;&#20174;&#21442;&#32771;&#23494;&#24230;&#20013;&#37319;&#26679;&#24182;&#35745;&#31639;&#65288;&#38750;&#24402;&#19968;&#21270;&#30340;&#65289;&#30446;&#26631;&#23545;&#21442;&#32771;&#23494;&#24230;&#30340;&#27604;&#29575;&#12290;&#36890;&#36807;&#27714;&#35299;&#36816;&#36755;&#26679;&#26412;&#27839;&#20004;&#20010;&#23494;&#24230;&#30340;&#20960;&#20309;&#28151;&#21512;&#30340;&#36895;&#24230;&#22330;&#30340;&#27850;&#26494;&#26041;&#31243;&#26469;&#33719;&#24471;&#22343;&#22330;ODE&#65292;&#36825;&#26159;&#19968;&#31181;&#29305;&#23450;&#30340;Fisher-Rao&#26799;&#24230;&#27969;&#30340;&#36335;&#24452;&#12290;&#25105;&#20204;&#37319;&#29992;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26041;&#27861;&#26469;&#33719;&#24471;&#36895;&#24230;&#22330;&#30340;&#27850;&#26494;&#26041;&#31243;&#65292;&#36825;&#20351;&#24471;&#27850;&#26494;&#26041;&#31243;&#21487;&#22788;&#29702;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#31163;&#25955;&#21270;&#26377;&#38480;&#26679;&#26412;&#30340;&#32467;&#26524;&#22343;&#22330;ODE&#65292;&#24418;&#25104;&#19968;&#20010;&#31616;&#21333;&#30340;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#12290;&#22343;&#22330;ODE&#36824;&#21487;&#20197;&#36890;&#36807;&#31163;&#25955;&#26102;&#38388;&#35270;&#35282;&#20174;&#33945;&#26480;-&#23433;&#26222;&#23572;&#26041;&#31243;&#30340;&#36830;&#32493;&#32447;&#24615;&#21270;&#30340;&#26497;&#38480;&#20013;&#25512;&#23548;&#20986;&#26469;&#65292;&#36825;&#22312;&#19968;&#20010;&#24050;&#30693;&#30340;&#26694;&#26550;&#20869;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new mean-field ODE and corresponding interacting particle systems for sampling from an unnormalized target density or Bayesian posterior. The interacting particle systems are gradient-free, available in closed form, and only require the ability to sample from the reference density and compute the (unnormalized) target-to-reference density ratio. The mean-field ODE is obtained by solving a Poisson equation for a velocity field that transports samples along the geometric mixture of the two densities, which is the path of a particular Fisher-Rao gradient flow. We employ a reproducing kernel Hilbert space ansatz for the velocity field, which makes the Poisson equation tractable and enables us to discretize the resulting mean-field ODE over finite samples, as a simple interacting particle system. The mean-field ODE can be additionally be derived from a discrete-time perspective as the limit of successive linearizations of the Monge-Amp\`ere equations within a framework known 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#25968;&#25454;&#22810;&#26679;&#24615;&#27010;&#24565;&#26469;&#32479;&#19968;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#22522;&#20110;&#29256;&#26412;&#31354;&#38388;&#12289;&#27491;&#21017;&#21270;&#20248;&#21270;&#21644;&#21518;&#39564;&#37319;&#26679;&#30340;&#31639;&#27861;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#36798;&#21040;&#20102;&#21487;&#27604;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.03301</link><description>&lt;p&gt;
&#22312;&#26679;&#26412;&#39640;&#25928;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65306;&#25968;&#25454;&#22810;&#26679;&#24615;&#12289;&#21518;&#39564;&#37319;&#26679;&#65292;&#20197;&#21450;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling, and Beyond. (arXiv:2401.03301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#25968;&#25454;&#22810;&#26679;&#24615;&#27010;&#24565;&#26469;&#32479;&#19968;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#22522;&#20110;&#29256;&#26412;&#31354;&#38388;&#12289;&#27491;&#21017;&#21270;&#20248;&#21270;&#21644;&#21518;&#39564;&#37319;&#26679;&#30340;&#31639;&#27861;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#36798;&#21040;&#20102;&#21487;&#27604;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35797;&#22270;&#29702;&#35299;&#20160;&#20040;&#20419;&#36827;&#20102;&#23545;&#20110;&#24207;&#36125;&#21494;&#26031;&#20915;&#31574;&#30340;&#21382;&#21490;&#25968;&#25454;&#38598;&#36827;&#34892;&#26679;&#26412;&#39640;&#25928;&#23398;&#20064;&#65292;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#34987;&#31216;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#20110;&#22312;&#21033;&#29992;&#65288;&#20540;&#65289;&#20989;&#25968;&#36924;&#36817;&#30340;&#21516;&#26102;&#20139;&#21463;&#26679;&#26412;&#25928;&#29575;&#30340;&#31639;&#27861;&#24863;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#21253;&#25324;&#31163;&#32447;RL&#20013;&#35206;&#30422;&#24230;&#37327;&#30340;&#20808;&#21069;&#27010;&#24565;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#27010;&#24565;&#26469;&#35299;&#20915;&#36825;&#20123;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#19988;&#21033;&#29992;&#36825;&#20010;&#27010;&#24565;&#23558;&#22522;&#20110;&#29256;&#26412;&#31354;&#38388;&#65288;VS&#65289;&#12289;&#27491;&#21017;&#21270;&#20248;&#21270;&#65288;RO&#65289;&#21644;&#21518;&#39564;&#37319;&#26679;&#65288;PS&#65289;&#30340;&#19977;&#20010;&#19981;&#21516;&#31867;&#21035;&#30340;&#31163;&#32447;RL&#31639;&#27861;&#36827;&#34892;&#32479;&#19968;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#35777;&#26126;&#65292;&#22522;&#20110;VS&#12289;&#22522;&#20110;RO&#21644;&#22522;&#20110;PS&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;\emph{&#21487;&#27604;}&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#36825;&#24674;&#22797;&#20102;&#22312;&#26377;&#38480;&#21644;&#32447;&#24615;&#27169;&#22411;&#31867;&#21035;&#19979;&#30340;&#26368;&#20248;&#24615;&#30340;&#26631;&#20934;&#20551;&#35774;&#30340;&#36793;&#30028;&#12290;&#36825;&#20010;&#32467;&#26524;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#19981;&#20855;&#26377;&#26377;&#21033;&#24615;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We seek to understand what facilitates sample-efficient learning from historical datasets for sequential decision-making, a problem that is popularly known as offline reinforcement learning (RL). Further, we are interested in algorithms that enjoy sample efficiency while leveraging (value) function approximation. In this paper, we address these fundamental questions by (i) proposing a notion of data diversity that subsumes the previous notions of coverage measures in offline RL and (ii) using this notion to {unify} three distinct classes of offline RL algorithms based on version spaces (VS), regularized optimization (RO), and posterior sampling (PS). We establish that VS-based, RO-based, and PS-based algorithms, under standard assumptions, achieve \emph{comparable} sample efficiency, which recovers the state-of-the-art sub-optimality bounds for finite and linear model classes with the standard assumptions. This result is surprising, given that the prior work suggested an unfavorable sa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36712;&#36857;&#23548;&#21521;&#30340;&#31232;&#30095;&#22870;&#21169;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#31034;&#33539;&#36712;&#36857;&#65292;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#23454;&#29616;&#26356;&#24555;&#36895;&#12289;&#26356;&#39640;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.02225</link><description>&lt;p&gt;
&#22522;&#20110;&#36712;&#36857;&#23548;&#21521;&#30340;&#31232;&#30095;&#22870;&#21169;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Trajectory-Oriented Policy Optimization with Sparse Rewards. (arXiv:2401.02225v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02225
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36712;&#36857;&#23548;&#21521;&#30340;&#31232;&#30095;&#22870;&#21169;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#31034;&#33539;&#36712;&#36857;&#65292;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#23454;&#29616;&#26356;&#24555;&#36895;&#12289;&#26356;&#39640;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#22312;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#20013;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#20123;&#31232;&#30095;&#22870;&#21169;&#36890;&#24120;&#21482;&#34920;&#31034;&#20219;&#21153;&#26159;&#21542;&#37096;&#20998;&#25110;&#23436;&#20840;&#23436;&#25104;&#65292;&#36825;&#24847;&#21619;&#30528;&#22312;&#20195;&#29702;&#33719;&#24471;&#26377;&#29992;&#21453;&#39304;&#20043;&#21069;&#24517;&#39035;&#25191;&#34892;&#35768;&#22810;&#25506;&#32034;&#21160;&#20316;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;DRL&#31639;&#27861;&#26080;&#27861;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#23398;&#20064;&#21487;&#34892;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#31034;&#33539;&#36712;&#36857;&#30340;&#26041;&#27861;&#65292;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#36827;&#34892;&#26356;&#24555;&#36895;&#21644;&#26356;&#39640;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#36890;&#36807;&#23558;&#31163;&#32447;&#31034;&#33539;&#36712;&#36857;&#35270;&#20026;&#25351;&#23548;&#32780;&#19981;&#26159;&#27169;&#20223;&#23427;&#20204;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#19968;&#31181;&#20351;&#29366;&#24577;-&#21160;&#20316;&#35775;&#38382;&#20998;&#24067;&#19982;&#31163;&#32447;&#31034;&#33539;&#30456;&#21305;&#37197;&#30340;&#31574;&#30053;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;(MMD)&#30340;&#36712;&#36857;&#36317;&#31163;&#65292;&#24182;&#23558;&#31574;&#30053;&#20248;&#21270;&#24314;&#27169;&#20026;&#19968;&#20010;&#21463;&#36317;&#31163;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) remains challenging in tasks with sparse rewards. These sparse rewards often only indicate whether the task is partially or fully completed, meaning that many exploration actions must be performed before the agent obtains useful feedback. Hence, most existing DRL algorithms fail to learn feasible policies within a reasonable time frame. To overcome this problem, we develop an approach that exploits offline demonstration trajectories for faster and more efficient online RL in sparse reward settings. Our key insight is that by regarding offline demonstration trajectories as guidance, instead of imitating them, our method learns a policy whose state-action visitation marginal distribution matches that of offline demonstrations. Specifically, we introduce a novel trajectory distance based on maximum mean discrepancy (MMD) and formulate policy optimization as a distance-constrained optimization problem. Then, we show that this distance-constrained optimizat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#30028;&#25439;&#22833;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#21442;&#32771;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#33258;&#30001;&#21442;&#25968;&#35299;&#20915;&#20102;&#19968;&#20123;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#28789;&#27963;&#30340;&#20551;&#35774;&#20135;&#29983;&#20102;&#26032;&#30340;&#24191;&#20041;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2401.01148</link><description>&lt;p&gt;
&#26080;&#30028;&#25439;&#22833;&#30340;PAC-Bayes-Chernoff&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayes-Chernoff bounds for unbounded losses. (arXiv:2401.01148v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01148
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#30028;&#25439;&#22833;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#21442;&#32771;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#33258;&#30001;&#21442;&#25968;&#35299;&#20915;&#20102;&#19968;&#20123;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#28789;&#27963;&#30340;&#20551;&#35774;&#20135;&#29983;&#20102;&#26032;&#30340;&#24191;&#20041;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#26080;&#30028;&#25439;&#22833;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#21442;&#32771;&#30028;&#38480;&#12290;&#36825;&#20010;&#32467;&#26524;&#21487;&#20197;&#29702;&#35299;&#20026;Chernoff&#30028;&#38480;&#30340;PAC-Bayes&#29256;&#26412;&#12290;&#35777;&#26126;&#25216;&#24039;&#20381;&#36182;&#20110;&#36890;&#36807;Cram&#233;r&#21464;&#25442;&#23545;&#25439;&#22833;&#36827;&#34892;&#32479;&#19968;&#36793;&#30028;&#30340;&#23614;&#37096;&#38543;&#26426;&#21464;&#37327;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#25105;&#20204;&#20027;&#35201;&#32467;&#26524;&#30340;&#20004;&#20010;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30028;&#38480;&#35299;&#20915;&#20102;&#35768;&#22810;PAC-Bayes&#30028;&#38480;&#19978;&#30340;&#33258;&#30001;&#21442;&#25968;&#20248;&#21270;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#22312;&#25439;&#22833;&#20989;&#25968;&#19978;&#36827;&#34892;&#28789;&#27963;&#30340;&#20551;&#35774;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#24191;&#20041;&#20102;&#20043;&#21069;&#30340;&#30028;&#38480;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#26368;&#23567;&#21270;&#26469;&#33719;&#24471;&#31867;&#20284;Gibbs&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new high-probability PAC-Bayes oracle bound for unbounded losses. This result can be understood as a PAC-Bayes version of the Chernoff bound. The proof technique relies on uniformly bounding the tail of certain random variable based on the Cram\'er transform of the loss. We highlight two applications of our main result. First, we show that our bound solves the open problem of optimizing the free parameter on many PAC-Bayes bounds. Finally, we show that our approach allows working with flexible assumptions on the loss function, resulting in novel bounds that generalize previous ones and can be minimized to obtain Gibbs-like posteriors.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#35328;&#35821;&#19981;&#27969;&#30021;&#31243;&#24230;&#33258;&#21160;&#35843;&#25972;&#33647;&#29289;&#65292;&#36890;&#36807;&#23545;&#33647;&#29289;&#32452;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21270;&#65292;&#33021;&#22815;&#25910;&#25947;&#21040;&#33391;&#22909;&#30340;&#29992;&#33647;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2312.11509</link><description>&lt;p&gt;
&#38754;&#21521;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33647;&#29289;&#35843;&#25972;&#31995;&#32479;&#20197;&#20943;&#23569;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#35770;&#25991;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Toward A Reinforcement-Learning-Based System for Adjusting Medication to Minimize Speech Disfluency. (arXiv:2312.11509v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11509
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#35328;&#35821;&#19981;&#27969;&#30021;&#31243;&#24230;&#33258;&#21160;&#35843;&#25972;&#33647;&#29289;&#65292;&#36890;&#36807;&#23545;&#33647;&#29289;&#32452;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21270;&#65292;&#33021;&#22815;&#25910;&#25947;&#21040;&#33391;&#22909;&#30340;&#29992;&#33647;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#33258;&#21160;&#20026;&#24739;&#26377;&#19982;&#24515;&#29702;&#20581;&#24247;&#30456;&#20851;&#30340;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#34394;&#25311;&#24739;&#32773;&#24320;&#20855;&#33647;&#29289;&#22788;&#26041;&#65292;&#24182;&#26681;&#25454;&#38646;&#25104;&#26412;&#39057;&#32321;&#27979;&#37327;&#32467;&#26524;&#65292;&#35843;&#25972;&#33647;&#29289;&#21644;&#21058;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31995;&#32479;&#30340;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#19968;&#20010;&#22312;&#25105;&#20204;&#26500;&#24314;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#26816;&#27979;&#21644;&#35780;&#20272;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#27169;&#22359;&#65292;&#20197;&#21450;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#25214;&#21040;&#33391;&#22909;&#33647;&#29289;&#32452;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#20004;&#20010;&#27169;&#22359;&#65292;&#25105;&#20204;&#20174;&#25991;&#29486;&#20013;&#25910;&#38598;&#20102;&#20851;&#20110;&#33647;&#29289;&#27835;&#30103;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#25928;&#26524;&#30340;&#25968;&#25454;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#20449;&#30340;&#24739;&#32773;&#27169;&#25311;&#31995;&#32479;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#33021;&#22815;&#25910;&#25947;&#21040;&#19968;&#20010;&#33391;&#22909;&#30340;&#29992;&#33647;&#26041;&#26696;&#12290;&#25105;&#20204;&#25910;&#38598;&#24182;&#23545;&#21487;&#33021;&#23384;&#22312;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#20154;&#32676;&#36827;&#34892;&#20102;&#25968;&#25454;&#26631;&#27880;&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;:
&lt;/p&gt;
&lt;p&gt;
We propose a Reinforcement-Learning-based system that would automatically prescribe a hypothetical patient medication that may help the patient with their mental-health-related speech disfluency, and adjust the medication and the dosages in response to zero-cost frequent measurement of the fluency of the patient. We demonstrate the components of the system: a module that detects and evaluates speech disfluency on a large dataset we built, and a Reinforcement Learning algorithm that automatically finds good combinations of medications. To support the two modules, we collect data on the effect of psychiatric medications for speech disfluency from the literature, and build a plausible patient simulation system. We demonstrate that the Reinforcement Learning system is, under some circumstances, able to converge to a good medication regime. We collect and label a dataset of people with possible speech disfluency and demonstrate our methods using that dataset. Our work is a proof of concept:
&lt;/p&gt;</description></item><item><title>TiMix&#26159;&#19968;&#31181;&#23558;&#25991;&#26412;&#24863;&#30693;&#30340;&#22270;&#20687;&#28151;&#21512;&#25216;&#26415;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#28151;&#21512;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#24182;&#20174;&#20114;&#20449;&#24687;&#30340;&#35282;&#24230;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#24182;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.08846</link><description>&lt;p&gt;
TiMix: &#25991;&#26412;&#24863;&#30693;&#22270;&#20687;&#28151;&#21512;&#29992;&#20110;&#26377;&#25928;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TiMix: Text-aware Image Mixing for Effective Vision-Language Pre-training. (arXiv:2312.08846v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08846
&lt;/p&gt;
&lt;p&gt;
TiMix&#26159;&#19968;&#31181;&#23558;&#25991;&#26412;&#24863;&#30693;&#30340;&#22270;&#20687;&#28151;&#21512;&#25216;&#26415;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#28151;&#21512;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#24182;&#20174;&#20114;&#20449;&#24687;&#30340;&#35282;&#24230;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#24182;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#65288;SMCL&#65289;&#36890;&#36807;&#23545;&#40784;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#65292;&#26174;&#33879;&#25512;&#36827;&#20102;&#29616;&#20195;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32593;&#32476;&#25910;&#38598;&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#20013;&#23384;&#22312;&#22122;&#22768;&#65292;&#25193;&#22823;SMCL&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#38754;&#20020;&#30528;&#30456;&#24403;&#22823;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#25552;&#39640;VLP&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#26412;&#24863;&#30693;&#22270;&#20687;&#28151;&#21512;&#65288;TiMix&#65289;&#65292;&#23558;&#22522;&#20110;&#28151;&#21512;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#38598;&#25104;&#21040;SMCL&#20013;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#20174;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#30340;&#35282;&#24230;&#23545;TiMix&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#34920;&#26126;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#30340;&#28151;&#21512;&#25968;&#25454;&#26679;&#26412;&#38544;&#24335;&#22320;&#20316;&#20026;&#23545;&#27604;&#25439;&#22833;&#30340;&#27491;&#21017;&#21270;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#20351;&#29992;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;TiMix&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Multi-modal Contrastive Learning (SMCL) remarkably advances modern Vision-Language Pre-training (VLP) models by aligning visual and linguistic modalities. Due to noises in web-harvested text-image pairs, however, scaling up training data volume in SMCL presents considerable obstacles in terms of computational cost and data inefficiency. To improve data efficiency in VLP, we propose Text-aware Image Mixing (TiMix), which integrates mix-based data augmentation techniques into SMCL, yielding significant performance improvements without significantly increasing computational overhead. We provide a theoretical analysis of TiMixfrom a mutual information (MI) perspective, showing that mixed data samples for cross-modal contrastive learning implicitly serve as a regularizer for the contrastive loss. The experimental results demonstrate that TiMix exhibits a comparable performance on downstream tasks, even with a reduced amount of training data and shorter training time, when be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#31616;&#21333;&#30340;&#21151;&#29575;&#20998;&#26512;&#26041;&#27861;&#65292;&#22312;32&#20301;&#24494;&#25511;&#21046;&#22120;&#19978;&#25552;&#21462;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26550;&#26500;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2311.01344</link><description>&lt;p&gt;
&#29992;&#31616;&#21333;&#30340;&#21151;&#29575;&#20998;&#26512;&#22312;32&#20301;&#24494;&#25511;&#21046;&#22120;&#19978;&#38405;&#35835;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Like an Open Book? Read Neural Network Architecture with Simple Power Analysis on 32-bit Microcontrollers. (arXiv:2311.01344v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#31616;&#21333;&#30340;&#21151;&#29575;&#20998;&#26512;&#26041;&#27861;&#65292;&#22312;32&#20301;&#24494;&#25511;&#21046;&#22120;&#19978;&#25552;&#21462;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26550;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#25552;&#21462;&#26159;AI&#31995;&#32479;&#23433;&#20840;&#30340;&#19968;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#20851;&#27880;&#28857;&#12290;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#35828;&#65292;&#26550;&#26500;&#26159;&#23545;&#25163;&#35797;&#22270;&#24674;&#22797;&#30340;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#20316;&#20026;&#19968;&#31995;&#21015;&#37325;&#22797;&#35745;&#31639;&#22359;&#65292;&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23558;&#20135;&#29983;&#29420;&#29305;&#30340;&#20391;&#20449;&#36947;&#27844;&#38706;&#12290;&#24403;&#30446;&#26631;&#24179;&#21488;&#22312;&#29289;&#29702;&#19978;&#21487;&#35775;&#38382;&#26102;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#20449;&#36947;&#27844;&#38706;&#26469;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#12290;&#36890;&#36807;&#32467;&#21512;&#23545;&#28145;&#24230;&#23398;&#20064;&#23454;&#36341;&#30340;&#29702;&#35770;&#30693;&#35782;&#21644;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;&#23454;&#29616;&#24211;&#65288;ARM CMSIS-NN&#65289;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#30340;&#30446;&#30340;&#26159;&#22238;&#31572;&#36825;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#36890;&#36807;&#31616;&#21333;&#22320;&#26816;&#26597;&#19968;&#20010;EM&#20391;&#20449;&#36947;&#36319;&#36394;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#21462;&#22810;&#36828;&#30340;&#26550;&#26500;&#20449;&#24687;&#65311;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20256;&#32479;MLP&#21644;CNN&#27169;&#22411;&#30340;&#25552;&#21462;&#26041;&#27861;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#39640;&#31471;32&#20301;&#24494;&#25511;&#21046;&#22120;&#65288;Cortex-M7&#65289;&#19978;&#36816;&#34892;&#65292;&#20165;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#27169;&#24335;&#35782;&#21035;&#20998;&#26512;&#12290;&#23613;&#31649;&#23384;&#22312;&#20960;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#65292;&#20294;&#25105;&#20204;&#22768;&#31216;&#65292;&#19982;&#21442;&#25968;&#25552;&#21462;&#30456;&#21453;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#20174;&#31616;&#21333;&#30340;&#21151;&#29575;&#20998;&#26512;&#20013;&#25552;&#21462;&#20986;&#26550;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model extraction is a growing concern for the security of AI systems. For deep neural network models, the architecture is the most important information an adversary aims to recover. Being a sequence of repeated computation blocks, neural network models deployed on edge-devices will generate distinctive side-channel leakages. The latter can be exploited to extract critical information when targeted platforms are physically accessible. By combining theoretical knowledge about deep learning practices and analysis of a widespread implementation library (ARM CMSIS-NN), our purpose is to answer this critical question: how far can we extract architecture information by simply examining an EM side-channel trace? For the first time, we propose an extraction methodology for traditional MLP and CNN models running on a high-end 32-bit microcontroller (Cortex-M7) that relies only on simple pattern recognition analysis. Despite few challenging cases, we claim that, contrary to parameters extraction
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#20026;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#25512;&#23548;&#20986;&#39640;&#36136;&#37327;&#30340;&#36890;&#29992;&#20020;&#24202;&#29305;&#24449;&#23884;&#20837;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#38477;&#32500;&#25216;&#26415;&#21487;&#35270;&#21270;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#65292;&#24182;&#22312;MIMIC-III&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00768</link><description>&lt;p&gt;
&#29992;&#20110;&#20020;&#24202;&#29305;&#24449;&#23884;&#20837;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Language Model Training Paradigms for Clinical Feature Embeddings. (arXiv:2311.00768v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#20026;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#25512;&#23548;&#20986;&#39640;&#36136;&#37327;&#30340;&#36890;&#29992;&#20020;&#24202;&#29305;&#24449;&#23884;&#20837;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#38477;&#32500;&#25216;&#26415;&#21487;&#35270;&#21270;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#65292;&#24182;&#22312;MIMIC-III&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#34920;&#31034;&#23398;&#20064;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23545;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#25512;&#23548;&#20986;&#20020;&#24202;&#29305;&#24449;&#65288;&#22914;&#24515;&#29575;&#21644;&#34880;&#21387;&#65289;&#30340;&#36890;&#29992;&#23884;&#20837;&#12290;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#65292;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#20020;&#24202;&#29305;&#24449;&#23884;&#20837;&#65292;&#23454;&#29616;&#27604;&#29616;&#26377;&#30340;&#26102;&#38388;&#27493;&#21644;&#24739;&#32773;&#32423;&#21035;&#34920;&#31034;&#23398;&#20064;&#26356;&#32454;&#31890;&#24230;&#30340;&#34920;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#38477;&#32500;&#25216;&#26415;&#21487;&#35270;&#21270;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#65292;&#24182;&#35266;&#23519;&#21040;&#19982;&#20808;&#21069;&#30340;&#20020;&#24202;&#30693;&#35782;&#39640;&#24230;&#19968;&#33268;&#12290;&#25105;&#20204;&#36824;&#22312;MIMIC-III&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#20020;&#24202;&#29305;&#24449;&#23884;&#20837;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#21457;&#24067;&#22312;&#32593;&#19978;&#20197;&#20379;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In research areas with scarce data, representation learning plays a significant role. This work aims to enhance representation learning for clinical time series by deriving universal embeddings for clinical features, such as heart rate and blood pressure. We use self-supervised training paradigms for language models to learn high-quality clinical feature embeddings, achieving a finer granularity than existing time-step and patient-level representation learning. We visualize the learnt embeddings via unsupervised dimension reduction techniques and observe a high degree of consistency with prior clinical knowledge. We also evaluate the model performance on the MIMIC-III benchmark and demonstrate the effectiveness of using clinical feature embeddings. We publish our code online for replication.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23545;&#28041;&#21450;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#20559;&#35265;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#20849;&#28085;&#30422;&#20102;&#20845;&#31181;&#20027;&#35201;&#30340;&#20559;&#35265;&#31867;&#22411;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#20559;&#35265;&#22788;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.19917</link><description>&lt;p&gt;
&#25581;&#31034;&#20559;&#35265;&#21644;&#19981;&#24179;&#31561;&#65306;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#20013;&#20559;&#35265;&#26816;&#27979;&#21644;&#32531;&#35299;&#30340;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unmasking Bias and Inequities: A Systematic Review of Bias Detection and Mitigation in Healthcare Artificial Intelligence Using Electronic Health Records. (arXiv:2310.19917v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;&#28041;&#21450;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#20559;&#35265;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#20849;&#28085;&#30422;&#20102;&#20845;&#31181;&#20027;&#35201;&#30340;&#20559;&#35265;&#31867;&#22411;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#20559;&#35265;&#22788;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#22312;&#21307;&#30103;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#21040;&#27426;&#36814;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#20559;&#35265;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#31995;&#32479;&#32508;&#36848;&#28041;&#21450;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#20559;&#35265;&#12290;&#26041;&#27861;&#65306;&#36981;&#24490;Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA)&#20934;&#21017;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#12290;&#20174;PubMed&#12289;Web of Science&#21644;&#30005;&#27668;&#21644;&#30005;&#23376;&#24037;&#31243;&#24072;&#23398;&#20250;&#20013;&#26816;&#32034;&#20102;2010&#24180;1&#26376;1&#26085;&#33267;2022&#24180;10&#26376;31&#26085;&#26399;&#38388;&#21457;&#34920;&#30340;&#25991;&#31456;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#20845;&#31181;&#20027;&#35201;&#30340;&#20559;&#35265;&#31867;&#22411;&#65292;&#24182;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#20559;&#35265;&#22788;&#29702;&#26041;&#27861;&#12290;&#32467;&#26524;&#65306;&#22312;&#26816;&#32034;&#21040;&#30340;252&#31687;&#25991;&#31456;&#20013;&#65292;&#26377;20&#31687;&#31526;&#21512;&#26368;&#32456;&#32508;&#36848;&#30340;&#32435;&#20837;&#26631;&#20934;&#12290;&#26412;&#32508;&#36848;&#28085;&#30422;&#20102;&#20845;&#31181;&#20559;&#35265;&#20013;&#30340;&#20116;&#31181;&#65306;&#20843;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#36873;&#25321;&#20559;&#35265;&#65307;&#20845;&#39033;&#30740;&#31350;&#38024;&#23545;&#38544;&#24615;&#20559;&#35265;&#65307;&#20116;&#39033;&#30740;&#31350;&#23545;&#28151;&#26434;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#65307;&#22235;&#39033;&#30740;&#31350;&#23545;&#27979;&#37327;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#65307;&#20004;&#39033;&#30740;&#31350;&#23545;&#31639;&#27861;&#20559;&#35265;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#22312;&#20559;&#35265;&#22788;&#29702;&#26041;&#27861;&#26041;&#38754;&#65292;&#26377;&#21313;&#39033;&#30740;&#31350;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objectives: Artificial intelligence (AI) applications utilizing electronic health records (EHRs) have gained popularity, but they also introduce various types of bias. This study aims to systematically review the literature that address bias in AI research utilizing EHR data. Methods: A systematic review was conducted following the Preferred Reporting Items for Systematic Reviews and Meta-analyses (PRISMA) guideline. We retrieved articles published between January 1, 2010, and October 31, 2022, from PubMed, Web of Science, and the Institute of Electrical and Electronics Engineers. We defined six major types of bias and summarized the existing approaches in bias handling. Results: Out of the 252 retrieved articles, 20 met the inclusion criteria for the final review. Five out of six bias were covered in this review: eight studies analyzed selection bias; six on implicit bias; five on confounding bias; four on measurement bias; two on algorithmic bias. For bias handling approaches, ten st
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#30495;&#23454;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#29305;&#24449;&#21028;&#26029;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18168</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Personas as a Way to Model Truthfulness in Language Models. (arXiv:2310.18168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#30495;&#23454;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#29305;&#24449;&#21028;&#26029;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#20114;&#32852;&#32593;&#19978;&#30340;&#22823;&#37327;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#25991;&#26412;&#20013;&#26082;&#21253;&#21547;&#20102;&#20107;&#23454;&#65292;&#20063;&#21253;&#21547;&#20102;&#35823;&#23548;&#24615;&#30340;&#20449;&#24687;&#12290;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20174;&#36825;&#20123;&#30456;&#20114;&#30683;&#30462;&#30340;&#25968;&#25454;&#20013;&#36776;&#21035;&#30495;&#23454;&#19982;&#34394;&#20551;&#21527;&#65311;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24314;&#27169;&#19981;&#21516;&#20135;&#29983;&#25991;&#26412;&#30340;&#20010;&#20307;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#20551;&#35774;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#26469;&#32858;&#31867;&#30495;&#23454;&#25991;&#26412;&#65306;&#19968;&#32676;&#24456;&#21487;&#33021;&#20135;&#29983;&#30495;&#23454;&#25991;&#26412;&#24182;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#20010;&#20307;&#12290;&#20363;&#22914;&#65292;&#21487;&#20449;&#28304;&#22914;&#32500;&#22522;&#30334;&#31185;&#21644;&#31185;&#23398;&#26399;&#21002;&#36890;&#24120;&#20351;&#29992;&#27491;&#24335;&#30340;&#20889;&#20316;&#39118;&#26684;&#24182;&#25552;&#20986;&#19968;&#33268;&#30340;&#20027;&#24352;&#12290;&#36890;&#36807;&#24314;&#27169;&#36825;&#19968;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#27599;&#20010;&#20010;&#20307;&#29983;&#25104;&#35757;&#32451;&#25991;&#26412;&#30340;&#29305;&#23450;&#19978;&#19979;&#25991;&#20043;&#22806;&#12290;&#20363;&#22914;&#65292;&#27169;&#22411;&#21487;&#20197;&#25512;&#26029;&#20986;&#8220;&#32500;&#22522;&#30334;&#31185;&#8221;&#36825;&#20010;&#20010;&#20307;&#22312;&#8220;&#31185;&#23398;&#8221;&#29983;&#25104;&#30340;&#20027;&#39064;&#19978;&#20250;&#34920;&#29616;&#20986;&#30495;&#23454;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20849;&#20139;&#19968;&#20010;&#20154;&#35774;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20004;&#20010;&#35266;&#23519;&#32467;&#26524;&#20026;&#20154;&#35774;&#20551;&#35774;&#25552;&#20379;&#20102;&#35777;&#25454;&#65306;&#65288;1&#65289;&#25105;&#20204;&#21487;&#20197;&#25506;&#27979;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#21028;&#26029;&#30495;&#23454;&#24615;&#30340;&#33021;&#21147;&#65307;&#65288;2&#65289;&#27169;&#22411;&#21487;&#20197;&#20174;&#30456;&#20851;&#29305;&#24449;&#20013;&#25512;&#27979;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? Expanding on the view that LLMs can model different agents producing the corpora, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. For example, trustworthy sources like Wikipedia and Science usually use formal writing styles and make consistent claims. By modeling this persona, LLMs can generalize truthfulness beyond the specific contexts in which each agent generated the training text. For example, the model can infer that the agent "Wikipedia" will behave truthfully on topics that were only generated by "Science" because they share a persona. We first show evidence for the persona hypothesis via two observations: (1) we can probe whether a mod
&lt;/p&gt;</description></item><item><title>LASER&#26159;&#19968;&#31181;&#26032;&#30340;&#21387;&#32553;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#26799;&#24230;&#30340;&#20302;&#31209;&#32467;&#26500;&#22312;&#22122;&#22768;&#36890;&#36947;&#19978;&#39640;&#25928;&#20256;&#36755;&#26799;&#24230;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#26696;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;GPT&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#25345;&#32493;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.13033</link><description>&lt;p&gt;
LASER&#65306;&#26080;&#32447;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#30340;&#32447;&#24615;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
LASER: Linear Compression in Wireless Distributed Optimization. (arXiv:2310.13033v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13033
&lt;/p&gt;
&lt;p&gt;
LASER&#26159;&#19968;&#31181;&#26032;&#30340;&#21387;&#32553;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#26799;&#24230;&#30340;&#20302;&#31209;&#32467;&#26500;&#22312;&#22122;&#22768;&#36890;&#36947;&#19978;&#39640;&#25928;&#20256;&#36755;&#26799;&#24230;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#26696;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;GPT&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#25345;&#32493;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24182;&#34892;SGD&#26159;&#20998;&#24067;&#24335;&#20248;&#21270;&#30340;&#20107;&#23454;&#19978;&#30340;&#31639;&#27861;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#12290;&#23613;&#31649;&#23427;&#26377;&#24456;&#22810;&#20248;&#28857;&#65292;&#20294;&#36890;&#20449;&#29942;&#39048;&#26159;&#20854;&#20013;&#25345;&#20037;&#23384;&#22312;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#22823;&#22810;&#25968;&#21387;&#32553;&#26041;&#26696;&#35201;&#20040;&#20551;&#35774;&#36890;&#20449;&#38142;&#36335;&#26080;&#22122;&#22768;&#65292;&#35201;&#20040;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#26080;&#27861;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#20171;&#32461;&#20102;LASER&#65306;&#26080;&#32447;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#30340;&#32447;&#24615;&#21387;&#32553;&#12290;LASER&#21033;&#29992;&#26799;&#24230;&#30340;&#22266;&#26377;&#20302;&#31209;&#32467;&#26500;&#65292;&#22312;&#22122;&#22768;&#36890;&#36947;&#19978;&#39640;&#25928;&#20256;&#36755;&#26799;&#24230;&#12290;&#23613;&#31649;&#20139;&#21463;&#19982;&#32463;&#20856;SGD&#30456;&#20284;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;LASER&#22312;&#21508;&#31181;&#23454;&#38469;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#25345;&#32493;&#30340;&#20248;&#21183;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;GPT&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#65292;&#23427;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;&#26041;&#26696;&#12290;&#22312;&#21518;&#32773;&#20013;&#65292;&#25105;&#20204;&#30456;&#23545;&#20110;&#22122;&#22768;&#36890;&#36947;&#19978;&#30340;&#22522;&#20934;&#27169;&#22411;&#22312;&#22256;&#24785;&#24230;&#19978;&#33719;&#24471;&#20102;50-64%&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-parallel SGD is the de facto algorithm for distributed optimization, especially for large scale machine learning. Despite its merits, communication bottleneck is one of its persistent issues. Most compression schemes to alleviate this either assume noiseless communication links, or fail to achieve good performance on practical tasks. In this paper, we close this gap and introduce LASER: LineAr CompreSsion in WirEless DistRibuted Optimization. LASER capitalizes on the inherent low-rank structure of gradients and transmits them efficiently over the noisy channels. Whilst enjoying theoretical guarantees similar to those of the classical SGD, LASER shows consistent gains over baselines on a variety of practical benchmarks. In particular, it outperforms the state-of-the-art compression schemes on challenging computer vision and GPT language modeling tasks. On the latter, we obtain $50$-$64 \%$ improvement in perplexity over our baselines for noisy channels.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26641;&#32467;&#26500;&#19978;&#21033;&#29992;Wasserstein&#36317;&#31163;&#36827;&#34892;&#31616;&#21270;&#34920;&#31034;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SimCLR&#21644;&#36127;TWD&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#20272;&#35745;&#31616;&#21270;&#34920;&#31034;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25214;&#21040;&#20102;&#31283;&#23450;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.10143</link><description>&lt;p&gt;
&#29992;Wasserstein&#36317;&#31163;&#36827;&#34892;&#31616;&#21270;&#34920;&#31034;&#23398;&#20064;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Simplicial Representation Learning with Wasserstein Distance. (arXiv:2310.10143v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26641;&#32467;&#26500;&#19978;&#21033;&#29992;Wasserstein&#36317;&#31163;&#36827;&#34892;&#31616;&#21270;&#34920;&#31034;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SimCLR&#21644;&#36127;TWD&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#20272;&#35745;&#31616;&#21270;&#34920;&#31034;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25214;&#21040;&#20102;&#31283;&#23450;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#26641;&#32467;&#26500;&#19978;&#21033;&#29992;1-Wasserstein&#36317;&#31163;&#36827;&#34892;&#31616;&#21270;&#34920;&#31034;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#26641;- Wasserstein&#36317;&#31163;(TWD)&#23450;&#20041;&#20026;&#20004;&#20010;&#26641;&#23884;&#20837;&#21521;&#37327;&#20043;&#38388;&#30340;L1&#36317;&#31163;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#22522;&#20110;SimCLR&#21644;&#36127;TWD&#20316;&#20026;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#20272;&#35745;&#31616;&#21270;&#34920;&#31034;&#12290;&#22312;SimCLR&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#19982;&#23454;&#21521;&#37327;&#23884;&#20837;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#65292;&#20294;&#26159;&#23578;&#26410;&#23545;&#21033;&#29992;L1&#36317;&#31163;&#19982;&#31616;&#21270;&#23884;&#20837;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#12290;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#35757;&#32451;L1&#36317;&#31163;&#22312;&#25968;&#20540;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#24448;&#24448;&#20250;&#20135;&#29983;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#27010;&#29575;&#27169;&#22411;&#30340;&#36873;&#25321;&#20063;&#26377;&#24456;&#22810;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#20174;&#23454;&#35777;&#35282;&#24230;&#25506;&#31350;&#20102;&#29992;TWD&#20248;&#21270;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#31574;&#30053;&#65292;&#24182;&#25214;&#21040;&#20102;&#31283;&#23450;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#31181;&#31867;&#22411;TWD&#30340;&#32452;&#21512;&#65288;&#24635; ...
&lt;/p&gt;
&lt;p&gt;
In this paper, we delve into the problem of simplicial representation learning utilizing the 1-Wasserstein distance on a tree structure (a.k.a., Tree-Wasserstein distance (TWD)), where TWD is defined as the L1 distance between two tree-embedded vectors. Specifically, we consider a framework for simplicial representation estimation employing a self-supervised learning approach based on SimCLR with a negative TWD as a similarity measure. In SimCLR, the cosine similarity with real-vector embeddings is often utilized; however, it has not been well studied utilizing L1-based measures with simplicial embeddings. A key challenge is that training the L1 distance is numerically challenging and often yields unsatisfactory outcomes, and there are numerous choices for probability models. Thus, this study empirically investigates a strategy for optimizing self-supervised learning with TWD and find a stable training procedure. More specifically, we evaluate the combination of two types of TWD (total
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39640;&#32500;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#25351;&#25968;&#26426;&#21046;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#24182;&#25552;&#20986;&#20102;Metropolis-Hastings&#31639;&#27861;&#26469;&#20811;&#26381;&#25351;&#25968;&#25628;&#32034;&#31354;&#38388;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#23450;&#36793;&#30028;&#26465;&#20214;&#19979;&#33021;&#22815;&#23454;&#29616;&#24378;&#27169;&#22411;&#24674;&#22797;&#24615;&#36136;&#65292;&#24182;&#20855;&#26377;&#22810;&#39033;&#24335;&#28151;&#21512;&#26102;&#38388;&#21644;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2310.07852</link><description>&lt;p&gt;
&#20851;&#20110;&#36890;&#36807;&#25351;&#25968;&#26426;&#21046;&#36827;&#34892;&#39640;&#32500;&#31169;&#26377;&#27169;&#22411;&#36873;&#25321;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Computational Complexity of Private High-dimensional Model Selection via the Exponential Mechanism. (arXiv:2310.07852v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39640;&#32500;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#25351;&#25968;&#26426;&#21046;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#24182;&#25552;&#20986;&#20102;Metropolis-Hastings&#31639;&#27861;&#26469;&#20811;&#26381;&#25351;&#25968;&#25628;&#32034;&#31354;&#38388;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#23450;&#36793;&#30028;&#26465;&#20214;&#19979;&#33021;&#22815;&#23454;&#29616;&#24378;&#27169;&#22411;&#24674;&#22797;&#24615;&#36136;&#65292;&#24182;&#20855;&#26377;&#22810;&#39033;&#24335;&#28151;&#21512;&#26102;&#38388;&#21644;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#39640;&#32500;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#24046;&#20998;&#38544;&#31169;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#25928;&#29992;&#20445;&#35777;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#24191;&#20026;&#20154;&#30693;&#30340;&#25351;&#25968;&#26426;&#21046;&#26469;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#65292;&#24182;&#22312;&#19968;&#23450;&#36793;&#30028;&#26465;&#20214;&#19979;&#65292;&#24314;&#31435;&#20102;&#20854;&#24378;&#27169;&#22411;&#24674;&#22797;&#24615;&#36136;&#12290;&#28982;&#32780;&#65292;&#25351;&#25968;&#26426;&#21046;&#30340;&#25351;&#25968;&#25628;&#32034;&#31354;&#38388;&#23548;&#33268;&#20102;&#20005;&#37325;&#30340;&#35745;&#31639;&#29942;&#39048;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Metropolis-Hastings&#31639;&#27861;&#26469;&#36827;&#34892;&#37319;&#26679;&#27493;&#39588;&#65292;&#24182;&#22312;&#38382;&#39064;&#21442;&#25968;$n$&#12289;$p$&#21644;$s$&#20013;&#24314;&#31435;&#20102;&#20854;&#21040;&#31283;&#24577;&#20998;&#24067;&#30340;&#22810;&#39033;&#24335;&#28151;&#21512;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#20854;&#28151;&#21512;&#24615;&#36136;&#24314;&#31435;&#20102;Metropolis-Hastings&#38543;&#26426;&#34892;&#36208;&#30340;&#26368;&#32456;&#20272;&#35745;&#30340;&#36817;&#20284;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#20123;&#35828;&#26126;&#24615;&#27169;&#25311;&#65292;&#21360;&#35777;&#20102;&#25105;&#20204;&#20027;&#35201;&#32467;&#26524;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of model selection in a high-dimensional sparse linear regression model under the differential privacy framework. In particular, we consider the problem of differentially private best subset selection and study its utility guarantee. We adopt the well-known exponential mechanism for selecting the best model, and under a certain margin condition, we establish its strong model recovery property. However, the exponential search space of the exponential mechanism poses a serious computational bottleneck. To overcome this challenge, we propose a Metropolis-Hastings algorithm for the sampling step and establish its polynomial mixing time to its stationary distribution in the problem parameters $n,p$, and $s$. Furthermore, we also establish approximate differential privacy for the final estimates of the Metropolis-Hastings random walk using its mixing property. Finally, we also perform some illustrative simulations that echo the theoretical findings of our main results
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#25552;&#31034;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#28508;&#22312;&#21521;&#37327;&#12289;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#21644;&#24615;&#36136;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#20998;&#23376;&#36827;&#34892;&#35757;&#32451;&#21518;&#36827;&#34892;&#27169;&#22411;&#20998;&#24067;&#30340;&#36880;&#28176;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2310.03253</link><description>&lt;p&gt;
&#28508;&#22312;&#25552;&#31034;Transformer&#27169;&#22411;&#22312;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Molecule Design by Latent Prompt Transformer. (arXiv:2310.03253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28508;&#22312;&#25552;&#31034;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#20998;&#23376;&#35774;&#35745;&#20013;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#28508;&#22312;&#21521;&#37327;&#12289;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#21644;&#24615;&#36136;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#20998;&#23376;&#36827;&#34892;&#35757;&#32451;&#21518;&#36827;&#34892;&#27169;&#22411;&#20998;&#24067;&#30340;&#36880;&#28176;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#20998;&#23376;&#35774;&#35745;&#31561;&#20855;&#26377;&#25361;&#25112;&#24615;&#20248;&#21270;&#38382;&#39064;&#30340;&#28508;&#22312;&#25552;&#31034;Transformer&#27169;&#22411;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#25214;&#21040;&#20855;&#26377;&#30446;&#26631;&#21270;&#23398;&#25110;&#29983;&#29289;&#24615;&#36136;&#26368;&#20248;&#20540;&#30340;&#20998;&#23376;&#65292;&#35813;&#20540;&#21487;&#20197;&#30001;&#29616;&#26377;&#36719;&#20214;&#35745;&#31639;&#24471;&#20986;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#21253;&#25324;&#19977;&#20010;&#32452;&#20214;&#65306;&#65288;1&#65289;&#28508;&#22312;&#21521;&#37327;&#65292;&#20854;&#20808;&#39564;&#20998;&#24067;&#30001;&#39640;&#26031;&#30333;&#22122;&#22768;&#21521;&#37327;&#30340;Unet&#21464;&#25442;&#24314;&#27169;&#12290;&#65288;2&#65289;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#65288;1&#65289;&#20013;&#32473;&#23450;&#28508;&#22312;&#21521;&#37327;&#30340;&#26465;&#20214;&#19979;&#29983;&#25104;&#22522;&#20110;&#23383;&#31526;&#20018;&#30340;&#20998;&#23376;&#34920;&#31034;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20197;&#65288;1&#65289;&#20013;&#30340;&#28508;&#22312;&#21521;&#37327;&#20316;&#20026;&#25552;&#31034;&#30340;&#22240;&#26524;Transformer&#27169;&#22411;&#12290;&#65288;3&#65289;&#24615;&#36136;&#39044;&#27979;&#27169;&#22411;&#65292;&#26681;&#25454;&#65288;1&#65289;&#20013;&#30340;&#28508;&#22312;&#21521;&#37327;&#36827;&#34892;&#38750;&#32447;&#24615;&#22238;&#24402;&#39044;&#27979;&#20998;&#23376;&#30340;&#30446;&#26631;&#24615;&#36136;&#20540;&#12290;&#25105;&#20204;&#31216;&#35813;&#25552;&#20986;&#30340;&#27169;&#22411;&#20026;&#28508;&#22312;&#25552;&#31034;Transformer&#27169;&#22411;&#12290;&#22312;&#23545;&#29616;&#26377;&#20998;&#23376;&#21450;&#20854;&#24615;&#36136;&#20540;&#36827;&#34892;&#21021;&#27493;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#36880;&#28176;&#36716;&#31227;&#27169;&#22411;&#20998;&#24067;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a latent prompt Transformer model for solving challenging optimization problems such as molecule design, where the goal is to find molecules with optimal values of a target chemical or biological property that can be computed by an existing software. Our proposed model consists of three components. (1) A latent vector whose prior distribution is modeled by a Unet transformation of a Gaussian white noise vector. (2) A molecule generation model that generates the string-based representation of molecule conditional on the latent vector in (1). We adopt the causal Transformer model that takes the latent vector in (1) as prompt. (3) A property prediction model that predicts the value of the target property of a molecule based on a non-linear regression on the latent vector in (1). We call the proposed model the latent prompt Transformer model. After initial training of the model on existing molecules and their property values, we then gradually shift the model distributi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#23398;&#20064;&#23454;&#29616;&#30446;&#26631;&#36798;&#25104;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#21021;&#22987;&#29366;&#24577;&#19979;&#20174;&#39044;&#23450;&#20041;&#25110;&#26032;&#30446;&#26631;&#36798;&#25104;&#65292;&#32780;&#26080;&#38656;&#23398;&#20064;&#21333;&#29420;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.02505</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#23398;&#20064;&#23454;&#29616;&#30446;&#26631;&#36798;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learning to Reach Goals via Diffusion. (arXiv:2310.02505v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#23398;&#20064;&#23454;&#29616;&#30446;&#26631;&#36798;&#25104;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#21021;&#22987;&#29366;&#24577;&#19979;&#20174;&#39044;&#23450;&#20041;&#25110;&#26032;&#30446;&#26631;&#36798;&#25104;&#65292;&#32780;&#26080;&#38656;&#23398;&#20064;&#21333;&#29420;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31867;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#36845;&#20195;&#21435;&#22122;&#23558;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#38543;&#26426;&#22122;&#22768;&#26144;&#23556;&#21040;&#30446;&#26631;&#27969;&#24418;&#19978;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#25918;&#22312;&#25193;&#25955;&#24314;&#27169;&#30340;&#32972;&#26223;&#19979;&#65292;&#32473;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#12290;&#31867;&#20284;&#20110;&#25193;&#25955;&#36807;&#31243;&#65292;&#20854;&#20013;&#21033;&#29992;&#39640;&#26031;&#22122;&#22768;&#21019;&#24314;&#38543;&#26426;&#36712;&#36857;&#65292;&#20351;&#20854;&#36828;&#31163;&#25968;&#25454;&#27969;&#24418;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#36828;&#31163;&#28508;&#22312;&#30446;&#26631;&#29366;&#24577;&#30340;&#36712;&#36857;&#12290;&#28982;&#21518;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#31867;&#20284;&#20110;&#35780;&#20998;&#20989;&#25968;&#30340;&#30446;&#26631;&#26465;&#20214;&#31574;&#30053;&#12290;&#36825;&#20010;&#31216;&#20026;Merlin&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20219;&#24847;&#21021;&#22987;&#29366;&#24577;&#19979;&#20174;&#39044;&#23450;&#20041;&#25110;&#26032;&#30446;&#26631;&#36798;&#25104;&#65292;&#32780;&#26080;&#38656;&#23398;&#20064;&#21333;&#29420;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#36873;&#25321;&#65292;&#29992;&#20110;&#21462;&#20195;&#25193;&#25955;&#20013;&#30340;&#39640;&#26031;&#22122;&#22768;&#27169;&#22411; - &#32531;&#20914;&#21306;&#20013;&#30340;&#21453;&#21521;&#25773;&#25918;&#65292;&#21453;&#21521;&#21160;&#21147;&#23398;&#27169;&#22411;&#21644;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#31163;&#32447;&#30446;&#26631;&#36798;&#25104;&#20219;&#21153;&#19978;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks.
&lt;/p&gt;</description></item><item><title>OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02031</link><description>&lt;p&gt;
OceanGPT&#65306;&#29992;&#20110;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02031
&lt;/p&gt;
&lt;p&gt;
OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#31185;&#23398;&#26159;&#25506;&#32034;&#20805;&#28385;&#29983;&#21629;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#30340;&#28023;&#27915;&#30340;&#31185;&#23398;&#65292;&#32771;&#34385;&#21040;&#28023;&#27915;&#35206;&#30422;&#20102;&#22320;&#29699;&#34920;&#38754;&#30340;70&#65285;&#20197;&#19978;&#65292;&#36825;&#19968;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#25913;&#21464;&#20102;&#31185;&#23398;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;&#22312;&#20854;&#20182;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;LLM&#36890;&#24120;&#26080;&#27861;&#28385;&#36275;&#28023;&#27915;&#23398;&#23478;&#31561;&#39046;&#22495;&#19987;&#23478;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#23545;LLM&#22312;&#28023;&#27915;&#31185;&#23398;&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#36825;&#20854;&#20013;&#30340;&#26681;&#26412;&#21407;&#22240;&#21487;&#33021;&#26159;&#28023;&#27915;&#25968;&#25454;&#30340;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#24615;&#36136;&#65292;&#20197;&#21450;&#23545;&#26356;&#39640;&#30340;&#31890;&#24230;&#21644;&#20016;&#23500;&#30340;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#39318;&#20010;&#28023;&#27915;&#39046;&#22495;&#30340;LLM&#8212;&#8212;OceanGPT&#65292;&#35813;&#27169;&#22411;&#25797;&#38271;&#21508;&#31181;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;DoInstruct&#65292;&#29992;&#20110;&#33258;&#21160;&#33719;&#21462;&#22823;&#37327;&#30340;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#65292;&#23427;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#29983;&#25104;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#23548;&#21521;&#30340;&#26041;&#27861;&#29992;&#20110;&#36817;&#20284;&#35745;&#31639;&#20219;&#24847;OT&#25104;&#26412;&#20989;&#25968;&#30340;&#36830;&#32493;&#29109;OT&#24052;&#27663;&#20013;&#24515;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#19982;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#23398;&#20064;&#36807;&#31243;&#26080;&#32541;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2310.01105</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#23548;&#21521;&#30340;&#36830;&#32493;&#29109;&#24052;&#27663;&#20013;&#24515;&#20272;&#35745;&#26041;&#27861;&#21450;&#20854;&#22312;&#19968;&#33324;&#25104;&#26412;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Energy-Guided Continuous Entropic Barycenter Estimation for General Costs. (arXiv:2310.01105v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#23548;&#21521;&#30340;&#26041;&#27861;&#29992;&#20110;&#36817;&#20284;&#35745;&#31639;&#20219;&#24847;OT&#25104;&#26412;&#20989;&#25968;&#30340;&#36830;&#32493;&#29109;OT&#24052;&#27663;&#20013;&#24515;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#19982;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#23398;&#20064;&#36807;&#31243;&#26080;&#32541;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#36755;&#36816;&#65288;OT&#65289;&#24052;&#27663;&#20013;&#24515;&#26159;&#19968;&#31181;&#22312;&#25429;&#25417;&#27010;&#29575;&#20998;&#24067;&#20960;&#20309;&#29305;&#24615;&#30340;&#21516;&#26102;&#23545;&#20854;&#36827;&#34892;&#24179;&#22343;&#30340;&#25968;&#23398;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#35745;&#31639;&#20219;&#24847;OT&#25104;&#26412;&#20989;&#25968;&#30340;&#36830;&#32493;&#29109;OT&#24052;&#27663;&#20013;&#24515;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#26368;&#36817;&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#21463;&#21040;&#20851;&#27880;&#30340;&#22522;&#20110;&#24369;OT&#30340;&#36830;&#32493;&#29109;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#23545;&#20598;&#37325;&#26500;&#12290;&#38500;&#20102;&#21019;&#26032;&#24615;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#20855;&#26377;&#20197;&#19979;&#33509;&#24178;&#20248;&#21183;&#29305;&#28857;&#65306;&#65288;i&#65289;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#24674;&#22797;&#35299;&#30340;&#36136;&#37327;&#30028;&#38480;&#65307;&#65288;ii&#65289;&#35813;&#26041;&#27861;&#19982;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#23398;&#20064;&#36807;&#31243;&#26080;&#32541;&#36830;&#25509;&#65292;&#21487;&#20197;&#20351;&#29992;&#32463;&#36807;&#33391;&#22909;&#35843;&#25972;&#30340;&#31639;&#27861;&#35299;&#20915;&#24863;&#20852;&#36259;&#30340;&#38382;&#39064;&#65307;&#65288;iii&#65289;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#36991;&#20813;&#20351;&#29992;&#26497;&#23567;-&#26497;&#22823;&#12289;&#24378;&#21270;&#31561;&#22797;&#26434;&#25216;&#24039;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;s
&lt;/p&gt;
&lt;p&gt;
Optimal transport (OT) barycenters are a mathematically grounded way of averaging probability distributions while capturing their geometric properties. In short, the barycenter task is to take the average of a collection of probability distributions w.r.t. given OT discrepancies. We propose a novel algorithm for approximating the continuous Entropic OT (EOT) barycenter for arbitrary OT cost functions. Our approach is built upon the dual reformulation of the EOT problem based on weak OT, which has recently gained the attention of the ML community. Beyond its novelty, our method enjoys several advantageous properties: (i) we establish quality bounds for the recovered solution; (ii) this approach seemlessly interconnects with the Energy-Based Models (EBMs) learning procedure enabling the use of well-tuned algorithms for the problem of interest; (iii) it provides an intuitive optimization scheme avoiding min-max, reinforce and other intricate technical tricks. For validation, we consider s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;OptGNN&#65292;&#21033;&#29992;&#21322;&#23450;&#35268;&#21010;&#24037;&#20855;&#33719;&#24471;&#22823;&#31867;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#12290;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#31639;&#27861;&#21644;&#20256;&#32479;&#31639;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;OptGNN&#30340;&#33021;&#21147;&#35774;&#35745;&#20102;&#19968;&#20010;&#20135;&#29983;&#20248;&#21270;&#30340;&#23545;&#20598;&#35777;&#20070;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.00526</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#21542;&#20316;&#20026;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Graph Neural Networks Optimal Approximation Algorithms?. (arXiv:2310.00526v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;OptGNN&#65292;&#21033;&#29992;&#21322;&#23450;&#35268;&#21010;&#24037;&#20855;&#33719;&#24471;&#22823;&#31867;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#12290;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#31639;&#27861;&#21644;&#20256;&#32479;&#31639;&#27861;&#65292;&#21516;&#26102;&#21033;&#29992;OptGNN&#30340;&#33021;&#21147;&#35774;&#35745;&#20102;&#19968;&#20010;&#20135;&#29983;&#20248;&#21270;&#30340;&#23545;&#20598;&#35777;&#20070;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33021;&#22815;&#20351;&#29992;&#21322;&#23450;&#35268;&#21010;&#65288;SDP&#65289;&#24378;&#22823;&#30340;&#31639;&#27861;&#24037;&#20855;&#26469;&#33719;&#24471;&#22823;&#31867;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26368;&#20248;&#36817;&#20284;&#31639;&#27861;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#21487;&#20197;&#34920;&#31034;&#26368;&#24378;&#22823;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#21069;&#25552;&#26159;&#20551;&#35774;&#21807;&#19968;&#28216;&#25103;&#29468;&#24819;&#25104;&#31435;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#32467;&#26524;&#26500;&#24314;&#20102;&#39640;&#25928;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;OptGNN&#65292;&#23427;&#22312;&#35832;&#22914;&#26368;&#22823;&#21106;&#21644;&#26368;&#22823;&#29420;&#31435;&#38598;&#31561;&#37325;&#35201;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#19978;&#33719;&#24471;&#20102;&#39640;&#36136;&#37327;&#30340;&#36817;&#20284;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#19981;&#20165;&#36229;&#36807;&#20102;&#31070;&#32463;&#32593;&#32476;&#22522;&#32447;&#31639;&#27861;&#65292;&#36824;&#36229;&#36807;&#20102;&#20256;&#32479;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;OptGNN&#25429;&#25417;&#20984;&#26494;&#24347;&#30340;&#33021;&#21147;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#20135;&#29983;&#20248;&#21270;&#30340;&#23545;&#20598;&#35777;&#20070;&#65288;&#30830;&#23450;&#24615;&#19978;&#30028;&#65289;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we design graph neural network architectures that can be used to obtain optimal approximation algorithms for a large class of combinatorial optimization problems using powerful algorithmic tools from semidefinite programming (SDP). Concretely, we prove that polynomial-sized message passing algorithms can represent the most powerful polynomial time algorithms for Max Constraint Satisfaction Problems assuming the Unique Games Conjecture. We leverage this result to construct efficient graph neural network architectures, OptGNN, that obtain high-quality approximate solutions on landmark combinatorial optimization problems such as Max Cut and maximum independent set. Our approach achieves strong empirical results across a wide range of real-world and synthetic datasets against both neural baselines and classical algorithms. Finally, we take advantage of OptGNN's ability to capture convex relaxations to design an algorithm for producing dual certificates of optimality (bounds on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#38543;&#26426;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#38543;&#26426;&#24179;&#28369;&#24341;&#20837;&#30340;&#26041;&#24046;&#19982;&#20998;&#31867;&#22120;&#30340;Lipschitz&#24120;&#25968;&#21644;&#36793;&#30028;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#37319;&#29992;&#21333;&#32431;&#24418;&#25237;&#24433;&#25216;&#26415;&#26469;&#22686;&#21152;&#35748;&#35777;&#40065;&#26834;&#21322;&#24452;&#12290;</title><link>http://arxiv.org/abs/2309.16883</link><description>&lt;p&gt;
&#22686;&#24378;&#38543;&#26426;&#24179;&#28369;&#30340;Lipschitz-&#26041;&#24046;-&#36793;&#30028;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing. (arXiv:2309.16883v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#38543;&#26426;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#38543;&#26426;&#24179;&#28369;&#24341;&#20837;&#30340;&#26041;&#24046;&#19982;&#20998;&#31867;&#22120;&#30340;Lipschitz&#24120;&#25968;&#21644;&#36793;&#30028;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#37319;&#29992;&#21333;&#32431;&#24418;&#25237;&#24433;&#25216;&#26415;&#26469;&#22686;&#21152;&#35748;&#35777;&#40065;&#26834;&#21322;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#22122;&#22768;&#36755;&#20837;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#26102;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#38469;&#24212;&#29992;&#21463;&#21040;&#20854;&#19981;&#31283;&#23450;&#30340;&#39044;&#27979;&#30340;&#38459;&#30861;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35748;&#35777;&#21322;&#24452;&#26159;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#35774;&#35745;&#19968;&#20010;&#20855;&#26377;&#36275;&#22815;&#35748;&#35777;&#21322;&#24452;&#30340;&#39640;&#25928;&#20998;&#31867;&#22120;&#21602;&#65311;&#38543;&#26426;&#24179;&#28369;&#36890;&#36807;&#22312;&#36755;&#20837;&#20013;&#27880;&#20837;&#22122;&#22768;&#26469;&#33719;&#24471;&#24179;&#28369;&#19988;&#26356;&#40065;&#26834;&#30340;&#20998;&#31867;&#22120;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#39318;&#20808;&#23637;&#31034;&#20102;&#38543;&#26426;&#24179;&#28369;&#24341;&#20837;&#30340;&#26041;&#24046;&#19982;&#20998;&#31867;&#22120;&#30340;&#21478;&#22806;&#20004;&#20010;&#37325;&#35201;&#23646;&#24615;&#65292;&#21363;&#20854;Lipschitz&#24120;&#25968;&#21644;&#36793;&#30028;&#20043;&#38388;&#30340;&#23494;&#20999;&#20851;&#31995;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#22522;&#20998;&#31867;&#22120;&#30340;Lipschitz&#24120;&#25968;&#23545;&#24179;&#28369;&#20998;&#31867;&#22120;&#21644;&#32463;&#39564;&#26041;&#24046;&#30340;&#21452;&#37325;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#21152;&#35748;&#35777;&#40065;&#26834;&#21322;&#24452;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#21333;&#32431;&#24418;&#25237;&#24433;&#25216;&#26415;&#65292;&#20197;&#20415;&#36890;&#36807;Bernst&#30340;&#26041;&#24046;-&#36793;&#30028;&#26435;&#34913;&#26469;&#21033;&#29992;&#22522;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-life applications of deep neural networks are hindered by their unsteady predictions when faced with noisy inputs and adversarial attacks. The certified radius is in this context a crucial indicator of the robustness of models. However how to design an efficient classifier with a sufficient certified radius? Randomized smoothing provides a promising framework by relying on noise injection in inputs to obtain a smoothed and more robust classifier. In this paper, we first show that the variance introduced by randomized smoothing closely interacts with two other important properties of the classifier, i.e. its Lipschitz constant and margin. More precisely, our work emphasizes the dual impact of the Lipschitz constant of the base classifier, on both the smoothed classifier and the empirical variance. Moreover, to increase the certified robust radius, we introduce a different simplex projection technique for the base classifier to leverage the variance-margin trade-off thanks to Bernst
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;P-ROCKET&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#21024;&#38500;&#21367;&#31215;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#36827;&#34892;&#21098;&#26525;&#12290;</title><link>http://arxiv.org/abs/2309.08499</link><description>&lt;p&gt;
P-ROCKET: &#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
P-ROCKET: Pruning Random Convolution Kernels for Time Series Classification. (arXiv:2309.08499v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;P-ROCKET&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#21024;&#38500;&#21367;&#31215;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#36827;&#34892;&#21098;&#26525;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#65292;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#27169;&#22411;ROCKET&#21644;MINIROCKET&#22240;&#20854;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;ROCKET&#21644;MINIROCKET&#21033;&#29992;&#26080;&#38656;&#35757;&#32451;&#30340;&#38543;&#26426;&#19968;&#32500;&#21367;&#31215;&#26680;&#65292;&#21487;&#20197;&#24555;&#36895;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#39640;&#25928;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20840;&#38754;&#25429;&#25417;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#65292;&#36825;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#26469;&#35828;&#26159;&#19981;&#20860;&#23481;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#36827;&#21270;&#31639;&#27861;S-ROCKET&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#21098;&#26525;&#20887;&#20313;&#30340;&#21367;&#31215;&#26680;&#12290;&#28982;&#32780;&#65292;&#36827;&#21270;&#31639;&#27861;&#26412;&#36523;&#30340;&#29305;&#24615;&#23548;&#33268;&#22312;S-ROCKET&#20013;&#35780;&#20272;&#21367;&#31215;&#26680;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#20013;&#65292;&#19982;&#30452;&#25509;&#35780;&#20272;&#20855;&#26377;&#38750;&#26174;&#33879;&#24046;&#24322;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#30340;S-ROCKET&#19981;&#21516;&#65292;&#25105;&#20204;&#20174;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#21024;&#38500;&#21367;&#31215;&#26680;&#65292;&#36890;&#36807;&#28040;&#38500;&#24207;&#21015;&#20013;&#30340;&#30456;&#20851;&#36830;&#25509;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, two time series classification models, ROCKET and MINIROCKET, have attracted much attention for their low training cost and state-of-the-art accuracy. Utilizing random 1-D convolutional kernels without training, ROCKET and MINIROCKET can rapidly extract features from time series data, allowing for the efficient fitting of linear classifiers. However, to comprehensively capture useful features, a large number of random kernels are required, which is incompatible for resource-constrained devices. Therefore, a heuristic evolutionary algorithm named S-ROCKET is devised to recognize and prune redundant kernels. Nevertheless, the inherent nature of evolutionary algorithms renders the evaluation of kernels within S-ROCKET an unacceptable time-consuming process. In this paper, diverging from S-ROCKET, which directly evaluates random kernels with nonsignificant differences, we remove kernels from a feature selection perspective by eliminating associating connections in the sequ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Beta&#25955;&#24230;&#30340;&#28145;&#24230;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#38754;&#37096;&#29305;&#24449;&#25552;&#21462;&#12289;&#25991;&#26723;&#20027;&#39064;&#35782;&#21035;&#21644;&#39640;&#20809;&#35889;&#22270;&#20687;&#26448;&#26009;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.08249</link><description>&lt;p&gt;
&#24102;&#26377;Beta&#25955;&#24230;&#30340;&#28145;&#24230;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Deep Nonnegative Matrix Factorization with Beta Divergences. (arXiv:2309.08249v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Beta&#25955;&#24230;&#30340;&#28145;&#24230;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#38754;&#37096;&#29305;&#24449;&#25552;&#21462;&#12289;&#25991;&#26723;&#20027;&#39064;&#35782;&#21035;&#21644;&#39640;&#20809;&#35889;&#22270;&#20687;&#26448;&#26009;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;deep NMF&#65289;&#26368;&#36817;&#25104;&#20026;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#25552;&#21462;&#22810;&#23618;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#29616;&#26377;&#30340;&#28145;&#24230;NMF&#27169;&#22411;&#21644;&#31639;&#27861;&#20027;&#35201;&#37117;&#20197;&#26368;&#23567;&#20108;&#20056;&#35823;&#24046;&#20026;&#35780;&#20272;&#26631;&#20934;&#65292;&#36825;&#21487;&#33021;&#19981;&#26159;&#35780;&#20272;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#36817;&#20284;&#36136;&#37327;&#30340;&#26368;&#21512;&#36866;&#25351;&#26631;&#12290;&#20363;&#22914;&#65292;&#24403;&#22788;&#29702;&#38899;&#39057;&#20449;&#21495;&#21644;&#25991;&#26723;&#31561;&#25968;&#25454;&#31867;&#22411;&#26102;&#65292;&#24191;&#27867;&#35748;&#21487;&#30340;&#26159;$\beta$-divergences&#25552;&#20379;&#20102;&#26356;&#36866;&#21512;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#22522;&#20110;$\beta$-divergences&#24320;&#21457;&#20102;&#26032;&#30340;&#28145;&#24230;NMF&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#24182;&#23558;&#36825;&#20123;&#25216;&#26415;&#24212;&#29992;&#20110;&#38754;&#37096;&#29305;&#24449;&#25552;&#21462;&#12289;&#25991;&#26723;&#38598;&#21512;&#20013;&#30340;&#20027;&#39064;&#35782;&#21035;&#20197;&#21450;&#39640;&#20809;&#35889;&#22270;&#20687;&#20013;&#26448;&#26009;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Nonnegative Matrix Factorization (deep NMF) has recently emerged as a valuable technique for extracting multiple layers of features across different scales. However, all existing deep NMF models and algorithms have primarily centered their evaluation on the least squares error, which may not be the most appropriate metric for assessing the quality of approximations on diverse datasets. For instance, when dealing with data types such as audio signals and documents, it is widely acknowledged that $\beta$-divergences offer a more suitable alternative. In this paper, we develop new models and algorithms for deep NMF using $\beta$-divergences. Subsequently, we apply these techniques to the extraction of facial features, the identification of topics within document collections, and the identification of materials within hyperspectral images.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#12289;&#21344;&#29992;&#27169;&#24335;&#21644;&#22825;&#27668;&#26465;&#20214;&#30340;LSTM&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#24314;&#31569;&#33021;&#32791;&#65292;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#31934;&#24230;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02908</link><description>&lt;p&gt;
DECODE: &#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#21644;&#29615;&#22659;&#22240;&#32032;&#30340;&#25968;&#25454;&#39537;&#21160;&#33021;&#32791;&#39044;&#27979;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
DECODE: Data-driven Energy Consumption Prediction leveraging Historical Data and Environmental Factors in Buildings. (arXiv:2309.02908v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#12289;&#21344;&#29992;&#27169;&#24335;&#21644;&#22825;&#27668;&#26465;&#20214;&#30340;LSTM&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#24314;&#31569;&#33021;&#32791;&#65292;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#31934;&#24230;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#20013;&#30340;&#33021;&#32791;&#39044;&#27979;&#22312;&#26377;&#25928;&#30340;&#33021;&#28304;&#31649;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#31934;&#30830;&#30340;&#39044;&#27979;&#23545;&#20110;&#23454;&#29616;&#20248;&#21270;&#30340;&#33021;&#32791;&#21644;&#30005;&#32593;&#20998;&#37197;&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21382;&#21490;&#33021;&#28304;&#25968;&#25454;&#12289;&#21344;&#29992;&#27169;&#24335;&#21644;&#22825;&#27668;&#26465;&#20214;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#24314;&#31569;&#33021;&#32791;&#12290;&#19982;&#29616;&#26377;&#30340;&#39044;&#27979;&#27169;&#22411;&#30456;&#27604;&#65292;LSTM&#27169;&#22411;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#30701;&#26399;&#12289;&#20013;&#26399;&#21644;&#38271;&#26399;&#33021;&#32791;&#39044;&#27979;&#65292;&#36866;&#29992;&#20110;&#20303;&#23429;&#21644;&#21830;&#19994;&#24314;&#31569;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;LSTM&#27169;&#22411;&#19982;&#32447;&#24615;&#22238;&#24402;&#12289;&#20915;&#31574;&#26641;&#21644;&#38543;&#26426;&#26862;&#26519;&#31561;&#24050;&#26377;&#30340;&#39044;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20196;&#20154;&#40723;&#33310;&#30340;&#26159;&#65292;&#25552;&#20986;&#30340;LSTM&#27169;&#22411;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#23427;&#20855;&#26377;&#20986;&#33394;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;R2&#24471;&#20998;&#20026;0.97&#65292;&#26368;&#20302;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#20026;0.007&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;&#27169;&#22411;&#30340;&#21478;&#19968;&#20010;&#20248;&#28857;&#26159;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#33021;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy prediction in buildings plays a crucial role in effective energy management. Precise predictions are essential for achieving optimal energy consumption and distribution within the grid. This paper introduces a Long Short-Term Memory (LSTM) model designed to forecast building energy consumption using historical energy data, occupancy patterns, and weather conditions. The LSTM model provides accurate short, medium, and long-term energy predictions for residential and commercial buildings compared to existing prediction models. We compare our LSTM model with established prediction methods, including linear regression, decision trees, and random forest. Encouragingly, the proposed LSTM model emerges as the superior performer across all metrics. It demonstrates exceptional prediction accuracy, boasting the highest R2 score of 0.97 and the most favorable mean absolute error (MAE) of 0.007. An additional advantage of our developed model is its capacity to achieve efficient energy consu
&lt;/p&gt;</description></item><item><title>&#22810;&#36884;&#24452;&#36866;&#37197;&#22120;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;"&#23545;&#40784;&#22686;&#24378;&#22120;"&#21152;&#28145;&#27169;&#24577;&#23545;&#40784;&#65292;&#23454;&#29616;&#39640;&#21487;&#36716;&#31227;&#24615;&#65292;&#21487;&#26377;&#25928;&#20943;&#23569;&#35843;&#25972;&#21442;&#25968;&#30340;&#26102;&#38388;&#24182;&#25552;&#39640;&#38646;&#26679;&#26412;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.01516</link><description>&lt;p&gt;
&#22810;&#36884;&#24452;&#36866;&#37197;&#22120;&#65306;&#20026;&#21487;&#25193;&#23637;&#30340;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#35843;&#25972;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval. (arXiv:2309.01516v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01516
&lt;/p&gt;
&lt;p&gt;
&#22810;&#36884;&#24452;&#36866;&#37197;&#22120;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;"&#23545;&#40784;&#22686;&#24378;&#22120;"&#21152;&#28145;&#27169;&#24577;&#23545;&#40784;&#65292;&#23454;&#29616;&#39640;&#21487;&#36716;&#31227;&#24615;&#65292;&#21487;&#26377;&#25928;&#20943;&#23569;&#35843;&#25972;&#21442;&#25968;&#30340;&#26102;&#38388;&#24182;&#25552;&#39640;&#38646;&#26679;&#26412;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#35268;&#27169;&#19981;&#26029;&#22686;&#21152;&#65292;&#23558;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#35843;&#25972;&#21040;&#19987;&#38376;&#30340;&#20219;&#21153;&#19978;&#24050;&#25104;&#20026;&#19968;&#20010;&#35745;&#31639;&#21644;&#20869;&#23384;&#23494;&#38598;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#24494;&#35843;&#26041;&#27861;&#38656;&#35201;&#20026;&#27599;&#20010;&#26032;&#20219;&#21153;&#36827;&#34892;&#23396;&#31435;&#12289;&#31351;&#20030;&#30340;&#37325;&#26032;&#35843;&#25972;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#39640;&#25928;&#35843;&#25972;&#25216;&#26415;&#32463;&#24120;&#24573;&#35270;&#27169;&#24577;&#23545;&#40784;&#65292;&#20165;&#20851;&#27880;&#26032;&#20219;&#21153;&#30340;&#30693;&#35782;&#25552;&#21462;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#36884;&#24452;&#36866;&#37197;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#21253;&#21547;&#20102;&#19968;&#20010;&#8220;&#23545;&#40784;&#22686;&#24378;&#22120;&#8221;&#65292;&#21487;&#20197;&#21152;&#28145;&#27169;&#24577;&#23545;&#40784;&#65292;&#23454;&#29616;&#39640;&#24230;&#30340;&#21487;&#36716;&#31227;&#24615;&#32780;&#26080;&#38656;&#35843;&#25972;&#39044;&#35757;&#32451;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#21521;LMMs&#28155;&#21152;&#20102;&#19981;&#21040;1.25%&#30340;&#39069;&#22806;&#21442;&#25968;&#65292;&#20197;BEiT-3&#27169;&#22411;&#20026;&#20363;&#12290;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#24615;&#33021;&#19978;&#20855;&#26377;&#20248;&#21183;&#65292;&#21516;&#26102;&#32553;&#30701;&#20102;&#39640;&#36798;57%&#30340;&#24494;&#35843;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of Large Multi-Modal Models (LMMs) increases consistently, the adaptation of these pre-trained models to specialized tasks has become a computationally and memory-intensive challenge. Traditional fine-tuning methods require isolated, exhaustive retuning for each new task, limiting the models' versatility. Moreover, current efficient adaptation techniques often overlook modality alignment, focusing only on the knowledge extraction of new tasks. To tackle these issues, we introduce Multiway-Adapter, an innovative framework incorporating an 'Alignment Enhancer' to deepen modality alignment, enabling high transferability without tuning pre-trained parameters. Our method adds fewer than 1.25\% of additional parameters to LMMs, exemplified by the BEiT-3 model in our study. This leads to superior zero-shot image-text retrieval performance compared to fully fine-tuned models, while achieving up to a 57\% reduction in fine-tuning time. Our approach offers a resource-efficient and ef
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23545;&#20110;&#23545;&#40784;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#35328;&#65292;&#35774;&#35745;&#21453;&#39304;&#36873;&#25321;&#26159;&#35780;&#20998;&#36824;&#26159;&#25490;&#21517;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#23384;&#22312;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#24182;&#19988;&#27880;&#37322;&#32773;&#30340;&#20559;&#35265;&#20063;&#20250;&#24433;&#21709;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#20063;&#23545;&#35780;&#20272;&#32467;&#26524;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.15812</link><description>&lt;p&gt;
&#36879;&#36807;&#20559;&#22909;&#30475;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#33719;&#21462;&#65306;&#25581;&#31034;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models. (arXiv:2308.15812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23545;&#20110;&#23545;&#40784;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#35328;&#65292;&#35774;&#35745;&#21453;&#39304;&#36873;&#25321;&#26159;&#35780;&#20998;&#36824;&#26159;&#25490;&#21517;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#23384;&#22312;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#24182;&#19988;&#27880;&#37322;&#32773;&#30340;&#20559;&#35265;&#20063;&#20250;&#24433;&#21709;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#20063;&#23545;&#35780;&#20272;&#32467;&#26524;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#21644;&#24847;&#22270;&#30340;&#23545;&#40784;&#25215;&#35834;&#28041;&#21450;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25110;&#20154;&#31867;&#21453;&#39304;&#12290;&#31264;&#23494;&#30340;&#21453;&#39304;&#27880;&#37322;&#33719;&#21462;&#21644;&#25972;&#21512;&#25104;&#26412;&#36739;&#39640;&#65292;&#32780;&#31232;&#30095;&#30340;&#21453;&#39304;&#21017;&#28041;&#21450;&#32467;&#26500;&#24615;&#35774;&#35745;&#36873;&#25321;&#65292;&#21363;&#35780;&#20998;&#65288;&#20363;&#22914;&#65292;&#22312;1-7&#30340;&#33539;&#22260;&#20869;&#23545;&#22238;&#31572;A&#36827;&#34892;&#35780;&#20998;&#65289;&#21644;&#25490;&#21517;&#65288;&#20363;&#22914;&#65292;&#22238;&#31572;A&#26159;&#21542;&#27604;&#22238;&#31572;B&#26356;&#22909;&#65311;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#23545;LLMs&#30340;&#23545;&#40784;&#21644;&#35780;&#20272;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#22312;&#20154;&#31867;&#21644;AI&#27880;&#37322;&#32773;&#20013;&#37117;&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#36798;&#21040;&#20102;60%&#12290;&#25105;&#20204;&#30340;&#21518;&#32493;&#20998;&#26512;&#30830;&#23450;&#20102;&#35299;&#37322;&#36825;&#20010;&#29616;&#35937;&#30340;&#21508;&#31181;&#27880;&#37322;&#32773;&#20559;&#35265;&#26041;&#38754;&#65292;&#27604;&#22914;&#20154;&#31867;&#27880;&#37322;&#32773;&#26356;&#21916;&#27426;&#23494;&#38598;&#22238;&#31572;&#24182;&#22312;&#20004;&#20010;&#36873;&#39033;&#20043;&#38388;&#26356;&#38738;&#30544;&#20934;&#30830;&#24615;&#12290;&#20196;&#25105;&#20204;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#23545;&#23545;&#40784;&#30340;LLMs&#30340;&#35780;&#20272;&#20063;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#30340;&#35780;&#20272;&#32467;&#26524;&#22240;&#20026;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) with human values and intents critically involves the use of human or AI feedback. While dense feedback annotations are expensive to acquire and integrate, sparse feedback presents a structural design choice between ratings (e.g., score Response A on a scale of 1-7) and rankings (e.g., is Response A better than Response B?). In this work, we analyze the effect of this design choice for the alignment and evaluation of LLMs. We uncover an inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators. Our subsequent analysis identifies various facets of annotator biases that explain this phenomena, such as human annotators would rate denser responses higher while preferring accuracy during pairwise judgments. To our surprise, we also observe that the choice of feedback protocol also has a significant effect on the evaluation of aligned LLMs. In particular, we find that LLMs
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Laplace-LoRA&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#26469;&#22686;&#24378;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13111</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20302;&#31209;&#36866;&#24212;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bayesian low-rank adaptation for large language models. (arXiv:2308.13111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Laplace-LoRA&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#26469;&#22686;&#24378;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#65288;PEFT&#65289;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25104;&#26412;&#39640;&#25928;&#24494;&#35843;&#30340;&#26032;&#33539;&#24335;&#65292;&#20854;&#20013;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;LLMs&#24448;&#24448;&#21464;&#24471;&#36807;&#20110;&#33258;&#20449;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26102;&#12290;&#36125;&#21494;&#26031;&#26041;&#27861;&#20855;&#26377;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#21487;&#20316;&#20026;&#20943;&#36731;&#36807;&#24230;&#33258;&#20449;&#24182;&#22686;&#24378;&#26657;&#20934;&#33021;&#21147;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Laplace-LoRA&#65292;&#19968;&#31181;&#30452;&#35266;&#32780;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#23427;&#23558;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#24212;&#29992;&#20110;LoRA&#21442;&#25968;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#32463;&#36807;&#24494;&#35843;&#30340;LLMs&#30340;&#26657;&#20934;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) has emerged as a new paradigm for cost-efficient fine-tuning of large language models (LLMs), with low-rank adaptation (LoRA) being a widely adopted choice. However, fine-tuned LLMs often become overconfident especially on when fine-tuned on smaller datasets. Bayesian methods, with their inherent ability to estimate uncertainty, serve as potent tools to mitigate overconfidence and enhance calibration. In this work, we introduce Laplace-LoRA, a straightforward yet effective Bayesian method, which applies the Laplace approximation to the LoRA parameters and, considerably boosts the calibration of fine-tuned LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36317;&#31163;&#32467;&#26500;&#32534;&#30721;&#65288;HDSE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#23618;&#27425;&#22270;&#32467;&#26500;&#12290;&#32463;&#36807;&#22312;12&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#25928;&#26524;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.11129</link><description>&lt;p&gt;
&#20351;&#29992;&#23618;&#27425;&#32467;&#26500;&#36317;&#31163;&#25429;&#25417;&#22810;&#23618;&#27425;&#22270;&#32467;&#26500;&#30340;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances. (arXiv:2308.11129v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36317;&#31163;&#32467;&#26500;&#32534;&#30721;&#65288;HDSE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#22810;&#23618;&#27425;&#22270;&#32467;&#26500;&#12290;&#32463;&#36807;&#22312;12&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#25928;&#26524;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21464;&#21387;&#22120;&#38656;&#35201;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#26469;&#24471;&#20986;&#26377;&#24847;&#20041;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25552;&#35758;&#24456;&#23569;&#28041;&#21450;&#25429;&#25417;&#26356;&#38271;&#36317;&#31163;&#12289;&#23618;&#27425;&#32467;&#26500;&#25110;&#31038;&#21306;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#32780;&#36825;&#20123;&#22312;&#20998;&#23376;&#12289;&#31038;&#20132;&#32593;&#32476;&#21644;&#24341;&#29992;&#32593;&#32476;&#31561;&#21508;&#31181;&#22270;&#24418;&#20013;&#37117;&#20250;&#20986;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36317;&#31163;&#32467;&#26500;&#32534;&#30721;&#65288;HDSE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#23618;&#27425;&#36317;&#31163;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#22810;&#23618;&#27425;&#12289;&#23618;&#27425;&#21270;&#30340;&#24615;&#36136;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20135;&#29983;&#20102;&#19968;&#20010;&#21487;&#20197;&#28789;&#27963;&#19982;&#29616;&#26377;&#22270;&#21464;&#21387;&#22120;&#38598;&#25104;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#19982;&#20854;&#20182;&#20301;&#32622;&#34920;&#31034;&#21516;&#26102;&#24212;&#29992;&#12290;&#36890;&#36807;&#22312;12&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;HDSE&#26041;&#27861;&#25104;&#21151;&#25552;&#21319;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#22522;&#32447;&#21464;&#21387;&#22120;&#65292;&#22312;10&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph transformers need strong inductive biases to derive meaningful attention scores. Yet, current proposals rarely address methods capturing longer ranges, hierarchical structures, or community structures, as they appear in various graphs such as molecules, social networks, and citation networks. In this paper, we propose a hierarchy-distance structural encoding (HDSE), which models a hierarchical distance between the nodes in a graph focusing on its multi-level, hierarchical nature. In particular, this yields a framework which can be flexibly integrated with existing graph transformers, allowing for simultaneous application with other positional representations. Through extensive experiments on 12 real-world datasets, we demonstrate that our HDSE method successfully enhances various types of baseline transformers, achieving state-of-the-art empirical performances on 10 benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#24819;&#27861;&#22270;&#65288;GoT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#23558;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#65292;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#65292;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#12290;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20248;&#21183;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20351;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;</title><link>http://arxiv.org/abs/2308.09687</link><description>&lt;p&gt;
&#24819;&#27861;&#22270;&#65306;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Graph of Thoughts: Solving Elaborate Problems with Large Language Models. (arXiv:2308.09687v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09687
&lt;/p&gt;
&lt;p&gt;
&#24819;&#27861;&#22270;&#65288;GoT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#23558;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#65292;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#65292;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#12290;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20248;&#21183;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20351;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24819;&#27861;&#22270;&#65288;Graph of Thoughts&#65292;GoT&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25552;&#31034;&#33021;&#21147;&#19978;&#36229;&#36234;&#20102;Chain-of-Thought&#25110;Tree of Thoughts&#65288;ToT&#65289;&#31561;&#33539;&#24335;&#12290;GoT&#30340;&#20851;&#38190;&#24605;&#24819;&#21644;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#23558;LLM&#29983;&#25104;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#20854;&#20013;&#20449;&#24687;&#21333;&#20803;&#65288;"LLM&#24819;&#27861;"&#65289;&#26159;&#39030;&#28857;&#65292;&#36793;&#34920;&#31034;&#36825;&#20123;&#39030;&#28857;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#23558;&#20219;&#24847;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#12289;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;&#20248;&#21183;&#65292;&#20363;&#22914;&#22312;&#25490;&#24207;&#20219;&#21153;&#19978;&#36136;&#37327;&#25552;&#39640;&#20102;62%&#65292;&#21516;&#26102;&#25104;&#26412;&#38477;&#20302;&#20102;&#36229;&#36807;31%&#12290;&#25105;&#20204;&#30830;&#20445;GoT&#33021;&#22815;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#24320;&#21019;&#26032;&#30340;&#25552;&#31034;&#26041;&#26696;&#12290;&#36825;&#39033;&#24037;&#20316;&#20351;&#24471;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by &gt;31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinki
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.07134</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#26159;&#22270;&#34920;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22914;ChatGPT&#65292;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36880;&#28176;&#21462;&#20195;&#20102;CNN&#21644;RNN&#65292;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#32479;&#19968;&#36215;&#26469;&#12290;&#19982;&#30456;&#23545;&#29420;&#31435;&#23384;&#22312;&#30340;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#25991;&#26412;&#65289;&#30456;&#27604;&#65292;&#22270;&#34920;&#26159;&#19968;&#31181;&#21253;&#21547;&#20016;&#23500;&#32467;&#26500;&#21644;&#20851;&#31995;&#20449;&#24687;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#21516;&#26102;&#65292;&#20316;&#20026;&#26368;&#20855;&#34920;&#29616;&#21147;&#30340;&#23186;&#20171;&#20043;&#19968;&#65292;&#33258;&#28982;&#35821;&#35328;&#22312;&#25551;&#36848;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23558;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#32435;&#20837;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#30340;&#29616;&#26377;&#24037;&#20316;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#38271;&#65292;&#25506;&#32034;LLMs&#26159;&#21542;&#20063;&#21487;&#20197;&#26367;&#20195;GNNs&#25104;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructGLM&#65288;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#65289;&#31639;&#27861;&#65292;&#31995;&#32479;&#22320;&#35774;&#35745;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
&lt;/p&gt;</description></item><item><title>GIT-Mol&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#22312;&#20998;&#23376;&#31185;&#23398;&#20013;&#22788;&#29702;&#22270;&#20687;&#12289;&#22270;&#24418;&#21644;&#25991;&#26412;&#20449;&#24687;&#12290;&#36890;&#36807;&#26032;&#25552;&#20986;&#30340;GIT-Former&#26550;&#26500;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;&#22810;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#23545;&#40784;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;GIT-Mol&#22312;&#24615;&#36136;&#39044;&#27979;&#21644;&#20998;&#23376;&#29983;&#25104;&#26377;&#25928;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#36824;&#21487;&#29992;&#20110;&#21270;&#21512;&#29289;&#21517;&#31216;&#35782;&#21035;&#21644;&#21270;&#23398;&#21453;&#24212;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.06911</link><description>&lt;p&gt;
GIT-Mol&#65306;&#19968;&#31181;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20998;&#23376;&#31185;&#23398;&#20013;&#30340;&#22270;&#20687;&#65292;&#22270;&#24418;&#21644;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text. (arXiv:2308.06911v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06911
&lt;/p&gt;
&lt;p&gt;
GIT-Mol&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#22312;&#20998;&#23376;&#31185;&#23398;&#20013;&#22788;&#29702;&#22270;&#20687;&#12289;&#22270;&#24418;&#21644;&#25991;&#26412;&#20449;&#24687;&#12290;&#36890;&#36807;&#26032;&#25552;&#20986;&#30340;GIT-Former&#26550;&#26500;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;&#22810;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#23545;&#40784;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;GIT-Mol&#22312;&#24615;&#36136;&#39044;&#27979;&#21644;&#20998;&#23376;&#29983;&#25104;&#26377;&#25928;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#36824;&#21487;&#29992;&#20110;&#21270;&#21512;&#29289;&#21517;&#31216;&#35782;&#21035;&#21644;&#21270;&#23398;&#21453;&#24212;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#22788;&#29702;&#20998;&#23376;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#20026;&#20998;&#23376;&#31185;&#23398;&#20013;&#30340;&#21019;&#26032;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#20855;&#26377;&#22797;&#26434;&#20998;&#23376;&#32467;&#26500;&#25110;&#22270;&#20687;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GIT-Mol&#65292;&#19968;&#31181;&#38598;&#25104;&#20102;&#22270;&#24418;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#20419;&#36827;&#22810;&#27169;&#24577;&#20998;&#23376;&#25968;&#25454;&#30340;&#38598;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GIT-Former&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#23558;&#25152;&#26377;&#27169;&#24577;&#23545;&#40784;&#21040;&#32479;&#19968;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#24615;&#36136;&#39044;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;5%-10%&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#65292;&#24182;&#22312;&#20998;&#23376;&#29983;&#25104;&#26377;&#25928;&#24615;&#26041;&#38754;&#25552;&#39640;&#20102;20.2%&#12290;&#36890;&#36807;&#20219;&#24847;&#21040;&#35821;&#35328;&#30340;&#20998;&#23376;&#32763;&#35793;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#28508;&#21147;&#36827;&#34892;&#26356;&#22810;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#20363;&#22914;&#21270;&#21512;&#29289;&#21517;&#31216;&#35782;&#21035;&#21644;&#21270;&#23398;&#21453;&#24212;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have made significant strides in natural language processing, enabling innovative applications in molecular science by processing textual representations of molecules. However, most existing language models cannot capture the rich information with complex molecular structures or images. In this paper, we introduce GIT-Mol, a multi-modal large language model that integrates the Graph, Image, and Text information. To facilitate the integration of multi-modal molecular data, we propose GIT-Former, a novel architecture that is capable of aligning all modalities into a unified latent space. We achieve a 5%-10% accuracy increase in properties prediction and a 20.2% boost in molecule generation validity compared to the baselines. With the any-to-language molecular translation strategy, our model has the potential to perform more downstream tasks, such as compound name recognition and chemical reaction prediction.
&lt;/p&gt;</description></item><item><title>SITTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#25104;&#21151;&#22320;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2307.05591</link><description>&lt;p&gt;
SITTA: &#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SITTA: A Semantic Image-Text Alignment for Image Captioning. (arXiv:2307.05591v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05591
&lt;/p&gt;
&lt;p&gt;
SITTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#25104;&#21151;&#22320;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22270;&#20687;&#30340;&#25991;&#26412;&#21644;&#35821;&#20041;&#29702;&#35299;&#23545;&#20110;&#29983;&#25104;&#36866;&#24403;&#30340;&#25551;&#36848;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#38656;&#35201;&#26816;&#27979;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#65292;&#24314;&#27169;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35780;&#20272;&#22330;&#26223;&#30340;&#35821;&#20041;&#65292;&#24182;&#23558;&#25552;&#21462;&#30340;&#30693;&#35782;&#34920;&#31034;&#22312;&#35821;&#35328;&#31354;&#38388;&#20013;&#12290;&#20026;&#20102;&#22312;&#20445;&#35777;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#30340;&#21516;&#26102;&#23454;&#29616;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#34987;&#26465;&#20214;&#21270;&#20026;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#65288;&#22270;&#20687;-&#25991;&#26412;&#65289;&#27169;&#22411;&#65292;&#20801;&#35768;&#20351;&#29992;&#22270;&#20687;&#36755;&#20837;&#12290;&#36825;&#35201;&#27714;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#20013;&#26816;&#27979;&#21040;&#30340;&#35821;&#20041;&#19982;&#29983;&#25104;&#24615;LM&#30340;&#35821;&#35328;&#34920;&#31034;&#36827;&#34892;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26368;&#22909;&#22320;&#23558;&#35270;&#35273;&#32534;&#30721;&#22120;&#26816;&#27979;&#21040;&#30340;&#35821;&#20041;&#20256;&#36882;&#32473;LM&#36824;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#30340;&#26032;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#23558;&#20004;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#30340;&#35821;&#20041;&#36716;&#31227;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#23558;&#22810;&#27169;&#24577;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#23884;&#20837;&#31354;&#38388;&#19982;&#29983;&#25104;&#24615;LM&#30340;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual and semantic comprehension of images is essential for generating proper captions. The comprehension requires detection of objects, modeling of relations between them, an assessment of the semantics of the scene and, finally, representing the extracted knowledge in a language space. To achieve rich language capabilities while ensuring good image-language mappings, pretrained language models (LMs) were conditioned on pretrained multi-modal (image-text) models that allow for image inputs. This requires an alignment of the image representation of the multi-modal model with the language representations of a generative LM. However, it is not clear how to best transfer semantics detected by the vision encoder of the multi-modal model to the LM. We introduce two novel ways of constructing a linear mapping that successfully transfers semantics between the embedding spaces of the two pretrained models. The first aligns the embedding space of the multi-modal language encoder with the embe
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#24050;&#30693;&#30340;UCB&#31867;&#22411;&#26041;&#27861;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#29366;&#24577;&#34920;&#31034;&#65288;PSRs&#65289;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#22870;&#21169;&#39033;&#26469;&#19978;&#30028;t</title><link>http://arxiv.org/abs/2307.00405</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#39640;&#25928;&#30340;UCB&#31867;&#22411;&#31639;&#27861;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#29366;&#24577;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient UCB-type Algorithms For Learning Predictive State Representations. (arXiv:2307.00405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00405
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#24050;&#30693;&#30340;UCB&#31867;&#22411;&#26041;&#27861;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#29366;&#24577;&#34920;&#31034;&#65288;PSRs&#65289;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#22870;&#21169;&#39033;&#26469;&#19978;&#30028;t
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33324;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#26088;&#22312;&#36890;&#36807;&#22522;&#20110;&#36807;&#21435;&#35266;&#23519;&#21644;&#34892;&#21160;&#30340;&#21382;&#21490;&#26469;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22914;&#26524;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#21487;&#20197;&#29992;&#39044;&#27979;&#29366;&#24577;&#34920;&#31034;&#65288;PSRs&#65289;&#24314;&#27169;&#20302;&#31209;&#32467;&#26500;&#65292;&#37027;&#20040;&#23427;&#26159;&#21487;&#32479;&#35745;&#23398;&#20064;&#30340;&#12290;&#23613;&#31649;&#26377;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#39044;&#20808;&#35774;&#35745;&#22909;&#30340;&#27493;&#39588;&#25110;&#32773;&#26159;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#30340;&#25110;&#32773;&#26159;&#19981;&#21487;&#35745;&#31639;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19978;&#38480;&#32622;&#20449;&#21306;&#38388;&#65288;UCB&#65289;&#26041;&#27861;&#22312;&#36172;&#21338;&#26426;&#21644;MDPs&#20013;&#34987;&#25104;&#21151;&#22320;&#20316;&#20026;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#65292;&#20294;&#23545;PSR&#36825;&#31181;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#36824;&#27809;&#26377;&#36827;&#34892;&#30740;&#31350;&#65292;&#36825;&#26159;&#30001;&#20110;&#22312;&#36825;&#31181;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#20048;&#35266;&#22411;&#22870;&#21169;&#30340;&#35774;&#35745;&#21313;&#20998;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;PSRs&#30340;&#31532;&#19968;&#31181;&#24050;&#30693;&#30340;UCB&#31867;&#22411;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#20010;&#26032;&#30340;&#22870;&#21169;&#39033;&#26469;&#19978;&#30028;t
&lt;/p&gt;
&lt;p&gt;
The general sequential decision-making problem, which includes Markov decision processes (MDPs) and partially observable MDPs (POMDPs) as special cases, aims at maximizing a cumulative reward by making a sequence of decisions based on a history of observations and actions over time. Recent studies have shown that the sequential decision-making problem is statistically learnable if it admits a low-rank structure modeled by predictive state representations (PSRs). Despite these advancements, existing approaches typically involve oracles or steps that are not computationally efficient. On the other hand, the upper confidence bound (UCB) based approaches, which have served successfully as computationally efficient methods in bandits and MDPs, have not been investigated for more general PSRs, due to the difficulty of optimistic bonus design in these more challenging settings. This paper proposes the first known UCB-type approach for PSRs, featuring a novel bonus term that upper bounds the t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#31454;&#20105;&#29615;&#22659;&#19979;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#25552;&#39640;&#25968;&#25454;&#34920;&#31034;&#36136;&#37327;&#21487;&#33021;&#20250;&#23548;&#33268;&#20379;&#24212;&#21830;&#25972;&#20307;&#39044;&#27979;&#20934;&#30830;&#24615;&#38477;&#20302;&#65292;&#20174;&#32780;&#38477;&#20302;&#31038;&#20250;&#31119;&#21033;&#12290;</title><link>http://arxiv.org/abs/2306.14670</link><description>&lt;p&gt;
&#31454;&#20105;&#29615;&#22659;&#19979;&#36125;&#21494;&#26031;&#39118;&#38505;&#30340;&#25552;&#39640;&#21487;&#33021;&#23548;&#33268;&#31038;&#20250;&#31119;&#21033;&#30340;&#38477;&#20302;
&lt;/p&gt;
&lt;p&gt;
Improved Bayes Risk Can Yield Reduced Social Welfare Under Competition. (arXiv:2306.14670v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#31454;&#20105;&#29615;&#22659;&#19979;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#25552;&#39640;&#25968;&#25454;&#34920;&#31034;&#36136;&#37327;&#21487;&#33021;&#20250;&#23548;&#33268;&#20379;&#24212;&#21830;&#25972;&#20307;&#39044;&#27979;&#20934;&#30830;&#24615;&#38477;&#20302;&#65292;&#20174;&#32780;&#38477;&#20302;&#31038;&#20250;&#31119;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#32553;&#25918;&#23450;&#24459;&#31561;&#36235;&#21183;&#39044;&#35745;&#20250;&#23548;&#33268;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36235;&#21183;&#21482;&#32771;&#34385;&#20102;&#21333;&#20010;&#27169;&#22411;&#20379;&#24212;&#21830;&#30340;&#35270;&#35282;&#65292;&#32780;&#23454;&#38469;&#19978;&#20379;&#24212;&#21830;&#20043;&#38388;&#24120;&#24120;&#31454;&#20105;&#29992;&#25143;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#31454;&#20105;&#21487;&#20197;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#36825;&#20123;&#32553;&#25918;&#36235;&#21183;&#30340;&#34892;&#20026;&#65292;&#29978;&#33267;&#21487;&#33021;&#36896;&#25104;&#25972;&#20307;&#39044;&#27979;&#20934;&#30830;&#24615;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#22823;&#32780;&#38750;&#21333;&#35843;&#25110;&#38477;&#20302;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#20998;&#31867;&#20219;&#21153;&#30340;&#31454;&#20105;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#34920;&#31034;&#20316;&#20026;&#30740;&#31350;&#35268;&#27169;&#22686;&#21152;&#30340;&#24433;&#21709;&#30340;&#38236;&#22836;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#19968;&#23478;&#24066;&#22330;&#19978;&#65292;&#25913;&#21892;&#25968;&#25454;&#34920;&#31034;&#36136;&#37327;&#65288;&#25353;&#36125;&#21494;&#26031;&#39118;&#38505;&#35745;&#37327;&#65289;&#21487;&#33021;&#20250;&#38477;&#20302;&#31454;&#20105;&#27169;&#22411;&#20379;&#24212;&#21830;&#30340;&#25972;&#20307;&#39044;&#27979;&#20934;&#30830;&#24615;&#65288;&#21363;&#31038;&#20250;&#31119;&#21033;&#65289;&#12290;&#25105;&#20204;&#30340;&#20363;&#23376;&#28085;&#30422;&#20102;&#31616;&#21333;&#35774;&#32622;&#20013;&#30340;&#23553;&#38381;&#24335;&#20844;&#24335;&#21040;&#39044;&#35757;&#32451;&#30340; CIFAR-10 &#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the scale of machine learning models increases, trends such as scaling laws anticipate consistent downstream improvements in predictive accuracy. However, these trends take the perspective of a single model-provider in isolation, while in reality providers often compete with each other for users. In this work, we demonstrate that competition can fundamentally alter the behavior of these scaling trends, even causing overall predictive accuracy across users to be non-monotonic or decreasing with scale. We define a model of competition for classification tasks, and use data representations as a lens for studying the impact of increases in scale. We find many settings where improving data representation quality (as measured by Bayes risk) decreases the overall predictive accuracy across users (i.e., social welfare) for a marketplace of competing model-providers. Our examples range from closed-form formulas in simple settings to simulations with pretrained representations on CIFAR-10. At
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#27931;&#20234;&#27169;&#22411;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#28608;&#27963;&#20248;&#21270;&#20026;&#27169;&#22411;&#21019;&#24314;&#31614;&#21517;&#65292;&#28982;&#21518;&#35757;&#32451;&#20998;&#31867;&#22120;&#26469;&#26816;&#27979;&#29305;&#27931;&#20234;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04877</link><description>&lt;p&gt;
&#20351;&#29992;&#28608;&#27963;&#20248;&#21270;&#36827;&#34892;&#29305;&#27931;&#20234;&#27169;&#22411;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Trojan Model Detection Using Activation Optimization. (arXiv:2306.04877v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#27931;&#20234;&#27169;&#22411;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#28608;&#27963;&#20248;&#21270;&#20026;&#27169;&#22411;&#21019;&#24314;&#31614;&#21517;&#65292;&#28982;&#21518;&#35757;&#32451;&#20998;&#31867;&#22120;&#26469;&#26816;&#27979;&#29305;&#27931;&#20234;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#25454;&#30340;&#19981;&#21487;&#29992;&#24615;&#25110;&#22823;&#35268;&#27169;&#65292;&#20197;&#21450;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39640;&#35745;&#31639;&#21644;&#20154;&#21147;&#25104;&#26412;&#65292;&#36890;&#24120;&#20250;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#20381;&#36182;&#20110;&#24320;&#28304;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#20174;&#23433;&#20840;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#20570;&#27861;&#38750;&#24120;&#20196;&#20154;&#25285;&#24551;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#20250;&#34987;&#24863;&#26579;&#29305;&#27931;&#20234;&#25915;&#20987;&#65292;&#22312;&#36825;&#31181;&#25915;&#20987;&#20013;&#65292;&#25915;&#20987;&#32773;&#23884;&#20837;&#19968;&#20010;&#35302;&#21457;&#22120;&#22312;&#27169;&#22411;&#20013;&#65292;&#20351;&#24471;&#24403;&#35302;&#21457;&#22120;&#23384;&#22312;&#20110;&#36755;&#20837;&#20013;&#26102;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#25511;&#21046;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#27931;&#20234;&#27169;&#22411;&#26816;&#27979;&#26041;&#27861;&#30340;&#21021;&#27493;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26681;&#25454;&#28608;&#27963;&#20248;&#21270;&#20026;&#27169;&#22411;&#21019;&#24314;&#31614;&#21517;&#12290;&#28982;&#21518;&#35757;&#32451;&#20998;&#31867;&#22120;&#26469;&#26816;&#27979;&#29305;&#27931;&#20234;&#27169;&#22411;&#24182;&#32473;&#20986;&#20854;&#31614;&#21517;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to data's unavailability or large size, and the high computational and human labor costs of training machine learning models, it is a common practice to rely on open source pre-trained models whenever possible. However, this practice is worry some from the security perspective. Pre-trained models can be infected with Trojan attacks, in which the attacker embeds a trigger in the model such that the model's behavior can be controlled by the attacker when the trigger is present in the input. In this paper, we present our preliminary work on a novel method for Trojan model detection. Our method creates a signature for a model based on activation optimization. A classifier is then trained to detect a Trojan model given its signature. Our method achieves state of the art performance on two public datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#32622;&#25442;&#19981;&#21464;&#30340;&#39640;&#32500;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21518;&#23558;&#20854;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#25490;&#38500;&#24322;&#24120;&#30340;&#21943;&#27880;&#12290;</title><link>http://arxiv.org/abs/2306.03933</link><description>&lt;p&gt;
&#39640;&#32500;&#21644;&#32622;&#25442;&#19981;&#21464;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional and Permutation Invariant Anomaly Detection. (arXiv:2306.03933v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03933
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#32622;&#25442;&#19981;&#21464;&#30340;&#39640;&#32500;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21518;&#23558;&#20854;&#29992;&#20110;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#25490;&#38500;&#24322;&#24120;&#30340;&#21943;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23398;&#20064;&#39640;&#32500;&#27010;&#29575;&#23494;&#24230;&#30340;&#22256;&#38590;&#65292;&#26032;&#29289;&#29702;&#36807;&#31243;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#23616;&#38480;&#20110;&#20302;&#32500;&#31354;&#38388;&#12290;&#29305;&#21035;&#26159;&#22312;&#25104;&#20998;&#32423;&#21035;&#19978;&#65292;&#23558;&#32622;&#25442;&#19981;&#21464;&#24615;&#21644;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#31561;&#33391;&#22909;&#24615;&#36136;&#21512;&#24182;&#21040;&#27969;&#34892;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#20013;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31890;&#23376;&#29289;&#29702;&#25968;&#25454;&#32622;&#25442;&#19981;&#21464;&#23494;&#24230;&#20272;&#35745;&#22120;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#23398;&#20064;&#21040;&#30340;&#23494;&#24230;&#29992;&#20316;&#32622;&#25442;&#19981;&#21464;&#30340;&#24322;&#24120;&#26816;&#27979;&#35780;&#20998;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#21151;&#25928;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#22312;&#20165;&#20855;&#22791;&#32972;&#26223;&#20551;&#35774;&#19979;&#30340;&#21487;&#33021;&#24615;&#36739;&#20302;&#30340;&#21943;&#27880;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#21040;&#30340;&#23494;&#24230;&#27604;&#19982;&#34987;&#30417;&#30563;&#20998;&#31867;&#31639;&#27861;&#33719;&#24471;&#30340;&#23494;&#24230;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for anomaly detection of new physics processes are often limited to low-dimensional spaces due to the difficulty of learning high-dimensional probability densities. Particularly at the constituent level, incorporating desirable properties such as permutation invariance and variable-length inputs becomes difficult within popular density estimation methods. In this work, we introduce a permutation-invariant density estimator for particle physics data based on diffusion models, specifically designed to handle variable-length inputs. We demonstrate the efficacy of our methodology by utilizing the learned density as a permutation-invariant anomaly detection score, effectively identifying jets with low likelihood under the background-only hypothesis. To validate our density estimation method, we investigate the ratio of learned densities and compare to those obtained by a supervised classification algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31561;&#21464;Transformer&#29992;&#20110;&#23398;&#20064;3D&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#21452;&#23618;&#27880;&#24847;&#21147;&#27169;&#22359;&#12289;&#21069;&#39304;&#27169;&#22359;&#21644;&#23618;&#24402;&#19968;&#21270;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#26159;E&#65288;3&#65289;&#31561;&#21464;&#30340;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#22359;&#32423;&#21644;&#21407;&#23376;&#32423;&#30340;&#20132;&#20114;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#39044;&#27979;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#20146;&#21644;&#21147;&#12289;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#37197;&#20307;&#25928;&#21147;&#26041;&#38754;&#20248;&#20110;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01474</link><description>&lt;p&gt;
&#36890;&#29992;&#31561;&#21464;Transformer&#65306;&#29992;&#20110;3D&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning. (arXiv:2306.01474v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31561;&#21464;Transformer&#29992;&#20110;&#23398;&#20064;3D&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#21452;&#23618;&#27880;&#24847;&#21147;&#27169;&#22359;&#12289;&#21069;&#39304;&#27169;&#22359;&#21644;&#23618;&#24402;&#19968;&#21270;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#26159;E&#65288;3&#65289;&#31561;&#21464;&#30340;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#22359;&#32423;&#21644;&#21407;&#23376;&#32423;&#30340;&#20132;&#20114;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#39044;&#27979;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#20146;&#21644;&#21147;&#12289;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#37197;&#20307;&#25928;&#21147;&#26041;&#38754;&#20248;&#20110;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#23398;&#21644;&#33647;&#29289;&#24320;&#21457;&#20013;&#30340;&#35768;&#22810;&#36807;&#31243;&#28041;&#21450;&#19981;&#21516;&#20998;&#23376;&#20043;&#38388;&#30340;&#21508;&#31181;3D&#30456;&#20114;&#20316;&#29992;&#65292;&#20363;&#22914;&#34507;&#30333;&#36136;&#19982;&#34507;&#30333;&#36136;&#65292;&#34507;&#30333;&#36136;&#19982;&#23567;&#20998;&#23376;&#31561;&#12290;&#35774;&#35745;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#26469;&#23398;&#20064;&#26222;&#36866;&#30340;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#20294;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#20998;&#23376;&#36890;&#24120;&#20197;&#19981;&#21516;&#31890;&#24230;&#34920;&#31034;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#23558;3D&#20998;&#23376;&#36890;&#29992;&#34920;&#31034;&#20026;&#38598;&#21512;&#30340;&#20960;&#20309;&#22270;&#24418;&#22270;&#65292;&#19982;&#20256;&#32479;&#21333;&#23618;&#34920;&#31034;&#24418;&#24335;&#24418;&#25104;&#23545;&#27604;&#12290;&#22312;&#25552;&#20986;&#30340;&#32479;&#19968;&#34920;&#31034;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#29992;&#31561;&#21464;Transformer&#65288;GET&#65289;&#65292;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#31232;&#30095;&#22359;&#32423;&#21644;&#23494;&#38598;&#21407;&#23376;&#32423;&#20132;&#20114;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GET&#30001;&#21452;&#23618;&#27880;&#24847;&#21147;&#27169;&#22359;&#12289;&#21069;&#39304;&#27169;&#22359;&#21644;&#23618;&#24402;&#19968;&#21270;&#27169;&#22359;&#32452;&#25104;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#26159;E&#65288;3&#65289;&#31561;&#21464;&#30340;&#65292;&#20197;&#28385;&#36275;3D&#19990;&#30028;&#30340;&#23545;&#31216;&#24615;&#12290;&#22312;&#39044;&#27979;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#20146;&#21644;&#21147;&#12289;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#21644;&#37197;&#20307;&#25928;&#21147;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#34920;&#26126;GET&#20248;&#20110;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many processes in biology and drug discovery involve various 3D interactions between different molecules, such as protein and protein, protein and small molecule, etc. Designing a generalist model to learn universal molecular interactions is valuable yet challenging, given that different molecules are usually represented in different granularity. In this paper, we first propose to universally represent a 3D molecule as a geometric graph of sets, in contrast to conventional single-level representations. Upon the proposed unified representation, we then propose a Generalist Equivariant Transformer (GET) to effectively capture both sparse block-level and dense atom-level interactions. To be specific, GET consists of a bilevel attention module, a feed-forward module and a layer normalization module, where, notably, each module is E(3) equivariant to meet the symmetry of 3D world. Extensive experiments on the prediction of protein-protein affinity, ligand binding affinity, and ligand effica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#33258;&#33976;&#39311;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#35757;&#32451;&#23458;&#25143;&#31471;&#27169;&#22411;&#65292;&#35813;&#27491;&#21017;&#21270;&#26041;&#26696;&#22522;&#20110;&#23458;&#25143;&#31471;&#26412;&#22320;&#27169;&#22411;&#39044;&#27979;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#23458;&#25143;&#31471;&#30340;&#26631;&#31614;&#20998;&#24067;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#30446;&#21069;&#27969;&#34892;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.19600</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#33258;&#33976;&#39311;&#19979;&#30340;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning on Heterogeneous Data via Adaptive Self-Distillation. (arXiv:2305.19600v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#33258;&#33976;&#39311;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#35757;&#32451;&#23458;&#25143;&#31471;&#27169;&#22411;&#65292;&#35813;&#27491;&#21017;&#21270;&#26041;&#26696;&#22522;&#20110;&#23458;&#25143;&#31471;&#26412;&#22320;&#27169;&#22411;&#39044;&#27979;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#23458;&#25143;&#31471;&#30340;&#26631;&#31614;&#20998;&#24067;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#30446;&#21069;&#27969;&#34892;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#20351;&#24471;&#23458;&#25143;&#26426;&#21487;&#20197;&#32858;&#21512;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#32780;&#26080;&#38656;&#20849;&#20139;&#20219;&#20309;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#20174;&#32780;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23454;&#36341;&#20013;&#21457;&#29616;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#35266;&#23519;&#21040;&#30340;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#22343;&#21248;&#24615;&#65288;&#20363;&#22914;&#31867;&#21035;&#19981;&#24179;&#34913;&#65289;&#12290;&#22312;&#36825;&#31181;&#19981;&#22343;&#21248;&#30340;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#20250;&#20986;&#29616;&#8220;&#23458;&#25143;&#26426;&#28418;&#31227;&#8221;&#38382;&#39064;&#65292;&#23548;&#33268;&#27599;&#20010;&#23458;&#25143;&#31471;&#25910;&#25947;&#21040;&#20854;&#33258;&#24049;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#36825;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#24182;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#33258;&#33976;&#39311;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#35757;&#32451;&#23458;&#25143;&#31471;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#22522;&#20110;&#23458;&#25143;&#31471;&#26412;&#22320;&#27169;&#22411;&#39044;&#27979;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#23458;&#25143;&#31471;&#30340;&#26631;&#31614;&#20998;&#24067;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#27491;&#21017;&#21270;&#25216;&#26415;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#22312;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#20043;&#19978;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#23458;&#25143;&#31471;&#25110;&#26381;&#21153;&#22120;&#20195;&#30721;&#36827;&#34892;&#20219;&#20309;&#26356;&#25913;&#65292;&#22240;&#27492;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#37096;&#32626;&#24615;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning paradigm that enables clients to jointly train a global model by aggregating the locally trained models without sharing any local training data. In practice, there can often be substantial heterogeneity (e.g., class imbalance) across the local data distributions observed by each of these clients. Under such non-iid data distributions across clients, FL suffers from the 'client-drift' problem where every client converges to its own local optimum. This results in slower convergence and poor performance of the aggregated model. To address this limitation, we propose a novel regularization technique based on adaptive self-distillation (ASD) for training models on the client side. Our regularization scheme adaptively adjusts to the client's training data based on: (1) the closeness of the local model's predictions with that of the global model and (2) the client's label distribution. The proposed regularization can be easily integrated atop exis
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;RNNs&#36924;&#36817;&#38750;&#32447;&#24615;&#24207;&#21015;&#20851;&#31995;&#30340;&#36870;&#36817;&#20284;&#23450;&#29702;&#65292;&#36827;&#19968;&#27493;&#23558;&#20808;&#21069;&#22312;&#32447;&#24615;RNNs&#20013;&#35782;&#21035;&#20986;&#30340;&#35760;&#24518;&#38590;&#39064;&#25512;&#24191;&#21040;&#20102;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.19190</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#36870;&#36817;&#20284;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Inverse Approximation Theory for Nonlinear Recurrent Neural Networks. (arXiv:2305.19190v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#20351;&#29992;RNNs&#36924;&#36817;&#38750;&#32447;&#24615;&#24207;&#21015;&#20851;&#31995;&#30340;&#36870;&#36817;&#20284;&#23450;&#29702;&#65292;&#36827;&#19968;&#27493;&#23558;&#20808;&#21069;&#22312;&#32447;&#24615;RNNs&#20013;&#35782;&#21035;&#20986;&#30340;&#35760;&#24518;&#38590;&#39064;&#25512;&#24191;&#21040;&#20102;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;RNNs&#26469;&#36924;&#36817;&#38750;&#32447;&#24615;&#24207;&#21015;&#20851;&#31995;&#30340;&#36870;&#36817;&#20284;&#23450;&#29702;&#12290;&#36825;&#26159;&#36817;&#20284;&#29702;&#35770;&#20013;&#30340;&#19968;&#31181;&#31216;&#20026;Bernstein&#22411;&#32467;&#26524;&#30340;&#32467;&#26524;&#65292;&#23427;&#22312;&#20551;&#35774;&#30446;&#26631;&#20989;&#25968;&#21487;&#20197;&#36890;&#36807;&#20551;&#35774;&#31354;&#38388;&#26377;&#25928;&#36924;&#36817;&#30340;&#26465;&#20214;&#19979;&#25512;&#23548;&#20986;&#30446;&#26631;&#20989;&#25968;&#30340;&#23646;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38750;&#32447;&#24615;&#24207;&#21015;&#20851;&#31995;&#21487;&#20197;&#34987;&#20855;&#26377;hardtanh/tanh&#28608;&#27963;&#20989;&#25968;&#30340;RNNs&#31283;&#23450;&#36924;&#36817;&#30340;&#26102;&#20505;&#65292;&#24517;&#39035;&#20855;&#26377;&#19968;&#20010;&#25351;&#25968;&#34928;&#20943;&#30340;&#35760;&#24518;&#32467;&#26500;--&#36825;&#20010;&#27010;&#24565;&#21487;&#20197;&#34987;&#26126;&#30830;&#23450;&#20041;&#12290;&#36825;&#23558;&#20808;&#21069;&#22312;&#32447;&#24615;RNNs&#20013;&#35782;&#21035;&#20986;&#30340;&#35760;&#24518;&#38590;&#39064;&#25512;&#24191;&#21040;&#20102;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#24773;&#20917;&#65292;&#24182;&#37327;&#21270;&#20102;RNN&#26550;&#26500;&#22312;&#23398;&#20064;&#20855;&#26377;&#38271;&#26399;&#35760;&#24518;&#30340;&#24207;&#21015;&#20851;&#31995;&#26102;&#30340;&#37325;&#35201;&#38480;&#21046;&#12290;&#22522;&#20110;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21407;&#21017;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#36827;&#34892;&#20102;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove an inverse approximation theorem for the approximation of nonlinear sequence-to-sequence relationships using RNNs. This is a so-called Bernstein-type result in approximation theory, which deduces properties of a target function under the assumption that it can be effectively approximated by a hypothesis space. In particular, we show that nonlinear sequence relationships, viewed as functional sequences, that can be stably approximated by RNNs with hardtanh/tanh activations must have an exponential decaying memory structure -- a notion that can be made precise. This extends the previously identified curse of memory in linear RNNs into the general nonlinear setting, and quantifies the essential limitations of the RNN architecture for learning sequential relationships with long-term memory. Based on the analysis, we propose a principled reparameterization method to overcome the limitations. Our theoretical results are confirmed by numerical experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Kernel-SSL&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#22312;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#20043;&#19978;&#24182;&#20248;&#21270;&#20102;&#20854;&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19979;&#34920;&#29616;&#26174;&#33879;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;4.6%&#12290;</title><link>http://arxiv.org/abs/2305.17326</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#26680;KL&#25955;&#24230;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;Kernel-SSL
&lt;/p&gt;
&lt;p&gt;
Kernel-SSL: Kernel KL Divergence for Self-supervised Learning. (arXiv:2305.17326v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Kernel-SSL&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24314;&#31435;&#22312;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#20043;&#19978;&#24182;&#20248;&#21270;&#20102;&#20854;&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19979;&#34920;&#29616;&#26174;&#33879;&#36229;&#36234;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#36890;&#24120;&#23558;&#19968;&#20010;&#27491;&#38170;&#28857;&#26679;&#26412;&#19982;&#35768;&#22810;&#36127;&#26679;&#26412;&#36827;&#34892;&#27604;&#36739;&#65292;&#26469;&#23436;&#25104;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#12290;&#30456;&#21453;&#65292;&#38750;&#23545;&#27604;&#23398;&#20064;&#65292;&#20363;&#22914;BYOL&#12289;SimSiam&#21644;Barlow Twins&#31561;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#26174;&#24335;&#20351;&#29992;&#36127;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;SSL&#12290;&#21463;&#23545;&#27604;&#23398;&#20064;&#29616;&#26377;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22810;&#31181;&#29616;&#26377;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#29702;&#35299;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Kernel-SSL&#65292;&#30452;&#25509;&#20248;&#21270;RKHS&#20013;&#30340;&#22343;&#20540;&#23884;&#20837;&#21644;&#21327;&#26041;&#24046;&#31639;&#23376;&#12290;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;Kernel-SSL&#22312;&#32447;&#24615;&#35780;&#20272;&#35774;&#32622;&#19979;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#22823;&#24133;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#36827;&#34892;100&#20010;epoch&#30340;&#39044;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;SimCLR&#34920;&#29616;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning usually compares one positive anchor sample with lots of negative samples to perform Self-Supervised Learning (SSL). Alternatively, non-contrastive learning, as exemplified by methods like BYOL, SimSiam, and Barlow Twins, accomplishes SSL without the explicit use of negative samples. Inspired by the existing analysis for contrastive learning, we provide a reproducing kernel Hilbert space (RKHS) understanding of many existing non-contrastive learning methods. Subsequently, we propose a novel loss function, Kernel-SSL, which directly optimizes the mean embedding and the covariance operator within the RKHS. In experiments, our method Kernel-SSL outperforms state-of-the-art methods by a large margin on ImageNet datasets under the linear evaluation settings. Specifically, when performing 100 epochs pre-training, our method outperforms SimCLR by 4.6%.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312; mini-batch &#20013;&#26174;&#24335;&#22320;&#23398;&#20064;&#35823;&#24046;&#30340;&#24207;&#21015;&#30456;&#20851;&#24615;&#65292;&#26469;&#25552;&#39640;&#28145;&#24230;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.17028</link><description>&lt;p&gt;
&#28145;&#24230;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26356;&#22909;Batch&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Better Batch for Deep Probabilistic Time Series Forecasting. (arXiv:2305.17028v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312; mini-batch &#20013;&#26174;&#24335;&#22320;&#23398;&#20064;&#35823;&#24046;&#30340;&#24207;&#21015;&#30456;&#20851;&#24615;&#65292;&#26469;&#25552;&#39640;&#28145;&#24230;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22240;&#20854;&#33021;&#22815;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#27169;&#22411;&#36807;&#20110;&#31616;&#21333;&#21270;&#38382;&#39064;&#65292;&#20551;&#35774;&#35823;&#24046;&#36807;&#31243;&#26159;&#19982;&#26102;&#38388;&#26080;&#20851;&#30340;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#35823;&#24046;&#36807;&#31243;&#20013;&#30340;&#24207;&#21015;&#30456;&#20851;&#24615;&#12290;&#36825;&#21487;&#33021;&#20250;&#38477;&#20302;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#20351;&#36825;&#20123;&#27169;&#22411;&#23545;&#20915;&#31574;&#24615;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#20943;&#24369;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#35823;&#24046;&#33258;&#30456;&#20851;&#24615;&#32435;&#20837;&#32771;&#34385;&#65292;&#20197;&#22686;&#24378;&#27010;&#29575;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#26500;&#36896;&#19968;&#20010;mini-batch&#65292;&#20316;&#20026;$D$&#20010;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#27573;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#26174;&#24335;&#22320;&#23398;&#20064;&#19968;&#20010;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#35206;&#30422;&#20102;&#30456;&#37051;&#26102;&#38388;&#27493;&#20043;&#38388;&#30340;&#35823;&#24046;&#30456;&#20851;&#24615;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#21487;&#29992;&#20110;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#22686;&#24378;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep probabilistic time series forecasting has gained significant attention due to its ability to provide valuable uncertainty quantification for decision-making tasks. However, many existing models oversimplify the problem by assuming the error process is time-independent, thereby overlooking the serial correlation in the error process. This oversight can potentially diminish the accuracy of the forecasts, rendering these models less effective for decision-making purposes. To overcome this limitation, we propose an innovative training method that incorporates error autocorrelation to enhance the accuracy of probabilistic forecasting. Our method involves constructing a mini-batch as a collection of $D$ consecutive time series segments for model training and explicitly learning a covariance matrix over each mini-batch that encodes the error correlation among adjacent time steps. The resulting covariance matrix can be used to improve prediction accuracy and enhance uncertainty quantifica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35889;&#35282;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21457;&#29616;GNNs&#22312;&#24230;&#20998;&#24067;&#21644;&#35889;&#20998;&#24067;&#20559;&#31227;&#26102;&#22343;&#34920;&#29616;&#25935;&#24863;&#65292;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#25581;&#31034;&#20102; GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15611</link><description>&lt;p&gt;
&#22522;&#20110;&#35889;&#35282;&#24230;&#21078;&#26512;&#29983;&#29289;&#25968;&#25454;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#65306;&#35266;&#28857;&#21644;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Size Generalizability of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective. (arXiv:2305.15611v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35889;&#35282;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21457;&#29616;GNNs&#22312;&#24230;&#20998;&#24067;&#21644;&#35889;&#20998;&#24067;&#20559;&#31227;&#26102;&#22343;&#34920;&#29616;&#25935;&#24863;&#65292;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#25581;&#31034;&#20102; GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#26159;&#21542;&#20855;&#26377;&#20174;&#23567;&#22270;&#20013;&#23398;&#20064;&#30340;&#30693;&#35782;&#21487;&#25512;&#24191;&#21040;&#21516;&#19968;&#39046;&#22495;&#30340;&#22823;&#22270;&#20013;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#22823;&#23567;&#30340;&#22270;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#23588;&#20854;&#26159;&#24230;&#20998;&#24067;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#29289;&#25968;&#25454;&#38598;&#20013;&#65292;&#24230;&#25968;&#26159;&#26377;&#30028;&#30340;&#65292;&#22240;&#27492;&#24230;&#20998;&#24067;&#30340;&#20559;&#31227;&#24456;&#23567;&#12290;&#21363;&#20351;&#24230;&#20998;&#24067;&#20559;&#31227;&#24456;&#23567;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;GNNs&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#26263;&#31034;&#26377;&#20854;&#20182;&#21407;&#22240;&#12290;&#20107;&#23454;&#19978;&#65292;&#20197;&#24448;&#23545;&#20110;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#21508;&#31181;&#22270;&#23610;&#23544;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#31867;&#22411;&#21644;&#23646;&#24615;&#30340;&#25506;&#32034;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#20197;&#21069;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#20998;&#26512;&#22823;&#22810;&#38598;&#20013;&#22312;&#31354;&#38388;&#39046;&#22495;&#12290;&#20026;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#37319;&#29992;&#35889;&#35282;&#24230;&#21435;&#30740;&#31350;GNNs&#22312;&#29983;&#29289;&#22270;&#25968;&#25454;&#19978;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#19968;&#20010;&#26032;&#26694;&#26550;&#26469;&#27169;&#25311;&#21508;&#31181;&#31867;&#22411;&#30340;&#24230;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#21033;&#29992;&#23427;&#26469;&#27979;&#35797;GNNs &#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#38500;&#20102;&#24230;&#20998;&#24067;&#20559;&#31227;&#22806;&#65292;GNNs &#36824;&#23545;&#22270;&#22823;&#23567;&#21464;&#21270;&#24341;&#36215;&#30340;&#35889;&#20998;&#24067;&#20559;&#31227;&#24456;&#25935;&#24863;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;GNN&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;&#65292;&#19968;&#20123;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#20855;&#26377;&#23610;&#23544;&#27867;&#21270;&#24615;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20851;&#20110;GNNs&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#30340;&#26032;&#35266;&#28857;&#21644;&#23454;&#36341;&#65292;&#24182;&#20026;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#27934;&#23519;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the question of whether the knowledge learned by graph neural networks (GNNs) from small graphs is generalizable to large graphs in the same domain. Prior works suggest that the distribution shift, particularly in the degree distribution, between graphs of different sizes can lead to performance degradation in the graph classification task. However, this may not be the case for biological datasets where the degrees are bounded and the distribution shift of degrees is small. Even with little degree distribution shift, our observations show that GNNs' performance on larger graphs from the same datasets still degrades, suggesting other causes. In fact, there has been a lack of exploration in real datasets to understand the types and properties of distribution shifts caused by various graph sizes. Furthermore, previous analyses of size generalizability mostly focus on the spatial domain.  To fill these gaps, we take the spectral perspective and study the size generalizabilit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#21322;&#23545;&#20598;&#20844;&#24335;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;OT&#65292;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;OT&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14777</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#21322;&#23545;&#20598;&#20844;&#24335;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling through the Semi-dual Formulation of Unbalanced Optimal Transport. (arXiv:2305.14777v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#21322;&#23545;&#20598;&#20844;&#24335;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;OT&#65292;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;OT&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#38382;&#39064;&#30740;&#31350;&#19968;&#31181;&#36816;&#36755;&#26144;&#23556;&#65292;&#33021;&#22815;&#22312;&#26368;&#23567;&#21270;&#32473;&#23450;&#25104;&#26412;&#20989;&#25968;&#30340;&#21516;&#26102;&#36830;&#25509;&#20004;&#20010;&#20998;&#24067;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;OT&#24050;&#34987;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#20013;&#30340;&#21487;&#36861;&#28335;&#30340;&#20808;&#39564;&#20998;&#24067;&#21644;&#25968;&#25454;&#20043;&#38388;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;OT&#30340;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#31163;&#32676;&#28857;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#35757;&#32451;&#26399;&#38388;&#38754;&#20020;&#20248;&#21270;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#65288;UOT&#65289;&#21322;&#23545;&#20598;&#20844;&#24335;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#12290;&#19982;OT&#19981;&#21516;&#65292;UOT&#28040;&#38500;&#20102;&#20998;&#24067;&#21305;&#37197;&#30340;&#30828;&#24615;&#32422;&#26463;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#23545;&#25239;&#31163;&#32676;&#28857;&#30340;&#40065;&#26834;&#24615;&#65292;&#35757;&#32451;&#26399;&#38388;&#30340;&#31283;&#23450;&#24615;&#20197;&#21450;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;UOT&#20043;&#38388;&#20998;&#24067;&#24046;&#24322;&#30340;&#29702;&#35770;&#19978;&#38480;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;OT&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;CIFAR-10&#21644;CelebA-HQ-256&#19978;&#23454;&#29616;&#20102;&#20998;&#21035;&#20026;2.97&#21644;5.80&#30340;FID&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport (OT) problem investigates a transport map that bridges two distributions while minimizing a given cost function. In this regard, OT between tractable prior distribution and data has been utilized for generative modeling tasks. However, OT-based methods are susceptible to outliers and face optimization challenges during training. In this paper, we propose a novel generative model based on the semi-dual formulation of Unbalanced Optimal Transport (UOT). Unlike OT, UOT relaxes the hard constraint on distribution matching. This approach provides better robustness against outliers, stability during training, and faster convergence. We validate these properties empirically through experiments. Moreover, we study the theoretical upper-bound of divergence between distributions in UOT. Our model outperforms existing OT-based generative models, achieving FID scores of 2.97 on CIFAR-10 and 5.80 on CelebA-HQ-256.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#35299;&#20915;&#22312;&#20445;&#25252;&#38544;&#31169;&#21644;&#28385;&#36275;&#20869;&#23384;&#21644;&#25512;&#29702;&#26102;&#38388;&#35201;&#27714;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#19968;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#20197;&#26368;&#22823;&#21270;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26694;&#26550;&#30340;&#20851;&#38190;&#26159;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#26377;&#36873;&#25321;&#24615;&#30340;&#39044;&#35757;&#32451;&#65292;&#20351;&#20844;&#20849;&#20998;&#24067;&#38752;&#36817;&#31169;&#26377;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2305.13865</link><description>&lt;p&gt;
&#38024;&#23545;&#31169;&#26377;&#24494;&#35843;&#30340;&#26377;&#36873;&#25321;&#24615;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Selective Pre-training for Private Fine-tuning. (arXiv:2305.13865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#35299;&#20915;&#22312;&#20445;&#25252;&#38544;&#31169;&#21644;&#28385;&#36275;&#20869;&#23384;&#21644;&#25512;&#29702;&#26102;&#38388;&#35201;&#27714;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#19968;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#20197;&#26368;&#22823;&#21270;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26694;&#26550;&#30340;&#20851;&#38190;&#26159;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#26377;&#36873;&#25321;&#24615;&#30340;&#39044;&#35757;&#32451;&#65292;&#20351;&#20844;&#20849;&#20998;&#24067;&#38752;&#36817;&#31169;&#26377;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#25105;&#20204;&#24819;&#22312;&#30005;&#23376;&#37038;&#20214;&#23458;&#25143;&#31471;&#25110;&#25991;&#23383;&#22788;&#29702;&#22120;&#20013;&#35757;&#32451;&#25991;&#26412;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#24517;&#39035;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#65292;&#24182;&#36981;&#23432;&#29305;&#23450;&#30340;&#22266;&#23450;&#22823;&#23567;&#65292;&#20197;&#28385;&#36275;&#20869;&#23384;&#21644;&#25512;&#29702;&#26102;&#38388;&#35201;&#27714;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26377;&#19968;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;D_pub&#21644;&#19968;&#20010;&#23545;&#24212;&#20110;&#19979;&#28216;&#20219;&#21153;T&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;D_priv&#12290;&#25105;&#20204;&#22914;&#20309;&#22312;D_pub&#19978;&#39044;&#35757;&#32451;&#19968;&#20010;&#22266;&#23450;&#22823;&#23567;&#30340;&#27169;&#22411;M&#65292;&#24182;&#22312;D_priv&#19978;&#24494;&#35843;&#23427;&#65292;&#20351;&#24471;M&#30456;&#23545;&#20110;T&#30340;&#24615;&#33021;&#26368;&#22823;&#21270;&#65292;&#24182;&#19988;M&#30456;&#23545;&#20110;D_priv&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#65311;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;D_pub&#30340;&#19968;&#20010;&#23376;&#38598;&#19978;&#39044;&#35757;&#32451;&#65292;&#23558;&#20844;&#20849;&#20998;&#24067;&#19982;&#31169;&#26377;&#20998;&#24067;&#38752;&#36817;&#65292;&#26159;&#26368;&#22823;&#21270;M&#39044;&#35757;&#32451;&#21518;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#29305;&#21035;&#26159;&#22312;&#27169;&#22411;&#22823;&#23567;&#30456;&#23545;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#12290;&#38500;&#20102;&#24615;&#33021;&#25913;&#36827;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#25552;&#20379;&#20102;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Suppose we want to train text prediction models in email clients or word processors. The models must preserve the privacy of user data and adhere to a specific fixed size to meet memory and inference time requirements. We introduce a generic framework to solve this problem. Specifically, we are given a public dataset $D_\text{pub}$ and a private dataset $D_\text{priv}$ corresponding to a downstream task $T$. How should we pre-train a fixed-size model $M$ on $D_\text{pub}$ and fine-tune it on $D_\text{priv}$ such that performance of $M$ with respect to $T$ is maximized and $M$ satisfies differential privacy with respect to $D_\text{priv}$? We show that pre-training on a {\em subset} of dataset $D_\text{pub}$ that brings the public distribution closer to the private distribution is a crucial ingredient to maximize the transfer learning abilities of $M$ after pre-training, especially in the regimes where model sizes are relatively small. Besides performance improvements, our framework als
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#23454;&#20102;&#24403;Transformer&#22788;&#29702;&#19968;&#31995;&#21015;token&#26102;&#65292;&#20986;&#29616;&#8220;&#39046;&#23548;&#32773;&#8221;&#30340;&#32463;&#39564;&#35266;&#23519;&#65292;&#21363;&#38543;&#30528;&#26102;&#38388;&#36235;&#20110;&#26080;&#31351;&#22823;&#65292;&#20195;&#34920;token&#30340;&#31890;&#23376;&#20250;&#32858;&#38598;&#22312;&#29305;&#23450;&#30340;&#26497;&#38480;&#23545;&#35937;&#38468;&#36817;&#65292;&#36825;&#21462;&#20915;&#20110;&#20215;&#20540;&#30697;&#38453;&#30340;&#35889;&#12290;</title><link>http://arxiv.org/abs/2305.05465</link><description>&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#21160;&#24577;&#20013;&#30340;&#32858;&#31867;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
The emergence of clusters in self-attention dynamics. (arXiv:2305.05465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#23454;&#20102;&#24403;Transformer&#22788;&#29702;&#19968;&#31995;&#21015;token&#26102;&#65292;&#20986;&#29616;&#8220;&#39046;&#23548;&#32773;&#8221;&#30340;&#32463;&#39564;&#35266;&#23519;&#65292;&#21363;&#38543;&#30528;&#26102;&#38388;&#36235;&#20110;&#26080;&#31351;&#22823;&#65292;&#20195;&#34920;token&#30340;&#31890;&#23376;&#20250;&#32858;&#38598;&#22312;&#29305;&#23450;&#30340;&#26497;&#38480;&#23545;&#35937;&#38468;&#36817;&#65292;&#36825;&#21462;&#20915;&#20110;&#20215;&#20540;&#30697;&#38453;&#30340;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;Transformer&#35270;&#20026;&#30456;&#20114;&#20316;&#29992;&#30340;&#31890;&#23376;&#31995;&#32479;&#65292;&#24403;&#26435;&#37325;&#19981;&#38543;&#26102;&#38388;&#21464;&#21270;&#26102;&#65292;&#26412;&#25991;&#25551;&#36848;&#20102;&#23398;&#20064;&#34920;&#31034;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20195;&#34920;token&#30340;&#31890;&#23376;&#38543;&#30528;&#26102;&#38388;&#36235;&#20110;&#26080;&#31351;&#22823;&#32780;&#36235;&#21521;&#20110;&#29305;&#23450;&#30340;&#26497;&#38480;&#23545;&#35937;&#12290;&#20986;&#29616;&#30340;&#26497;&#38480;&#23545;&#35937;&#31867;&#22411;&#21462;&#20915;&#20110;&#20215;&#20540;&#30697;&#38453;&#30340;&#35889;&#12290;&#27492;&#22806;&#65292;&#22312;&#19968;&#32500;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#27880;&#24847;&#21147;&#30697;&#38453;&#25910;&#25947;&#20110;&#20302;&#31209;&#24067;&#23572;&#30697;&#38453;&#12290;&#36825;&#20123;&#32467;&#26524;&#30340;&#32452;&#21512;&#22312;&#25968;&#23398;&#19978;&#35777;&#23454;&#20102;Vaswani&#31561;&#20154;&#30340;&#32463;&#39564;&#35266;&#23519;&#65292;&#21363;Transformer&#22788;&#29702;&#19968;&#31995;&#21015;token&#26102;&#20250;&#20986;&#29616;&#8220;&#39046;&#23548;&#32773;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Viewing Transformers as interacting particle systems, we describe the geometry of learned representations when the weights are not time dependent. We show that particles, representing tokens, tend to cluster toward particular limiting objects as time tends to infinity. The type of limiting object that emerges depends on the spectrum of the value matrix. Additionally, in the one-dimensional case we prove that the self-attention matrix converges to a low-rank Boolean matrix. The combination of these results mathematically confirms the empirical observation made by Vaswani et al. \cite{vaswani2017attention} that \emph{leaders} appear in a sequence of tokens when processed by Transformers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Kullback-Leibler Maillard Sampling (KL-MS)&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#26377;&#30028;&#22870;&#21169;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#23454;&#29616;KL&#31354;&#38388;&#30340;&#25193;&#23637;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#28176;&#36817;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.14989</link><description>&lt;p&gt;
Kullback-Leibler Maillard&#37319;&#26679;&#22312;&#26377;&#30028;&#22870;&#21169;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Kullback-Leibler Maillard Sampling for Multi-armed Bandits with Bounded Rewards. (arXiv:2304.14989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Kullback-Leibler Maillard Sampling (KL-MS)&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#26377;&#30028;&#22870;&#21169;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#23454;&#29616;KL&#31354;&#38388;&#30340;&#25193;&#23637;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#28176;&#36817;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22870;&#21169;&#20998;&#24067;&#38598;&#20013;&#22312;&#21306;&#38388;$[0,1]$&#20869;&#30340;$K$&#33218;&#25968;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Kullback-Leibler Maillard Sampling (KL-MS)&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#26159;Maillard&#37319;&#26679;&#22312;KL&#31354;&#38388;&#30340;&#33258;&#28982;&#25193;&#23637;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;KL-MS&#22312;Bernoulli&#22870;&#21169;&#26102;&#20855;&#26377;&#28176;&#36817;&#26368;&#20248;&#24615;&#33021;&#65292;&#20854;&#26368;&#22351;&#24773;&#20917;&#36951;&#25022;&#24230;&#19978;&#30028;&#20026;$O(\sqrt{\mu^*(1-\mu^*) K T \ln K} + K \ln T)$&#65292;&#20854;&#20013;$\mu^*$&#26159;&#26368;&#20248;&#33218;&#30340;&#26399;&#26395;&#22870;&#21169;&#65292;$T$&#26159;&#26102;&#27573;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study $K$-armed bandit problems where the reward distributions of the arms are all supported on the $[0,1]$ interval. It has been a challenge to design regret-efficient randomized exploration algorithms in this setting. Maillard sampling~\cite{maillard13apprentissage}, an attractive alternative to Thompson sampling, has recently been shown to achieve competitive regret guarantees in the sub-Gaussian reward setting~\cite{bian2022maillard} while maintaining closed-form action probabilities, which is useful for offline policy evaluation. In this work, we propose the Kullback-Leibler Maillard Sampling (KL-MS) algorithm, a natural extension of Maillard sampling for achieving KL-style gap-dependent regret bound. We show that KL-MS enjoys the asymptotic optimality when the rewards are Bernoulli and has a worst-case regret bound of the form $O(\sqrt{\mu^*(1-\mu^*) K T \ln K} + K \ln T)$, where $\mu^*$ is the expected reward of the optimal arm, and $T$ is the time horizon length.
&lt;/p&gt;</description></item><item><title>MUDiff &#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#25193;&#25955;&#36807;&#31243;&#26469;&#29983;&#25104;&#20840;&#38754;&#30340;&#20998;&#23376;&#34920;&#31034;&#12290;&#20351;&#29992;&#25193;&#25955;&#36807;&#31243;&#21487;&#20197;&#25429;&#25417;&#20998;&#23376;&#36807;&#31243;&#30340;&#27010;&#29575;&#26412;&#36136;&#65292;&#24182;&#25506;&#32034;&#19981;&#21516;&#22240;&#32032;&#23545;&#20998;&#23376;&#32467;&#26500;&#21644;&#24615;&#36136;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21253;&#25324;&#19968;&#20010;&#26032;&#39062;&#30340;&#22270;&#24418;&#36716;&#25442;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.14621</link><description>&lt;p&gt;
MUDiff: &#32479;&#19968;&#25193;&#25955;&#29983;&#25104;&#23436;&#25972;&#20998;&#23376;
&lt;/p&gt;
&lt;p&gt;
MUDiff: Unified Diffusion for Complete Molecule Generation. (arXiv:2304.14621v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14621
&lt;/p&gt;
&lt;p&gt;
MUDiff &#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#25193;&#25955;&#36807;&#31243;&#26469;&#29983;&#25104;&#20840;&#38754;&#30340;&#20998;&#23376;&#34920;&#31034;&#12290;&#20351;&#29992;&#25193;&#25955;&#36807;&#31243;&#21487;&#20197;&#25429;&#25417;&#20998;&#23376;&#36807;&#31243;&#30340;&#27010;&#29575;&#26412;&#36136;&#65292;&#24182;&#25506;&#32034;&#19981;&#21516;&#22240;&#32032;&#23545;&#20998;&#23376;&#32467;&#26500;&#21644;&#24615;&#36136;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21253;&#25324;&#19968;&#20010;&#26032;&#39062;&#30340;&#22270;&#24418;&#36716;&#25442;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23376;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#25193;&#25955;&#36807;&#31243;&#26469;&#29983;&#25104;&#20998;&#23376;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#29983;&#25104;&#20102;&#20998;&#23376;&#30340;&#20840;&#38754;&#34920;&#31034;&#65292;&#21253;&#25324;&#21407;&#23376;&#29305;&#24449;&#12289;&#20108;&#32500;&#31163;&#25955;&#20998;&#23376;&#32467;&#26500;&#21644;&#19977;&#32500;&#36830;&#32493;&#20998;&#23376;&#22352;&#26631;&#12290;&#20351;&#29992;&#25193;&#25955;&#36807;&#31243;&#21487;&#20197;&#25429;&#25417;&#20998;&#23376;&#36807;&#31243;&#30340;&#27010;&#29575;&#26412;&#36136;&#65292;&#24182;&#25506;&#32034;&#19981;&#21516;&#22240;&#32032;&#23545;&#20998;&#23376;&#32467;&#26500;&#21644;&#24615;&#36136;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24418;&#36716;&#25442;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#12290;&#36716;&#25442;&#22120;&#23545;&#27431;&#20960;&#37324;&#24471;&#21464;&#25442;&#26159;&#31561;&#21464;&#30340;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20064;&#19981;&#21464;&#30340;&#21407;&#23376;&#21644;&#36793;&#30028;&#34920;&#31034;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#23376;&#22352;&#26631;&#30340;&#31561;&#21464;&#24615;&#12290;&#36825;&#31181;&#36716;&#25442;&#22120;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#23545;&#20960;&#20309;&#21464;&#25442;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#20998;&#23376;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#21644;&#19982;&#29616;&#26377;&#26041;&#27861;&#30340;&#27604;&#36739;&#35780;&#20272;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#20854;&#29983;&#25104;&#26356;&#31283;&#23450;&#21644;&#26377;&#25928;&#30340;&#20998;&#23376;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new model for generating molecular data by combining discrete and continuous diffusion processes. Our model generates a comprehensive representation of molecules, including atom features, 2D discrete molecule structures, and 3D continuous molecule coordinates. The use of diffusion processes allows for capturing the probabilistic nature of molecular processes and the ability to explore the effect of different factors on molecular structures and properties. Additionally, we propose a novel graph transformer architecture to denoise the diffusion process. The transformer is equivariant to Euclidean transformations, allowing it to learn invariant atom and edge representations while preserving the equivariance of atom coordinates. This transformer can be used to learn molecular representations robust to geometric transformations. We evaluate the performance of our model through experiments and comparisons with existing methods, showing its ability to generate more stable and val
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#20010;&#21560;&#24341;&#23376;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;Koopman&#31639;&#23376;&#25552;&#21319;&#21644;&#37325;&#26500;&#26426;&#21046;&#65292;&#36890;&#36807;&#21033;&#29992;&#21560;&#24341;&#22495;&#20043;&#38388;&#30340;&#22266;&#26377;&#23545;&#31216;&#24615;&#65292;&#21482;&#38656;&#19977;&#20010;&#33258;&#30001;&#24230;&#30340;&#32447;&#24615;&#37325;&#26500;&#23601;&#21487;&#20197;&#20840;&#23616;&#32447;&#24615;&#21270;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2304.11860</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#20010;&#21560;&#24341;&#23376;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#25552;&#21319;&#21644;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
On the lifting and reconstruction of nonlinear systems with multiple attractors. (arXiv:2304.11860v2 [math.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#20010;&#21560;&#24341;&#23376;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;Koopman&#31639;&#23376;&#25552;&#21319;&#21644;&#37325;&#26500;&#26426;&#21046;&#65292;&#36890;&#36807;&#21033;&#29992;&#21560;&#24341;&#22495;&#20043;&#38388;&#30340;&#22266;&#26377;&#23545;&#31216;&#24615;&#65292;&#21482;&#38656;&#19977;&#20010;&#33258;&#30001;&#24230;&#30340;&#32447;&#24615;&#37325;&#26500;&#23601;&#21487;&#20197;&#20840;&#23616;&#32447;&#24615;&#21270;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Koopman&#31639;&#23376;&#36890;&#36807;&#20851;&#27880;&#19981;&#21464;&#23376;&#31354;&#38388;&#20013;&#30340;&#35266;&#27979;&#37327;&#30340;&#28436;&#21270;&#65292;&#25552;&#20379;&#20102;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#32447;&#24615;&#35270;&#35282;&#12290;&#24863;&#20852;&#36259;&#30340;&#35266;&#27979;&#37327;&#36890;&#24120;&#26159;&#20174;Koopman&#29305;&#24449;&#20989;&#25968;&#32447;&#24615;&#37325;&#26500;&#20986;&#26469;&#30340;&#12290;&#23613;&#31649;Koopman&#31639;&#23376;&#22312;&#36807;&#21435;&#20960;&#24180;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23545;&#20110;&#20855;&#26377;&#22810;&#20010;&#31283;&#23450;&#28857;&#30340;&#21160;&#21147;&#31995;&#32479;&#65292;&#20851;&#20110;Koopman&#31639;&#23376;&#30340;&#36866;&#29992;&#24615;&#23384;&#22312;&#19968;&#20123;&#35823;&#35299;&#12290;&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#20855;&#26377;&#22810;&#20010;&#21560;&#24341;&#23376;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;Koopman&#31639;&#23376;&#25552;&#21319;&#26426;&#21046;&#12290;&#36890;&#36807;&#32771;&#34385;Duffing&#25391;&#33633;&#22120;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#36807;&#21033;&#29992;&#21560;&#24341;&#22495;&#20043;&#38388;&#30340;&#22266;&#26377;&#23545;&#31216;&#24615;&#65292;Koopman&#21487;&#35266;&#27979;&#31354;&#38388;&#20013;&#20855;&#26377;&#19977;&#20010;&#33258;&#30001;&#24230;&#30340;&#32447;&#24615;&#37325;&#26500;&#23601;&#36275;&#20197;&#20840;&#23616;&#32447;&#24615;&#21270;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Koopman operator provides a linear perspective on non-linear dynamics by focusing on the evolution of observables in an invariant subspace. Observables of interest are typically linearly reconstructed from the Koopman eigenfunctions. Despite the broad use of Koopman operators over the past few years, there exist some misconceptions about the applicability of Koopman operators to dynamical systems with more than one fixed point. In this work, an explanation is provided for the mechanism of lifting for the Koopman operator of nonlinear systems with multiple attractors. Considering the example of the Duffing oscillator, we show that by exploiting the inherent symmetry between the basins of attraction, a linear reconstruction with three degrees of freedom in the Koopman observable space is sufficient to globally linearize the system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#35760;&#24518;&#25193;&#23637;BERT&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#25193;&#23637;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;200&#19975;&#20010;&#26631;&#35760;&#65292;&#26377;&#26395;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#22788;&#29702;&#24182;&#20026;&#20869;&#23384;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#23454;&#29616;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.11062</link><description>&lt;p&gt;
&#21033;&#29992;RMT&#23558;Transformer&#25193;&#23637;&#21040;100&#19975;&#20010;&#26631;&#35760;&#21450;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling Transformer to 1M tokens and beyond with RMT. (arXiv:2304.11062v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#35760;&#24518;&#25193;&#23637;BERT&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#25193;&#23637;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;200&#19975;&#20010;&#26631;&#35760;&#65292;&#26377;&#26395;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#22788;&#29702;&#24182;&#20026;&#20869;&#23384;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#23454;&#29616;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#35760;&#24518;&#25193;&#23637;BERT&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#26041;&#27861;&#65292;BERT&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#26377;&#25928;&#30340;&#22522;&#20110;Transformer&#27169;&#22411;&#20043;&#19968;&#12290;&#36890;&#36807;&#21033;&#29992;&#24490;&#29615;&#35760;&#24518;Transformer&#26550;&#26500;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#27169;&#22411;&#30340;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#22686;&#21152;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;200&#19975;&#20010;&#26631;&#35760;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#30340;&#20869;&#23384;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#23384;&#20648;&#21644;&#22788;&#29702;&#26412;&#22320;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#23454;&#29616;&#36755;&#20837;&#24207;&#21015;&#21508;&#37096;&#20998;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#21160;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#28508;&#21147;&#26469;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#22788;&#29702;&#65292;&#24182;&#33021;&#22815;&#20026;&#20869;&#23384;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#23454;&#29616;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This technical report presents the application of a recurrent memory to extend the context length of BERT, one of the most effective Transformer-based models in natural language processing. By leveraging the Recurrent Memory Transformer architecture, we have successfully increased the model's effective context length to an unprecedented two million tokens, while maintaining high memory retrieval accuracy. Our method allows for the storage and processing of both local and global information and enables information flow between segments of the input sequence through the use of recurrence. Our experiments demonstrate the effectiveness of our approach, which holds significant potential to enhance long-term dependency handling in natural language understanding and generation tasks as well as enable large-scale context processing for memory-intensive applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#26080;&#30417;&#30563;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#26469;&#21457;&#29616;&#25968;&#25454;&#20013;&#26377;&#24847;&#20041;&#30340;&#23646;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#31471;&#20540;&#22240;&#26524;&#20998;&#31163; (CDEV) &#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#27979;&#35797;&#40120;&#40060;&#36890;&#20449;&#31995;&#32479;&#24182;&#21457;&#29616;&#20854;&#20013;&#23384;&#22312;&#35821;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.10931</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#25506;&#32034;&#21644;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#36924;&#36817;&#26410;&#30693;&#30340;&#36890;&#20449;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Approaching an unknown communication system by latent space exploration and causal inference. (arXiv:2303.10931v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#26080;&#30417;&#30563;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#26469;&#21457;&#29616;&#25968;&#25454;&#20013;&#26377;&#24847;&#20041;&#30340;&#23646;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#31471;&#20540;&#22240;&#26524;&#20998;&#31163; (CDEV) &#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#27979;&#35797;&#40120;&#40060;&#36890;&#20449;&#31995;&#32479;&#24182;&#21457;&#29616;&#20854;&#20013;&#23384;&#22312;&#35821;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#26080;&#30417;&#30563;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#26469;&#21457;&#29616;&#25968;&#25454;&#20013;&#26377;&#24847;&#20041;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#23558;&#23545;&#21333;&#20010;&#28508;&#22312;&#21464;&#37327;&#30340;&#25805;&#20316;&#19982;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#31216;&#20026;&#26497;&#31471;&#20540;&#22240;&#26524;&#20998;&#31163; (CDEV) &#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#23545;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#27934;&#23519;&#21147;&#12290;&#36890;&#36807;&#27492;&#25216;&#26415;&#65292;&#25105;&#20204;&#21487;&#20197;&#25512;&#26029;&#27169;&#22411;&#23558;&#26410;&#30693;&#25968;&#25454;&#32534;&#30721;&#20026;&#26377;&#24847;&#20041;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#27979;&#35797;&#40120;&#40060;&#30340;&#36890;&#20449;&#31995;&#32479;&#20013;&#23384;&#22312;&#21738;&#20123;&#26377;&#24847;&#20041;&#30340;&#23646;&#24615;&#65292;&#40120;&#40060;&#36890;&#20449;&#26159;&#26368;&#20855;&#26377;&#21560;&#24341;&#21147;&#21644;&#30740;&#31350;&#19981;&#36275;&#30340;&#21160;&#29289;&#36890;&#20449;&#20043;&#19968;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#23398;&#20064;&#21040;&#26377;&#24847;&#20041;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#24182;&#27979;&#35797;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#36825;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#26469;&#35299;&#26512;&#21478;&#19968;&#20010;&#25105;&#20204;&#27809;&#26377;&#22320;&#38754;&#30495;&#30456;&#30340;&#22768;&#38899;&#36890;&#20449;&#31995;&#32479;&#30340;&#23646;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#34920;&#26126;&#65292;&#40120;&#40060;&#22312;&#20854;&#22768;&#38899;&#36890;&#20449;&#20013;&#23384;&#22312;&#35821;&#27861;&#65292;&#36825;&#26159;&#20197;&#21069;&#19981;&#30693;&#36947;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a methodology for discovering meaningful properties in data by exploring the latent space of unsupervised deep generative models. We combine manipulation of individual latent variables to extreme values outside the training range with methods inspired by causal inference into an approach we call causal disentanglement with extreme values (CDEV) and show that this approach yields insights for model interpretability. Using this technique, we can infer what properties of unknown data the model encodes as meaningful. We apply the methodology to test what is meaningful in the communication system of sperm whales, one of the most intriguing and understudied animal communication systems. We train a network that has been shown to learn meaningful representations of speech and test whether we can leverage such unsupervised learning to decipher the properties of another vocal communication system for which we have no ground truth. The proposed technique suggests that sperm wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20026;&#39046;&#22495;&#22806;&#27867;&#21270;&#35774;&#35745;&#25968;&#25454;&#22686;&#24378;&#30340;&#21407;&#21017;&#65292;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#22312;&#20445;&#30041;&#40065;&#26834;&#29305;&#24449;&#30340;&#21516;&#26102;&#38543;&#26426;&#21270;&#34394;&#20551;&#30340;&#39046;&#22495;&#30456;&#20851;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#39046;&#22495;&#22806;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.11861</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#22686;&#24378;&#23454;&#29616;&#39046;&#22495;&#22806;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Out-of-Domain Robustness via Targeted Augmentations. (arXiv:2302.11861v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20026;&#39046;&#22495;&#22806;&#27867;&#21270;&#35774;&#35745;&#25968;&#25454;&#22686;&#24378;&#30340;&#21407;&#21017;&#65292;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#22312;&#20445;&#30041;&#40065;&#26834;&#29305;&#24449;&#30340;&#21516;&#26102;&#38543;&#26426;&#21270;&#34394;&#20551;&#30340;&#39046;&#22495;&#30456;&#20851;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#39046;&#22495;&#22806;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#39046;&#22495;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#24448;&#24448;&#22312;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#19978;&#34920;&#29616;&#19979;&#38477;&#65292;&#27604;&#22914;&#37326;&#29983;&#21160;&#29289;&#30417;&#27979;&#27169;&#22411;&#22312;&#26032;&#30340;&#25668;&#20687;&#26426;&#20301;&#32622;&#19978;&#37096;&#32626;&#26102;&#20250;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20026;&#39046;&#22495;&#22806;&#27867;&#21270;&#35774;&#35745;&#25968;&#25454;&#22686;&#24378;&#30340;&#21407;&#21017;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20851;&#27880;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22330;&#26223;&#65292;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#65292;&#19968;&#20123;&#19982;&#39046;&#22495;&#30456;&#20851;&#30340;&#29305;&#24449;&#26159;&#40065;&#26834;&#30340;&#65292;&#21363;&#19968;&#20123;&#22312;&#19981;&#21516;&#39046;&#22495;&#38388;&#21464;&#21270;&#30340;&#29305;&#24449;&#23545;&#27867;&#21270;&#26377;&#39044;&#27979;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#22312;&#19978;&#36848;&#30340;&#37326;&#29983;&#21160;&#29289;&#30417;&#27979;&#24212;&#29992;&#20013;&#65292;&#22270;&#20687;&#32972;&#26223;&#22312;&#25668;&#20687;&#26426;&#20301;&#32622;&#19978;&#19981;&#21516;&#65292;&#20294;&#21487;&#20197;&#25351;&#31034;&#26646;&#24687;&#22320;&#31867;&#22411;&#65292;&#20174;&#32780;&#24110;&#21161;&#39044;&#27979;&#34987;&#25668;&#21160;&#29289;&#30340;&#29289;&#31181;&#12290;&#22312;&#23545;&#32447;&#24615;&#35774;&#32622;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26377;&#38024;&#23545;&#24615;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21363;&#26377;&#36873;&#25321;&#24615;&#22320;&#38543;&#26426;&#21270;&#34394;&#20551;&#30340;&#39046;&#22495;&#30456;&#20851;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;&#40065;&#26834;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26377;&#38024;&#23545;&#24615;&#30340;&#22686;&#24378;&#21487;&#20197;&#25552;&#39640;&#39046;&#22495;&#22806;&#24615;&#33021;&#65292;&#20351;&#27169;&#22411;&#22312;&#36739;&#23569;&#39046;&#22495;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#29616;&#26377;&#30340;&#36890;&#29992;&#22686;&#24378;&#26041;&#27861;&#26410;&#33021;&#23454;&#29616;&#39046;&#22495;&#22806;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models trained on one set of domains often suffer performance drops on unseen domains, e.g., when wildlife monitoring models are deployed in new camera locations. In this work, we study principles for designing data augmentations for out-of-domain (OOD) generalization. In particular, we focus on real-world scenarios in which some domain-dependent features are robust, i.e., some features that vary across domains are predictive OOD. For example, in the wildlife monitoring application above, image backgrounds vary across camera locations but indicate habitat type, which helps predict the species of photographed animals. Motivated by theoretical analysis on a linear setting, we propose targeted augmentations, which selectively randomize spurious domain-dependent features while preserving robust ones. We prove that targeted augmentations improve OOD performance, allowing models to generalize better with fewer domains. In contrast, existing approaches such as generic augmentations, which fai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#25345;&#32493;&#23398;&#20064;&#30340;&#20840;&#38754;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#25345;&#32493;&#23398;&#20064;&#30340;&#19968;&#33324;&#30446;&#26631;&#65292;&#22238;&#39038;&#20102;&#21508;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#19968;&#20123;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#39046;&#22495;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.00487</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#32508;&#36848;&#65306;&#29702;&#35770;&#12289;&#26041;&#27861;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Continual Learning: Theory, Method and Application. (arXiv:2302.00487v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#25345;&#32493;&#23398;&#20064;&#30340;&#20840;&#38754;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#25345;&#32493;&#23398;&#20064;&#30340;&#19968;&#33324;&#30446;&#26631;&#65292;&#22238;&#39038;&#20102;&#21508;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#19968;&#20123;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#39046;&#22495;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#31995;&#32479;&#38656;&#35201;&#22312;&#20854;&#29983;&#21629;&#21608;&#26399;&#20869;&#19981;&#26029;&#33719;&#21462;&#12289;&#26356;&#26032;&#12289;&#31215;&#32047;&#21644;&#21033;&#29992;&#30693;&#35782;&#20197;&#24212;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#36825;&#31181;&#33021;&#21147;&#31216;&#20026;&#25345;&#32493;&#23398;&#20064;&#65292;&#20026;AI&#31995;&#32479;&#33258;&#36866;&#24212;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#25345;&#32493;&#23398;&#20064;&#30340;&#19968;&#20010;&#26174;&#33879;&#38480;&#21046;&#26159;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#23398;&#20064;&#19968;&#20010;&#26032;&#20219;&#21153;&#36890;&#24120;&#20250;&#23548;&#33268;&#26087;&#20219;&#21153;&#30340;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#12290;&#36817;&#24180;&#26469;&#65292;&#19981;&#26029;&#28044;&#29616;&#30340;&#21508;&#31181;&#36827;&#23637;&#22823;&#22823;&#25193;&#23637;&#20102;&#25345;&#32493;&#23398;&#20064;&#30340;&#29702;&#35299;&#21644;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25345;&#32493;&#23398;&#20064;&#32508;&#36848;&#65292;&#26088;&#22312;&#36830;&#25509;&#22522;&#26412;&#35774;&#32622;&#12289;&#29702;&#35770;&#22522;&#30784;&#12289;&#20195;&#34920;&#24615;&#26041;&#27861;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#29616;&#26377;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#27010;&#25324;&#20102;&#25345;&#32493;&#23398;&#20064;&#30340;&#19968;&#33324;&#30446;&#26631;&#65306;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#21644;&#32456;&#36523;&#30340;&#23398;&#20064;&#65292;&#20197;&#21450;&#23454;&#29616;&#26356;&#28145;&#23618;&#27425;&#30340;&#34920;&#24449;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#21508;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#27491;&#21017;&#21270;&#12289;&#22522;&#20110;&#22238;&#25918;&#21644;&#22522;&#20110;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19968;&#20123;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#26426;&#22120;&#20154;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20123;&#24320;&#25918;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
To cope with real-world dynamics, an intelligent agent needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance degradation of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#30340;&#24341;&#23548;&#26694;&#33258;&#30001;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#29305;&#24449;&#34701;&#21512;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22312;&#24694;&#21155;&#22825;&#27668;&#19979;&#29289;&#20307;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.06108</link><description>&lt;p&gt;
RaLiBEV: &#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#24341;&#23548;&#26694;&#33258;&#30001;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#30340;&#34701;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object Detection System. (arXiv:2211.06108v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#30340;&#24341;&#23548;&#26694;&#33258;&#30001;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#29305;&#24449;&#34701;&#21512;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22312;&#24694;&#21155;&#22825;&#27668;&#19979;&#29289;&#20307;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#65292;&#28608;&#20809;&#38647;&#36798;&#21644;&#38647;&#36798;&#22312;&#24863;&#30693;&#21608;&#22260;&#29615;&#22659;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28608;&#20809;&#38647;&#36798;&#25552;&#20379;&#31934;&#30830;&#30340;&#19977;&#32500;&#31354;&#38388;&#24863;&#30693;&#20449;&#24687;&#65292;&#20294;&#22312;&#38654;&#31561;&#24694;&#21155;&#22825;&#27668;&#19979;&#26080;&#27861;&#24037;&#20316;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#38647;&#36798;&#20449;&#21495;&#30001;&#20110;&#27874;&#38271;&#30340;&#29305;&#24615;&#22312;&#36935;&#21040;&#38632;&#28404;&#25110;&#38654;&#31890;&#26102;&#20250;&#21457;&#29983;&#34893;&#23556;&#65292;&#20294;&#23427;&#21463;&#21040;&#22823;&#22122;&#22768;&#30340;&#24178;&#25200;&#12290;&#26368;&#36817;&#30340;&#26368;&#26032;&#30740;&#31350;&#34920;&#26126;&#65292;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#34701;&#21512;&#21487;&#20197;&#22312;&#24694;&#21155;&#22825;&#27668;&#19979;&#23454;&#29616;&#24378;&#20581;&#30340;&#26816;&#27979;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#37319;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20174;&#27599;&#20010;&#20256;&#24863;&#22120;&#25968;&#25454;&#27969;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#28982;&#21518;&#23545;&#40784;&#21644;&#27719;&#32858;&#20004;&#20010;&#20998;&#25903;&#30340;&#29305;&#24449;&#20197;&#39044;&#27979;&#29289;&#20307;&#26816;&#27979;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26631;&#31614;&#20998;&#37197;&#21644;&#34701;&#21512;&#31574;&#30053;&#30340;&#31616;&#21333;&#35774;&#35745;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#36793;&#30028;&#26694;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#34701;&#21512;&#23398;&#20064;&#30340;&#24341;&#23548;&#26694;&#33258;&#30001;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#23558;&#26469;&#33258;&#38647;&#36798;&#30340;&#36317;&#31163;-&#26041;&#20301;&#29305;&#24449;&#34701;&#21512;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
In autonomous driving systems, LiDAR and radar play important roles in the perception of the surrounding environment. LiDAR provides accurate 3D spatial sensing information but cannot work in adverse weather like fog. On the other hand, the radar signal can be diffracted when encountering raindrops or mist particles thanks to its wavelength, but it suffers from large noise. Recent state-of-the-art works reveal that fusion of radar and LiDAR can lead to robust detection in adverse weather. The existing works adopt convolutional neural network architecture to extract features from each sensor data stream, then align and aggregate the two branch features to predict object detection results. However, these methods have low accuracy of bounding box estimations due to a simple design of label assignment and fusion strategies. In this paper, we propose a bird's-eye view fusion learning-based anchor box-free object detection system, which fuses the feature derived from the radar range-azimuth 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#20934;&#31639;&#26415;&#28151;&#21512;&#12289;&#25955;&#24230;&#26368;&#23567;&#21270;&#21644;Bregman&#20449;&#24687;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#36890;&#36807;&#22312;&#23494;&#24230;&#20989;&#25968;&#30340;&#21333;&#35843;&#23884;&#20837;&#19979;&#20351;&#29992;Bregman&#25955;&#24230;&#65292;&#25105;&#20204;&#23558;&#24120;&#35265;&#30340;&#25955;&#24230;&#20989;&#25968;&#19982;&#36864;&#28779;&#36335;&#24452;&#19978;&#30340;&#20013;&#38388;&#23494;&#24230;&#20851;&#32852;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2209.07481</link><description>&lt;p&gt;
&#20934;&#31639;&#26415;&#28151;&#21512;&#12289;&#25955;&#24230;&#26368;&#23567;&#21270;&#21644;Bregman&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Quasi-Arithmetic Mixtures, Divergence Minimization, and Bregman Information. (arXiv:2209.07481v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#20934;&#31639;&#26415;&#28151;&#21512;&#12289;&#25955;&#24230;&#26368;&#23567;&#21270;&#21644;Bregman&#20449;&#24687;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#36890;&#36807;&#22312;&#23494;&#24230;&#20989;&#25968;&#30340;&#21333;&#35843;&#23884;&#20837;&#19979;&#20351;&#29992;Bregman&#25955;&#24230;&#65292;&#25105;&#20204;&#23558;&#24120;&#35265;&#30340;&#25955;&#24230;&#20989;&#25968;&#19982;&#36864;&#28779;&#36335;&#24452;&#19978;&#30340;&#20013;&#38388;&#23494;&#24230;&#20851;&#32852;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#29992;&#20110;&#20174;&#22797;&#26434;&#20998;&#24067;&#20013;&#37319;&#26679;&#21644;&#20272;&#35745;&#24402;&#19968;&#21270;&#24120;&#25968;&#36890;&#24120;&#27169;&#25311;&#27839;&#30528;&#36830;&#25509;&#21487;&#36319;&#36394;&#21021;&#22987;&#20998;&#24067;&#21644;&#30446;&#26631;&#23494;&#24230;&#30340;&#36864;&#28779;&#36335;&#24452;&#30340;&#20013;&#38388;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#20934;&#31639;&#26415;&#24179;&#22343;&#26500;&#24314;&#20102;&#36864;&#28779;&#36335;&#24452;&#65292;&#24182;&#35299;&#37322;&#20102;&#30001;&#27492;&#20135;&#29983;&#30340;&#20013;&#38388;&#23494;&#24230;&#26159;&#26368;&#23567;&#21270;&#26399;&#26395;&#25955;&#24230;&#21040;&#31471;&#28857;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#23494;&#24230;&#20989;&#25968;&#30340;&#21333;&#35843;&#23884;&#20837;&#19979;&#20351;&#29992;Bregman&#25955;&#24230;&#23545;&#36825;&#20010;&#8220;&#36136;&#24515;&#8221;&#24615;&#36136;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#20174;&#32780;&#23558;&#24120;&#35265;&#30340;&#25955;&#24230;&#65288;&#22914;Amari&#21644;Renyi&#30340;alpha&#25955;&#24230;&#12289;&#65288;alpha&#65292;beta&#65289;&#25955;&#24230;&#21644;Jensen-Shannon&#25955;&#24230;&#65289;&#19982;&#27839;&#30528;&#36864;&#28779;&#36335;&#24452;&#30340;&#20013;&#38388;&#23494;&#24230;&#20851;&#32852;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#20986;&#20102;&#21442;&#25968;&#21270;&#26063;&#12289;&#20934;&#31639;&#26415;&#24179;&#22343;&#21644;&#25955;&#24230;&#20989;&#25968;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20351;&#29992;&#20102;Zhang&#30340;rho-tau Bregman&#25955;&#24230;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Markov Chain Monte Carlo methods for sampling from complex distributions and estimating normalization constants often simulate samples from a sequence of intermediate distributions along an annealing path, which bridges between a tractable initial distribution and a target density of interest. Prior work has constructed annealing paths using quasi-arithmetic means, and interpreted the resulting intermediate densities as minimizing an expected divergence to the endpoints. We provide a comprehensive analysis of this 'centroid' property using Bregman divergences under a monotonic embedding of the density function, thereby associating common divergences such as Amari's and Renyi's ${\alpha}$-divergences, ${(\alpha,\beta)}$-divergences, and the Jensen-Shannon divergence with intermediate densities along an annealing path. Our analysis highlights the interplay between parametric families, quasi-arithmetic means, and divergence functions using the rho-tau Bregman divergence framework of Zhang
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#20316;&#20026;&#21442;&#25968;&#20998;&#24067;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#26031;&#28151;&#21512;&#30340;&#29109;&#36817;&#20284;&#20026;&#21333;&#23792;&#39640;&#26031;&#30340;&#29109;&#20043;&#21644;&#26469;&#35299;&#20915;&#22810;&#23792;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#36817;&#20284;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2202.13059</link><description>&lt;p&gt;
&#29992;&#29109;&#36817;&#20284;&#30340;&#39640;&#26031;&#28151;&#21512;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Variational Inference with Gaussian Mixture by Entropy Approximation. (arXiv:2202.13059v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.13059
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#20316;&#20026;&#21442;&#25968;&#20998;&#24067;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#26031;&#28151;&#21512;&#30340;&#29109;&#36817;&#20284;&#20026;&#21333;&#23792;&#39640;&#26031;&#30340;&#29109;&#20043;&#21644;&#26469;&#35299;&#20915;&#22810;&#23792;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#36817;&#20284;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#26159;&#19968;&#31181;&#29992;&#20110;&#36817;&#20284;&#26080;&#27861;&#22788;&#29702;&#30340;&#21518;&#39564;&#20998;&#24067;&#20197;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#21333;&#23792;&#30340;&#39640;&#26031;&#20998;&#24067;&#36890;&#24120;&#34987;&#36873;&#25321;&#20316;&#20026;&#21442;&#25968;&#20998;&#24067;&#65292;&#24456;&#38590;&#36924;&#36817;&#22810;&#23792;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#20316;&#20026;&#21442;&#25968;&#20998;&#24067;&#12290;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#30340;&#19968;&#20010;&#20027;&#35201;&#38590;&#28857;&#26159;&#22914;&#20309;&#36817;&#20284;&#39640;&#26031;&#28151;&#21512;&#30340;&#29109;&#12290;&#25105;&#20204;&#23558;&#39640;&#26031;&#28151;&#21512;&#30340;&#29109;&#36817;&#20284;&#20026;&#21333;&#23792;&#39640;&#26031;&#30340;&#29109;&#20043;&#21644;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#35745;&#31639;&#24471;&#21040;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#30495;&#23454;&#29109;&#19982;&#36817;&#20284;&#29109;&#20043;&#38388;&#30340;&#36817;&#20284;&#35823;&#24046;&#65292;&#20197;&#20415;&#25581;&#31034;&#25105;&#20204;&#30340;&#36817;&#20284;&#20309;&#26102;&#36215;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36817;&#20284;&#35823;&#24046;&#30001;&#39640;&#26031;&#28151;&#21512;&#22343;&#20540;&#20043;&#38388;&#36317;&#31163;&#19982;&#26041;&#24046;&#20043;&#21644;&#30340;&#27604;&#29575;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#24403;&#39640;&#26031;&#28151;&#21512;&#32452;&#20214;&#30340;&#25968;&#37327;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;&#36817;&#20284;&#35823;&#24046;&#36235;&#36817;&#20110;&#38646;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational inference is a technique for approximating intractable posterior distributions in order to quantify the uncertainty of machine learning. Although the unimodal Gaussian distribution is usually chosen as a parametric distribution, it hardly approximates the multimodality. In this paper, we employ the Gaussian mixture distribution as a parametric distribution. A main difficulty of variational inference with the Gaussian mixture is how to approximate the entropy of the Gaussian mixture. We approximate the entropy of the Gaussian mixture as the sum of the entropy of the unimodal Gaussian, which can be analytically calculated. In addition, we theoretically analyze the approximation error between the true entropy and approximated one in order to reveal when our approximation works well. Specifically, the approximation error is controlled by the ratios of the distances between the means to the sum of the variances of the Gaussian mixture. Furthermore, it converges to zero when the 
&lt;/p&gt;</description></item></channel></rss>