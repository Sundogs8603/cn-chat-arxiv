<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>LightPath&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#21644;&#21487;&#20280;&#32553;&#30340;&#36335;&#24452;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#19979;&#38477;&#20302;&#36164;&#28304;&#28040;&#32791;&#21644;&#23454;&#29616;&#21487;&#20280;&#32553;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10171</link><description>&lt;p&gt;
LightPath: &#36731;&#37327;&#32423;&#21644;&#21487;&#20280;&#32553;&#30340;&#36335;&#24452;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LightPath: Lightweight and Scalable Path Representation Learning. (arXiv:2307.10171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10171
&lt;/p&gt;
&lt;p&gt;
LightPath&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#21644;&#21487;&#20280;&#32553;&#30340;&#36335;&#24452;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#19979;&#38477;&#20302;&#36164;&#28304;&#28040;&#32791;&#21644;&#23454;&#29616;&#21487;&#20280;&#32553;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#36335;&#24452;&#24191;&#27867;&#24212;&#29992;&#20110;&#26234;&#33021;&#20132;&#36890;&#21644;&#26234;&#33021;&#22478;&#24066;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#26381;&#21153;&#36825;&#20123;&#24212;&#29992;&#65292;&#36335;&#24452;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#25552;&#20379;&#36335;&#24452;&#30340;&#32039;&#20945;&#34920;&#31034;&#65292;&#20197;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#36335;&#24452;&#25490;&#24207;&#21644;&#26053;&#34892;&#25104;&#26412;&#20272;&#35745;&#65289;&#20013;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#25805;&#20316;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#19979;&#21644;&#32511;&#33394;&#35745;&#31639;&#38480;&#21046;&#19979;&#65292;&#36335;&#24452;&#34920;&#31034;&#23398;&#20064;&#30340;&#36731;&#37327;&#32423;&#21644;&#21487;&#20280;&#32553;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36335;&#24452;&#34920;&#31034;&#23398;&#20064;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20934;&#30830;&#24615;&#65292;&#23545;&#36164;&#28304;&#28040;&#32791;&#21644;&#21487;&#20280;&#32553;&#24615;&#27425;&#35201;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#21644;&#21487;&#20280;&#32553;&#30340;&#36335;&#24452;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;LightPath&#65292;&#26088;&#22312;&#38477;&#20302;&#36164;&#28304;&#28040;&#32791;&#65292;&#23454;&#29616;&#21487;&#20280;&#32553;&#24615;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Movement paths are used widely in intelligent transportation and smart city applications. To serve such applications, path representation learning aims to provide compact representations of paths that enable efficient and accurate operations when used for different downstream tasks such as path ranking and travel cost estimation. In many cases, it is attractive that the path representation learning is lightweight and scalable; in resource-limited environments and under green computing limitations, it is essential. Yet, existing path representation learning studies focus on accuracy and pay at most secondary attention to resource consumption and scalability.  We propose a lightweight and scalable path representation learning framework, termed LightPath, that aims to reduce resource consumption and achieve scalability without affecting accuracy, thus enabling broader applicability. More specifically, we first propose a sparse auto-encoder that ensures that the framework achieves good sca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24635;&#32467;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#24212;&#29992;&#25104;&#21151;&#26696;&#20363;&#65292;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#24555;&#36895;&#20102;&#35299;&#35813;&#39046;&#22495;&#30340;&#24403;&#21069;&#29366;&#24577;&#24182;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.10169</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25361;&#25112;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Challenges and Applications of Large Language Models. (arXiv:2307.10169v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24635;&#32467;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#24212;&#29992;&#25104;&#21151;&#26696;&#20363;&#65292;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#24555;&#36895;&#20102;&#35299;&#35813;&#39046;&#22495;&#30340;&#24403;&#21069;&#29366;&#24577;&#24182;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#35752;&#35770;&#20013;&#20174;&#19981;&#23384;&#22312;&#21040;&#26080;&#22788;&#19981;&#22312;&#21482;&#29992;&#20102;&#20960;&#24180;&#30340;&#26102;&#38388;&#12290;&#30001;&#20110;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#24456;&#38590;&#30830;&#23450;&#21097;&#20313;&#30340;&#25361;&#25112;&#21644;&#24050;&#32463;&#21462;&#24471;&#30340;&#24212;&#29992;&#25104;&#21151;&#12290;&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#31995;&#32479;&#30340;&#19968;&#32452;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#24212;&#29992;&#25104;&#21151;&#26696;&#20363;&#65292;&#20197;&#20415;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#26356;&#24555;&#22320;&#20102;&#35299;&#35813;&#39046;&#22495;&#30340;&#24403;&#21069;&#29366;&#24577;&#24182;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learning discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to establish a systematic set of open problems and application successes so that ML researchers can comprehend the field's current state more quickly and become productive.
&lt;/p&gt;</description></item><item><title>VITS&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#21464;&#20998;&#25512;&#29702;&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#12290;&#23427;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#21518;&#39564;&#36817;&#20284;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#24182;&#19988;&#22312;&#32447;&#24615;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#20013;&#36798;&#21040;&#19982;&#20256;&#32479;TS&#30456;&#21516;&#38454;&#25968;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2307.10167</link><description>&lt;p&gt;
VITS: &#22522;&#20110;&#21464;&#20998;&#25512;&#29702;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#29992;&#20110;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
VITS : Variational Inference Thomson Sampling for contextual bandits. (arXiv:2307.10167v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10167
&lt;/p&gt;
&lt;p&gt;
VITS&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#21464;&#20998;&#25512;&#29702;&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#12290;&#23427;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#21518;&#39564;&#36817;&#20284;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#24182;&#19988;&#22312;&#32447;&#24615;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#20013;&#36798;&#21040;&#19982;&#20256;&#32479;TS&#30456;&#21516;&#38454;&#25968;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#29992;&#20110;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;TS&#65289;&#31639;&#27861;&#30340;&#21464;&#20307;&#12290;&#20256;&#32479;&#30340;TS&#31639;&#27861;&#22312;&#27599;&#36718;&#38656;&#35201;&#20174;&#24403;&#21069;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#25277;&#26679;&#65292;&#32780;&#36825;&#36890;&#24120;&#26159;&#38590;&#20197;&#35745;&#31639;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#36817;&#20284;&#25512;&#29702;&#25216;&#26415;&#24182;&#25552;&#20379;&#25509;&#36817;&#21518;&#39564;&#20998;&#24067;&#30340;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#36817;&#20284;&#25216;&#26415;&#35201;&#20040;&#20272;&#35745;&#19981;&#20934;&#30830;&#65288;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#65289;&#65292;&#35201;&#20040;&#35745;&#31639;&#24320;&#38144;&#36739;&#22823;&#65288;MCMC&#26041;&#27861;&#65292;&#38598;&#25104;&#25277;&#26679;...&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22522;&#20110;&#39640;&#26031;&#21464;&#20998;&#25512;&#29702;&#30340;&#21464;&#20998;&#25512;&#29702;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;VITS&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#21518;&#39564;&#36817;&#20284;&#65292;&#24182;&#19988;&#23481;&#26131;&#20174;&#20013;&#25277;&#26679;&#65292;&#32780;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#26159;TS&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#32447;&#24615;&#24773;&#22659;&#32972;&#31163;&#38382;&#39064;&#20013;&#65292;VITS&#23454;&#29616;&#20102;&#19982;&#20256;&#32479;TS&#30456;&#21516;&#38454;&#25968;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#19978;&#30028;&#65292;&#19982;&#32500;&#24230;&#21644;&#22238;&#21512;&#25968;&#25104;&#27491;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce and analyze a variant of the Thompson sampling (TS) algorithm for contextual bandits. At each round, traditional TS requires samples from the current posterior distribution, which is usually intractable. To circumvent this issue, approximate inference techniques can be used and provide samples with distribution close to the posteriors. However, current approximate techniques yield to either poor estimation (Laplace approximation) or can be computationally expensive (MCMC methods, Ensemble sampling...). In this paper, we propose a new algorithm, Varational Inference Thompson sampling VITS, based on Gaussian Variational Inference. This scheme provides powerful posterior approximations which are easy to sample from, and is computationally efficient, making it an ideal choice for TS. In addition, we show that VITS achieves a sub-linear regret bound of the same order in the dimension and number of round as traditional TS for linear contextual bandit. Finally, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#21518;&#38376;&#25915;&#20987;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#27809;&#26377;&#20851;&#20110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#21518;&#38376;&#25915;&#20987;&#19982;&#25968;&#25454;&#20013;&#33258;&#28982;&#20135;&#29983;&#30340;&#29305;&#24449;&#26159;&#19981;&#21487;&#21306;&#20998;&#30340;&#65292;&#22240;&#27492;&#38590;&#20197;&#26816;&#27979;&#12290;&#20316;&#32773;&#36824;&#37325;&#26032;&#23457;&#35270;&#29616;&#26377;&#30340;&#25269;&#24481;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#19968;&#31181;&#20851;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#26367;&#20195;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2307.10163</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Rethinking Backdoor Attacks. (arXiv:2307.10163v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#21518;&#38376;&#25915;&#20987;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#27809;&#26377;&#20851;&#20110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#21518;&#38376;&#25915;&#20987;&#19982;&#25968;&#25454;&#20013;&#33258;&#28982;&#20135;&#29983;&#30340;&#29305;&#24449;&#26159;&#19981;&#21487;&#21306;&#20998;&#30340;&#65292;&#22240;&#27492;&#38590;&#20197;&#26816;&#27979;&#12290;&#20316;&#32773;&#36824;&#37325;&#26032;&#23457;&#35270;&#29616;&#26377;&#30340;&#25269;&#24481;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#19968;&#31181;&#20851;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#26367;&#20195;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21518;&#38376;&#25915;&#20987;&#20013;&#65292;&#23545;&#25163;&#20250;&#23558;&#24694;&#24847;&#26500;&#36896;&#30340;&#21518;&#38376;&#31034;&#20363;&#25554;&#20837;&#35757;&#32451;&#38598;&#20013;&#65292;&#20351;&#24471;&#29983;&#25104;&#30340;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25805;&#32437;&#12290;&#38450;&#24481;&#36825;&#31181;&#25915;&#20987;&#36890;&#24120;&#28041;&#21450;&#23558;&#36825;&#20123;&#25554;&#20837;&#30340;&#31034;&#20363;&#35270;&#20026;&#35757;&#32451;&#38598;&#20013;&#30340;&#24322;&#24120;&#20540;&#65292;&#24182;&#20351;&#29992;&#40065;&#26834;&#32479;&#35745;&#23398;&#30340;&#25216;&#26415;&#26469;&#26816;&#27979;&#21644;&#21024;&#38500;&#23427;&#20204;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#35299;&#20915;&#21518;&#38376;&#25915;&#20987;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#20851;&#20110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#21518;&#38376;&#25915;&#20987;&#19982;&#25968;&#25454;&#20013;&#33258;&#28982;&#20135;&#29983;&#30340;&#29305;&#24449;&#26159;&#19981;&#21487;&#21306;&#20998;&#30340;--&#22240;&#27492;&#26080;&#27861;&#22312;&#19968;&#33324;&#24847;&#20041;&#19978;&#8220;&#26816;&#27979;&#8221;&#23427;&#20204;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#29616;&#26377;&#30340;&#25269;&#24481;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#24449;&#23427;&#20204;&#25152;&#20570;&#20986;&#30340;&#65288;&#24120;&#24120;&#26159;&#28508;&#22312;&#30340;&#65289;&#20551;&#35774;&#20197;&#21450;&#23427;&#20204;&#20381;&#36182;&#30340;&#20551;&#35774;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#20851;&#20110;&#21518;&#38376;&#25915;&#20987;&#30340;&#26367;&#20195;&#35270;&#35282;&#65306;&#20551;&#35774;&#36825;&#20123;&#25915;&#20987;&#23545;&#24212;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#26368;&#24378;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a backdoor attack, an adversary inserts maliciously constructed backdoor examples into a training set to make the resulting model vulnerable to manipulation. Defending against such attacks typically involves viewing these inserted examples as outliers in the training set and using techniques from robust statistics to detect and remove them.  In this work, we present a different approach to the backdoor attack problem. Specifically, we show that without structural information about the training data distribution, backdoor attacks are indistinguishable from naturally-occurring features in the data--and thus impossible to "detect" in a general sense. Then, guided by this observation, we revisit existing defenses against backdoor attacks and characterize the (often latent) assumptions they make and on which they depend. Finally, we explore an alternative perspective on backdoor attacks: one that assumes these attacks correspond to the strongest feature in the training data. Under this a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#23548;&#20803;&#20803;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#26469;&#23454;&#29616;&#31038;&#20132;&#36710;&#36742;&#22810;&#26679;&#39550;&#39542;&#31574;&#30053;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#35757;&#32451;&#31574;&#30053;&#22686;&#24378;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10160</link><description>&lt;p&gt;
&#36890;&#36807;&#24341;&#23548;&#20803;&#20803;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#40065;&#26834;&#30340;&#39550;&#39542;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Driving Policy Learning with Guided Meta Reinforcement Learning. (arXiv:2307.10160v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#23548;&#20803;&#20803;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#26469;&#23454;&#29616;&#31038;&#20132;&#36710;&#36742;&#22810;&#26679;&#39550;&#39542;&#31574;&#30053;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#35757;&#32451;&#31574;&#30053;&#22686;&#24378;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#22312;&#20132;&#20114;&#24335;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#33258;&#20027;&#23548;&#33322;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#21916;&#30340;&#25104;&#26524;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#37319;&#29992;&#22266;&#23450;&#30340;&#34892;&#20026;&#31574;&#30053;&#26469;&#25511;&#21046;&#35757;&#32451;&#29615;&#22659;&#20013;&#30340;&#31038;&#20132;&#36710;&#36742;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#21040;&#30340;&#39550;&#39542;&#31574;&#30053;&#36807;&#25311;&#21512;&#29615;&#22659;&#65292;&#20351;&#20854;&#38590;&#20197;&#19982;&#20855;&#26377;&#19981;&#21516;&#12289;&#26410;&#35265;&#36807;&#34892;&#20026;&#30340;&#36710;&#36742;&#33391;&#22909;&#20132;&#20114;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#26679;&#30340;&#39550;&#39542;&#31574;&#30053;&#20316;&#20026;&#19968;&#20010;&#21333;&#19968;&#30340;&#20803;&#20803;&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#38543;&#26426;&#21270;&#31038;&#20132;&#36710;&#36742;&#30340;&#22522;&#20110;&#20132;&#20114;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#23454;&#29616;&#29305;&#23450;&#30446;&#26631;&#30340;&#24341;&#23548;&#31574;&#30053;&#26377;&#25928;&#22320;&#35757;&#32451;&#20803;&#20803;&#31574;&#30053;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#31574;&#30053;&#65292;&#20351;&#29992;&#31038;&#20132;&#36710;&#36742;&#30001;&#23398;&#20064;&#21040;&#30340;&#20803;&#20803;&#31574;&#30053;&#25511;&#21046;&#30340;&#29615;&#22659;&#65292;&#26469;&#22686;&#24378;&#33258;&#20027;&#36710;&#36742;&#30340;&#39550;&#39542;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#19968;&#31181;&#33021;&#22815;&#24456;&#22909;&#22320;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#24773;&#20917;&#30340;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep reinforcement learning (DRL) has shown promising results for autonomous navigation in interactive traffic scenarios, existing work typically adopts a fixed behavior policy to control social vehicles in the training environment. This may cause the learned driving policy to overfit the environment, making it difficult to interact well with vehicles with different, unseen behaviors. In this work, we introduce an efficient method to train diverse driving policies for social vehicles as a single meta-policy. By randomizing the interaction-based reward functions of social vehicles, we can generate diverse objectives and efficiently train the meta-policy through guiding policies that achieve specific objectives. We further propose a training strategy to enhance the robustness of the ego vehicle's driving policy using the environment where social vehicles are controlled by the learned meta-policy. Our method successfully learns an ego driving policy that generalizes well to unsee
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#22270;&#30340;&#20960;&#20309;&#24615;&#36136;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#31163;&#25955;Ricci&#26354;&#29575;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#22270;&#32467;&#26500;&#20013;&#30340;&#23494;&#38598;&#36830;&#25509;&#23376;&#32467;&#26500;&#65292;&#21253;&#25324;&#21333;&#25104;&#21592;&#31038;&#21306;&#21644;&#28151;&#21512;&#25104;&#21592;&#31038;&#21306;&#65292;&#20197;&#21450;&#22312;&#32447;&#22270;&#19978;&#30340;&#31038;&#21306;&#26816;&#27979;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#35777;&#25454;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2307.10155</link><description>&lt;p&gt;
&#22522;&#20110;&#26354;&#29575;&#30340;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Curvature-based Clustering on Graphs. (arXiv:2307.10155v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#22270;&#30340;&#20960;&#20309;&#24615;&#36136;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#31163;&#25955;Ricci&#26354;&#29575;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#22270;&#32467;&#26500;&#20013;&#30340;&#23494;&#38598;&#36830;&#25509;&#23376;&#32467;&#26500;&#65292;&#21253;&#25324;&#21333;&#25104;&#21592;&#31038;&#21306;&#21644;&#28151;&#21512;&#25104;&#21592;&#31038;&#21306;&#65292;&#20197;&#21450;&#22312;&#32447;&#22270;&#19978;&#30340;&#31038;&#21306;&#26816;&#27979;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#35777;&#25454;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#33410;&#28857;&#32858;&#31867;&#65288;&#25110;&#31038;&#21306;&#26816;&#27979;&#65289;&#26159;&#32463;&#20856;&#30340;&#22270;&#23398;&#20064;&#20219;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#22270;&#30340;&#20960;&#20309;&#24615;&#36136;&#26469;&#35782;&#21035;&#23494;&#38598;&#36830;&#25509;&#30340;&#23376;&#32467;&#26500;&#20197;&#24418;&#25104;&#32858;&#31867;&#25110;&#31038;&#21306;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#31163;&#25955;Ricci&#26354;&#29575;&#21450;&#20854;&#30456;&#20851;&#30340;&#20960;&#20309;&#27969;&#65292;&#36890;&#36807;&#36825;&#20123;&#27969;&#65292;&#22270;&#30340;&#36793;&#26435;&#37325;&#28436;&#21270;&#20197;&#25581;&#31034;&#20854;&#31038;&#21306;&#32467;&#26500;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#31181;&#31163;&#25955;&#26354;&#29575;&#27010;&#24565;&#65292;&#24182;&#20998;&#26512;&#20102;&#30456;&#24212;&#31639;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;&#19982;&#20043;&#21069;&#30340;&#25991;&#29486;&#30456;&#27604;&#65292;&#25105;&#20204;&#19981;&#20165;&#30740;&#31350;&#20102;&#21333;&#25104;&#21592;&#31038;&#21306;&#26816;&#27979;&#65292;&#21363;&#27599;&#20010;&#33410;&#28857;&#21482;&#23646;&#20110;&#19968;&#20010;&#31038;&#21306;&#65292;&#36824;&#30740;&#31350;&#20102;&#28151;&#21512;&#25104;&#21592;&#31038;&#21306;&#26816;&#27979;&#65292;&#21363;&#31038;&#21306;&#21487;&#33021;&#37325;&#21472;&#12290;&#23545;&#20110;&#21518;&#32773;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#32447;&#22270;&#19978;&#25191;&#34892;&#31038;&#21306;&#26816;&#27979;&#26377;&#30410;&#22788;&#65292;&#21363;&#22270;&#30340;&#23545;&#20598;&#22270;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#22522;&#20110;&#26354;&#29575;&#30340;&#32858;&#31867;&#31639;&#27861;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20960;&#20010;&#37325;&#26032;&#23454;&#29616;&#21644;&#35780;&#20272;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised node clustering (or community detection) is a classical graph learning task. In this paper, we study algorithms, which exploit the geometry of the graph to identify densely connected substructures, which form clusters or communities. Our method implements discrete Ricci curvatures and their associated geometric flows, under which the edge weights of the graph evolve to reveal its community structure. We consider several discrete curvature notions and analyze the utility of the resulting algorithms. In contrast to prior literature, we study not only single-membership community detection, where each node belongs to exactly one community, but also mixed-membership community detection, where communities may overlap. For the latter, we argue that it is beneficial to perform community detection on the line graph, i.e., the graph's dual. We provide both theoretical and empirical evidence for the utility of our curvature-based clustering algorithms. In addition, we give several re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20154;&#24418;&#26426;&#22120;&#20154;&#20351;&#29992;&#26631;&#20934;&#24418;&#24335;&#30340;&#22870;&#21169;&#22609;&#36896;&#21644;&#28508;&#22312;&#22522;&#20110;&#22870;&#21169;&#30340;&#22609;&#36896;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#39640;&#32500;&#31995;&#32479;&#20013;&#65292;&#28508;&#22312;&#22522;&#20110;&#22870;&#21169;&#30340;&#22609;&#36896;&#65288;PBRS&#65289;&#23545;&#20110;&#25910;&#25947;&#36895;&#24230;&#30340;&#25552;&#21319;&#25928;&#26524;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2307.10142</link><description>&lt;p&gt;
&#23545;&#20110;&#23398;&#20064;&#20154;&#24418;&#26426;&#26800;&#34892;&#36208;&#30340;&#28508;&#22312;&#22522;&#20110;&#22870;&#21169;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Potential Based Rewards for Learning Humanoid Locomotion. (arXiv:2307.10142v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20154;&#24418;&#26426;&#22120;&#20154;&#20351;&#29992;&#26631;&#20934;&#24418;&#24335;&#30340;&#22870;&#21169;&#22609;&#36896;&#21644;&#28508;&#22312;&#22522;&#20110;&#22870;&#21169;&#30340;&#22609;&#36896;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#39640;&#32500;&#31995;&#32479;&#20013;&#65292;&#28508;&#22312;&#22522;&#20110;&#22870;&#21169;&#30340;&#22609;&#36896;&#65288;PBRS&#65289;&#23545;&#20110;&#25910;&#25947;&#36895;&#24230;&#30340;&#25552;&#21319;&#25928;&#26524;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#21457;&#26377;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;(RL)&#27969;&#31243;&#20013;&#65292;&#20027;&#35201;&#25361;&#25112;&#24448;&#24448;&#26159;&#35774;&#35745;&#21644;&#35843;&#25972;&#22870;&#21169;&#20989;&#25968;&#12290;&#33391;&#22909;&#35774;&#35745;&#30340;&#22609;&#24418;&#22870;&#21169;&#21487;&#20197;&#21152;&#24555;&#23398;&#20064;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#21046;&#23450;&#22870;&#21169;&#21487;&#33021;&#19982;&#26399;&#26395;&#30340;&#34892;&#20026;&#30456;&#20914;&#31361;&#65292;&#22914;&#26524;&#27809;&#26377;&#36866;&#24403;&#35843;&#25972;&#65292;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#25311;&#21512;&#29978;&#33267;&#19981;&#31283;&#23450;&#30340;&#24615;&#33021;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#28508;&#22312;&#22522;&#20110;&#22870;&#21169;&#30340;&#22609;&#24418;(PBRS)&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#26368;&#20248;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#25351;&#23548;&#23398;&#20064;&#36807;&#31243;&#12290;&#23613;&#31649;&#26377;&#20960;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#28508;&#22312;&#22522;&#20110;&#22870;&#21169;&#30340;&#22609;&#24418;&#26469;&#21152;&#24555;&#23398;&#20064;&#25910;&#25947;&#30340;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#23616;&#38480;&#20110;&#32593;&#26684;&#19990;&#30028;&#21644;&#20302;&#32500;&#31995;&#32479;&#65292;&#32780;&#22312;&#26426;&#22120;&#20154;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20027;&#35201;&#20381;&#36182;&#20110;&#26631;&#20934;&#24418;&#24335;&#30340;&#22870;&#21169;&#22609;&#36896;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20351;&#29992;PBRS&#30340;&#26631;&#20934;&#24418;&#24335;&#21644;&#22609;&#24418;&#36827;&#34892;&#20102;&#20154;&#24418;&#26426;&#22120;&#20154;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36825;&#20010;&#39640;&#32500;&#31995;&#32479;&#20013;&#65292;PBRS&#30340;&#25910;&#25947;&#36895;&#24230;&#21482;&#26377;&#24494;&#23567;&#30340;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;PBRS&#22870;&#21169;&#39033;&#20855;&#26377;&#37325;&#22823;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main challenge in developing effective reinforcement learning (RL) pipelines is often the design and tuning the reward functions. Well-designed shaping reward can lead to significantly faster learning. Naively formulated rewards, however, can conflict with the desired behavior and result in overfitting or even erratic performance if not properly tuned. In theory, the broad class of potential based reward shaping (PBRS) can help guide the learning process without affecting the optimal policy. Although several studies have explored the use of potential based reward shaping to accelerate learning convergence, most have been limited to grid-worlds and low-dimensional systems, and RL in robotics has predominantly relied on standard forms of reward shaping. In this paper, we benchmark standard forms of shaping with PBRS for a humanoid robot. We find that in this high-dimensional system, PBRS has only marginal benefits in convergence speed. However, the PBRS reward terms are significantly
&lt;/p&gt;</description></item><item><title>Quarl&#26159;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#37327;&#23376;&#30005;&#36335;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#31070;&#32463;&#26550;&#26500;&#21644;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;&#37327;&#23376;&#30005;&#36335;&#20248;&#21270;&#20013;&#30340;&#21160;&#20316;&#31354;&#38388;&#36739;&#22823;&#21644;&#38750;&#22343;&#21248;&#29366;&#24577;&#34920;&#31034;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Quarl&#22312;&#20960;&#20046;&#25152;&#26377;&#22522;&#20934;&#30005;&#36335;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#20248;&#21270;&#22120;&#65292;&#24182;&#19988;&#22312;&#36895;&#24230;&#19978;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2307.10120</link><description>&lt;p&gt;
Quarl: &#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#37327;&#23376;&#30005;&#36335;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Quarl: A Learning-Based Quantum Circuit Optimizer. (arXiv:2307.10120v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10120
&lt;/p&gt;
&lt;p&gt;
Quarl&#26159;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#37327;&#23376;&#30005;&#36335;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#31070;&#32463;&#26550;&#26500;&#21644;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#36807;&#31243;&#65292;&#35299;&#20915;&#20102;&#37327;&#23376;&#30005;&#36335;&#20248;&#21270;&#20013;&#30340;&#21160;&#20316;&#31354;&#38388;&#36739;&#22823;&#21644;&#38750;&#22343;&#21248;&#29366;&#24577;&#34920;&#31034;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Quarl&#22312;&#20960;&#20046;&#25152;&#26377;&#22522;&#20934;&#30005;&#36335;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#20248;&#21270;&#22120;&#65292;&#24182;&#19988;&#22312;&#36895;&#24230;&#19978;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#30005;&#36335;&#20248;&#21270;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21151;&#33021;&#31561;&#25928;&#30340;&#30005;&#36335;&#25628;&#32034;&#31354;&#38388;&#38750;&#24120;&#22823;&#65292;&#24182;&#19988;&#24517;&#39035;&#24212;&#29992;&#26242;&#26102;&#38477;&#20302;&#24615;&#33021;&#30340;&#36716;&#25442;&#26469;&#23454;&#29616;&#26368;&#32456;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Quarl&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#37327;&#23376;&#30005;&#36335;&#20248;&#21270;&#22120;&#12290;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#37327;&#23376;&#30005;&#36335;&#20248;&#21270;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#24222;&#22823;&#19988;&#21464;&#21270;&#30340;&#21160;&#20316;&#31354;&#38388;&#20197;&#21450;&#19981;&#22343;&#21248;&#30340;&#29366;&#24577;&#34920;&#31034;&#12290;Quarl&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#26550;&#26500;&#21644;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#36807;&#31243;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#31070;&#32463;&#26550;&#26500;&#23558;&#21160;&#20316;&#31354;&#38388;&#20998;&#35299;&#20026;&#20004;&#20010;&#37096;&#20998;&#65292;&#24182;&#22312;&#20854;&#29366;&#24577;&#34920;&#31034;&#20013;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20004;&#20010;&#37096;&#20998;&#37117;&#21463;&#21040;&#26412;&#22320;&#25512;&#29702;&#20027;&#23548;&#20840;&#23616;&#30005;&#36335;&#25512;&#29702;&#30340;&#30452;&#35273;&#25351;&#23548;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;Quarl&#22312;&#20960;&#20046;&#25152;&#26377;&#22522;&#20934;&#30005;&#36335;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#30005;&#36335;&#20248;&#21270;&#22120;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;Quarl&#19981;&#20165;&#36798;&#21040;&#20102;&#27492;&#21069;&#30340;&#26368;&#20339;&#27700;&#24179;&#65292;&#24182;&#19988;&#22312;&#36895;&#24230;&#19978;&#20063;&#27604;&#20854;&#20182;&#20248;&#21270;&#22120;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing quantum circuits is challenging due to the very large search space of functionally equivalent circuits and the necessity of applying transformations that temporarily decrease performance to achieve a final performance improvement. This paper presents Quarl, a learning-based quantum circuit optimizer. Applying reinforcement learning (RL) to quantum circuit optimization raises two main challenges: the large and varying action space and the non-uniform state representation. Quarl addresses these issues with a novel neural architecture and RL-training procedure. Our neural architecture decomposes the action space into two parts and leverages graph neural networks in its state representation, both of which are guided by the intuition that optimization decisions can be mostly guided by local reasoning while allowing global circuit-wide reasoning. Our evaluation shows that Quarl significantly outperforms existing circuit optimizers on almost all benchmark circuits. Surprisingly, Qu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#25193;&#23637;&#30340;&#22270;&#35780;&#20272;&#25351;&#26631;&#65288;GAMs&#65289;&#65292;&#36866;&#29992;&#20110;&#22238;&#24402;&#20219;&#21153;&#21644;&#36830;&#32493;&#37051;&#25509;&#30697;&#38453;&#12290;&#20027;&#35201;&#20851;&#27880;&#30340;&#20004;&#20010;GAMs&#26159;&#21516;&#36136;&#24615;&#21644;&#36328;&#31867;&#37051;&#22495;&#30456;&#20284;&#24230;&#65288;CCNS&#65289;&#12290;&#36825;&#20123;&#25193;&#23637;&#30340;&#25351;&#26631;&#33021;&#22815;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#35780;&#20272;&#22270;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10112</link><description>&lt;p&gt;
&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Extended Graph Assessment Metrics for Graph Neural Networks. (arXiv:2307.10112v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#25193;&#23637;&#30340;&#22270;&#35780;&#20272;&#25351;&#26631;&#65288;GAMs&#65289;&#65292;&#36866;&#29992;&#20110;&#22238;&#24402;&#20219;&#21153;&#21644;&#36830;&#32493;&#37051;&#25509;&#30697;&#38453;&#12290;&#20027;&#35201;&#20851;&#27880;&#30340;&#20004;&#20010;GAMs&#26159;&#21516;&#36136;&#24615;&#21644;&#36328;&#31867;&#37051;&#22495;&#30456;&#20284;&#24230;&#65288;CCNS&#65289;&#12290;&#36825;&#20123;&#25193;&#23637;&#30340;&#25351;&#26631;&#33021;&#22815;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#35780;&#20272;&#22270;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#23558;&#24739;&#32773;&#38431;&#21015;&#37325;&#32452;&#20026;&#25152;&#35859;&#30340;&#20154;&#21475;&#22270;&#26102;&#65292;&#26368;&#21021;&#29420;&#31435;&#30340;&#25968;&#25454;&#28857;&#21487;&#20197;&#21512;&#24182;&#25104;&#19968;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#22270;&#32467;&#26500;&#12290;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#21487;&#20197;&#20351;&#29992;&#36825;&#31181;&#20154;&#21475;&#22270;&#36827;&#34892;&#21307;&#23398;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#36866;&#21512;&#30340;&#22270;&#32467;&#26500;&#30340;&#26500;&#24314;&#26159;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27493;&#39588;&#65292;&#23427;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#30528;&#37325;&#35201;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#24050;&#32463;&#24341;&#20837;&#20102;&#19981;&#21516;&#30340;&#22270;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#22270;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25351;&#26631;&#20165;&#36866;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#21644;&#31163;&#25955;&#30340;&#37051;&#25509;&#30697;&#38453;&#65292;&#21482;&#35206;&#30422;&#20102;&#19968;&#23567;&#37096;&#20998;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38024;&#23545;&#22238;&#24402;&#20219;&#21153;&#21644;&#36830;&#32493;&#37051;&#25509;&#30697;&#38453;&#30340;&#25193;&#23637;&#22270;&#35780;&#20272;&#25351;&#26631;&#65288;GAMs&#65289;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#20004;&#20010;&#20855;&#20307;&#30340;GAMs&#65306;&#21516;&#36136;&#24615;&#21644;&#36328;&#31867;&#37051;&#22495;&#30456;&#20284;&#24230;&#65288;CCNS&#65289;&#12290;&#25105;&#20204;&#23558;GAMs&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#22810;&#20010;&#36339;&#36291;&#65292;&#24182;&#20026;&#22238;&#24402;&#20219;&#21153;&#23450;&#20041;&#20102;&#21516;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When re-structuring patient cohorts into so-called population graphs, initially independent data points can be incorporated into one interconnected graph structure. This population graph can then be used for medical downstream tasks using graph neural networks (GNNs). The construction of a suitable graph structure is a challenging step in the learning pipeline that can have severe impact on model performance. To this end, different graph assessment metrics have been introduced to evaluate graph structures. However, these metrics are limited to classification tasks and discrete adjacency matrices, only covering a small subset of real-world applications. In this work, we introduce extended graph assessment metrics (GAMs) for regression tasks and continuous adjacency matrices. We focus on two GAMs in specific: \textit{homophily} and \textit{cross-class neighbourhood similarity} (CCNS). We extend the notion of GAMs to more than one hop, define homophily for regression tasks, as well as con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28176;&#36827;&#31232;&#30095;&#21270;&#26041;&#27861;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20197;&#25913;&#21892;&#24494;&#35843;&#24615;&#33021;&#12290;GradDrop&#21450;&#20854;&#21464;&#20307;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38543;&#26426;&#23631;&#34109;&#26799;&#24230;&#65292;&#26377;&#25928;&#22320;&#36827;&#34892;&#26799;&#24230;&#31232;&#30095;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.10098</link><description>&lt;p&gt;
&#28176;&#36827;&#31232;&#30095;&#21270;&#29992;&#20110;Transformer&#27169;&#22411;&#30340;&#36974;&#32617;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Gradient Sparsification For Masked Fine-Tuning of Transformers. (arXiv:2307.10098v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28176;&#36827;&#31232;&#30095;&#21270;&#26041;&#27861;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20197;&#25913;&#21892;&#24494;&#35843;&#24615;&#33021;&#12290;GradDrop&#21450;&#20854;&#21464;&#20307;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38543;&#26426;&#23631;&#34109;&#26799;&#24230;&#65292;&#26377;&#25928;&#22320;&#36827;&#34892;&#26799;&#24230;&#31232;&#30095;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21521;&#19979;&#28216;&#20219;&#21153;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#24494;&#35843;&#21487;&#36890;&#36807;&#20923;&#32467;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#26799;&#24230;&#24182;&#21482;&#26356;&#26032;&#26032;&#28155;&#21152;&#30340;&#20998;&#31867;&#23618;&#30340;&#26799;&#24230;&#65292;&#25110;&#36890;&#36807;&#23545;&#25152;&#26377;&#21442;&#25968;&#36827;&#34892;&#26799;&#24230;&#26356;&#26032;&#26469;&#23454;&#29616;&#12290;&#28176;&#36827;&#35299;&#20923;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#35299;&#20923;&#25972;&#20010;&#23618;&#30340;&#26799;&#24230;&#65292;&#20197;&#22312;&#23384;&#20648;&#21644;&#35757;&#32451;&#36895;&#24230;&#19982;&#27867;&#21270;&#24615;&#33021;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#28176;&#36827;&#35299;&#20923;&#25972;&#20010;&#35757;&#32451;&#26159;&#21542;&#26159;&#26368;&#20248;&#36873;&#25321;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#31232;&#30095;&#21464;&#20307;&#30340;&#28176;&#36827;&#35299;&#20923;&#21487;&#33021;&#21487;&#20197;&#25552;&#39640;&#24494;&#35843;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38543;&#26426;&#23631;&#34109;&#26799;&#24230;&#26469;&#27491;&#21017;&#21270;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#25913;&#21892;&#25972;&#20307;&#24494;&#35843;&#24615;&#33021;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;GradDrop&#21450;&#20854;&#21464;&#20307;&#65292;&#19968;&#31867;&#26799;&#24230;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#26799;&#24230;&#36827;&#34892;&#23631;&#34109;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning pretrained self-supervised language models is widely adopted for transfer learning to downstream tasks. Fine-tuning can be achieved by freezing gradients of the pretrained network and only updating gradients of a newly added classification layer, or by performing gradient updates on all parameters. Gradual unfreezing makes a trade-off between the two by gradually unfreezing gradients of whole layers during training. This has been an effective strategy to trade-off between storage and training speed with generalization performance. However, it is not clear whether gradually unfreezing layers throughout training is optimal, compared to sparse variants of gradual unfreezing which may improve fine-tuning performance. In this paper, we propose to stochastically mask gradients to regularize pretrained language models for improving overall fine-tuned performance. We introduce GradDrop and variants thereof, a class of gradient sparsification methods that mask gradients during the b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#36317;&#31163;&#65292;&#22686;&#24378;&#30340;Gromov-Wasserstein&#65292;&#23427;&#22312;Gromov-Wasserstein&#36317;&#31163;&#30340;&#22522;&#30784;&#19978;&#24341;&#20837;&#20102;&#23545;&#21464;&#25442;&#21018;&#24230;&#30340;&#25511;&#21046;&#21644;&#29305;&#24449;&#23545;&#40784;&#65292;&#24182;&#24212;&#29992;&#20110;&#21333;&#32454;&#32990;&#22810;&#32452;&#23398;&#21644;&#36801;&#31227;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10093</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#19981;&#21464;&#24615;&#24182;&#24341;&#20837;&#20808;&#39564;&#30693;&#35782;&#22312;Gromov-Wasserstein&#36317;&#31163;&#20013;
&lt;/p&gt;
&lt;p&gt;
Revisiting invariances and introducing priors in Gromov-Wasserstein distances. (arXiv:2307.10093v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#36317;&#31163;&#65292;&#22686;&#24378;&#30340;Gromov-Wasserstein&#65292;&#23427;&#22312;Gromov-Wasserstein&#36317;&#31163;&#30340;&#22522;&#30784;&#19978;&#24341;&#20837;&#20102;&#23545;&#21464;&#25442;&#21018;&#24230;&#30340;&#25511;&#21046;&#21644;&#29305;&#24449;&#23545;&#40784;&#65292;&#24182;&#24212;&#29992;&#20110;&#21333;&#32454;&#32990;&#22810;&#32452;&#23398;&#21644;&#36801;&#31227;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#33021;&#22815;&#27604;&#36739;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#27979;&#24230;&#24182;&#19988;&#23545;&#31561;&#24230;&#21464;&#25442;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;Gromov-Wasserstein&#36317;&#31163;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26377;&#24456;&#22810;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24212;&#29992;&#20013;&#65292;&#36825;&#31181;&#19981;&#21464;&#24615;&#21487;&#33021;&#36807;&#20110;&#28789;&#27963;&#32780;&#19981;&#21487;&#21462;&#12290;&#27492;&#22806;&#65292;Gromov-Wasserstein&#36317;&#31163;&#20165;&#32771;&#34385;&#36755;&#20837;&#25968;&#25454;&#38598;&#20013;&#30340;&#25104;&#23545;&#26679;&#26412;&#30456;&#20284;&#24615;&#65292;&#32780;&#24573;&#30053;&#21407;&#22987;&#29305;&#24449;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#36317;&#31163;&#65292;&#31216;&#20026;&#22686;&#24378;&#30340;Gromov-Wasserstein&#65292;&#23427;&#20801;&#35768;&#23545;&#21464;&#25442;&#30340;&#21018;&#24230;&#26377;&#19968;&#23450;&#25511;&#21046;&#12290;&#23427;&#36824;&#32467;&#21512;&#20102;&#29305;&#24449;&#23545;&#40784;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#36755;&#20837;&#25968;&#25454;&#19978;&#30340;&#20808;&#39564;&#30693;&#35782;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#30340;&#29702;&#35770;&#27934;&#23519;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#21333;&#32454;&#32990;&#22810;&#32452;&#23398;&#23545;&#40784;&#20219;&#21153;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gromov-Wasserstein distance has found many applications in machine learning due to its ability to compare measures across metric spaces and its invariance to isometric transformations. However, in certain applications, this invariance property can be too flexible, thus undesirable. Moreover, the Gromov-Wasserstein distance solely considers pairwise sample similarities in input datasets, disregarding the raw feature representations. We propose a new optimal transport-based distance, called Augmented Gromov-Wasserstein, that allows for some control over the level of rigidity to transformations. It also incorporates feature alignments, enabling us to better leverage prior knowledge on the input data for improved performance. We present theoretical insights into the proposed metric. We then demonstrate its usefulness for single-cell multi-omic alignment tasks and a transfer learning scenario in machine learning.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Android in the Wild (AITW)&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#35774;&#22791;&#25511;&#21046;&#31995;&#32479;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#20154;&#31867;&#31034;&#33539;&#30340;&#35774;&#22791;&#20132;&#20114;&#12289;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#22810;&#31181;Android&#29256;&#26412;&#21644;&#35774;&#22791;&#31867;&#22411;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#20174;&#35270;&#35273;&#22806;&#35266;&#20013;&#25512;&#26029;&#29992;&#25143;&#30028;&#38754;&#20013;&#21487;&#29992;&#30340;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2307.10088</link><description>&lt;p&gt;
&#22312;&#37326;&#22806;&#30340;Android&#65306;&#29992;&#20110;Android&#35774;&#22791;&#25511;&#21046;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Android in the Wild: A Large-Scale Dataset for Android Device Control. (arXiv:2307.10088v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10088
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Android in the Wild (AITW)&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#35774;&#22791;&#25511;&#21046;&#31995;&#32479;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#20154;&#31867;&#31034;&#33539;&#30340;&#35774;&#22791;&#20132;&#20114;&#12289;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#22810;&#31181;Android&#29256;&#26412;&#21644;&#35774;&#22791;&#31867;&#22411;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#20174;&#35270;&#35273;&#22806;&#35266;&#20013;&#25512;&#26029;&#29992;&#25143;&#30028;&#38754;&#20013;&#21487;&#29992;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#33021;&#22815;&#35299;&#37322;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#24182;&#30452;&#25509;&#25511;&#21046;&#25968;&#23383;&#35774;&#22791;&#29992;&#25143;&#30028;&#38754;&#25191;&#34892;&#30340;&#35774;&#22791;&#25511;&#21046;&#31995;&#32479;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35774;&#22791;&#25511;&#21046;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#65292;Android in the Wild (AITW)&#65292;&#35813;&#25968;&#25454;&#38598;&#27604;&#24403;&#21069;&#25968;&#25454;&#38598;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#35774;&#22791;&#20132;&#20114;&#30340;&#20154;&#31867;&#31034;&#33539;&#65292;&#21253;&#25324;&#23631;&#24149;&#21644;&#25805;&#20316;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12290;&#23427;&#21253;&#25324;715k&#20010;&#21095;&#38598;&#65292;&#28085;&#30422;30k&#20010;&#19981;&#21516;&#30340;&#25351;&#20196;&#65292;&#22235;&#20010;Android&#29256;&#26412;&#65288;v10-13&#65289;&#65292;&#20197;&#21450;&#20843;&#31181;&#19981;&#21516;&#30340;&#35774;&#22791;&#31867;&#22411;&#65288;&#20174;Pixel 2 XL&#21040;Pixel 6&#65289;&#21644;&#19981;&#21516;&#30340;&#23631;&#24149;&#20998;&#36776;&#29575;&#12290;&#23427;&#21253;&#21547;&#38656;&#35201;&#35821;&#35328;&#21644;&#35270;&#35273;&#19978;&#19979;&#25991;&#30340;&#35821;&#20041;&#29702;&#35299;&#30340;&#22810;&#27493;&#39588;&#20219;&#21153;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65306;&#24517;&#39035;&#20174;&#23427;&#20204;&#30340;&#35270;&#35273;&#22806;&#35266;&#20013;&#25512;&#26029;&#20986;&#29992;&#25143;&#30028;&#38754;&#20013;&#21487;&#29992;&#30340;&#25805;&#20316;&#12290;&#32780;&#19988;&#65292;&#34892;&#21160;&#31354;&#38388;&#19981;&#20877;&#26159;&#31616;&#21333;&#30340;&#22522;&#20110;&#29992;&#25143;&#30028;&#38754;&#20803;&#32032;&#30340;&#34892;&#21160;&#65292;&#32780;&#26159;&#21253;&#21547;&#31934;&#30830;&#30340;&#25163;&#21183;&#65288;&#20363;&#22914;&#65292;&#27700;&#24179;&#28378;&#21160;&#65289;
&lt;/p&gt;
&lt;p&gt;
There is a growing interest in device-control systems that can interpret human natural language instructions and execute them on a digital device by directly controlling its user interface. We present a dataset for device-control research, Android in the Wild (AITW), which is orders of magnitude larger than current datasets. The dataset contains human demonstrations of device interactions, including the screens and actions, and corresponding natural language instructions. It consists of 715k episodes spanning 30k unique instructions, four versions of Android (v10-13),and eight device types (Pixel 2 XL to Pixel 6) with varying screen resolutions. It contains multi-step tasks that require semantic understanding of language and visual context. This dataset poses a new challenge: actions available through the user interface must be inferred from their visual appearance. And, instead of simple UI element-based actions, the action space consists of precise gestures (e.g., horizontal scrolls 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#27010;&#29575;&#20027;&#25104;&#20998;&#20998;&#26512;&#22312;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#21452;&#37325;&#34920;&#36848;&#26041;&#27861;&#65292;&#24182;&#21457;&#23637;&#20102;&#36866;&#29992;&#20110;&#26680;&#26041;&#27861;&#30340;&#29983;&#25104;&#26694;&#26550;&#12290;&#20316;&#32773;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#33021;&#20860;&#23481;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#24182;&#22312;&#34394;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.10078</link><description>&lt;p&gt;
&#27010;&#29575;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#21452;&#37325;&#34920;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Dual Formulation for Probabilistic Principal Component Analysis. (arXiv:2307.10078v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#27010;&#29575;&#20027;&#25104;&#20998;&#20998;&#26512;&#22312;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#21452;&#37325;&#34920;&#36848;&#26041;&#27861;&#65292;&#24182;&#21457;&#23637;&#20102;&#36866;&#29992;&#20110;&#26680;&#26041;&#27861;&#30340;&#29983;&#25104;&#26694;&#26550;&#12290;&#20316;&#32773;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#33021;&#20860;&#23481;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#24182;&#22312;&#34394;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#23545;&#27010;&#29575;&#20027;&#25104;&#20998;&#20998;&#26512;&#36827;&#34892;&#20102;&#34920;&#36848;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#20248;&#35299;&#22312;&#23545;&#20598;&#31354;&#38388;&#20013;&#30340;&#34920;&#31034;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#21457;&#23637;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#26680;&#26041;&#27861;&#30340;&#29983;&#25104;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#21560;&#32435;&#20102;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#24182;&#22312;&#19968;&#20010;&#34394;&#25311;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we characterize Probabilistic Principal Component Analysis in Hilbert spaces and demonstrate how the optimal solution admits a representation in dual space. This allows us to develop a generative framework for kernel methods. Furthermore, we show how it englobes Kernel Principal Component Analysis and illustrate its working on a toy and a real dataset.
&lt;/p&gt;</description></item><item><title>RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RNAformer&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#36724;&#21521;&#27880;&#24847;&#21147;&#21644;&#28508;&#31354;&#38388;&#22238;&#25910;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#28508;&#31354;&#38388;&#30452;&#25509;&#24314;&#27169;&#37051;&#25509;&#30697;&#38453;&#30340;&#26550;&#26500;&#21644;&#25193;&#23637;&#27169;&#22411;&#35268;&#27169;&#26469;&#33719;&#24471;&#24615;&#33021;&#25913;&#21892;&#12290;&#35813;&#26041;&#27861;&#22312;TS0&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#20197;&#23398;&#20064;RNA&#25240;&#21472;&#36807;&#31243;&#30340;&#29983;&#29289;&#29289;&#29702;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.10073</link><description>&lt;p&gt;
&#21487;&#20280;&#32553;&#30340;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Scalable Deep Learning for RNA Secondary Structure Prediction. (arXiv:2307.10073v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10073
&lt;/p&gt;
&lt;p&gt;
RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RNAformer&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#36724;&#21521;&#27880;&#24847;&#21147;&#21644;&#28508;&#31354;&#38388;&#22238;&#25910;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#28508;&#31354;&#38388;&#30452;&#25509;&#24314;&#27169;&#37051;&#25509;&#30697;&#38453;&#30340;&#26550;&#26500;&#21644;&#25193;&#23637;&#27169;&#22411;&#35268;&#27169;&#26469;&#33719;&#24471;&#24615;&#33021;&#25913;&#21892;&#12290;&#35813;&#26041;&#27861;&#22312;TS0&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#20197;&#23398;&#20064;RNA&#25240;&#21472;&#36807;&#31243;&#30340;&#29983;&#29289;&#29289;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RNAformer&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#36724;&#21521;&#27880;&#24847;&#21147;&#21644;&#28508;&#31354;&#38388;&#22238;&#25910;&#30340;&#31616;&#27905;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#28508;&#31354;&#38388;&#30452;&#25509;&#35774;&#35745;&#37051;&#25509;&#30697;&#38453;&#30340;&#26550;&#26500;&#65292;&#24182;&#25193;&#23637;&#27169;&#22411;&#30340;&#35268;&#27169;&#26469;&#33719;&#24471;&#24615;&#33021;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27969;&#34892;&#30340;TS0&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36229;&#36807;&#20102;&#20351;&#29992;&#22806;&#37096;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;RNAformer&#21487;&#20197;&#23398;&#20064;RNA&#25240;&#21472;&#36807;&#31243;&#30340;&#29983;&#29289;&#29289;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of RNA secondary structure prediction has made significant progress with the adoption of deep learning techniques. In this work, we present the RNAformer, a lean deep learning model using axial attention and recycling in the latent space. We gain performance improvements by designing the architecture for modeling the adjacency matrix directly in the latent space and by scaling the size of the model. Our approach achieves state-of-the-art performance on the popular TS0 benchmark dataset and even outperforms methods that use external information. Further, we show experimentally that the RNAformer can learn a biophysical model of the RNA folding process.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#28304;&#25968;&#25454;&#30340;&#26694;&#26550;&#26469;&#20272;&#35745;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#22312;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20266;&#26631;&#31614;&#21644;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#31639;&#27861;&#36827;&#34892;&#20934;&#30830;&#24615;&#20272;&#35745;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20837;&#36827;&#34892;&#33258;&#36866;&#24212;&#23545;&#25239;&#24615;&#25200;&#21160;&#26469;&#22788;&#29702;&#38169;&#35823;&#20266;&#26631;&#31614;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.10062</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#28304;&#26679;&#26412;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#23545;&#25239;&#25200;&#21160;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#20934;&#30830;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Accuracy Estimation of Deep Visual Models using Domain-Adaptive Adversarial Perturbation without Source Samples. (arXiv:2307.10062v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#28304;&#25968;&#25454;&#30340;&#26694;&#26550;&#26469;&#20272;&#35745;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#22312;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20266;&#26631;&#31614;&#21644;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#31639;&#27861;&#36827;&#34892;&#20934;&#30830;&#24615;&#20272;&#35745;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20837;&#36827;&#34892;&#33258;&#36866;&#24212;&#23545;&#25239;&#24615;&#25200;&#21160;&#26469;&#22788;&#29702;&#38169;&#35823;&#20266;&#26631;&#31614;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#21407;&#22240;&#26159;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#19968;&#20123;&#26041;&#27861;&#21033;&#29992;&#24102;&#26631;&#31614;&#30340;&#28304;&#25968;&#25454;&#26469;&#20272;&#35745;&#30446;&#26631;&#22495;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#20445;&#23494;&#24615;&#25110;&#26381;&#21153;&#35774;&#22791;&#19978;&#30340;&#36164;&#28304;&#38480;&#21046;&#65292;&#35775;&#38382;&#24102;&#26631;&#31614;&#30340;&#28304;&#25968;&#25454;&#36890;&#24120;&#38750;&#24120;&#22256;&#38590;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#20934;&#30830;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#20266;&#26631;&#31614;&#36827;&#34892;&#20934;&#30830;&#24615;&#20272;&#35745;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#37319;&#29992;&#20102;&#26368;&#36817;&#30340;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#31639;&#27861;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34913;&#37327;&#20102;&#20174;&#28304;&#20551;&#35774;&#28436;&#21270;&#32780;&#26469;&#30340;&#30446;&#26631;&#20266;&#26631;&#35760;&#20989;&#25968;&#19982;&#28304;&#20551;&#35774;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#29575;&#12290;&#20026;&#20102;&#20943;&#36731;&#30001;&#20110;&#29702;&#24819;&#32852;&#21512;&#20551;&#35774;&#39118;&#38505;&#36739;&#39640;&#32780;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#20266;&#26631;&#31614;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#37319;&#29992;&#33258;&#36866;&#24212;&#23545;&#25239;&#24615;&#25200;&#21160;&#26469;&#22788;&#29702;&#30446;&#26631;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying deep visual models can lead to performance drops due to the discrepancies between source and target distributions. Several approaches leverage labeled source data to estimate target domain accuracy, but accessing labeled source data is often prohibitively difficult due to data confidentiality or resource limitations on serving devices. Our work proposes a new framework to estimate model accuracy on unlabeled target data without access to source data. We investigate the feasibility of using pseudo-labels for accuracy estimation and evolve this idea into adopting recent advances in source-free domain adaptation algorithms. Our approach measures the disagreement rate between the source hypothesis and the target pseudo-labeling function, adapted from the source hypothesis. We mitigate the impact of erroneous pseudo-labels that may arise due to a high ideal joint hypothesis risk by employing adaptive adversarial perturbation on the input of the target model. Our proposed source-fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#29992;&#20110;&#22823;&#28065;&#27169;&#25311;&#30340;&#23376;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#65292;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#24182;&#19982;&#20998;&#26512;&#27169;&#22411;&#30456;&#27604;&#33021;&#22815;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#20854;&#20013;&#19968;&#20010;&#27169;&#22411;&#34701;&#20837;&#20102;&#22810;&#20010;&#19981;&#21464;&#24615;&#65292;&#21478;&#19968;&#20010;&#21482;&#34701;&#20837;&#20102;&#20285;&#21033;&#30053;&#19981;&#21464;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10060</link><description>&lt;p&gt;
&#20934;&#30830;&#30340;&#28145;&#24230;&#23398;&#20064;&#23376;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#29992;&#20110;&#22823;&#28065;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Accurate deep learning sub-grid scale models for large eddy simulations. (arXiv:2307.10060v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#29992;&#20110;&#22823;&#28065;&#27169;&#25311;&#30340;&#23376;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#65292;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#24182;&#19982;&#20998;&#26512;&#27169;&#22411;&#30456;&#27604;&#33021;&#22815;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#20854;&#20013;&#19968;&#20010;&#27169;&#22411;&#34701;&#20837;&#20102;&#22810;&#20010;&#19981;&#21464;&#24615;&#65292;&#21478;&#19968;&#20010;&#21482;&#34701;&#20837;&#20102;&#20285;&#21033;&#30053;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#29992;&#20110;&#22823;&#28065;&#27169;&#25311;&#30340;&#23376;&#32593;&#26684;&#23610;&#24230;&#65288;SGS&#65289;&#28237;&#27969;&#27169;&#22411;&#31995;&#21015;&#12290;&#23427;&#20204;&#30340;&#24320;&#21457;&#38656;&#35201;&#21046;&#23450;&#32463;&#36807;&#29289;&#29702;&#39564;&#35777;&#30340;&#24378;&#22823;&#32780;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#31639;&#27861;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#20998;&#26512;&#24314;&#27169;&#25216;&#26415;&#19981;&#21516;&#65292;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#20135;&#29983;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#30340;&#39640;&#38454;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#36890;&#36807;&#20174;&#20004;&#20010;&#25705;&#25830;&#38647;&#35834;&#25968;&#32422;&#20026;395&#21644;590&#30340;&#20856;&#22411;&#36890;&#36947;&#27969;&#30340;&#30452;&#25509;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#26174;&#24335;&#28388;&#27874;&#65292;&#25552;&#20379;&#20102;&#29992;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#20934;&#30830;&#25968;&#25454;&#12290;&#36825;&#20004;&#32452;&#27169;&#22411;&#20351;&#29992;&#19981;&#21516;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#20854;&#20013;&#19968;&#31181;&#26550;&#26500;&#20351;&#29992;&#24352;&#37327;&#22522;&#31070;&#32463;&#32593;&#32476;&#65288;TBNN&#65289;&#65292;&#23884;&#20837;&#20102;&#31616;&#21270;&#30340;&#20998;&#26512;&#27169;&#22411;&#24418;&#24335;&#30340;&#19968;&#33324;&#26377;&#25928;&#31896;&#24615;&#20551;&#35774;&#65292;&#20174;&#32780;&#34701;&#20837;&#20102;&#20285;&#21033;&#30053;&#12289;&#26059;&#36716;&#21644;&#21453;&#23556;&#19981;&#21464;&#24615;&#12290;&#32780;&#21478;&#19968;&#31181;&#26550;&#26500;&#26159;&#19968;&#20010;&#30456;&#23545;&#31616;&#21333;&#30340;&#32593;&#32476;&#65292;&#23427;&#21482;&#33021;&#34701;&#20837;&#20285;&#21033;&#30053;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present two families of sub-grid scale (SGS) turbulence models developed for large-eddy simulation (LES) purposes. Their development required the formulation of physics-informed robust and efficient Deep Learning (DL) algorithms which, unlike state-of-the-art analytical modeling techniques can produce high-order complex non-linear relations between inputs and outputs. Explicit filtering of data from direct simulations of the canonical channel flow at two friction Reynolds numbers $Re_\tau\approx 395$ and 590 provided accurate data for training and testing. The two sets of models use different network architectures. One of the architectures uses tensor basis neural networks (TBNN) and embeds the simplified analytical model form of the general effective-viscosity hypothesis, thus incorporating the Galilean, rotational and reflectional invariances. The other architecture is that of a relatively simple network, that is able to incorporate the Galilean invariance only. However, this simp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#24179;&#28369;&#38750;&#20984;&#20248;&#21270;&#20013;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#21333;&#26102;&#38388;&#23610;&#24230;&#21644;&#21452;&#26102;&#38388;&#23610;&#24230;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;&#20102;&#22810;&#31181;&#24050;&#30693;&#30340;SGD&#31867;&#22411;&#26041;&#27861;&#12290;&#23545;&#20110;&#26377;&#38480;&#21644;&#24418;&#24335;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#27493;&#38271;&#21644;&#21021;&#22987;&#28857;&#19978;&#25214;&#21040;Clarke&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.10053</link><description>&lt;p&gt;
&#38750;&#24179;&#28369;&#38750;&#20984;&#20248;&#21270;&#20013;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Convergence Guarantees for Stochastic Subgradient Methods in Nonsmooth Nonconvex Optimization. (arXiv:2307.10053v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#24179;&#28369;&#38750;&#20984;&#20248;&#21270;&#20013;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#21333;&#26102;&#38388;&#23610;&#24230;&#21644;&#21452;&#26102;&#38388;&#23610;&#24230;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;&#20102;&#22810;&#31181;&#24050;&#30693;&#30340;SGD&#31867;&#22411;&#26041;&#27861;&#12290;&#23545;&#20110;&#26377;&#38480;&#21644;&#24418;&#24335;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#27493;&#38271;&#21644;&#21021;&#22987;&#28857;&#19978;&#25214;&#21040;Clarke&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#27861;&#21450;&#20854;&#21464;&#31181;&#22312;&#35757;&#32451;&#30001;&#38750;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#26500;&#24314;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20026;&#26356;&#26032;&#21160;&#37327;&#39033;&#21644;&#21464;&#37327;&#30340;&#27493;&#38271;&#20998;&#37197;&#20102;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#12290;&#22312;&#19968;&#20123;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#21333;&#26102;&#38388;&#23610;&#24230;&#21644;&#21452;&#26102;&#38388;&#23610;&#24230;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21253;&#21547;&#20102;&#24456;&#22810;&#24050;&#30693;&#30340;SGD&#31867;&#22411;&#26041;&#27861;&#65292;&#21253;&#25324;heavy-ball SGD&#12289;SignSGD&#12289;Lion&#12289;normalized SGD&#21644;clipped SGD&#12290;&#27492;&#22806;&#65292;&#24403;&#30446;&#26631;&#20989;&#25968;&#37319;&#29992;&#26377;&#38480;&#21644;&#24418;&#24335;&#26102;&#65292;&#25105;&#20204;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#35777;&#26126;&#20102;&#36825;&#20123;SGD&#31867;&#22411;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;SGD&#31867;&#22411;&#26041;&#27861;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#27493;&#38271;&#21644;&#21021;&#22987;&#28857;&#19978;&#33021;&#22815;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340;Clarke&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the convergence properties of the stochastic gradient descent (SGD) method and its variants, especially in training neural networks built from nonsmooth activation functions. We develop a novel framework that assigns different timescales to stepsizes for updating the momentum terms and variables, respectively. Under mild conditions, we prove the global convergence of our proposed framework in both single-timescale and two-timescale cases. We show that our proposed framework encompasses a wide range of well-known SGD-type methods, including heavy-ball SGD, SignSGD, Lion, normalized SGD and clipped SGD. Furthermore, when the objective function adopts a finite-sum formulation, we prove the convergence properties for these SGD-type methods based on our proposed framework. In particular, we prove that these SGD-type methods find the Clarke stationary points of the objective function with randomly chosen stepsizes and initial points under mild assumptions. Preli
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#65292;&#21363;&#19978;&#19979;&#25991;&#21487;&#38752;&#24615;&#65292;&#23427;&#32771;&#34385;&#21040;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#20351;&#29992;&#30340;&#8220;&#27491;&#30830;&#8221;&#29305;&#24449;&#21487;&#33021;&#20250;&#26377;&#25152;&#21464;&#21270;&#12290;&#20316;&#32773;&#25552;&#20986;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;ENP&#33021;&#22815;&#39318;&#20808;&#35782;&#21035;&#32473;&#23450;&#19978;&#19979;&#25991;&#20013;&#35201;&#20351;&#29992;&#30340;&#30456;&#20851;&#29305;&#24449;&#65292;&#28982;&#21518;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10026</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#21487;&#38752;&#24615;&#65306;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#19981;&#21516;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Contextual Reliability: When Different Features Matter in Different Contexts. (arXiv:2307.10026v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#65292;&#21363;&#19978;&#19979;&#25991;&#21487;&#38752;&#24615;&#65292;&#23427;&#32771;&#34385;&#21040;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#20351;&#29992;&#30340;&#8220;&#27491;&#30830;&#8221;&#29305;&#24449;&#21487;&#33021;&#20250;&#26377;&#25152;&#21464;&#21270;&#12290;&#20316;&#32773;&#25552;&#20986;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;ENP&#33021;&#22815;&#39318;&#20808;&#35782;&#21035;&#32473;&#23450;&#19978;&#19979;&#25991;&#20013;&#35201;&#20351;&#29992;&#30340;&#30456;&#20851;&#29305;&#24449;&#65292;&#28982;&#21518;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#22240;&#20381;&#36182;&#22122;&#22768;&#30456;&#20851;&#24615;&#32780;&#20135;&#29983;&#28798;&#38590;&#24615;&#38169;&#35823;&#12290;&#22823;&#37096;&#20998;&#30340;&#30740;&#31350;&#20551;&#35774;&#26377;&#19968;&#20010;&#26126;&#30830;&#30340;&#34394;&#20551;&#21644;&#21487;&#38752;&#29305;&#24449;&#30340;&#20108;&#20998;&#27861;&#65292;&#28982;&#32780;&#36825;&#36890;&#24120;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#20363;&#22914;&#65292;&#22823;&#37096;&#20998;&#24773;&#20917;&#19979;&#25105;&#20204;&#19981;&#24819;&#35753;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#31616;&#21333;&#22320;&#22797;&#21046;&#21608;&#22260;&#36710;&#36742;&#30340;&#36895;&#24230; -- &#22914;&#26524;&#37051;&#36817;&#30340;&#36710;&#36742;&#36829;&#31456;&#65292;&#25105;&#20204;&#19981;&#24819;&#35753;&#25105;&#20204;&#30340;&#36710;&#20063;&#38383;&#32418;&#28783;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#19981;&#33021;&#31616;&#21333;&#22320;&#24378;&#21046;&#35201;&#27714;&#19981;&#21464;&#24615;&#20197;&#25490;&#38500;&#19979;&#19968;&#26465;&#36710;&#36947;&#30340;&#36895;&#24230;&#65292;&#22240;&#20026;&#36825;&#21487;&#33021;&#25552;&#20379;&#20102;&#20851;&#20110;&#19968;&#20010;&#26080;&#27861;&#35266;&#27979;&#21040;&#30340;&#20154;&#34892;&#27178;&#36947;&#19978;&#34892;&#20154;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#26222;&#36941;&#22320;&#24573;&#35270;&#26377;&#26102;&#21487;&#38752;&#65288;&#20294;&#24182;&#38750;&#24635;&#26159;&#65289;&#30340;&#29305;&#24449;&#20250;&#23548;&#33268;&#24615;&#33021;&#38750;&#40065;&#26834;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31216;&#20026;&#19978;&#19979;&#25991;&#21487;&#38752;&#24615;&#30340;&#35774;&#23450;&#65292;&#32771;&#34385;&#21040;&#20351;&#29992;&#8220;&#27491;&#30830;&#8221;&#29305;&#24449;&#21487;&#33021;&#20250;&#22240;&#19978;&#19979;&#25991;&#30340;&#19981;&#21516;&#32780;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#20010;&#31216;&#20026;&#26174;&#24335;&#38750;&#34394;&#20551;&#29305;&#24449;&#39044;&#27979;&#65288;ENP&#65289;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#39318;&#20808;&#35782;&#21035;&#32473;&#23450;&#19978;&#19979;&#25991;&#20013;&#35201;&#20351;&#29992;&#30340;&#30456;&#20851;&#29305;&#24449;&#65292;&#28982;&#21518;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks often fail catastrophically by relying on spurious correlations. Most prior work assumes a clear dichotomy into spurious and reliable features; however, this is often unrealistic. For example, most of the time we do not want an autonomous car to simply copy the speed of surrounding cars -- we don't want our car to run a red light if a neighboring car does so. However, we cannot simply enforce invariance to next-lane speed, since it could provide valuable information about an unobservable pedestrian at a crosswalk. Thus, universally ignoring features that are sometimes (but not always) reliable can lead to non-robust performance. We formalize a new setting called contextual reliability which accounts for the fact that the "right" features to use may vary depending on the context. We propose and analyze a two-stage framework called Explicit Non-spurious feature Prediction (ENP) which first identifies the relevant features to use for a given context, then trains a mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#27431;&#30431;&#21644;&#33521;&#22269;&#30340;&#22269;&#23478;&#23618;&#38754;&#21382;&#21490;&#27665;&#24847;&#35843;&#26597;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#25968;&#25454;&#30340;&#31354;&#30333;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25581;&#31034;&#22810;&#27169;&#24577;&#25968;&#25454;&#19982;&#36873;&#27665;&#34892;&#20026;&#20043;&#38388;&#30340;&#22797;&#26434;&#20114;&#21160;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2307.10022</link><description>&lt;p&gt;
Europepolls: &#19968;&#20010;&#20851;&#20110;&#27431;&#30431;&#21644;&#33521;&#22269;&#30340;&#22269;&#23478;&#23618;&#38754;&#27665;&#24847;&#35843;&#26597;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Europepolls: A Dataset of Country-Level Opinion Polling Data for the European Union and the UK. (arXiv:2307.10022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#27431;&#30431;&#21644;&#33521;&#22269;&#30340;&#22269;&#23478;&#23618;&#38754;&#21382;&#21490;&#27665;&#24847;&#35843;&#26597;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#25968;&#25454;&#30340;&#31354;&#30333;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25581;&#31034;&#22810;&#27169;&#24577;&#25968;&#25454;&#19982;&#36873;&#27665;&#34892;&#20026;&#20043;&#38388;&#30340;&#22797;&#26434;&#20114;&#21160;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#27431;&#30431;&#21644;&#33521;&#22269;&#22269;&#23478;&#23618;&#38754;&#21382;&#21490;&#27665;&#24847;&#35843;&#26597;&#25968;&#25454;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#27431;&#30431;&#27665;&#24847;&#35843;&#26597;&#25968;&#25454;&#30340;&#31354;&#30333;&#65292;&#19968;&#20123;&#29616;&#26377;&#25968;&#25454;&#38598;&#20165;&#28085;&#30422;&#36807;&#21435;&#20116;&#24180;&#65292;&#38480;&#21046;&#20102;&#30740;&#31350;&#26426;&#20250;&#12290;&#21516;&#26102;&#65292;&#19968;&#20123;&#26356;&#22823;&#30340;&#19987;&#26377;&#25968;&#25454;&#38598;&#20197;&#35270;&#35273;&#39044;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#26684;&#24335;&#25552;&#20379;&#65292;&#26368;&#21518;&#65292;&#34429;&#28982;&#21487;&#33021;&#26377;&#20854;&#20182;&#22823;&#22411;&#22269;&#23478;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#65292;&#20294;&#30001;&#20110;&#35821;&#35328;&#38556;&#30861;&#32780;&#26080;&#27861;&#33719;&#21462;&#12290;&#25968;&#25454;&#25910;&#38598;&#33258;&#32500;&#22522;&#30334;&#31185;&#65292;&#24182;&#20351;&#29992;pandas&#24211;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#21407;&#22987;&#25968;&#25454;&#21644;&#39044;&#22788;&#29702;&#25968;&#25454;&#22343;&#20026;.csv&#26684;&#24335;&#12290;&#25105;&#24076;&#26395;&#65292;&#37492;&#20110;&#26368;&#36817;&#22312;&#35821;&#35328;&#27169;&#22411;&#21644;&#28145;&#24230;&#23398;&#20064;&#31561;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#36825;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#23558;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#25581;&#31034;&#22810;&#27169;&#24577;&#25968;&#25454;&#65288;&#26032;&#38395;&#25991;&#31456;&#12289;&#32463;&#27982;&#25351;&#26631;&#12289;&#31038;&#20132;&#23186;&#20307;&#65289;&#19982;&#36873;&#27665;&#34892;&#20026;&#20043;&#38388;&#30340;&#22797;&#26434;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
I propose an open dataset of country-level historical opinion polling data for the European Union and the UK. The dataset aims to fill a gap in available opinion polling data for the European Union. Some existing datasets are restricted to the past five years, limiting research opportunities. At the same time, some larger proprietary datasets exist but are available only in a visual preprocessed time series format. Finally, while other large datasets for individual countries might exist, these could be inaccessible due to language barriers. The data was gathered from Wikipedia, and preprocessed using the pandas library. Both the raw and the preprocessed data are in the .csv format. I hope that given the recent advances in LLMs and deep learning in general, this large dataset will enable researchers to uncover complex interactions between multimodal data (news articles, economic indicators, social media) and voting behavior. The raw data, the preprocessed data, and the preprocessing scr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TbExplain&#30340;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;XAI&#25216;&#26415;&#21644;&#39044;&#35757;&#32451;&#30340;&#23545;&#35937;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#25991;&#26412;&#24418;&#24335;&#35299;&#37322;&#22330;&#26223;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#32416;&#27491;&#39044;&#27979;&#21644;&#36827;&#34892;&#25991;&#26412;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2307.10003</link><description>&lt;p&gt;
TbExplain: &#19968;&#31181;&#22330;&#26223;&#20998;&#31867;&#27169;&#22411;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#35299;&#37322;&#26041;&#27861;&#19982;&#32479;&#35745;&#39044;&#27979;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
TbExplain: A Text-based Explanation Method for Scene Classification Models with the Statistical Prediction Correction. (arXiv:2307.10003v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TbExplain&#30340;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;XAI&#25216;&#26415;&#21644;&#39044;&#35757;&#32451;&#30340;&#23545;&#35937;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#25991;&#26412;&#24418;&#24335;&#35299;&#37322;&#22330;&#26223;&#20998;&#31867;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#32416;&#27491;&#39044;&#27979;&#21644;&#36827;&#34892;&#25991;&#26412;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;(XAI)&#30340;&#39046;&#22495;&#26088;&#22312;&#25552;&#39640;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#24314;&#31435;&#22522;&#20110;&#36755;&#20837;&#29305;&#24449;&#37325;&#35201;&#24615;&#20540;&#30340;&#28909;&#22270;&#26159;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#39044;&#27979;&#30340;&#22522;&#26412;&#26041;&#27861;&#20043;&#19968;&#12290;&#28909;&#22270;&#22312;&#20154;&#31867;&#20013;&#20960;&#20046;&#21487;&#20197;&#29702;&#35299;&#65292;&#20294;&#24182;&#38750;&#27809;&#26377;&#32570;&#38519;&#12290;&#20363;&#22914;&#65292;&#38750;&#19987;&#19994;&#29992;&#25143;&#21487;&#33021;&#19981;&#23436;&#20840;&#29702;&#35299;&#28909;&#22270;&#30340;&#36923;&#36753;&#65288;&#21363;&#20351;&#29992;&#19981;&#21516;&#24378;&#24230;&#25110;&#39068;&#33394;&#31361;&#20986;&#26174;&#31034;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#20851;&#30340;&#20687;&#32032;&#30340;&#36923;&#36753;&#65289;&#12290;&#27492;&#22806;&#65292;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#20851;&#30340;&#36755;&#20837;&#22270;&#20687;&#30340;&#23545;&#35937;&#21644;&#21306;&#22495;&#36890;&#24120;&#26080;&#27861;&#23436;&#20840;&#36890;&#36807;&#28909;&#22270;&#21306;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;TbExplain&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;XAI&#25216;&#26415;&#21644;&#39044;&#35757;&#32451;&#30340;&#23545;&#35937;&#26816;&#27979;&#22120;&#65292;&#20197;&#25991;&#26412;&#24418;&#24335;&#35299;&#37322;&#22330;&#26223;&#20998;&#31867;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;TbExplain&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#32416;&#27491;&#39044;&#27979;&#21644;&#36827;&#34892;&#25991;&#26412;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Explainable Artificial Intelligence (XAI) aims to improve the interpretability of black-box machine learning models. Building a heatmap based on the importance value of input features is a popular method for explaining the underlying functions of such models in producing their predictions. Heatmaps are almost understandable to humans, yet they are not without flaws. Non-expert users, for example, may not fully understand the logic of heatmaps (the logic in which relevant pixels to the model's prediction are highlighted with different intensities or colors). Additionally, objects and regions of the input image that are relevant to the model prediction are frequently not entirely differentiated by heatmaps. In this paper, we propose a framework called TbExplain that employs XAI techniques and a pre-trained object detector to present text-based explanations of scene classification models. Moreover, TbExplain incorporates a novel method to correct predictions and textually exp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;Beta-VAE&#26694;&#26550;&#26469;&#36843;&#20351;&#32593;&#32476;&#23398;&#20064;&#35299;&#32544;&#32544;&#32469;&#34920;&#31034;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#23545;&#21098;&#26525;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#35299;&#32544;&#32544;&#32469;&#34920;&#31034;&#23545;&#21098;&#26525;&#36807;&#31243;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.09994</link><description>&lt;p&gt;
&#21098;&#26525;&#31070;&#32463;&#32593;&#32476;&#23545;&#21098;&#26525;&#32467;&#26524;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Disentanglement on Pruning Neural Networks. (arXiv:2307.09994v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;Beta-VAE&#26694;&#26550;&#26469;&#36843;&#20351;&#32593;&#32476;&#23398;&#20064;&#35299;&#32544;&#32544;&#32469;&#34920;&#31034;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#23545;&#21098;&#26525;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#35299;&#32544;&#32544;&#32469;&#34920;&#31034;&#23545;&#21098;&#26525;&#36807;&#31243;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20197;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#23454;&#29616;&#29305;&#23450;&#20219;&#21153;&#65292;&#38656;&#35201;&#20943;&#23567;&#20854;&#23384;&#20648;&#21344;&#29992;&#12289;&#21151;&#32791;&#21644;&#24310;&#36831;&#12290;&#36890;&#36807;&#39640;&#25928;&#30340;&#27169;&#22411;&#21387;&#32553;&#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#32593;&#32476;&#20135;&#29983;&#30340;&#35299;&#32544;&#32544;&#32469;&#30340;&#28508;&#22312;&#34920;&#31034;&#26159;&#23454;&#29616;&#27169;&#22411;&#21387;&#32553;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#20027;&#35201;&#20445;&#30041;&#20102;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#20002;&#24323;&#20102;&#23545;&#35813;&#20219;&#21153;&#26080;&#29992;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#20351;&#29992;Beta-VAE&#26694;&#26550;&#32467;&#21512;&#26631;&#20934;&#21098;&#26525;&#20934;&#21017;&#65292;&#30740;&#31350;&#20102;&#36843;&#20351;&#32593;&#32476;&#23398;&#20064;&#35299;&#32544;&#32544;&#32469;&#34920;&#31034;&#23545;&#20998;&#31867;&#20219;&#21153;&#30340;&#21098;&#26525;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;MNIST&#21644;CIFAR10&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#35299;&#32544;&#32544;&#32469;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying deep learning neural networks on edge devices, to accomplish task specific objectives in the real-world, requires a reduction in their memory footprint, power consumption, and latency. This can be realized via efficient model compression. Disentangled latent representations produced by variational autoencoder (VAE) networks are a promising approach for achieving model compression because they mainly retain task-specific information, discarding useless information for the task at hand. We make use of the Beta-VAE framework combined with a standard criterion for pruning to investigate the impact of forcing the network to learn disentangled representations on the pruning process for the task of classification. In particular, we perform experiments on MNIST and CIFAR10 datasets, examine disentanglement challenges, and propose a path forward for future works.
&lt;/p&gt;</description></item><item><title>UniMatch&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#29992;&#25143;-&#29289;&#21697;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#21516;&#26102;&#36827;&#34892;&#29289;&#21697;&#25512;&#33616;&#21644;&#29992;&#25143;&#23450;&#20301;&#65292;&#20943;&#23569;&#20102;&#21830;&#23478;&#36141;&#20080;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;&#21033;&#29992;&#22810;&#39033;&#20998;&#24067;&#24314;&#27169;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30697;&#38453;&#65292;&#24182;&#36890;&#36807;&#21452;&#21521;&#20559;&#24046;&#26657;&#27491;&#30340;&#25439;&#22833;&#20989;&#25968;&#25351;&#23548;&#27169;&#22411;&#23398;&#20064;&#29992;&#25143;-&#29289;&#21697;&#32852;&#21512;&#27010;&#29575;&#65292;&#23454;&#29616;&#20102;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.09989</link><description>&lt;p&gt;
UniMatch:&#19968;&#20010;&#32479;&#19968;&#30340;&#29992;&#25143;-&#29289;&#21697;&#21305;&#37197;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#29992;&#36884;&#21830;&#23478;&#33829;&#38144;
&lt;/p&gt;
&lt;p&gt;
UniMatch: A Unified User-Item Matching Framework for the Multi-purpose Merchant Marketing. (arXiv:2307.09989v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09989
&lt;/p&gt;
&lt;p&gt;
UniMatch&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#29992;&#25143;-&#29289;&#21697;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#21516;&#26102;&#36827;&#34892;&#29289;&#21697;&#25512;&#33616;&#21644;&#29992;&#25143;&#23450;&#20301;&#65292;&#20943;&#23569;&#20102;&#21830;&#23478;&#36141;&#20080;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;&#21033;&#29992;&#22810;&#39033;&#20998;&#24067;&#24314;&#27169;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30697;&#38453;&#65292;&#24182;&#36890;&#36807;&#21452;&#21521;&#20559;&#24046;&#26657;&#27491;&#30340;&#25439;&#22833;&#20989;&#25968;&#25351;&#23548;&#27169;&#22411;&#23398;&#20064;&#29992;&#25143;-&#29289;&#21697;&#32852;&#21512;&#27010;&#29575;&#65292;&#23454;&#29616;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#20113;&#26381;&#21153;&#36827;&#34892;&#31169;&#26377;&#39046;&#22495;&#33829;&#38144;&#26102;&#65292;&#21830;&#23478;&#36890;&#24120;&#38656;&#35201;&#20026;&#22810;&#20010;&#33829;&#38144;&#30446;&#30340;&#36141;&#20080;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23548;&#33268;&#25104;&#26412;&#38750;&#24120;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29992;&#25143;-&#29289;&#21697;&#21305;&#37197;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#27169;&#22411;&#21516;&#26102;&#36827;&#34892;&#29289;&#21697;&#25512;&#33616;&#21644;&#29992;&#25143;&#23450;&#20301;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30697;&#38453;&#36827;&#34892;&#22810;&#39033;&#20998;&#24067;&#24314;&#27169;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#19978;&#36848;&#24182;&#21457;&#24314;&#27169;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#21521;&#20559;&#24046;&#26657;&#27491;&#30340;NCE loss&#26469;&#23454;&#29616;&#12290;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#36890;&#36807;&#32416;&#27491;&#30001;&#20110;&#25209;&#27425;&#20869;&#36127;&#37319;&#26679;&#24341;&#36215;&#30340;&#29992;&#25143;&#21644;&#29289;&#21697;&#20559;&#24046;&#65292;&#24341;&#23548;&#27169;&#22411;&#23398;&#20064;&#29992;&#25143;-&#29289;&#21697;&#32852;&#21512;&#27010;&#29575;p(u,i)&#65292;&#32780;&#19981;&#26159;&#26465;&#20214;&#27010;&#29575;p(i|u)&#25110;p(u|i)&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23545;&#27169;&#22411;&#26550;&#26500;&#20855;&#26377;&#28789;&#27963;&#30340;&#36866;&#24212;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
When doing private domain marketing with cloud services, the merchants usually have to purchase different machine learning models for the multiple marketing purposes, leading to a very high cost. We present a unified user-item matching framework to simultaneously conduct item recommendation and user targeting with just one model. We empirically demonstrate that the above concurrent modeling is viable via modeling the user-item interaction matrix with the multinomial distribution, and propose a bidirectional bias-corrected NCE loss for the implementation. The proposed loss function guides the model to learn the user-item joint probability $p(u,i)$ instead of the conditional probability $p(i|u)$ or $p(u|i)$ through correcting both the users and items' biases caused by the in-batch negative sampling. In addition, our framework is model-agnostic enabling a flexible adaptation of different model architectures. Extensive experiments demonstrate that our framework results in significant perfo
&lt;/p&gt;</description></item><item><title>TinyTrain&#26159;&#19968;&#31181;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26356;&#26032;&#27169;&#22411;&#30340;&#37096;&#20998;&#24182;&#22788;&#29702;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#22823;&#22823;&#32553;&#30701;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;&#36890;&#36807;&#20219;&#21153;&#33258;&#36866;&#24212;&#30340;&#31232;&#30095;&#26356;&#26032;&#26041;&#27861;&#65292;TinyTrain&#33021;&#22815;&#22312;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20943;&#23567;&#35745;&#31639;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#23545;&#26410;&#30693;&#20219;&#21153;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.09988</link><description>&lt;p&gt;
TinyTrain&#65306;&#22312;&#26497;&#31471;&#36793;&#32536;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TinyTrain: Deep Neural Network Training at the Extreme Edge. (arXiv:2307.09988v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09988
&lt;/p&gt;
&lt;p&gt;
TinyTrain&#26159;&#19968;&#31181;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26356;&#26032;&#27169;&#22411;&#30340;&#37096;&#20998;&#24182;&#22788;&#29702;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#22823;&#22823;&#32553;&#30701;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;&#36890;&#36807;&#20219;&#21153;&#33258;&#36866;&#24212;&#30340;&#31232;&#30095;&#26356;&#26032;&#26041;&#27861;&#65292;TinyTrain&#33021;&#22815;&#22312;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20943;&#23567;&#35745;&#31639;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#23545;&#26410;&#30693;&#20219;&#21153;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#22791;&#19978;&#30340;&#35757;&#32451;&#23545;&#20110;&#29992;&#25143;&#20010;&#24615;&#21270;&#21644;&#38544;&#31169;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#24494;&#25511;&#21046;&#22120;&#21333;&#20803;&#65288;MCU&#65289;&#30340;&#26222;&#21450;&#65292;&#30001;&#20110;&#21463;&#38480;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#20197;&#21450;&#26631;&#27880;&#30340;&#29992;&#25143;&#25968;&#25454;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#65292;&#36825;&#39033;&#20219;&#21153;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24573;&#35270;&#20102;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#38656;&#35201;&#36807;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#65288;&#20363;&#22914;&#20960;&#20010;&#23567;&#26102;&#65289;&#65292;&#25110;&#32773;&#23548;&#33268;&#37325;&#22823;&#30340;&#20934;&#30830;&#24615;&#25439;&#22833;&#65288;&#8805;10%&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TinyTrain&#65292;&#19968;&#31181;&#35774;&#22791;&#19978;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26356;&#26032;&#27169;&#22411;&#30340;&#37096;&#20998;&#65292;&#24182;&#26126;&#30830;&#22788;&#29702;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#22823;&#24133;&#32553;&#30701;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;TinyTrain&#24341;&#20837;&#20102;&#19968;&#31181;&#20219;&#21153;&#33258;&#36866;&#24212;&#30340;&#31232;&#30095;&#26356;&#26032;&#26041;&#27861;&#65292;&#26681;&#25454;&#22810;&#30446;&#26631;&#20934;&#21017;&#21160;&#24577;&#36873;&#25321;&#23618;/&#36890;&#36947;&#65292;&#21516;&#26102;&#25429;&#25417;&#29992;&#25143;&#25968;&#25454;&#12289;&#20869;&#23384;&#21644;&#30446;&#26631;&#35774;&#22791;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#26410;&#30693;&#20219;&#21153;&#19978;&#33719;&#24471;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#20943;&#23567;&#35745;&#31639;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;TinyTrain&#22312;&#25972;&#20307;&#24494;&#35843;&#30340;&#22522;&#30784;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
On-device training is essential for user personalisation and privacy. With the pervasiveness of IoT devices and microcontroller units (MCU), this task becomes more challenging due to the constrained memory and compute resources, and the limited availability of labelled user data. Nonetheless, prior works neglect the data scarcity issue, require excessively long training time (e.g. a few hours), or induce substantial accuracy loss ($\geq$10\%). We propose TinyTrain, an on-device training approach that drastically reduces training time by selectively updating parts of the model and explicitly coping with data scarcity. TinyTrain introduces a task-adaptive sparse-update method that dynamically selects the layer/channel based on a multi-objective criterion that jointly captures user data, the memory, and the compute capabilities of the target device, leading to high accuracy on unseen tasks with reduced computation and memory footprint. TinyTrain outperforms vanilla fine-tuning of the enti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#21512;&#23398;&#20064;&#32773;&#25512;&#33616;&#36741;&#21161;&#30340;&#32852;&#37030;&#23458;&#25143;&#31471;&#36873;&#25321;(LRef-FedCS)&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#21442;&#19982;&#32773;&#25152;&#20135;&#29983;&#30340;&#25104;&#26412;&#65292;&#24182;&#30830;&#20445;&#22312;&#20998;&#23618;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#32852;&#37030;&#23398;&#20064;&#30340;&#38271;&#26399;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09977</link><description>&lt;p&gt;
&#38754;&#21521;&#20998;&#23618;&#29289;&#32852;&#32593;&#32593;&#32476;&#30340;&#25104;&#26412;&#26377;&#25928;&#32852;&#37030;&#23398;&#20064;&#30340;&#23398;&#20064;&#32773;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Learner Referral for Cost-Effective Federated Learning Over Hierarchical IoT Networks. (arXiv:2307.09977v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#21512;&#23398;&#20064;&#32773;&#25512;&#33616;&#36741;&#21161;&#30340;&#32852;&#37030;&#23458;&#25143;&#31471;&#36873;&#25321;(LRef-FedCS)&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#21442;&#19982;&#32773;&#25152;&#20135;&#29983;&#30340;&#25104;&#26412;&#65292;&#24182;&#30830;&#20445;&#22312;&#20998;&#23618;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#32852;&#37030;&#23398;&#20064;&#30340;&#38271;&#26399;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#36890;&#36807;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#23458;&#25143;&#31471;&#19978;&#26412;&#22320;&#35757;&#32451;&#21442;&#25968;&#30340;&#20998;&#24067;&#24335;&#26041;&#24335;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#30340;&#33539;&#20363;&#65292;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;&#22120;&#30340;&#35206;&#30422;&#33539;&#22260;&#20869;&#24182;&#38750;&#25152;&#26377;&#23458;&#25143;&#31471;&#37117;&#27880;&#20876;&#21040;&#32852;&#37030;&#23398;&#20064;&#32593;&#32476;&#26102;&#65292;&#32852;&#37030;&#23398;&#20064;&#23601;&#19981;&#36866;&#29992;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#31181;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#21512;&#23398;&#20064;&#32773;&#25512;&#33616;&#36741;&#21161;&#30340;&#32852;&#37030;&#23458;&#25143;&#31471;&#36873;&#25321;(LRef-FedCS)&#65292;&#20197;&#21450;&#36890;&#20449;&#21644;&#35745;&#31639;&#36164;&#28304;&#35843;&#24230;&#65292;&#21644;&#26412;&#22320;&#27169;&#22411;&#20934;&#30830;&#24615;&#20248;&#21270;(LMAO)&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#21442;&#19982;&#32773;&#25152;&#20135;&#29983;&#30340;&#25104;&#26412;&#65292;&#30830;&#20445;&#22312;&#20998;&#23618;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#32852;&#37030;&#23398;&#20064;&#30340;&#38271;&#26399;&#20844;&#24179;&#24615;&#12290;&#21033;&#29992;Lyapunov&#20248;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#23558;&#21407;&#22987;&#38382;&#39064;&#37325;&#26032;&#36716;&#21270;&#20026;&#36880;&#27493;&#32852;&#21512;&#20248;&#21270;&#38382;&#39064;(JOP)&#12290;&#38543;&#21518;&#65292;&#20026;&#20102;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#38750;&#20984;JOP&#65292;&#25105;&#20204;&#20998;&#21035;&#21644;&#36845;&#20195;&#22320;&#35299;&#20915;LRef-FedCS&#21644;LMAO&#12290;
&lt;/p&gt;
&lt;p&gt;
The paradigm of federated learning (FL) to address data privacy concerns by locally training parameters on resource-constrained clients in a distributed manner has garnered significant attention. Nonetheless, FL is not applicable when not all clients within the coverage of the FL server are registered with the FL network. To bridge this gap, this paper proposes joint learner referral aided federated client selection (LRef-FedCS), along with communications and computing resource scheduling, and local model accuracy optimization (LMAO) methods. These methods are designed to minimize the cost incurred by the worst-case participant and ensure the long-term fairness of FL in hierarchical Internet of Things (HieIoT) networks. Utilizing the Lyapunov optimization technique, we reformulate the original problem into a stepwise joint optimization problem (JOP). Subsequently, to tackle the mixed-integer non-convex JOP, we separatively and iteratively address LRef-FedCS and LMAO through the central
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;GAISSA&#39033;&#30446;&#30340;&#24895;&#26223;&#12289;&#30446;&#26631;&#21644;&#39044;&#26399;&#25104;&#26524;&#65292;&#35813;&#39033;&#30446;&#26088;&#22312;&#20026;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#36719;&#20214;&#24037;&#31243;&#24072;&#25552;&#20379;&#22522;&#20110;&#26550;&#26500;&#30340;&#26041;&#27861;&#21644;&#24037;&#20855;&#65292;&#20197;&#24320;&#21457;&#32511;&#33394;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2307.09964</link><description>&lt;p&gt;
&#36808;&#21521;&#32511;&#33394;&#20154;&#24037;&#26234;&#33021;&#36719;&#20214;&#31995;&#32479;&#65306;&#22522;&#20110;&#26550;&#26500;&#30340;&#26041;&#27861;&#65288;GAISSA&#65289;
&lt;/p&gt;
&lt;p&gt;
Towards green AI-based software systems: an architecture-centric approach (GAISSA). (arXiv:2307.09964v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09964
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;GAISSA&#39033;&#30446;&#30340;&#24895;&#26223;&#12289;&#30446;&#26631;&#21644;&#39044;&#26399;&#25104;&#26524;&#65292;&#35813;&#39033;&#30446;&#26088;&#22312;&#20026;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#36719;&#20214;&#24037;&#31243;&#24072;&#25552;&#20379;&#22522;&#20110;&#26550;&#26500;&#30340;&#26041;&#27861;&#21644;&#24037;&#20855;&#65292;&#20197;&#24320;&#21457;&#32511;&#33394;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#25104;&#26524;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#26041;&#38754;&#36229;&#36807;&#20102;&#20154;&#31867;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21644;&#20174;&#20013;&#25512;&#26029;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#24456;&#39640;&#65292;&#36825;&#23545;&#24403;&#21069;&#33021;&#28304;&#25928;&#29575;&#31038;&#20250;&#38656;&#27714;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#35770;&#25991;&#25551;&#36848;&#20102;GAISSA&#39033;&#30446;&#30340;&#20027;&#35201;&#24895;&#26223;&#12289;&#30446;&#26631;&#21644;&#39044;&#26399;&#25104;&#26524;&#12290;GAISSA&#39033;&#30446;&#26088;&#22312;&#20026;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#36719;&#20214;&#24037;&#31243;&#24072;&#25552;&#20379;&#22522;&#20110;&#26550;&#26500;&#30340;&#26041;&#27861;&#21644;&#24037;&#20855;&#65292;&#20197;&#24314;&#27169;&#21644;&#24320;&#21457;&#32511;&#33394;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#34429;&#28982;&#39033;&#30446;&#22788;&#20110;&#21021;&#26399;&#38454;&#27573;&#65292;&#20294;&#25105;&#20204;&#25551;&#36848;&#20102;&#30446;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#23454;&#29616;GAISSA&#30446;&#26631;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, AI-based systems have achieved outstanding results and have outperformed humans in different domains. However, the processes of training AI models and inferring from them require high computational resources, which pose a significant challenge in the current energy efficiency societal demand. To cope with this challenge, this research project paper describes the main vision, goals, and expected outcomes of the GAISSA project. The GAISSA project aims at providing data scientists and software engineers tool-supported, architecture-centric methods for the modelling and development of green AI-based systems. Although the project is in an initial stage, we describe the current research results, which illustrate the potential to achieve GAISSA objectives.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;XSkill&#65292;&#19968;&#31181;&#36328;&#20307;&#29616;&#30340;&#25216;&#33021;&#21457;&#29616;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#26080;&#26631;&#31614;&#30340;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#25805;&#32437;&#35270;&#39057;&#20013;&#32431;&#31929;&#22320;&#21457;&#29616;&#36328;&#20307;&#29616;&#25216;&#33021;&#21407;&#22411;&#65292;&#24182;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#31574;&#30053;&#23558;&#36825;&#20123;&#25216;&#33021;&#36716;&#31227;&#21040;&#26426;&#22120;&#20154;&#21160;&#20316;&#20013;&#65292;&#22312;&#26410;&#35265;&#20219;&#21153;&#20013;&#23436;&#25104;&#23398;&#20064;&#21040;&#30340;&#25216;&#33021;&#30340;&#32452;&#21512;&#12290;&#20223;&#30495;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#21457;&#29616;&#30340;&#25216;&#33021;&#21407;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#20419;&#36827;&#25216;&#33021;&#36716;&#31227;&#21644;&#32452;&#21512;&#65292;&#20174;&#32780;&#26500;&#24314;&#20986;&#26356;&#36890;&#29992;&#21644;&#21487;&#25193;&#23637;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2307.09955</link><description>&lt;p&gt;
XSkill&#65306;&#36328;&#20307;&#29616;&#25216;&#33021;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
XSkill: Cross Embodiment Skill Discovery. (arXiv:2307.09955v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;XSkill&#65292;&#19968;&#31181;&#36328;&#20307;&#29616;&#30340;&#25216;&#33021;&#21457;&#29616;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#26080;&#26631;&#31614;&#30340;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#25805;&#32437;&#35270;&#39057;&#20013;&#32431;&#31929;&#22320;&#21457;&#29616;&#36328;&#20307;&#29616;&#25216;&#33021;&#21407;&#22411;&#65292;&#24182;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#31574;&#30053;&#23558;&#36825;&#20123;&#25216;&#33021;&#36716;&#31227;&#21040;&#26426;&#22120;&#20154;&#21160;&#20316;&#20013;&#65292;&#22312;&#26410;&#35265;&#20219;&#21153;&#20013;&#23436;&#25104;&#23398;&#20064;&#21040;&#30340;&#25216;&#33021;&#30340;&#32452;&#21512;&#12290;&#20223;&#30495;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#21457;&#29616;&#30340;&#25216;&#33021;&#21407;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#20419;&#36827;&#25216;&#33021;&#36716;&#31227;&#21644;&#32452;&#21512;&#65292;&#20174;&#32780;&#26500;&#24314;&#20986;&#26356;&#36890;&#29992;&#21644;&#21487;&#25193;&#23637;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#31034;&#33539;&#35270;&#39057;&#26159;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#24191;&#27867;&#25968;&#25454;&#28304;&#65292;&#24182;&#19988;&#26159;&#34920;&#36798;&#25152;&#38656;&#34892;&#20026;&#30340;&#30452;&#35266;&#29992;&#25143;&#30028;&#38754;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#20154;&#31867;&#35270;&#39057;&#20013;&#25552;&#21462;&#21487;&#37325;&#29992;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#25216;&#33021;&#38754;&#20020;&#30528;&#20307;&#29616;&#24046;&#24322;&#21644;&#26410;&#35266;&#23519;&#21040;&#30340;&#34892;&#21160;&#21442;&#25968;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#31181;&#20307;&#29616;&#24046;&#36317;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;XSkill&#65292;&#19968;&#31181;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20174;&#26080;&#26631;&#31614;&#30340;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#25805;&#32437;&#35270;&#39057;&#20013;&#32431;&#31929;&#22320;&#21457;&#29616;&#21517;&#20026;&#25216;&#33021;&#21407;&#22411;&#30340;&#36328;&#20307;&#29616;&#34920;&#31034;&#65292;&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#31574;&#30053;&#23558;&#25216;&#33021;&#34920;&#31034;&#36716;&#31227;&#21040;&#26426;&#22120;&#20154;&#21160;&#20316;&#65292;&#24182;&#26368;&#32456;&#20351;&#29992;&#20154;&#31867;&#25552;&#31034;&#35270;&#39057;&#23436;&#25104;&#23398;&#20064;&#21040;&#30340;&#25216;&#33021;&#26469;&#23436;&#25104;&#26410;&#35265;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21457;&#29616;&#30340;&#25216;&#33021;&#21407;&#22411;&#20419;&#36827;&#20102;&#26410;&#35265;&#20219;&#21153;&#30340;&#25216;&#33021;&#36716;&#31227;&#21644;&#32452;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#36890;&#29992;&#21644;&#21487;&#25193;&#23637;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human demonstration videos are a widely available data source for robot learning and an intuitive user interface for expressing desired behavior. However, directly extracting reusable robot manipulation skills from unstructured human videos is challenging due to the big embodiment difference and unobserved action parameters. To bridge this embodiment gap, this paper introduces XSkill, an imitation learning framework that 1) discovers a cross-embodiment representation called skill prototypes purely from unlabeled human and robot manipulation videos, 2) transfers the skill representation to robot actions using conditional diffusion policy, and finally, 3) composes the learned skill to accomplish unseen tasks specified by a human prompt video. Our experiments in simulation and real-world environments show that the discovered skill prototypes facilitate both skill transfer and composition for unseen tasks, resulting in a more general and scalable imitation learning framework. The performan
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#25552;&#39640;&#29992;&#25143;&#38271;&#26399;&#28385;&#24847;&#24230;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#39044;&#27979;&#24310;&#36831;&#22870;&#21169;&#30340;&#27169;&#22411;&#21644;&#35774;&#35745;&#19968;&#20010;&#21033;&#29992;&#35813;&#27169;&#22411;&#30340;&#36172;&#21338;&#31639;&#27861;&#26469;&#35299;&#20915;&#20102;&#36890;&#36807;&#27979;&#37327;&#30701;&#26399;&#20195;&#29702;&#22870;&#21169;&#21453;&#26144;&#23454;&#38469;&#38271;&#26399;&#30446;&#26631;&#19981;&#23436;&#32654;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.09943</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;: &#36807;&#21435;&#26366;&#32763;&#35793;&#12298;Impatient Bandits: Optimizing for the Long-Term Without Delay&#12299;
&lt;/p&gt;
&lt;p&gt;
Impatient Bandits: Optimizing for the Long-Term Without Delay. (arXiv:2307.09943v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09943
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;&#65306;&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#25552;&#39640;&#29992;&#25143;&#38271;&#26399;&#28385;&#24847;&#24230;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#39044;&#27979;&#24310;&#36831;&#22870;&#21169;&#30340;&#27169;&#22411;&#21644;&#35774;&#35745;&#19968;&#20010;&#21033;&#29992;&#35813;&#27169;&#22411;&#30340;&#36172;&#21338;&#31639;&#27861;&#26469;&#35299;&#20915;&#20102;&#36890;&#36807;&#27979;&#37327;&#30701;&#26399;&#20195;&#29702;&#22870;&#21169;&#21453;&#26144;&#23454;&#38469;&#38271;&#26399;&#30446;&#26631;&#19981;&#23436;&#32654;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;&#65306;&#25512;&#33616;&#31995;&#32479;&#22312;&#22312;&#32447;&#24179;&#21488;&#19978;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#21151;&#33021;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#26126;&#30830;&#22320;&#34987;&#20219;&#21153;&#20026;&#25552;&#39640;&#29992;&#25143;&#30340;&#38271;&#26399;&#28385;&#24847;&#24230;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#20869;&#23481;&#25506;&#32034;&#20219;&#21153;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20855;&#26377;&#24310;&#36831;&#22870;&#21169;&#30340;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#36873;&#25321;&#23398;&#20064;&#20449;&#21495;&#26102;&#23384;&#22312;&#26126;&#26174;&#30340;&#26435;&#34913;&#65306;&#31561;&#24453;&#23436;&#20840;&#30340;&#22870;&#21169;&#21487;&#33021;&#38656;&#35201;&#20960;&#21608;&#26102;&#38388;&#65292;&#36825;&#20250;&#24433;&#21709;&#23398;&#20064;&#21457;&#29983;&#30340;&#36895;&#24230;&#65292;&#32780;&#27979;&#37327;&#30701;&#26399;&#20195;&#29702;&#22870;&#21169;&#21017;&#19981;&#23436;&#32654;&#22320;&#21453;&#26144;&#20102;&#23454;&#38469;&#30340;&#38271;&#26399;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#27493;&#39588;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#39044;&#27979;&#24310;&#36831;&#22870;&#21169;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25972;&#21512;&#36804;&#20170;&#25152;&#33719;&#24471;&#30340;&#25152;&#26377;&#20449;&#24687;&#12290;&#36890;&#36807;&#36125;&#21494;&#26031;&#28388;&#27874;&#22120;&#32452;&#21512;&#23436;&#25972;&#30340;&#35266;&#23519;&#32467;&#26524;&#20197;&#21450;&#37096;&#20998;&#65288;&#30701;&#26399;&#25110;&#20013;&#26399;&#65289;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#24471;&#21040;&#27010;&#29575;&#20449;&#24565;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21033;&#29992;&#36825;&#20010;&#26032;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#36172;&#21338;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#24555;&#36895;&#23398;&#20064;&#35782;&#21035;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems are a ubiquitous feature of online platforms. Increasingly, they are explicitly tasked with increasing users' long-term satisfaction. In this context, we study a content exploration task, which we formalize as a multi-armed bandit problem with delayed rewards. We observe that there is an apparent trade-off in choosing the learning signal: Waiting for the full reward to become available might take several weeks, hurting the rate at which learning happens, whereas measuring short-term proxy rewards reflects the actual long-term goal only imperfectly. We address this challenge in two steps. First, we develop a predictive model of delayed rewards that incorporates all information obtained to date. Full observations as well as partial (short or medium-term) outcomes are combined through a Bayesian filter to obtain a probabilistic belief. Second, we devise a bandit algorithm that takes advantage of this new predictive model. The algorithm quickly learns to identify conten
&lt;/p&gt;</description></item><item><title>TREEMENT&#26159;&#19968;&#31181;&#37319;&#29992;&#20010;&#24615;&#21270;&#21160;&#24577;&#26641;&#29366;&#35760;&#24518;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#27169;&#22411;&#65292;&#21033;&#29992;&#23618;&#27425;&#21270;&#20020;&#24202;&#26412;&#20307;&#30693;&#35782;&#21644;&#21512;&#26684;&#26631;&#20934;&#23884;&#20837;&#23398;&#20064;&#65292;&#25552;&#20379;&#20934;&#30830;&#32780;&#26377;&#35299;&#37322;&#24615;&#30340;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2307.09942</link><description>&lt;p&gt;
TREEMENT: &#21487;&#35299;&#37322;&#30340;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#27169;&#22411;&#36890;&#36807;&#20010;&#24615;&#21270;&#21160;&#24577;&#26641;&#29366;&#35760;&#24518;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TREEMENT: Interpretable Patient-Trial Matching via Personalized Dynamic Tree-Based Memory Network. (arXiv:2307.09942v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09942
&lt;/p&gt;
&lt;p&gt;
TREEMENT&#26159;&#19968;&#31181;&#37319;&#29992;&#20010;&#24615;&#21270;&#21160;&#24577;&#26641;&#29366;&#35760;&#24518;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#27169;&#22411;&#65292;&#21033;&#29992;&#23618;&#27425;&#21270;&#20020;&#24202;&#26412;&#20307;&#30693;&#35782;&#21644;&#21512;&#26684;&#26631;&#20934;&#23884;&#20837;&#23398;&#20064;&#65292;&#25552;&#20379;&#20934;&#30830;&#32780;&#26377;&#35299;&#37322;&#24615;&#30340;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#23545;&#20110;&#33647;&#29289;&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24448;&#24448;&#38754;&#20020;&#26114;&#36149;&#32780;&#20302;&#25928;&#30340;&#24739;&#32773;&#25307;&#21215;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#32437;&#21521;&#24739;&#32773;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#21644;&#20020;&#24202;&#35797;&#39564;&#30340;&#21512;&#26684;&#26631;&#20934;&#33258;&#21160;&#21305;&#37197;&#24739;&#32773;&#19982;&#20020;&#24202;&#35797;&#39564;&#65292;&#20197;&#21152;&#36895;&#24739;&#32773;&#25307;&#21215;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#35201;&#20040;&#20381;&#36182;&#20110;&#26080;&#27861;&#25193;&#23637;&#21040;&#20854;&#20182;&#35797;&#39564;&#30340;&#19987;&#23478;&#35268;&#21017;&#65292;&#35201;&#20040;&#20197;&#40657;&#31665;&#27169;&#22411;&#36827;&#34892;&#38750;&#24120;&#36890;&#29992;&#30340;&#21305;&#37197;&#65292;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#20250;&#23548;&#33268;&#27169;&#22411;&#32467;&#26524;&#38590;&#20197;&#37319;&#29992;&#12290;&#20026;&#20102;&#25552;&#20379;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;TREEMENT&#30340;&#20010;&#24615;&#21270;&#21160;&#24577;&#26641;&#29366;&#35760;&#24518;&#32593;&#32476;&#27169;&#22411;&#12290;&#23427;&#21033;&#29992;&#23618;&#27425;&#21270;&#20020;&#24202;&#26412;&#20307;&#30693;&#35782;&#25193;&#23637;&#20102;&#20174;&#24207;&#21015;EHR&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#20010;&#24615;&#21270;&#24739;&#32773;&#34920;&#31034;&#65292;&#28982;&#21518;&#20351;&#29992;&#22522;&#20110;&#21512;&#26684;&#26631;&#20934;&#23884;&#20837;&#23398;&#20064;&#30340;&#27880;&#24847;&#21147;&#26463;&#25628;&#32034;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical trials are critical for drug development but often suffer from expensive and inefficient patient recruitment. In recent years, machine learning models have been proposed for speeding up patient recruitment via automatically matching patients with clinical trials based on longitudinal patient electronic health records (EHR) data and eligibility criteria of clinical trials. However, they either depend on trial-specific expert rules that cannot expand to other trials or perform matching at a very general level with a black-box model where the lack of interpretability makes the model results difficult to be adopted.  To provide accurate and interpretable patient trial matching, we introduce a personalized dynamic tree-based memory network model named TREEMENT. It utilizes hierarchical clinical ontologies to expand the personalized patient representation learned from sequential EHR data, and then uses an attentional beam-search query learned from eligibility criteria embedding to o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#31639;&#27861;&#25552;&#20986;&#65292;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#21033;&#29992;&#19981;&#31283;&#23450;&#29305;&#24449;&#26469;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09933</link><description>&lt;p&gt;
Spuriosity&#24182;&#27809;&#26377;&#23548;&#33268;&#20998;&#31867;&#22120;&#22833;&#36133;&#65306;&#21033;&#29992;&#19981;&#21464;&#30340;&#39044;&#27979;&#26469;&#21033;&#29992;&#34394;&#20551;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Spuriosity Didn't Kill the Classifier: Using Invariant Predictions to Harness Spurious Features. (arXiv:2307.09933v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#31639;&#27861;&#25552;&#20986;&#65292;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#21033;&#29992;&#19981;&#31283;&#23450;&#29305;&#24449;&#26469;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36991;&#20813;&#22312;&#22495;&#22806;&#25968;&#25454;&#19978;&#30340;&#22833;&#36133;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#35797;&#22270;&#25552;&#21462;&#20855;&#26377;&#19982;&#26631;&#31614;&#22312;&#19981;&#21516;&#22495;&#20043;&#38388;&#31283;&#23450;&#25110;&#19981;&#21464;&#20851;&#31995;&#30340;&#29305;&#24449;&#65292;&#33293;&#24323;&#19982;&#26631;&#31614;&#22312;&#19981;&#21516;&#22495;&#20043;&#38388;&#20851;&#31995;&#21464;&#21270;&#30340;"&#34394;&#20551;"&#25110;&#19981;&#31283;&#23450;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#19981;&#31283;&#23450;&#29305;&#24449;&#24120;&#24120;&#25658;&#24102;&#20851;&#20110;&#26631;&#31614;&#30340;&#34917;&#20805;&#20449;&#24687;&#65292;&#22914;&#26524;&#22312;&#27979;&#35797;&#22495;&#20013;&#27491;&#30830;&#20351;&#29992;&#65292;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#26174;&#31034;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#22914;&#20309;&#22312;&#27979;&#35797;&#22495;&#20013;&#20351;&#29992;&#36825;&#20123;&#19981;&#31283;&#23450;&#29305;&#24449;&#26159;&#21487;&#33021;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#22522;&#20110;&#31283;&#23450;&#29305;&#24449;&#30340;&#20266;&#26631;&#31614;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#25351;&#23548;&#26469;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#21069;&#25552;&#26159;&#22312;&#32473;&#23450;&#26631;&#31614;&#30340;&#26465;&#20214;&#19979;&#65292;&#31283;&#23450;&#29305;&#24449;&#21644;&#19981;&#31283;&#23450;&#29305;&#24449;&#26159;&#26465;&#20214;&#29420;&#31435;&#30340;&#12290;&#22522;&#20110;&#36825;&#20010;&#29702;&#35770;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31283;&#23450;&#29305;&#24449;&#22686;&#24378;&#65288;SFB&#65289;&#31639;&#27861;&#65306;(i)&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#20998;&#31163;&#31283;&#23450;&#29305;&#24449;&#21644;&#26465;&#20214;&#29420;&#31435;&#19981;&#31283;&#23450;&#29305;&#24449;&#30340;&#39044;&#27979;&#22120;&#65307;(ii)&#20351;&#29992;&#31283;&#23450;&#29305;&#24449;&#39044;&#27979;&#26469;&#36866;&#24212;&#27979;&#35797;&#22495;
&lt;/p&gt;
&lt;p&gt;
To avoid failures on out-of-distribution data, recent works have sought to extract features that have a stable or invariant relationship with the label across domains, discarding the "spurious" or unstable features whose relationship with the label changes across domains. However, unstable features often carry complementary information about the label that could boost performance if used correctly in the test domain. Our main contribution is to show that it is possible to learn how to use these unstable features in the test domain without labels. In particular, we prove that pseudo-labels based on stable features provide sufficient guidance for doing so, provided that stable and unstable features are conditionally independent given the label. Based on this theoretical insight, we propose Stable Feature Boosting (SFB), an algorithm for: (i) learning a predictor that separates stable and conditionally-independent unstable features; and (ii) using the stable-feature predictions to adapt t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#21019;&#24314;&#20855;&#26377;&#34920;&#36798;&#21147;&#30340;&#36328;&#27169;&#24577;&#25551;&#36848;&#31526;&#65292;&#36890;&#36807;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#29992;&#28857;&#31215;&#36924;&#36817;&#29616;&#26377;&#30340;&#24230;&#37327;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#30340;&#21487;&#21464;&#24418;&#20840;&#23616;&#37197;&#20934;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30452;&#25509;&#20351;&#29992;&#65292;&#20165;&#38656;&#26367;&#25442;&#30456;&#20284;&#24615;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.09931</link><description>&lt;p&gt;
DISA: &#21487;&#24494;&#20998;&#30456;&#20284;&#24615;&#36924;&#36817;&#29992;&#20110;&#36890;&#29992;&#22810;&#27169;&#24577;&#37197;&#20934;
&lt;/p&gt;
&lt;p&gt;
DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration. (arXiv:2307.09931v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#21019;&#24314;&#20855;&#26377;&#34920;&#36798;&#21147;&#30340;&#36328;&#27169;&#24577;&#25551;&#36848;&#31526;&#65292;&#36890;&#36807;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#29992;&#28857;&#31215;&#36924;&#36817;&#29616;&#26377;&#30340;&#24230;&#37327;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#30340;&#21487;&#21464;&#24418;&#20840;&#23616;&#37197;&#20934;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30452;&#25509;&#20351;&#29992;&#65292;&#20165;&#38656;&#26367;&#25442;&#30456;&#20284;&#24615;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22270;&#20687;&#37197;&#20934;&#26159;&#35768;&#22810;&#22270;&#20687;&#24341;&#23548;&#36807;&#31243;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21448;&#24517;&#19981;&#21487;&#23569;&#30340;&#19968;&#27493;&#12290;&#22823;&#22810;&#25968;&#37197;&#20934;&#31639;&#27861;&#20381;&#36182;&#20110;&#35745;&#31639;&#22797;&#26434;&#12289;&#39057;&#32321;&#38750;&#21487;&#24494;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#20197;&#24212;&#23545;&#24433;&#20687;&#27169;&#24577;&#20043;&#38388;&#35299;&#21078;&#32467;&#26500;&#22806;&#35266;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#30340;&#35299;&#21078;&#23398;-&#27169;&#24577;&#32452;&#21512;&#65292;&#24182;&#19981;&#33021;&#25512;&#24191;&#21040;&#26032;&#30340;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#21019;&#24314;&#20855;&#26377;&#34920;&#36798;&#21147;&#30340;&#36328;&#27169;&#24577;&#25551;&#36848;&#31526;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#30340;&#21487;&#21464;&#24418;&#20840;&#23616;&#37197;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19968;&#20010;&#23567;&#22411;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#29992;&#28857;&#31215;&#36924;&#36817;&#29616;&#26377;&#30340;&#24230;&#37327;&#65292;&#23454;&#29616;&#20102;&#21487;&#24494;&#20998;&#35757;&#32451;&#65292;&#21516;&#26102;&#26080;&#38656;&#37197;&#20934;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#22522;&#20110;&#23616;&#37096;&#22359;&#30340;&#24230;&#37327;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#21487;&#20197;&#30452;&#25509;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;&#65292;&#21482;&#38656;&#23558;&#30456;&#20284;&#24615;&#24230;&#37327;&#26367;&#25442;&#20026;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal image registration is a challenging but essential step for numerous image-guided procedures. Most registration algorithms rely on the computation of complex, frequently non-differentiable similarity metrics to deal with the appearance discrepancy of anatomical structures between imaging modalities. Recent Machine Learning based approaches are limited to specific anatomy-modality combinations and do not generalize to new settings. We propose a generic framework for creating expressive cross-modal descriptors that enable fast deformable global registration. We achieve this by approximating existing metrics with a dot-product in the feature space of a small convolutional neural network (CNN) which is inherently differentiable can be trained without registered data. Our method is several orders of magnitude faster than local patch-based metrics and can be directly applied in clinical settings by replacing the similarity measure with the proposed one. Experiments on three differe
&lt;/p&gt;</description></item><item><title>TimeTuner&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#20998;&#26512;&#20154;&#21592;&#29702;&#35299;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#27169;&#22411;&#34892;&#20026;&#19982;&#26102;&#38388;&#34920;&#31034;&#30340;&#20851;&#31995;&#65292;&#24182;&#35299;&#20915;&#33258;&#21160;&#21270;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09916</link><description>&lt;p&gt;
TimeTuner: &#35786;&#26029;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#26102;&#38388;&#34920;&#31034;&#30340;&#23545;&#29031;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations. (arXiv:2307.09916v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09916
&lt;/p&gt;
&lt;p&gt;
TimeTuner&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#20998;&#26512;&#20154;&#21592;&#29702;&#35299;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#27169;&#22411;&#34892;&#20026;&#19982;&#26102;&#38388;&#34920;&#31034;&#30340;&#20851;&#31995;&#65292;&#24182;&#35299;&#20915;&#33258;&#21160;&#21270;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#65292;&#35768;&#22810;&#21162;&#21147;&#33268;&#21147;&#20110;&#35774;&#35745;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#24448;&#24448;&#24402;&#22240;&#20110;&#26377;&#25928;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#20419;&#36827;&#20102;&#29305;&#24449;&#24037;&#31243;&#21644;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#21270;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#22312;&#34701;&#20837;&#20808;&#39564;&#30693;&#35782;&#12289;&#35782;&#21035;&#21464;&#37327;&#38388;&#30456;&#20114;&#20316;&#29992;&#21644;&#36873;&#25321;&#35780;&#20272;&#25351;&#26631;&#20197;&#30830;&#20445;&#27169;&#22411;&#21487;&#38752;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#21363;TimeTuner&#65292;&#26088;&#22312;&#24110;&#21161;&#20998;&#26512;&#20154;&#21592;&#29702;&#35299;&#27169;&#22411;&#34892;&#20026;&#19982;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#30340;&#23616;&#37096;&#30456;&#20851;&#24615;&#12289;&#24179;&#31283;&#24615;&#21644;&#31890;&#24230;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#35813;&#31995;&#32479;&#20027;&#35201;&#21253;&#25324;&#20197;&#19979;&#20004;&#20010;&#38454;&#27573;&#25216;&#26415;&#65306;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#23545;&#29031;&#35299;&#37322;&#26469;&#24314;&#31435;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) approaches are being increasingly used for time-series forecasting, with many efforts devoted to designing complex DL models. Recent studies have shown that the DL success is often attributed to effective data representations, fostering the fields of feature engineering and representation learning. However, automated approaches for feature learning are typically limited with respect to incorporating prior knowledge, identifying interactions among variables, and choosing evaluation metrics to ensure that the models are reliable. To improve on these limitations, this paper contributes a novel visual analytics framework, namely TimeTuner, designed to help analysts understand how model behaviors are associated with localized correlations, stationarity, and granularity of time-series representations. The system mainly consists of the following two-stage technique: We first leverage counterfactual explanations to connect the relationships among time-series representations,
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#25237;&#24433;&#32593;&#32476;&#23398;&#20064;&#26102;&#38388;&#40784;&#27425;&#21160;&#21147;&#31995;&#32479;&#30340;&#26377;&#24847;&#20041;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20248;&#21270;&#31867;&#20284;&#20110;&#32463;&#20856;&#30456;&#20851;&#20998;&#26512;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#36991;&#20813;&#20102;&#30697;&#38453;&#27714;&#36870;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#27491;&#21017;&#21270;&#26041;&#26696;&#36827;&#19968;&#27493;&#22686;&#24378;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.09912</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#26102;&#38388;&#40784;&#27425;&#21160;&#21147;&#31995;&#32479;&#30340;&#28145;&#24230;&#25237;&#24433;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep projection networks for learning time-homogeneous dynamical systems. (arXiv:2307.09912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09912
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#25237;&#24433;&#32593;&#32476;&#23398;&#20064;&#26102;&#38388;&#40784;&#27425;&#21160;&#21147;&#31995;&#32479;&#30340;&#26377;&#24847;&#20041;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20248;&#21270;&#31867;&#20284;&#20110;&#32463;&#20856;&#30456;&#20851;&#20998;&#26512;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#36991;&#20813;&#20102;&#30697;&#38453;&#27714;&#36870;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#27491;&#21017;&#21270;&#26041;&#26696;&#36827;&#19968;&#27493;&#22686;&#24378;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#33324;&#30340;&#26102;&#38388;&#40784;&#27425;&#21160;&#21147;&#31995;&#32479;&#65292;&#21253;&#25324;&#31163;&#25955;&#21644;&#36830;&#32493;&#30340;&#65292;&#24182;&#30740;&#31350;&#20102;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#29366;&#24577;&#30340;&#26377;&#24847;&#20041;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;&#36825;&#23545;&#20110;&#23398;&#20064;&#31995;&#32479;&#30340;&#21069;&#21521;&#20256;&#36755;&#31639;&#23376;&#33267;&#20851;&#37325;&#35201;&#65292;&#35813;&#31639;&#23376;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#26410;&#26469;&#30340;&#29366;&#24577;&#25110;&#21487;&#35266;&#27979;&#37327;&#12290;&#34920;&#31034;&#36890;&#24120;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#65292;&#19982;&#25237;&#24433;&#31639;&#23376;&#30456;&#20851;&#32852;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#31867;&#20284;&#20110;&#32463;&#20856;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#19982;CCA&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#20989;&#25968;&#36991;&#20813;&#20102;&#30697;&#38453;&#27714;&#36870;&#65292;&#22240;&#27492;&#36890;&#24120;&#26356;&#31283;&#23450;&#19988;&#36866;&#29992;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#20989;&#25968;&#26159;CCA&#30340;&#19968;&#20010;&#32039;&#26494;&#24347;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#25552;&#20986;&#20004;&#31181;&#27491;&#21017;&#21270;&#26041;&#26696;&#26469;&#22686;&#24378;&#23427;&#65292;&#19968;&#31181;&#40723;&#21169;&#34920;&#31034;&#30340;&#20998;&#37327;&#27491;&#20132;&#65292;&#32780;&#21478;&#19968;&#31181;&#21033;&#29992;&#20102; Chapman-Kolmogorov &#26041;&#31243;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
We consider the general class of time-homogeneous dynamical systems, both discrete and continuous, and study the problem of learning a meaningful representation of the state from observed data. This is instrumental for the task of learning a forward transfer operator of the system, that in turn can be used for forecasting future states or observables. The representation, typically parametrized via a neural network, is associated with a projection operator and is learned by optimizing an objective function akin to that of canonical correlation analysis (CCA). However, unlike CCA, our objective avoids matrix inversions and therefore is generally more stable and applicable to challenging scenarios. Our objective is a tight relaxation of CCA and we further enhance it by proposing two regularization schemes, one encouraging the orthogonality of the components of the representation while the other exploiting Chapman-Kolmogorov's equation. We apply our method to challenging discrete dynamical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#20998;&#31867;&#20013;&#30340;&#37325;&#22797;&#35266;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#31616;&#21333;&#30340;&#20998;&#31867;&#35268;&#21017;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#37325;&#22797;&#35266;&#27979;&#27425;&#25968;$t\to\infty$&#26102;&#65292;&#26465;&#20214;&#38169;&#35823;&#27010;&#29575;&#20197;&#25351;&#25968;&#36895;&#24230;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2307.09896</link><description>&lt;p&gt;
&#37325;&#22797;&#35266;&#27979;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Repeated Observations for Classification. (arXiv:2307.09896v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#20998;&#31867;&#20013;&#30340;&#37325;&#22797;&#35266;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#31616;&#21333;&#30340;&#20998;&#31867;&#35268;&#21017;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#37325;&#22797;&#35266;&#27979;&#27425;&#25968;$t\to\infty$&#26102;&#65292;&#26465;&#20214;&#38169;&#35823;&#27010;&#29575;&#20197;&#25351;&#25968;&#36895;&#24230;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#21442;&#25968;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#37325;&#22797;&#35266;&#27979;&#38382;&#39064;&#12290;&#20196;$\bX$&#20026;$d$&#32500;&#29305;&#24449;&#21521;&#37327;&#65292;$Y$&#20026;&#21462;&#20540;&#22312;$\{1,\dots ,M\}$&#20043;&#38388;&#30340;&#26631;&#31614;&#12290;&#19982;&#36890;&#24120;&#26679;&#26412;&#37327;$n$&#36739;&#22823;&#65292;&#32500;&#24230;&#36739;&#20302;$d$&#30340;&#35774;&#32622;&#19981;&#21516;&#30340;&#26159;&#65292;&#26412;&#25991;&#22788;&#29702;&#30340;&#26159;&#24403;&#25105;&#20204;&#19981;&#26159;&#35266;&#27979;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#29305;&#24449;&#21521;&#37327;$\bX$&#65292;&#32780;&#26159;&#32473;&#20986;$t$&#20010;&#37325;&#22797;&#30340;&#29305;&#24449;&#21521;&#37327;$\bV_1,\dots ,\bV_t $&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#31616;&#21333;&#30340;&#20998;&#31867;&#35268;&#21017;&#65292;&#20351;&#24471;&#26465;&#20214;&#38169;&#35823;&#27010;&#29575;&#30340;&#25910;&#25947;&#36895;&#24230;&#38543;&#30528;$t\to\infty$&#21576;&#25351;&#25968;&#25910;&#25947;&#12290;&#22312;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20123;&#29305;&#23450;&#30340;&#27169;&#22411;&#65292;&#22914;&#22522;&#20110;&#21517;&#20041;&#23494;&#24230;&#30340;&#40065;&#26834;&#26816;&#27979;&#65292;&#21407;&#22411;&#20998;&#31867;&#65292;&#32447;&#24615;&#21464;&#25442;&#65292;&#32447;&#24615;&#20998;&#31867;&#21644;&#23610;&#24230;&#21464;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem nonparametric classification with repeated observations. Let $\bX$ be the $d$ dimensional feature vector and let $Y$ denote the label taking values in $\{1,\dots ,M\}$. In contrast to usual setup with large sample size $n$ and relatively low dimension $d$, this paper deals with the situation, when instead of observing a single feature vector $\bX$ we are given $t$ repeated feature vectors $\bV_1,\dots ,\bV_t $. Some simple classification rules are presented such that the conditional error probabilities have exponential convergence rate of convergence as $t\to\infty$. In the analysis, we investigate particular models like robust detection by nominal densities, prototype classification, linear transformation, linear classification, scaling.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#31216;&#22343;&#34913;&#23398;&#20064;&#26041;&#27861;&#65292;&#20801;&#35768;&#22312;&#21482;&#33021;&#36890;&#36807;&#37319;&#26679;&#33719;&#24471;&#25968;&#25454;&#21644;&#28508;&#22312;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;VAEs&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#19982;&#20256;&#32479;ELBO&#23398;&#20064;&#26041;&#27861;&#33719;&#24471;&#30340;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09883</link><description>&lt;p&gt;
VAE&#30340;&#23545;&#31216;&#22343;&#34913;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Symmetric Equilibrium Learning of VAEs. (arXiv:2307.09883v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#31216;&#22343;&#34913;&#23398;&#20064;&#26041;&#27861;&#65292;&#20801;&#35768;&#22312;&#21482;&#33021;&#36890;&#36807;&#37319;&#26679;&#33719;&#24471;&#25968;&#25454;&#21644;&#28508;&#22312;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;VAEs&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#19982;&#20256;&#32479;ELBO&#23398;&#20064;&#26041;&#27861;&#33719;&#24471;&#30340;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#35270;&#20026;&#35299;&#30721;&#22120;-&#32534;&#30721;&#22120;&#23545;&#65292;&#23558;&#25968;&#25454;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;VAEs&#30340;&#26631;&#20934;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#26368;&#22823;&#21270;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#65292;&#23384;&#22312;&#26126;&#26174;&#30340;&#19981;&#23545;&#31216;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#38656;&#35201;&#19968;&#20010;&#38381;&#21512;&#24418;&#24335;&#30340;&#20808;&#39564;&#28508;&#22312;&#20998;&#24067;&#12290;&#36825;&#38480;&#21046;&#20102;VAEs&#22312;&#26356;&#22797;&#26434;&#30340;&#24773;&#20917;&#19979;&#30340;&#36866;&#29992;&#24615;&#65292;&#22914;&#19968;&#33324;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#20351;&#29992;&#22797;&#26434;&#30340;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#20808;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32435;&#20160;&#22343;&#34913;&#23398;&#20064;&#26041;&#27861;&#65292;&#25918;&#23485;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#22312;&#21482;&#33021;&#36890;&#36807;&#37319;&#26679;&#33719;&#24471;&#25968;&#25454;&#21644;&#28508;&#22312;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;VAEs&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#21644;&#31616;&#21333;&#24615;&#20351;&#20854;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#23398;&#20064;&#22330;&#26223;&#21644;&#19979;&#28216;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#23398;&#20064;&#30340;&#27169;&#22411;&#19982;ELBO&#23398;&#20064;&#33719;&#24471;&#30340;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#23454;&#36341;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We view variational autoencoders (VAE) as decoder-encoder pairs, which map distributions in the data space to distributions in the latent space and vice versa. The standard learning approach for VAEs, i.e. maximisation of the evidence lower bound (ELBO), has an obvious asymmetry in that respect. Moreover, it requires a closed form a-priori latent distribution. This limits the applicability of VAEs in more complex scenarios, such as general semi-supervised learning and employing complex generative models as priors. We propose a Nash equilibrium learning approach that relaxes these restrictions and allows learning VAEs in situations where both the data and the latent distributions are accessible only by sampling. The flexibility and simplicity of this approach allows its application to a wide range of learning scenarios and downstream tasks. We show experimentally that the models learned by this method are comparable to those obtained by ELBO learning and demonstrate its applicability fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21333;&#21521;&#27969;&#36827;&#34892;&#23545;&#25239;&#24615;&#20284;&#28982;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#35299;&#20915;&#20102;Wasserstein GAN&#20013;&#20998;&#21306;&#20989;&#25968;&#26377;&#20559;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#29983;&#25104;&#22120;&#30340;&#29109;&#65292;&#25552;&#39640;&#20102;&#27169;&#24335;&#35206;&#30422;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#29983;&#25104;&#26679;&#26412;&#30340;&#23494;&#24230;&#26469;&#23454;&#29616;&#23545;&#20998;&#21306;&#20989;&#25968;&#30340;&#26080;&#20559;&#20272;&#35745;&#21644;&#29983;&#25104;&#22120;&#29109;&#30340;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2307.09882</link><description>&lt;p&gt;
&#36890;&#36807;&#21333;&#21521;&#27969;&#36827;&#34892;&#23545;&#25239;&#24615;&#20284;&#28982;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Adversarial Likelihood Estimation with One-way Flows. (arXiv:2307.09882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21333;&#21521;&#27969;&#36827;&#34892;&#23545;&#25239;&#24615;&#20284;&#28982;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#35299;&#20915;&#20102;Wasserstein GAN&#20013;&#20998;&#21306;&#20989;&#25968;&#26377;&#20559;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#29983;&#25104;&#22120;&#30340;&#29109;&#65292;&#25552;&#39640;&#20102;&#27169;&#24335;&#35206;&#30422;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#29983;&#25104;&#26679;&#26412;&#30340;&#23494;&#24230;&#26469;&#23454;&#29616;&#23545;&#20998;&#21306;&#20989;&#25968;&#30340;&#26080;&#20559;&#20272;&#35745;&#21644;&#29983;&#25104;&#22120;&#29109;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#20294;&#26080;&#27861;&#25552;&#20379;&#26679;&#26412;&#21608;&#22260;&#30340;&#27010;&#29575;&#23494;&#24230;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#27880;&#24847;&#21040;&#22312;&#33021;&#37327;&#27169;&#22411;&#30340;&#35774;&#32622;&#20013;&#65292;&#26368;&#22823;&#21270;&#23545;&#25968;&#20284;&#28982;&#21487;&#20197;&#23548;&#33268;&#21028;&#21035;&#22120;&#25552;&#20379;&#38750;&#24402;&#19968;&#21270;&#30340;&#23494;&#24230;&#65288;&#36890;&#24120;&#31216;&#20026;&#33021;&#37327;&#65289;&#30340;&#23545;&#25239;&#24615;&#26694;&#26550;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#23637;&#20102;&#36825;&#19968;&#35266;&#28857;&#65292;&#32467;&#21512;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#24182;&#23637;&#31034;&#20102;&#20197;&#19979;&#20869;&#23481;&#65306;1&#65289;Wasserstein GAN&#23545;&#20998;&#21306;&#20989;&#25968;&#36827;&#34892;&#20102;&#26377;&#20559;&#20272;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26080;&#20559;&#20272;&#35745;&#26041;&#27861;&#65307;2&#65289;&#22312;&#26368;&#20248;&#21270;&#20284;&#28982;&#26102;&#65292;&#24517;&#39035;&#26368;&#22823;&#21270;&#29983;&#25104;&#22120;&#30340;&#29109;&#12290;&#36825;&#34987;&#20551;&#35774;&#20250;&#25552;&#20379;&#26356;&#22909;&#30340;&#27169;&#24335;&#35206;&#30422;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#26126;&#30830;&#35745;&#31639;&#20102;&#29983;&#25104;&#26679;&#26412;&#30340;&#23494;&#24230;&#12290;&#36825;&#26159;&#35774;&#35745;&#26080;&#20559;&#20272;&#35745;&#20998;&#21306;&#20989;&#25968;&#20197;&#21450;&#35745;&#31639;&#29983;&#25104;&#22120;&#29109;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#29983;&#25104;&#23494;&#24230;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#22411;&#30340;&#27969;&#32593;&#32476;&#26469;&#33719;&#24471;&#30340;&#65292;&#31216;&#20026;&#21333;&#21521;&#27969;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) can produce high-quality samples, but do not provide an estimate of the probability density around the samples. However, it has been noted that maximizing the log-likelihood within an energy-based setting can lead to an adversarial framework where the discriminator provides unnormalized density (often called energy). We further develop this perspective, incorporate importance sampling, and show that 1) Wasserstein GAN performs a biased estimate of the partition function, and we propose instead to use an unbiased estimator; 2) when optimizing for likelihood, one must maximize generator entropy. This is hypothesized to provide a better mode coverage. Different from previous works, we explicitly compute the density of the generated samples. This is the key enabler to designing an unbiased estimator of the partition function and computation of the generator entropy term. The generator density is obtained via a new type of flow network, called one-way 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#23545;&#22478;&#24066;&#22522;&#30784;&#35774;&#26045;&#30456;&#20114;&#20381;&#36182;&#32593;&#32476;&#20013;&#30340;&#33030;&#24369;&#33410;&#28857;&#36827;&#34892;&#20102;&#20934;&#30830;&#24314;&#27169;&#21644;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2307.09866</link><description>&lt;p&gt;
&#26816;&#27979;&#22478;&#24066;&#22522;&#30784;&#35774;&#26045;&#30456;&#20114;&#20381;&#36182;&#32593;&#32476;&#20013;&#30340;&#33030;&#24369;&#33410;&#28857;
&lt;/p&gt;
&lt;p&gt;
Detecting Vulnerable Nodes in Urban Infrastructure Interdependent Network. (arXiv:2307.09866v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09866
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#23545;&#22478;&#24066;&#22522;&#30784;&#35774;&#26045;&#30456;&#20114;&#20381;&#36182;&#32593;&#32476;&#20013;&#30340;&#33030;&#24369;&#33410;&#28857;&#36827;&#34892;&#20102;&#20934;&#30830;&#24314;&#27169;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21644;&#25551;&#36848;&#22478;&#24066;&#22522;&#30784;&#35774;&#26045;&#30340;&#33030;&#24369;&#24615;&#23545;&#25105;&#20204;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#36825;&#20123;&#22522;&#30784;&#35774;&#26045;&#26159;&#22478;&#24066;&#27491;&#24120;&#36816;&#34892;&#25152;&#24517;&#38656;&#30340;&#24037;&#31243;&#35774;&#26045;&#65292;&#20197;&#32593;&#32476;&#30340;&#24418;&#24335;&#33258;&#28982;&#23384;&#22312;&#12290;&#28508;&#22312;&#30340;&#24212;&#29992;&#21253;&#25324;&#20445;&#25252;&#33030;&#24369;&#35774;&#26045;&#21644;&#35774;&#35745;&#31283;&#20581;&#30340;&#25299;&#25169;&#32467;&#26500;&#31561;&#12290;&#30001;&#20110;&#19981;&#21516;&#25299;&#25169;&#29305;&#24615;&#21644;&#22522;&#30784;&#35774;&#26045;&#33030;&#24369;&#24615;&#20197;&#21450;&#20854;&#22797;&#26434;&#30340;&#28436;&#21270;&#26426;&#21046;&#20043;&#38388;&#30340;&#24378;&#20851;&#32852;&#65292;&#19968;&#20123;&#21551;&#21457;&#24335;&#20998;&#26512;&#21644;&#26426;&#22120;&#36741;&#21161;&#20998;&#26512;&#22312;&#35299;&#20915;&#36825;&#31181;&#22330;&#26223;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#30456;&#20114;&#20381;&#36182;&#32593;&#32476;&#24314;&#27169;&#20026;&#24322;&#26500;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20934;&#30830;&#22320;&#25551;&#36848;&#22478;&#24066;&#31995;&#32479;&#30340;&#33030;&#24369;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#29702;&#35299;&#21644;&#20998;&#26512;&#24322;&#26500;&#22270;&#65292;&#20174;&#32780;&#33021;&#22815;&#25429;&#25417;&#32423;&#32852;&#22833;&#36133;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding and characterizing the vulnerability of urban infrastructures, which refers to the engineering facilities essential for the regular running of cities and that exist naturally in the form of networks, is of great value to us. Potential applications include protecting fragile facilities and designing robust topologies, etc. Due to the strong correlation between different topological characteristics and infrastructure vulnerability and their complicated evolution mechanisms, some heuristic and machine-assisted analysis fall short in addressing such a scenario. In this paper, we model the interdependent network as a heterogeneous graph and propose a system based on graph neural network with reinforcement learning, which can be trained on real-world data, to characterize the vulnerability of the city system accurately. The presented system leverages deep learning techniques to understand and analyze the heterogeneous graph, which enables us to capture the risk of cascade failu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#32676;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23450;&#20041;&#32467;&#26500;&#21160;&#21147;&#23398;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#32467;&#21512;&#20102;&#29289;&#29702;&#22522;&#30784;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#30456;&#20284;&#29289;&#29702;&#29616;&#35937;&#30340;&#20154;&#32676;&#20013;&#23398;&#20064;&#20851;&#31995;&#65292;&#26500;&#24314;&#21487;&#36716;&#31227;&#12289;&#21487;&#35299;&#37322;&#12289;&#20540;&#24471;&#20449;&#36182;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.09862</link><description>&lt;p&gt;
&#36808;&#21521;&#22522;&#20110;&#20154;&#32676;&#20449;&#24687;&#30340;&#32467;&#26500;&#21160;&#21147;&#23398;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#23450;&#20041;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards a population-informed approach to the definition of data-driven models for structural dynamics. (arXiv:2307.09862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#32676;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23450;&#20041;&#32467;&#26500;&#21160;&#21147;&#23398;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#32467;&#21512;&#20102;&#29289;&#29702;&#22522;&#30784;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#30456;&#20284;&#29289;&#29702;&#29616;&#35937;&#30340;&#20154;&#32676;&#20013;&#23398;&#20064;&#20851;&#31995;&#65292;&#26500;&#24314;&#21487;&#36716;&#31227;&#12289;&#21487;&#35299;&#37322;&#12289;&#20540;&#24471;&#20449;&#36182;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#24433;&#21709;&#20102;&#35768;&#22810;&#39046;&#22495;&#20013;&#22810;&#31181;&#29616;&#35937;&#30340;&#24314;&#27169;&#26041;&#24335;&#65292;&#20854;&#20013;&#20043;&#19968;&#23601;&#26159;&#32467;&#26500;&#21160;&#21147;&#23398;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26159;&#38382;&#39064;&#29305;&#23450;&#30340;&#65292;&#20182;&#20204;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#24448;&#24448;&#25928;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#32467;&#21512;&#29289;&#29702;&#22522;&#30784;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26041;&#27861;&#34987;&#24320;&#21457;&#20986;&#26469;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#20063;&#38656;&#35201;&#20998;&#26512;&#32773;&#23545;&#38382;&#39064;&#30340;&#29289;&#29702;&#22522;&#30784;&#26377;&#25152;&#20102;&#35299;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25512;&#21160;&#20351;&#29992;&#20174;&#30456;&#20284;&#29289;&#29702;&#29616;&#35937;&#30340;&#20154;&#32676;&#20013;&#23398;&#20064;&#36825;&#31181;&#20851;&#31995;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#24320;&#21457;&#21463;&#21040;&#29289;&#29702;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#26377;&#38480;&#20803;&#27169;&#22411;&#30340;&#21551;&#21457;&#12290;&#36825;&#20123;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#21487;&#36716;&#31227;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#21644;&#20540;&#24471;&#20449;&#36182;&#30340;&#65292;&#32780;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35828;&#65292;&#36825;&#20123;&#23646;&#24615;&#24182;&#19981;&#26159;&#36731;&#26494;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has affected the way in which many phenomena for various domains are modelled, one of these domains being that of structural dynamics. However, because machine-learning algorithms are problem-specific, they often fail to perform efficiently in cases of data scarcity. To deal with such issues, combination of physics-based approaches and machine learning algorithms have been developed. Although such methods are effective, they also require the analyser's understanding of the underlying physics of the problem. The current work is aimed at motivating the use of models which learn such relationships from a population of phenomena, whose underlying physics are similar. The development of such models is motivated by the way that physics-based models, and more specifically finite element models, work. Such models are considered transferrable, explainable and trustworthy, attributes which are not trivially imposed or achieved for machine-learning models. For this reason, machin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23547;&#25214;&#20449;&#29992;&#25351;&#25968;&#26399;&#26435;&#30340;&#26368;&#20339;&#23545;&#20914;&#31574;&#30053;&#12290;&#36890;&#36807;&#24212;&#29992;&#20449;&#20219;&#21306;&#22495;&#27874;&#21160;&#29575;&#20248;&#21270;&#31639;&#27861;&#65292;&#35777;&#26126;&#25152;&#24471;&#21040;&#30340;&#23545;&#20914;&#31574;&#30053;&#20248;&#20110;&#20256;&#32479;&#30340;Black &amp; Scholes&#30340;Delta&#23545;&#20914;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.09844</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#20449;&#29992;&#25351;&#25968;&#26399;&#26435;&#23545;&#20914;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Credit Index Option Hedging. (arXiv:2307.09844v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23547;&#25214;&#20449;&#29992;&#25351;&#25968;&#26399;&#26435;&#30340;&#26368;&#20339;&#23545;&#20914;&#31574;&#30053;&#12290;&#36890;&#36807;&#24212;&#29992;&#20449;&#20219;&#21306;&#22495;&#27874;&#21160;&#29575;&#20248;&#21270;&#31639;&#27861;&#65292;&#35777;&#26126;&#25152;&#24471;&#21040;&#30340;&#23545;&#20914;&#31574;&#30053;&#20248;&#20110;&#20256;&#32479;&#30340;Black &amp; Scholes&#30340;Delta&#23545;&#20914;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30528;&#37325;&#20110;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25214;&#21040;&#20449;&#29992;&#25351;&#25968;&#26399;&#26435;&#30340;&#26368;&#20339;&#23545;&#20914;&#31574;&#30053;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#20851;&#27880;&#29616;&#23454;&#24615;&#65292;&#21363;&#31163;&#25955;&#26102;&#38388;&#12289;&#20132;&#26131;&#25104;&#26412;&#65307;&#29978;&#33267;&#22312;&#30495;&#23454;&#24066;&#22330;&#25968;&#25454;&#19978;&#27979;&#35797;&#25105;&#20204;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#31639;&#27861;&#8212;&#8212;&#20449;&#20219;&#21306;&#22495;&#27874;&#21160;&#29575;&#20248;&#21270;&#65288;TRVO&#65289;&#31639;&#27861;&#65292;&#24182;&#19988;&#35777;&#26126;&#25152;&#24471;&#21040;&#30340;&#23545;&#20914;&#31574;&#30053;&#20248;&#20110;&#20174;&#19994;&#32773;&#30340;Black &amp; Scholes&#30340;Delta&#23545;&#20914;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on finding the optimal hedging strategy of a credit index option using reinforcement learning. We take a practical approach, where the focus is on realism i.e. discrete time, transaction costs; even testing our policy on real market data. We apply a state of the art algorithm, the Trust Region Volatility Optimization (TRVO) algorithm and show that the derived hedging strategy outperforms the practitioner's Black &amp; Scholes delta hedge.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25237;&#24433;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#20960;&#20046;&#32447;&#24615;&#26102;&#38388;&#20869;&#23558;&#30697;&#38453;&#25237;&#24433;&#21040; $\ell_{1,\infty}$ &#29699;&#38754;&#12290;&#35813;&#31639;&#27861;&#26131;&#20110;&#23454;&#29616;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#25910;&#25947;&#21040;&#31934;&#30830;&#35299;&#12290;&#21516;&#26102;&#65292;&#23558;&#35813;&#31639;&#27861;&#24212;&#29992;&#20110;&#33258;&#32534;&#30721;&#22120;&#35757;&#32451;&#20013;&#21487;&#20197;&#23454;&#29616;&#29305;&#24449;&#36873;&#25321;&#21644;&#26435;&#37325;&#30340;&#31232;&#30095;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.09836</link><description>&lt;p&gt;
&#22312;&#20960;&#20046;&#32447;&#24615;&#26102;&#38388;&#20869;&#25237;&#24433;&#21040; $\ell_{1,\infty}$ &#29699;&#38754;&#65307;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Near-Linear Time Projection onto the $\ell_{1,\infty}$ Ball; Application to Sparse Autoencoders. (arXiv:2307.09836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25237;&#24433;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#20960;&#20046;&#32447;&#24615;&#26102;&#38388;&#20869;&#23558;&#30697;&#38453;&#25237;&#24433;&#21040; $\ell_{1,\infty}$ &#29699;&#38754;&#12290;&#35813;&#31639;&#27861;&#26131;&#20110;&#23454;&#29616;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#25910;&#25947;&#21040;&#31934;&#30830;&#35299;&#12290;&#21516;&#26102;&#65292;&#23558;&#35813;&#31639;&#27861;&#24212;&#29992;&#20110;&#33258;&#32534;&#30721;&#22120;&#35757;&#32451;&#20013;&#21487;&#20197;&#23454;&#29616;&#29305;&#24449;&#36873;&#25321;&#21644;&#26435;&#37325;&#30340;&#31232;&#30095;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#23547;&#25214;&#31232;&#30095;&#24615;&#23545;&#20110;&#21152;&#36895;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#33267;&#20851;&#37325;&#35201;&#12290;&#25237;&#24433;&#21040; $\ell_{1,2}$ &#21644; $\ell_{1,\infty}$ &#26159;&#31232;&#30095;&#21270;&#21644;&#38477;&#20302;&#31070;&#32463;&#32593;&#32476;&#25972;&#20307;&#25104;&#26412;&#30340;&#26368;&#39640;&#25928;&#25216;&#26415;&#20043;&#19968;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340; $\ell_{1,\infty}$ &#33539;&#25968;&#29699;&#38754;&#30340;&#25237;&#24433;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#30340;&#26368;&#22351;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026; $\mathcal{O}\big(nm+J\log(nm)\big)$&#65292;&#20854;&#20013;&#30697;&#38453;&#20026; $\mathbb{R}^{n\times m}$&#12290;$J$ &#26159;&#19968;&#20010;&#22312;&#31232;&#30095;&#24615;&#39640;&#26102;&#36235;&#36817;&#20110;0&#65292;&#22312;&#31232;&#30095;&#24615;&#20302;&#26102;&#36235;&#36817;&#20110; $nm$ &#30340;&#39033;&#12290;&#35813;&#31639;&#27861;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#20445;&#35777;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#25910;&#25947;&#21040;&#31934;&#30830;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#26102;&#23558; $\ell_{1,\infty}$ &#29699;&#38754;&#25237;&#24433;&#32435;&#20837;&#20854;&#20013;&#65292;&#20197;&#24378;&#21046;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#21644;&#26435;&#37325;&#30340;&#31232;&#30095;&#21270;&#12290;&#22312;&#25105;&#20204;&#30340;&#29983;&#29289;&#23398;&#24212;&#29992;&#20013;&#65292;&#31232;&#30095;&#21270;&#20027;&#35201;&#20986;&#29616;&#22312;&#32534;&#30721;&#22120;&#20013;&#65292;&#20197;&#23454;&#29616;&#29305;&#24449;&#36873;&#25321;&#65292;&#22240;&#20026;&#21482;&#26377;&#38750;&#24120;&#23567;&#30340;&#19968;&#37096;&#20998;&#25968;&#25454;&#65288;&lt;2%&#65289;&#26159;&#30456;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Looking for sparsity is nowadays crucial to speed up the training of large-scale neural networks. Projections onto the $\ell_{1,2}$ and $\ell_{1,\infty}$ are among the most efficient techniques to sparsify and reduce the overall cost of neural networks. In this paper, we introduce a new projection algorithm for the $\ell_{1,\infty}$ norm ball. The worst-case time complexity of this algorithm is $\mathcal{O}\big(nm+J\log(nm)\big)$ for a matrix in $\mathbb{R}^{n\times m}$. $J$ is a term that tends to 0 when the sparsity is high, and to $nm$ when the sparsity is low. Its implementation is easy and it is guaranteed to converge to the exact solution in a finite time. Moreover, we propose to incorporate the $\ell_{1,\infty}$ ball projection while training an autoencoder to enforce feature selection and sparsity of the weights. Sparsification appears in the encoder to primarily do feature selection due to our application in biology, where only a very small part ($&lt;2\%$) of the data is relevan
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#31070;&#32463;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;(DON)&#22312;&#23545;Lipschitz&#31639;&#23376;&#30340;&#36924;&#36817;&#36895;&#29575;&#30028;&#19978;&#30340;&#26222;&#36866;&#24615;&#65292;&#19981;&#38656;&#35201;G&#26159;&#20840;&#32431;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.09835</link><description>&lt;p&gt;
&#21363;&#20351;G&#26159;Lipschitz&#25110;Holder&#36830;&#32493;&#30340;&#65292;&#25105;&#20204;&#20063;&#24314;&#31435;&#20102;&#19968;&#31867;&#31070;&#32463;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;(DON)&#23545;Lipschitz&#31639;&#23376;&#30340;&#26222;&#36941;&#24615;&#21644;&#36924;&#36817;&#36895;&#29575;&#30028;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Deep Operator Network Approximation Rates for Lipschitz Operators. (arXiv:2307.09835v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09835
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#31070;&#32463;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;(DON)&#22312;&#23545;Lipschitz&#31639;&#23376;&#30340;&#36924;&#36817;&#36895;&#29575;&#30028;&#19978;&#30340;&#26222;&#36866;&#24615;&#65292;&#19981;&#38656;&#35201;G&#26159;&#20840;&#32431;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#21487;&#20998;Hilbert&#31354;&#38388;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#20010;&#31867;&#20284;&#20110;Lipschitz&#65288;&#25110;Holder&#65289;&#36830;&#32493;&#26144;&#23556;G&#65306;X&#8594;Y&#30340;&#31070;&#32463;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;(DON)&#30340;&#26222;&#36941;&#24615;&#21644;&#34920;&#36798;&#36895;&#29575;&#30028;. DON&#26550;&#26500;&#20351;&#29992;&#32447;&#24615;&#32534;&#30721;&#22120;E&#21644;&#35299;&#30721;&#22120;D&#36890;&#36807;X&#65292;Y&#30340;&#65288;&#21452;&#27491;&#20132;&#65289;Riesz&#22522;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#22312;&#24207;&#21015;&#31354;&#38388;l^2(N)&#19978;&#26159;Lipschitz&#36830;&#32493;&#30340;&#26080;&#38480;&#32500;&#21442;&#25968;&#22352;&#26631;&#26144;&#23556;&#30340;&#36817;&#20284;&#32593;&#32476;. &#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#29616;&#22312;&#30340;&#34920;&#36798;&#36895;&#29575;&#32467;&#26524;&#19981;&#38656;&#35201;G&#26159;&#20840;&#32431;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We establish universality and expression rate bounds for a class of neural Deep Operator Networks (DON) emulating Lipschitz (or H\"older) continuous maps $\mathcal G:\mathcal X\to\mathcal Y$ between (subsets of) separable Hilbert spaces $\mathcal X$, $\mathcal Y$. The DON architecture considered uses linear encoders $\mathcal E$ and decoders $\mathcal D$ via (biorthogonal) Riesz bases of $\mathcal X$, $\mathcal Y$, and an approximator network of an infinite-dimensional, parametric coordinate map that is Lipschitz continuous on the sequence space $\ell^2(\mathbb N)$. Unlike previous works ([Herrmann, Schwab and Zech: Neural and Spectral operator surrogates: construction and expression rate bounds, SAM Report, 2022], [Marcati and Schwab: Exponential Convergence of Deep Operator Networks for Elliptic Partial Differential Equations, SAM Report, 2022]), which required for example $\mathcal G$ to be holomorphic, the present expression rate results require mere Lipschitz (or H\"older) continu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#33258;&#28982;&#22270;&#20687;&#19978;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#20542;&#21521;&#20110;&#25214;&#21040;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#39318;&#20808;&#23398;&#21040;&#30340;&#20869;&#23481;&#21462;&#20915;&#20110;&#26368;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#39057;&#29575;&#29305;&#24449;&#65292;&#36825;&#21487;&#20197;&#26159;&#20302;&#39057;&#25110;&#39640;&#39057;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20063;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#26631;&#20934;&#21644;&#26041;&#27861;&#26469;&#35782;&#21035;&#39057;&#29575;&#24555;&#25463;&#36335;&#24452;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#22270;&#20687;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09829</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#23398;&#21040;&#20102;&#20160;&#20040;&#65311;&#22522;&#20110;&#39057;&#29575;&#30340;&#24555;&#25463;&#36335;&#24452;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
What do neural networks learn in image classification? A frequency shortcut perspective. (arXiv:2307.09829v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#33258;&#28982;&#22270;&#20687;&#19978;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#20542;&#21521;&#20110;&#25214;&#21040;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#39318;&#20808;&#23398;&#21040;&#30340;&#20869;&#23481;&#21462;&#20915;&#20110;&#26368;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#39057;&#29575;&#29305;&#24449;&#65292;&#36825;&#21487;&#20197;&#26159;&#20302;&#39057;&#25110;&#39640;&#39057;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20063;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#26631;&#20934;&#21644;&#26041;&#27861;&#26469;&#35782;&#21035;&#39057;&#29575;&#24555;&#25463;&#36335;&#24452;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#22270;&#20687;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39057;&#29575;&#20998;&#26512;&#23545;&#20110;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#26426;&#21046;&#38750;&#24120;&#26377;&#29992;&#12290;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;NNs&#30340;&#23398;&#20064;&#21160;&#24577;&#19978;&#65292;&#32780;&#24456;&#23569;&#26377;&#20851;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25193;&#23637;&#20102;&#39057;&#29575;&#24555;&#25463;&#36335;&#24452;&#30340;&#29702;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35774;&#35745;&#20102;&#22312;&#19981;&#21516;&#39057;&#27573;&#19978;&#20855;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;NNs&#20542;&#21521;&#20110;&#25214;&#21040;&#20998;&#31867;&#30340;&#31616;&#21333;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#20854;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#39318;&#20808;&#23398;&#21040;&#30340;&#20869;&#23481;&#21462;&#20915;&#20110;&#26368;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#39057;&#29575;&#29305;&#24449;&#65292;&#21487;&#20197;&#26159;&#20302;&#39057;&#25110;&#39640;&#39057;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#28982;&#22270;&#20687;&#39564;&#35777;&#20102;&#36825;&#19968;&#29616;&#35937;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#34913;&#37327;&#31867;&#21035;&#39057;&#29575;&#29305;&#24449;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35782;&#21035;&#39057;&#29575;&#24555;&#25463;&#36335;&#24452;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39057;&#29575;&#24555;&#25463;&#36335;&#24452;&#21487;&#20197;&#22522;&#20110;&#32441;&#29702;&#25110;&#24418;&#29366;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#20309;&#31181;&#26041;&#24335;&#33021;&#22815;&#26368;&#22909;&#22320;&#31616;&#21270;&#30446;&#26631;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Frequency analysis is useful for understanding the mechanisms of representation learning in neural networks (NNs). Most research in this area focuses on the learning dynamics of NNs for regression tasks, while little for classification. This study empirically investigates the latter and expands the understanding of frequency shortcuts. First, we perform experiments on synthetic datasets, designed to have a bias in different frequency bands. Our results demonstrate that NNs tend to find simple solutions for classification, and what they learn first during training depends on the most distinctive frequency characteristics, which can be either low- or high-frequencies. Second, we confirm this phenomenon on natural images. We propose a metric to measure class-wise frequency characteristics and a method to identify frequency shortcuts. The results show that frequency shortcuts can be texture-based or shape-based, depending on what best simplifies the objective. Third, we validate the transf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;NAFLD&#39044;&#27979;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#32508;&#21512;&#20020;&#24202;&#25968;&#25454;&#38598;&#21644;&#26234;&#33021;&#26041;&#27861;&#65292;&#20026;NAFLD&#30340;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.09823</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#30142;&#30149;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Learning based Prediction for Disease. (arXiv:2307.09823v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;NAFLD&#39044;&#27979;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#32508;&#21512;&#20020;&#24202;&#25968;&#25454;&#38598;&#21644;&#26234;&#33021;&#26041;&#27861;&#65292;&#20026;NAFLD&#30340;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#37202;&#31934;&#24615;&#33026;&#32938;&#24615;&#32925;&#30149;&#65288;NAFLD&#65289;&#26159;&#24930;&#24615;&#32925;&#30149;&#30340;&#26368;&#24120;&#35265;&#21407;&#22240;&#65292;&#21487;&#20197;&#36890;&#36807;&#20934;&#30830;&#39044;&#27979;&#26469;&#39044;&#38450;&#26202;&#26399;&#32420;&#32500;&#21270;&#21644;&#32925;&#30828;&#21270;&#12290;&#28982;&#32780;&#65292;&#32925;&#27963;&#26816;&#20316;&#20026;NAFLD&#35786;&#26029;&#30340;&#37329;&#26631;&#20934;&#26159;&#20405;&#20837;&#24615;&#30340;&#12289;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#23481;&#26131;&#20986;&#29616;&#37319;&#26679;&#35823;&#24046;&#12290;&#22240;&#27492;&#65292;&#38750;&#20405;&#20837;&#24615;&#30740;&#31350;&#38750;&#24120;&#26377;&#21069;&#26223;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#32508;&#21512;&#30740;&#31350;&#25968;&#25454;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26234;&#33021;&#26041;&#27861;&#65292;&#30446;&#21069;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;NAFLD&#35786;&#26029;&#31995;&#32479;&#65288;DeepFLDDiag&#65289;&#65292;&#32467;&#21512;&#20102;&#19968;&#20010;&#32508;&#21512;&#20020;&#24202;&#25968;&#25454;&#38598;&#65288;FLDData&#65289;&#21644;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;NAFLD&#39044;&#27979;&#26041;&#27861;&#65288;DeepFLD&#65289;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;6000&#22810;&#21517;&#21442;&#19982;&#32773;&#30340;&#20307;&#26684;&#26816;&#26597;&#12289;&#23454;&#39564;&#23460;&#26816;&#26597;&#21644;&#24433;&#20687;&#23398;&#30740;&#31350;&#12289;&#24191;&#27867;&#30340;&#38382;&#21367;&#35843;&#26597;&#20197;&#21450;&#37096;&#20998;&#21442;&#19982;&#32773;&#30340;&#38754;&#37096;&#22270;&#20687;&#65292;&#23545;&#20020;&#24202;&#30740;&#31350;&#26469;&#35828;&#26159;&#20840;&#38754;&#21644;&#26377;&#20215;&#20540;&#30340;&#12290;&#20174;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#23450;&#37327;&#20998;&#26512;&#24182;&#36873;&#25321;&#23545;NAFLD&#39044;&#27979;&#26368;&#26377;&#36129;&#29486;&#30340;&#20020;&#24202;&#20803;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non alcoholic fatty liver disease (NAFLD) is the most common cause of chronic liver disease, which can be predicted accurately to prevent advanced fibrosis and cirrhosis. While, a liver biopsy, the gold standard for NAFLD diagnosis, is invasive, expensive, and prone to sampling errors. Therefore, non-invasive studies are extremely promising, yet they are still in their infancy due to the lack of comprehensive research data and intelligent methods for multi-modal data. This paper proposes a NAFLD diagnosis system (DeepFLDDiag) combining a comprehensive clinical dataset (FLDData) and a multi-modal learning based NAFLD prediction method (DeepFLD). The dataset includes over 6000 participants physical examinations, laboratory and imaging studies, extensive questionnaires, and facial images of partial participants, which is comprehensive and valuable for clinical studies. From the dataset, we quantitatively analyze and select clinical metadata that most contribute to NAFLD prediction. Furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36816;&#31639;&#31526;AST&#65292;&#26469;&#23398;&#20064;&#27599;&#20010;&#36890;&#36947;&#30340;&#38408;&#20540;&#65292;&#22312;&#21160;&#24577;&#30913;&#20849;&#25391;&#25104;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09818</link><description>&lt;p&gt;
&#21160;&#24577;&#30913;&#20849;&#25391;&#25104;&#20687;&#30340;&#28145;&#24230;&#23637;&#24320;&#25910;&#32553;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep unrolling Shrinkage Network for Dynamic MR imaging. (arXiv:2307.09818v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36816;&#31639;&#31526;AST&#65292;&#26469;&#23398;&#20064;&#27599;&#20010;&#36890;&#36947;&#30340;&#38408;&#20540;&#65292;&#22312;&#21160;&#24577;&#30913;&#20849;&#25391;&#25104;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31232;&#30095;&#20808;&#39564;&#30340;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#22312;&#21160;&#24577;&#30913;&#20849;&#25391;&#25104;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#36890;&#24120;&#37319;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#21462;&#21464;&#25442;&#22495;&#65292;&#28982;&#21518;&#23545;CNN&#21464;&#25442;&#25968;&#25454;&#24212;&#29992;&#36719;&#38408;&#20540;&#36816;&#31639;&#31526;&#26469;&#24378;&#21046;&#25191;&#34892;&#31232;&#30095;&#20808;&#39564;&#12290;&#28982;&#32780;&#65292;&#36719;&#38408;&#20540;&#36816;&#31639;&#31526;&#36890;&#24120;&#22312;&#25152;&#26377;&#36890;&#36947;&#19978;&#34987;&#38480;&#21046;&#20026;&#30456;&#21516;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36816;&#31639;&#31526;&#65292;&#31216;&#20026;&#36890;&#36947;&#27880;&#24847;&#21147;&#36719;&#38408;&#20540;&#65288;AST&#65289;&#65292;&#35813;&#36816;&#31639;&#31526;&#23398;&#20064;&#27599;&#20010;&#36890;&#36947;&#30340;&#38408;&#20540;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23637;&#24320;&#25910;&#32553;&#32593;&#32476;&#65288;DUS-Net&#65289;&#65292;&#36890;&#36807;&#23637;&#24320;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#22120;&#65288;ADMM&#65289;&#26469;&#20248;&#21270;&#21464;&#25442;&#21518;&#30340;$l_1$&#33539;&#25968;&#21160;&#24577;MR&#37325;&#24314;&#27169;&#22411;&#12290;&#22312;&#19968;&#20010;&#24320;&#25918;&#30340;&#21160;&#24577;MR&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;DUS-Net&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;\url{https:}&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep unrolling networks that utilize sparsity priors have achieved great success in dynamic magnetic resonance (MR) imaging. The convolutional neural network (CNN) is usually utilized to extract the transformed domain, and then the soft thresholding (ST) operator is applied to the CNN-transformed data to enforce the sparsity priors. However, the ST operator is usually constrained to be the same across all channels of the CNN-transformed data. In this paper, we propose a novel operator, called soft thresholding with channel attention (AST), that learns the threshold for each channel. In particular, we put forward a novel deep unrolling shrinkage network (DUS-Net) by unrolling the alternating direction method of multipliers (ADMM) for optimizing the transformed $l_1$ norm dynamic MR reconstruction model. Experimental results on an open-access dynamic cine MR dataset demonstrate that the proposed DUS-Net outperforms the state-of-the-art methods. The source code is available at \url{https:
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31232;&#30095;&#27491;&#21017;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#27969;&#24418;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#31232;&#30095;&#33258;&#36866;&#24212;&#30340;&#20146;&#21644;&#30697;&#38453;&#65292;&#24182;&#22312;&#36830;&#32493;&#26497;&#38480;&#19979;&#19982;&#25289;&#26222;&#25289;&#26031;&#22411;&#31639;&#23376;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2307.09816</link><description>&lt;p&gt;
&#29992;&#31232;&#30095;&#27491;&#21017;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#27969;&#24418;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Manifold Learning with Sparse Regularised Optimal Transport. (arXiv:2307.09816v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09816
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31232;&#30095;&#27491;&#21017;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#27969;&#24418;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#31232;&#30095;&#33258;&#36866;&#24212;&#30340;&#20146;&#21644;&#30697;&#38453;&#65292;&#24182;&#22312;&#36830;&#32493;&#26497;&#38480;&#19979;&#19982;&#25289;&#26222;&#25289;&#26031;&#22411;&#31639;&#23376;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#23398;&#20064;&#26159;&#29616;&#20195;&#32479;&#35745;&#23398;&#21644;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#20219;&#21153;&#12290;&#35768;&#22810;&#25968;&#25454;&#38598;&#65288;&#32454;&#32990;&#12289;&#25991;&#26723;&#12289;&#22270;&#20687;&#12289;&#20998;&#23376;&#65289;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#23884;&#20837;&#22312;&#39640;&#32500;&#29615;&#22659;&#31354;&#38388;&#20013;&#30340;&#28857;&#20113;&#65292;&#28982;&#32780;&#25968;&#25454;&#22266;&#26377;&#30340;&#33258;&#30001;&#24230;&#36890;&#24120;&#36828;&#36828;&#23569;&#20110;&#29615;&#22659;&#32500;&#24230;&#30340;&#25968;&#37327;&#12290;&#26816;&#27979;&#25968;&#25454;&#23884;&#20837;&#30340;&#28508;&#22312;&#27969;&#24418;&#26159;&#35768;&#22810;&#19979;&#28216;&#20998;&#26512;&#30340;&#20808;&#20915;&#26465;&#20214;&#12290;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#32463;&#24120;&#21463;&#21040;&#22122;&#22768;&#35266;&#27979;&#21644;&#25277;&#26679;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#25552;&#21462;&#20851;&#20110;&#28508;&#22312;&#27969;&#24418;&#30340;&#20449;&#24687;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#31216;&#29256;&#26412;&#30340;&#26368;&#20248;&#20256;&#36755;&#21644;&#20108;&#27425;&#27491;&#21017;&#21270;&#30340;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#26500;&#24314;&#20102;&#19968;&#20010;&#31232;&#30095;&#33258;&#36866;&#24212;&#30340;&#20146;&#21644;&#30697;&#38453;&#65292;&#21487;&#20197;&#35299;&#37322;&#20026;&#21452;&#38543;&#26426;&#26680;&#24402;&#19968;&#21270;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36830;&#32493;&#26497;&#38480;&#19979;&#20135;&#29983;&#30340;&#26680;&#19982;&#25289;&#26222;&#25289;&#26031;&#22411;&#31639;&#23376;&#19968;&#33268;&#65292;&#24182;&#24314;&#31435;&#20102;&#35813;&#26041;&#27861;&#30340;&#20581;&#22766;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manifold learning is a central task in modern statistics and data science. Many datasets (cells, documents, images, molecules) can be represented as point clouds embedded in a high dimensional ambient space, however the degrees of freedom intrinsic to the data are usually far fewer than the number of ambient dimensions. The task of detecting a latent manifold along which the data are embedded is a prerequisite for a wide family of downstream analyses. Real-world datasets are subject to noisy observations and sampling, so that distilling information about the underlying manifold is a major challenge. We propose a method for manifold learning that utilises a symmetric version of optimal transport with a quadratic regularisation that constructs a sparse and adaptive affinity matrix, that can be interpreted as a generalisation of the bistochastic kernel normalisation. We prove that the resulting kernel is consistent with a Laplace-type operator in the continuous limit, establish robustness
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36845;&#20195;&#26694;&#26550;GenKL&#65292;&#36890;&#36807;$(\alpha, \beta)$-&#24191;&#20041;KL&#25955;&#24230;&#26469;&#35299;&#20915;Web&#22270;&#20687;&#20013;&#30340;&#26631;&#31614;&#27169;&#31946;&#21644;&#38750;&#31526;&#21512;&#23454;&#20363;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.09810</link><description>&lt;p&gt;
GenKL&#65306;&#36890;&#36807;&#26032;&#30340;&#24191;&#20041;KL&#25955;&#24230;&#65292;&#35299;&#20915;Web&#22270;&#20687;&#20013;&#30340;&#26631;&#31614;&#27169;&#31946;&#21644;&#26631;&#31614;&#19981;&#31526;&#30340;&#36845;&#20195;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GenKL: An Iterative Framework for Resolving Label Ambiguity and Label Non-conformity in Web Images Via a New Generalized KL Divergence. (arXiv:2307.09810v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09810
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36845;&#20195;&#26694;&#26550;GenKL&#65292;&#36890;&#36807;$(\alpha, \beta)$-&#24191;&#20041;KL&#25955;&#24230;&#26469;&#35299;&#20915;Web&#22270;&#20687;&#20013;&#30340;&#26631;&#31614;&#27169;&#31946;&#21644;&#38750;&#31526;&#21512;&#23454;&#20363;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#25972;&#29702;&#30340;Web&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#65292;&#23384;&#22312;&#30528;&#27169;&#31946;&#65288;ID&#65289;&#23454;&#20363;&#21644;&#38750;&#31526;&#21512;&#65288;OOD&#65289;&#23454;&#20363;&#65292;&#25105;&#20204;&#32479;&#31216;&#20026;&#38750;&#31526;&#21512;&#65288;NC&#65289;&#23454;&#20363;&#12290;&#22312;&#35768;&#22810;&#26368;&#36817;&#30340;&#35299;&#20915;NC&#23454;&#20363;&#20135;&#29983;&#30340;&#36127;&#38754;&#24433;&#21709;&#30340;&#26041;&#27861;&#20013;&#65292;&#26680;&#24515;&#30340;&#38544;&#21547;&#20551;&#35774;&#26159;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#29109;&#26469;&#25214;&#21040;NC&#23454;&#20363;&#12290;&#20026;&#20102;&#23450;&#20041;"&#29109;"&#65292;&#25105;&#20204;&#23558;&#19968;&#20010;&#23454;&#20363;&#30340;&#36755;&#20986;&#39044;&#27979;&#21521;&#37327;&#35299;&#37322;&#20026;&#19968;&#20010;&#22810;&#39033;&#24335;&#38543;&#26426;&#21464;&#37327;&#30340;&#21442;&#25968;&#21521;&#37327;&#65292;&#30456;&#23545;&#20110;&#19968;&#20123;&#20855;&#26377;softmax&#36755;&#20986;&#23618;&#30340;&#35757;&#32451;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#29109;&#26368;&#22823;&#21270;&#26159;&#22522;&#20110;&#29702;&#24819;&#21270;&#20551;&#35774;&#65292;&#21363;NC&#23454;&#20363;&#30340;&#39044;&#27979;&#26159;"&#20960;&#20046;"&#22343;&#21248;&#20998;&#24067;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;Web&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#65292;&#26377;&#35768;&#22810;NC&#23454;&#20363;&#30340;&#39044;&#27979;&#36828;&#38750;&#22343;&#21248;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#29109;&#26368;&#22823;&#21270;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$(\alpha, \beta)$-&#24191;&#20041;KL&#25955;&#24230;&#65292;$\mathcal{D}_{\text{KL}}^{\alpha, beta}$&#65292;&#20174;&#32780;&#20801;&#35768;&#26356;&#22909;&#22320;&#22788;&#29702;&#38750;&#22343;&#21248;&#20998;&#24067;&#21644;&#26631;&#31614;&#27169;&#31946;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Web image datasets curated online inherently contain ambiguous in-distribution (ID) instances and out-of-distribution (OOD) instances, which we collectively call non-conforming (NC) instances. In many recent approaches for mitigating the negative effects of NC instances, the core implicit assumption is that the NC instances can be found via entropy maximization. For "entropy" to be well-defined, we are interpreting the output prediction vector of an instance as the parameter vector of a multinomial random variable, with respect to some trained model with a softmax output layer. Hence, entropy maximization is based on the idealized assumption that NC instances have predictions that are "almost" uniformly distributed. However, in real-world web image datasets, there are numerous NC instances whose predictions are far from being uniformly distributed. To tackle the limitation of entropy maximization, we propose $(\alpha, \beta)$-generalized KL divergence, $\mathcal{D}_{\text{KL}}^{\alpha,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25955;&#26694;&#26550;&#30340;&#22270;&#24418;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#33410;&#28857;&#20043;&#38388;&#30340;&#32622;&#20449;&#24230;&#24182;&#32858;&#21512;&#26799;&#24230;&#20449;&#24687;&#26469;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#20013;&#22830;&#26381;&#21153;&#22120;&#25925;&#38556;&#21644;&#32593;&#32476;&#25299;&#25169;&#25193;&#23637;&#24615;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.09801</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25955;&#26694;&#26550;&#30340;&#22270;&#24418;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Federated Learning Based on the Decentralized Framework. (arXiv:2307.09801v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09801
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25955;&#26694;&#26550;&#30340;&#22270;&#24418;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#23450;&#33410;&#28857;&#20043;&#38388;&#30340;&#32622;&#20449;&#24230;&#24182;&#32858;&#21512;&#26799;&#24230;&#20449;&#24687;&#26469;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#20013;&#22830;&#26381;&#21153;&#22120;&#25925;&#38556;&#21644;&#32593;&#32476;&#25299;&#25169;&#25193;&#23637;&#24615;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#23398;&#20064;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#36825;&#20123;&#22330;&#26223;&#38656;&#35201;&#26356;&#22810;&#30340;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#20010;&#20307;&#35774;&#22791;&#25110;&#25968;&#25454;&#20013;&#24515;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#22270;&#24418;&#32852;&#37030;&#23398;&#20064;&#20027;&#35201;&#22522;&#20110;&#32463;&#20856;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;&#23458;&#25143;&#31471;-&#26381;&#21153;&#22120;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#23458;&#25143;&#31471;-&#26381;&#21153;&#22120;&#26694;&#26550;&#38754;&#20020;&#35832;&#22914;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#21333;&#28857;&#25925;&#38556;&#21644;&#32593;&#32476;&#25299;&#25169;&#30340;&#25193;&#23637;&#24615;&#24046;&#31561;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#25955;&#26694;&#26550;&#21040;&#22270;&#24418;&#32852;&#37030;&#23398;&#20064;&#20013;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#33410;&#28857;&#20043;&#38388;&#30340;&#25968;&#25454;&#30456;&#20284;&#24615;&#30830;&#23450;&#33410;&#28857;&#20043;&#38388;&#30340;&#32622;&#20449;&#24230;&#65292;&#28982;&#21518;&#36890;&#36807;&#32447;&#24615;&#21152;&#26435;&#22522;&#20110;&#32622;&#20449;&#24230;&#26469;&#32858;&#21512;&#26799;&#24230;&#20449;&#24687;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;FedAvg&#12289;Fedprox&#12289;GCFL&#21644;GCFL+&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph learning has a wide range of applications in many scenarios, which require more need for data privacy. Federated learning is an emerging distributed machine learning approach that leverages data from individual devices or data centers to improve the accuracy and generalization of the model, while also protecting the privacy of user data. Graph-federated learning is mainly based on the classical federated learning framework i.e., the Client-Server framework. However, the Client-Server framework faces problems such as a single point of failure of the central server and poor scalability of network topology. First, we introduce the decentralized framework to graph-federated learning. Second, determine the confidence among nodes based on the similarity of data among nodes, subsequently, the gradient information is then aggregated by linear weighting based on confidence. Finally, the proposed method is compared with FedAvg, Fedprox, GCFL, and GCFL+ to verify the effectiveness of the pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22240;&#23376;&#27169;&#22411;&#32467;&#26500;&#26469;&#20135;&#29983;&#36981;&#23432;&#23618;&#27425;&#32467;&#26500;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;&#27169;&#22411;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26679;&#26412;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#39044;&#27979;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.09797</link><description>&lt;p&gt;
&#20855;&#26377;&#19968;&#33268;&#32858;&#21512;&#30340;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Forecasting with Coherent Aggregation. (arXiv:2307.09797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22240;&#23376;&#27169;&#22411;&#32467;&#26500;&#26469;&#20135;&#29983;&#36981;&#23432;&#23618;&#27425;&#32467;&#26500;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;&#27169;&#22411;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26679;&#26412;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#39044;&#27979;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#20934;&#30830;&#33719;&#24471;&#36981;&#23432;&#23618;&#27425;&#32467;&#26500;&#30340;&#27010;&#29575;&#39044;&#27979;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#36816;&#33829;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#33021;&#28304;&#31649;&#29702;&#12289;&#20379;&#24212;&#38142;&#35268;&#21010;&#21644;&#36164;&#28304;&#37197;&#32622;&#31561;&#39046;&#22495;&#12290;&#23545;&#20110;&#22810;&#21464;&#37327;&#39044;&#27979;&#65292;&#22522;&#26412;&#25361;&#25112;&#22312;&#20110;&#39044;&#27979;&#36890;&#24120;&#38656;&#35201;&#19982;&#23618;&#27425;&#32467;&#26500;&#20445;&#25345;&#19968;&#33268;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22240;&#23376;&#27169;&#22411;&#32467;&#26500;&#36890;&#36807;&#26500;&#24314;&#26469;&#20135;&#29983;&#19968;&#33268;&#30340;&#39044;&#27979;&#12290;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#35266;&#23519;&#32467;&#26524;&#65288;&#21487;&#20132;&#25442;&#24615;&#65289;&#65306;&#32622;&#25442;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#22522;&#26412;&#32423;&#21035;&#24207;&#21015;&#19981;&#20250;&#25913;&#21464;&#23427;&#20204;&#30340;&#32858;&#21512;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#22240;&#23376;&#12289;&#23427;&#20204;&#30340;&#21152;&#36733;&#21644;&#22522;&#26412;&#32423;&#21035;&#20998;&#24067;&#30340;&#21442;&#25968;&#65307;&#23427;&#20135;&#29983;&#21487;&#20197;&#26681;&#25454;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#24494;&#20998;&#30340;&#26679;&#26412;&#65307;&#22240;&#27492;&#23427;&#21487;&#20197;&#23545;&#20219;&#20309;&#22522;&#20110;&#26679;&#26412;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#21253;&#25324;&#36830;&#32493;&#25490;&#21517;&#27010;&#29575;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining accurate probabilistic forecasts while respecting hierarchical information is an important operational challenge in many applications, perhaps most obviously in energy management, supply chain planning, and resource allocation. The basic challenge, especially for multivariate forecasting, is that forecasts are often required to be coherent with respect to the hierarchical structure. In this paper, we propose a new model which leverages a factor model structure to produce coherent forecasts by construction. This is a consequence of a simple (exchangeability) observation: permuting \textit{}base-level series in the hierarchy does not change their aggregates. Our model uses a convolutional neural network to produce parameters for the factors, their loadings and base-level distributions; it produces samples which can be differentiated with respect to the model's parameters; and it can therefore optimize for any sample-based loss function, including the Continuous Ranked Probabili
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#20803;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#26089;&#26399;&#26102;&#38388;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#25968;&#25454;&#38598;&#36827;&#34892;&#23545;&#25239;&#23398;&#20064;&#21644;&#21033;&#29992;&#39069;&#22806;&#26679;&#26412;&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#24182;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20248;&#20110;&#21333;&#20219;&#21153;&#23398;&#20064;&#12289;&#32852;&#21512;&#23398;&#20064;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#32463;&#20856;&#39044;&#27979;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.09796</link><description>&lt;p&gt;
&#39044;&#27979;&#26089;&#26399;&#26102;&#38388;&#24207;&#21015;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Forecasting Early with Meta Learning. (arXiv:2307.09796v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#20803;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#26089;&#26399;&#26102;&#38388;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#25968;&#25454;&#38598;&#36827;&#34892;&#23545;&#25239;&#23398;&#20064;&#21644;&#21033;&#29992;&#39069;&#22806;&#26679;&#26412;&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#24182;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20248;&#20110;&#21333;&#20219;&#21153;&#23398;&#20064;&#12289;&#32852;&#21512;&#23398;&#20064;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#32463;&#20856;&#39044;&#27979;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#24207;&#21015;&#30340;&#26089;&#26399;&#35266;&#23519;&#26399;&#20013;&#65292;&#21487;&#33021;&#21482;&#26377;&#23569;&#37327;&#21382;&#21490;&#35266;&#23519;&#21487;&#29992;&#20110;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#20808;&#21069;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#24212;&#29992;&#20803;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39069;&#22806;&#25968;&#25454;&#38598;&#26679;&#26412;&#36827;&#34892;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#25968;&#25454;&#38598;&#36827;&#34892;&#23545;&#25239;&#23398;&#20064;&#65292;&#23398;&#20064;&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#30340;&#36741;&#21161;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;(FEML)&#37197;&#22791;&#20849;&#20139;&#30340;&#21367;&#31215;&#20027;&#24178;&#32593;&#32476;&#65292;&#21487;&#20197;&#23398;&#20064;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#38271;&#24230;&#36755;&#20837;&#30340;&#29305;&#24449;&#65292;&#24182;&#20855;&#26377;&#38024;&#23545;&#19981;&#21516;&#36755;&#20986;&#38271;&#24230;&#30340;&#25968;&#25454;&#38598;&#29305;&#23450;&#39044;&#27979;&#22836;&#12290;&#25105;&#20204;&#35777;&#26126;FEML&#21487;&#20197;&#22312;&#25968;&#25454;&#38598;&#20043;&#38388;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#22312;&#23545;&#25239;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#36827;&#34892;&#38468;&#21152;&#23398;&#20064;&#65292;&#25552;&#39640;&#19982;&#21333;&#20219;&#21153;&#23398;&#20064;&#12289;&#32852;&#21512;&#23398;&#20064;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#32463;&#20856;&#39044;&#27979;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the early observation period of a time series, there might be only a few historic observations available to learn a model. However, in cases where an existing prior set of datasets is available, Meta learning methods can be applicable. In this paper, we devise a Meta learning method that exploits samples from additional datasets and learns to augment time series through adversarial learning as an auxiliary task for the target dataset. Our model (FEML), is equipped with a shared Convolutional backbone that learns features for varying length inputs from different datasets and has dataset specific heads to forecast for different output lengths. We show that FEML can meta learn across datasets and by additionally learning on adversarial generated samples as auxiliary samples for the target dataset, it can improve the forecasting performance compared to single task learning, and various solutions adapted from Joint learning, Multi-task learning and classic forecasting baselines.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#38899;&#39057;&#23884;&#20837;&#27169;&#22411;&#22312;&#19981;&#21516;&#38899;&#20048;&#25991;&#21270;&#21644;&#39118;&#26684;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#36328;&#25991;&#21270;&#38899;&#20048;&#29702;&#35299;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.09795</link><description>&lt;p&gt;
&#20174;&#35199;&#26041;&#21040;&#19996;&#26041;&#65306;&#35841;&#26356;&#33021;&#29702;&#35299;&#20854;&#20182;&#20154;&#30340;&#38899;&#20048;&#65311;
&lt;/p&gt;
&lt;p&gt;
From West to East: Who can understand the music of the others better?. (arXiv:2307.09795v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09795
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#38899;&#39057;&#23884;&#20837;&#27169;&#22411;&#22312;&#19981;&#21516;&#38899;&#20048;&#25991;&#21270;&#21644;&#39118;&#26684;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#36328;&#25991;&#21270;&#38899;&#20048;&#29702;&#35299;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#21457;&#23637;&#24050;&#32463;&#20986;&#29616;&#20102;&#19968;&#20123;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23884;&#20837;&#21487;&#20197;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#32477;&#22823;&#22810;&#25968;&#36825;&#20123;&#27169;&#22411;&#37117;&#26159;&#22312;&#35199;&#26041;&#27969;&#34892;/&#25671;&#28378;&#38899;&#20048;&#21644;&#30456;&#20851;&#39118;&#26684;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#30740;&#31350;&#38382;&#39064;&#65292;&#21363;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#29992;&#26469;&#23398;&#20064;&#19981;&#21516;&#38899;&#20048;&#25991;&#21270;&#21644;&#39118;&#26684;&#30340;&#34920;&#31034;&#65292;&#25110;&#32773;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#26500;&#24314;&#31867;&#20284;&#30340;&#38899;&#20048;&#38899;&#39057;&#23884;&#20837;&#27169;&#22411;&#65292;&#35757;&#32451;&#25968;&#25454;&#26469;&#33258;&#19981;&#21516;&#30340;&#25991;&#21270;&#25110;&#39118;&#26684;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#26469;&#20102;&#35299;&#25968;&#25454;&#25152;&#23646;&#19981;&#21516;&#38899;&#20048;&#25991;&#21270;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#20010;&#35199;&#26041;&#38899;&#20048;&#25968;&#25454;&#38598;&#65292;&#20004;&#20010;&#26469;&#33258;&#19996;&#22320;&#20013;&#28023;&#25991;&#21270;&#30340;&#20256;&#32479;/&#27665;&#38388;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#20004;&#20010;&#23646;&#20110;&#21360;&#24230;&#33402;&#26415;&#38899;&#20048;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19977;&#20010;&#28145;&#24230;&#38899;&#39057;&#23884;&#20837;&#27169;&#22411;&#65292;&#24182;&#36328;&#39046;&#22495;&#36827;&#34892;&#20102;&#36716;&#31227;&#23398;&#20064;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#21644;&#19968;&#20010;Transformer-based&#30340;&#27169;&#22411;&#65292;&#20197;&#22312;&#27599;&#20010;&#30446;&#26631;&#20013;&#36827;&#34892;&#33258;&#21160;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in MIR have led to several benchmark deep learning models whose embeddings can be used for a variety of downstream tasks. At the same time, the vast majority of these models have been trained on Western pop/rock music and related styles. This leads to research questions on whether these models can be used to learn representations for different music cultures and styles, or whether we can build similar music audio embedding models trained on data from different cultures or styles. To that end, we leverage transfer learning methods to derive insights about the similarities between the different music cultures to which the data belongs to. We use two Western music datasets, two traditional/folk datasets coming from eastern Mediterranean cultures, and two datasets belonging to Indian art music. Three deep audio embedding models are trained and transferred across domains, including two CNN-based and a Transformer-based architecture, to perform auto-tagging for each targe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35745;&#31639;&#36882;&#24402;&#25945;&#23398;&#32500;&#24230;&#65288;RTD&#65289;&#38382;&#39064;&#30340;&#38590;&#24230;&#65292;&#35777;&#26126;&#20102;&#22312;&#25351;&#25968;&#26102;&#38388;&#20551;&#35774;&#65288;ETH&#65289;&#19979;&#65292;&#35813;&#38382;&#39064;&#30340;&#35745;&#31639;&#26102;&#38388;&#20026;$n^{\Omega(\log n)}$&#65292;&#19982;&#26292;&#21147;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#30456;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2307.09792</link><description>&lt;p&gt;
&#35745;&#31639;&#36882;&#24402;&#25945;&#23398;&#32500;&#24230;&#30340;&#38590;&#24230;&#30340;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
A Note on Hardness of Computing Recursive Teaching Dimension. (arXiv:2307.09792v1 [cs.CC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35745;&#31639;&#36882;&#24402;&#25945;&#23398;&#32500;&#24230;&#65288;RTD&#65289;&#38382;&#39064;&#30340;&#38590;&#24230;&#65292;&#35777;&#26126;&#20102;&#22312;&#25351;&#25968;&#26102;&#38388;&#20551;&#35774;&#65288;ETH&#65289;&#19979;&#65292;&#35813;&#38382;&#39064;&#30340;&#35745;&#31639;&#26102;&#38388;&#20026;$n^{\Omega(\log n)}$&#65292;&#19982;&#26292;&#21147;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#31616;&#30701;&#30340;&#27880;&#37322;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35745;&#31639;&#36882;&#24402;&#25945;&#23398;&#32500;&#24230;&#65288;RTD&#65289;&#38382;&#39064;&#65288;&#32473;&#23450;&#20316;&#20026;&#36755;&#20837;&#30340;&#27010;&#24565;&#31867;&#65289;&#38656;&#35201;$n^{\Omega(\log n)}$&#30340;&#26102;&#38388;&#65292;&#20551;&#35774;&#25351;&#25968;&#26102;&#38388;&#20551;&#35774;&#65288;ETH&#65289;&#25104;&#31435;&#12290;&#36825;&#19982;&#38382;&#39064;&#30340;&#26292;&#21147;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;$n^{O(\log n)}$&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this short note, we show that the problem of computing the recursive teaching dimension (RTD) for a concept class (given explicitly as input) requires $n^{\Omega(\log n)}$-time, assuming the exponential time hypothesis (ETH). This matches the running time $n^{O(\log n)}$ of the brute-force algorithm for the problem.
&lt;/p&gt;</description></item><item><title>ZeroQuant-FP&#36890;&#36807;&#20351;&#29992;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892;LLMs&#35757;&#32451;&#21518;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24179;&#34913;&#35745;&#31639;&#25928;&#29575;&#21644;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#30340;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;FP8&#28608;&#27963;&#20248;&#20110;INT8&#65292;&#24182;&#19988;FP4&#26435;&#37325;&#34920;&#29616;&#19982;INT4&#30456;&#24403;&#29978;&#33267;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2307.09782</link><description>&lt;p&gt;
ZeroQuant-FP: &#20351;&#29992;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892;LLMs&#35757;&#32451;&#21518;&#37327;&#21270;&#30340;&#19968;&#39033;&#39134;&#36291;
&lt;/p&gt;
&lt;p&gt;
ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats. (arXiv:2307.09782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09782
&lt;/p&gt;
&lt;p&gt;
ZeroQuant-FP&#36890;&#36807;&#20351;&#29992;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892;LLMs&#35757;&#32451;&#21518;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24179;&#34913;&#35745;&#31639;&#25928;&#29575;&#21644;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#30340;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;FP8&#28608;&#27963;&#20248;&#20110;INT8&#65292;&#24182;&#19988;FP4&#26435;&#37325;&#34920;&#29616;&#19982;INT4&#30456;&#24403;&#29978;&#33267;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22797;&#26434;&#39046;&#22495;&#20013;&#65292;&#24179;&#34913;&#35745;&#31639;&#25928;&#29575;&#21644;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#35752;&#28014;&#28857;&#65288;FP&#65289;&#37327;&#21270;&#30340;&#21487;&#34892;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;FP8&#21644;FP4&#65292;&#20197;&#24212;&#23545;&#22343;&#21248;&#37327;&#21270;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#22788;&#29702;&#31163;&#32676;&#20540;&#65292;&#24182;&#21463;&#21040;NVIDIA H100&#30828;&#20214;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#35843;&#26597;&#21457;&#29616;&#65292;&#22312;LLMs&#20013;&#65292;FP8&#28608;&#27963;&#22987;&#32456;&#20248;&#20110;&#20854;&#25972;&#25968;&#65288;INT8&#65289;&#31561;&#25928;&#65292;&#24615;&#33021;&#20248;&#21183;&#22312;&#21253;&#21547;&#36229;&#36807;&#21313;&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#20013;&#26356;&#20026;&#26126;&#26174;&#12290;&#23545;&#20110;&#26435;&#37325;&#37327;&#21270;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;FP4&#30340;&#24615;&#33021;&#19982;INT4&#30456;&#24403;&#65292;&#29978;&#33267;&#26356;&#20248;&#65292;&#31616;&#21270;&#20102;&#22312;&#20687;H100&#36825;&#26679;&#25903;&#25345;FP&#30340;&#30828;&#20214;&#19978;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#20943;&#23569;&#30001;&#26435;&#37325;&#21644;&#28608;&#27963;&#20043;&#38388;&#24046;&#24322;&#24341;&#36215;&#30340;&#31934;&#24230;&#23545;&#40784;&#24320;&#38144;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#32553;&#25918;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the complex domain of large language models (LLMs), striking a balance between computational efficiency and maintaining model quality is a formidable challenge. Navigating the inherent limitations of uniform quantization, particularly when dealing with outliers, and motivated by the launch of NVIDIA's H100 hardware, this study delves into the viability of floating-point (FP) quantization, particularly focusing on FP8 and FP4, as a potential solution. Our comprehensive investigation reveals that for LLMs, FP8 activation consistently outshines its integer (INT8) equivalent, with the performance edge becoming more noticeable in models possessing parameters beyond one billion. For weight quantization, our findings indicate that FP4 exhibits comparable, if not superior, performance to INT4, simplifying deployment on FP-supported hardware like H100. To mitigate the overhead from precision alignment caused by the disparity between weights and activations, we propose two scaling constraints
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20998;&#23618;&#22270;&#20687;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#21512;&#25104;&#24037;&#20316;&#27969;&#31243;&#21644;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#23618;&#36974;&#32617;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.09781</link><description>&lt;p&gt;
Text2Layer: &#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20998;&#23618;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text2Layer: Layered Image Generation using Latent Diffusion Model. (arXiv:2307.09781v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09781
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20998;&#23618;&#22270;&#20687;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#21512;&#25104;&#24037;&#20316;&#27969;&#31243;&#21644;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#23618;&#36974;&#32617;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23618;&#21512;&#25104;&#26159;&#19994;&#20313;&#29233;&#22909;&#32773;&#21644;&#19987;&#19994;&#20154;&#22763;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#22270;&#20687;&#32534;&#36753;&#24037;&#20316;&#27969;&#20043;&#19968;&#12290;&#21463;&#21040;&#25193;&#25955;&#27169;&#22411;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#20174;&#20998;&#23618;&#22270;&#20687;&#29983;&#25104;&#30340;&#35282;&#24230;&#25506;&#32034;&#22270;&#23618;&#21512;&#25104;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#29983;&#25104;&#32972;&#26223;&#12289;&#21069;&#26223;&#12289;&#22270;&#23618;&#36974;&#32617;&#21644;&#21512;&#25104;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#29983;&#25104;&#19968;&#24133;&#22270;&#20687;&#12290;&#20026;&#20102;&#23454;&#29616;&#20998;&#23618;&#22270;&#20687;&#29983;&#25104;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#37325;&#24314;&#20998;&#23618;&#22270;&#20687;&#65292;&#24182;&#22312;&#28508;&#22312;&#34920;&#31034;&#19978;&#35757;&#32451;&#20102;&#25193;&#25955;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#30340;&#19968;&#20010;&#22909;&#22788;&#26159;&#22312;&#39640;&#36136;&#37327;&#22270;&#20687;&#36755;&#20986;&#20043;&#22806;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#21512;&#25104;&#24037;&#20316;&#27969;&#31243;&#12290;&#21478;&#19968;&#20010;&#22909;&#22788;&#26159;&#29983;&#25104;&#27604;&#36890;&#36807;&#22270;&#20687;&#20998;&#21106;&#30340;&#29420;&#31435;&#27493;&#39588;&#20135;&#29983;&#30340;&#22270;&#23618;&#36974;&#32617;&#26356;&#39640;&#36136;&#37327;&#30340;&#22270;&#23618;&#36974;&#32617;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20998;&#23618;&#22270;&#20687;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#22880;&#23450;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Layer compositing is one of the most popular image editing workflows among both amateurs and professionals. Motivated by the success of diffusion models, we explore layer compositing from a layered image generation perspective. Instead of generating an image, we propose to generate background, foreground, layer mask, and the composed image simultaneously. To achieve layered image generation, we train an autoencoder that is able to reconstruct layered images and train diffusion models on the latent representation. One benefit of the proposed problem is to enable better compositing workflows in addition to the high-quality image output. Another benefit is producing higher-quality layer masks compared to masks produced by a separate step of image segmentation. Experimental results show that the proposed method is able to generate high-quality layered images and initiates a benchmark for future work.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ICECREAM&#30340;&#26032;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#21333;&#19968;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#32479;&#35745;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20449;&#24687;&#35770;&#37327;&#21270;&#34913;&#37327;&#20102;&#21464;&#37327;&#32852;&#21512;&#23545;&#30446;&#26631;&#21464;&#37327;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#33021;&#22815;&#30830;&#23450;&#19968;&#20010;&#29305;&#23450;&#32467;&#26524;&#25152;&#38656;&#30340;&#20851;&#38190;&#22240;&#32032;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2307.09779</link><description>&lt;p&gt;
&#36229;&#36234;&#21333;&#19968;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#26032;&#26041;&#27861;ICECREAM
&lt;/p&gt;
&lt;p&gt;
Beyond Single-Feature Importance with ICECREAM. (arXiv:2307.09779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09779
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ICECREAM&#30340;&#26032;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#21333;&#19968;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#32479;&#35745;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20449;&#24687;&#35770;&#37327;&#21270;&#34913;&#37327;&#20102;&#21464;&#37327;&#32852;&#21512;&#23545;&#30446;&#26631;&#21464;&#37327;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#33021;&#22815;&#30830;&#23450;&#19968;&#20010;&#29305;&#23450;&#32467;&#26524;&#25152;&#38656;&#30340;&#20851;&#38190;&#22240;&#32032;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36755;&#20986;&#30340;&#30830;&#23450;&#29305;&#24449;&#38598;&#21512;&#26159;&#21738;&#20123;&#65311;&#20113;&#35745;&#31639;&#24212;&#29992;&#22833;&#36133;&#30340;&#21407;&#22240;&#26159;&#21738;&#20123;&#32452;&#20214;&#65311;&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;ICECREAM&#65292;&#35299;&#31572;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#34913;&#37327;&#21464;&#37327;&#32852;&#21512;&#23545;&#30446;&#26631;&#21464;&#37327;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#35299;&#37322;&#24615;&#21644;&#22240;&#26524;&#36129;&#29486;&#20998;&#26512;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#30830;&#23450;&#19968;&#20010;&#29305;&#23450;&#32467;&#26524;&#25152;&#38656;&#30340;&#20851;&#38190;&#22240;&#32032;&#38598;&#21512;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#23545;&#20010;&#20307;&#22240;&#32032;&#36827;&#34892;&#37325;&#35201;&#24615;&#25490;&#24207;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ICECREAM&#22312;&#35299;&#37322;&#24615;&#21644;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Which set of features was responsible for a certain output of a machine learning model? Which components caused the failure of a cloud computing application? These are just two examples of questions we are addressing in this work by Identifying Coalition-based Explanations for Common and Rare Events in Any Model (ICECREAM). Specifically, we propose an information-theoretic quantitative measure for the influence of a coalition of variables on the distribution of a target variable. This allows us to identify which set of factors is essential to obtain a certain outcome, as opposed to well-established explainability and causal contribution analysis methods which can assign contributions only to individual factors and rank them by their importance. In experiments with synthetic and real-world data, we show that ICECREAM outperforms state-of-the-art methods for explainability and root cause analysis, and achieves impressive accuracy in both tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;(ST-VQC)&#29992;&#20110;&#22312;NISQ&#35774;&#22791;&#19978;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#65292;&#36890;&#36807;&#25972;&#21512;&#38750;&#32447;&#24615;&#29305;&#24449;&#25552;&#39640;&#23398;&#20064;&#27169;&#22411;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#29420;&#29305;&#30340;&#32534;&#30721;&#37327;&#23376;&#23376;&#30005;&#36335;&#21644;&#36880;&#23618;&#35745;&#31639;&#30340;&#37327;&#23376;&#23376;&#30005;&#36335;&#23454;&#29616;&#22522;&#20110;&#26102;&#38388;&#30340;&#28145;&#24230;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.09771</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#65292;&#29992;&#20110;&#22312;NISQ&#35774;&#22791;&#19978;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Novel Spatial-Temporal Variational Quantum Circuit to Enable Deep Learning on NISQ Devices. (arXiv:2307.09771v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09771
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;(ST-VQC)&#29992;&#20110;&#22312;NISQ&#35774;&#22791;&#19978;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#65292;&#36890;&#36807;&#25972;&#21512;&#38750;&#32447;&#24615;&#29305;&#24449;&#25552;&#39640;&#23398;&#20064;&#27169;&#22411;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#29420;&#29305;&#30340;&#32534;&#30721;&#37327;&#23376;&#23376;&#30005;&#36335;&#21644;&#36880;&#23618;&#35745;&#31639;&#30340;&#37327;&#23376;&#23376;&#30005;&#36335;&#23454;&#29616;&#22522;&#20110;&#26102;&#38388;&#30340;&#28145;&#24230;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#20197;&#20854;&#22312;&#39640;&#32500;&#24230;&#36890;&#36807;&#21472;&#21152;&#21644;&#32416;&#32544;&#36827;&#34892;&#26497;&#24182;&#34892;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#26377;&#28508;&#21147;&#65292;&#20294;&#29616;&#26377;&#30340;&#37327;&#23376;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#65288;VQC&#65289;&#65292;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#19981;&#21487;&#32447;&#24615;&#20998;&#31163;&#30340;&#25968;&#25454;&#38598;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#36935;&#21040;&#20102;&#21487;&#37096;&#32626;&#24615;&#38382;&#39064;&#65292;&#20351;&#24471;&#23558;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#21040;&#23454;&#38469;&#30340;&#37327;&#23376;&#35774;&#22791;&#21518;&#65292;&#20854;&#20934;&#30830;&#29575;&#22823;&#24133;&#19979;&#38477;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#31354;&#35774;&#35745;&#65292;&#21363;ST-VQC&#65292;&#20197;&#23558;&#38750;&#32447;&#24615;&#25972;&#21512;&#21040;&#37327;&#23376;&#23398;&#20064;&#20013;&#65292;&#24182;&#25552;&#39640;&#23398;&#20064;&#27169;&#22411;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ST-VQC&#21487;&#20197;&#36890;&#36807;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#22359;&#30340;&#32534;&#30721;&#37327;&#23376;&#23376;&#30005;&#36335;&#21644;&#36880;&#23618;&#35745;&#31639;&#30340;&#37327;&#23376;&#23376;&#30005;&#36335;&#26469;&#25552;&#21462;&#31354;&#38388;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#22522;&#20110;&#26102;&#38388;&#30340;&#28145;&#24230;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;SWAP&#29289;&#29702;&#30005;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum computing presents a promising approach for machine learning with its capability for extremely parallel computation in high-dimension through superposition and entanglement. Despite its potential, existing quantum learning algorithms, such as Variational Quantum Circuits(VQCs), face challenges in handling more complex datasets, particularly those that are not linearly separable. What's more, it encounters the deployability issue, making the learning models suffer a drastic accuracy drop after deploying them to the actual quantum devices. To overcome these limitations, this paper proposes a novel spatial-temporal design, namely ST-VQC, to integrate non-linearity in quantum learning and improve the robustness of the learning model to noise. Specifically, ST-VQC can extract spatial features via a novel block-based encoding quantum sub-circuit coupled with a layer-wise computation quantum sub-circuit to enable temporal-wise deep learning. Additionally, a SWAP-Free physical circuit 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#25955;&#22270;&#40654;&#26354;&#29575;&#22686;&#24378;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#25554;&#20837;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#21464;&#25442;&#20989;&#25968;&#30340;&#26354;&#29575;&#20449;&#24687;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#32531;&#35299;GNN&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#31561;&#35745;&#31639;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.09768</link><description>&lt;p&gt;
&#26354;&#29575;&#22914;&#20309;&#22686;&#24378;Framelet&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#36866;&#24212;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
How Curvature Enhance the Adaptation Power of Framelet GCNs. (arXiv:2307.09768v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#25955;&#22270;&#40654;&#26354;&#29575;&#22686;&#24378;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#25554;&#20837;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#21464;&#25442;&#20989;&#25968;&#30340;&#26354;&#29575;&#20449;&#24687;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#32531;&#35299;GNN&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#31561;&#35745;&#31639;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#24314;&#27169;&#22270;&#32467;&#26500;&#25968;&#25454;&#26041;&#38754;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23558;GNN&#24212;&#29992;&#20110;&#21508;&#31181;&#22270;&#20998;&#31867;&#21644;&#39044;&#27979;&#20219;&#21153;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#26696;&#20363;&#65292;&#20294;&#26159;&#21542;&#23436;&#20840;&#21033;&#29992;&#20102;&#22270;&#30340;&#20960;&#20309;&#20449;&#24687;&#26469;&#25552;&#39640;GNN&#30340;&#23398;&#20064;&#24615;&#33021;&#23578;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#36890;&#36807;&#31163;&#25955;&#22270;&#40654;&#26354;&#29575;&#24341;&#20837;&#20102;&#19968;&#31181;&#22686;&#24378;GNN&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23450;&#20041;&#22312;&#22270;&#36793;&#19978;&#30340;&#22270;&#40654;&#26354;&#29575;&#27979;&#37327;&#20102;&#20449;&#24687;&#20174;&#19968;&#20010;&#33410;&#28857;&#21040;&#21478;&#19968;&#20010;&#33410;&#28857;&#22312;&#20854;&#37051;&#22495;&#20013;&#30340;&#20256;&#36882;&#38590;&#24230;&#12290;&#21463;&#22270;&#40654;&#26354;&#29575;&#22312;&#22270;&#35774;&#32622;&#20013;&#30340;&#20960;&#20309;&#31867;&#27604;&#21551;&#21457;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#25554;&#20837;&#19981;&#21516;&#30340;&#12289;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#21464;&#25442;&#20989;&#25968;$\zeta$&#30340;&#26354;&#29575;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#32531;&#35299;GNN&#20013;&#20986;&#29616;&#30340;&#19968;&#20123;&#24050;&#30693;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#20363;&#22914;&#36807;&#24230;&#24179;&#28369;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#20855;&#26377;&#38750;&#24120;&#27491;&#30340;&#22270;&#40654;&#26354;&#29575;&#65288;&#21363;$\ka
&lt;/p&gt;
&lt;p&gt;
Graph neural network (GNN) has been demonstrated powerful in modeling graph-structured data. However, despite many successful cases of applying GNNs to various graph classification and prediction tasks, whether the graph geometrical information has been fully exploited to enhance the learning performance of GNNs is not yet well understood. This paper introduces a new approach to enhance GNN by discrete graph Ricci curvature. Specifically, the graph Ricci curvature defined on the edges of a graph measures how difficult the information transits on one edge from one node to another based on their neighborhoods. Motivated by the geometric analogy of Ricci curvature in the graph setting, we prove that by inserting the curvature information with different carefully designed transformation function $\zeta$, several known computational issues in GNN such as over-smoothing can be alleviated in our proposed model. Furthermore, we verified that edges with very positive Ricci curvature (i.e., $\ka
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#32447;&#24615;&#21464;&#25442;&#21644;&#31614;&#21517;&#21464;&#25442;&#20316;&#20026;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#65292;&#26082;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#24615;&#65292;&#21448;&#24341;&#20837;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#20984;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09767</link><description>&lt;p&gt;
Sig-Splines&#65306;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#36890;&#29992;&#36924;&#36817;&#21644;&#20984;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Sig-Splines: universal approximation and convex calibration of time series generative models. (arXiv:2307.09767v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09767
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#32447;&#24615;&#21464;&#25442;&#21644;&#31614;&#21517;&#21464;&#25442;&#20316;&#20026;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#65292;&#26082;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#24615;&#65292;&#21448;&#24341;&#20837;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#20984;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#21464;&#37327;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#12290;&#21463;&#31070;&#32463;&#26679;&#26465;&#27969;&#26500;&#36896;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;&#32447;&#24615;&#21464;&#25442;&#21644;&#31614;&#21517;&#21464;&#25442;&#20316;&#20026;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#32541;&#26367;&#20195;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#22266;&#26377;&#30340;&#36890;&#29992;&#24615;&#65292;&#36824;&#24341;&#20837;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#20984;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel generative model for multivariate discrete-time time series data. Drawing inspiration from the construction of neural spline flows, our algorithm incorporates linear transformations and the signature transform as a seamless substitution for traditional neural networks. This approach enables us to achieve not only the universality property inherent in neural networks but also introduces convexity in the model's parameters.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27169;&#24335;&#35782;&#21035;&#21644;&#38543;&#26426;&#28388;&#27874;&#29702;&#35770;&#30340;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#24378;&#21270;&#20102;&#22522;&#20110;POD&#30340;&#21453;&#24212;&#25193;&#25955;&#22797;&#26434;&#32593;&#32476;&#27169;&#22411;&#31616;&#21270;&#25216;&#26415;&#65292;&#22312;&#21463;&#25200;&#21160;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#20195;&#29702;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09762</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#28388;&#27874;&#21644;&#27169;&#24335;&#35782;&#21035;&#24378;&#21270;&#22522;&#20110;POD&#30340;&#21453;&#24212;&#25193;&#25955;&#22797;&#26434;&#32593;&#32476;&#27169;&#22411;&#31616;&#21270;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Reinforcing POD based model reduction techniques in reaction-diffusion complex networks using stochastic filtering and pattern recognition. (arXiv:2307.09762v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09762
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27169;&#24335;&#35782;&#21035;&#21644;&#38543;&#26426;&#28388;&#27874;&#29702;&#35770;&#30340;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#24378;&#21270;&#20102;&#22522;&#20110;POD&#30340;&#21453;&#24212;&#25193;&#25955;&#22797;&#26434;&#32593;&#32476;&#27169;&#22411;&#31616;&#21270;&#25216;&#26415;&#65292;&#22312;&#21463;&#25200;&#21160;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#20195;&#29702;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#32593;&#32476;&#34987;&#29992;&#20110;&#24314;&#27169;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#65292;&#28982;&#32780;&#36825;&#20123;&#31995;&#32479;&#30340;&#32500;&#24230;&#20351;&#24471;&#20854;&#20998;&#26512;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;POD&#31561;&#38477;&#32500;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#36755;&#20837;&#25968;&#25454;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#23558;&#27169;&#24335;&#35782;&#21035;&#21644;&#38543;&#26426;&#28388;&#27874;&#29702;&#35770;&#30340;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#22686;&#24378;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21463;&#25200;&#21160;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20195;&#29702;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#28982;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(ODEs)&#22312;&#29305;&#23450;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#26694;&#26550;&#19982;&#22522;&#20110;&#31070;&#32463;ODE&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex networks are used to model many real-world systems. However, the dimensionality of these systems can make them challenging to analyze. Dimensionality reduction techniques like POD can be used in such cases. However, these models are susceptible to perturbations in the input data. We propose an algorithmic framework that combines techniques from pattern recognition (PR) and stochastic filtering theory to enhance the output of such models. The results of our study show that our method can improve the accuracy of the surrogate model under perturbed inputs. Deep Neural Networks (DNNs) are susceptible to adversarial attacks. However, recent research has revealed that neural Ordinary Differential Equations (ODEs) exhibit robustness in specific applications. We benchmark our algorithmic framework with a Neural ODE-based approach as a reference.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39564;&#35777;&#20102;&#26497;&#38480;&#23398;&#20064;&#26426;&#65288;ELM&#65289;&#19981;&#23384;&#22312;&#35889;&#20559;&#24046;&#65288;SB&#65289;&#65292;&#24182;&#36890;&#36807;&#23454;&#26045;&#20613;&#31435;&#21494;&#29305;&#24449;&#23884;&#20837;&#30340;&#21464;&#20307;&#28040;&#38500;&#20102;&#35889;&#20559;&#24046;&#65292;&#24182;&#19988;&#25104;&#21151;&#23558;ELM&#24212;&#29992;&#20110;&#39640;&#39057;&#20998;&#36776;&#29575;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#65288;&#22914;PINNs&#65289;&#12290;</title><link>http://arxiv.org/abs/2307.09759</link><description>&lt;p&gt;
&#20351;&#29992;&#38646;&#39057;&#35889;&#20559;&#24046;&#26500;&#24314;&#26497;&#38480;&#23398;&#20064;&#26426;
&lt;/p&gt;
&lt;p&gt;
Constructing Extreme Learning Machines with zero Spectral Bias. (arXiv:2307.09759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39564;&#35777;&#20102;&#26497;&#38480;&#23398;&#20064;&#26426;&#65288;ELM&#65289;&#19981;&#23384;&#22312;&#35889;&#20559;&#24046;&#65288;SB&#65289;&#65292;&#24182;&#36890;&#36807;&#23454;&#26045;&#20613;&#31435;&#21494;&#29305;&#24449;&#23884;&#20837;&#30340;&#21464;&#20307;&#28040;&#38500;&#20102;&#35889;&#20559;&#24046;&#65292;&#24182;&#19988;&#25104;&#21151;&#23558;ELM&#24212;&#29992;&#20110;&#39640;&#39057;&#20998;&#36776;&#29575;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#65288;&#22914;PINNs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#35889;&#20559;&#24046;&#26159;&#19968;&#31181;&#22312;&#21069;&#39304;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#35266;&#23519;&#21040;&#30340;&#29616;&#35937;&#65292;&#21363;&#23398;&#20064;&#30340;&#20989;&#25968;&#30340;&#39640;&#39057;&#25104;&#20998;&#25910;&#25947;&#36895;&#24230;&#27604;&#20302;&#39057;&#25104;&#20998;&#24930;&#12290;&#36825;&#22312;&#21508;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#23545;&#20110;&#37027;&#20123;&#39640;&#39057;&#20998;&#36776;&#29575;&#33267;&#20851;&#37325;&#35201;&#30340;&#39046;&#22495;&#65292;&#22914;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#65292;&#36825;&#36896;&#25104;&#20102;&#25216;&#26415;&#25361;&#25112;&#12290;&#26497;&#38480;&#23398;&#20064;&#26426;&#65288;ELM&#65289;&#21487;&#20197;&#28040;&#38500;&#35299;&#31639;&#36807;&#31243;&#20013;&#20135;&#29983;&#35889;&#20559;&#24046;&#65288;SB&#65289;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#29702;&#35770;&#19978;&#24212;&#35813;&#27809;&#26377;&#35889;&#20559;&#24046;&#12290;&#26412;&#30740;&#31350;&#39564;&#35777;&#20102;&#36825;&#19968;&#20551;&#35774;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#35777;&#26126;&#23427;&#26159;&#19981;&#27491;&#30830;&#30340;&#12290;&#28982;&#32780;&#65292;ELM&#30340;&#32467;&#26500;&#20351;&#23427;&#20204;&#33258;&#28982;&#22320;&#36866;&#21512;&#20110;&#23454;&#26045;&#20613;&#31435;&#21494;&#29305;&#24449;&#23884;&#20837;&#30340;&#21464;&#20307;&#65292;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#20943;&#36731;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35889;&#20559;&#24046;&#12290;&#26412;&#30740;&#31350;&#23454;&#26045;&#24182;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#23436;&#20840;&#28040;&#38500;&#20102;&#35889;&#20559;&#24046;&#65292;&#22240;&#27492;&#20351;ELM&#22312;&#23454;&#38469;&#38382;&#39064;&#65288;&#22914;PINNs&#65289;&#30340;&#24212;&#29992;&#25104;&#20026;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
The phenomena of Spectral Bias, where the higher frequency components of a function being learnt in a feedforward Artificial Neural Network (ANN) are seen to converge more slowly than the lower frequencies, is observed ubiquitously across ANNs. This has created technology challenges in fields where resolution of higher frequencies is crucial, like in Physics Informed Neural Networks (PINNs). Extreme Learning Machines (ELMs) that obviate an iterative solution process which provides the theoretical basis of Spectral Bias (SB), should in principle be free of the same. This work verifies the reliability of this assumption, and shows that it is incorrect. However, the structure of ELMs makes them naturally amenable to implementation of variants of Fourier Feature Embeddings, which have been shown to mitigate SB in ANNs. This approach is implemented and verified to completely eliminate SB, thus bringing into feasibility the application of ELMs for practical problems like PINNs where resoluti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#26420;&#32032;&#20998;&#24067;&#21305;&#37197;&#30340;&#20004;&#20010;&#32570;&#28857;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#21644;&#26377;&#21069;&#26223;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2307.09742</link><description>&lt;p&gt;
&#25552;&#21319;&#25968;&#25454;&#38598;&#21387;&#32553;&#30340;&#20998;&#24067;&#21305;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improved Distribution Matching for Dataset Condensation. (arXiv:2307.09742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#26420;&#32032;&#20998;&#24067;&#21305;&#37197;&#30340;&#20004;&#20010;&#32570;&#28857;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#21644;&#26377;&#21069;&#26223;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#21387;&#32553;&#26088;&#22312;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#21387;&#32553;&#25104;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#35757;&#32451;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20943;&#23569;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#23384;&#20648;&#25104;&#26412;&#21644;&#35757;&#32451;&#24037;&#20316;&#37327;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#26159;&#20197;&#20248;&#21270;&#20026;&#23548;&#21521;&#30340;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#20248;&#21270;&#36807;&#31243;&#20013;&#25191;&#34892;&#26799;&#24230;&#25110;&#21442;&#25968;&#21305;&#37197;&#26469;&#21387;&#32553;&#25968;&#25454;&#38598;&#65292;&#21363;&#20351;&#22312;&#23567;&#22411;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#20063;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#26356;&#21152;&#39640;&#25928;&#21644;&#26377;&#21069;&#26223;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#38024;&#23545;&#26420;&#32032;&#20998;&#24067;&#21305;&#37197;&#30340;&#20004;&#20010;&#37325;&#35201;&#32570;&#28857;&#65288;&#21363;&#19981;&#24179;&#34913;&#30340;&#29305;&#24449;&#25968;&#37327;&#21644;&#26080;&#25928;&#30340;&#23884;&#20837;&#36317;&#31163;&#35745;&#31639;&#65289;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#25216;&#26415;&#65288;&#21363;&#20998;&#21306;&#21644;&#25193;&#23637;&#22686;&#24378;&#12289;&#39640;&#25928;&#21644;&#20016;&#23500;&#30340;&#27169;&#22411;&#37319;&#26679;&#21644;&#31867;&#21035;&#24863;&#30693;&#30340;&#20998;&#24067;&#27491;&#21017;&#21270;&#65289;&#12290;&#25105;&#20204;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#20248;&#20110;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset Condensation aims to condense a large dataset into a smaller one while maintaining its ability to train a well-performing model, thus reducing the storage cost and training effort in deep learning applications. However, conventional dataset condensation methods are optimization-oriented and condense the dataset by performing gradient or parameter matching during model optimization, which is computationally intensive even on small datasets and models. In this paper, we propose a novel dataset condensation method based on distribution matching, which is more efficient and promising. Specifically, we identify two important shortcomings of naive distribution matching (i.e., imbalanced feature numbers and unvalidated embeddings for distance computation) and address them with three novel techniques (i.e., partitioning and expansion augmentation, efficient and enriched model sampling, and class-aware distribution regularization). Our simple yet effective method outperforms most previo
&lt;/p&gt;</description></item><item><title>RaTE&#26159;&#19968;&#31181;&#26080;&#26631;&#31614;&#30340;&#33258;&#21160;&#20998;&#31867;&#35780;&#20998;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#21487;&#37325;&#22797;&#30340;&#33258;&#21160;&#20998;&#31867;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;RaTE&#19982;&#20154;&#31867;&#35780;&#21028;&#20855;&#26377;&#36739;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#20154;&#20026;&#38477;&#20302;&#20998;&#31867;&#27861;&#20250;&#23548;&#33268;RaTE&#35780;&#20998;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2307.09706</link><description>&lt;p&gt;
RaTE: &#36890;&#36807;&#22635;&#34917;&#31354;&#30333;&#23454;&#29616;&#21487;&#37325;&#22797;&#30340;&#33258;&#21160;&#20998;&#31867;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RaTE: a Reproducible automatic Taxonomy Evaluation by Filling the Gap. (arXiv:2307.09706v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09706
&lt;/p&gt;
&lt;p&gt;
RaTE&#26159;&#19968;&#31181;&#26080;&#26631;&#31614;&#30340;&#33258;&#21160;&#20998;&#31867;&#35780;&#20998;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#21487;&#37325;&#22797;&#30340;&#33258;&#21160;&#20998;&#31867;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;RaTE&#19982;&#20154;&#31867;&#35780;&#21028;&#20855;&#26377;&#36739;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#20154;&#20026;&#38477;&#20302;&#20998;&#31867;&#27861;&#20250;&#23548;&#33268;RaTE&#35780;&#20998;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#27861;&#23545;&#20110;&#30693;&#35782;&#34920;&#31034;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#33258;&#21160;&#20998;&#31867;&#26500;&#24314;&#30340;&#30740;&#31350;&#20173;&#28982;&#20381;&#36182;&#20110;&#20154;&#24037;&#35780;&#20272;&#26469;&#35780;&#20998;&#25552;&#20986;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#35748;&#20026;&#33258;&#21160;&#20998;&#31867;&#35780;&#20272;&#21644;&#20998;&#31867;&#26500;&#24314;&#19968;&#26679;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RaTE&#65292;&#19968;&#31181;&#26080;&#26631;&#31614;&#30340;&#33258;&#21160;&#20998;&#31867;&#35780;&#20998;&#26041;&#27861;&#65292;&#23427;&#20381;&#36182;&#20110;&#19968;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#35813;&#35780;&#20272;&#26041;&#27861;&#24212;&#29992;&#20110;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#33258;&#21160;&#20998;&#31867;&#26500;&#24314;&#31639;&#27861;&#65292;&#24182;&#20174;Yelp&#39046;&#22495;&#26500;&#24314;&#20102;&#19971;&#20010;&#20998;&#31867;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#65306;1&#65289;RaTE&#19982;&#20154;&#31867;&#35780;&#21028;&#30340;&#30456;&#20851;&#24615;&#36739;&#39640;&#65307;2&#65289;&#20154;&#20026;&#38477;&#20302;&#20998;&#31867;&#27861;&#20250;&#23548;&#33268;RaTE&#35780;&#20998;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Taxonomies are an essential knowledge representation, yet most studies on automatic taxonomy construction (ATC) resort to manual evaluation to score proposed algorithms. We argue that automatic taxonomy evaluation (ATE) is just as important as taxonomy construction. We propose RaTE, an automatic label-free taxonomy scoring procedure, which relies on a large pre-trained language model. We apply our evaluation procedure to three state-of-the-art ATC algorithms with which we built seven taxonomies from the Yelp domain, and show that 1) RaTE correlates well with human judgments and 2) artificially degrading a taxonomy leads to decreasing RaTE score.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.09702</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;LLM&#24341;&#23548;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient Guided Generation for LLMs. (arXiv:2307.09702v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#35760;&#24207;&#21015;&#29983;&#25104;&#36807;&#31243;&#20013;&#20960;&#20046;&#19981;&#22686;&#21152;&#20219;&#20309;&#24320;&#38144;&#65292;&#24182;&#20351;&#24471;&#24341;&#23548;&#29983;&#25104;&#22312;&#23454;&#38469;&#20013;&#21487;&#34892;&#12290;&#22312;&#24320;&#28304;Python&#24211;Outlines&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article we describe an efficient approach to guiding language model text generation with regular expressions and context-free grammars. Our approach adds little to no overhead to the token sequence generation process, and makes guided generation feasible in practice. An implementation is provided in the open source Python library Outlines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;STRAPPER&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#35757;&#32451;&#22686;&#24378;&#21644;&#21516;&#20276;&#27491;&#21017;&#21270;&#23454;&#29616;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#20316;&#32773;&#21457;&#29616;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30456;&#20284;&#24615;&#38519;&#38449;&#29616;&#35937;&#65292;&#21363;&#30456;&#20284;&#30340;&#29255;&#27573;&#23545;&#21487;&#33021;&#20250;&#23384;&#22312;&#25130;&#28982;&#30456;&#21453;&#30340;&#20559;&#22909;&#65292;&#23545;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#36896;&#25104;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.09692</link><description>&lt;p&gt;
STRAPPER&#65306;&#36890;&#36807;&#33258;&#35757;&#32451;&#22686;&#24378;&#21644;&#21516;&#20276;&#27491;&#21017;&#21270;&#23454;&#29616;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
STRAPPER: Preference-based Reinforcement Learning via Self-training Augmentation and Peer Regularization. (arXiv:2307.09692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;STRAPPER&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#35757;&#32451;&#22686;&#24378;&#21644;&#21516;&#20276;&#27491;&#21017;&#21270;&#23454;&#29616;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#20316;&#32773;&#21457;&#29616;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#30456;&#20284;&#24615;&#38519;&#38449;&#29616;&#35937;&#65292;&#21363;&#30456;&#20284;&#30340;&#29255;&#27573;&#23545;&#21487;&#33021;&#20250;&#23384;&#22312;&#25130;&#28982;&#30456;&#21453;&#30340;&#20559;&#22909;&#65292;&#23545;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#36896;&#25104;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;PbRL&#65289;&#25215;&#35834;&#36890;&#36807;&#20108;&#36827;&#21046;&#20154;&#31867;&#20559;&#22909;&#23398;&#20064;&#22797;&#26434;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20154;&#31867;&#21442;&#19982;&#30340;&#24418;&#24335;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#26469;&#20026;&#29255;&#27573;&#23545;&#20998;&#37197;&#20559;&#22909;&#26631;&#31614;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#20854;&#22823;&#35268;&#27169;&#24212;&#29992;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23581;&#35797;&#37325;&#22797;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#29255;&#27573;&#65292;&#38544;&#21547;&#22320;&#38416;&#26126;&#20102;&#29255;&#27573;&#30340;&#20998;&#24067;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#20154;&#20204;&#30340;&#21162;&#21147;&#12290;&#24182;&#19988;&#36827;&#19968;&#27493;&#32771;&#34385;&#20102;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26469;&#25552;&#39640;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#19982;&#26222;&#36890;&#30340;&#20998;&#31867;&#20219;&#21153;&#19981;&#21516;&#65292;PbRL&#20013;&#23384;&#22312;&#19968;&#20010;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#23450;&#20041;&#20026;&#30456;&#20284;&#24615;&#38519;&#38449;&#30340;&#29420;&#29305;&#29616;&#35937;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#20154;&#31867;&#23545;&#20110;&#30456;&#20284;&#30340;&#29255;&#27573;&#23545;&#21487;&#33021;&#20250;&#23384;&#22312;&#25130;&#28982;&#30456;&#21453;&#30340;&#20559;&#22909;&#65292;&#20294;&#36825;&#31181;&#30456;&#20284;&#24615;&#21487;&#33021;&#20250;&#23548;&#33268;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#22312;PbRL&#20013;&#22833;&#36133;&#12290;&#30001;&#20110;&#30456;&#20284;&#24615;&#38519;&#38449;&#30340;&#23384;&#22312;&#65292;&#36825;&#26679;&#30340;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#19981;&#36866;&#24403;&#22320;&#22686;&#24378;&#20102;&#27169;&#22411;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preference-based reinforcement learning (PbRL) promises to learn a complex reward function with binary human preference. However, such human-in-the-loop formulation requires considerable human effort to assign preference labels to segment pairs, hindering its large-scale applications. Recent approache has tried to reuse unlabeled segments, which implicitly elucidates the distribution of segments and thereby alleviates the human effort. And consistency regularization is further considered to improve the performance of semi-supervised learning. However, we notice that, unlike general classification tasks, in PbRL there exits a unique phenomenon that we defined as similarity trap in this paper. Intuitively, human can have diametrically opposite preferredness for similar segment pairs, but such similarity may trap consistency regularization fail in PbRL. Due to the existence of similarity trap, such consistency regularization improperly enhances the consistency possiblity of the model's pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;DRL&#30340;&#21452;&#26102;&#38388;&#23610;&#24230;&#26041;&#26696;&#65292;&#26088;&#22312;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26381;&#21153;&#32531;&#23384;&#12289;&#21327;&#20316;&#21368;&#36733;&#21644;&#35745;&#31639;&#36890;&#20449;&#36164;&#28304;&#20998;&#37197;&#26469;&#25552;&#39640;MEC&#31995;&#32479;&#20013;&#30340;&#38271;&#26399;&#26381;&#21153;&#36136;&#37327;&#24182;&#38477;&#20302;&#32531;&#23384;&#20999;&#25442;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.09691</link><description>&lt;p&gt;
&#22312;&#21327;&#20316;&#24335;MEC&#31995;&#32479;&#20013;&#30340;&#32852;&#21512;&#26381;&#21153;&#32531;&#23384;&#12289;&#36890;&#20449;&#21644;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#65306;&#22522;&#20110;DRL&#30340;&#21452;&#26102;&#38388;&#23610;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Joint Service Caching, Communication and Computing Resource Allocation in Collaborative MEC Systems: A DRL-based Two-timescale Approach. (arXiv:2307.09691v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09691
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;DRL&#30340;&#21452;&#26102;&#38388;&#23610;&#24230;&#26041;&#26696;&#65292;&#26088;&#22312;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26381;&#21153;&#32531;&#23384;&#12289;&#21327;&#20316;&#21368;&#36733;&#21644;&#35745;&#31639;&#36890;&#20449;&#36164;&#28304;&#20998;&#37197;&#26469;&#25552;&#39640;MEC&#31995;&#32479;&#20013;&#30340;&#38271;&#26399;&#26381;&#21153;&#36136;&#37327;&#24182;&#38477;&#20302;&#32531;&#23384;&#20999;&#25442;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22810;&#32500;&#36164;&#28304;&#30340;&#38480;&#21046;&#65292;&#28385;&#36275;&#32456;&#31471;&#30340;&#20005;&#26684;&#26381;&#21153;&#36136;&#37327;&#35201;&#27714;&#23545;&#22810;&#25509;&#20837;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#31995;&#32479;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21327;&#20316;&#24335;MEC&#26694;&#26550;&#65292;&#20419;&#36827;&#36793;&#32536;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36164;&#28304;&#20849;&#20139;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26381;&#21153;&#32531;&#23384;&#12289;&#21327;&#20316;&#21368;&#36733;&#12289;&#35745;&#31639;&#21644;&#36890;&#20449;&#36164;&#28304;&#20998;&#37197;&#26469;&#26368;&#22823;&#21270;&#38271;&#26399;&#30340;&#26381;&#21153;&#36136;&#37327;&#21644;&#38477;&#20302;&#32531;&#23384;&#20999;&#25442;&#25104;&#26412;&#12290;&#26381;&#21153;&#32531;&#23384;&#21644;&#20854;&#20182;&#36164;&#28304;&#20998;&#37197;&#20043;&#38388;&#30340;&#21452;&#26102;&#38388;&#23610;&#24230;&#29305;&#24615;&#21644;&#26102;&#38388;&#22238;&#24402;&#20851;&#31995;&#20351;&#35299;&#20915;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#21452;&#26102;&#38388;&#23610;&#24230;&#26041;&#26696;&#65292;&#31216;&#20026;DGL-DDPG&#65292;&#23427;&#30001;&#30701;&#26399;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#21644;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;LSTM-DDPG&#65289;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meeting the strict Quality of Service (QoS) requirements of terminals has imposed a signiffcant challenge on Multiaccess Edge Computing (MEC) systems, due to the limited multidimensional resources. To address this challenge, we propose a collaborative MEC framework that facilitates resource sharing between the edge servers, and with the aim to maximize the long-term QoS and reduce the cache switching cost through joint optimization of service caching, collaborative offfoading, and computation and communication resource allocation. The dual timescale feature and temporal recurrence relationship between service caching and other resource allocation make solving the problem even more challenging. To solve it, we propose a deep reinforcement learning (DRL)-based dual timescale scheme, called DGL-DDPG, which is composed of a short-term genetic algorithm (GA) and a long short-term memory network-based deep deterministic policy gradient (LSTM-DDPG). In doing so, we reformulate the optimizatio
&lt;/p&gt;</description></item><item><title>Amazon-M2&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#22810;&#21306;&#22495;&#36141;&#29289;&#20250;&#35805;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#22686;&#24378;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#29702;&#35299;&#29992;&#25143;&#20559;&#22909;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.09688</link><description>&lt;p&gt;
Amazon-M2: &#19968;&#20010;&#29992;&#20110;&#25512;&#33616;&#21644;&#25991;&#26412;&#29983;&#25104;&#30340;&#22810;&#35821;&#35328;&#22810;&#21306;&#22495;&#36141;&#29289;&#20250;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation. (arXiv:2307.09688v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09688
&lt;/p&gt;
&lt;p&gt;
Amazon-M2&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#22810;&#21306;&#22495;&#36141;&#29289;&#20250;&#35805;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#22686;&#24378;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#29702;&#35299;&#29992;&#25143;&#20559;&#22909;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30005;&#23376;&#21830;&#21153;&#26469;&#35828;&#65292;&#24314;&#27169;&#23458;&#25143;&#36141;&#29289;&#24847;&#22270;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#30452;&#25509;&#24433;&#21709;&#29992;&#25143;&#20307;&#39564;&#21644;&#21442;&#19982;&#24230;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#29702;&#35299;&#23458;&#25143;&#30340;&#20559;&#22909;&#23545;&#20110;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#25216;&#26415;&#21033;&#29992;&#23458;&#25143;&#20250;&#35805;&#25968;&#25454;&#26469;&#39044;&#27979;&#20182;&#20204;&#30340;&#19979;&#19968;&#27425;&#20114;&#21160;&#65292;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#21040;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20250;&#35805;&#25968;&#25454;&#38598;&#22312;&#39033;&#30446;&#23646;&#24615;&#12289;&#29992;&#25143;&#22810;&#26679;&#24615;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#19981;&#33021;&#20840;&#38754;&#22320;&#25429;&#25417;&#29992;&#25143;&#34892;&#20026;&#21644;&#20559;&#22909;&#30340;&#35889;&#31995;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Amazon Multilingual Multi-locale Shopping Session Dataset&#65292;&#21363;Amazon-M2&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#30001;&#26469;&#33258;&#20845;&#20010;&#19981;&#21516;&#21306;&#22495;&#30340;&#25968;&#30334;&#19975;&#29992;&#25143;&#20250;&#35805;&#32452;&#25104;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#20135;&#21697;&#30340;&#20027;&#35201;&#35821;&#35328;&#26159;&#33521;&#35821;&#12289;&#24503;&#35821;&#12289;&#26085;&#35821;&#12289;&#27861;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#22686;&#24378;&#20010;&#24615;&#21270;&#21644;&#29702;&#35299;&#29992;&#25143;&#20559;&#22909;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling customer shopping intentions is a crucial task for e-commerce, as it directly impacts user experience and engagement. Thus, accurately understanding customer preferences is essential for providing personalized recommendations. Session-based recommendation, which utilizes customer session data to predict their next interaction, has become increasingly popular. However, existing session datasets have limitations in terms of item attributes, user diversity, and dataset scale. As a result, they cannot comprehensively capture the spectrum of user behaviors and preferences. To bridge this gap, we present the Amazon Multilingual Multi-locale Shopping Session Dataset, namely Amazon-M2. It is the first multilingual dataset consisting of millions of user sessions from six different locales, where the major languages of products are English, German, Japanese, French, Italian, and Spanish. Remarkably, the dataset can help us enhance personalization and understanding of user preferences, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ReLU&#23618;&#22312;&#38381;&#29699;&#21644;&#38750;&#36127;&#37096;&#20998;&#19978;&#30340;&#21487;&#36870;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#20984;&#20960;&#20309;&#35270;&#35282;&#21644;&#26694;&#26550;&#29702;&#35770;&#20013;&#30340;&#23545;&#20598;&#27010;&#24565;&#26469;&#39564;&#35777;&#21644;&#37325;&#26500;ReLU&#23618;&#30340;&#35745;&#31639;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.09672</link><description>&lt;p&gt;
ReLU&#23618;&#30340;&#20984;&#20960;&#20309;&#12289;&#29699;&#19978;&#30340;&#21487;&#36870;&#24615;&#21644;&#23616;&#37096;&#37325;&#26500;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convex Geometry of ReLU-layers, Injectivity on the Ball and Local Reconstruction. (arXiv:2307.09672v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ReLU&#23618;&#22312;&#38381;&#29699;&#21644;&#38750;&#36127;&#37096;&#20998;&#19978;&#30340;&#21487;&#36870;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#20984;&#20960;&#20309;&#35270;&#35282;&#21644;&#26694;&#26550;&#29702;&#35770;&#20013;&#30340;&#23545;&#20598;&#27010;&#24565;&#26469;&#39564;&#35777;&#21644;&#37325;&#26500;ReLU&#23618;&#30340;&#35745;&#31639;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26694;&#26550;&#29702;&#35770;&#30340;&#26041;&#27861;&#30740;&#31350;&#20102;ReLU&#23618;&#22312;&#38381;&#29699;&#21644;&#38750;&#36127;&#37096;&#20998;&#19978;&#30340;&#21487;&#36870;&#24615;&#12290;&#29305;&#21035;&#24378;&#35843;&#20102;&#29699;&#30340;&#21322;&#24452;&#21644;&#20559;&#32622;&#21521;&#37327;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#32467;&#21512;&#20102;&#20984;&#20960;&#20309;&#30340;&#35270;&#35282;&#65292;&#24471;&#21040;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21512;&#29702;&#30340;&#32422;&#26463;&#26465;&#20214;&#19979;&#39564;&#35777;ReLU&#23618;&#30340;&#21487;&#36870;&#24615;&#65292;&#20854;&#20013;&#32422;&#26463;&#26465;&#20214;&#26159;&#20559;&#32622;&#21521;&#37327;&#30340;&#19978;&#30028;&#12290;&#25991;&#20013;&#36824;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#37325;&#26500;&#20844;&#24335;&#65292;&#28789;&#24863;&#26469;&#33258;&#26694;&#26550;&#29702;&#35770;&#20013;&#30340;&#23545;&#20598;&#27010;&#24565;&#12290;&#25152;&#26377;&#36825;&#20123;&#37117;&#20026;&#37327;&#21270;ReLU&#23618;&#30340;&#21487;&#36870;&#24615;&#20197;&#21450;&#29699;&#19978;&#20219;&#24847;&#36755;&#20837;&#21521;&#37327;&#30340;&#20855;&#20307;&#37325;&#26500;&#31639;&#27861;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper uses a frame-theoretic setting to study the injectivity of a ReLU-layer on the closed ball of $\mathbb{R}^n$ and its non-negative part. In particular, the interplay between the radius of the ball and the bias vector is emphasized. Together with a perspective from convex geometry, this leads to a computationally feasible method of verifying the injectivity of a ReLU-layer under reasonable restrictions in terms of an upper bound of the bias vector. Explicit reconstruction formulas are provided, inspired by the duality concept from frame theory. All this gives rise to the possibility of quantifying the invertibility of a ReLU-layer and a concrete reconstruction algorithm for any input vector on the ball.
&lt;/p&gt;</description></item><item><title>JAZZVAR&#26159;&#19968;&#20221;&#21253;&#21547;&#29237;&#22763;&#20048;&#26631;&#20934;&#26354;&#29420;&#22863;&#20013;&#21464;&#22863;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#38899;&#20048;&#37325;&#32472;&#20219;&#21153;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2307.09670</link><description>&lt;p&gt;
JAZZVAR&#65306;&#19968;&#20221;&#21253;&#21547;&#29237;&#22763;&#20048;&#26631;&#20934;&#26354;&#29420;&#22863;&#20013;&#21464;&#22863;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38899;&#20048;&#37325;&#32472;
&lt;/p&gt;
&lt;p&gt;
JAZZVAR: A Dataset of Variations found within Solo Piano Performances of Jazz Standards for Music Overpainting. (arXiv:2307.09670v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09670
&lt;/p&gt;
&lt;p&gt;
JAZZVAR&#26159;&#19968;&#20221;&#21253;&#21547;&#29237;&#22763;&#20048;&#26631;&#20934;&#26354;&#29420;&#22863;&#20013;&#21464;&#22863;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#38899;&#20048;&#37325;&#32472;&#20219;&#21153;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29237;&#22763;&#38050;&#29748;&#23478;&#24120;&#24120;&#20250;&#20197;&#29420;&#29305;&#30340;&#26041;&#24335;&#28436;&#32462;&#29237;&#22763;&#20048;&#26631;&#20934;&#26354;&#12290;&#36825;&#20123;&#28436;&#32462;&#20013;&#30340;&#29255;&#27573;&#21487;&#20197;&#34987;&#35270;&#20026;&#21464;&#22863;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#20174;&#29420;&#22863;&#29237;&#22763;&#38050;&#29748;&#28436;&#22863;&#20013;&#25163;&#21160;&#25552;&#21462;&#20102;&#36825;&#20123;&#21464;&#22863;&#12290;JAZZVAR&#25968;&#25454;&#38598;&#26159;&#19968;&#32452;502&#20010;&#21464;&#22863;&#21644;&#21407;&#22987;MIDI&#29255;&#27573;&#30340;&#38598;&#21512;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#21464;&#22863;&#37117;&#26377;&#19968;&#20010;&#30456;&#24212;&#30340;&#21407;&#22987;&#29255;&#27573;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#21407;&#22987;&#29237;&#22763;&#20048;&#26631;&#20934;&#26354;&#30340;&#26059;&#24459;&#21644;&#21644;&#24358;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21516;&#20110;&#35768;&#22810;&#29616;&#26377;&#30340;&#29237;&#22763;&#20048;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#19987;&#27880;&#20110;&#29237;&#22763;&#20048;&#28436;&#22863;&#20013;&#30340;&#21363;&#20852;&#28436;&#22863;&#37096;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#33719;&#21462;&#21644;&#25490;&#24207;&#26354;&#30446;&#30340;&#31574;&#21010;&#36807;&#31243;&#65292;&#21019;&#24314;&#21407;&#22987;&#29255;&#27573;&#21644;&#21464;&#22863;&#23545;&#30340;&#27969;&#31243;&#65292;&#20197;&#21450;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#29983;&#25104;&#38899;&#20048;&#20219;&#21153;&#8212;&#8212;&#38899;&#20048;&#37325;&#32472;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#22312;JAZZVAR&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22522;&#32447;Transformer&#27169;&#22411;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36824;&#26377;&#20854;&#20182;&#28508;&#22312;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jazz pianists often uniquely interpret jazz standards. Passages from these interpretations can be viewed as sections of variation. We manually extracted such variations from solo jazz piano performances. The JAZZVAR dataset is a collection of 502 pairs of Variation and Original MIDI segments. Each Variation in the dataset is accompanied by a corresponding Original segment containing the melody and chords from the original jazz standard. Our approach differs from many existing jazz datasets in the music information retrieval (MIR) community, which often focus on improvisation sections within jazz performances. In this paper, we outline the curation process for obtaining and sorting the repertoire, the pipeline for creating the Original and Variation pairs, and our analysis of the dataset. We also introduce a new generative music task, Music Overpainting, and present a baseline Transformer model trained on the JAZZVAR dataset for this task. Other potential applications of our dataset inc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20013;&#23884;&#20837;&#21644;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#20197;&#35821;&#35328;&#20026;&#26680;&#24515;&#25512;&#29702;&#24037;&#20855;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#29615;&#22659;&#20013;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#25506;&#32034;&#25928;&#29575;&#21644;&#25968;&#25454;&#22797;&#29992;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22797;&#29992;&#23398;&#21040;&#30340;&#25216;&#33021;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.09668</link><description>&lt;p&gt;
&#36808;&#21521;&#20855;&#26377;&#22522;&#30784;&#27169;&#22411;&#30340;&#32479;&#19968;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Towards A Unified Agent with Foundation Models. (arXiv:2307.09668v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20013;&#23884;&#20837;&#21644;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#20197;&#35821;&#35328;&#20026;&#26680;&#24515;&#25512;&#29702;&#24037;&#20855;&#30340;&#26694;&#26550;&#65292;&#24182;&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#29615;&#22659;&#20013;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#25506;&#32034;&#25928;&#29575;&#21644;&#25968;&#25454;&#22797;&#29992;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22797;&#29992;&#23398;&#21040;&#30340;&#25216;&#33021;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#20154;&#31867;&#24847;&#22270;&#12289;&#25512;&#29702;&#12289;&#22330;&#26223;&#29702;&#35299;&#21644;&#35268;&#21010;&#34892;&#20026;&#31561;&#26041;&#38754;&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#36825;&#20123;&#33021;&#21147;&#23884;&#20837;&#21644;&#21033;&#29992;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26234;&#33021;&#20307;&#20013;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20197;&#35821;&#35328;&#20316;&#20026;&#26680;&#24515;&#25512;&#29702;&#24037;&#20855;&#30340;&#26694;&#26550;&#65292;&#25506;&#32034;&#20102;&#36825;&#22914;&#20309;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#24212;&#23545;&#19968;&#31995;&#21015;&#22522;&#30784;RL&#25361;&#25112;&#65292;&#22914;&#39640;&#25928;&#25506;&#32034;&#12289;&#22797;&#29992;&#32463;&#39564;&#25968;&#25454;&#12289;&#35843;&#24230;&#25216;&#33021;&#21644;&#20174;&#35266;&#23519;&#20013;&#23398;&#20064;&#65292;&#36825;&#20123;&#20256;&#32479;&#19978;&#38656;&#35201;&#21333;&#29420;&#35774;&#35745;&#30340;&#22402;&#30452;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#31232;&#30095;&#22870;&#21169;&#30340;&#27169;&#25311;&#26426;&#22120;&#20154;&#25805;&#20316;&#29615;&#22659;&#20013;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#38656;&#35201;&#22534;&#21472;&#19968;&#32452;&#29289;&#20307;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25506;&#32034;&#25928;&#29575;&#21644;&#33021;&#22815;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#22797;&#29992;&#25968;&#25454;&#26041;&#38754;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22797;&#29992;&#23398;&#21040;&#30340;&#25216;&#33021;&#35299;&#20915;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models and Vision Language Models have recently demonstrated unprecedented capabilities in terms of understanding human intentions, reasoning, scene understanding, and planning-like behaviour, in text form, among many others. In this work, we investigate how to embed and leverage such abilities in Reinforcement Learning (RL) agents. We design a framework that uses language as the core reasoning tool, exploring how this enables an agent to tackle a series of fundamental RL challenges, such as efficient exploration, reusing experience data, scheduling skills, and learning from observations, which traditionally require separate, vertically designed algorithms. We test our method on a sparse-reward simulated robotic manipulation environment, where a robot needs to stack a set of objects. We demonstrate substantial performance improvements over baselines in exploration efficiency and ability to reuse data from offline datasets, and illustrate how to reuse learned skills to solve no
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#22270;&#36716;&#25442;&#22120;&#39044;&#27979;&#30740;&#31350;&#31038;&#21306;&#20013;&#30340;&#25216;&#26415;&#19987;&#38271;&#21644;&#33021;&#21147;&#28436;&#36827;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20840;&#29699;&#23433;&#20840;&#21644;&#26680;&#19981;&#25193;&#25955;&#31561;&#39046;&#22495;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.09665</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#22270;&#36716;&#25442;&#22120;&#39044;&#27979;&#30740;&#31350;&#31038;&#21306;&#20013;&#30340;&#25216;&#26415;&#19987;&#38271;&#21644;&#33021;&#21147;&#28436;&#36827;
&lt;/p&gt;
&lt;p&gt;
Anticipating Technical Expertise and Capability Evolution in Research Communities using Dynamic Graph Transformers. (arXiv:2307.09665v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09665
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#22270;&#36716;&#25442;&#22120;&#39044;&#27979;&#30740;&#31350;&#31038;&#21306;&#20013;&#30340;&#25216;&#26415;&#19987;&#38271;&#21644;&#33021;&#21147;&#28436;&#36827;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20840;&#29699;&#23433;&#20840;&#21644;&#26680;&#19981;&#25193;&#25955;&#31561;&#39046;&#22495;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#39044;&#27979;&#20840;&#29699;&#25216;&#26415;&#19987;&#38271;&#21644;&#33021;&#21147;&#28436;&#36827;&#36235;&#21183;&#23545;&#22269;&#23478;&#21644;&#20840;&#29699;&#23433;&#20840;&#38750;&#24120;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#26680;&#19981;&#25193;&#25955;&#65288;NN&#65289;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31561;&#24555;&#36895;&#20852;&#36215;&#30340;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;&#32479;&#35745;&#20851;&#31995;&#23398;&#20064;&#26041;&#27861;&#65288;&#20363;&#22914;&#65292;&#21327;&#20316;&#32593;&#32476;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#65289;&#65292;&#24182;&#21046;&#23450;&#20102;&#20351;&#29992;&#21160;&#24577;&#24322;&#26500;&#22270;&#34920;&#31034;&#26469;&#39044;&#27979;&#25216;&#26415;&#19987;&#38271;&#21644;&#33021;&#21147;&#28436;&#36827;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26032;&#30340;&#33021;&#21147;&#26469;&#39044;&#27979;&#19981;&#21516;&#31890;&#24230;&#65288;&#20363;&#22914;&#31185;&#23398;&#23478;&#21644;&#26426;&#26500;&#32423;&#21035;&#65289;&#30340;&#21327;&#20316;&#27169;&#24335;&#65292;&#20316;&#32773;&#34892;&#20026;&#21644;&#25216;&#26415;&#33021;&#21147;&#28436;&#36827;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#21160;&#24577;&#22270;&#36716;&#25442;&#22120;&#65288;DGT&#65289;&#31070;&#32463;&#26550;&#26500;&#65292;&#36890;&#36807;&#65288;a&#65289;&#39044;&#27979;&#24322;&#26500;&#65288;&#32780;&#19981;&#26159;&#21516;&#26500;&#65289;&#33410;&#28857;&#21644;&#36793;&#32536;&#65292;&#24182;&#65288;b&#65289;&#20381;&#36182;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#29305;&#24449;&#65292;&#25512;&#21160;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to anticipate technical expertise and capability evolution trends globally is essential for national and global security, especially in safety-critical domains like nuclear nonproliferation (NN) and rapidly emerging fields like artificial intelligence (AI). In this work, we extend traditional statistical relational learning approaches (e.g., link prediction in collaboration networks) and formulate a problem of anticipating technical expertise and capability evolution using dynamic heterogeneous graph representations. We develop novel capabilities to forecast collaboration patterns, authorship behavior, and technical capability evolution at different granularities (e.g., scientist and institution levels) in two distinct research fields. We implement a dynamic graph transformer (DGT) neural architecture, which pushes the state-of-the-art graph neural network models by (a) forecasting heterogeneous (rather than homogeneous) nodes and edges, and (b) relying on both discrete -- 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20943;&#38454;&#24314;&#27169;&#26041;&#27861;&#65292;&#31216;&#20026;BO-ML-ROM&#65292;&#29992;&#20110;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#21644;&#25104;&#26412;&#65292;&#24182;&#21516;&#26102;&#23454;&#29616;&#24341;&#23548;&#27874;&#20256;&#25773;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.09661</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#24341;&#23548;&#27874;&#20256;&#25773;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20943;&#38454;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Physics-based Reduced Order Modeling for Uncertainty Quantification of Guided Wave Propagation using Bayesian Optimization. (arXiv:2307.09661v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20943;&#38454;&#24314;&#27169;&#26041;&#27861;&#65292;&#31216;&#20026;BO-ML-ROM&#65292;&#29992;&#20110;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#21644;&#25104;&#26412;&#65292;&#24182;&#21516;&#26102;&#23454;&#29616;&#24341;&#23548;&#27874;&#20256;&#25773;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#23402;&#29983;&#32972;&#26223;&#19979;&#65292;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#65288;SHM&#65289;&#26500;&#25104;&#20102;&#22522;&#20110;&#29366;&#24577;&#30340;&#32500;&#25252;&#30340;&#25903;&#26609;&#65292;&#20419;&#36827;&#20102;&#34394;&#25311;&#21644;&#29289;&#29702;&#36164;&#20135;&#20043;&#38388;&#30340;&#20114;&#36830;&#12290;&#24341;&#23548;&#27874;&#20256;&#25773;&#65288;GWP&#65289;&#36890;&#24120;&#29992;&#20110;SHM&#20013;&#30340;&#32467;&#26500;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;GWP&#23545;&#32467;&#26500;&#26448;&#26009;&#24615;&#36136;&#30340;&#21464;&#21270;&#38750;&#24120;&#25935;&#24863;&#65292;&#23481;&#26131;&#23548;&#33268;&#38169;&#35823;&#21578;&#35686;&#12290;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#65292;&#32463;&#24120;&#24212;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#12290;&#35745;&#31639;&#21147;&#23398;&#26159;&#27169;&#25311;GWP&#30340;&#26377;&#29992;&#24037;&#20855;&#65292;&#36890;&#24120;&#29992;&#20110;UQ&#12290;&#21363;&#20415;&#22914;&#27492;&#65292;UQ&#26041;&#27861;&#30340;&#24212;&#29992;&#38656;&#35201;&#22823;&#37327;&#30340;&#27169;&#25311;&#65292;&#32780;&#22823;&#35268;&#27169;&#30340;&#30636;&#24577;&#25968;&#20540;GWP&#35299;&#20915;&#26041;&#26696;&#20250;&#22686;&#21152;&#35745;&#31639;&#24320;&#38144;&#12290;&#24120;&#29992;&#30340;&#20943;&#38454;&#27169;&#22411;&#65288;ROMs&#65289;&#36890;&#24120;&#29992;&#20110;&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#20869;&#25552;&#20379;&#25968;&#20540;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;ROM&#65292;&#21363;BO-ML-ROM&#65292;&#20197;&#20943;&#23569;&#19982;&#35745;&#31639;&#26102;&#38388;&#30456;&#20851;&#30340;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
In the context of digital twins, structural health monitoring (SHM) constitutes the backbone of condition-based maintenance, facilitating the interconnection between virtual and physical assets. Guided wave propagation (GWP) is commonly employed for the inspection of structures in SHM. However, GWP is sensitive to variations in the material properties of the structure, leading to false alarms. In this direction, uncertainty quantification (UQ) is regularly applied to improve the reliability of predictions. Computational mechanics is a useful tool for the simulation of GWP, and is often applied for UQ. Even so, the application of UQ methods requires numerous simulations, while large-scale, transient numerical GWP solutions increase the computational cost. Reduced order models (ROMs) are commonly employed to provide numerical results in a limited amount of time. In this paper, we propose a machine learning (ML)-based ROM, mentioned as BO-ML-ROM, to decrease the computational time related
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#20248;&#20808;&#38431;&#21015;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#24494;&#20998;&#30340;&#27169;&#22359;&#65292;&#24182;&#36890;&#36807;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#20854;&#22312;&#31639;&#27861;&#25512;&#29702;&#21644;&#25429;&#25417;&#38271;&#36317;&#31163;&#20132;&#20114;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09660</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#20248;&#20808;&#38431;&#21015;
&lt;/p&gt;
&lt;p&gt;
Neural Priority Queues for Graph Neural Networks. (arXiv:2307.09660v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#20248;&#20808;&#38431;&#21015;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#24494;&#20998;&#30340;&#27169;&#22359;&#65292;&#24182;&#36890;&#36807;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#20854;&#22312;&#31639;&#27861;&#25512;&#29702;&#21644;&#25429;&#25417;&#38271;&#36317;&#31163;&#20132;&#20114;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#20013;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#25104;&#21151;&#12290;&#35768;&#22810;&#20256;&#32479;&#31639;&#27861;&#20351;&#29992;&#26174;&#24335;&#20869;&#23384;&#26469;&#34920;&#31034;&#25968;&#25454;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23558;GNN&#19982;&#22806;&#37096;&#20869;&#23384;&#30456;&#32467;&#21512;&#30340;&#25506;&#32034;&#36824;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#20248;&#20808;&#38431;&#21015;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#31639;&#27861;&#20248;&#20808;&#38431;&#21015;&#65292;&#29992;&#20110;GNN&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#25512;&#21160;&#20102;&#19968;&#31181;&#20869;&#23384;&#27169;&#22359;&#30340;&#26399;&#26395;&#65292;&#35777;&#26126;&#20102;&#31070;&#32463;&#20248;&#20808;&#38431;&#21015;&#28385;&#36275;&#35813;&#26399;&#26395;&#65292;&#24182;&#36890;&#36807;&#23545;&#31639;&#27861;&#25512;&#29702;&#30340;&#29702;&#35299;&#26469;&#35299;&#37322;&#20854;&#20351;&#29992;&#12290;&#36890;&#36807;&#22312;CLRS-30&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#31070;&#32463;&#20248;&#20808;&#38431;&#21015;&#22312;&#25429;&#25417;&#38271;&#36317;&#31163;&#20132;&#20114;&#26041;&#38754;&#38750;&#24120;&#26377;&#29992;&#65292;&#36825;&#22312;Long-Range Graph Benchmark&#30340;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#23454;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have shown considerable success in neural algorithmic reasoning. Many traditional algorithms make use of an explicit memory in the form of a data structure. However, there has been limited exploration on augmenting GNNs with external memory. In this paper, we present Neural Priority Queues, a differentiable analogue to algorithmic priority queues, for GNNs. We propose and motivate a desiderata for memory modules, and show that Neural PQs exhibit the desiderata, and reason about their use with algorithmic reasoning. This is further demonstrated by empirical results on the CLRS-30 dataset. Furthermore, we find the Neural PQs useful in capturing long-range interactions, as empirically shown on a dataset from the Long-Range Graph Benchmark.
&lt;/p&gt;</description></item><item><title>HAT-CL&#26159;&#19968;&#20010;&#22522;&#20110;&#20219;&#21153;&#30340;&#30828;&#27880;&#24847;&#21147;PyTorch&#24211;&#65292;&#20197;&#25552;&#20379;&#23545;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#36890;&#36807;&#25913;&#21892;HAT&#30340;&#21487;&#29992;&#24615;&#21644;&#20860;&#23481;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#23545;&#29616;&#26377;&#32593;&#32476;&#22797;&#29992;&#30340;&#25903;&#25345;&#65292;&#23454;&#29616;&#20102;&#23545;PyTorch&#27169;&#22359;&#30340;&#33258;&#21160;&#21270;&#26799;&#24230;&#25805;&#20316;&#21644;&#36716;&#25442;&#12290;&#27492;&#22806;&#65292;HAT-CL&#36824;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#25513;&#30721;&#25805;&#20316;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2307.09653</link><description>&lt;p&gt;
HAT-CL: &#29992;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#22522;&#20110;&#20219;&#21153;&#30340;&#30828;&#27880;&#24847;&#21147;PyTorch&#24211;
&lt;/p&gt;
&lt;p&gt;
HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual Learning. (arXiv:2307.09653v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09653
&lt;/p&gt;
&lt;p&gt;
HAT-CL&#26159;&#19968;&#20010;&#22522;&#20110;&#20219;&#21153;&#30340;&#30828;&#27880;&#24847;&#21147;PyTorch&#24211;&#65292;&#20197;&#25552;&#20379;&#23545;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#36890;&#36807;&#25913;&#21892;HAT&#30340;&#21487;&#29992;&#24615;&#21644;&#20860;&#23481;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#23545;&#29616;&#26377;&#32593;&#32476;&#22797;&#29992;&#30340;&#25903;&#25345;&#65292;&#23454;&#29616;&#20102;&#23545;PyTorch&#27169;&#22359;&#30340;&#33258;&#21160;&#21270;&#26799;&#24230;&#25805;&#20316;&#21644;&#36716;&#25442;&#12290;&#27492;&#22806;&#65292;HAT-CL&#36824;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#25513;&#30721;&#25805;&#20316;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#21363;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#20007;&#22833;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#32473;&#20154;&#20204;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#30828;&#27880;&#24847;&#21147;&#20219;&#21153;(HAT)&#26426;&#21046;&#22312;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20854;&#23454;&#38469;&#23454;&#29616;&#21463;&#21040;&#20102;&#21487;&#29992;&#24615;&#21644;&#20860;&#23481;&#24615;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#32570;&#20047;&#23545;&#29616;&#26377;&#32593;&#32476;&#22797;&#29992;&#30340;&#25903;&#25345;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;HAT-CL&#65292;&#36825;&#26159;HAT&#26426;&#21046;&#30340;&#29992;&#25143;&#21451;&#22909;&#12289;&#19982;PyTorch&#20860;&#23481;&#30340;&#37325;&#26032;&#35774;&#35745;&#12290;HAT-CL&#19981;&#20165;&#33258;&#21160;&#21270;&#20102;&#26799;&#24230;&#25805;&#20316;&#65292;&#36824;&#31616;&#21270;&#20102;PyTorch&#27169;&#22359;&#36716;&#21270;&#20026;HAT&#27169;&#22359;&#30340;&#36807;&#31243;&#12290;&#23427;&#36890;&#36807;&#25552;&#20379;&#19968;&#22871;&#20840;&#38754;&#30340;&#27169;&#22359;&#65292;&#21487;&#20197;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#26550;&#26500;&#20013;&#12290;&#27492;&#22806;&#65292;HAT-CL&#36824;&#25552;&#20379;&#20102;&#19982;TIMM&#24211;&#24179;&#28369;&#38598;&#25104;&#30340;&#21487;&#29992;&#30340;HAT&#32593;&#32476;&#12290;&#38500;&#20102;&#23545;HAT&#30340;&#37325;&#26032;&#35774;&#35745;&#21644;&#37325;&#26032;&#23454;&#29616;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#29992;&#20110;HAT&#30340;&#26032;&#39062;&#30340;&#25513;&#30721;&#25805;&#20316;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Catastrophic forgetting, the phenomenon in which a neural network loses previously obtained knowledge during the learning of new tasks, poses a significant challenge in continual learning. The Hard-Attention-to-the-Task (HAT) mechanism has shown potential in mitigating this problem, but its practical implementation has been complicated by issues of usability and compatibility, and a lack of support for existing network reuse. In this paper, we introduce HAT-CL, a user-friendly, PyTorch-compatible redesign of the HAT mechanism. HAT-CL not only automates gradient manipulation but also streamlines the transformation of PyTorch modules into HAT modules. It achieves this by providing a comprehensive suite of modules that can be seamlessly integrated into existing architectures. Additionally, HAT-CL offers ready-to-use HAT networks that are smoothly integrated with the TIMM library. Beyond the redesign and reimplementation of HAT, we also introduce novel mask manipulation techniques for HAT,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;BadNets&#35774;&#35745;&#20102;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#65292;&#25581;&#31034;&#20102;&#22403;&#22334;&#37038;&#20214;&#36807;&#28388;&#22120;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#28508;&#22312;&#28431;&#27934;&#65292;&#24378;&#35843;&#20102;&#35780;&#20272;&#21644;&#25913;&#36827;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09649</link><description>&lt;p&gt;
BadNets&#22312;&#22403;&#22334;&#37038;&#20214;&#36807;&#28388;&#22120;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of BadNets in Spam Filters. (arXiv:2307.09649v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;BadNets&#35774;&#35745;&#20102;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#65292;&#25581;&#31034;&#20102;&#22403;&#22334;&#37038;&#20214;&#36807;&#28388;&#22120;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#28508;&#22312;&#28431;&#27934;&#65292;&#24378;&#35843;&#20102;&#35780;&#20272;&#21644;&#25913;&#36827;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22403;&#22334;&#37038;&#20214;&#36807;&#28388;&#22120;&#26159;&#29616;&#20195;&#30005;&#23376;&#37038;&#20214;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#24110;&#21161;&#29992;&#25143;&#38450;&#27490;&#21463;&#21040;&#19981;&#38656;&#35201;&#21644;&#28508;&#22312;&#26377;&#23475;&#30340;&#37038;&#20214;&#30340;&#20405;&#25200;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36807;&#28388;&#22120;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#39537;&#21160;&#23427;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22312;&#22403;&#22334;&#37038;&#20214;&#36807;&#28388;&#39046;&#22495;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;&#36890;&#36807;&#23637;&#31034;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20379;&#24212;&#38142;&#20013;&#30340;&#28508;&#22312;&#28431;&#27934;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#23545;&#22403;&#22334;&#37038;&#20214;&#36807;&#28388;&#22120;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#36827;&#34892;&#20180;&#32454;&#32771;&#34385;&#21644;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21518;&#38376;&#25915;&#20987;&#33021;&#22815;&#26377;&#25928;&#22320;&#29992;&#20110;&#35782;&#21035;&#22403;&#22334;&#37038;&#20214;&#36807;&#28388;&#22120;&#20013;&#30340;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#35813;&#39046;&#22495;&#36827;&#34892;&#25345;&#32493;&#30417;&#25511;&#21644;&#25913;&#36827;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spam filters are a crucial component of modern email systems, as they help to protect users from unwanted and potentially harmful emails. However, the effectiveness of these filters is dependent on the quality of the machine learning models that power them. In this paper, we design backdoor attacks in the domain of spam filtering. By demonstrating the potential vulnerabilities in the machine learning model supply chain, we highlight the need for careful consideration and evaluation of the models used in spam filters. Our results show that the backdoor attacks can be effectively used to identify vulnerabilities in spam filters and suggest the need for ongoing monitoring and improvement in this area.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#22411;Adam&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20851;&#38190;&#21160;&#37327;&#39033;&#30340;&#32531;&#20914;&#21306;&#26469;&#20419;&#36827;&#23545;&#26356;&#24179;&#22374;&#26368;&#23567;&#20540;&#30340;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26631;&#20934;&#30340;&#30417;&#30563;&#35821;&#35328;&#24314;&#27169;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#20960;&#31181;Adam&#21464;&#20307;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09638</link><description>&lt;p&gt;
&#36890;&#36807;&#20851;&#38190;&#38454;&#27573;&#20419;&#36827;&#35760;&#24518;&#22686;&#24378;&#22411;Adam&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Promoting Exploration in Memory-Augmented Adam using Critical Momenta. (arXiv:2307.09638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#22686;&#24378;&#22411;Adam&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20851;&#38190;&#21160;&#37327;&#39033;&#30340;&#32531;&#20914;&#21306;&#26469;&#20419;&#36827;&#23545;&#26356;&#24179;&#22374;&#26368;&#23567;&#20540;&#30340;&#25506;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26631;&#20934;&#30340;&#30417;&#30563;&#35821;&#35328;&#24314;&#27169;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#20960;&#31181;Adam&#21464;&#20307;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26799;&#24230;&#20248;&#21270;&#22120;&#65292;&#29305;&#21035;&#26159;Adam&#65292;&#22312;&#35757;&#32451;&#22823;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#36825;&#31181;&#20248;&#21270;&#22120;&#30340;&#20248;&#21183;&#22312;&#20110;&#20854;&#24555;&#36895;&#25910;&#25947;&#24615;&#65292;&#21516;&#26102;&#23545;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#26356;&#21152;&#40065;&#26834;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#27604;&#38750;&#33258;&#36866;&#24212;&#26041;&#27861;&#27867;&#21270;&#25928;&#26524;&#26356;&#24046;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#36825;&#31181;&#24615;&#33021;&#24046;&#36317;&#24402;&#22240;&#20110;&#36873;&#25321;&#24179;&#22374;&#26368;&#23567;&#20540;&#65306;&#33258;&#36866;&#24212;&#26041;&#27861;&#20542;&#21521;&#20110;&#22312;&#25439;&#22833;&#20989;&#25968;&#26354;&#38754;&#20013;&#26356;&#23574;&#38160;&#30340;&#30406;&#22320;&#20013;&#23547;&#25214;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35760;&#24518;&#22686;&#24378;&#22411;Adam&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20851;&#38190;&#21160;&#37327;&#39033;&#30340;&#32531;&#20914;&#21306;&#26469;&#20419;&#36827;&#23545;&#26356;&#24179;&#22374;&#26368;&#23567;&#20540;&#30340;&#25506;&#32034;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#32531;&#20914;&#21306;&#30340;&#20351;&#29992;&#20351;&#24471;&#20248;&#21270;&#22120;&#22914;&#26524;&#30406;&#22320;&#30340;&#21560;&#24341;&#33539;&#22260;&#19981;&#22815;&#23485;&#65292;&#23601;&#20250;&#36229;&#20986;&#20854;&#33539;&#22260;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#30340;&#30417;&#30563;&#35821;&#35328;&#24314;&#27169;&#21644;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#20960;&#31181;Adam&#21464;&#20307;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive gradient-based optimizers, particularly Adam, have left their mark in training large-scale deep learning models. The strength of such optimizers is that they exhibit fast convergence while being more robust to hyperparameter choice. However, they often generalize worse than non-adaptive methods. Recent studies have tied this performance gap to flat minima selection: adaptive methods tend to find solutions in sharper basins of the loss landscape, which in turn hurts generalization. To overcome this issue, we propose a new memory-augmented version of Adam that promotes exploration towards flatter minima by using a buffer of critical momentum terms during training. Intuitively, the use of the buffer makes the optimizer overshoot outside the basin of attraction if it is not wide enough. We empirically show that our method improves the performance of several variants of Adam on standard supervised language modelling and image classification tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;ESG&#37329;&#34701;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#25353;&#20844;&#21496;ESG&#35780;&#20998;&#35843;&#25972;&#22238;&#25253;&#30340;&#24066;&#22330;&#20013;&#65292;DRL&#20195;&#29702;&#30340;&#34920;&#29616;&#20248;&#20110;&#26631;&#20934;&#24066;&#22330;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2307.09631</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;ESG&#37329;&#34701;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for ESG financial portfolio management. (arXiv:2307.09631v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;ESG&#37329;&#34701;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#25353;&#20844;&#21496;ESG&#35780;&#20998;&#35843;&#25972;&#22238;&#25253;&#30340;&#24066;&#22330;&#20013;&#65292;DRL&#20195;&#29702;&#30340;&#34920;&#29616;&#20248;&#20110;&#26631;&#20934;&#24066;&#22330;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#29615;&#22659;&#12289;&#31038;&#20250;&#21644;&#27835;&#29702;&#65288;ESG&#65289;&#37329;&#34701;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#22522;&#20110;ESG&#35780;&#20998;&#30340;&#24066;&#22330;&#30417;&#31649;&#30340;&#28508;&#22312;&#30410;&#22788;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;Advantage Actor-Critic&#65288;A2C&#65289;&#20195;&#29702;&#65292;&#24182;&#20351;&#29992;OpenAI Gym&#20013;&#30340;&#29615;&#22659;&#36827;&#34892;&#23454;&#39564;&#65292;&#36825;&#20123;&#29615;&#22659;&#26159;&#20174;FinRL&#24179;&#21488;&#25913;&#32534;&#30340;&#12290;&#35813;&#30740;&#31350;&#21253;&#25324;&#23545;DRL&#20195;&#29702;&#22312;&#26631;&#20934;&#36947;&#29756;&#26031;&#24037;&#19994;&#24179;&#22343;&#25351;&#25968;&#65288;DJIA&#65289;&#24066;&#22330;&#26465;&#20214;&#21644;&#25353;&#20844;&#21496;ESG&#35780;&#20998;&#35843;&#25972;&#22238;&#25253;&#30340;&#24773;&#26223;&#19979;&#30340;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#12290;&#22312;ESG&#35843;&#25511;&#24066;&#22330;&#20013;&#65292;&#26681;&#25454;&#25237;&#36164;&#32452;&#21512;&#30340;&#22238;&#25253;&#21644;ESG&#35780;&#20998;&#25353;&#27604;&#20363;&#20998;&#37197;&#36164;&#37329;&#65292;&#32780;&#23545;&#20110;&#20302;&#20110;&#25351;&#25968;&#24179;&#22343;ESG&#35780;&#20998;&#30340;&#25237;&#36164;&#32452;&#21512;&#21017;&#24449;&#25910;&#31246;&#27454;&#12290;&#32467;&#26524;&#20196;&#20154;&#24778;&#35766;&#22320;&#34920;&#26126;&#65292;&#22312;ESG&#35843;&#25511;&#24066;&#22330;&#20013;&#65292;DRL&#20195;&#29702;&#30340;&#34920;&#29616;&#20248;&#20110;&#26631;&#20934;DJIA&#24066;&#22330;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23558;ESG&#21464;&#37327;&#32435;&#20837;&#27169;&#22411;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the application of Deep Reinforcement Learning (DRL) for Environment, Social, and Governance (ESG) financial portfolio management, with a specific focus on the potential benefits of ESG score-based market regulation. We leveraged an Advantage Actor-Critic (A2C) agent and conducted our experiments using environments encoded within the OpenAI Gym, adapted from the FinRL platform. The study includes a comparative analysis of DRL agent performance under standard Dow Jones Industrial Average (DJIA) market conditions and a scenario where returns are regulated in line with company ESG scores. In the ESG-regulated market, grants were proportionally allotted to portfolios based on their returns and ESG scores, while taxes were assigned to portfolios below the mean ESG score of the index. The results intriguingly reveal that the DRL agent within the ESG-regulated market outperforms the standard DJIA market setup. Furthermore, we considered the inclusion of ESG variables i
&lt;/p&gt;</description></item><item><title>Dataset Grouper&#26159;&#19968;&#20010;&#24211;&#65292;&#29992;&#20110;&#21019;&#24314;&#22823;&#35268;&#27169;&#32676;&#32452;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#65292;&#24182;&#20811;&#26381;&#20102;&#20869;&#23384;&#38480;&#21046;&#12289;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#19982;&#19981;&#21516;&#30340;&#36719;&#20214;&#26694;&#26550;&#20860;&#23481;&#12290;&#23454;&#39564;&#35777;&#26126;&#23427;&#21487;&#20197;&#23454;&#29616;&#27604;&#20197;&#21069;&#26356;&#22823;&#35268;&#27169;&#30340;&#32852;&#37030;&#35821;&#35328;&#24314;&#27169;&#27169;&#25311;&#12290;</title><link>http://arxiv.org/abs/2307.09619</link><description>&lt;p&gt;
&#38754;&#21521;&#32852;&#37030;&#22522;&#30784;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#25968;&#25454;&#38598;&#27969;&#27700;&#32447;&#65306;&#29992;&#20110;&#32676;&#32452;&#32467;&#26500;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning. (arXiv:2307.09619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09619
&lt;/p&gt;
&lt;p&gt;
Dataset Grouper&#26159;&#19968;&#20010;&#24211;&#65292;&#29992;&#20110;&#21019;&#24314;&#22823;&#35268;&#27169;&#32676;&#32452;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#65292;&#24182;&#20811;&#26381;&#20102;&#20869;&#23384;&#38480;&#21046;&#12289;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#19982;&#19981;&#21516;&#30340;&#36719;&#20214;&#26694;&#26550;&#20860;&#23481;&#12290;&#23454;&#39564;&#35777;&#26126;&#23427;&#21487;&#20197;&#23454;&#29616;&#27604;&#20197;&#21069;&#26356;&#22823;&#35268;&#27169;&#30340;&#32852;&#37030;&#35821;&#35328;&#24314;&#27169;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Dataset Grouper&#30340;&#24211;&#65292;&#29992;&#20110;&#21019;&#24314;&#22823;&#35268;&#27169;&#30340;&#32676;&#32452;&#32467;&#26500;&#21270;&#65288;&#20363;&#22914;&#32852;&#37030;&#65289;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#22522;&#30784;&#27169;&#22411;&#35268;&#27169;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#25311;&#12290;&#35813;&#24211;&#20801;&#35768;&#26681;&#25454;&#29992;&#25143;&#25351;&#23450;&#30340;&#20998;&#21306;&#21019;&#24314;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#32676;&#32452;&#32467;&#26500;&#21270;&#29256;&#26412;&#65292;&#24182;&#30452;&#25509;&#23548;&#33268;&#21508;&#31181;&#26377;&#29992;&#30340;&#24322;&#26500;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#25554;&#20837;&#29616;&#26377;&#30340;&#36719;&#20214;&#26694;&#26550;&#20013;&#12290;Dataset Grouper&#20855;&#26377;&#19977;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#33021;&#22815;&#36866;&#24212;&#21363;&#20351;&#21333;&#20010;&#32676;&#32452;&#30340;&#25968;&#25454;&#38598;&#20063;&#22826;&#22823;&#26080;&#27861;&#25918;&#20837;&#20869;&#23384;&#30340;&#24773;&#20917;&#12290;&#20854;&#27425;&#65292;&#23427;&#25552;&#20379;&#20102;&#28789;&#27963;&#24615;&#65292;&#26082;&#21487;&#20197;&#36873;&#25321;&#22522;&#30784;&#65288;&#38750;&#20998;&#21306;&#65289;&#25968;&#25454;&#38598;&#65292;&#20063;&#21487;&#20197;&#23450;&#20041;&#20998;&#21306;&#12290;&#26368;&#21518;&#65292;&#23427;&#19982;&#26694;&#26550;&#26080;&#20851;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;Dataset Grouper&#20801;&#35768;&#22312;&#27604;&#20808;&#21069;&#24037;&#20316;&#20013;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#32852;&#37030;&#35821;&#35328;&#24314;&#27169;&#27169;&#25311;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20687;FedAvg&#36825;&#26679;&#30340;&#31639;&#27861;&#26356;&#20687;&#20803;&#23398;&#20064;&#26041;&#27861;&#32780;&#19981;&#26159;&#32463;&#39564;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a library, Dataset Grouper, to create large-scale group-structured (e.g., federated) datasets, enabling federated learning simulation at the scale of foundation models. This library allows the creation of group-structured versions of existing datasets based on user-specified partitions, and directly leads to a variety of useful heterogeneous datasets that can be plugged into existing software frameworks. Dataset Grouper offers three key advantages. First, it scales to settings where even a single group's dataset is too large to fit in memory. Second, it provides flexibility, both in choosing the base (non-partitioned) dataset and in defining partitions. Finally, it is framework-agnostic. We empirically demonstrate that Dataset Grouper allows for large-scale federated language modeling simulations on datasets that are orders of magnitude larger than in previous work. Our experimental results show that algorithms like FedAvg operate more as meta-learning methods than as empi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#31070;&#32463;&#24433;&#20687;&#23398;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26131;&#35299;&#37322;&#24615;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#36817;&#24180;&#26469;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22914;&#20309;&#29702;&#35299;&#27169;&#22411;&#20915;&#31574;&#30340;&#30452;&#35273;&#65292;&#36824;&#38656;&#25506;&#32034;&#22914;&#20309;&#39564;&#35777;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09615</link><description>&lt;p&gt;
&#28145;&#20837;&#25506;&#31350;&#21487;&#35299;&#37322;&#24615;&#28145;&#24230;&#23398;&#20064;&#22312;&#31070;&#32463;&#24433;&#20687;&#23398;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Looking deeper into interpretable deep learning in neuroimaging: a comprehensive survey. (arXiv:2307.09615v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#31070;&#32463;&#24433;&#20687;&#23398;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26131;&#35299;&#37322;&#24615;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#36817;&#24180;&#26469;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22914;&#20309;&#29702;&#35299;&#27169;&#22411;&#20915;&#31574;&#30340;&#30452;&#35273;&#65292;&#36824;&#38656;&#25506;&#32034;&#22914;&#20309;&#39564;&#35777;&#35299;&#37322;&#24615;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#22240;&#20854;&#33021;&#22815;&#30452;&#25509;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#36827;&#34892;&#31471;&#21040;&#31471;&#23398;&#20064;&#65292;&#20943;&#36731;&#20102;&#23545;&#38169;&#35823;&#26131;&#21457;&#29305;&#24449;&#25552;&#21462;&#38454;&#27573;&#30340;&#25285;&#24551;&#32780;&#22791;&#21463;&#38738;&#30544;&#12290;&#26368;&#36817;&#30340;DL&#22522;&#20110;&#31070;&#32463;&#24433;&#20687;&#23398;&#30340;&#30740;&#31350;&#20063;&#22312;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#26080;&#27861;&#25104;&#21151;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#12290;&#26368;&#36817;&#20960;&#24180;&#65292;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#32463;&#21382;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#20027;&#35201;&#29992;&#20110;&#25581;&#31034;&#27169;&#22411;&#22914;&#20309;&#20316;&#20986;&#20915;&#31574;&#30340;&#30452;&#35273;&#65292;&#36825;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#37329;&#34701;&#21644;&#25191;&#27861;&#26426;&#26500;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#35299;&#37322;&#24615;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#30740;&#31350;&#20154;&#21592;&#20173;&#19981;&#28165;&#26970;&#21518;&#26399;&#26041;&#27861;&#25581;&#31034;&#20102;&#27169;&#22411;&#23398;&#20064;&#30340;&#21738;&#20010;&#26041;&#38754;&#20197;&#21450;&#22914;&#20309;&#39564;&#35777;&#20854;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#32508;&#21512;&#35780;&#36848;&#20102;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#31070;&#32463;&#24433;&#20687;&#23398;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) models have been popular due to their ability to learn directly from the raw data in an end-to-end paradigm, alleviating the concern of a separate error-prone feature extraction phase. Recent DL-based neuroimaging studies have also witnessed a noticeable performance advancement over traditional machine learning algorithms. But the challenges of deep learning models still exist because of the lack of transparency in these models for their successful deployment in real-world applications. In recent years, Explainable AI (XAI) has undergone a surge of developments mainly to get intuitions of how the models reached the decisions, which is essential for safety-critical domains such as healthcare, finance, and law enforcement agencies. While the interpretability domain is advancing noticeably, researchers are still unclear about what aspect of model learning a post hoc method reveals and how to validate its reliability. This paper comprehensively reviews interpretable deep
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#21464;&#37327;&#36890;&#36947;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#32467;&#21512;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;TS2Vec&#25439;&#22833;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#35774;&#32622;&#19979;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.09614</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#22810;&#21464;&#37327;&#36890;&#36947;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Multi-view self-supervised learning for multivariate variable-channel time series. (arXiv:2307.09614v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#21464;&#37327;&#36890;&#36947;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#32467;&#21512;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;TS2Vec&#25439;&#22833;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#35774;&#32622;&#19979;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22810;&#21464;&#37327;&#29983;&#29289;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#26159;&#19968;&#39033;&#32321;&#37325;&#21644;&#26114;&#36149;&#30340;&#20219;&#21153;&#12290;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#36890;&#36807;&#23545;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#26469;&#20943;&#23569;&#23545;&#22823;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#36755;&#20837;&#36890;&#36947;&#30340;&#38598;&#21512;&#22312;&#19981;&#21516;&#24212;&#29992;&#20043;&#38388;&#36890;&#24120;&#20250;&#26377;&#25152;&#21464;&#21270;&#65292;&#32780;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#24182;&#19981;&#20801;&#35768;&#22312;&#20855;&#26377;&#19981;&#21516;&#36755;&#20837;&#36890;&#36947;&#38598;&#21512;&#30340;&#25968;&#25454;&#38598;&#20043;&#38388;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#19968;&#31181;&#32534;&#30721;&#22120;&#26469;&#20998;&#21035;&#22788;&#29702;&#25152;&#26377;&#36755;&#20837;&#36890;&#36947;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#22312;&#36890;&#36947;&#20043;&#38388;&#25552;&#21462;&#21333;&#19968;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19968;&#20010;&#20855;&#26377;&#20845;&#20010;&#33041;&#30005;&#22270;&#36890;&#36947;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#19968;&#20010;&#20855;&#26377;&#20004;&#20010;&#19981;&#21516;&#33041;&#30005;&#22270;&#36890;&#36947;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26469;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20855;&#26377;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#19981;&#20855;&#26377;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#22312;&#19981;&#21516;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#19979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;TS2Vec&#25439;&#22833;&#22312;&#22823;&#22810;&#25968;&#35774;&#32622;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#25152;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labeling of multivariate biomedical time series data is a laborious and expensive process. Self-supervised contrastive learning alleviates the need for large, labeled datasets through pretraining on unlabeled data. However, for multivariate time series data the set of input channels often varies between applications, and most existing work does not allow for transfer between datasets with different sets of input channels. We propose learning one encoder to operate on all input channels individually. We then use a message passing neural network to extract a single representation across channels. We demonstrate the potential of this method by pretraining our network on a dataset with six EEG channels and finetuning on a dataset with two different EEG channels. We compare networks with and without the message passing neural network across different contrastive loss functions. We show that our method combined with the TS2Vec loss outperforms all other methods in most settings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;NeuroSeqRet&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#26816;&#32034;&#36830;&#32493;&#26102;&#38388;&#20107;&#20214;&#24207;&#21015;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#21704;&#24076;&#21644;&#31070;&#32463;&#26102;&#38388;&#28857;&#36807;&#31243;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20026;&#36755;&#20837;&#30340;&#26597;&#35810;&#24207;&#21015;&#36820;&#22238;&#19968;&#20010;&#30456;&#20851;&#24207;&#21015;&#30340;&#25490;&#24207;&#21015;&#34920;&#12290;</title><link>http://arxiv.org/abs/2307.09613</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#21704;&#24076;&#30340;&#31070;&#32463;&#26102;&#38388;&#28857;&#36807;&#31243;&#26816;&#32034;&#36830;&#32493;&#26102;&#38388;&#20107;&#20214;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Retrieving Continuous Time Event Sequences using Neural Temporal Point Processes with Learnable Hashing. (arXiv:2307.09613v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09613
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;NeuroSeqRet&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#26816;&#32034;&#36830;&#32493;&#26102;&#38388;&#20107;&#20214;&#24207;&#21015;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#21704;&#24076;&#21644;&#31070;&#32463;&#26102;&#38388;&#28857;&#36807;&#31243;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20026;&#36755;&#20837;&#30340;&#26597;&#35810;&#24207;&#21015;&#36820;&#22238;&#19968;&#20010;&#30456;&#20851;&#24207;&#21015;&#30340;&#25490;&#24207;&#21015;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24050;&#32463;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#21464;&#24471;&#26222;&#36941;&#12290;&#22240;&#27492;&#65292;&#20197;&#36830;&#32493;&#26102;&#38388;&#20107;&#20214;&#24207;&#21015;&#65288;CTES&#65289;&#24418;&#24335;&#20135;&#29983;&#30340;&#25968;&#25454;&#37327;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#38024;&#23545;CTES&#25968;&#25454;&#38598;&#30340;&#24403;&#21069;&#30740;&#31350;&#30340;&#37325;&#35201;&#37096;&#20998;&#26159;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#19979;&#19968;&#20010;&#20107;&#20214;&#39044;&#27979;&#12289;&#38271;&#26399;&#39044;&#27979;&#12289;&#24207;&#21015;&#20998;&#31867;&#31561;&#65289;&#30340;&#27169;&#22411;&#12290;&#20351;&#29992;&#26631;&#35760;&#30340;&#26102;&#38388;&#28857;&#36807;&#31243;&#65288;MTPP&#65289;&#36827;&#34892;&#39044;&#27979;&#24314;&#27169;&#30340;&#26368;&#26032;&#21457;&#23637;&#20351;&#24471;&#33021;&#22815;&#20934;&#30830;&#22320;&#23545;&#28041;&#21450;CTES&#30340;&#20960;&#20010;&#23454;&#38469;&#24212;&#29992;&#36827;&#34892;&#34920;&#24449;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;CTES&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#24615;&#65292;&#36807;&#21435;&#30340;&#25991;&#29486;&#24573;&#35270;&#20102;&#22823;&#35268;&#27169;&#26816;&#32034;&#26102;&#38388;&#24207;&#21015;&#30340;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;CTES&#26816;&#32034;&#65292;&#25105;&#20204;&#25351;&#30340;&#26159;&#23545;&#20110;&#19968;&#20010;&#36755;&#20837;&#26597;&#35810;&#24207;&#21015;&#65292;&#26816;&#32034;&#31995;&#32479;&#24517;&#39035;&#20174;&#19968;&#20010;&#22823;&#30340;&#35821;&#26009;&#24211;&#20013;&#36820;&#22238;&#19968;&#20010;&#30456;&#20851;&#24207;&#21015;&#30340;&#25490;&#24207;&#21015;&#34920;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NeuroSeqRet&#65292;&#36825;&#26159;&#19968;&#20010;&#20856;&#22411;&#30340;
&lt;/p&gt;
&lt;p&gt;
Temporal sequences have become pervasive in various real-world applications. Consequently, the volume of data generated in the form of continuous time-event sequence(s) or CTES(s) has increased exponentially in the past few years. Thus, a significant fraction of the ongoing research on CTES datasets involves designing models to address downstream tasks such as next-event prediction, long-term forecasting, sequence classification etc. The recent developments in predictive modeling using marked temporal point processes (MTPP) have enabled an accurate characterization of several real-world applications involving the CTESs. However, due to the complex nature of these CTES datasets, the task of large-scale retrieval of temporal sequences has been overlooked by the past literature. In detail, by CTES retrieval we mean that for an input query sequence, a retrieval system must return a ranked list of relevant sequences from a large corpus. To tackle this, we propose NeuroSeqRet, a first-of-its
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20934;&#30830;&#27169;&#22411;&#12290;&#22312;&#23454;&#39564;&#20013;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#24555;&#30340;&#36816;&#34892;&#36895;&#24230;&#24182;&#33021;&#22815;&#21457;&#29616;&#21512;&#29702;&#30340;&#27169;&#22411;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2307.09607</link><description>&lt;p&gt;
&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#23398;&#20064;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#32467;&#26500;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Sequential Monte Carlo Learning for Time Series Structure Discovery. (arXiv:2307.09607v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20934;&#30830;&#27169;&#22411;&#12290;&#22312;&#23454;&#39564;&#20013;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#24555;&#30340;&#36816;&#34892;&#36895;&#24230;&#24182;&#33021;&#22815;&#21457;&#29616;&#21512;&#29702;&#30340;&#27169;&#22411;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21457;&#29616;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20934;&#30830;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#39640;&#26031;&#36807;&#31243;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#31526;&#21495;&#31354;&#38388;&#19978;&#24037;&#20316;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#20808;&#39564;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#65288;SMC&#65289;&#21644;&#26059;&#25442;MCMC&#30340;&#26032;&#22411;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#21518;&#39564;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#8220;&#22312;&#32447;&#8221;&#35774;&#32622;&#20013;&#20351;&#29992;&#65292;&#20854;&#20013;&#26032;&#25968;&#25454;&#39034;&#24207;&#22320;&#21512;&#24182;&#22312;&#26102;&#38388;&#20013;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#8220;&#31163;&#32447;&#8221;&#35774;&#32622;&#20013;&#20351;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#30340;&#23884;&#22871;&#23376;&#38598;&#23545;&#21518;&#39564;&#36827;&#34892;&#36864;&#28779;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#30340;&#23454;&#39564;&#27979;&#37327;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#20043;&#21069;&#38024;&#23545;&#30456;&#21516;&#27169;&#22411;&#26063;&#30340;MCMC&#21644;&#36138;&#24515;&#25628;&#32034;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#25552;&#20379;10&#20493;&#33267;100&#20493;&#30340;&#36816;&#34892;&#26102;&#38388;&#21152;&#36895;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;1,428&#20010;&#35745;&#37327;&#32463;&#27982;&#25968;&#25454;&#38598;&#30340;&#30693;&#21517;&#22522;&#20934;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#30340;&#39640;&#26031;&#36807;&#31243;&#26102;&#38388;&#24207;&#21015;&#32467;&#26500;&#23398;&#20064;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21457;&#29616;&#21512;&#29702;&#30340;&#27169;&#22411;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new approach to automatically discovering accurate models of complex time series data. Working within a Bayesian nonparametric prior over a symbolic space of Gaussian process time series models, we present a novel structure learning algorithm that integrates sequential Monte Carlo (SMC) and involutive MCMC for highly effective posterior inference. Our method can be used both in "online" settings, where new data is incorporated sequentially in time, and in "offline" settings, by using nested subsets of historical data to anneal the posterior. Empirical measurements on real-world time series show that our method can deliver 10x--100x runtime speedups over previous MCMC and greedy-search structure learning algorithms targeting the same model family. We use our method to perform the first large-scale evaluation of Gaussian process time series structure learning on a prominent benchmark of 1,428 econometric datasets. The results show that our method discovers sensible 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;&#26679;&#26465;&#34920;&#31034;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#19981;&#20877;&#38656;&#35201;&#20984;&#22810;&#36793;&#24418;&#21644;&#20998;&#27573;&#32447;&#24615;&#32593;&#32476;&#25805;&#20316;&#31526;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#25972;&#20010;&#32593;&#32476;&#19978;&#25191;&#34892;&#12290;&#36825;&#39033;&#24037;&#20316;&#19981;&#20165;&#22635;&#34917;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#36924;&#36817;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#36824;&#20351;&#24471;&#32593;&#32476;&#29305;&#24449;&#22270;&#30340;&#21487;&#35270;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09602</link><description>&lt;p&gt;
&#20351;&#29992;&#20984;&#20985;&#34920;&#31034;&#30340;Legendre&#21464;&#25442;&#23558;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;max-affine&#26679;&#26465;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
A max-affine spline approximation of neural networks using the Legendre transform of a convex-concave representation. (arXiv:2307.09602v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09602
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;&#26679;&#26465;&#34920;&#31034;&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#19981;&#20877;&#38656;&#35201;&#20984;&#22810;&#36793;&#24418;&#21644;&#20998;&#27573;&#32447;&#24615;&#32593;&#32476;&#25805;&#20316;&#31526;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#25972;&#20010;&#32593;&#32476;&#19978;&#25191;&#34892;&#12290;&#36825;&#39033;&#24037;&#20316;&#19981;&#20165;&#22635;&#34917;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#36924;&#36817;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#36824;&#20351;&#24471;&#32593;&#32476;&#29305;&#24449;&#22270;&#30340;&#21487;&#35270;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;&#26679;&#26465;&#34920;&#31034;&#30340;&#26032;&#31639;&#27861;&#12290;&#19982;&#20197;&#21069;&#38656;&#35201;&#20984;&#22810;&#36793;&#24418;&#21644;&#20998;&#27573;&#32447;&#24615;&#32593;&#32476;&#25805;&#20316;&#31526;&#26469;&#21019;&#24314;max-affine&#26679;&#26465;&#24418;&#24335;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#26412;&#25991;&#25918;&#23485;&#20102;&#36825;&#20010;&#32422;&#26463;&#12290;&#21807;&#19968;&#30340;&#32422;&#26463;&#26159;&#20989;&#25968;&#24212;&#35813;&#26159;&#26377;&#30028;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;&#26126;&#30830;&#23450;&#20041;&#30340;&#20108;&#38454;&#23548;&#25968;&#65292;&#23613;&#31649;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#36825;&#24182;&#19981;&#26159;&#20005;&#26684;&#24517;&#38656;&#30340;&#12290;&#36825;&#31181;&#26041;&#27861;&#20063;&#21487;&#20197;&#22312;&#25972;&#20010;&#32593;&#32476;&#19978;&#25191;&#34892;&#65292;&#32780;&#19981;&#26159;&#22312;&#27599;&#20010;&#23618;&#19978;&#29420;&#31435;&#25191;&#34892;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19968;&#26679;&#65292;&#36825;&#22635;&#34917;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#36924;&#36817;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#21516;&#26102;&#20063;&#23454;&#29616;&#20102;&#32593;&#32476;&#29305;&#24449;&#22270;&#30340;&#21487;&#35270;&#21270;&#12290;&#36890;&#36807;&#20174;&#19968;&#31995;&#21015;&#26550;&#26500;&#20013;&#25552;&#21462;&#36924;&#36817;&#35823;&#24046;&#21644;&#29305;&#24449;&#22270;&#36827;&#34892;&#25968;&#23398;&#35777;&#26126;&#21644;&#23454;&#39564;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a novel algorithm for transforming a neural network into a spline representation. Unlike previous work that required convex and piecewise-affine network operators to create a max-affine spline alternate form, this work relaxes this constraint. The only constraint is that the function be bounded and possess a well-define second derivative, although this was shown experimentally to not be strictly necessary. It can also be performed over the whole network rather than on each layer independently. As in previous work, this bridges the gap between neural networks and approximation theory but also enables the visualisation of network feature maps. Mathematical proof and experimental investigation of the technique is performed with approximation error and feature maps being extracted from a range of architectures, including convolutional neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#23646;&#24615;&#22270;&#20855;&#26377;&#19981;&#21516;&#30340;&#39640;&#39057;&#20869;&#23481;&#65292;&#28388;&#38500;&#39640;&#39057;&#29575;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09591</link><description>&lt;p&gt;
&#26799;&#24230;&#21453;&#20987;&#65306;&#22914;&#20309;&#28388;&#38500;&#39640;&#39057;&#29575;&#25552;&#39640;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Gradient strikes back: How filtering out high frequencies improves explanations. (arXiv:2307.09591v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#23646;&#24615;&#22270;&#20855;&#26377;&#19981;&#21516;&#30340;&#39640;&#39057;&#20869;&#23481;&#65292;&#28388;&#38500;&#39640;&#39057;&#29575;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26032;&#22411;&#22522;&#20110;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#30340;&#21457;&#23637;&#36805;&#29467;&#65292;&#36880;&#28176;&#21462;&#20195;&#20102;&#26087;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#22411;&#26041;&#27861;&#20026;&#20309;&#20248;&#20110;&#26799;&#24230;&#22411;&#26041;&#27861;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#20174;&#32463;&#39564;&#35266;&#23519;&#24320;&#22987;&#65306;&#36825;&#20004;&#31181;&#26041;&#27861;&#20135;&#29983;&#30340;&#23646;&#24615;&#22270;&#20855;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#21151;&#29575;&#35889;&#65292;&#26799;&#24230;&#22411;&#26041;&#27861;&#25581;&#31034;&#20102;&#27604;&#39044;&#27979;&#22411;&#26041;&#27861;&#26356;&#22810;&#30340;&#39640;&#39057;&#20869;&#23481;&#12290;&#36825;&#19968;&#35266;&#23519;&#24341;&#21457;&#20102;&#22810;&#20010;&#38382;&#39064;&#65306;&#36825;&#31181;&#39640;&#39057;&#20449;&#24687;&#30340;&#26469;&#28304;&#26159;&#20160;&#20040;&#65292;&#23427;&#26159;&#21542;&#30495;&#27491;&#21453;&#26144;&#20102;&#31995;&#32479;&#25152;&#20316;&#20986;&#30340;&#20915;&#31574;&#65311;&#26368;&#21518;&#65292;&#20026;&#20160;&#20040;&#22312;&#22810;&#20010;&#35780;&#20215;&#25351;&#26631;&#19979;&#65292;&#39044;&#27979;&#22411;&#26041;&#27861;&#20013;&#32570;&#20047;&#39640;&#39057;&#20449;&#24687;&#23558;&#20135;&#29983;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#20998;&#25968;&#65311;&#25105;&#20204;&#20998;&#26512;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;&#35270;&#35273;&#20998;&#31867;&#27169;&#22411;&#30340;&#26799;&#24230;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#21253;&#21547;&#26469;&#33258;&#39640;&#39057;&#30340;&#22122;&#22768;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an explosion in the development of novel prediction-based attribution methods, which have slowly been supplanting older gradient-based methods to explain the decisions of deep neural networks. However, it is still not clear why prediction-based methods outperform gradient-based ones. Here, we start with an empirical observation: these two approaches yield attribution maps with very different power spectra, with gradient-based methods revealing more high-frequency content than prediction-based methods. This observation raises multiple questions: What is the source of this high-frequency information, and does it truly reflect decisions made by the system? Lastly, why would the absence of high-frequency information in prediction-based methods yield better explainability scores along multiple metrics? We analyze the gradient of three representative visual classification models and observe that it contains noisy information emanating from high-frequencies. Furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23398;&#20064;&#32593;&#32476;&#20013;&#20195;&#29702;&#20043;&#38388;&#30340;&#22240;&#26524;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#35780;&#20272;&#25972;&#20307;&#24433;&#21709;&#21147;&#21644;&#21457;&#29616;&#39640;&#24230;&#26377;&#24433;&#21709;&#21147;&#30340;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2307.09575</link><description>&lt;p&gt;
&#31038;&#20132;&#23398;&#20064;&#32593;&#32476;&#20013;&#30340;&#22240;&#26524;&#24433;&#21709;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Causal Influences over Social Learning Networks. (arXiv:2307.09575v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23398;&#20064;&#32593;&#32476;&#20013;&#20195;&#29702;&#20043;&#38388;&#30340;&#22240;&#26524;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#35780;&#20272;&#25972;&#20307;&#24433;&#21709;&#21147;&#21644;&#21457;&#29616;&#39640;&#24230;&#26377;&#24433;&#21709;&#21147;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30456;&#20114;&#36830;&#25509;&#19988;&#32463;&#36807;&#26102;&#38388;&#20132;&#20114;&#30340;&#20195;&#29702;&#20043;&#38388;&#30340;&#22240;&#26524;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#35770;&#25991;&#32771;&#23519;&#20102;&#31038;&#20132;&#23398;&#20064;&#27169;&#22411;&#21644;&#20998;&#24067;&#24335;&#20915;&#31574;&#21327;&#35758;&#30340;&#21160;&#24577;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#34920;&#26126;&#20195;&#29702;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#24182;&#35299;&#37322;&#32593;&#32476;&#19978;&#24433;&#21709;&#27969;&#21160;&#30340;&#34920;&#36798;&#24335;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#22240;&#26524;&#20851;&#31995;&#21462;&#20915;&#20110;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#27599;&#20010;&#20195;&#29702;&#23545;&#20110;&#20182;&#20204;&#35797;&#22270;&#35299;&#20915;&#30340;&#25512;&#29702;&#38382;&#39064;&#30340;&#20449;&#24687;&#27700;&#24179;&#12290;&#22522;&#20110;&#36825;&#20123;&#32467;&#35770;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#35780;&#20272;&#20195;&#29702;&#20043;&#38388;&#30340;&#25972;&#20307;&#24433;&#21709;&#21147;&#65292;&#20197;&#21457;&#29616;&#39640;&#24230;&#26377;&#24433;&#21709;&#21147;&#30340;&#20195;&#29702;&#12290;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#20174;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#24517;&#35201;&#30340;&#27169;&#22411;&#21442;&#25968;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#21644;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#36890;&#36807;&#32771;&#34385;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#30340;Twitter&#25968;&#25454;&#21152;&#20197;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates causal influences between agents linked by a social graph and interacting over time. In particular, the work examines the dynamics of social learning models and distributed decision-making protocols, and derives expressions that reveal the causal relations between pairs of agents and explain the flow of influence over the network. The results turn out to be dependent on the graph topology and the level of information that each agent has about the inference problem they are trying to solve. Using these conclusions, the paper proposes an algorithm to rank the overall influence between agents to discover highly influential agents. It also provides a method to learn the necessary model parameters from raw observational data. The results and the proposed algorithm are illustrated by considering both synthetic data and real Twitter data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27809;&#26377;&#22522;&#20934;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#21464;&#37327;&#23376;&#38598;&#19978;&#23398;&#20064;&#30340;&#22240;&#26524;&#22270;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#26816;&#27979;&#65292;&#26469;&#20266;&#35777;&#22240;&#26524;&#20851;&#31995;&#30340;&#25512;&#26029;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09552</link><description>&lt;p&gt;
&#33258;&#25105;&#20860;&#23481;&#24615;&#65306;&#22312;&#27809;&#26377;&#22522;&#20934;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-Compatibility: Evaluating Causal Discovery without Ground Truth. (arXiv:2307.09552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27809;&#26377;&#22522;&#20934;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#21464;&#37327;&#23376;&#38598;&#19978;&#23398;&#20064;&#30340;&#22240;&#26524;&#22270;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#26816;&#27979;&#65292;&#26469;&#20266;&#35777;&#22240;&#26524;&#20851;&#31995;&#30340;&#25512;&#26029;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22240;&#26524;&#22522;&#26412;&#20107;&#23454;&#38750;&#24120;&#32597;&#35265;&#65292;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#36890;&#24120;&#21482;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#36825;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#27169;&#25311;&#21453;&#26144;&#20102;&#20851;&#20110;&#22122;&#22768;&#20998;&#24067;&#12289;&#27169;&#22411;&#31867;&#21035;&#31561;&#29983;&#25104;&#36807;&#31243;&#30340;&#24120;&#35265;&#20551;&#35774;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27809;&#26377;&#22522;&#20934;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#30340;&#36755;&#20986;&#36827;&#34892;&#20266;&#35777;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#23613;&#31649;&#32479;&#35745;&#23398;&#20064;&#23547;&#27714;&#25968;&#25454;&#28857;&#23376;&#38598;&#20043;&#38388;&#30340;&#31283;&#23450;&#24615;&#65292;&#20294;&#22240;&#26524;&#23398;&#20064;&#24212;&#35813;&#23547;&#27714;&#21464;&#37327;&#23376;&#38598;&#20043;&#38388;&#30340;&#31283;&#23450;&#24615;&#12290;&#22522;&#20110;&#36825;&#20010;&#35265;&#35299;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#22312;&#19981;&#21516;&#21464;&#37327;&#23376;&#38598;&#19978;&#23398;&#20064;&#30340;&#22240;&#26524;&#22270;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#27010;&#24565;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26816;&#27979;&#19981;&#20860;&#23481;&#24615;&#21487;&#20197;&#20266;&#35777;&#22240;&#26524;&#20851;&#31995;&#34987;&#38169;&#35823;&#25512;&#26029;&#30340;&#21407;&#22240;&#65292;&#36825;&#26159;&#22240;&#20026;&#20551;&#35774;&#36829;&#21453;&#25110;&#26377;&#38480;&#26679;&#26412;&#25928;&#24212;&#24102;&#26469;&#30340;&#38169;&#35823;&#12290;&#34429;&#28982;&#36890;&#36807;&#36825;&#31181;&#20860;&#23481;&#24615;&#27979;&#35797;&#21482;&#26159;&#23545;&#33391;&#22909;&#24615;&#33021;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#23427;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
As causal ground truth is incredibly rare, causal discovery algorithms are commonly only evaluated on simulated data. This is concerning, given that simulations reflect common preconceptions about generating processes regarding noise distributions, model classes, and more. In this work, we propose a novel method for falsifying the output of a causal discovery algorithm in the absence of ground truth. Our key insight is that while statistical learning seeks stability across subsets of data points, causal learning should seek stability across subsets of variables. Motivated by this insight, our method relies on a notion of compatibility between causal graphs learned on different subsets of variables. We prove that detecting incompatibilities can falsify wrongly inferred causal relations due to violation of assumptions or errors from finite sample effects. Although passing such compatibility tests is only a necessary criterion for good performance, we argue that it provides strong evidenc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#35821;&#20041;&#26223;&#35266;&#33539;&#24335;&#65292;&#29992;&#20110;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#21147;&#23398;&#65292;&#23558;&#20854;&#35270;&#20026;&#22312;&#22270;&#19978;&#30340;&#36335;&#24452;&#65292;&#22270;&#30340;&#33410;&#28857;&#23545;&#24212;&#20110;&#32593;&#32476;&#23398;&#20064;&#34920;&#31034;&#20013;&#30340;&#26032;&#31639;&#27861;&#12290;&#36825;&#31181;&#25277;&#35937;&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#30740;&#31350;&#36807;&#30340;&#38382;&#39064;&#26469;&#35299;&#37322;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2307.09550</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#20041;&#26223;&#35266;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
The semantic landscape paradigm for neural networks. (arXiv:2307.09550v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#35821;&#20041;&#26223;&#35266;&#33539;&#24335;&#65292;&#29992;&#20110;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#21147;&#23398;&#65292;&#23558;&#20854;&#35270;&#20026;&#22312;&#22270;&#19978;&#30340;&#36335;&#24452;&#65292;&#22270;&#30340;&#33410;&#28857;&#23545;&#24212;&#20110;&#32593;&#32476;&#23398;&#20064;&#34920;&#31034;&#20013;&#30340;&#26032;&#31639;&#27861;&#12290;&#36825;&#31181;&#25277;&#35937;&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#30740;&#31350;&#36807;&#30340;&#38382;&#39064;&#26469;&#35299;&#37322;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#20196;&#20154;&#30528;&#36855;&#30340;&#29616;&#35937;&#65292;&#20174;&#21487;&#39044;&#27979;&#30340;&#32553;&#25918;&#23450;&#24459;&#21040;&#35757;&#32451;&#26102;&#38388;&#12289;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#32593;&#32476;&#22823;&#23567;&#30340;&#19981;&#21487;&#39044;&#27979;&#30340;&#26032;&#33021;&#21147;&#30340;&#20986;&#29616;&#12290;&#23545;&#36825;&#20123;&#29616;&#35937;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20123;&#32593;&#32476;&#30340;&#23398;&#20064;&#34920;&#31034;&#20013;&#32534;&#30721;&#30340;&#27010;&#24565;&#21644;&#31639;&#27861;&#30340;&#23384;&#22312;&#12290;&#34429;&#28982;&#22312;&#35299;&#37322;&#36825;&#20123;&#35266;&#23519;&#21040;&#30340;&#29616;&#35937;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#29702;&#35299;&#12289;&#35299;&#21078;&#21644;&#39044;&#27979;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#32479;&#19968;&#26694;&#26550;&#23578;&#32570;&#20047;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#20041;&#26223;&#35266;&#33539;&#24335;&#65292;&#36825;&#26159;&#19968;&#20010;&#27010;&#24565;&#24615;&#21644;&#25968;&#23398;&#26694;&#26550;&#65292;&#25551;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#21147;&#23398;&#65292;&#20854;&#20013;&#30340;&#36335;&#24452;&#34987;&#35270;&#20026;&#25972;&#20010;&#32593;&#32476;&#20013;&#23398;&#20064;&#34920;&#31034;&#20869;&#22312;&#30340;&#26032;&#31639;&#27861;&#12290;&#36825;&#31181;&#25277;&#35937;&#20351;&#25105;&#20204;&#33021;&#22815;&#20197;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#30740;&#31350;&#36807;&#30340;&#38382;&#39064;&#26469;&#25551;&#36848;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks exhibit a fascinating spectrum of phenomena ranging from predictable scaling laws to the unpredictable emergence of new capabilities as a function of training time, dataset size and network size. Analysis of these phenomena has revealed the existence of concepts and algorithms encoded within the learned representations of these networks. While significant strides have been made in explaining observed phenomena separately, a unified framework for understanding, dissecting, and predicting the performance of neural networks is lacking. Here, we introduce the semantic landscape paradigm, a conceptual and mathematical framework that describes the training dynamics of neural networks as trajectories on a graph whose nodes correspond to emergent algorithms that are instrinsic to the learned representations of the networks. This abstraction enables us to describe a wide range of neural network phenomena in terms of well studied problems in statistical physics. Specifically
&lt;/p&gt;</description></item><item><title>DreaMR &#26159;&#22522;&#20110;&#25193;&#25955;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#29305;&#24322;&#24615;&#12289;&#21512;&#29702;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#35299;&#37322;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2307.09547</link><description>&lt;p&gt;
DreaMR: &#22522;&#20110;&#25193;&#25955;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
DreaMR: Diffusion-driven Counterfactual Explanation for Functional MRI. (arXiv:2307.09547v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09547
&lt;/p&gt;
&lt;p&gt;
DreaMR &#26159;&#22522;&#20110;&#25193;&#25955;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#29305;&#24322;&#24615;&#12289;&#21512;&#29702;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#35299;&#37322;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20998;&#26512;&#22312;&#20174;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#27979;&#37327;&#20013;&#26816;&#27979;&#35748;&#30693;&#29366;&#24577;&#26041;&#38754;&#25552;&#20379;&#20102;&#25935;&#24863;&#24615;&#30340;&#39134;&#36291;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28145;&#24230;&#27169;&#22411;&#23545;&#20854;&#36755;&#20837;&#36827;&#34892;&#23618;&#27425;&#38750;&#32447;&#24615;&#21464;&#25442;&#65292;&#35299;&#37322;&#33041;&#37096;&#21709;&#24212;&#21644;&#35748;&#30693;&#29366;&#24577;&#20043;&#38388;&#30340;&#20851;&#32852;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#28145;&#24230;fMRI&#20998;&#31867;&#22120;&#30340;&#24120;&#35265;&#35299;&#37322;&#26041;&#27861;&#20013;&#65292;&#24402;&#22240;&#26041;&#27861;&#26174;&#31034;&#20986;&#36739;&#24046;&#30340;&#29305;&#24322;&#24615;&#65292;&#25200;&#21160;&#26041;&#27861;&#26174;&#31034;&#20986;&#26377;&#38480;&#30340;&#21512;&#29702;&#24615;&#12290;&#34429;&#28982;&#21453;&#20107;&#23454;&#29983;&#25104;&#25215;&#35834;&#35299;&#20915;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#20294;&#20197;&#24448;&#30340;&#26041;&#27861;&#20351;&#29992;&#21464;&#20998;&#25110;&#23545;&#25239;&#24615;&#20808;&#39564;&#65292;&#23548;&#33268;&#26679;&#26412;&#19968;&#33268;&#24615;&#19981;&#20339;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#21453;&#20107;&#23454;&#26041;&#27861;DreaMR&#65292;&#20197;&#23454;&#29616;&#20855;&#26377;&#39640;&#29305;&#24322;&#24615;&#65292;&#21512;&#29702;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;fMRI&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning analyses have offered sensitivity leaps in detection of cognitive states from functional MRI (fMRI) measurements across the brain. Yet, as deep models perform hierarchical nonlinear transformations on their input, interpreting the association between brain responses and cognitive states is challenging. Among common explanation approaches for deep fMRI classifiers, attribution methods show poor specificity and perturbation methods show limited plausibility. While counterfactual generation promises to address these limitations, previous methods use variational or adversarial priors that yield suboptimal sample fidelity. Here, we introduce the first diffusion-driven counterfactual method, DreaMR, to enable fMRI interpretation with high specificity, plausibility and fidelity. DreaMR performs diffusion-based resampling of an input fMRI sample to alter the decision of a downstream classifier, and then computes the minimal difference between the original and counterfactual sampl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#35760;&#24518;&#21270;&#30340;&#23616;&#37096;&#21270;&#29616;&#35937;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#35760;&#24518;&#21270;&#24182;&#19981;&#23616;&#38480;&#20110;&#20010;&#21035;&#23618;&#65292;&#32780;&#26159;&#22312;&#27169;&#22411;&#30340;&#22810;&#20010;&#23618;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#31070;&#32463;&#20803;&#20013;&#21457;&#29983;&#12290;</title><link>http://arxiv.org/abs/2307.09542</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#35760;&#24518;&#21270;&#26159;&#21542;&#21487;&#20197;&#34987;&#23616;&#37096;&#21270;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Neural Network Memorization Be Localized?. (arXiv:2307.09542v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#35760;&#24518;&#21270;&#30340;&#23616;&#37096;&#21270;&#29616;&#35937;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#35760;&#24518;&#21270;&#24182;&#19981;&#23616;&#38480;&#20110;&#20010;&#21035;&#23618;&#65292;&#32780;&#26159;&#22312;&#27169;&#22411;&#30340;&#22810;&#20010;&#23618;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#31070;&#32463;&#20803;&#20013;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21162;&#21147;&#22312;&#35299;&#37322;&#28145;&#24230;&#36229;&#21442;&#25968;&#21270;&#32593;&#32476;&#20013;&#35760;&#24518;&#21270;&#21644;&#27010;&#25324;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26102;&#65292;&#25552;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#27169;&#22411;&#30340;&#26368;&#21518;&#20960;&#23618;&#20013;$\textit{&#35760;&#24518;&#21270;}$&#8220;&#22256;&#38590;&#8221;&#26679;&#26412;&#30340;&#33021;&#21147;&#12290;&#35760;&#24518;&#21270;&#26159;&#25351;&#22312;&#35757;&#32451;&#38598;&#30340;$\textit{&#38750;&#20856;&#22411;}$&#26679;&#26412;&#19978;&#33021;&#22815;&#27491;&#30830;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35760;&#24518;&#21270;&#29616;&#35937;&#24182;&#19981;&#23616;&#38480;&#20110;&#20010;&#21035;&#23618;&#65292;&#32780;&#26159;&#22312;&#27169;&#22411;&#30340;&#21508;&#20010;&#23618;&#20013;&#30340;&#19968;&#23567;&#32452;&#31070;&#32463;&#20803;&#20013;&#21457;&#29983;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#19977;&#31181;&#23454;&#39564;&#26041;&#38754;&#30340;&#25910;&#25947;&#35777;&#25454;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#23618;&#23545;&#20110;&#26679;&#26412;&#30340;&#35760;&#24518;&#21270;&#26159;&#20887;&#20313;&#30340;&#65292;&#32780;&#23545;&#26679;&#26412;&#35760;&#24518;&#21270;&#30340;&#36129;&#29486;&#36739;&#22823;&#30340;&#23618;&#65292;&#24182;&#19981;&#19968;&#23450;&#26159;&#26368;&#21518;&#30340;&#23618;&#12290;&#36825;&#19977;&#20010;&#35777;&#25454;&#26469;&#28304;&#21253;&#25324;$\textit{&#26799;&#24230;&#36861;&#36394;}$&#65288;&#27979;&#37327;&#26799;&#24230;&#33539;&#25968;&#26469;&#33258;&#20110;&#35760;&#24518;&#21270;&#21644;&#24178;&#20928;&#26679;&#26412;&#30340;&#36129;&#29486;&#65289;&#65292;$\textit{&#23618;&#37325;&#32622;}$&#65288;&#23558;&#35757;&#32451;&#36807;&#31243;&#20013;&#29305;&#23450;&#27169;&#22411;&#26435;&#37325;&#26367;&#25442;&#20026;&#20808;&#21069;&#30340;&#35757;&#32451;&#26816;&#26597;&#28857;&#65289;&#65292;&#20197;&#21450;$\textit{...}$
&lt;/p&gt;
&lt;p&gt;
Recent efforts at explaining the interplay of memorization and generalization in deep overparametrized networks have posited that neural networks $\textit{memorize}$ "hard" examples in the final few layers of the model. Memorization refers to the ability to correctly predict on $\textit{atypical}$ examples of the training set. In this work, we show that rather than being confined to individual layers, memorization is a phenomenon confined to a small set of neurons in various layers of the model. First, via three experimental sources of converging evidence, we find that most layers are redundant for the memorization of examples and the layers that contribute to example memorization are, in general, not the final layers. The three sources are $\textit{gradient accounting}$ (measuring the contribution to the gradient norms from memorized and clean examples), $\textit{layer rewinding}$ (replacing specific model weights of a converged model with previous training checkpoints), and $\textit{
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#24341;&#23548;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#31574;&#30053;&#20135;&#29983;&#36879;&#26126;&#21644;&#26080;&#20559;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#30830;&#20445;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.09494</link><description>&lt;p&gt;
&#36879;&#26126;&#30340;6G RAN&#20999;&#29255;&#20013;&#22522;&#20110;&#35299;&#37322;&#30340;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Explanation-Guided Fair Federated Learning for Transparent 6G RAN Slicing. (arXiv:2307.09494v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09494
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#24341;&#23548;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#31574;&#30053;&#20135;&#29983;&#36879;&#26126;&#21644;&#26080;&#20559;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#30830;&#20445;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#30340;&#38646;&#35302;&#25720;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;6G&#32593;&#32476;&#33258;&#21160;&#21270;&#38656;&#35201;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#24314;&#31435;&#23545;AI&#40657;&#30418;&#23376;&#30340;&#20449;&#20219;&#65292;&#39044;&#35745;AI&#30340;&#21487;&#20449;&#24230;&#23558;&#19982;&#36890;&#20449;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#19968;&#36215;&#20316;&#20026;&#21487;&#37327;&#21270;&#30340;&#26381;&#21153;&#32423;&#21035;&#21327;&#35758;&#25351;&#26631;&#12290;&#36825;&#38656;&#35201;&#21033;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#36755;&#20986;&#26469;&#29983;&#25104;&#36879;&#26126;&#21644;&#26080;&#20559;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35299;&#37322;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;(EGFL)&#26469;&#30830;&#20445;&#22312;&#35757;&#32451;&#36816;&#34892;&#26102;&#36890;&#36807;Jensen-Shannon (JS)&#25955;&#24230;&#21033;&#29992;XAI&#31574;&#30053;&#30340;&#27169;&#22411;&#35299;&#37322;&#20197;&#30830;&#20445;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22238;&#24518;&#24230;&#25351;&#26631;&#20316;&#20026;&#20248;&#21270;&#20219;&#21153;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#39044;&#27979;&#27599;&#20010;&#20999;&#29255;RAN&#30340;&#20002;&#21253;&#27010;&#29575;&#26469;&#35828;&#26126;&#25152;&#25552;&#20986;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Future zero-touch artificial intelligence (AI)-driven 6G network automation requires building trust in the AI black boxes via explainable artificial intelligence (XAI), where it is expected that AI faithfulness would be a quantifiable service-level agreement (SLA) metric along with telecommunications key performance indicators (KPIs). This entails exploiting the XAI outputs to generate transparent and unbiased deep neural networks (DNNs). Motivated by closed-loop (CL) automation and explanation-guided learning (EGL), we design an explanation-guided federated learning (EGFL) scheme to ensure trustworthy predictions by exploiting the model explanation emanating from XAI strategies during the training run time via Jensen-Shannon (JS) divergence. Specifically, we predict per-slice RAN dropped traffic probability to exemplify the proposed concept while respecting fairness goals formulated in terms of the recall metric which is included as a constraint in the optimization task. Finally, the 
&lt;/p&gt;</description></item><item><title>PLiNIO&#26159;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#22522;&#20110;&#26799;&#24230;&#20248;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26041;&#27861;&#24211;&#65292;&#21487;&#20197;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#39640;&#25928;&#30340;DNNs&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.09488</link><description>&lt;p&gt;
PLiNIO: &#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#22797;&#26434;&#24230;&#24863;&#30693;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26041;&#27861;&#24211;
&lt;/p&gt;
&lt;p&gt;
PLiNIO: A User-Friendly Library of Gradient-based Methods for Complexity-aware DNN Optimization. (arXiv:2307.09488v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09488
&lt;/p&gt;
&lt;p&gt;
PLiNIO&#26159;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#22522;&#20110;&#26799;&#24230;&#20248;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26041;&#27861;&#24211;&#65292;&#21487;&#20197;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#39640;&#25928;&#30340;DNNs&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#31934;&#24230;&#19988;&#39640;&#25928;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#22312;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#25191;&#34892;&#30340;&#38656;&#27714;&#24456;&#39640;&#12290;&#20026;&#20102;&#22312;&#26032;&#24212;&#29992;&#20013;&#25214;&#21040;&#36825;&#26679;&#30340;DNNs&#65292;&#38656;&#35201;&#33258;&#21160;&#21270;&#30340;&#20248;&#21270;&#27969;&#31243;&#65292;&#22240;&#20026;&#25163;&#21160;&#25506;&#32034;&#24040;&#22823;&#30340;&#36229;&#21442;&#25968;&#32452;&#21512;&#31354;&#38388;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PLiNIO&#65292;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#23427;&#23454;&#29616;&#20102;&#19968;&#22871;&#22522;&#20110;&#36731;&#37327;&#32423;&#26799;&#24230;&#20248;&#21270;&#30340;&#26368;&#26032;DNN&#35774;&#35745;&#33258;&#21160;&#21270;&#25216;&#26415;&#65292;&#32479;&#19968;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#12290;&#36890;&#36807;&#22312;&#20960;&#20010;&#36793;&#32536;&#30456;&#20851;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PLiNIO&#20013;&#21508;&#31181;&#20248;&#21270;&#30340;&#32452;&#21512;&#21487;&#20197;&#22312;&#31934;&#24230; vs &#27169;&#22411;&#22823;&#23567;&#26041;&#38754;&#36229;&#36234;&#22522;&#32447;&#31639;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#19982;&#22522;&#32447;&#32467;&#26500;&#30456;&#27604;&#65292;PLiNIO&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;94.34%&#30340;&#20869;&#23384;&#20943;&#23569;&#65292;&#20934;&#30830;&#29575;&#21482;&#19979;&#38477;&#19981;&#21040;1%&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate yet efficient Deep Neural Networks (DNNs) are in high demand, especially for applications that require their execution on constrained edge devices. Finding such DNNs in a reasonable time for new applications requires automated optimization pipelines since the huge space of hyper-parameter combinations is impossible to explore extensively by hand. In this work, we propose PLiNIO, an open-source library implementing a comprehensive set of state-of-the-art DNN design automation techniques, all based on lightweight gradient-based optimization, under a unified and user-friendly interface. With experiments on several edge-relevant tasks, we show that combining the various optimizations available in PLiNIO leads to rich sets of solutions that Pareto-dominate the considered baselines in terms of accuracy vs model size. Noteworthy, PLiNIO achieves up to 94.34% memory reduction for a &lt;1% accuracy drop compared to a baseline architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;$k$-matroid&#32422;&#26463;&#21644;$m$-knapsack&#32422;&#26463;&#30340;&#20132;&#38598;&#19979;&#36827;&#34892;&#23376;&#27169;&#27714;&#35299;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#31639;&#27861;SPROUT&#65292;&#24182;&#24341;&#20837;&#20102;&#37096;&#20998;&#26522;&#20030;&#21644;&#24179;&#28369;&#25216;&#26415;&#20197;&#25552;&#39640;&#20854;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#36924;&#36817;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.09487</link><description>&lt;p&gt;
&#22312;&#26063;&#20132;&#21644;&#32972;&#21253;&#32422;&#26463;&#20132;&#38598;&#19979;&#30340;&#23376;&#27169;&#27714;&#35299;&#26368;&#22823;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Submodular Maximization under the Intersection of Matroid and Knapsack Constraints. (arXiv:2307.09487v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;$k$-matroid&#32422;&#26463;&#21644;$m$-knapsack&#32422;&#26463;&#30340;&#20132;&#38598;&#19979;&#36827;&#34892;&#23376;&#27169;&#27714;&#35299;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#31639;&#27861;SPROUT&#65292;&#24182;&#24341;&#20837;&#20102;&#37096;&#20998;&#26522;&#20030;&#21644;&#24179;&#28369;&#25216;&#26415;&#20197;&#25552;&#39640;&#20854;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#36924;&#36817;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#27169;&#27714;&#35299;&#26368;&#22823;&#21270;&#38382;&#39064;&#22312;&#35832;&#22810;&#24212;&#29992;&#20013;&#32463;&#24120;&#20986;&#29616;&#65292;&#24182;&#19988;&#21560;&#24341;&#20102;&#26469;&#33258;&#20154;&#24037;&#26234;&#33021;&#12289;&#37329;&#34701;&#21644;&#36816;&#31609;&#23398;&#31561;&#39046;&#22495;&#30340;&#24191;&#27867;&#30740;&#31350;&#20851;&#27880;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#32771;&#34385;&#20102;&#21333;&#19968;&#31181;&#31867;&#30340;&#32422;&#26463;&#65292;&#32780;&#24456;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#39064;&#24120;&#24120;&#28041;&#21450;&#22810;&#20010;&#32422;&#26463;&#12290;&#26412;&#25991;&#32771;&#34385;&#22312;$k$-matroid&#32422;&#26463;&#21644;$m$-knapsack&#32422;&#26463;&#30340;&#20132;&#38598;&#19979;&#36827;&#34892;&#23376;&#27169;&#27714;&#35299;&#26368;&#22823;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#23558;&#37096;&#20998;&#26522;&#20030;&#26041;&#27861;&#24341;&#20837;&#21040;&#21516;&#26102;&#36138;&#24515;&#31639;&#27861;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;SPROUT&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SPROUT&#21487;&#20197;&#23454;&#29616;&#27604;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#26356;&#22909;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#36924;&#36817;&#20445;&#35777;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#38543;&#26426;&#26522;&#20030;&#21644;&#24179;&#28369;&#25216;&#26415;&#24341;&#20837;&#21040;SPROUT&#20013;&#20197;&#25552;&#39640;&#20854;&#25928;&#29575;&#65292;&#24471;&#21040;&#20102;SPROUT++&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20445;&#25345;&#31867;&#20284;&#30340;&#36924;&#36817;&#20445;&#35777;&#12290;&#22312;&#30005;&#24433;&#25512;&#33616;&#21644;&#21152;&#26435;&#26368;&#22823;&#21106;&#30340;&#24212;&#29992;&#23454;&#39564;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Submodular maximization arises in many applications, and has attracted a lot of research attentions from various areas such as artificial intelligence, finance and operations research. Previous studies mainly consider only one kind of constraint, while many real-world problems often involve several constraints. In this paper, we consider the problem of submodular maximization under the intersection of two commonly used constraints, i.e., $k$-matroid constraint and $m$-knapsack constraint, and propose a new algorithm SPROUT by incorporating partial enumeration into the simultaneous greedy framework. We prove that SPROUT can achieve a polynomial-time approximation guarantee better than the state-of-the-art algorithms. Then, we introduce the random enumeration and smooth techniques into SPROUT to improve its efficiency, resulting in the SPROUT++ algorithm, which can keep a similar approximation guarantee. Experiments on the applications of movie recommendation and weighted max-cut demonst
&lt;/p&gt;</description></item><item><title>MolFM&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#20998;&#23376;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#20851;&#27880;&#23454;&#29616;&#20102;&#20998;&#23376;&#32467;&#26500;&#12289;&#25991;&#26412;&#21644;&#30693;&#35782;&#22270;&#35889;&#20043;&#38388;&#30340;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2307.09484</link><description>&lt;p&gt;
MolFM:&#19968;&#31181;&#22810;&#27169;&#24577;&#20998;&#23376;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MolFM: A Multimodal Molecular Foundation Model. (arXiv:2307.09484v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09484
&lt;/p&gt;
&lt;p&gt;
MolFM&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#20998;&#23376;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#36328;&#27169;&#24577;&#20851;&#27880;&#23454;&#29616;&#20102;&#20998;&#23376;&#32467;&#26500;&#12289;&#25991;&#26412;&#21644;&#30693;&#35782;&#22270;&#35889;&#20043;&#38388;&#30340;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#30693;&#35782;&#23384;&#22312;&#20110;&#19977;&#31181;&#19981;&#21516;&#30340;&#20449;&#24687;&#26469;&#28304;&#27169;&#24335;&#20013;&#65306;&#20998;&#23376;&#32467;&#26500;&#12289;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#21644;&#30693;&#35782;&#24211;&#12290;&#26377;&#25928;&#25972;&#21512;&#26469;&#33258;&#36825;&#20123;&#27169;&#24577;&#30340;&#20998;&#23376;&#30693;&#35782;&#23545;&#20419;&#36827;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#20998;&#23376;&#22522;&#30784;&#27169;&#22411;&#22312;&#25429;&#25417;&#20998;&#23376;&#32467;&#26500;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#32852;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20204;&#20013;&#30340;&#20219;&#20309;&#19968;&#20010;&#37117;&#27809;&#26377;&#23581;&#35797;&#21033;&#29992;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#33719;&#24471;&#30340;&#20016;&#23500;&#20998;&#23376;&#19987;&#19994;&#30693;&#35782;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MolFM&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#20998;&#23376;&#22522;&#30784;&#27169;&#22411;&#65292;&#26088;&#22312;&#20419;&#36827;&#20174;&#20998;&#23376;&#32467;&#26500;&#12289;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#21644;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#23376;&#32467;&#26500;&#20013;&#30340;&#21407;&#23376;&#12289;&#20998;&#23376;&#23454;&#20307;&#30340;&#37051;&#23621;&#21644;&#35821;&#20041;&#30456;&#20851;&#25991;&#26412;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#20851;&#27880;&#65292;&#20197;&#20419;&#36827;&#36328;&#27169;&#24577;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#36328;&#27169;&#24577;&#39044;&#35757;&#32451;&#25429;&#25417;&#21040;&#20102;&#20998;&#23376;&#32467;&#26500;&#12289;&#25991;&#26412;&#21644;&#30693;&#35782;&#22270;&#35889;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular knowledge resides within three different modalities of information sources: molecular structures, biomedical documents, and knowledge bases. Effective incorporation of molecular knowledge from these modalities holds paramount significance in facilitating biomedical research. However, existing multimodal molecular foundation models exhibit limitations in capturing intricate connections between molecular structures and texts, and more importantly, none of them attempt to leverage a wealth of molecular expertise derived from knowledge graphs. In this study, we introduce MolFM, a multimodal molecular foundation model designed to facilitate joint representation learning from molecular structures, biomedical texts, and knowledge graphs. We propose cross-modal attention between atoms of molecular structures, neighbors of molecule entities and semantically related texts to facilitate cross-modal comprehension. We provide theoretical analysis that our cross-modal pre-training captures
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#30005;&#36335;&#20998;&#26512;&#22312;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#23545;70B&#27611;&#20011;&#40736;&#27169;&#22411;&#36827;&#34892;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#36923;&#36753;&#23618;&#24402;&#22240;&#21644;&#28608;&#27963;&#20462;&#34917;&#25216;&#26415;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#22836;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2307.09458</link><description>&lt;p&gt;
&#30005;&#36335;&#20998;&#26512;&#30340;&#21487;&#35299;&#37322;&#24615;&#26159;&#21542;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65311;&#26469;&#33258;&#27611;&#20011;&#40736;&#20013;&#22810;&#39033;&#36873;&#25321;&#33021;&#21147;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla. (arXiv:2307.09458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#30005;&#36335;&#20998;&#26512;&#22312;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#23545;70B&#27611;&#20011;&#40736;&#27169;&#22411;&#36827;&#34892;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#36923;&#36753;&#23618;&#24402;&#22240;&#21644;&#28608;&#27963;&#20462;&#34917;&#25216;&#26415;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#22836;&#30340;&#35821;&#20041;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#36335;&#20998;&#26512;&#26159;&#19968;&#31181;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#26426;&#21046;&#30340;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20998;&#26512;&#37117;&#26159;&#22312;&#36828;&#31163;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#23567;&#22411;&#27169;&#22411;&#20013;&#36827;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;70B&#27611;&#20011;&#40736;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#19968;&#39033;&#26696;&#20363;&#30740;&#31350;&#65292;&#26088;&#22312;&#27979;&#35797;&#30005;&#36335;&#20998;&#26512;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#35843;&#26597;&#20102;&#27611;&#20011;&#40736;&#22312;&#30693;&#36947;&#27491;&#30830;&#31572;&#26696;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#33021;&#22815;&#35782;&#21035;&#20986;&#27491;&#30830;&#31572;&#26696;&#26631;&#31614;&#12290;&#25105;&#20204;&#21457;&#29616;&#24050;&#26377;&#30340;&#36923;&#36753;&#23618;&#24402;&#22240;&#12289;&#27880;&#24847;&#21147;&#27169;&#24335;&#21487;&#35270;&#21270;&#21644;&#28608;&#27963;&#20462;&#34917;&#25216;&#26415;&#22312;&#27611;&#20011;&#40736;&#27169;&#22411;&#20013;&#20855;&#26377;&#33258;&#28982;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#21644;&#20998;&#31867;&#19968;&#23567;&#32452;&#8220;&#36755;&#20986;&#33410;&#28857;&#8221;&#65288;&#27880;&#24847;&#21147;&#22836;&#21644;&#22810;&#23618;&#24863;&#30693;&#26426;&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#8220;&#27491;&#30830;&#23383;&#27597;&#8221;&#31867;&#21035;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26088;&#22312;&#20102;&#35299;&#20854;&#29305;&#24449;&#30340;&#35821;&#20041;&#65292;&#32467;&#26524;&#26377;&#25152;&#19981;&#21516;&#12290;&#23545;&#20110;&#27491;&#24120;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#25105;&#20204;&#26174;&#33879;&#21387;&#32553;&#20102;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
\emph{Circuit analysis} is a promising technique for understanding the internal mechanisms of language models. However, existing analyses are done in small models far from the state of the art. To address this, we present a case study of circuit analysis in the 70B Chinchilla model, aiming to test the scalability of circuit analysis. In particular, we study multiple-choice question answering, and investigate Chinchilla's capability to identify the correct answer \emph{label} given knowledge of the correct answer \emph{text}. We find that the existing techniques of logit attribution, attention pattern visualization, and activation patching naturally scale to Chinchilla, allowing us to identify and categorize a small set of `output nodes' (attention heads and MLPs).  We further study the `correct letter' category of attention heads aiming to understand the semantics of their features, with mixed results. For normal multiple-choice question answers, we significantly compress the query, ke
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;&#20998;&#31867;&#32534;&#30721;&#22120;&#22522;&#20934;&#30740;&#31350;&#65292;&#36890;&#36807;&#23545;&#26469;&#33258;&#19981;&#21516;&#23478;&#26063;&#30340;32&#31181;&#32534;&#30721;&#22120;&#37197;&#32622;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#20197;&#21450;36&#31181;&#23454;&#39564;&#22240;&#32032;&#21644;50&#20010;&#25968;&#25454;&#38598;&#30340;&#32452;&#21512;&#65292;&#23637;&#31034;&#20102;&#25968;&#25454;&#38598;&#36873;&#25321;&#12289;&#23454;&#39564;&#22240;&#32032;&#21644;&#32858;&#21512;&#31574;&#30053;&#23545;&#22522;&#20934;&#30740;&#31350;&#32467;&#35770;&#30340;&#28145;&#36828;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.09191</link><description>&lt;p&gt;
&#29992;&#20110;&#20108;&#20998;&#31867;&#30340;&#20998;&#31867;&#32534;&#30721;&#22120;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A benchmark of categorical encoders for binary classification. (arXiv:2307.09191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;&#20998;&#31867;&#32534;&#30721;&#22120;&#22522;&#20934;&#30740;&#31350;&#65292;&#36890;&#36807;&#23545;&#26469;&#33258;&#19981;&#21516;&#23478;&#26063;&#30340;32&#31181;&#32534;&#30721;&#22120;&#37197;&#32622;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#20197;&#21450;36&#31181;&#23454;&#39564;&#22240;&#32032;&#21644;50&#20010;&#25968;&#25454;&#38598;&#30340;&#32452;&#21512;&#65292;&#23637;&#31034;&#20102;&#25968;&#25454;&#38598;&#36873;&#25321;&#12289;&#23454;&#39564;&#22240;&#32032;&#21644;&#32858;&#21512;&#31574;&#30053;&#23545;&#22522;&#20934;&#30740;&#31350;&#32467;&#35770;&#30340;&#28145;&#36828;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#32534;&#30721;&#22120;&#23558;&#20998;&#31867;&#29305;&#24449;&#36716;&#21270;&#20026;&#25968;&#23383;&#34920;&#31034;&#65292;&#23545;&#20110;&#24191;&#27867;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35828;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#29616;&#26377;&#30340;&#32534;&#30721;&#22120;&#22522;&#20934;&#30740;&#31350;&#30001;&#20110;&#36873;&#25321;&#26377;&#38480;&#30340;&#32534;&#30721;&#22120;&#12289;&#23454;&#39564;&#22240;&#32032;&#21644;&#25968;&#25454;&#38598;&#65292;&#32570;&#20047;&#26222;&#36866;&#24615;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#37319;&#29992;&#20102;&#19981;&#21516;&#30340;&#32858;&#21512;&#31574;&#30053;&#65292;&#32467;&#26524;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;&#20998;&#31867;&#32534;&#30721;&#22120;&#22522;&#20934;&#30740;&#31350;&#65292;&#21253;&#25324;&#23545;&#26469;&#33258;&#19981;&#21516;&#23478;&#26063;&#30340;32&#31181;&#32534;&#30721;&#22120;&#37197;&#32622;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#20197;&#21450;36&#31181;&#23454;&#39564;&#22240;&#32032;&#21644;50&#20010;&#25968;&#25454;&#38598;&#30340;&#32452;&#21512;&#12290;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#25968;&#25454;&#38598;&#36873;&#25321;&#12289;&#23454;&#39564;&#22240;&#32032;&#21644;&#32858;&#21512;&#31574;&#30053;&#23545;&#22522;&#20934;&#30740;&#31350;&#32467;&#35770;&#30340;&#28145;&#36828;&#24433;&#21709;&#65292;&#36825;&#26159;&#20197;&#21069;&#30340;&#32534;&#30721;&#22120;&#22522;&#20934;&#30740;&#31350;&#24573;&#35270;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Categorical encoders transform categorical features into numerical representations that are indispensable for a wide range of machine learning models. Existing encoder benchmark studies lack generalizability because of their limited choice of (1) encoders, (2) experimental factors, and (3) datasets. Additionally, inconsistencies arise from the adoption of varying aggregation strategies. This paper is the most comprehensive benchmark of categorical encoders to date, including an extensive evaluation of 32 configurations of encoders from diverse families, with 36 combinations of experimental factors, and on 50 datasets. The study shows the profound influence of dataset selection, experimental factors, and aggregation strategies on the benchmark's conclusions -- aspects disregarded in previous encoder benchmarks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#25237;&#24433;&#22836;&#30340;&#31232;&#30095;&#24615;&#65292;&#21457;&#29616;&#36890;&#36807;&#22312;&#25237;&#24433;&#23376;&#31354;&#38388;&#20013;&#25191;&#34892;&#23545;&#27604;&#25439;&#22833;&#21487;&#20197;&#25552;&#21319;&#34920;&#31034;&#30340;&#36136;&#37327;&#65292;&#24314;&#35758;&#21482;&#26377;&#19968;&#37096;&#20998;&#29305;&#24449;&#26159;&#24517;&#35201;&#30340;&#65292;&#32780;&#31232;&#30095;&#30340;&#25237;&#24433;&#22836;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08913</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#25237;&#24433;&#22836;&#30340;&#31232;&#30095;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards the Sparseness of Projection Head in Self-Supervised Learning. (arXiv:2307.08913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08913
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#25237;&#24433;&#22836;&#30340;&#31232;&#30095;&#24615;&#65292;&#21457;&#29616;&#36890;&#36807;&#22312;&#25237;&#24433;&#23376;&#31354;&#38388;&#20013;&#25191;&#34892;&#23545;&#27604;&#25439;&#22833;&#21487;&#20197;&#25552;&#21319;&#34920;&#31034;&#30340;&#36136;&#37327;&#65292;&#24314;&#35758;&#21482;&#26377;&#19968;&#37096;&#20998;&#29305;&#24449;&#26159;&#24517;&#35201;&#30340;&#65292;&#32780;&#31232;&#30095;&#30340;&#25237;&#24433;&#22836;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#25104;&#20026;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#34920;&#31034;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#31181;&#25104;&#21151;&#30340;SSL&#26041;&#27861;&#26159;&#23545;&#27604;&#23398;&#20064;&#65292;&#20854;&#26088;&#22312;&#23558;&#27491;&#26679;&#26412;&#32858;&#38598;&#22312;&#19968;&#36215;&#65292;&#23558;&#36127;&#26679;&#26412;&#25512;&#24320;&#12290;&#35768;&#22810;&#24403;&#21069;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#37117;&#20351;&#29992;&#21442;&#25968;&#21270;&#30340;&#25237;&#24433;&#22836;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#21644;&#29702;&#35770;&#25506;&#32034;&#65292;&#25105;&#20204;&#23545;&#25237;&#24433;&#22836;&#30340;&#20869;&#37096;&#26426;&#21046;&#21450;&#20854;&#19982;&#32500;&#24230;&#25240;&#21472;&#29616;&#35937;&#30340;&#20851;&#31995;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25237;&#24433;&#22836;&#36890;&#36807;&#22312;&#19968;&#20010;&#25237;&#24433;&#23376;&#31354;&#38388;&#20013;&#25191;&#34892;&#23545;&#27604;&#25439;&#22833;&#65292;&#25552;&#21319;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#22312;&#26368;&#23567;&#21270;&#19968;&#20010;&#23567;&#25209;&#37327;&#25968;&#25454;&#30340;&#23545;&#27604;&#25439;&#22833;&#26102;&#65292;&#21482;&#26377;&#19968;&#37096;&#20998;&#29305;&#24449;&#26159;&#24517;&#35201;&#30340;&#12290;&#29702;&#35770;&#20998;&#26512;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#31232;&#30095;&#30340;&#25237;&#24433;&#22836;&#21487;&#20197;&#22686;&#24378;&#27867;&#21270;&#24615;&#33021;&#65292;&#22240;&#27492;&#25105;&#20204;&#24341;&#20837;SparseHead&#36825;&#19968;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, self-supervised learning (SSL) has emerged as a promising approach for extracting valuable representations from unlabeled data. One successful SSL method is contrastive learning, which aims to bring positive examples closer while pushing negative examples apart. Many current contrastive learning approaches utilize a parameterized projection head. Through a combination of empirical analysis and theoretical investigation, we provide insights into the internal mechanisms of the projection head and its relationship with the phenomenon of dimensional collapse. Our findings demonstrate that the projection head enhances the quality of representations by performing contrastive loss in a projected subspace. Therefore, we propose an assumption that only a subset of features is necessary when minimizing the contrastive loss of a mini-batch of data. Theoretical analysis further suggests that a sparse projection head can enhance generalization, leading us to introduce SparseHead - 
&lt;/p&gt;</description></item><item><title>Retentive Network&#65288;RetNet&#65289;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#24182;&#34892;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#24182;&#34892;&#12289;&#24490;&#29615;&#21644;&#20998;&#22359;&#24490;&#29615;&#19977;&#31181;&#35745;&#31639;&#33539;&#24335;&#65292;RetNet&#20855;&#26377;&#35757;&#32451;&#24182;&#34892;&#21270;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#39640;&#25928;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.08621</link><description>&lt;p&gt;
Retentive Network: &#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;Transformer&#30340;&#32487;&#20219;&#32773;
&lt;/p&gt;
&lt;p&gt;
Retentive Network: A Successor to Transformer for Large Language Models. (arXiv:2307.08621v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08621
&lt;/p&gt;
&lt;p&gt;
Retentive Network&#65288;RetNet&#65289;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#24182;&#34892;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#24182;&#34892;&#12289;&#24490;&#29615;&#21644;&#20998;&#22359;&#24490;&#29615;&#19977;&#31181;&#35745;&#31639;&#33539;&#24335;&#65292;RetNet&#20855;&#26377;&#35757;&#32451;&#24182;&#34892;&#21270;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#39640;&#25928;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Retentive Network (RetNet)&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#26550;&#26500;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#35757;&#32451;&#24182;&#34892;&#12289;&#20302;&#25104;&#26412;&#25512;&#29702;&#21644;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#25512;&#23548;&#20986;&#20102;&#24490;&#29615;&#21644;&#27880;&#24847;&#21147;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24207;&#21015;&#24314;&#27169;&#30340;&#20445;&#30041;&#26426;&#21046;&#65292;&#25903;&#25345;&#19977;&#31181;&#35745;&#31639;&#33539;&#24335;&#65292;&#21363;&#24182;&#34892;&#12289;&#24490;&#29615;&#21644;&#20998;&#22359;&#24490;&#29615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24182;&#34892;&#34920;&#31034;&#20801;&#35768;&#36827;&#34892;&#35757;&#32451;&#24182;&#34892;&#21270;&#12290;&#24490;&#29615;&#34920;&#31034;&#33021;&#22815;&#23454;&#29616;&#20302;&#25104;&#26412;&#30340;$O(1)$&#25512;&#29702;&#65292;&#20174;&#32780;&#25552;&#39640;&#35299;&#30721;&#21534;&#21520;&#37327;&#12289;&#24310;&#36831;&#21644;GPU&#20869;&#23384;&#65292;&#21516;&#26102;&#19981;&#25439;&#22833;&#24615;&#33021;&#12290;&#20998;&#22359;&#24490;&#29615;&#34920;&#31034;&#20415;&#20110;&#20351;&#29992;&#32447;&#24615;&#22797;&#26434;&#24230;&#36827;&#34892;&#39640;&#25928;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#65292;&#20854;&#20013;&#27599;&#20010;&#22359;&#21487;&#20197;&#24182;&#34892;&#32534;&#30721;&#65292;&#21516;&#26102;&#36827;&#34892;&#24490;&#29615;&#25688;&#35201;&#12290;&#35821;&#35328;&#24314;&#27169;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RetNet&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#25193;&#23637;&#32467;&#26524;&#12289;&#24182;&#34892;&#35757;&#32451;&#12289;&#20302;&#25104;&#26412;&#37096;&#32626;&#21644;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient i
&lt;/p&gt;</description></item><item><title>M-FLAG&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;&#21644;&#24341;&#20837;&#27491;&#20132;&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#28508;&#31354;&#38388;&#20960;&#20309;&#20851;&#31995;&#12290;&#22312;&#21307;&#23398;&#24433;&#20687;&#20998;&#31867;&#12289;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#19978;&#65292;M-FLAG&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;78%&#30340;&#21442;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.08347</link><description>&lt;p&gt;
M-FLAG&#65306;&#20351;&#29992;&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;&#21644;&#28508;&#31354;&#38388;&#20960;&#20309;&#20248;&#21270;&#30340;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization. (arXiv:2307.08347v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08347
&lt;/p&gt;
&lt;p&gt;
M-FLAG&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;&#21644;&#24341;&#20837;&#27491;&#20132;&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#28508;&#31354;&#38388;&#20960;&#20309;&#20851;&#31995;&#12290;&#22312;&#21307;&#23398;&#24433;&#20687;&#20998;&#31867;&#12289;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#19978;&#65292;M-FLAG&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;78%&#30340;&#21442;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#21307;&#23398;&#24433;&#20687;&#21644;&#20020;&#24202;&#25991;&#26412;&#30340;&#29305;&#24449;&#20849;&#23398;&#20064;&#21644;&#38598;&#25104;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#35757;&#32451;&#36215;&#26469;&#24182;&#19981;&#23481;&#26131;&#65292;&#24182;&#19988;&#28508;&#31354;&#38388;&#34920;&#31034;&#21487;&#20197;&#38750;&#24120;&#22797;&#26434;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21629;&#21517;&#20026;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#19982;&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;&#21644;&#28508;&#31354;&#38388;&#20960;&#20309;&#20248;&#21270;&#65288;M-FLAG&#65289;&#65292;&#21033;&#29992;&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;&#26469;&#31283;&#23450;&#21644;&#39640;&#25928;&#22320;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#20132;&#25439;&#22833;&#20989;&#25968;&#26469;&#21327;&#35843;&#28508;&#31354;&#38388;&#20960;&#20309;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28508;&#21147;&#65306;&#21307;&#23398;&#24433;&#20687;&#20998;&#31867;&#12289;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#12290;&#36890;&#36807;&#23545;&#20116;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;M-FLAG&#22312;&#20943;&#23569;78&#65285;&#30340;&#21442;&#25968;&#30340;&#21516;&#26102;&#65292;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;M-FLAG&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical vision-language models enable co-learning and integrating features from medical imaging and clinical text. However, these models are not easy to train and the latent representation space can be complex. Here we propose a novel way for pre-training and regularising medical vision-language models. The proposed method, named Medical vision-language pre-training with Frozen language models and Latent spAce Geometry optimization (M-FLAG), leverages a frozen language model for training stability and efficiency and introduces a novel orthogonality loss to harmonize the latent space geometry. We demonstrate the potential of the pre-trained model on three downstream tasks: medical image classification, segmentation, and object detection. Extensive experiments across five public datasets demonstrate that M-FLAG significantly outperforms existing medical vision-language pre-training approaches and reduces the number of parameters by 78\%. Notably, M-FLAG achieves outstanding performance o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;&#30340;&#29702;&#35299;&#65292;&#29305;&#21035;&#20851;&#27880;&#26367;&#20195;&#35757;&#32451;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#24179;&#28369;&#24615;&#21644;&#26799;&#24230;&#30456;&#20284;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21457;&#29616;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26367;&#20195;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#36716;&#21464;&#25552;&#20986;&#20102;&#26032;&#30340;&#25512;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.07873</link><description>&lt;p&gt;
&#25506;&#32034;&#20174;&#26367;&#20195;&#35757;&#32451;&#20013;&#29702;&#35299;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Adversarial Transferability From Surrogate Training. (arXiv:2307.07873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;&#30340;&#29702;&#35299;&#65292;&#29305;&#21035;&#20851;&#27880;&#26367;&#20195;&#35757;&#32451;&#12290;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#30340;&#24179;&#28369;&#24615;&#21644;&#26799;&#24230;&#30456;&#20284;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21457;&#29616;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26367;&#20195;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#36716;&#21464;&#25552;&#20986;&#20102;&#26032;&#30340;&#25512;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;DNNs&#30340;&#23545;&#25239;&#26679;&#26412;(AEs)&#24050;&#32463;&#34920;&#26126;&#26159;&#21487;&#36716;&#31227;&#30340;&#65306;&#25104;&#21151;&#27450;&#39575;&#30333;&#30418;&#23376;&#26367;&#20195;&#27169;&#22411;&#30340;AEs&#20063;&#21487;&#20197;&#27450;&#39575;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#30340;&#20854;&#20182;&#40657;&#30418;&#27169;&#22411;&#12290;&#34429;&#28982;&#35768;&#22810;&#32463;&#39564;&#30740;&#31350;&#25552;&#20379;&#20102;&#29983;&#25104;&#39640;&#24230;&#21487;&#36716;&#31227;AE&#30340;&#25351;&#23548;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#32570;&#20047;&#35299;&#37322;&#29978;&#33267;&#23548;&#33268;&#19981;&#19968;&#33268;&#30340;&#24314;&#35758;&#12290;&#26412;&#25991;&#22312;&#29702;&#35299;&#23545;&#25239;&#24615;&#21487;&#36716;&#31227;&#24615;&#26041;&#38754;&#36808;&#20986;&#20102;&#19968;&#27493;&#65292;&#29305;&#21035;&#20851;&#27880;&#26367;&#20195;&#26041;&#38754;&#12290;&#20174;&#30528;&#21517;&#30340;&#23567;&#20581;&#22766;&#24615;&#29616;&#35937;&#24320;&#22987;&#65292;&#36890;&#36807;&#20197;&#36731;&#24494;&#25200;&#21160;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#23545;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#20854;&#24402;&#22240;&#20110;&#20004;&#20010;&#20027;&#35201;&#22240;&#32032;&#20043;&#38388;&#30340;&#26435;&#34913;&#65306;&#27169;&#22411;&#30340;&#24179;&#28369;&#24615;&#21644;&#26799;&#24230;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#23427;&#20204;&#30340;&#20849;&#21516;&#25928;&#26524;&#19978;&#65292;&#32780;&#19981;&#26159;&#23427;&#20204;&#19982;&#21487;&#36716;&#31227;&#24615;&#30340;&#21333;&#29420;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#25512;&#27979;&#25968;&#25454;&#20998;&#24067;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs that successfully fool white-box surrogate models can also deceive other black-box models with different architectures. Although a bunch of empirical studies have provided guidance on generating highly transferable AEs, many of these findings lack explanations and even lead to inconsistent advice. In this paper, we take a further step towards understanding adversarial transferability, with a particular focus on surrogate aspects. Starting from the intriguing little robustness phenomenon, where models adversarially trained with mildly perturbed adversarial samples can serve as better surrogates, we attribute it to a trade-off between two predominant factors: model smoothness and gradient similarity. Our investigations focus on their joint effects, rather than their separate correlations with transferability. Through a series of theoretical and empirical analyses, we conjecture that the data distribution shift in
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#30340;&#24378;&#21270;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#24037;&#19994;4.0&#20013;&#23454;&#26102;&#39044;&#27979;&#24322;&#24120;&#12290;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#24037;&#19994;&#26412;&#20307;&#35770;&#65292;&#20026;&#26234;&#33021;&#21046;&#36896;&#25552;&#20379;&#20102;&#24418;&#24335;&#21270;&#30693;&#35782;&#65292;&#24182;&#19988;&#36890;&#36807;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#25552;&#21462;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#30452;&#25509;&#38598;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20197;&#21069;&#20174;&#26410;&#34987;&#25506;&#32034;&#36807;&#12290;</title><link>http://arxiv.org/abs/2307.06975</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#30340;&#24378;&#21270;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#24037;&#19994;4.0&#20013;&#30340;&#23454;&#26102;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic Empowered Denoising Diffusion Probabilistic Models for Real-time Anomaly Detection in Industry 4.0. (arXiv:2307.06975v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#30340;&#24378;&#21270;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#24037;&#19994;4.0&#20013;&#23454;&#26102;&#39044;&#27979;&#24322;&#24120;&#12290;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#24037;&#19994;&#26412;&#20307;&#35770;&#65292;&#20026;&#26234;&#33021;&#21046;&#36896;&#25552;&#20379;&#20102;&#24418;&#24335;&#21270;&#30693;&#35782;&#65292;&#24182;&#19988;&#36890;&#36807;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#25552;&#21462;&#25193;&#25955;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#30452;&#25509;&#38598;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20197;&#21069;&#20174;&#26410;&#34987;&#25506;&#32034;&#36807;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;4.0&#23558;&#29289;&#32852;&#32593;&#12289;&#22823;&#25968;&#25454;&#21644;&#20154;&#24037;&#26234;&#33021;&#31561;&#25968;&#23383;&#25216;&#26415;&#25972;&#21512;&#21040;&#21046;&#36896;&#21644;&#24037;&#19994;&#27969;&#31243;&#20013;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#21644;&#29983;&#20135;&#21147;&#12290;&#38543;&#30528;&#36825;&#20123;&#25216;&#26415;&#30340;&#20114;&#32852;&#20114;&#36890;&#21644;&#30456;&#20114;&#20381;&#36182;&#31243;&#24230;&#36234;&#26469;&#36234;&#39640;&#65292;&#24037;&#19994;4.0&#31995;&#32479;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#65292;&#36825;&#23601;&#24102;&#26469;&#20102;&#35782;&#21035;&#21644;&#20572;&#27490;&#21487;&#33021;&#22312;&#21046;&#36896;&#36807;&#31243;&#20013;&#24341;&#36215;&#24178;&#25200;&#30340;&#24322;&#24120;&#30340;&#22256;&#38590;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#24037;&#19994;4.0&#27969;&#31243;&#20013;&#30340;&#23454;&#26102;&#24322;&#24120;&#39044;&#27979;&#12290;&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#24037;&#19994;&#26412;&#20307;&#35770;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#22312;&#26234;&#33021;&#21046;&#36896;&#20013;&#28155;&#21152;&#20102;&#24418;&#24335;&#21270;&#30693;&#35782;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36890;&#36807;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#25552;&#21462;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#23558;&#20854;&#37096;&#32626;&#21040;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#65292;&#30452;&#25509;&#38598;&#25104;&#21040;&#21046;&#36896;&#36807;&#31243;&#20013;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20197;&#21069;&#20174;&#26410;&#34987;&#25506;&#32034;&#36807;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industry 4.0 involves the integration of digital technologies, such as IoT, Big Data, and AI, into manufacturing and industrial processes to increase efficiency and productivity. As these technologies become more interconnected and interdependent, Industry 4.0 systems become more complex, which brings the difficulty of identifying and stopping anomalies that may cause disturbances in the manufacturing process. This paper aims to propose a diffusion-based model for real-time anomaly prediction in Industry 4.0 processes. Using a neuro-symbolic approach, we integrate industrial ontologies in the model, thereby adding formal knowledge on smart manufacturing. Finally, we propose a simple yet effective way of distilling diffusion models through Random Fourier Features for deployment on an embedded system for direct integration into the manufacturing process. To the best of our knowledge, this approach has never been explored before.
&lt;/p&gt;</description></item><item><title>IntelliGraphs&#26159;&#19968;&#32452;&#26032;&#30340;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#12290;&#20854;&#20013;&#21253;&#21547;&#20855;&#26377;&#36923;&#36753;&#35268;&#21017;&#34920;&#36798;&#30340;&#35821;&#20041;&#30340;&#23376;&#22270;&#65292;&#29992;&#20110;&#35780;&#20272;&#23376;&#22270;&#25512;&#26029;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.06698</link><description>&lt;p&gt;
IntelliGraphs: &#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
IntelliGraphs: Datasets for Benchmarking Knowledge Graph Generation. (arXiv:2307.06698v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06698
&lt;/p&gt;
&lt;p&gt;
IntelliGraphs&#26159;&#19968;&#32452;&#26032;&#30340;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#12290;&#20854;&#20013;&#21253;&#21547;&#20855;&#26377;&#36923;&#36753;&#35268;&#21017;&#34920;&#36798;&#30340;&#35821;&#20041;&#30340;&#23376;&#22270;&#65292;&#29992;&#20110;&#35780;&#20272;&#23376;&#22270;&#25512;&#26029;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#36830;&#32493;&#34920;&#31034;&#12290;&#25991;&#29486;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#20219;&#21153;&#26159;&#39044;&#27979;&#23454;&#20307;&#20043;&#38388;&#30340;&#32570;&#22833;&#38142;&#25509;&#12290;&#28982;&#32780;&#65292;&#30693;&#35782;&#22270;&#35889;&#19981;&#20165;&#20165;&#26159;&#38142;&#25509;&#30340;&#38598;&#21512;&#65292;&#36824;&#20855;&#26377;&#20854;&#32467;&#26500;&#20013;&#30340;&#35821;&#20041;&#12290;&#35821;&#20041;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#26597;&#35810;&#22238;&#31572;&#25110;&#25512;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23376;&#22270;&#25512;&#26029;&#20219;&#21153;&#65292;&#20854;&#20013;&#19968;&#20010;&#27169;&#22411;&#24517;&#39035;&#29983;&#25104;&#21487;&#33021;&#30340;&#24182;&#19988;&#35821;&#20041;&#19978;&#26377;&#25928;&#30340;&#23376;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;IntelliGraphs&#65292;&#19968;&#20010;&#21253;&#21547;&#20116;&#20010;&#26032;&#30340;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#30340;&#38598;&#21512;&#12290;IntelliGraphs&#25968;&#25454;&#38598;&#21253;&#21547;&#20855;&#26377;&#36923;&#36753;&#35268;&#21017;&#34920;&#36798;&#30340;&#35821;&#20041;&#30340;&#23376;&#22270;&#65292;&#29992;&#20110;&#35780;&#20272;&#23376;&#22270;&#25512;&#26029;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#20135;&#29983;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22235;&#20010;&#26032;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;&#20256;&#32479;KGE&#30340;&#19977;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#21040;&#35821;&#20041;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#19968;&#22522;&#20934;&#23558;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embedding (KGE) models are used to learn continuous representations of entities and relations. A key task in the literature is predicting missing links between entities. However, Knowledge Graphs are not just sets of links but also have semantics underlying their structure. Semantics is crucial in several downstream tasks, such as query answering or reasoning. We introduce the subgraph inference task, where a model has to generate likely and semantically valid subgraphs. We propose IntelliGraphs, a set of five new Knowledge Graph datasets. The IntelliGraphs datasets contain subgraphs with semantics expressed in logical rules for evaluating subgraph inference. We also present the dataset generator that produced the synthetic datasets. We designed four novel baseline models, which include three models based on traditional KGEs. We evaluate their expressiveness and show that these models cannot capture the semantics. We believe this benchmark will encourage the development
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#23545;&#25239;&#25915;&#20987;&#37325;&#26032;&#35774;&#23450;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#36890;&#36807;&#29983;&#25104;&#22270;&#20687;&#22122;&#22768;&#26469;&#28385;&#36275;&#26032;&#20852;&#36235;&#21183;&#65292;&#24182;&#23558;&#22522;&#30784;&#27169;&#22411;&#24341;&#20837;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#12290;&#34429;&#28982;&#22522;&#30784;&#27169;&#22411;&#30340;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#32570;&#20047;&#23545;&#24212;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2307.06608</link><description>&lt;p&gt;
&#23558;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#24341;&#20837;&#65306;&#26397;&#30528;&#26356;&#23454;&#29992;&#30340;&#23545;&#25239;&#25915;&#20987;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Introducing Foundation Models as Surrogate Models: Advancing Towards More Practical Adversarial Attacks. (arXiv:2307.06608v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#23545;&#25239;&#25915;&#20987;&#37325;&#26032;&#35774;&#23450;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#36890;&#36807;&#29983;&#25104;&#22270;&#20687;&#22122;&#22768;&#26469;&#28385;&#36275;&#26032;&#20852;&#36235;&#21183;&#65292;&#24182;&#23558;&#22522;&#30784;&#27169;&#22411;&#24341;&#20837;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#12290;&#34429;&#28982;&#22522;&#30784;&#27169;&#22411;&#30340;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#32570;&#20047;&#23545;&#24212;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26080;&#30418;&#23545;&#25239;&#25915;&#20987;&#25104;&#20026;&#20102;&#26368;&#23454;&#29992;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25915;&#20987;&#26041;&#24335;&#65292;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#27169;&#22411;&#30340;&#26550;&#26500;&#12289;&#26435;&#37325;&#21644;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#26080;&#30418;&#35774;&#32622;&#20013;&#65292;&#23545;&#20110;&#20195;&#29702;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#30340;&#28508;&#21147;&#21644;&#28789;&#27963;&#24615;&#32570;&#20047;&#35748;&#35782;&#12290;&#21463;&#21040;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#30340;&#20852;&#36259;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;1&#65289;&#23558;&#23545;&#25239;&#25915;&#20987;&#37325;&#26032;&#35774;&#23450;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#26159;&#29983;&#25104;&#22270;&#20687;&#22122;&#22768;&#20197;&#28385;&#36275;&#26032;&#20852;&#36235;&#21183;&#65307;2&#65289;&#23558;&#22522;&#30784;&#27169;&#22411;&#24341;&#20837;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#30340;&#21019;&#26032;&#24605;&#24819;&#12290;&#36890;&#36807;&#21033;&#29992;&#38750;&#40065;&#26834;&#29305;&#24449;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#36873;&#25321;&#20195;&#29702;&#27169;&#22411;&#30340;&#20004;&#20010;&#25351;&#23548;&#21407;&#21017;&#65292;&#20197;&#35299;&#37322;&#20026;&#20160;&#20040;&#22522;&#30784;&#27169;&#22411;&#26159;&#36825;&#19968;&#35282;&#33394;&#30340;&#26368;&#20339;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#30683;&#30462;&#22320;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#20998;&#26512;&#36825;&#31181;&#24847;&#22806;&#34892;&#20026;&#65292;&#25105;&#20204;&#24402;&#22240;&#20110;&#32570;&#20047;&#19978;&#36848;&#25351;&#23548;&#21407;&#21017;&#25152;&#38656;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the no-box adversarial attack, in which the attacker lacks access to the model's architecture, weights, and training data, become the most practical and challenging attack setup. However, there is an unawareness of the potential and flexibility inherent in the surrogate model selection process on no-box setting. Inspired by the burgeoning interest in utilizing foundational models to address downstream tasks, this paper adopts an innovative idea that 1) recasting adversarial attack as a downstream task. Specifically, image noise generation to meet the emerging trend and 2) introducing foundational models as surrogate models. Harnessing the concept of non-robust features, we elaborate on two guiding principles for surrogate model selection to explain why the foundational model is an optimal choice for this role. However, paradoxically, we observe that these foundational models underperform. Analyzing this unexpected behavior within the feature space, we attribute the lackluster
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#24369;&#30417;&#30563;&#26465;&#20214;&#19979;&#30340;&#38899;&#35270;&#39057;&#20107;&#20214;&#23450;&#20301;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#20197;&#26356;&#32454;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#20272;&#35745;&#26631;&#31614;&#65292;&#24182;&#25552;&#20986;&#36741;&#21161;&#30446;&#26631;&#26469;&#22788;&#29702;&#21512;&#25104;&#35270;&#39057;&#30340;&#20998;&#24067;&#22806;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06385</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#26465;&#20214;&#19979;&#30340;&#38899;&#35270;&#39057;&#20107;&#20214;&#23450;&#20301;&#30340;&#26102;&#38388;&#26631;&#31614;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Temporal Label-Refinement for Weakly-Supervised Audio-Visual Event Localization. (arXiv:2307.06385v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#24369;&#30417;&#30563;&#26465;&#20214;&#19979;&#30340;&#38899;&#35270;&#39057;&#20107;&#20214;&#23450;&#20301;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#20197;&#26356;&#32454;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#20272;&#35745;&#26631;&#31614;&#65292;&#24182;&#25552;&#20986;&#36741;&#21161;&#30446;&#26631;&#26469;&#22788;&#29702;&#21512;&#25104;&#35270;&#39057;&#30340;&#20998;&#24067;&#22806;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#35270;&#39057;&#20107;&#20214;&#23450;&#20301;&#26159;&#25351;&#22312;&#35270;&#39057;&#20013;&#23545;&#21516;&#26102;&#21487;&#35265;&#21644;&#21487;&#21548;&#21040;&#30340;&#20107;&#20214;&#36827;&#34892;&#26102;&#38388;&#23450;&#20301;&#21644;&#20998;&#31867;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#24369;&#30417;&#30563;&#26465;&#20214;&#19979;&#30340;&#38899;&#35270;&#39057;&#20107;&#20214;&#23450;&#20301;&#38382;&#39064;&#65292;&#35757;&#32451;&#36807;&#31243;&#20013;&#21482;&#26377;&#35270;&#39057;&#32423;&#21035;&#30340;&#20107;&#20214;&#26631;&#31614;&#65288;&#20165;&#26377;&#20107;&#20214;&#26159;&#21542;&#20986;&#29616;&#65292;&#20294;&#27809;&#26377;&#26102;&#38388;&#20301;&#32622;&#20449;&#24687;&#65289;&#21487;&#29992;&#20110;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#24605;&#36335;&#26159;&#20351;&#29992;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#20197;&#26356;&#32454;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#20272;&#35745;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#26631;&#31614;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#27493;&#39588;&#30830;&#23450;&#35757;&#32451;&#35270;&#39057;&#20013;&#27599;&#20010;&#24103;&#29255;&#27573;&#30340;&#26631;&#31614;&#23376;&#38598;: (i) &#29992;&#21478;&#19968;&#20010;&#35270;&#39057;&#20013;&#19982;&#35270;&#39057;&#32423;&#21035;&#26631;&#31614;&#27809;&#26377;&#37325;&#21472;&#30340;&#24103;&#26367;&#25442;&#29255;&#27573;&#22806;&#30340;&#24103;&#65292; (ii) &#23558;&#36825;&#20010;&#21512;&#25104;&#35270;&#39057;&#36755;&#20837;&#22522;&#30784;&#27169;&#22411;&#65292;&#20165;&#25552;&#21462;&#35813;&#29255;&#27573;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#22788;&#29702;&#21512;&#25104;&#35270;&#39057;&#30340;&#20998;&#24067;&#22806;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36741;&#21161;&#30446;&#26631;&#65292;&#29992;&#20110;&#24341;&#20837;&#26356;&#22810;&#22810;&#26679;&#21270;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio-Visual Event Localization (AVEL) is the task of temporally localizing and classifying \emph{audio-visual events}, i.e., events simultaneously visible and audible in a video. In this paper, we solve AVEL in a weakly-supervised setting, where only video-level event labels (their presence/absence, but not their locations in time) are available as supervision for training. Our idea is to use a base model to estimate labels on the training data at a finer temporal resolution than at the video level and re-train the model with these labels. I.e., we determine the subset of labels for each \emph{slice} of frames in a training video by (i) replacing the frames outside the slice with those from a second video having no overlap in video-level labels, and (ii) feeding this synthetic video into the base model to extract labels for just the slice in question. To handle the out-of-distribution nature of our synthetic videos, we propose an auxiliary objective for the base model that induces mor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#37027;&#37324;&#30452;&#25509;&#33719;&#21462;&#21453;&#39304;&#26469;&#35782;&#21035;&#20010;&#24615;&#21270;&#30340;&#26080;&#20851;&#32039;&#35201;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#24182;&#33719;&#24471;&#36866;&#24212;&#20010;&#24615;&#21270;&#29992;&#25143;&#30446;&#26631;&#30340;&#25919;&#31574;&#12290;</title><link>http://arxiv.org/abs/2307.06333</link><description>&lt;p&gt;
&#35786;&#26029;&#12289;&#21453;&#39304;&#12289;&#36866;&#24212;&#24615;: &#29992;&#20110;&#27979;&#35797;&#26102;&#25919;&#31574;&#35843;&#25972;&#30340;&#20154;-&#26426;&#29615;&#36335;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for Test-Time Policy Adaptation. (arXiv:2307.06333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06333
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#37027;&#37324;&#30452;&#25509;&#33719;&#21462;&#21453;&#39304;&#26469;&#35782;&#21035;&#20010;&#24615;&#21270;&#30340;&#26080;&#20851;&#32039;&#35201;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#24182;&#33719;&#24471;&#36866;&#24212;&#20010;&#24615;&#21270;&#29992;&#25143;&#30446;&#26631;&#30340;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25919;&#31574;&#24120;&#24120;&#30001;&#20110;&#20998;&#24067;&#20559;&#31227;&#32780;&#22833;&#25928;&#8212;&#8212;&#21363;&#24403;&#25919;&#31574;&#22312;&#26032;&#29615;&#22659;&#20013;&#37096;&#32626;&#26102;&#65292;&#29366;&#24577;&#21644;&#22870;&#21169;&#21457;&#29983;&#21464;&#21270;&#12290;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#36890;&#36807;&#20351;&#27169;&#22411;&#23545;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#21464;&#21270;&#20855;&#26377;&#19981;&#21464;&#24615;&#26469;&#22686;&#21152;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#32773;&#22312;&#20107;&#20808;&#24448;&#24448;&#19981;&#30693;&#36947;&#21738;&#20123;&#27010;&#24565;&#26159;&#26080;&#20851;&#32039;&#35201;&#30340;&#65292;&#23588;&#20854;&#26159;&#24403;&#19981;&#21516;&#30340;&#26368;&#32456;&#29992;&#25143;&#23545;&#20219;&#21153;&#25191;&#34892;&#26041;&#24335;&#26377;&#19981;&#21516;&#30340;&#20559;&#22909;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20114;&#21160;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#20174;&#29992;&#25143;&#37027;&#37324;&#33719;&#24471;&#21453;&#39304;&#26469;&#35782;&#21035;&#20010;&#24615;&#21270;&#30340;&#26080;&#20851;&#32039;&#35201;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#29983;&#25104;&#21453;&#20107;&#23454;&#28436;&#31034;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#24555;&#36895;&#30830;&#23450;&#21487;&#33021;&#19982;&#20219;&#21153;&#30456;&#20851;&#21644;&#26080;&#20851;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#21033;&#29992;&#26080;&#20851;&#32039;&#35201;&#30340;&#27010;&#24565;&#30340;&#30693;&#35782;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#20174;&#32780;&#33719;&#24471;&#36866;&#24212;&#20110;&#20010;&#24615;&#21270;&#29992;&#25143;&#30446;&#26631;&#30340;&#25919;&#31574;&#12290;&#25105;&#20204;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;(1)&#20351;&#29992;&#25143;&#33021;&#22815;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Policies often fail due to distribution shift -- changes in the state and reward that occur when a policy is deployed in new environments. Data augmentation can increase robustness by making the model invariant to task-irrelevant changes in the agent's observation. However, designers don't know which concepts are irrelevant a priori, especially when different end users have different preferences about how the task is performed. We propose an interactive framework to leverage feedback directly from the user to identify personalized task-irrelevant concepts. Our key idea is to generate counterfactual demonstrations that allow users to quickly identify possible task-relevant and irrelevant concepts. The knowledge of task-irrelevant concepts is then used to perform data augmentation and thus obtain a policy adapted to personalized user objectives. We present experiments validating our framework on discrete and continuous control tasks with real human users. Our method (1) enables users to 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#20845;&#31181;&#22522;&#20934;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#21644;&#19968;&#31181;&#26032;&#25552;&#20986;&#30340;&#22522;&#20110; GFlowNets &#30340;&#26041;&#27861;&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616; GFlowNets &#20855;&#26377;&#25429;&#25417;&#21508;&#31181;&#26377;&#29992;&#21644;&#22810;&#26679;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.04988</link><description>&lt;p&gt;
&#29992;&#20110;&#19979;&#28216;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#36125;&#21494;&#26031;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Bayesian Causal Discovery Methods for Downstream Treatment Effect Estimation. (arXiv:2307.04988v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04988
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#20845;&#31181;&#22522;&#20934;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#21644;&#19968;&#31181;&#26032;&#25552;&#20986;&#30340;&#22522;&#20110; GFlowNets &#30340;&#26041;&#27861;&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616; GFlowNets &#20855;&#26377;&#25429;&#25417;&#21508;&#31181;&#26377;&#29992;&#21644;&#22810;&#26679;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#20013;&#22240;&#26524;&#24615;&#30340;&#23454;&#38469;&#24212;&#29992;&#34987;&#24191;&#27867;&#35748;&#21487;&#65292;&#22240;&#26524;&#21457;&#29616;&#21644;&#25512;&#29702;&#22312;&#26412;&#36136;&#19978;&#26159;&#30456;&#20114;&#20132;&#32455;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#35780;&#20272;&#20013;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#36317;&#65292;&#23545;&#19979;&#28216;&#25512;&#29702;&#30340;&#37325;&#35270;&#31243;&#24230;&#19981;&#36275;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20845;&#31181;&#24050;&#24314;&#31435;&#30340;&#22522;&#20934;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110; GFlowNets &#30340;&#26032;&#26041;&#27861;&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#23454;&#26045;&#19968;&#20010;&#31283;&#20581;&#30340;&#35780;&#20272;&#36807;&#31243;&#65292;&#25105;&#20204;&#20026;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#36825;&#20123;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#32771;&#34385;&#20102;&#21512;&#25104;&#21644;&#30495;&#23454;&#22330;&#26223;&#20197;&#21450;&#20302;&#25968;&#25454;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GFlowNets &#20855;&#26377;&#26377;&#25928;&#25429;&#25417;&#21508;&#31181;&#26377;&#29992;&#21644;&#22810;&#26679;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The practical utility of causality in decision-making is widely recognized, with causal discovery and inference being inherently intertwined. Nevertheless, a notable gap exists in the evaluation of causal discovery methods, where insufficient emphasis is placed on downstream inference. To address this gap, we evaluate six established baseline causal discovery methods and a newly proposed method based on GFlowNets, on the downstream task of treatment effect estimation. Through the implementation of a robust evaluation procedure, we offer valuable insights into the efficacy of these causal discovery methods for treatment effect estimation, considering both synthetic and real-world scenarios, as well as low-data scenarios. Furthermore, the results of our study demonstrate that GFlowNets possess the capability to effectively capture a wide range of useful and diverse ATE modes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;CLIP&#27169;&#22411;&#25552;&#39640;&#35270;&#35273;&#20851;&#31995;&#39044;&#27979;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#22312;UVTransE&#26694;&#26550;&#20013;&#37319;&#29992;&#22522;&#20110;CLIP&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#21450;&#24341;&#20837;&#23545;&#27604;&#35757;&#32451;&#31574;&#30053;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CREPE&#27169;&#22411;&#65292;&#31616;&#21270;&#20102;&#29616;&#26377;&#22797;&#26434;&#30340;&#22270;&#24418;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.04838</link><description>&lt;p&gt;
CREPE&#65306;&#20351;&#29992;CLIP&#30340;&#21487;&#23398;&#20064;&#25552;&#31034;&#25552;&#39640;&#35270;&#35273;&#20851;&#31995;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CREPE: Learnable Prompting With CLIP Improves Visual Relationship Prediction. (arXiv:2307.04838v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;CLIP&#27169;&#22411;&#25552;&#39640;&#35270;&#35273;&#20851;&#31995;&#39044;&#27979;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#22312;UVTransE&#26694;&#26550;&#20013;&#37319;&#29992;&#22522;&#20110;CLIP&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#21450;&#24341;&#20837;&#23545;&#27604;&#35757;&#32451;&#31574;&#30053;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CREPE&#27169;&#22411;&#65292;&#31616;&#21270;&#20102;&#29616;&#26377;&#22797;&#26434;&#30340;&#22270;&#24418;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#65292;&#29305;&#21035;&#26159;CLIP&#65292;&#22312;&#39044;&#27979;&#35270;&#35273;&#30446;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20854;&#20013;&#28041;&#21450;&#23558;&#22270;&#20687;&#30340;&#35270;&#35273;&#29305;&#24449;&#35299;&#37322;&#20026;&#22522;&#20110;&#35821;&#35328;&#30340;&#20851;&#31995;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20351;&#29992;&#22797;&#26434;&#30340;&#22270;&#24418;&#27169;&#22411;&#65292;&#21033;&#29992;&#35821;&#35328;&#32447;&#32034;&#21644;&#35270;&#35273;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#20551;&#35774;CLIP&#23884;&#20837;&#20013;&#30340;&#24378;&#35821;&#35328;&#20808;&#39564;&#21487;&#20197;&#31616;&#21270;&#36825;&#20123;&#22270;&#24418;&#27169;&#22411;&#65292;&#20026;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#38138;&#24179;&#36947;&#36335;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;UVTransE&#20851;&#31995;&#39044;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#22330;&#26223;&#20013;&#30340;&#20027;&#20307;&#12289;&#23458;&#20307;&#21644;&#24182;&#38598;&#26694;&#23884;&#20837;&#26469;&#23398;&#20064;&#20851;&#31995;&#20316;&#20026;&#19968;&#20010;&#24179;&#31227;&#23884;&#20837;&#12290;&#25105;&#20204;&#22312;UVTransE&#26694;&#26550;&#20869;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#22522;&#20110;CLIP&#30340;&#20027;&#20307;&#12289;&#23458;&#20307;&#21644;&#24182;&#38598;&#26694;&#34920;&#31034;&#30340;&#35774;&#35745;&#65292;&#24182;&#25552;&#20986;&#20102;CREPE&#65288;CLIP&#22686;&#24378;&#35859;&#35789;&#20272;&#35745;&#65289;&#12290;CREPE&#21033;&#29992;&#25152;&#26377;&#19977;&#20010;&#36793;&#30028;&#26694;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#33258;&#21160;&#23398;&#20064;&#35270;&#35273;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the potential of Vision-Language Models (VLMs), specifically CLIP, in predicting visual object relationships, which involves interpreting visual features from images into language-based relations. Current state-of-the-art methods use complex graphical models that utilize language cues and visual features to address this challenge. We hypothesize that the strong language priors in CLIP embeddings can simplify these graphical models paving for a simpler approach. We adopt the UVTransE relation prediction framework, which learns the relation as a translational embedding with subject, object, and union box embeddings from a scene. We systematically explore the design of CLIP-based subject, object, and union-box representations within the UVTransE framework and propose CREPE (CLIP Representation Enhanced Predicate Estimation). CREPE utilizes text-based representations for all three bounding boxes and introduces a novel contrastive training strategy to automatically
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#36866;&#24212;&#20154;&#32676;&#22270;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#33041;&#40836;&#20272;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#20154;&#32676;&#22270;&#32467;&#26500;&#65292;&#25552;&#39640;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.04639</link><description>&lt;p&gt;
&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#33258;&#36866;&#24212;&#20154;&#32676;&#22270;&#23398;&#20064;&#36827;&#34892;&#22810;&#27169;&#24577;&#33041;&#40836;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multimodal brain age estimation using interpretable adaptive population-graph learning. (arXiv:2307.04639v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04639
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#36866;&#24212;&#20154;&#32676;&#22270;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#33041;&#40836;&#20272;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#20154;&#32676;&#22270;&#32467;&#26500;&#65292;&#25552;&#39640;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#40836;&#20272;&#35745;&#22312;&#20020;&#24202;&#19978;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#20026;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65288;&#22914;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65289;&#31561;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#20154;&#32676;&#22270;&#21253;&#25324;&#21463;&#35797;&#32773;&#30340;&#22810;&#27169;&#24577;&#25104;&#20687;&#20449;&#24687;&#20197;&#21450;&#20154;&#32676;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#19982;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#19968;&#36215;&#20351;&#29992;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#20854;&#30410;&#22788;&#12290;&#20154;&#32676;&#22270;&#36890;&#24120;&#26159;&#38745;&#24577;&#30340;&#65292;&#24182;&#19988;&#20351;&#29992;&#38750;&#25104;&#20687;&#20449;&#24687;&#25163;&#21160;&#26500;&#24314;&#12290;&#28982;&#32780;&#65292;&#22270;&#30340;&#26500;&#24314;&#24182;&#19981;&#26159;&#19968;&#20010;&#24494;&#19981;&#36275;&#36947;&#30340;&#20219;&#21153;&#65292;&#21487;&#33021;&#20250;&#26174;&#33879;&#24433;&#21709;GCN&#30340;&#24615;&#33021;&#65292;&#32780;GCN&#23545;&#22270;&#30340;&#32467;&#26500;&#38750;&#24120;&#25935;&#24863;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23398;&#20064;&#20102;&#20248;&#21270;&#19979;&#28216;&#20219;&#21153;&#30340;&#20154;&#32676;&#22270;&#32467;&#26500;&#12290;&#19968;&#31181;&#27880;&#24847;&#26426;&#21046;&#20026;&#19968;&#32452;&#25104;&#20687;&#21644;&#38750;&#25104;&#20687;&#29305;&#24449;&#65288;&#34920;&#29616;&#22411;&#65289;&#20998;&#37197;&#26435;&#37325;&#65292;&#28982;&#21518;&#29992;&#20110;&#36793;&#32536;&#25552;&#21462;&#12290;&#29983;&#25104;&#30340;&#22270;&#29992;&#20110;&#35757;&#32451;GCN&#12290;&#25972;&#20010;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
Brain age estimation is clinically important as it can provide valuable information in the context of neurodegenerative diseases such as Alzheimer's. Population graphs, which include multimodal imaging information of the subjects along with the relationships among the population, have been used in literature along with Graph Convolutional Networks (GCNs) and have proved beneficial for a variety of medical imaging tasks. A population graph is usually static and constructed manually using non-imaging information. However, graph construction is not a trivial task and might significantly affect the performance of the GCN, which is inherently very sensitive to the graph structure. In this work, we propose a framework that learns a population graph structure optimized for the downstream task. An attention mechanism assigns weights to a set of imaging and non-imaging features (phenotypes), which are then used for edge extraction. The resulting graph is used to train the GCN. The entire pipeli
&lt;/p&gt;</description></item><item><title>Solvent&#26159;&#19968;&#20010;&#29992;&#20110;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#32479;&#19968;&#30740;&#31350;&#26694;&#26550;&#65292;&#25903;&#25345;&#26368;&#26032;&#27169;&#22411;&#37325;&#35201;&#32452;&#20214;&#30340;&#23454;&#29616;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#24314;&#27169;&#39046;&#22495;&#30340;&#26377;&#29992;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.04603</link><description>&lt;p&gt;
Solvent: &#19968;&#20010;&#29992;&#20110;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Solvent: A Framework for Protein Folding. (arXiv:2307.04603v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04603
&lt;/p&gt;
&lt;p&gt;
Solvent&#26159;&#19968;&#20010;&#29992;&#20110;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#32479;&#19968;&#30740;&#31350;&#26694;&#26550;&#65292;&#25903;&#25345;&#26368;&#26032;&#27169;&#22411;&#37325;&#35201;&#32452;&#20214;&#30340;&#23454;&#29616;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#24314;&#27169;&#39046;&#22495;&#30340;&#26377;&#29992;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33268;&#24615;&#21644;&#21487;&#38752;&#24615;&#23545;&#20110;&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#35768;&#22810;&#33879;&#21517;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22914;&#30446;&#26631;&#26816;&#27979;&#65292;&#24050;&#32463;&#36890;&#36807;&#31283;&#23450;&#30340;&#22522;&#20934;&#26694;&#26550;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#39564;&#35777;&#12290;&#22312;AlphaFold2&#20043;&#21518;&#65292;&#34507;&#30333;&#36136;&#25240;&#21472;&#20219;&#21153;&#24050;&#32463;&#36827;&#20837;&#20102;&#19968;&#20010;&#26032;&#38454;&#27573;&#65292;&#35768;&#22810;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;AlphaFold2&#30340;&#32452;&#20214;&#25552;&#20986;&#30340;&#12290;&#22312;&#34507;&#30333;&#36136;&#25240;&#21472;&#20013;&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#30740;&#31350;&#26694;&#26550;&#30340;&#37325;&#35201;&#24615;&#21253;&#25324;&#23454;&#29616;&#21644;&#22522;&#20934;&#65292;&#20197;&#19968;&#33268;&#19988;&#20844;&#24179;&#22320;&#27604;&#36739;&#21508;&#31181;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Solvent&#65292;&#19968;&#20010;&#25903;&#25345;&#26368;&#26032;&#27169;&#22411;&#37325;&#35201;&#32452;&#20214;&#30340;&#34507;&#30333;&#36136;&#25240;&#21472;&#26694;&#26550;&#12290;Solvent&#21253;&#21547;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#20195;&#30721;&#24211;&#20013;&#23454;&#29616;&#30340;&#19981;&#21516;&#27169;&#22411;&#65292;&#24182;&#25903;&#25345;&#22312;&#30456;&#21516;&#25968;&#25454;&#38598;&#19978;&#23545;&#23450;&#20041;&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;&#33879;&#21517;&#31639;&#27861;&#21450;&#20854;&#32452;&#20214;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#20197;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#24314;&#27169;&#39046;&#22495;&#25552;&#20379;&#26377;&#29992;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#24076;&#26395;Solvent&#33021;&#25552;&#39640;&#34507;&#30333;&#36136;&#25240;&#21472;&#30740;&#31350;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consistency and reliability are crucial for conducting AI research. Many famous research fields, such as object detection, have been compared and validated with solid benchmark frameworks. After AlphaFold2, the protein folding task has entered a new phase, and many methods are proposed based on the component of AlphaFold2. The importance of a unified research framework in protein folding contains implementations and benchmarks to consistently and fairly compare various approaches. To achieve this, we present Solvent, an protein folding framework that supports significant components of state-of-th-arts models in the manner of off-the-shelf interface Solvent contains different models implemented in a unified codebase and supports training and evaluation for defined models on the same dataset. We benchmark well-known algorithms and their components and provide experiments that give helpful insights into the protein structure modeling field. We hope that Solvent will increase the reliabili
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#34892;&#31243;&#26102;&#38388;&#23618;&#26512;&#25104;&#20687;&#26041;&#27861;&#65292;&#21033;&#29992;&#25935;&#24863;&#24615;&#20449;&#24687;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#21644;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#65292;&#20197;&#22788;&#29702;&#22320;&#36136;&#22797;&#26434;&#24615;&#20808;&#39564;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.04228</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#34892;&#31243;&#26102;&#38388;&#23618;&#26512;&#25104;&#20687;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#25935;&#24863;&#24615;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#21644;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#30340;&#22320;&#36136;&#22797;&#26434;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Efficient Bayesian travel-time tomography with geologically-complex priors using sensitivity-informed polynomial chaos expansion and deep generative networks. (arXiv:2307.04228v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#34892;&#31243;&#26102;&#38388;&#23618;&#26512;&#25104;&#20687;&#26041;&#27861;&#65292;&#21033;&#29992;&#25935;&#24863;&#24615;&#20449;&#24687;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#21644;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#65292;&#20197;&#22788;&#29702;&#22320;&#36136;&#22797;&#26434;&#24615;&#20808;&#39564;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33945;&#29305;&#21345;&#27931;&#39532;&#23572;&#21487;&#22827;&#38142;&#65288;MCMC&#65289;&#26041;&#27861;&#24120;&#24120;&#38754;&#20020;&#20004;&#20010;&#26681;&#26412;&#24615;&#25361;&#25112;&#65306;&#20808;&#39564;&#20998;&#24067;&#30340;&#20934;&#30830;&#21051;&#30011;&#21644;&#20284;&#28982;&#20989;&#25968;&#30340;&#39640;&#25928;&#35780;&#20272;&#12290;&#22312;&#23618;&#26512;&#25104;&#20687;&#30340;&#36125;&#21494;&#26031;&#30740;&#31350;&#20013;&#65292;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#26041;&#20415;&#22320;&#23450;&#20041;&#20808;&#39564;&#20998;&#24067;&#65292;&#24182;&#21516;&#26102;&#20511;&#21161;&#22522;&#20110;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#65288;PCE&#65289;&#30340;&#20934;&#30830;&#20195;&#29702;&#27169;&#22411;&#26469;&#26367;&#20195;&#35745;&#31639;&#23494;&#38598;&#30340;&#20840;&#29289;&#29702;&#27491;&#21521;&#27714;&#35299;&#22120;&#12290;&#24403;PCA&#26080;&#27861;&#30452;&#25509;&#25552;&#20379;&#23450;&#20041;&#20808;&#39564;&#20998;&#24067;&#30340;&#26041;&#24335;&#26102;&#65292;&#21487;&#20197;&#37319;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#65289;&#31561;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#20135;&#29983;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;VAE&#30340;&#28508;&#22312;&#21442;&#25968;&#19982;&#27491;&#21521;&#24314;&#27169;&#36755;&#20986;&#20043;&#38388;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#20195;&#29702;&#27169;&#22411;&#26159;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monte Carlo Markov Chain (MCMC) methods commonly confront two fundamental challenges: the accurate characterization of the prior distribution and the efficient evaluation of the likelihood. In the context of Bayesian studies on tomography, principal component analysis (PCA) can in some cases facilitate the straightforward definition of the prior distribution, while simultaneously enabling the implementation of accurate surrogate models based on polynomial chaos expansion (PCE) to replace computationally intensive full-physics forward solvers. When faced with scenarios where PCA does not offer a direct means of easily defining the prior distribution alternative methods like deep generative models (e.g., variational autoencoders (VAEs)), can be employed as viable options. However, accurately producing a surrogate capable of capturing the intricate non-linear relationship between the latent parameters of a VAE and the outputs of forward modeling presents a notable challenge. Indeed, while
&lt;/p&gt;</description></item><item><title>BOF-UCB&#26159;&#19968;&#31181;&#29992;&#20110;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#30340;&#32972;&#26223;&#32447;&#24615;&#36172;&#21338;&#26426;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#39057;&#29575;&#31639;&#27861;&#65292;&#20854;&#32467;&#21512;&#20102;&#36125;&#21494;&#26031;&#21644;&#39057;&#29575;&#23398;&#27966;&#21407;&#21017;&#65292;&#25552;&#39640;&#20102;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#23427;&#21033;&#29992;&#36125;&#21494;&#26031;&#26356;&#26032;&#25512;&#26029;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;&#39057;&#29575;&#23398;&#27966;&#26041;&#27861;&#35745;&#31639;&#19978;&#30028;&#20449;&#24515;&#30028;&#20197;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;BOF-UCB&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#26159;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#39034;&#24207;&#20915;&#31574;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.03587</link><description>&lt;p&gt;
BOF-UCB: &#19968;&#31181;&#29992;&#20110;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#30340;&#19978;&#19979;&#30028;&#20449;&#24515;&#31639;&#27861;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#39057;&#29575;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
BOF-UCB: A Bayesian-Optimistic Frequentist Algorithm for Non-Stationary Contextual Bandits. (arXiv:2307.03587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03587
&lt;/p&gt;
&lt;p&gt;
BOF-UCB&#26159;&#19968;&#31181;&#29992;&#20110;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#30340;&#32972;&#26223;&#32447;&#24615;&#36172;&#21338;&#26426;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#39057;&#29575;&#31639;&#27861;&#65292;&#20854;&#32467;&#21512;&#20102;&#36125;&#21494;&#26031;&#21644;&#39057;&#29575;&#23398;&#27966;&#21407;&#21017;&#65292;&#25552;&#39640;&#20102;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#12290;&#23427;&#21033;&#29992;&#36125;&#21494;&#26031;&#26356;&#26032;&#25512;&#26029;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;&#39057;&#29575;&#23398;&#27966;&#26041;&#27861;&#35745;&#31639;&#19978;&#30028;&#20449;&#24515;&#30028;&#20197;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;BOF-UCB&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#26159;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#39034;&#24207;&#20915;&#31574;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#39057;&#29575;&#19978;&#19979;&#30028;&#20449;&#24515;&#31639;&#27861;&#65288;BOF-UCB&#65289;&#65292;&#29992;&#20110;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#30340;&#38543;&#26426;&#32972;&#26223;&#32447;&#24615;&#36172;&#21338;&#26426;&#12290;&#36125;&#21494;&#26031;&#21644;&#39057;&#29575;&#23398;&#27966;&#21407;&#21017;&#30340;&#29420;&#29305;&#32467;&#21512;&#22686;&#24378;&#20102;&#31639;&#27861;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#12290;BOF-UCB&#31639;&#27861;&#21033;&#29992;&#39034;&#24207;&#36125;&#21494;&#26031;&#26356;&#26032;&#25512;&#26029;&#26410;&#30693;&#22238;&#24402;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#38543;&#21518;&#37319;&#29992;&#39057;&#29575;&#23398;&#27966;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#21518;&#39564;&#20998;&#24067;&#19978;&#30340;&#26399;&#26395;&#25910;&#30410;&#26469;&#35745;&#31639;&#19978;&#30028;&#20449;&#24515;&#30028;&#65288;UCB&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;BOF-UCB&#24615;&#33021;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#32463;&#20856;&#25511;&#21046;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;BOF-UCB&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#36827;&#34892;&#39034;&#24207;&#20915;&#31574;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel Bayesian-Optimistic Frequentist Upper Confidence Bound (BOF-UCB) algorithm for stochastic contextual linear bandits in non-stationary environments. This unique combination of Bayesian and frequentist principles enhances adaptability and performance in dynamic settings. The BOF-UCB algorithm utilizes sequential Bayesian updates to infer the posterior distribution of the unknown regression parameter, and subsequently employs a frequentist approach to compute the Upper Confidence Bound (UCB) by maximizing the expected reward over the posterior distribution. We provide theoretical guarantees of BOF-UCB's performance and demonstrate its effectiveness in balancing exploration and exploitation on synthetic datasets and classical control tasks in a reinforcement learning setting. Our results show that BOF-UCB outperforms existing methods, making it a promising solution for sequential decision-making in non-stationary environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03135</link><description>&lt;p&gt;
&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#24615;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Distilling Large Vision-Language Model with Out-of-Distribution Generalizability. (arXiv:2307.03135v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#35268;&#27169;&#21644;&#35745;&#31639;&#35201;&#27714;&#20351;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#21644;&#26102;&#38388;&#25935;&#24863;&#20219;&#21153;&#19978;&#30340;&#37096;&#32626;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#27169;&#22411;&#21387;&#32553;&#26159;&#21019;&#24314;&#26356;&#23567;&#12289;&#26356;&#24555;&#30340;&#27169;&#22411;&#20197;&#20445;&#25345;&#36739;&#22823;&#27169;&#22411;&#24615;&#33021;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#36731;&#37327;&#32423;&#23398;&#29983;&#27169;&#22411;&#20013;&#30340;&#36807;&#31243;&#65292;&#20351;&#29992;&#23567;&#22411;&#25110;&#20013;&#22411;&#25968;&#25454;&#38598;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#21487;&#27867;&#21270;&#30340;&#24320;&#25918;&#35789;&#27719;&#38382;&#39064;&#65292;&#36825;&#22312;&#20197;&#24448;&#30340;&#27169;&#22411;&#21387;&#32553;&#30740;&#31350;&#20013;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#20174;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;OOD&#21487;&#27867;&#21270;&#24615;&#65306;&#65288;1&#65289;&#26356;&#22909;&#22320;&#27169;&#20223;&#25945;&#24072;&#30340;&#35270;&#35273;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#22312;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#26041;&#38754;&#35880;&#24910;&#22320;&#20419;&#36827;&#26356;&#22909;&#30340;&#19968;&#33268;&#24615;&#65307;&#65288;2&#65289;&#36890;&#36807;&#20016;&#23500;&#23398;&#29983;&#27169;&#22411;&#30340;&#33258;&#20030;&#23398;&#20064;&#21644;&#25968;&#25454;&#25193;&#20805;&#26469;&#25552;&#39640;OOD&#21487;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large vision-language models have achieved outstanding performance, but their size and computational requirements make their deployment on resource-constrained devices and time-sensitive tasks impractical. Model distillation, the process of creating smaller, faster models that maintain the performance of larger models, is a promising direction towards the solution. This paper investigates the distillation of visual representations in large teacher vision-language models into lightweight student models using a smallor mid-scale dataset. Notably, this study focuses on open-vocabulary out-of-distribution (OOD) generalization, a challenging problem that has been overlooked in previous model distillation literature. We propose two principles from vision and language modality perspectives to enhance student's OOD generalization: (1) by better imitating teacher's visual representation space, and carefully promoting better coherence in vision-language alignment with the teacher; (2) by enric
&lt;/p&gt;</description></item><item><title>LongNet&#26159;&#19968;&#31181;&#21487;&#20197;&#25193;&#23637;&#21040;10&#20159;&#20010;&#26631;&#35760;&#30340;Transformer&#21464;&#20307;&#65292;&#36890;&#36807;&#25193;&#24352;&#27880;&#24847;&#21147;&#35299;&#20915;&#20102;&#24207;&#21015;&#38271;&#24230;&#21463;&#38480;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#32447;&#24615;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#23545;&#25968;&#20381;&#36182;&#20851;&#31995;&#65292;&#21487;&#20197;&#20316;&#20026;&#20998;&#24067;&#24335;&#35757;&#32451;&#22120;&#20351;&#29992;&#24182;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;Transformer&#20248;&#21270;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;LongNet&#22312;&#38271;&#24207;&#21015;&#21644;&#30701;&#24207;&#21015;&#19978;&#24615;&#33021;&#24378;&#22823;&#12290;</title><link>http://arxiv.org/abs/2307.02486</link><description>&lt;p&gt;
LongNet: &#23558;Transformer&#25193;&#23637;&#21040;10&#20159;&#20010;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
LongNet: Scaling Transformers to 1,000,000,000 Tokens. (arXiv:2307.02486v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02486
&lt;/p&gt;
&lt;p&gt;
LongNet&#26159;&#19968;&#31181;&#21487;&#20197;&#25193;&#23637;&#21040;10&#20159;&#20010;&#26631;&#35760;&#30340;Transformer&#21464;&#20307;&#65292;&#36890;&#36807;&#25193;&#24352;&#27880;&#24847;&#21147;&#35299;&#20915;&#20102;&#24207;&#21015;&#38271;&#24230;&#21463;&#38480;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#32447;&#24615;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#23545;&#25968;&#20381;&#36182;&#20851;&#31995;&#65292;&#21487;&#20197;&#20316;&#20026;&#20998;&#24067;&#24335;&#35757;&#32451;&#22120;&#20351;&#29992;&#24182;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;Transformer&#20248;&#21270;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;LongNet&#22312;&#38271;&#24207;&#21015;&#21644;&#30701;&#24207;&#21015;&#19978;&#24615;&#33021;&#24378;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#20195;&#65292;&#25193;&#23637;&#24207;&#21015;&#38271;&#24230;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#25110;&#27169;&#22411;&#34920;&#36798;&#21147;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#23548;&#33268;&#24207;&#21015;&#38271;&#24230;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LongNet&#65292;&#23427;&#26159;&#19968;&#31181;Transformer&#30340;&#21464;&#20307;&#65292;&#21487;&#20197;&#23558;&#24207;&#21015;&#38271;&#24230;&#25193;&#23637;&#21040;10&#20159;&#20010;&#26631;&#35760;&#20197;&#19978;&#65292;&#32780;&#19981;&#20250;&#29306;&#29298;&#23545;&#36739;&#30701;&#24207;&#21015;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#24352;&#27880;&#24847;&#21147;&#65292;&#38543;&#30528;&#36317;&#31163;&#30340;&#22686;&#22823;&#65292;&#23427;&#23558;&#27880;&#24847;&#33539;&#22260;&#25351;&#25968;&#32423;&#25193;&#23637;&#12290;LongNet&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#65306;1&#65289;&#23427;&#20855;&#26377;&#32447;&#24615;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#24207;&#21015;&#20013;&#20219;&#24847;&#20004;&#20010;&#26631;&#35760;&#20043;&#38388;&#30340;&#23545;&#25968;&#20381;&#36182;&#20851;&#31995;&#65307;2&#65289;&#23427;&#21487;&#20197;&#20316;&#20026;&#29992;&#20110;&#26497;&#38271;&#24207;&#21015;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#22120;&#65307;3&#65289;&#23427;&#30340;&#25193;&#24352;&#27880;&#24847;&#21147;&#26159;&#26631;&#20934;&#27880;&#24847;&#21147;&#30340;&#21363;&#25554;&#21363;&#29992;&#26367;&#20195;&#21697;&#65292;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#30340;&#20248;&#21270;&#26080;&#32541;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;LongNet&#22312;&#38271;&#24207;&#21015;&#21644;&#30701;&#24207;&#21015;&#19978;&#37117;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. To address this issue, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between any two tokens in a sequence; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LongNet yields strong performance on both long-se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;SwinGNN&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#25928;&#30340;2-WL&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#21644;&#31227;&#21160;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#32467;&#21512;&#20851;&#38190;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#38543;&#26426;&#32622;&#25442;&#30340;&#21518;&#22788;&#29702;&#25216;&#24039;&#36716;&#25442;&#29983;&#25104;&#30340;&#22270;&#24418;&#32479;&#35745;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.01646</link><description>&lt;p&gt;
SwinGNN:&#37325;&#26032;&#24605;&#32771;&#22312;&#22270;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation. (arXiv:2307.01646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;SwinGNN&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#25928;&#30340;2-WL&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#21644;&#31227;&#21160;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#32467;&#21512;&#20851;&#38190;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#38543;&#26426;&#32622;&#25442;&#30340;&#21518;&#22788;&#29702;&#25216;&#24039;&#36716;&#25442;&#29983;&#25104;&#30340;&#22270;&#24418;&#32479;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32622;&#25442;&#31561;&#21464;&#32593;&#32476;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#22270;&#25968;&#25454;&#30340;&#32622;&#25442;&#19981;&#21464;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#30456;&#23545;&#20110;&#38750;&#19981;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#19981;&#21464;&#27169;&#22411;&#36935;&#21040;&#20102;&#26356;&#22823;&#30340;&#23398;&#20064;&#25361;&#25112;&#65292;&#22240;&#20026;1&#65289;&#23427;&#20204;&#30340;&#30446;&#26631;&#20998;&#24067;&#26356;&#20855;&#27169;&#24577;&#24615;&#65307;2&#65289;&#23427;&#20204;&#30340;&#26368;&#20248;&#19968;&#27493;&#21435;&#22122;&#24471;&#20998;&#26159;&#20855;&#26377;&#26356;&#22810;&#25104;&#20998;&#30340;&#39640;&#26031;&#28151;&#21512;&#29289;&#30340;&#24471;&#20998;&#20989;&#25968;&#12290;&#21463;&#21040;&#36825;&#20010;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#19981;&#21464;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;&#8220;SwinGNN&#8221;&#65292;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36793;&#21040;&#36793;&#30340;2-WL&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;SwinTransformers&#20013;&#30340;&#31227;&#21160;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#31995;&#32479;&#24615;&#30340;&#23454;&#39564;&#21644;&#21078;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#31181;&#20851;&#38190;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21518;&#22788;&#29702;&#25216;&#24039;&#65292;&#21363;&#38543;&#26426;&#32622;&#25442;&#29983;&#25104;&#30340;&#22270;&#65292;&#21487;&#20197;&#35777;&#26126;&#23558;&#20219;&#20309;&#22270;&#36716;&#25442;&#25104;&#22270;&#24418;&#32479;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models based on permutation-equivariant networks can learn permutation-invariant distributions for graph data. However, in comparison to their non-invariant counterparts, we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes; 2) their optimal one-step denoising scores are the score functions of Gaussian mixtures with more components. Motivated by this analysis, we propose a non-invariant diffusion model, called $\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention inspired by SwinTransformers. Further, through systematic ablations, we identify several critical training and sampling techniques that significantly improve the sample quality of graph generation. At last, we introduce a simple post-processing trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably converts any graph ge
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#24314;&#31435;&#35821;&#20041;&#26377;&#24847;&#20041;&#12289;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#20449;&#24565;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#39044;&#27979;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#20449;&#24565;&#26469;&#20316;&#20026;&#20869;&#22312;&#22870;&#21169;&#20449;&#21495;&#65292;&#21487;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#20135;&#29983;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.01158</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#24515;&#29702;&#25512;&#29702;&#20316;&#20026;&#20869;&#22312;&#21160;&#26426;&#30340;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind as Intrinsic Motivation for Multi-Agent Reinforcement Learning. (arXiv:2307.01158v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#24314;&#31435;&#35821;&#20041;&#26377;&#24847;&#20041;&#12289;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#20449;&#24565;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#39044;&#27979;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#20449;&#24565;&#26469;&#20316;&#20026;&#20869;&#22312;&#22870;&#21169;&#20449;&#21495;&#65292;&#21487;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#20135;&#29983;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#20182;&#20154;&#20869;&#24515;&#29366;&#24577;&#23545;&#20110;&#20154;&#31867;&#30340;&#31038;&#20250;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26469;&#35828;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#20063;&#21487;&#20197;&#25552;&#20379;&#31867;&#20284;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#32593;&#32476;&#27169;&#22411;&#26469;&#24314;&#31435;&#35821;&#20041;&#26377;&#24847;&#20041;&#12289;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#20449;&#24565;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20108;&#38454;&#20449;&#24565;&#39044;&#27979;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#33021;&#22815;&#39044;&#27979;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#20449;&#24565;&#30340;&#33021;&#21147;&#21487;&#20197;&#20316;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#20869;&#22312;&#22870;&#21169;&#20449;&#21495;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#28151;&#21512;&#30340;&#21512;&#20316;&#31454;&#20105;&#29615;&#22659;&#20013;&#21576;&#29616;&#20102;&#21021;&#27493;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to model the mental states of others is crucial to human social intelligence, and can offer similar benefits to artificial agents with respect to the social dynamics induced in multi-agent settings. We present a method of grounding semantically meaningful, human-interpretable beliefs within policies modeled by deep networks. We then consider the task of 2nd-order belief prediction. We propose that ability of each agent to predict the beliefs of the other agents can be used as an intrinsic reward signal for multi-agent reinforcement learning. Finally, we present preliminary empirical results in a mixed cooperative-competitive environment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39044;&#27979;&#25991;&#26412;&#20013;&#30340;&#28909;&#38376;&#20803;&#32032;&#26469;&#20943;&#23569;GPU&#20869;&#23384;&#28040;&#32791;&#30340;&#26041;&#27861;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.14048</link><description>&lt;p&gt;
H$_2$O: &#39640;&#25928;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28909;&#38376;&#20803;&#32032;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models. (arXiv:2306.14048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39044;&#27979;&#25991;&#26412;&#20013;&#30340;&#28909;&#38376;&#20803;&#32032;&#26469;&#20943;&#23569;GPU&#20869;&#23384;&#28040;&#32791;&#30340;&#26041;&#27861;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#26368;&#36817;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;, &#20294;&#26159;&#30001;&#20110;&#25104;&#26412;&#36807;&#39640;&#65292;&#23427;&#20204;&#29305;&#21035;&#38590;&#20197;&#29992;&#20110;&#23545;&#35805;&#31995;&#32479;&#21644;&#25925;&#20107;&#21019;&#20316;&#31561;&#38656;&#35201;&#29983;&#25104;&#38271;&#20869;&#23481;&#30340;&#24212;&#29992;&#12290;&#38500;&#20102;&#27169;&#22411;&#21442;&#25968;&#22806;&#65292;&#36890;&#24120;&#36824;&#38656;&#35201;&#22312;GPU&#20869;&#23384;&#20013;&#23384;&#20648;&#22823;&#37327;&#20020;&#26102;&#29366;&#24577;&#20449;&#24687;&#65292;&#31216;&#20026;KV cache&#65292;&#23427;&#19982;&#24207;&#21015;&#38271;&#24230;&#21644;&#25209;&#37327;&#22823;&#23567;&#21576;&#32447;&#24615;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;, &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#29616;KV cache&#30340;&#26041;&#27861;&#65292;&#23427;&#26174;&#33879;&#22320;&#20943;&#23569;&#20102;&#20854;&#20869;&#23384;&#21344;&#29992;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#21457;&#29616;&#65292;&#21363;&#22312;&#35745;&#31639;&#27880;&#24847;&#21147;&#20998;&#25968;&#26102;&#65292;&#23567;&#37096;&#20998;&#26631;&#35760;&#36129;&#29486;&#26368;&#22823;&#20215;&#20540;&#12290;&#25105;&#20204;&#31216;&#36825;&#20123;&#26631;&#35760;&#20026;&#28909;&#38376;&#20803;&#32032;(H$_2$)&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;(i) H$_2$&#30340;&#20986;&#29616;&#26159;&#33258;&#28982;&#32780;&#28982;&#30340;&#65292;&#24182;&#19988;&#19982;&#25991;&#26412;&#20013;&#26631;&#35760;&#30340;&#39057;&#32321;&#20849;&#29616;&#24378;&#30456;&#20851;&#65307;(ii)&#21435;&#38500;&#23427;&#20204;&#20250;&#23548;&#33268;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;H$_2$O&#65292;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;LLM&#30340;&#28909;&#38376;&#20803;&#32032;&#39044;&#27979;&#22120;&#12290;H$_2$O&#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#32473;&#23450;&#24207;&#21015;&#20013;&#30340;&#28909;&#38376;&#20803;&#32032;&#65292;&#24182;&#22240;&#27492;&#20445;&#25345;&#19968;&#20010;&#26356;&#23567;&#30340;KV cache,&#23558;GPU&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#20102;50%&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;,H$_2$O&#21487;&#20197;&#22312;&#26174;&#33879;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#19982;&#23436;&#25972;KV cache&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the KV cache which significantly reduces its memory footprint. Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores. We call these tokens Heavy Hitters (H$_2$). Through a comprehensive investigation, we find that (i) the emergence of H$_2$ is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and (ii) removing them results in significant performance degradation. Based on these insi
&lt;/p&gt;</description></item><item><title>&#22312;Gradient-based Attribution Methods&#20013;&#65292;&#20351;&#29992;Pre Softmax&#20998;&#25968;&#25110;Post Softmax&#20998;&#25968;&#30340;&#26799;&#24230;&#30340;&#36873;&#25321;&#26377;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#65292;&#38656;&#35201;&#26681;&#25454;&#20855;&#20307;&#24773;&#20917;&#36827;&#34892;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.13197</link><description>&lt;p&gt;
Gradient-based Attribution Methods&#20013;Pre&#25110;Post-Softmax Scores&#65292;&#21738;&#20010;&#26356;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
Pre or Post-Softmax Scores in Gradient-based Attribution Methods, What is Best?. (arXiv:2306.13197v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13197
&lt;/p&gt;
&lt;p&gt;
&#22312;Gradient-based Attribution Methods&#20013;&#65292;&#20351;&#29992;Pre Softmax&#20998;&#25968;&#25110;Post Softmax&#20998;&#25968;&#30340;&#26799;&#24230;&#30340;&#36873;&#25321;&#26377;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#65292;&#38656;&#35201;&#26681;&#25454;&#20855;&#20307;&#24773;&#20917;&#36827;&#34892;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24037;&#20316;&#20316;&#20026;&#20998;&#31867;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#24402;&#22240;&#26041;&#27861;&#20351;&#29992;&#32593;&#32476;&#20998;&#25968;&#30340;&#26799;&#24230;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35752;&#35770;&#20351;&#29992;Pre Softmax&#20998;&#25968;&#21644;Post Softmax&#20998;&#25968;&#30340;&#26799;&#24230;&#20043;&#38388;&#30340;&#23454;&#38469;&#24046;&#24322;&#20197;&#21450;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient based attribution methods for neural networks working as classifiers use gradients of network scores. Here we discuss the practical differences between using gradients of pre-softmax scores versus post-softmax scores, and their respective advantages and disadvantages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32500;RL&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#35813;&#27169;&#22411;&#30340;&#20856;&#22411;&#21160;&#21147;&#23398;&#20026;&#19968;&#32452;&#38381;&#24335;ODE&#26041;&#31243;&#32452;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#19982;&#31070;&#32463;RL&#20195;&#29702;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#25429;&#25417;&#20102;&#29616;&#23454;&#19990;&#30028;RL&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.10404</link><description>&lt;p&gt;
RL&#24863;&#30693;&#26426;&#65306;&#39640;&#32500;&#31574;&#30053;&#23398;&#20064;&#30340;&#27867;&#21270;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
The RL Perceptron: Generalisation Dynamics of Policy Learning in High Dimensions. (arXiv:2306.10404v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32500;RL&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#35813;&#27169;&#22411;&#30340;&#20856;&#22411;&#21160;&#21147;&#23398;&#20026;&#19968;&#32452;&#38381;&#24335;ODE&#26041;&#31243;&#32452;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#19982;&#31070;&#32463;RL&#20195;&#29702;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#25429;&#25417;&#20102;&#29616;&#23454;&#19990;&#30028;RL&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#20855;&#26377;&#21464;&#38761;&#24615;&#65292;&#20026;&#20102;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#31995;&#32479;&#36890;&#24120;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#20174;&#20687;&#32032;&#25110;&#20854;&#20182;&#39640;&#32500;&#24863;&#23448;&#36755;&#20837;&#20013;&#23398;&#20064;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#37117;&#38598;&#20013;&#20110;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#25110;&#26368;&#22351;&#24773;&#20917;&#20998;&#26512;&#65292;&#20851;&#20110;&#39640;&#32500;&#24773;&#20917;&#19979;&#31574;&#30053;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#22522;&#26412;&#38382;&#39064;&#20173;&#26377;&#24453;&#35299;&#20915;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#30340;&#39640;&#32500;RL&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#25429;&#25417;&#22810;&#31181;&#23398;&#20064;&#21327;&#35758;&#65292;&#24182;&#23558;&#20854;&#20856;&#22411;&#21160;&#21147;&#23398;&#23548;&#20986;&#20026;&#19968;&#32452;&#38381;&#24335;&#24120;&#24494;&#20998;&#26041;&#31243;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#26368;&#20339;&#30340;&#23398;&#20064;&#29575;&#21644;&#20219;&#21153;&#38590;&#24230;&#35843;&#24230;&#65292;&#31867;&#20284;&#20110;&#35757;&#32451;&#20013;&#30340;&#36864;&#28779;&#26041;&#26696;&#21644;&#35838;&#31243;&#34920;&#65292;&#24182;&#34920;&#26126;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#20016;&#23500;&#30340;&#34892;&#20026;&#65292;&#21253;&#25324;&#22312;&#31232;&#30095;&#22870;&#21169;&#19979;&#30340;&#24310;&#36831;&#23398;&#20064;&#65307;&#26681;&#25454;&#22870;&#21169;&#22522;&#32447;&#19981;&#21516;&#30340;&#21508;&#31181;&#23398;&#20064;&#26041;&#26696;&#65307;&#20197;&#21450;&#30001;&#22870;&#21169;&#20005;&#26684;&#31243;&#24230;&#39537;&#21160;&#30340;&#36895;&#24230;-&#20934;&#30830;&#24230;&#26435;&#34913;&#12290;&#19982;&#31070;&#32463;RL&#20195;&#29702;&#36827;&#34892;&#30340;&#23454;&#39564;&#27604;&#36739;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#25429;&#25417;&#20102;&#29616;&#23454;&#19990;&#30028;RL&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) algorithms have proven transformative in a range of domains. To tackle real-world domains, these systems often use neural networks to learn policies directly from pixels or other high-dimensional sensory input. By contrast, much theory of RL has focused on discrete state spaces or worst-case analysis, and fundamental questions remain about the dynamics of policy learning in high-dimensional settings. Here, we propose a solvable high-dimensional model of RL that can capture a variety of learning protocols, and derive its typical dynamics as a set of closed-form ordinary differential equations (ODEs). We derive optimal schedules for the learning rates and task difficulty - analogous to annealing schemes and curricula during training in RL - and show that the model exhibits rich behaviour, including delayed learning under sparse rewards; a variety of learning regimes depending on reward baselines; and a speed-accuracy trade-off driven by reward stringency. Expe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#39640;&#32500;&#29983;&#25104;&#27169;&#22411;&#27979;&#24230;&#20013;&#20351;&#29992;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#25351;&#26631;&#23384;&#22312;&#19981;&#23545;&#31216;&#24615;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#35823;&#23548;&#24615;&#32467;&#35770;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#20462;&#27491;&#36825;&#31181;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2306.09618</link><description>&lt;p&gt;
&#39640;&#32500;&#29983;&#25104;&#27169;&#22411;&#27979;&#24230;&#20013;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#30340;&#19981;&#23545;&#31216;&#24615;&#65306;&#34913;&#37327;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#30340;&#20004;&#20010;&#37325;&#35201;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Emergent Asymmetry of Precision and Recall for Measuring Fidelity and Diversity of Generative Models in High Dimensions. (arXiv:2306.09618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#39640;&#32500;&#29983;&#25104;&#27169;&#22411;&#27979;&#24230;&#20013;&#20351;&#29992;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#25351;&#26631;&#23384;&#22312;&#19981;&#23545;&#31216;&#24615;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#35823;&#23548;&#24615;&#32467;&#35770;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#20462;&#27491;&#36825;&#31181;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#24230;&#21644;&#21484;&#22238;&#26159;&#34913;&#37327;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#30340;&#20004;&#20010;&#37325;&#35201;&#25351;&#26631;&#65292;&#23427;&#20204;&#20998;&#21035;&#34987;&#29992;&#26469;&#27979;&#37327;&#27169;&#22411;&#30340;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;k&#36817;&#37051;&#30340;&#24120;&#35265;&#36924;&#36817;&#26041;&#27861;&#65292;&#36825;&#20123;&#25351;&#26631;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#23481;&#26131;&#20986;&#29616;&#35823;&#23548;&#24615;&#32467;&#35770;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#35777;&#26126;&#20102;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#65292;&#20004;&#20010;&#19982;&#30495;&#23454;&#20998;&#24067;&#30340;&#25903;&#25345;&#31561;&#36317;&#31163;&#30340;&#27169;&#22411;&#20998;&#24067;&#21487;&#20197;&#20855;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#23545;&#31216;&#24615;&#12290;&#38024;&#23545;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#31616;&#21333;&#30340;&#20462;&#27491;&#26041;&#27861;&#26469;&#28040;&#38500;&#36825;&#31181;&#38169;&#35823;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precision and Recall are two prominent metrics of generative performance, which were proposed to separately measure the fidelity and diversity of generative models. Given their central role in comparing and improving generative models, understanding their limitations are crucially important. To that end, in this work, we identify a critical flaw in the common approximation of these metrics using k-nearest-neighbors, namely, that the very interpretations of fidelity and diversity that are assigned to Precision and Recall can fail in high dimensions, resulting in very misleading conclusions. Specifically, we empirically and theoretically show that as the number of dimensions grows, two model distributions with supports at equal point-wise distance from the support of the real distribution, can have vastly different Precision and Recall regardless of their respective distributions, hence an emergent asymmetry in high dimensions. Based on our theoretical insights, we then provide simple ye
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#35745;&#31639;&#26377;&#25928;$p$-&#38459;&#25239;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22810;&#31867;&#22270;&#32858;&#31867;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#21442;&#25968;$p$&#20559;&#21521;&#20110;&#20855;&#26377;&#39640;&#20869;&#37096;&#36830;&#36890;&#24615;&#25110;&#32773;&#26356;&#23567;&#30340;&#31751;&#20869;&#39030;&#28857;&#20043;&#38388;&#30340;&#26368;&#30701;&#36335;&#24452;&#36317;&#31163;&#30340;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2306.08617</link><description>&lt;p&gt;
&#22522;&#20110;&#36817;&#20284;&#26377;&#25928;&#30340;$p$-&#38459;&#25239;&#30340;&#22810;&#31867;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-class Graph Clustering via Approximated Effective $p$-Resistance. (arXiv:2306.08617v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#35745;&#31639;&#26377;&#25928;$p$-&#38459;&#25239;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22810;&#31867;&#22270;&#32858;&#31867;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#21442;&#25968;$p$&#20559;&#21521;&#20110;&#20855;&#26377;&#39640;&#20869;&#37096;&#36830;&#36890;&#24615;&#25110;&#32773;&#26356;&#23567;&#30340;&#31751;&#20869;&#39030;&#28857;&#20043;&#38388;&#30340;&#26368;&#30701;&#36335;&#24452;&#36317;&#31163;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36817;&#20284;&#35745;&#31639;&#26377;&#25928;$p$-&#38459;&#25239;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22810;&#31867;&#32858;&#31867;&#12290;&#22522;&#20110;&#22270;&#25289;&#26222;&#25289;&#26031;&#21644;&#20854;$p$-&#25289;&#26222;&#25289;&#26031;&#25512;&#24191;&#30340;&#35889;&#26041;&#27861;&#19968;&#30452;&#26159;&#38750;&#27431;&#20960;&#37324;&#24471;&#32858;&#31867;&#25216;&#26415;&#30340;&#25903;&#26609;&#12290;$p$-&#25289;&#26222;&#25289;&#26031;&#30340;&#20248;&#28857;&#22312;&#20110;&#21442;&#25968;$p$&#23545;&#32858;&#31867;&#32467;&#26500;&#20855;&#26377;&#21487;&#25511;&#20559;&#20506;&#12290;$p$-&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#27861;&#30340;&#32570;&#28857;&#22312;&#20110;&#38590;&#20197;&#35745;&#31639;&#31532;&#19977;&#21644;&#26356;&#39640;&#38454;&#29305;&#24449;&#21521;&#37327;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21160;&#26426;&#22312;&#20110;&#20351;&#29992;&#30001;$p$-&#25289;&#26222;&#25289;&#26031;&#24341;&#23548;&#30340;$p$-&#38459;&#25239;&#36827;&#34892;&#32858;&#31867;&#12290;&#23545;&#20110;$p$-&#38459;&#25239;&#32780;&#35328;&#65292;&#23567;$p$&#20250;&#20559;&#21521;&#20110;&#20855;&#26377;&#39640;&#20869;&#37096;&#36830;&#36890;&#24615;&#30340;&#32858;&#31867;&#65292;&#32780;&#22823;$p$&#21017;&#20250;&#20559;&#21521;&#20110;&#22823;&#23567;&#8220;&#33539;&#22260;&#8221;&#30340;&#32858;&#31867;&#65292;&#21363;&#26356;&#23567;&#30340;&#31751;&#20869;&#39030;&#28857;&#20043;&#38388;&#30340;&#26368;&#30701;&#36335;&#24452;&#36317;&#31163;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;$p$-&#38459;&#25239;&#25104;&#26412;&#24456;&#39640;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;$p$-&#38459;&#25239;&#30340;&#36817;&#20284;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19978;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper develops an approximation to the (effective) $p$-resistance and applies it to multi-class clustering. Spectral methods based on the graph Laplacian and its generalization to the graph $p$-Laplacian have been a backbone of non-euclidean clustering techniques. The advantage of the $p$-Laplacian is that the parameter $p$ induces a controllable bias on cluster structure. The drawback of $p$-Laplacian eigenvector based methods is that the third and higher eigenvectors are difficult to compute. Thus, instead, we are motivated to use the $p$-resistance induced by the $p$-Laplacian for clustering. For $p$-resistance, small $p$ biases towards clusters with high internal connectivity while large $p$ biases towards clusters of small ``extent,'' that is a preference for smaller shortest-path distances between vertices in the cluster. However, the $p$-resistance is expensive to compute. We overcome this by developing an approximation to the $p$-resistance. We prove upper and lower bounds
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36817;&#37051;&#23884;&#20837;&#26041;&#27861;&#26469;&#20272;&#35745;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20256;&#26579;&#25928;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ProEmb&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#21644;&#23545;&#25239;&#32593;&#32476;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#29983;&#25104;&#39640;&#32500;&#20195;&#29702;&#21464;&#37327;&#30340;&#24179;&#34913;&#20302;&#32500;&#34920;&#31034;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#26579;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.02479</link><description>&lt;p&gt;
&#20351;&#29992;&#36817;&#37051;&#23884;&#20837;&#20272;&#35745;&#20256;&#26579;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Contagion Effect Estimation Using Proximal Embeddings. (arXiv:2306.02479v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36817;&#37051;&#23884;&#20837;&#26041;&#27861;&#26469;&#20272;&#35745;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20256;&#26579;&#25928;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ProEmb&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#21644;&#23545;&#25239;&#32593;&#32476;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#29983;&#25104;&#39640;&#32500;&#20195;&#29702;&#21464;&#37327;&#30340;&#24179;&#34913;&#20302;&#32500;&#34920;&#31034;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#26579;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#26579;&#25928;&#24212;&#25351;&#30340;&#26159;&#31038;&#20132;&#32593;&#32476;&#20013;&#21516;&#20276;&#34892;&#20026;&#23545;&#20010;&#20307;&#32467;&#26524;&#30340;&#22240;&#26524;&#24433;&#21709;&#12290;&#22312;&#35266;&#23519;&#30740;&#31350;&#20013;&#20272;&#35745;&#20256;&#26579;&#25928;&#24212;&#30340;&#26174;&#33879;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#27809;&#26377;&#26410;&#27979;&#37327;&#30340;&#28151;&#26434;&#22240;&#32032;&#65292;&#20294;&#30001;&#20110;&#28508;&#22312;&#30340;&#21516;&#36136;&#24615;&#65292;&#20256;&#26579;&#21487;&#33021;&#20250;&#34987;&#28151;&#28102;&#65306;&#21516;&#36136;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#20542;&#21521;&#20110;&#19982;&#20855;&#26377;&#30456;&#20284;&#23646;&#24615;&#30340;&#21516;&#20276;&#24314;&#31435;&#32852;&#31995;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#20114;&#30456;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#30456;&#20284;&#30340;&#34892;&#20026;&#12290;&#35299;&#20915;&#28508;&#22312;&#21516;&#36136;&#24615;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#32771;&#34385;&#26410;&#35266;&#23519;&#28151;&#26434;&#22240;&#32032;&#30340;&#20195;&#29702;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#39640;&#32500;&#20195;&#29702;&#21464;&#37327;&#26102;&#65292;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#20256;&#26579;&#25928;&#24212;&#20272;&#35745;&#30340;&#20005;&#37325;&#20559;&#24046;&#65292;&#27491;&#22914;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#28436;&#31034;&#30340;&#37027;&#26679;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#36817;&#37051;&#23884;&#20837;&#65288;ProEmb&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#21644;&#23545;&#25239;&#32593;&#32476;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#29983;&#25104;&#19981;&#21516;&#22788;&#29702;&#32452;&#39640;&#32500;&#20195;&#29702;&#21464;&#37327;&#30340;&#24179;&#34913;&#20302;&#32500;&#34920;&#31034;&#65292;&#24182;&#19988;&#22312;&#22240;&#26524;&#25512;&#35770;&#20013;&#32771;&#34385;&#20102;&#20256;&#26579;&#25928;&#24212;&#30340;&#20272;&#35745;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contagion effect refers to the causal effect of peers' behavior on the outcome of an individual in social networks. While prominent methods for estimating contagion effects in observational studies often assume that there are no unmeasured confounders, contagion can be confounded due to latent homophily: nodes in a homophilous network tend to have ties to peers with similar attributes and can behave similarly without influencing one another. One way to account for latent homophily is by considering proxies for the unobserved confounders. However, in the presence of high-dimensional proxies, proxy-based methods can lead to substantially biased estimation of contagion effects, as we demonstrate in this paper. To tackle this issue, we introduce the novel Proximal Embeddings (ProEmb), a framework which integrates Variational Autoencoders (VAEs) and adversarial networks to generate balanced low-dimensional representations of high-dimensional proxies for different treatment groups and identi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#27169;&#22411;&#26469;&#35299;&#20915;&#30693;&#35782;&#36861;&#36394;&#20013;&#30340;&#22240;&#26524;&#21457;&#29616;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#29983;&#21453;&#24212;&#25968;&#25454;&#25214;&#21040;&#19981;&#21516;&#25216;&#33021;&#20043;&#38388;&#30340;&#28508;&#22312;&#22240;&#26524;&#20851;&#31995;&#12290;&#35813;&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#22240;&#26524;&#38376;&#24490;&#29615;&#21333;&#20803;&#27169;&#22359;&#65292;&#24182;&#20351;&#29992;&#20102;&#21487;&#23398;&#20064;&#30340;&#32622;&#25442;&#30697;&#38453;&#21644;&#19979;&#19977;&#35282;&#30697;&#38453;&#26469;&#34920;&#31034;&#22240;&#26524;&#39034;&#24207;&#21644;&#22240;&#26524;&#32467;&#26500;&#12290;&#22312;NeurIPS 2022&#25361;&#25112;&#36187;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#25104;&#32489;&#12290;</title><link>http://arxiv.org/abs/2305.16165</link><description>&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#20013;&#31471;&#21040;&#31471;&#22240;&#26524;&#21457;&#29616;&#30340;&#27010;&#24565;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Conceptual Model for End-to-End Causal Discovery in Knowledge Tracing. (arXiv:2305.16165v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#27169;&#22411;&#26469;&#35299;&#20915;&#30693;&#35782;&#36861;&#36394;&#20013;&#30340;&#22240;&#26524;&#21457;&#29616;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#29983;&#21453;&#24212;&#25968;&#25454;&#25214;&#21040;&#19981;&#21516;&#25216;&#33021;&#20043;&#38388;&#30340;&#28508;&#22312;&#22240;&#26524;&#20851;&#31995;&#12290;&#35813;&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#20010;&#22240;&#26524;&#38376;&#24490;&#29615;&#21333;&#20803;&#27169;&#22359;&#65292;&#24182;&#20351;&#29992;&#20102;&#21487;&#23398;&#20064;&#30340;&#32622;&#25442;&#30697;&#38453;&#21644;&#19979;&#19977;&#35282;&#30697;&#38453;&#26469;&#34920;&#31034;&#22240;&#26524;&#39034;&#24207;&#21644;&#22240;&#26524;&#32467;&#26500;&#12290;&#22312;NeurIPS 2022&#25361;&#25112;&#36187;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#35299;&#20915;&#30693;&#35782;&#36861;&#36394;&#20013;&#30340;&#22240;&#26524;&#21457;&#29616;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#23454;&#38469;&#23398;&#29983;&#21453;&#24212;&#25968;&#25454;&#25214;&#21040;&#19981;&#21516;&#25216;&#33021;&#20043;&#38388;&#30340;&#28508;&#22312;&#22240;&#26524;&#20851;&#31995;&#12290;&#35813;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#22312;&#20110;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#20102;&#35299;&#19981;&#21516;&#25216;&#33021;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#22823;&#37327;&#30340;A/B&#27979;&#35797;&#65292;&#36825;&#21487;&#20197;&#24110;&#21161;&#25945;&#32946;&#24037;&#20316;&#32773;&#26681;&#25454;&#25216;&#33021;&#20808;&#20915;&#26465;&#20214;&#20449;&#24687;&#35774;&#35745;&#26356;&#22909;&#30340;&#35838;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#20462;&#25913;&#21518;&#30340;&#28145;&#24230;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22240;&#26524;&#38376;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#20351;&#29992;&#20102;i&#65289;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#32622;&#25442;&#30697;&#38453;&#26469;&#30830;&#23450;&#25216;&#33021;&#38388;&#30340;&#22240;&#26524;&#39034;&#24207;&#65292;&#21644;ii&#65289;&#19968;&#20010;&#21487;&#36873;&#21487;&#23398;&#20064;&#30340;&#19979;&#19977;&#35282;&#30697;&#38453;&#26469;&#34920;&#31034;&#25216;&#33021;&#38388;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#25105;&#20204;&#36824;&#35814;&#32454;&#20171;&#32461;&#20102;&#22914;&#20309;&#20197;&#31471;&#21040;&#31471;&#12289;&#21487;&#24494;&#30340;&#26041;&#24335;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;NeurIPS 2022&#20851;&#20110;&#23398;&#20064;&#36335;&#24452;&#22240;&#26524;&#27934;&#23519;&#30340;&#25361;&#25112;&#36187;&#20219;&#21153;3&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we take a preliminary step towards solving the problem of causal discovery in knowledge tracing, i.e., finding the underlying causal relationship among different skills from real-world student response data. This problem is important since it can potentially help us understand the causal relationship between different skills without extensive A/B testing, which can potentially help educators to design better curricula according to skill prerequisite information. Specifically, we propose a conceptual solution, a novel causal gated recurrent unit (GRU) module in a modified deep knowledge tracing model, which uses i) a learnable permutation matrix for causal ordering among skills and ii) an optionally learnable lower-triangular matrix for causal structure among skills. We also detail how to learn the model parameters in an end-to-end, differentiable way. Our solution placed among the top entries in Task 3 of the NeurIPS 2022 Challenge on Causal Insights for Learning Paths i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#37319;&#26679;&#30830;&#23450;&#24615;&#34892;&#21015;&#24335;&#21644;Pfaffian&#28857;&#36807;&#31243;&#30340;&#29366;&#24577;&#21450;&#20854;&#20248;&#21270;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.15851</link><description>&lt;p&gt;
&#20851;&#20110;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#37319;&#26679;&#30830;&#23450;&#24615;&#34892;&#21015;&#24335;&#21644;Pfaffian&#28857;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
On sampling determinantal and Pfaffian point processes on a quantum computer. (arXiv:2305.15851v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#37319;&#26679;&#30830;&#23450;&#24615;&#34892;&#21015;&#24335;&#21644;Pfaffian&#28857;&#36807;&#31243;&#30340;&#29366;&#24577;&#21450;&#20854;&#20248;&#21270;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;(DPP) &#26368;&#26089;&#34987; Macchi &#20316;&#20026;&#37327;&#23376;&#20809;&#23398;&#27169;&#22411;&#24341;&#20837;&#65292;&#33258;&#37027;&#20197;&#21518;&#65292;&#23427;&#20204;&#24050;&#24191;&#27867;&#29992;&#20316;&#32479;&#35745;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#30340;&#27169;&#22411;&#21644;&#23376;&#25277;&#26679;&#24037;&#20855;&#12290;&#22823;&#22810;&#25968;&#24212;&#29992;&#38656;&#35201;&#20174;DPP&#25277;&#26679;&#65292;&#32771;&#34385;&#21040;&#20854;&#37327;&#23376;&#36215;&#28304;&#65292;&#33258;&#28982;&#20250;&#24819;&#30693;&#36947;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#25277;&#26679;DPP&#26159;&#21542;&#27604;&#22312;&#32463;&#20856;&#35745;&#31639;&#26426;&#19978;&#26356;&#23481;&#26131;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#26377;&#38480;&#29366;&#24577;&#31354;&#38388;&#19978;&#30340;DPP&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;$\{1,\dots,N\}$&#23376;&#38598;&#19978;&#30340;&#20998;&#24067;&#65292;&#30001;&#19968;&#20010;$N\times N$&#30340;Hermite&#20869;&#26680;&#30697;&#38453;&#21442;&#25968;&#21270;&#12290;&#26368;&#22522;&#26412;&#30340;&#37319;&#26679;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65292;&#22312;&#32463;&#20856;&#35745;&#31639;&#26426;&#19978;&#20998;&#21035;&#38656;&#35201; $\mathcal{O}(N^3)$ &#21644; $\mathcal{O}(Nr^2)$ &#30340;&#25805;&#20316;&#25104;&#26412;&#65292;&#20854;&#20013;$r$&#26159;&#20869;&#26680;&#30697;&#38453;&#30340;&#31209;&#12290;&#26412;&#25991;&#26088;&#22312;&#35752;&#35770;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;DPP&#37319;&#26679;&#31639;&#27861;&#30340;&#29366;&#24577;&#21450;&#20854;&#20248;&#21270;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
DPPs were introduced by Macchi as a model in quantum optics the 1970s. Since then, they have been widely used as models and subsampling tools in statistics and computer science. Most applications require sampling from a DPP, and given their quantum origin, it is natural to wonder whether sampling a DPP on a quantum computer is easier than on a classical one. We focus here on DPPs over a finite state space, which are distributions over the subsets of $\{1,\dots,N\}$ parametrized by an $N\times N$ Hermitian kernel matrix. Vanilla sampling consists in two steps, of respective costs $\mathcal{O}(N^3)$ and $\mathcal{O}(Nr^2)$ operations on a classical computer, where $r$ is the rank of the kernel matrix. A large first part of the current paper consists in explaining why the state-of-the-art in quantum simulation of fermionic systems already yields quantum DPP sampling algorithms. We then modify existing quantum circuits, and discuss their insertion in a full DPP sampling pipeline that start
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#24102;&#26377;&#30830;&#23450;&#24615;&#31574;&#30053;&#25628;&#32034;&#30340;&#31163;&#31574;&#30053;&#24179;&#22343;&#22238;&#25253;&#34892;&#21160;&#32773;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#31574;&#30053;&#21644;&#31163;&#31574;&#30053;&#30340;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#12290;&#20351;&#29992;&#36825;&#20123;&#23450;&#29702;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#22343;&#22238;&#25253;&#31163;&#31574;&#30053;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;ARO-DDPG&#65289;&#12290;&#35813;&#31639;&#27861;&#22312;&#28176;&#36817;&#25910;&#25947;&#24615;&#20998;&#26512;&#21644;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#20013;&#23637;&#31034;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#20102;$\epsilon$-&#26368;&#20248;&#31283;&#23450;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.12239</link><description>&lt;p&gt;
&#24102;&#26377;&#30830;&#23450;&#24615;&#31574;&#30053;&#25628;&#32034;&#30340;&#31163;&#31574;&#30053;&#24179;&#22343;&#22238;&#25253;&#34892;&#21160;&#32773;-&#35780;&#35770;&#23478;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Average Reward Actor-Critic with Deterministic Policy Search. (arXiv:2305.12239v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#24102;&#26377;&#30830;&#23450;&#24615;&#31574;&#30053;&#25628;&#32034;&#30340;&#31163;&#31574;&#30053;&#24179;&#22343;&#22238;&#25253;&#34892;&#21160;&#32773;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#31574;&#30053;&#21644;&#31163;&#31574;&#30053;&#30340;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#12290;&#20351;&#29992;&#36825;&#20123;&#23450;&#29702;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#22343;&#22238;&#25253;&#31163;&#31574;&#30053;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;ARO-DDPG&#65289;&#12290;&#35813;&#31639;&#27861;&#22312;&#28176;&#36817;&#25910;&#25947;&#24615;&#20998;&#26512;&#21644;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#20013;&#23637;&#31034;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#20102;$\epsilon$-&#26368;&#20248;&#31283;&#23450;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#22343;&#22238;&#25253;&#20934;&#21017;&#30456;&#23545;&#36739;&#23569;&#34987;&#30740;&#31350;&#65292;&#22240;&#20026;&#24378;&#21270;&#23398;&#20064;&#25991;&#29486;&#20013;&#30340;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#32771;&#34385;&#20102;&#36148;&#29616;&#22238;&#25253;&#20934;&#21017;&#12290;&#36817;&#26399;&#26377;&#19968;&#20123;&#20851;&#20110;&#22522;&#20110;&#31574;&#30053;&#30340;&#24179;&#22343;&#22238;&#25253;&#34892;&#21160;&#32773;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#24037;&#20316;&#65292;&#20294;&#31163;&#31574;&#30053;&#24179;&#22343;&#22238;&#25253;&#34892;&#21160;&#32773;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30456;&#23545;&#36739;&#23569;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#24179;&#22343;&#22238;&#25253;&#24615;&#33021;&#20934;&#21017;&#30340;&#22522;&#20110;&#31574;&#30053;&#21644;&#31163;&#31574;&#30053;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#12290;&#21033;&#29992;&#36825;&#20123;&#23450;&#29702;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#22343;&#22238;&#25253;&#31163;&#31574;&#30053;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65288;ARO-DDPG&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22522;&#20110;ODE&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#28176;&#36817;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#32467;&#26524;&#38543;&#26426;&#36924;&#36817;&#26041;&#26696;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65292;&#20351;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#22120;&#33719;&#24471;&#20102;&#19968;&#20010;$\epsilon$-&#26368;&#20248;&#31283;&#23450;&#31574;&#30053;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\Omega(\epsilon^{-2.5})$&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;ARO-DDPG&#31639;&#27861;&#30340;&#24179;&#22343;&#22238;&#25253;&#24615;&#33021;&#65292;&#24182;&#35266;&#23519;&#21040;&#26356;&#22909;&#30340;&#32463;&#39564;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The average reward criterion is relatively less studied as most existing works in the Reinforcement Learning literature consider the discounted reward criterion. There are few recent works that present on-policy average reward actor-critic algorithms, but average reward off-policy actor-critic is relatively less explored. In this work, we present both on-policy and off-policy deterministic policy gradient theorems for the average reward performance criterion. Using these theorems, we also present an Average Reward Off-Policy Deep Deterministic Policy Gradient (ARO-DDPG) Algorithm. We first show asymptotic convergence analysis using the ODE-based method. Subsequently, we provide a finite time analysis of the resulting stochastic approximation scheme with linear function approximator and obtain an $\epsilon$-optimal stationary policy with a sample complexity of $\Omega(\epsilon^{-2.5})$. We compare the average reward performance of our proposed ARO-DDPG algorithm and observe better empir
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;DeepMSS&#27169;&#22411;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;Segmentated-to-Survival&#65288;STS&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#28176;&#36827;&#32858;&#21512;&#32593;&#32476;&#65288;MMPAN&#65289;&#26469;&#25506;&#32034;&#32959;&#30244;&#20869;&#22806;&#30340;&#39044;&#21518;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#22686;&#24378;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#36827;&#34892;&#29983;&#23384;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#20004;&#20010;&#20844;&#20849;PET/CT&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.09946</link><description>&lt;p&gt;
DeepMSS&#65306;&#22522;&#20110;PET/CT&#22270;&#20687;&#30340;&#28145;&#24230;&#22810;&#27169;&#24577;&#20999;&#29255;&#21040;&#29983;&#23384;&#39044;&#27979;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DeepMSS: Deep Multi-Modality Segmentation-to-Survival Learning for Survival Outcome Prediction from PET/CT Images. (arXiv:2305.09946v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09946
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;DeepMSS&#27169;&#22411;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;Segmentated-to-Survival&#65288;STS&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#28176;&#36827;&#32858;&#21512;&#32593;&#32476;&#65288;MMPAN&#65289;&#26469;&#25506;&#32034;&#32959;&#30244;&#20869;&#22806;&#30340;&#39044;&#21518;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#22686;&#24378;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#36827;&#34892;&#29983;&#23384;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#20004;&#20010;&#20844;&#20849;PET/CT&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#39044;&#27979;&#26159;&#30284;&#30151;&#31649;&#29702;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#29992;&#20110;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#25191;&#34892;&#31471;&#21040;&#31471;&#30340;&#29983;&#23384;&#39044;&#27979;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#36890;&#36807;&#32852;&#21512;&#25191;&#34892;&#32959;&#30244;&#20998;&#21106;&#21644;&#29983;&#23384;&#39044;&#27979;&#65292;&#37319;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#25351;&#23548;&#27169;&#22411;&#25552;&#21462;&#19982;&#32959;&#30244;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#22312;&#25506;&#32034;&#32959;&#30244;&#22806;&#39044;&#21518;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#23616;&#37096;&#28107;&#24052;&#32467;&#36716;&#31227;&#21644;&#37051;&#36817;&#32452;&#32455;&#20405;&#34989;&#65289;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#22312;&#21033;&#29992;&#22810;&#27169;&#24577;&#22270;&#20687;&#26041;&#38754;&#27424;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeepMSS&#30340;&#28145;&#24230;&#22810;&#27169;&#24577;&#20999;&#29255;&#21040;&#29983;&#23384;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;Segmentated-to-Survival&#65288;STS&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#31163;&#20998;&#21106;&#21644;&#29983;&#23384;&#39044;&#27979;&#20219;&#21153;&#26469;&#36827;&#34892;&#12290;&#23545;&#20110;&#20998;&#21106;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#28176;&#36827;&#32858;&#21512;&#32593;&#32476;&#65288;MMPAN&#65289;&#26469;&#25506;&#32034;&#32959;&#30244;&#20869;&#22806;&#30340;&#39044;&#21518;&#20449;&#24687;&#12290;&#23545;&#20110;&#29983;&#23384;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#22686;&#24378;&#30340;&#28145;&#24230;&#29983;&#23384;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;MMPAN&#30340;&#29305;&#24449;&#34920;&#31034;&#24182;&#25191;&#34892;&#29983;&#23384;&#39044;&#27979;&#12290;&#22312;&#20004;&#20010;&#20844;&#20849;PET/CT&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;DeepMSS&#27169;&#22411;&#22312;&#29983;&#23384;&#39044;&#27979;&#26041;&#38754;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival prediction is a major concern for cancer management. Deep survival models based on deep learning have been widely adopted to perform end-to-end survival prediction from medical images. Recent deep survival models achieved promising performance by jointly performing tumor segmentation with survival prediction, where the models were guided to extract tumor-related information through Multi-Task Learning (MTL). However, existing deep survival models have difficulties in exploring out-of-tumor prognostic information (e.g., local lymph node metastasis and adjacent tissue invasions). In addition, existing deep survival models are underdeveloped in utilizing multi-modality images. Empirically-designed strategies were commonly adopted to fuse multi-modality information via fixed pre-designed networks. In this study, we propose a Deep Multi-modality Segmentation-to-Survival model (DeepMSS) for survival prediction from PET/CT images. Instead of adopting MTL, we propose a novel Segmentat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36317;&#31163;&#27861;&#24322;&#24120;&#26816;&#27979;&#20998;&#25968;&#36716;&#21270;&#20026;&#21487;&#35299;&#37322;&#30340;&#27010;&#29575;&#20272;&#35745;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19982;&#20854;&#20182;&#25968;&#25454;&#28857;&#30340;&#36317;&#31163;&#24314;&#27169;&#36317;&#31163;&#27010;&#29575;&#20998;&#24067;&#65292;&#23558;&#36317;&#31163;&#27861;&#24322;&#24120;&#26816;&#27979;&#20998;&#25968;&#36716;&#25442;&#20026;&#24322;&#24120;&#27010;&#29575;&#65292;&#25552;&#39640;&#20102;&#27491;&#24120;&#28857;&#21644;&#24322;&#24120;&#28857;&#20043;&#38388;&#30340;&#23545;&#27604;&#24230;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09446</link><description>&lt;p&gt;
&#27010;&#29575;&#36317;&#31163;&#27861;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Distance-Based Outlier Detection. (arXiv:2305.09446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36317;&#31163;&#27861;&#24322;&#24120;&#26816;&#27979;&#20998;&#25968;&#36716;&#21270;&#20026;&#21487;&#35299;&#37322;&#30340;&#27010;&#29575;&#20272;&#35745;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19982;&#20854;&#20182;&#25968;&#25454;&#28857;&#30340;&#36317;&#31163;&#24314;&#27169;&#36317;&#31163;&#27010;&#29575;&#20998;&#24067;&#65292;&#23558;&#36317;&#31163;&#27861;&#24322;&#24120;&#26816;&#27979;&#20998;&#25968;&#36716;&#25442;&#20026;&#24322;&#24120;&#27010;&#29575;&#65292;&#25552;&#39640;&#20102;&#27491;&#24120;&#28857;&#21644;&#24322;&#24120;&#28857;&#20043;&#38388;&#30340;&#23545;&#27604;&#24230;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36317;&#31163;&#27861;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#20998;&#25968;&#38590;&#20197;&#35299;&#37322;&#65292;&#22240;&#27492;&#22312;&#27809;&#26377;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#24456;&#38590;&#30830;&#23450;&#27491;&#24120;&#28857;&#21644;&#24322;&#24120;&#28857;&#20043;&#38388;&#30340;&#25130;&#26029;&#38408;&#20540;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#23558;&#36317;&#31163;&#27861;&#24322;&#24120;&#26816;&#27979;&#20998;&#25968;&#36716;&#21270;&#20026;&#21487;&#35299;&#37322;&#30340;&#27010;&#29575;&#20272;&#35745;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#35813;&#36716;&#25442;&#26159;&#25490;&#21517;&#31283;&#23450;&#30340;&#65292;&#24182;&#22686;&#21152;&#20102;&#27491;&#24120;&#28857;&#21644;&#24322;&#24120;&#28857;&#20043;&#38388;&#30340;&#23545;&#27604;&#24230;&#12290;&#30830;&#23450;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#20851;&#31995;&#26159;&#35782;&#21035;&#25968;&#25454;&#20013;&#26368;&#36817;&#37051;&#20851;&#31995;&#25152;&#24517;&#38656;&#30340;&#65292;&#28982;&#32780;&#22823;&#22810;&#25968;&#35745;&#31639;&#20986;&#30340;&#36317;&#31163;&#36890;&#24120;&#34987;&#20002;&#24323;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#19982;&#20854;&#20182;&#25968;&#25454;&#28857;&#30340;&#36317;&#31163;&#26469;&#24314;&#27169;&#36317;&#31163;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#38543;&#21518;&#20351;&#29992;&#36825;&#20123;&#20998;&#24067;&#23558;&#36317;&#31163;&#27861;&#24322;&#24120;&#26816;&#27979;&#20998;&#25968;&#36716;&#25442;&#20026;&#24322;&#24120;&#27010;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#27010;&#29575;&#36716;&#25442;&#19981;&#20250;&#24433;&#21709;&#20247;&#22810;&#34920;&#26684;&#21644;&#22270;&#20687;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#20294;&#20250;&#20135;&#29983;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The scores of distance-based outlier detection methods are difficult to interpret, making it challenging to determine a cut-off threshold between normal and outlier data points without additional context. We describe a generic transformation of distance-based outlier scores into interpretable, probabilistic estimates. The transformation is ranking-stable and increases the contrast between normal and outlier data points. Determining distance relationships between data points is necessary to identify the nearest-neighbor relationships in the data, yet, most of the computed distances are typically discarded. We show that the distances to other data points can be used to model distance probability distributions and, subsequently, use the distributions to turn distance-based outlier scores into outlier probabilities. Our experiments show that the probabilistic transformation does not impact detection performance over numerous tabular and image benchmark datasets but results in interpretable
&lt;/p&gt;</description></item><item><title>CB-HVTNet &#25552;&#20986;&#20102;&#19968;&#31181; Channel Boosted Hybrid Vision Transformer &#32593;&#32476;&#65292;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#29983;&#25104;&#22686;&#24378;&#36890;&#36947;&#65292;&#24182;&#32467;&#21512;&#20351;&#29992; Transformers &#21644; CNN&#65292;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#39640;&#25928;&#20934;&#30830;&#22320;&#35780;&#20272;&#28107;&#24052;&#32454;&#32990;&#12290;</title><link>http://arxiv.org/abs/2305.09211</link><description>&lt;p&gt;
CB-HVTNet&#65306;&#19968;&#31181;&#29992;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#28107;&#24052;&#32454;&#32990;&#35780;&#20272;&#30340;&#36890;&#36947;&#22686;&#24378;&#28151;&#21512;&#35270;&#35273; Transformer &#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CB-HVTNet: A channel-boosted hybrid vision transformer network for lymphocyte assessment in histopathological images. (arXiv:2305.09211v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09211
&lt;/p&gt;
&lt;p&gt;
CB-HVTNet &#25552;&#20986;&#20102;&#19968;&#31181; Channel Boosted Hybrid Vision Transformer &#32593;&#32476;&#65292;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#29983;&#25104;&#22686;&#24378;&#36890;&#36947;&#65292;&#24182;&#32467;&#21512;&#20351;&#29992; Transformers &#21644; CNN&#65292;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#39640;&#25928;&#20934;&#30830;&#22320;&#35780;&#20272;&#28107;&#24052;&#32454;&#32990;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#30001;&#20110;&#20854;&#23398;&#20064;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#24050;&#32463;&#20811;&#26381;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20840;&#23616;&#36879;&#35270;&#23398;&#20064;&#30340;&#32570;&#28857;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#20851;&#27880;&#30340;&#28966;&#28857;&#65292;&#29992;&#20110;&#22810;&#20010;&#19982;&#35270;&#35273;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#21307;&#30103;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#22810;&#22836;&#27880;&#24847;&#27169;&#22359;&#20165;&#25429;&#33719;&#20840;&#23616;&#32423;&#21035;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#36825;&#23545;&#20110;&#21307;&#23398;&#22270;&#20687;&#26469;&#35828;&#26159;&#19981;&#36275;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181; Channel Boosted Hybrid Vision Transformer&#65288;CB HVT&#65289;&#65292;&#23427;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#29983;&#25104;&#22686;&#24378;&#36890;&#36947;&#65292;&#24182;&#20351;&#29992; Transformers &#21644; CNN &#26469;&#20998;&#26512;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#30340;&#28107;&#24052;&#32454;&#32990;&#12290;&#25152;&#25552;&#20986;&#30340; CB HVT &#21253;&#25324;&#20116;&#20010;&#27169;&#22359;&#65292;&#21253;&#25324;&#36890;&#36947;&#29983;&#25104;&#27169;&#22359;&#12289;&#36890;&#36947;&#21033;&#29992;&#27169;&#22359;&#12289;&#36890;&#36947;&#21512;&#24182;&#27169;&#22359;&#12289;&#21306;&#22495;&#24863;&#30693;&#27169;&#22359;&#21644;&#26816;&#27979;&#21644;&#20998;&#27573;&#22836;&#65292;&#23427;&#20204;&#20849;&#21516;&#26377;&#25928;&#22320;&#35782;&#21035;&#28107;&#24052;&#32454;&#32990;&#12290;&#36890;&#36947;&#29983;&#25104;&#27169;&#22359;&#20351;&#29992;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#36827;&#34892;&#36890;&#36947;&#22686;&#24378;&#30340;&#24605;&#24819;&#21019;&#24314;&#22810;&#20010;&#24378;&#22823;&#30340;&#36890;&#36947;&#65292;&#28982;&#21518;&#19982; Transformers &#21644; CNN &#32467;&#21512;&#20351;&#29992;&#65292;&#20197;&#26356;&#22909;&#22320;&#20998;&#26512;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#30340;&#28107;&#24052;&#32454;&#32990;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340; CB HVT &#26159;&#21307;&#23398;&#35786;&#26029;&#20013;&#20934;&#30830;&#12289;&#39640;&#25928;&#35780;&#20272;&#28107;&#24052;&#32454;&#32990;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers, due to their ability to learn long range dependencies, have overcome the shortcomings of convolutional neural networks (CNNs) for global perspective learning. Therefore, they have gained the focus of researchers for several vision related tasks including medical diagnosis. However, their multi-head attention module only captures global level feature representations, which is insufficient for medical images. To address this issue, we propose a Channel Boosted Hybrid Vision Transformer (CB HVT) that uses transfer learning to generate boosted channels and employs both transformers and CNNs to analyse lymphocytes in histopathological images. The proposed CB HVT comprises five modules, including a channel generation module, channel exploitation module, channel merging module, region-aware module, and a detection and segmentation head, which work together to effectively identify lymphocytes. The channel generation module uses the idea of channel boosting through transfer learni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21704;&#33945;&#33707;&#29305; Hessian &#19968;&#33268;&#24615;&#30340;&#20840;&#20998;&#24067;&#24335;&#29275;&#39039;&#22411;&#20248;&#21270;&#31639;&#27861; Network-GIANT&#65292;&#23558;&#26799;&#24230;&#36319;&#36394;&#21644;&#29275;&#39039;&#22411;&#36845;&#20195;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#32463;&#35777;&#26126;&#23545;&#20005;&#26684;&#20984;&#21644;&#20809;&#28369;&#25439;&#22833;&#20989;&#25968;&#26377;&#21322;&#20840;&#23616;&#21644;&#25351;&#25968;&#25910;&#25947;&#21040;&#31934;&#30830;&#35299;&#30340;&#20445;&#35777;&#65292;&#23454;&#39564;&#35777;&#26126; Network-GIANT &#20248;&#20110;&#20854;&#20182;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914; Network-DANE &#21644; Newton-Raphson Consensus&#65289;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07898</link><description>&lt;p&gt;
Network-GIANT: &#22522;&#20110;&#21704;&#33945;&#33707;&#29305; Hessian &#19968;&#33268;&#24615;&#30340;&#20840;&#20998;&#24067;&#24335;&#29275;&#39039;&#22411;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Network-GIANT: Fully distributed Newton-type optimization via harmonic Hessian consensus. (arXiv:2305.07898v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21704;&#33945;&#33707;&#29305; Hessian &#19968;&#33268;&#24615;&#30340;&#20840;&#20998;&#24067;&#24335;&#29275;&#39039;&#22411;&#20248;&#21270;&#31639;&#27861; Network-GIANT&#65292;&#23558;&#26799;&#24230;&#36319;&#36394;&#21644;&#29275;&#39039;&#22411;&#36845;&#20195;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#32463;&#35777;&#26126;&#23545;&#20005;&#26684;&#20984;&#21644;&#20809;&#28369;&#25439;&#22833;&#20989;&#25968;&#26377;&#21322;&#20840;&#23616;&#21644;&#25351;&#25968;&#25910;&#25947;&#21040;&#31934;&#30830;&#35299;&#30340;&#20445;&#35777;&#65292;&#23454;&#39564;&#35777;&#26126; Network-GIANT &#20248;&#20110;&#20854;&#20182;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914; Network-DANE &#21644; Newton-Raphson Consensus&#65289;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20998;&#24067;&#24335;&#22810;&#20195;&#29702;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#20840;&#23616;&#30446;&#26631;&#26159;&#36890;&#36807;&#26412;&#22320;&#20248;&#21270;&#21644;&#33410;&#28857;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#25442;&#26469;&#26368;&#23567;&#21270;&#26412;&#22320;&#30446;&#26631;&#65288;&#32463;&#39564;&#25439;&#22833;&#65289;&#20989;&#25968;&#30340;&#24635;&#21644;&#12290; &#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29275;&#39039;&#22411;&#23436;&#20840;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;Network-GIANT&#65292;&#23427;&#22522;&#20110; GIANT&#65292;&#36825;&#26159;&#19968;&#31181;&#20381;&#36182;&#20110;&#38598;&#20013;&#24335;&#21442;&#25968;&#26381;&#21153;&#22120;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290; Network-GIANT &#31639;&#27861;&#26159;&#36890;&#36807;&#22312;&#27599;&#20010;&#33410;&#28857;&#19978;&#20351;&#29992;&#26799;&#24230;&#36319;&#36394;&#21644;&#29275;&#39039;&#22411;&#36845;&#20195;&#31639;&#27861;&#30340;&#32452;&#21512;&#20197;&#21450;&#26412;&#22320;&#26799;&#24230;&#21644;&#29275;&#39039;&#26356;&#26032;&#30340;&#20849;&#35782;&#24179;&#22343;&#26469;&#35774;&#35745;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20445;&#35777;&#20102;&#23545;&#32593;&#32476;&#19978;&#30340;&#20005;&#26684;&#20984;&#21644;&#20809;&#28369;&#25439;&#22833;&#20989;&#25968;&#30340;&#21322;&#20840;&#23616;&#21644;&#25351;&#25968;&#25910;&#25947;&#21040;&#31934;&#30830;&#35299;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102; Network-GIANT &#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914; Network-DANE &#21644; Newton-Raphson Consensus&#65289;&#30340;&#25910;&#25947;&#24615;&#33021;&#30340;&#23454;&#35777;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the problem of distributed multi-agent learning, where the global aim is to minimize a sum of local objective (empirical loss) functions through local optimization and information exchange between neighbouring nodes. We introduce a Newton-type fully distributed optimization algorithm, Network-GIANT, which is based on GIANT, a Federated learning algorithm that relies on a centralized parameter server. The Network-GIANT algorithm is designed via a combination of gradient-tracking and a Newton-type iterative algorithm at each node with consensus based averaging of local gradient and Newton updates. We prove that our algorithm guarantees semi-global and exponential convergence to the exact solution over the network assuming strongly convex and smooth loss functions. We provide empirical evidence of the superior convergence performance of Network-GIANT over other state-of-art distributed learning algorithms such as Network-DANE and Newton-Raphson Consensus.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#24341;&#23548;&#30340;&#31895;-&#32454;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#25903;&#25345;&#20174;&#31895;&#21040;&#32454;&#30340;&#22810;&#27425;&#36845;&#20195;&#65292;&#23454;&#29616;&#20102;&#26356;&#21152;&#31526;&#21512;&#20154;&#33041;&#24605;&#32500;&#26041;&#24335;&#30340;&#20195;&#30721;&#32534;&#20889;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.00909</link><description>&lt;p&gt;
&#22823;&#32434;&#20808;&#34892;&#65292;&#32454;&#33410;&#21518;&#33267;&#65306;&#22522;&#20110;&#35821;&#27861;&#24341;&#23548;&#30340;&#31895;-&#32454;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation. (arXiv:2305.00909v2 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00909
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#24341;&#23548;&#30340;&#31895;-&#32454;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#25903;&#25345;&#20174;&#31895;&#21040;&#32454;&#30340;&#22810;&#27425;&#36845;&#20195;&#65292;&#23454;&#29616;&#20102;&#26356;&#21152;&#31526;&#21512;&#20154;&#33041;&#24605;&#32500;&#26041;&#24335;&#30340;&#20195;&#30721;&#32534;&#20889;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19968;&#20010;&#22797;&#26434;&#31639;&#27861;&#30340;&#23454;&#29616;&#65292;&#20154;&#31867;&#31243;&#24207;&#21592;&#30340;&#20570;&#27861;&#36890;&#24120;&#26159;&#20808;&#27010;&#36848;&#19968;&#19979;&#25511;&#21046;&#27969;&#31243;&#65292;&#28982;&#21518;&#36845;&#20195;&#36827;&#34892;&#20016;&#23500;&#65292;&#26368;&#32456;&#29983;&#25104;&#19968;&#20123;&#31934;&#24515;&#21152;&#24037;&#30340;&#35821;&#27861;&#32467;&#26500;&#21644;&#23618;&#27425;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19968;&#27425;&#24615;&#29983;&#25104;&#20195;&#30721;&#65292;&#27809;&#26377;&#20013;&#38388;&#29615;&#33410;&#65292;&#20197;&#21453;&#26144;"&#22823;&#32434;&#20808;&#34892;&#65292;&#32454;&#33410;&#21518;&#33267;"&#30340;&#32467;&#26500;&#21270;&#24605;&#32500;&#36807;&#31243;&#12290;&#21463;&#21040;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#26368;&#26032;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChainCoder&#65292;&#36825;&#26159;&#19968;&#31181;&#31243;&#24207;&#32508;&#21512;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#36880;&#27493;&#29983;&#25104;Python&#20195;&#30721;&#65292;&#21363;&#20174;&#31895;&#21040;&#32454;&#36827;&#34892;&#22810;&#27425;&#36845;&#20195;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25277;&#35937;&#35821;&#27861;&#26641;&#35299;&#26512;&#23558;&#28304;&#20195;&#30721;&#20998;&#35299;&#20026;&#24067;&#23616;&#26694;&#26550;&#32452;&#20214;&#21644;&#38468;&#20214;&#32452;&#20214;&#65292;&#20197;&#26500;&#24314;&#23618;&#27425;&#34920;&#31034;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#39044;&#27979;&#30446;&#26631;&#37325;&#26032;&#21551;&#21160;&#65292;&#24418;&#25104;&#22810;&#27425;&#36890;&#36807;&#30446;&#26631;&#65292;&#27599;&#27425;&#29983;&#25104;&#19968;&#20010;&#23376;&#24207;&#21015;&#65292;&#36825;&#20123;&#23376;&#24207;&#21015;&#22312;&#23618;&#27425;&#32467;&#26500;&#20013;&#20018;&#32852;&#36215;&#26469;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#37327;&#36523;&#23450;&#21046;&#30340;Transformer&#20307;&#31995;&#32467;&#26500;&#26469;&#23454;&#29616;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a complicated algorithm, its implementation by a human programmer usually starts with outlining a rough control flow followed by iterative enrichments, eventually yielding carefully generated syntactic structures and variables in a hierarchy. However, state-of-the-art large language models generate codes in a single pass, without intermediate warm-ups to reflect the structured thought process of "outline-then-detail". Inspired by the recent success of chain-of-thought prompting, we propose ChainCoder, a program synthesis language model that generates Python code progressively, i.e. from coarse to fine in multiple passes. We first decompose source code into layout frame components and accessory components via abstract syntax tree parsing to construct a hierarchical representation. We then reform our prediction target into a multi-pass objective, each pass generates a subsequence, which is concatenated in the hierarchy. Finally, a tailored transformer architecture is leveraged to joi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26159;&#31532;&#19968;&#20010;&#23545;&#20010;&#20154;&#20449;&#24687;&#23398;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#36827;&#34892;&#23454;&#35777;&#21644;&#20998;&#26512;&#30740;&#31350;&#30340;&#24037;&#20316;&#65292;&#30740;&#31350;&#21253;&#25324;&#21407;&#22987;&#25968;&#25454;&#21644;&#25972;&#20010;&#26426;&#22120;&#23398;&#20064;&#21608;&#26399;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#25214;&#20986;&#20854;&#20013;&#30340;&#23454;&#36341;&#21644;&#36947;&#24503;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.15592</link><description>&lt;p&gt;
&#25581;&#31034;&#20010;&#20154;&#20449;&#24687;&#23398;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Uncovering Bias in Personal Informatics. (arXiv:2303.15592v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15592
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26159;&#31532;&#19968;&#20010;&#23545;&#20010;&#20154;&#20449;&#24687;&#23398;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#36827;&#34892;&#23454;&#35777;&#21644;&#20998;&#26512;&#30740;&#31350;&#30340;&#24037;&#20316;&#65292;&#30740;&#31350;&#21253;&#25324;&#21407;&#22987;&#25968;&#25454;&#21644;&#25972;&#20010;&#26426;&#22120;&#23398;&#20064;&#21608;&#26399;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#25214;&#20986;&#20854;&#20013;&#30340;&#23454;&#36341;&#21644;&#36947;&#24503;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#26234;&#33021;&#25163;&#26426;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#39537;&#21160;&#30340;&#20010;&#20154;&#20449;&#24687;&#23398;&#65288;PI&#65289;&#31995;&#32479;&#65292;&#36890;&#36807;&#25552;&#20379;&#26377;&#24847;&#20041;&#19988;&#21487;&#25191;&#34892;&#30340;&#35265;&#35299;&#65292;&#28040;&#38500;&#20102;&#29992;&#25143;&#19982;&#20854;&#20581;&#24247;&#20449;&#24687;&#20043;&#38388;&#30340;&#38556;&#30861;&#65292;&#20351;&#20154;&#20204;&#33021;&#22815;&#36807;&#19978;&#26356;&#20581;&#24247;&#30340;&#29983;&#27963;&#12290;&#20170;&#22825;&#65292;&#25968;&#21313;&#20159;&#29992;&#25143;&#20351;&#29992;&#27492;&#31867;&#31995;&#32479;&#26469;&#30417;&#27979;&#19981;&#20165;&#26159;&#36523;&#20307;&#27963;&#21160;&#21644;&#30561;&#30496;&#65292;&#36824;&#26377;&#29983;&#21629;&#20307;&#24449;&#12289;&#22899;&#24615;&#20581;&#24247;&#21644;&#24515;&#33039;&#20581;&#24247;&#31561;&#12290;&#23613;&#31649;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#24182;&#19988;&#22788;&#29702;&#25935;&#24863;&#30340;PI&#25968;&#25454;&#65292;&#20294;&#20559;&#35265;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#30340;&#35843;&#26597;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#23545;&#21253;&#25324;&#21407;&#22987;&#25968;&#25454;&#21644;&#25972;&#20010;&#26426;&#22120;&#23398;&#20064;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#20559;&#35265;&#36827;&#34892;&#20840;&#38754;&#23454;&#35777;&#21644;&#20998;&#26512;&#30740;&#31350;&#30340;&#24037;&#20316;&#65292;&#24182;&#20351;&#29992;&#36804;&#20170;&#20026;&#27490;&#26368;&#35814;&#32454;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personal informatics (PI) systems, powered by smartphones and wearables, enable people to lead healthier lifestyles by providing meaningful and actionable insights that break down barriers between users and their health information. Today, such systems are used by billions of users for monitoring not only physical activity and sleep but also vital signs and women's and heart health, among others. %Despite their widespread usage, the processing of particularly sensitive personal data, and their proximity to domains known to be susceptible to bias, such as healthcare, bias in PI has not been investigated systematically. Despite their widespread usage, the processing of sensitive PI data may suffer from biases, which may entail practical and ethical implications. In this work, we present the first comprehensive empirical and analytical study of bias in PI systems, including biases in raw data and in the entire machine learning life cycle. We use the most detailed framework to date for exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;IMWUT&#26399;&#21002;&#19978;&#36807;&#21435;&#20116;&#24180;&#21457;&#34920;&#30340;&#35770;&#25991;&#36827;&#34892;&#31995;&#32479;&#22238;&#39038;&#65292;&#21457;&#29616;UbiComp&#31038;&#21306;&#22312;&#31639;&#27861;&#20844;&#24179;&#26041;&#38754;&#30340;&#36827;&#23637;&#28382;&#21518;&#65292;&#23384;&#22312;&#25935;&#24863;&#23646;&#24615;&#20559;&#24046;&#23548;&#33268;&#30340;&#27495;&#35270;&#24615;&#32467;&#26524;&#65292;&#38656;&#35201;&#25506;&#32034;&#25253;&#21578;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#20197;&#35299;&#20915;&#36825;&#20123;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2303.15585</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#20844;&#24179;&#24615;&#30340;&#20851;&#38190;&#22238;&#39038;&#65306;&#36229;&#36234;&#20934;&#30830;&#24615;&#22312;&#31227;&#21160;&#21644;&#21487;&#31359;&#25140;&#35745;&#31639;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Beyond Accuracy: A Critical Review of Fairness in Machine Learning for Mobile and Wearable Computing. (arXiv:2303.15585v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;IMWUT&#26399;&#21002;&#19978;&#36807;&#21435;&#20116;&#24180;&#21457;&#34920;&#30340;&#35770;&#25991;&#36827;&#34892;&#31995;&#32479;&#22238;&#39038;&#65292;&#21457;&#29616;UbiComp&#31038;&#21306;&#22312;&#31639;&#27861;&#20844;&#24179;&#26041;&#38754;&#30340;&#36827;&#23637;&#28382;&#21518;&#65292;&#23384;&#22312;&#25935;&#24863;&#23646;&#24615;&#20559;&#24046;&#23548;&#33268;&#30340;&#27495;&#35270;&#24615;&#32467;&#26524;&#65292;&#38656;&#35201;&#25506;&#32034;&#25253;&#21578;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#20197;&#35299;&#20915;&#36825;&#20123;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#12289;&#21487;&#31359;&#25140;&#21644;&#26222;&#21450;&#35745;&#31639;&#39046;&#22495;&#27491;&#22312;&#32463;&#21382;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#38761;&#21629;&#24615;&#25972;&#21512;&#12290;&#35774;&#22791;&#29616;&#22312;&#21487;&#20197;&#35786;&#26029;&#30142;&#30149;&#12289;&#39044;&#27979;&#24515;&#33039;&#19981;&#35268;&#21017;&#21160;&#65292;&#21457;&#25496;&#20154;&#31867;&#35748;&#30693;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30456;&#20851;&#31639;&#27861;&#22312;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#12289;&#31181;&#26063;&#31561;&#65289;&#26041;&#38754;&#21487;&#33021;&#23384;&#22312;&#20559;&#24046;&#65292;&#23548;&#33268;&#27495;&#35270;&#24615;&#32467;&#26524;&#12290;&#36817;&#26399;&#65292;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#23398;&#65288;AI-Ethics&#65289;&#30740;&#31350;&#31038;&#21306;&#24320;&#22987;&#25506;&#32034;&#25253;&#21578;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#20197;&#25581;&#31034;&#24182;&#26368;&#32456;&#23545;&#25239;&#36825;&#20123;&#20559;&#24046;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#22312;&#36825;&#20123;&#25253;&#21578;&#26041;&#38754;UbiComp&#31038;&#21306;&#25152;&#37319;&#32435;&#30340;&#31243;&#24230;&#65292;&#24182;&#24378;&#35843;&#28508;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;&#36890;&#36807;&#23545;&#36807;&#21435;&#20116;&#24180;&#65288;2018-2022&#65289;&#22312;ACM&#20132;&#20114;&#12289;&#31227;&#21160;&#12289;&#21487;&#31359;&#25140;&#21644;&#26222;&#36866;&#25216;&#26415;&#65288;IMWUT&#65289;&#26399;&#21002;&#19978;&#21457;&#34920;&#30340;&#35770;&#25991;&#36827;&#34892;&#31995;&#32479;&#22238;&#39038;&#65292;&#25105;&#20204;&#21457;&#29616;UbiComp&#31038;&#21306;&#22312;&#31639;&#27861;&#20844;&#24179;&#26041;&#38754;&#30340;&#36827;&#23637;&#28382;&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of mobile, wearable, and ubiquitous computing (UbiComp) is undergoing a revolutionary integration of machine learning. Devices can now diagnose diseases, predict heart irregularities, and unlock the full potential of human cognition. However, the underlying algorithms are not immune to biases with respect to sensitive attributes (e.g., gender, race), leading to discriminatory outcomes. The research communities of HCI and AI-Ethics have recently started to explore ways of reporting information about datasets to surface and, eventually, counter those biases. The goal of this work is to explore the extent to which the UbiComp community has adopted such ways of reporting and highlight potential shortcomings. Through a systematic review of papers published in the Proceedings of the ACM Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) journal over the past 5 years (2018-2022), we found that progress on algorithmic fairness within the UbiComp community lags behind. 
&lt;/p&gt;</description></item><item><title>Sionna RT&#26159;&#19968;&#20010;GPU&#21152;&#36895;&#30340;&#24320;&#28304;&#24211;&#65292;&#23427;&#38598;&#25104;&#20102;&#21487;&#24494;&#20998;&#30340;&#20809;&#32447;&#36861;&#36394;&#22120;&#65292;&#21487;&#20197;&#29992;&#20110;&#27169;&#25311;&#26080;&#32447;&#30005;&#27874;&#20256;&#25773;&#12290;&#36825;&#20010;&#21151;&#33021;&#20351;&#24471;&#21487;&#20197;&#35745;&#31639;&#19982;&#22810;&#20010;&#31995;&#32479;&#21644;&#29615;&#22659;&#21442;&#25968;&#26377;&#20851;&#30340;&#37327;&#30340;&#26799;&#24230;&#65292;&#23545;&#20110;&#35832;&#22914;&#23398;&#20064;&#26080;&#32447;&#30005;&#26448;&#26009;&#21644;&#20248;&#21270;&#21457;&#23556;&#26426;&#26041;&#21521;&#31561;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#21516;&#26102;&#65292;&#21487;&#24494;&#20998;&#20809;&#32447;&#36861;&#36394;&#23545;&#20110;&#26032;&#39062;&#30340;&#30740;&#31350;&#26041;&#21521;&#22914;&#25968;&#23383;&#23402;&#29983;&#20063;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25512;&#21160;&#32773;&#12290;</title><link>http://arxiv.org/abs/2303.11103</link><description>&lt;p&gt;
Sionna RT&#65306;&#26080;&#32447;&#30005;&#20256;&#25773;&#24314;&#27169;&#30340;&#21487;&#24494;&#20998;&#20809;&#32447;&#36861;&#36394;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Sionna RT: Differentiable Ray Tracing for Radio Propagation Modeling. (arXiv:2303.11103v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11103
&lt;/p&gt;
&lt;p&gt;
Sionna RT&#26159;&#19968;&#20010;GPU&#21152;&#36895;&#30340;&#24320;&#28304;&#24211;&#65292;&#23427;&#38598;&#25104;&#20102;&#21487;&#24494;&#20998;&#30340;&#20809;&#32447;&#36861;&#36394;&#22120;&#65292;&#21487;&#20197;&#29992;&#20110;&#27169;&#25311;&#26080;&#32447;&#30005;&#27874;&#20256;&#25773;&#12290;&#36825;&#20010;&#21151;&#33021;&#20351;&#24471;&#21487;&#20197;&#35745;&#31639;&#19982;&#22810;&#20010;&#31995;&#32479;&#21644;&#29615;&#22659;&#21442;&#25968;&#26377;&#20851;&#30340;&#37327;&#30340;&#26799;&#24230;&#65292;&#23545;&#20110;&#35832;&#22914;&#23398;&#20064;&#26080;&#32447;&#30005;&#26448;&#26009;&#21644;&#20248;&#21270;&#21457;&#23556;&#26426;&#26041;&#21521;&#31561;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#21516;&#26102;&#65292;&#21487;&#24494;&#20998;&#20809;&#32447;&#36861;&#36394;&#23545;&#20110;&#26032;&#39062;&#30340;&#30740;&#31350;&#26041;&#21521;&#22914;&#25968;&#23383;&#23402;&#29983;&#20063;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25512;&#21160;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sionna&#26159;&#19968;&#20010;&#22522;&#20110;TensorFlow&#30340;GPU&#21152;&#36895;&#24320;&#28304;&#24211;&#65292;&#29992;&#20110;&#38142;&#36335;&#32423;&#27169;&#25311;&#12290;&#33258;v0.14&#29256;&#26412;&#20197;&#26469;&#65292;&#23427;&#38598;&#25104;&#20102;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#20809;&#32447;&#36861;&#36394;&#22120;&#65288;RT&#65289;&#65292;&#29992;&#20110;&#27169;&#25311;&#26080;&#32447;&#30005;&#27874;&#20256;&#25773;&#12290;&#36825;&#20010;&#29420;&#29305;&#21151;&#33021;&#20801;&#35768;&#35745;&#31639;&#19982;&#35768;&#22810;&#31995;&#32479;&#21644;&#29615;&#22659;&#21442;&#25968;&#26377;&#20851;&#30340;&#20449;&#36947;&#20914;&#28608;&#21709;&#24212;&#21644;&#20854;&#20182;&#30456;&#20851;&#37327;&#30340;&#26799;&#24230;&#65292;&#20363;&#22914;&#26448;&#26009;&#29305;&#24615;&#12289;&#22825;&#32447;&#22270;&#26696;&#12289;&#38453;&#21015;&#20960;&#20309;&#12289;&#21457;&#23556;&#26426;&#21644;&#25509;&#25910;&#26426;&#30340;&#26041;&#21521;&#21644;&#20301;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;Sionna RT&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#23637;&#31034;&#20102;&#23398;&#20064;&#26080;&#32447;&#30005;&#26448;&#26009;&#21644;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#21457;&#23556;&#26426;&#26041;&#21521;&#31561;&#31034;&#20363;&#24212;&#29992;&#12290;&#34429;&#28982;&#32463;&#20856;&#30340;&#20809;&#32447;&#36861;&#36394;&#23545;&#20110;6G&#30740;&#31350;&#35838;&#39064;&#22914;&#21487;&#37325;&#26500;&#26234;&#33021;&#34920;&#38754;&#12289;&#38598;&#25104;&#24863;&#30693;&#19982;&#36890;&#20449;&#20197;&#21450;&#29992;&#25143;&#23450;&#20301;&#26159;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#65292;&#20294;&#21487;&#24494;&#20998;&#20809;&#32447;&#36861;&#36394;&#26159;&#35768;&#22810;&#26032;&#39062;&#21644;&#20196;&#20154;&#20852;&#22859;&#30340;&#30740;&#31350;&#26041;&#21521;&#30340;&#20851;&#38190;&#25512;&#21160;&#32773;&#65292;&#20363;&#22914;&#25968;&#23383;&#23402;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sionna is a GPU-accelerated open-source library for link-level simulations based on TensorFlow. Since release v0.14 it integrates a differentiable ray tracer (RT) for the simulation of radio wave propagation. This unique feature allows for the computation of gradients of the channel impulse response and other related quantities with respect to many system and environment parameters, such as material properties, antenna patterns, array geometries, as well as transmitter and receiver orientations and positions. In this paper, we outline the key components of Sionna RT and showcase example applications such as learning radio materials and optimizing transmitter orientations by gradient descent. While classic ray tracing is a crucial tool for 6G research topics like reconfigurable intelligent surfaces, integrated sensing and communications, as well as user localization, differentiable ray tracing is a key enabler for many novel and exciting research directions, for example, digital twins.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20266;&#24433;&#38477;&#22122;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#31232;&#30095;&#35270;&#22270;&#19979;&#33258;&#21160;&#20986;&#34880;&#26816;&#27979;&#30340;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#19982;&#23436;&#20840;&#37319;&#26679;&#30340;&#22270;&#20687;&#36827;&#34892;&#21516;&#31561;&#31934;&#30830;&#24230;&#30340;&#20998;&#31867;&#21644;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.09340</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20266;&#24433;&#38477;&#22122;&#30340;&#31232;&#30095;&#35270;&#22270;CT&#22270;&#20687;&#33258;&#21160;&#20986;&#34880;&#26816;&#27979;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Automated Hemorrhage Detection in Sparse-view Computed Tomography via Deep Convolutional Neural Network based Artifact Reduction. (arXiv:2303.09340v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20266;&#24433;&#38477;&#22122;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#31232;&#30095;&#35270;&#22270;&#19979;&#33258;&#21160;&#20986;&#34880;&#26816;&#27979;&#30340;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#19982;&#23436;&#20840;&#37319;&#26679;&#30340;&#22270;&#20687;&#36827;&#34892;&#21516;&#31561;&#31934;&#30830;&#24230;&#30340;&#20998;&#31867;&#21644;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39045;&#20869;&#20986;&#34880;&#26159;&#19968;&#31181;&#20005;&#37325;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#38656;&#35201;&#24555;&#36895;&#19988;&#24120;&#24120;&#38750;&#24120;&#23494;&#38598;&#30340;&#21307;&#30103;&#27835;&#30103;&#12290;&#20026;&#20102;&#35786;&#26029;&#65292;&#36890;&#24120;&#35201;&#36827;&#34892;&#39045;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CCT&#65289;&#25195;&#25551;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36752;&#23556;&#24341;&#36215;&#30340;&#22686;&#21152;&#30340;&#20581;&#24247;&#39118;&#38505;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#38477;&#20302;&#36825;&#31181;&#28508;&#22312;&#39118;&#38505;&#30340;&#26368;&#37325;&#35201;&#31574;&#30053;&#26159;&#23613;&#21487;&#33021;&#20445;&#25345;&#36752;&#23556;&#21058;&#37327;&#20302;&#65292;&#24182;&#19982;&#35786;&#26029;&#20219;&#21153;&#19968;&#33268;&#12290; &#31232;&#30095;&#35270;&#22270;CT&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#25152;&#37319;&#38598;&#30340;&#35270;&#22270;&#24635;&#25968;&#65292;&#20174;&#32780;&#38477;&#20302;&#21058;&#37327;&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#20294;&#20195;&#20215;&#26159;&#38477;&#20302;&#22270;&#20687;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;U-Net&#26550;&#26500;&#26469;&#20943;&#23569;&#31232;&#30095;&#35270;&#22270;CCT&#30340;&#20266;&#24433;&#65292;&#20174;&#31232;&#30095;&#35270;&#22270;&#20013;&#39044;&#27979;&#23436;&#20840;&#37319;&#26679;&#30340;&#37325;&#24314;&#22270;&#20687;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#20986;&#34880;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#23436;&#20840;&#37319;&#26679;&#30340;CCT&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20266;&#24433;&#38477;&#22122;&#21518;&#30340;CCT&#22270;&#20687;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#21644;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#19982;&#23436;&#20840;&#37319;&#26679;&#30340;CCT&#22270;&#20687;&#27809;&#26377;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intracranial hemorrhage poses a serious health problem requiring rapid and often intensive medical treatment. For diagnosis, a Cranial Computed Tomography (CCT) scan is usually performed. However, the increased health risk caused by radiation is a concern. The most important strategy to reduce this potential risk is to keep the radiation dose as low as possible and consistent with the diagnostic task. Sparse-view CT can be an effective strategy to reduce dose by reducing the total number of views acquired, albeit at the expense of image quality. In this work, we use a U-Net architecture to reduce artifacts from sparse-view CCTs, predicting fully sampled reconstructions from sparse-view ones. We evaluate the hemorrhage detectability in the predicted CCTs with a hemorrhage classification convolutional neural network, trained on fully sampled CCTs to detect and classify different sub-types of hemorrhages. Our results suggest that the automated classification and detection accuracy of hemo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#24067;&#38647;-&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#35757;&#32451;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#28145;&#24230;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;&#65292;&#24182;&#22312;&#26377;&#38480;&#31209;&#30697;&#38453;&#31354;&#38388;&#20869;&#34920;&#24449;&#20851;&#38190;&#28857;&#21644;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#26368;&#32456;&#30830;&#23450;&#20102;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.03027</link><description>&lt;p&gt;
&#24067;&#38647;-&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#35757;&#32451;&#19979;&#30340;&#29983;&#25104;&#24335;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#30340;&#20851;&#38190;&#28857;&#21644;&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Critical Points and Convergence Analysis of Generative Deep Linear Networks Trained with Bures-Wasserstein Loss. (arXiv:2303.03027v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#24067;&#38647;-&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#35757;&#32451;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#28145;&#24230;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;&#65292;&#24182;&#22312;&#26377;&#38480;&#31209;&#30697;&#38453;&#31354;&#38388;&#20869;&#34920;&#24449;&#20851;&#38190;&#28857;&#21644;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#26368;&#32456;&#30830;&#23450;&#20102;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#20351;&#29992;&#24067;&#38647;-&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#35757;&#32451;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#28145;&#24230;&#30697;&#38453;&#20998;&#35299;&#27169;&#22411;&#12290;&#30456;&#36739;&#20110;&#20197;&#24448;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#25439;&#22833;&#20989;&#25968;&#21644;&#29983;&#25104;&#24335;&#35774;&#32622;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#22312;&#26377;&#38480;&#31209;&#30697;&#38453;&#31354;&#38388;&#20869;&#34920;&#24449;&#20102;&#35813;&#26041;&#27861;&#30340;&#20851;&#38190;&#28857;&#21644;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#38024;&#23545;&#20302;&#31209;&#30697;&#38453;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#30340;&#28023;&#26862;&#30697;&#38453;&#29702;&#35770;&#19978;&#21487;&#33021;&#20250;&#29190;&#28856;&#65292;&#36825;&#20026;&#20248;&#21270;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20013;&#20351;&#29992;&#25439;&#22833;&#30340;&#24179;&#28369;&#24494;&#25200;&#29256;&#26412;&#26102;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#21021;&#22987;&#26435;&#37325;&#30340;&#19968;&#23450;&#20551;&#35774;&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#26377;&#38480;&#27493;&#38271;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a deep matrix factorization model of covariance matrices trained with the Bures-Wasserstein distance. While recent works have made important advances in the study of the optimization problem for overparametrized low-rank matrix approximation, much emphasis has been placed on discriminative settings and the square loss. In contrast, our model considers another interesting type of loss and connects with the generative setting. We characterize the critical points and minimizers of the Bures-Wasserstein distance over the space of rank-bounded matrices. For low-rank matrices the Hessian of this loss can theoretically blow up, which creates challenges to analyze convergence of optimizaton methods. We establish convergence results for gradient flow using a smooth perturbative version of the loss and convergence results for finite step size gradient descent under certain assumptions on the initial weights.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#65292;Random Feature Propagation (RFP)&#65292;&#36890;&#36807;&#20018;&#32852;&#36845;&#20195;&#31639;&#27861;&#30340;&#20013;&#38388;&#27493;&#39588;&#20197;&#35745;&#31639;&#20256;&#25773;&#30697;&#38453;&#30340;&#20027;&#29305;&#24449;&#21521;&#37327;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#38543;&#26426;&#29305;&#24449;&#21644;&#35889;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.02918</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#29305;&#24449;&#20256;&#25773;&#23454;&#29616;&#22270;&#20301;&#32622;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Graph Positional Encoding via Random Feature Propagation. (arXiv:2303.02918v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#65292;Random Feature Propagation (RFP)&#65292;&#36890;&#36807;&#20018;&#32852;&#36845;&#20195;&#31639;&#27861;&#30340;&#20013;&#38388;&#27493;&#39588;&#20197;&#35745;&#31639;&#20256;&#25773;&#30697;&#38453;&#30340;&#20027;&#29305;&#24449;&#21521;&#37327;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#38543;&#26426;&#29305;&#24449;&#21644;&#35889;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#24378;GNN&#65292;&#24050;&#32463;&#30740;&#31350;&#20102;&#20004;&#31181;&#20027;&#35201;&#30340;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#26041;&#26696;&#65306;&#38543;&#26426;&#29305;&#24449;&#21644;&#35889;&#20301;&#32622;&#32534;&#30721;&#12290;&#28982;&#32780;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20004;&#31181;&#22686;&#24378;&#26041;&#26696;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#28982;&#27809;&#26377;&#28165;&#26224;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#65292;&#23427;&#24314;&#31435;&#20102;&#19978;&#36848;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25913;&#36827;&#20102;&#23427;&#20204;&#12290;&#36825;&#31181;&#21517;&#20026;Random Feature Propagation (RFP)&#30340;&#26032;&#26041;&#27861;&#21463;&#21040;&#20102;&#24130;&#36845;&#20195;&#26041;&#27861;&#21450;&#20854;&#25512;&#24191;&#30340;&#21551;&#21457;&#12290;&#23427;&#23558;&#29992;&#20110;&#35745;&#31639;&#20256;&#25773;&#30697;&#38453;&#30340;&#20027;&#29305;&#24449;&#21521;&#37327;&#30340;&#36845;&#20195;&#31639;&#27861;&#30340;&#20960;&#20010;&#20013;&#38388;&#27493;&#39588;&#36827;&#34892;&#20018;&#32852;&#65292;&#20174;&#38543;&#26426;&#33410;&#28857;&#29305;&#24449;&#24320;&#22987;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#20256;&#25773;&#27493;&#39588;&#26159;&#22522;&#20110;&#22270;&#30456;&#20851;&#30340;&#20256;&#25773;&#31639;&#23376;&#30340;&#65292;&#36825;&#20123;&#31639;&#23376;&#21487;&#20197;&#26159;&#39044;&#23450;&#20041;&#30340;&#65292;&#20063;&#21487;&#20197;&#26159;&#23398;&#20064;&#24471;&#21040;&#30340;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;RFP&#30340;&#29702;&#35770;&#21644;&#32463;&#39564;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20026;&#20351;&#29992;&#38543;&#26426;&#29305;&#24449;&#12289;&#23558;&#26089;&#26399;&#20256;&#25773;&#27493;&#39588;&#32435;&#20837;&#32771;&#34385;&#20197;&#21450;&#20351;&#29992;&#22270;&#30456;&#20851;&#30340;&#20256;&#25773;&#31639;&#23376;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two main families of node feature augmentation schemes have been explored for enhancing GNNs: random features and spectral positional encoding. Surprisingly, however, there is still no clear understanding of the relation between these two augmentation schemes. Here we propose a novel family of positional encoding schemes which draws a link between the above two approaches and improves over both. The new approach, named Random Feature Propagation (RFP), is inspired by the power iteration method and its generalizations. It concatenates several intermediate steps of an iterative algorithm for computing the dominant eigenvectors of a propagation matrix, starting from random node features. Notably, these propagation steps are based on graph-dependent propagation operators that can be either predefined or learned. We explore the theoretical and empirical benefits of RFP. First, we provide theoretical justifications for using random features, for incorporating early propagation steps, and for
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26368;&#22823;&#27844;&#38706;&#20998;&#26512;&#22122;&#22768;&#36845;&#20195;&#31639;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#22914;&#26524;&#26356;&#26032;&#20989;&#25968;&#22312;L2-&#33539;&#25968;&#19979;&#26377;&#30028;&#19988;&#21152;&#24615;&#22122;&#22768;&#20026;&#21508;&#21521;&#21516;&#24615;&#39640;&#26031;&#22122;&#22768;&#65292;&#21017;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#21322;&#23553;&#38381;&#24418;&#24335;&#19979;&#30340;&#26368;&#22823;&#27844;&#38706;&#19978;&#30028;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#26356;&#26032;&#20989;&#25968;&#30340;&#20551;&#35774;&#22914;&#20309;&#24433;&#21709;&#22122;&#22768;&#30340;&#26368;&#20248;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2302.14518</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#22823;&#27844;&#38706;&#20998;&#26512;&#22122;&#22768;&#36845;&#20195;&#31639;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Generalization Error Bounds for Noisy, Iterative Algorithms via Maximal Leakage. (arXiv:2302.14518v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14518
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26368;&#22823;&#27844;&#38706;&#20998;&#26512;&#22122;&#22768;&#36845;&#20195;&#31639;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#22914;&#26524;&#26356;&#26032;&#20989;&#25968;&#22312;L2-&#33539;&#25968;&#19979;&#26377;&#30028;&#19988;&#21152;&#24615;&#22122;&#22768;&#20026;&#21508;&#21521;&#21516;&#24615;&#39640;&#26031;&#22122;&#22768;&#65292;&#21017;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#21322;&#23553;&#38381;&#24418;&#24335;&#19979;&#30340;&#26368;&#22823;&#27844;&#38706;&#19978;&#30028;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#26356;&#26032;&#20989;&#25968;&#30340;&#20551;&#35774;&#22914;&#20309;&#24433;&#21709;&#22122;&#22768;&#30340;&#26368;&#20248;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37319;&#29992;&#20449;&#24687;&#35770;&#26694;&#26550;&#26469;&#20998;&#26512;&#19968;&#31867;&#36845;&#20195;&#24335;&#12289;&#24102;&#26377;&#22122;&#22768;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#34892;&#20026;&#12290;&#30001;&#20110;&#36825;&#31867;&#31639;&#27861;&#20855;&#26377;&#38543;&#26426;&#24615;&#65292;&#24182;&#19988;&#21253;&#21547;&#24120;&#29992;&#30340;&#31639;&#27861;&#65288;&#22914;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#65289;&#65292;&#25152;&#20197;&#22312;&#20449;&#24687;&#35770;&#24230;&#37327;&#19979;&#30740;&#31350;&#23427;&#20204;&#23588;&#20026;&#21512;&#36866;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#22823;&#27844;&#38706;&#65288;&#31561;&#20215;&#20110;&#26080;&#31351;&#38454; Sibson &#20114;&#20449;&#24687;&#65289;&#24230;&#37327;&#65292;&#22240;&#20854;&#26131;&#20110;&#20998;&#26512;&#19988;&#21487;&#20197;&#21516;&#26102;&#33719;&#24471;&#27867;&#21270;&#35823;&#24046;&#22823;&#27010;&#29575;&#19978;&#30028;&#21644;&#26399;&#26395;&#20540;&#19978;&#30028;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22914;&#26524;&#26356;&#26032;&#20989;&#25968;&#65288;&#22914;&#26799;&#24230;&#65289;&#22312;L2-&#33539;&#25968;&#19979;&#26377;&#30028;&#65292;&#19988;&#21152;&#24615;&#22122;&#22768;&#20026;&#21508;&#21521;&#21516;&#24615;&#39640;&#26031;&#22122;&#22768;&#65292;&#21017;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#21322;&#23553;&#38381;&#24418;&#24335;&#19979;&#30340;&#26368;&#22823;&#27844;&#38706;&#19978;&#30028;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26356;&#26032;&#20989;&#25968;&#30340;&#20551;&#35774;&#22914;&#20309;&#24433;&#21709;&#22122;&#22768;&#30340;&#26368;&#20248;&#36873;&#25321;&#65288;&#21363;&#26368;&#23567;&#21270;&#20135;&#29983;&#30340;&#26368;&#22823;&#27844;&#38706;&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35745;&#31639;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We adopt an information-theoretic framework to analyze the generalization behavior of the class of iterative, noisy learning algorithms. This class is particularly suitable for study under information-theoretic metrics as the algorithms are inherently randomized, and it includes commonly used algorithms such as Stochastic Gradient Langevin Dynamics (SGLD). Herein, we use the maximal leakage (equivalently, the Sibson mutual information of order infinity) metric, as it is simple to analyze, and it implies both bounds on the probability of having a large generalization error and on its expected value. We show that, if the update function (e.g., gradient) is bounded in $L_2$-norm and the additive noise is isotropic Gaussian noise, then one can obtain an upper-bound on maximal leakage in semi-closed form. Furthermore, we demonstrate how the assumptions on the update function affect the optimal (in the sense of minimizing the induced maximal leakage) choice of the noise. Finally, we compute 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20351;&#29992;&#26143;&#31995;&#30446;&#24405;&#36827;&#34892;&#22330;&#22320;&#32423;&#21035;&#30340;&#26080;&#30456;&#20284;&#24230;&#25512;&#26029;&#65292;&#21487;&#20197;&#22312;&#19981;&#21463;&#22825;&#25991;&#29289;&#29702;&#23398;&#21644;&#23376;&#32593;&#26684;&#27169;&#22411;&#21464;&#21270;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#39640;&#31934;&#24230;&#25512;&#26029;&#20986;&#937;m&#30340;&#20540;&#12290;</title><link>http://arxiv.org/abs/2302.14101</link><description>&lt;p&gt;
&#20855;&#26377;&#26143;&#31995;&#30340;&#40065;&#26834;&#22330;&#22320;&#32423;&#21035;&#26080;&#30456;&#20284;&#24230;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Robust Field-level Likelihood-free Inference with Galaxies. (arXiv:2302.14101v2 [astro-ph.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14101
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20351;&#29992;&#26143;&#31995;&#30446;&#24405;&#36827;&#34892;&#22330;&#22320;&#32423;&#21035;&#30340;&#26080;&#30456;&#20284;&#24230;&#25512;&#26029;&#65292;&#21487;&#20197;&#22312;&#19981;&#21463;&#22825;&#25991;&#29289;&#29702;&#23398;&#21644;&#23376;&#32593;&#26684;&#27169;&#22411;&#21464;&#21270;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#39640;&#31934;&#24230;&#25512;&#26029;&#20986;&#937;m&#30340;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;CAMELS&#39033;&#30446;&#26368;&#20808;&#36827;&#30340;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;&#26143;&#31995;&#30446;&#24405;&#26469;&#35757;&#32451;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#22330;&#22320;&#32423;&#21035;&#30340;&#26080;&#30456;&#20284;&#24230;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#26059;&#36716;&#12289;&#24179;&#31227;&#21644;&#32622;&#25442;&#30340;&#19981;&#21464;&#24615;&#65292;&#24182;&#19988;&#19981;&#23545;&#23610;&#24230;&#26045;&#21152;&#20219;&#20309;&#38480;&#21046;&#12290;&#20174;&#20165;&#21253;&#21547;&#22823;&#32422;1000&#20010;&#26143;&#31995;&#30340;&#19977;&#32500;&#20301;&#32622;&#21644;&#24452;&#21521;&#36895;&#24230;&#30340;&#26143;&#31995;&#30446;&#24405;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20197;&#22823;&#32422;12%&#30340;&#31934;&#24230;&#25512;&#26029;&#20986;&#937;m&#30340;&#20540;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#36890;&#36807;&#22312;&#26469;&#33258;&#25968;&#21315;&#20010;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;&#26143;&#31995;&#30446;&#24405;&#19978;&#27979;&#35797;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#25311;&#37117;&#26377;&#19981;&#21516;&#30340;&#36229;&#26032;&#26143;&#21644;AGN&#21453;&#39304;&#25928;&#29575;&#65292;&#24182;&#20351;&#29992;&#20116;&#31181;&#19981;&#21516;&#30340;&#20195;&#30721;&#21644;&#23376;&#32593;&#26684;&#27169;&#22411;&#36816;&#34892;- IllustrisTNG&#65292;SIMBA&#65292;Astrid&#65292;Magneticum&#65292;SWIFT-EAGLE - &#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#22825;&#20307;&#29289;&#29702;&#23398;&#12289;&#23376;&#32593;&#26684;&#29289;&#29702;&#23398;&#21644;&#20122;&#21704;&#27931;/&#26143;&#31995;&#21457;&#29616;&#22120;&#30340;&#21464;&#21270;&#26159;&#31283;&#20581;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#28085;&#30422;&#21442;&#25968;&#31354;&#38388;&#24191;&#27867;&#30340;1,024&#20010;&#27169;&#25311;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;- &#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
We train graph neural networks to perform field-level likelihood-free inference using galaxy catalogs from state-of-the-art hydrodynamic simulations of the CAMELS project. Our models are rotational, translational, and permutation invariant and do not impose any cut on scale. From galaxy catalogs that only contain $3$D positions and radial velocities of $\sim 1, 000$ galaxies in tiny $(25~h^{-1}{\rm Mpc})^3$ volumes our models can infer the value of $\Omega_{\rm m}$ with approximately $12$ % precision. More importantly, by testing the models on galaxy catalogs from thousands of hydrodynamic simulations, each having a different efficiency of supernova and AGN feedback, run with five different codes and subgrid models - IllustrisTNG, SIMBA, Astrid, Magneticum, SWIFT-EAGLE -, we find that our models are robust to changes in astrophysics, subgrid physics, and subhalo/galaxy finder. Furthermore, we test our models on $1,024$ simulations that cover a vast region in parameter space - variation
&lt;/p&gt;</description></item><item><title>CO-BED&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#30340;&#20449;&#24687;&#29702;&#35770;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#20248;&#21270;&#12290;&#23427;&#37319;&#29992;&#40657;&#31665;&#21464;&#20998;&#26041;&#27861;&#21516;&#26102;&#20272;&#35745;&#21644;&#20248;&#21270;&#35774;&#35745;&#65292;&#21487;&#20197;&#36866;&#24212;&#31163;&#25955;&#21160;&#20316;&#65292;&#24182;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.14015</link><description>&lt;p&gt;
CO-BED&#65306;&#36890;&#36807;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#30340;&#20449;&#24687;&#29702;&#35770;&#19978;&#19979;&#25991;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
CO-BED: Information-Theoretic Contextual Optimization via Bayesian Experimental Design. (arXiv:2302.14015v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14015
&lt;/p&gt;
&lt;p&gt;
CO-BED&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#30340;&#20449;&#24687;&#29702;&#35770;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#20248;&#21270;&#12290;&#23427;&#37319;&#29992;&#40657;&#31665;&#21464;&#20998;&#26041;&#27861;&#21516;&#26102;&#20272;&#35745;&#21644;&#20248;&#21270;&#35774;&#35745;&#65292;&#21487;&#20197;&#36866;&#24212;&#31163;&#25955;&#21160;&#20316;&#65292;&#24182;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#30340;&#35270;&#35282;&#23545;&#19978;&#19979;&#25991;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;CO-BED - &#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#20449;&#24687;&#29702;&#35770;&#21407;&#21017;&#35774;&#35745;&#19978;&#19979;&#25991;&#23454;&#39564;&#12290;&#22312;&#21046;&#23450;&#21512;&#36866;&#30340;&#22522;&#20110;&#20449;&#24687;&#30340;&#30446;&#26631;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#40657;&#31665;&#21464;&#20998;&#26041;&#27861;&#22312;&#21333;&#19968;&#38543;&#26426;&#26799;&#24230;&#26041;&#26696;&#20013;&#21516;&#26102;&#20272;&#35745;&#21644;&#20248;&#21270;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36866;&#24212;&#25105;&#20204;&#26694;&#26550;&#20013;&#30340;&#31163;&#25955;&#21160;&#20316;&#65292;&#25105;&#20204;&#25552;&#35758;&#21033;&#29992;&#36830;&#32493;&#26494;&#24347;&#26041;&#26696;&#65292;&#36825;&#21487;&#20197;&#33258;&#28982;&#22320;&#38598;&#25104;&#21040;&#25105;&#20204;&#21464;&#20998;&#30446;&#26631;&#20013;&#12290;&#22240;&#27492;&#65292;CO-BED&#20026;&#21508;&#31181;&#19978;&#19979;&#25991;&#20248;&#21270;&#38382;&#39064;&#25552;&#20379;&#20102;&#36890;&#29992;&#30340;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;&#35768;&#22810;&#23454;&#39564;&#20013;&#28436;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#19982;&#23450;&#21046;&#30340;&#12289;&#29305;&#23450;&#20110;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#30456;&#27604;&#65292;CO-BED&#20063;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We formalize the problem of contextual optimization through the lens of Bayesian experimental design and propose CO-BED -- a general, model-agnostic framework for designing contextual experiments using information-theoretic principles. After formulating a suitable information-based objective, we employ black-box variational methods to simultaneously estimate it and optimize the designs in a single stochastic gradient scheme. In addition, to accommodate discrete actions within our framework, we propose leveraging continuous relaxation schemes, which can naturally be integrated into our variational objective. As a result, CO-BED provides a general and automated solution to a wide range of contextual optimization problems. We illustrate its effectiveness in a number of experiments, where CO-BED demonstrates competitive performance even when compared to bespoke, model-specific alternatives.
&lt;/p&gt;</description></item><item><title>AlpaServe&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26381;&#21153;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#27169;&#22411;&#24182;&#34892;&#21644;&#32479;&#35745;&#22797;&#29992;&#65292;&#22312;&#25552;&#20379;&#22810;&#20010;&#27169;&#22411;&#26381;&#21153;&#26102;&#38477;&#20302;&#24310;&#36831;&#65292;&#25552;&#39640;&#22788;&#29702;&#36895;&#29575;&#21644;&#31361;&#21457;&#36127;&#36733;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.11665</link><description>&lt;p&gt;
AlpaServe&#65306;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26381;&#21153;&#30340;&#27169;&#22411;&#24182;&#34892;&#30340;&#32479;&#35745;&#22797;&#29992;
&lt;/p&gt;
&lt;p&gt;
AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving. (arXiv:2302.11665v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11665
&lt;/p&gt;
&lt;p&gt;
AlpaServe&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26381;&#21153;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#27169;&#22411;&#24182;&#34892;&#21644;&#32479;&#35745;&#22797;&#29992;&#65292;&#22312;&#25552;&#20379;&#22810;&#20010;&#27169;&#22411;&#26381;&#21153;&#26102;&#38477;&#20302;&#24310;&#36831;&#65292;&#25552;&#39640;&#22788;&#29702;&#36895;&#29575;&#21644;&#31361;&#21457;&#36127;&#36733;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#27169;&#22411;&#24182;&#34892;&#34987;&#35270;&#20026;&#19968;&#31181;&#23558;&#21333;&#20010;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25193;&#23637;&#21040;&#21333;&#20010;&#35774;&#22791;&#20869;&#23384;&#38480;&#21046;&#20043;&#22806;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#21363;&#20351;&#21333;&#20010;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#21333;&#20010;&#35774;&#22791;&#65292;&#27169;&#22411;&#24182;&#34892;&#36824;&#21487;&#20197;&#29992;&#20110;&#22810;&#20010;&#27169;&#22411;&#30340;&#32479;&#35745;&#22797;&#29992;&#65292;&#20197;&#38477;&#20302;&#25552;&#20379;&#26381;&#21153;&#26102;&#30340;&#24310;&#36831;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#26032;&#30340;&#26435;&#34913;&#31354;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26381;&#21153;&#31995;&#32479;AlpaServe&#65292;&#23427;&#30830;&#23450;&#20102;&#22312;&#20998;&#24067;&#24335;&#38598;&#32676;&#20013;&#25918;&#32622;&#21644;&#24182;&#34892;&#22788;&#29702;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38598;&#21512;&#30340;&#39640;&#25928;&#31574;&#30053;&#12290;&#29983;&#20135;&#24037;&#20316;&#36127;&#36733;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;AlpaServe&#21487;&#20197;&#22312;&#28385;&#36275;&#36229;&#36807;99%&#35831;&#27714;&#30340;&#24310;&#36831;&#32422;&#26463;&#30340;&#21516;&#26102;&#65292;&#20197;&#39640;&#36798;10&#20493;&#30340;&#36895;&#29575;&#22788;&#29702;&#35831;&#27714;&#25110;&#32773;&#22788;&#29702;6&#20493;&#20197;&#19978;&#30340;&#31361;&#21457;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model parallelism is conventionally viewed as a method to scale a single large deep learning model beyond the memory limits of a single device. In this paper, we demonstrate that model parallelism can be additionally used for the statistical multiplexing of multiple devices when serving multiple models, even when a single model can fit into a single device. Our work reveals a fundamental trade-off between the overhead introduced by model parallelism and the opportunity to exploit statistical multiplexing to reduce serving latency in the presence of bursty workloads. We explore the new trade-off space and present a novel serving system, AlpaServe, that determines an efficient strategy for placing and parallelizing collections of large deep learning models across a distributed cluster. Evaluation results on production workloads show that AlpaServe can process requests at up to 10x higher rates or 6x more burstiness while staying within latency constraints for more than 99% of requests.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#20851;&#20110;&#22312;&#32570;&#20047;&#30495;&#23454;&#35299;&#37322;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#21487;&#38752;&#20272;&#31639;&#35299;&#37322;&#26041;&#27861;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#36136;&#37327;&#20272;&#35745;&#22120;&#36827;&#34892;&#20803;&#35780;&#20272;&#65292;&#21033;&#29992;MetaQuantus&#26694;&#26550;&#20998;&#26512;&#20102;&#20272;&#35745;&#22120;&#30340;&#38887;&#24615;&#21644;&#21453;&#24212;&#29305;&#24449;&#65292;&#20174;&#32780;&#24110;&#21161;&#23454;&#36341;&#32773;&#36873;&#25321;&#26368;&#20339;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.07265</link><description>&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20803;&#35780;&#20272;&#38382;&#39064;&#65306;&#20351;&#29992;MetaQuantus&#35782;&#21035;&#21487;&#38752;&#30340;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
The Meta-Evaluation Problem in Explainable AI: Identifying Reliable Estimators with MetaQuantus. (arXiv:2302.07265v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07265
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#20851;&#20110;&#22312;&#32570;&#20047;&#30495;&#23454;&#35299;&#37322;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#21487;&#38752;&#20272;&#31639;&#35299;&#37322;&#26041;&#27861;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#36136;&#37327;&#20272;&#35745;&#22120;&#36827;&#34892;&#20803;&#35780;&#20272;&#65292;&#21033;&#29992;MetaQuantus&#26694;&#26550;&#20998;&#26512;&#20102;&#20272;&#35745;&#22120;&#30340;&#38887;&#24615;&#21644;&#21453;&#24212;&#29305;&#24449;&#65292;&#20174;&#32780;&#24110;&#21161;&#23454;&#36341;&#32773;&#36873;&#25321;&#26368;&#20339;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#20013;&#65292;&#30830;&#23450;&#22312;&#27809;&#26377;&#30495;&#23454;&#35299;&#37322;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#26368;&#21487;&#38752;&#22320;&#20272;&#31639;&#35299;&#37322;&#26041;&#27861;&#30340;&#36136;&#37327;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#31454;&#20105;&#35780;&#20272;&#26041;&#27861;&#65288;&#25110;&#8220;&#36136;&#37327;&#20272;&#35745;&#22120;&#8221;&#65289;&#29983;&#25104;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#26088;&#22312;&#34913;&#37327;&#35299;&#37322;&#26041;&#27861;&#30340;&#30456;&#21516;&#24615;&#36136;&#65292;&#32463;&#24120;&#21576;&#29616;&#20986;&#19981;&#19968;&#33268;&#30340;&#25490;&#21517;&#12290;&#36825;&#26679;&#30340;&#20998;&#27495;&#23545;&#20110;&#23454;&#36341;&#32773;&#26469;&#35828;&#24456;&#38590;&#35299;&#37322;&#65292;&#20174;&#32780;&#20351;&#20182;&#20204;&#38590;&#20197;&#36873;&#25321;&#34920;&#29616;&#26368;&#22909;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;XAI&#20013;&#30340;&#19981;&#21516;&#36136;&#37327;&#20272;&#35745;&#22120;&#36827;&#34892;&#20803;&#35780;&#20272;&#65288;"&#35780;&#20272;&#35780;&#20272;&#26041;&#27861;&#30340;&#36807;&#31243;"&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26032;&#26694;&#26550;MetaQuantus&#20998;&#26512;&#20102;&#36136;&#37327;&#20272;&#35745;&#22120;&#30340;&#20004;&#20010;&#20114;&#34917;&#24615;&#24615;&#33021;&#29305;&#24449;&#65306;&#23545;&#22122;&#22768;&#30340;&#38887;&#24615;&#21644;&#23545;&#38543;&#26426;&#24615;&#30340;&#21453;&#24212;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#23545;&#30495;&#23454;&#26631;&#31614;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the unsolved challenges in the field of Explainable AI (XAI) is determining how to most reliably estimate the quality of an explanation method in the absence of ground truth explanation labels. Resolving this issue is of utmost importance as the evaluation outcomes generated by competing evaluation methods (or ''quality estimators''), which aim at measuring the same property of an explanation method, frequently present conflicting rankings. Such disagreements can be challenging for practitioners to interpret, thereby complicating their ability to select the best-performing explanation method. We address this problem through a meta-evaluation of different quality estimators in XAI, which we define as ''the process of evaluating the evaluation method''. Our novel framework, MetaQuantus, analyses two complementary performance characteristics of a quality estimator: its resilience to noise and reactivity to randomness, thus circumventing the need for ground truth labels. We demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;ConCerNet&#65292;&#29992;&#20110;&#25552;&#39640;DNN&#21160;&#21147;&#23398;&#24314;&#27169;&#30340;&#21487;&#38752;&#24615;&#65292;&#23454;&#29616;&#23545;&#31995;&#32479;&#19981;&#21464;&#37327;&#30340;&#33258;&#21160;&#25429;&#25417;&#21644;&#20445;&#30041;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.05783</link><description>&lt;p&gt;
ConCerNet&#65306;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#33258;&#21160;&#21457;&#29616;&#23432;&#24658;&#24459;&#21644;&#21487;&#38752;&#21160;&#21147;&#23398;&#31995;&#32479;&#39044;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ConCerNet: A Contrastive Learning Based Framework for Automated Conservation Law Discovery and Trustworthy Dynamical System Prediction. (arXiv:2302.05783v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;ConCerNet&#65292;&#29992;&#20110;&#25552;&#39640;DNN&#21160;&#21147;&#23398;&#24314;&#27169;&#30340;&#21487;&#38752;&#24615;&#65292;&#23454;&#29616;&#23545;&#31995;&#32479;&#19981;&#21464;&#37327;&#30340;&#33258;&#21160;&#25429;&#25417;&#21644;&#20445;&#30041;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#20854;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#22312;&#21160;&#21147;&#23398;&#31995;&#32479;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#20986;&#26497;&#22823;&#30340;&#33021;&#21147;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#19981;&#36981;&#23432;&#29289;&#29702;&#32422;&#26463;&#65292;&#22914;&#23432;&#24658;&#23450;&#24459;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ConCerNet&#30340;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;DNN&#30340;&#21160;&#21147;&#23398;&#24314;&#27169;&#30340;&#21487;&#38752;&#24615;&#65292;&#36171;&#20104;&#19981;&#21464;&#30340;&#23646;&#24615;&#12290;ConCerNet&#30001;&#20004;&#20010;&#27493;&#39588;&#32452;&#25104;:(i)&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#33258;&#21160;&#25429;&#25417;&#36712;&#36857;&#35266;&#27979;&#20013;&#30340;&#31995;&#32479;&#19981;&#21464;&#37327;(&#21363;&#23432;&#24658;&#24615;&#36136;)&#65307;(ii)&#31070;&#32463;&#25237;&#24433;&#23618;&#65292;&#20445;&#35777;&#23398;&#20064;&#21040;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#20445;&#30041;&#23398;&#20064;&#21040;&#30340;&#19981;&#21464;&#37327;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23398;&#20064;&#21040;&#30340;&#28508;&#22312;&#34920;&#31034;&#21644;&#26410;&#30693;&#31995;&#32479;&#19981;&#21464;&#37327;&#20989;&#25968;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22352;&#26631;&#35823;&#24046;&#21644;&#23432;&#24658;&#25351;&#26631;&#26041;&#38754;&#22987;&#32456;&#27604;&#22522;&#32447;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#20351;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#21270;&#19988;&#19981;&#20381;&#36182;&#20110;&#20808;&#21069;&#30693;&#35782;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21160;&#21147;&#23398;&#26041;&#38754;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNN) have shown great capacity of modeling a dynamical system; nevertheless, they usually do not obey physics constraints such as conservation laws. This paper proposes a new learning framework named ConCerNet to improve the trustworthiness of the DNN based dynamics modeling to endow the invariant properties. ConCerNet consists of two steps: (i) a contrastive learning method to automatically capture the system invariants (i.e. conservation properties) along the trajectory observations; (ii) a neural projection layer to guarantee that the learned dynamics models preserve the learned invariants. We theoretically prove the functional relationship between the learned latent representation and the unknown system invariant function. Experiments show that our method consistently outperforms the baseline neural networks in both coordinate error and conservation metrics by a large margin. With neural network based parameterization and no dependence on prior knowledge, our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#26367;&#20195;&#27169;&#22411;&#26356;&#36125;&#21494;&#26031;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25915;&#20987;&#36125;&#21494;&#26031;&#27169;&#22411;&#20197;&#23454;&#29616;&#29702;&#24819;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#21487;&#36801;&#31227;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#24120;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.05086</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#26367;&#20195;&#27169;&#22411;&#26356;&#36125;&#21494;&#26031;&#21270;&#65292;&#21487;&#20197;&#22686;&#24378;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#21487;&#36801;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
Making Substitute Models More Bayesian Can Enhance Transferability of Adversarial Examples. (arXiv:2302.05086v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#26367;&#20195;&#27169;&#22411;&#26356;&#36125;&#21494;&#26031;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25915;&#20987;&#36125;&#21494;&#26031;&#27169;&#22411;&#20197;&#23454;&#29616;&#29702;&#24819;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#21487;&#36801;&#31227;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#24120;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#26679;&#26412;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#21487;&#36801;&#31227;&#24615;&#26159;&#35768;&#22810;&#40657;&#30418;&#25915;&#20987;&#30340;&#20851;&#38190;&#12290;&#20197;&#24448;&#30340;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#19968;&#20123;&#26367;&#20195;&#27169;&#22411;&#36755;&#20837;&#30340;&#22810;&#26679;&#24615;&#19978;&#20197;&#25913;&#21892;&#21487;&#36801;&#31227;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#25991;&#36873;&#25321;&#20102;&#26367;&#20195;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#20986;&#25915;&#20987;&#36125;&#21494;&#26031;&#27169;&#22411;&#20197;&#23454;&#29616;&#29702;&#24819;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#22522;&#20110;&#36125;&#21494;&#26031;&#20844;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#31934;&#35843;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#19982;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#35768;&#22810;&#24120;&#35265;&#39640;&#26031;&#21518;&#39564;&#36924;&#36817;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#20123;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#26032;&#30340;&#29616;&#26377;&#26041;&#27861;&#65288;&#22312;ImageNet&#19978;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#24179;&#22343;&#32477;&#23545;&#22686;&#21152;&#29575;&#32422;&#20026;19%&#65289;&#65292;&#24182;&#19988;&#19982;&#36825;&#20123;&#26368;&#26032;&#26041;&#27861;&#30340;&#32467;&#21512;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;: https:
&lt;/p&gt;
&lt;p&gt;
The transferability of adversarial examples across deep neural networks (DNNs) is the crux of many black-box attacks. Many prior efforts have been devoted to improving the transferability via increasing the diversity in inputs of some substitute models. In this paper, by contrast, we opt for the diversity in substitute models and advocate to attack a Bayesian model for achieving desirable transferability. Deriving from the Bayesian formulation, we develop a principled strategy for possible finetuning, which can be combined with many off-the-shelf Gaussian posterior approximations over DNN parameters. Extensive experiments have been conducted to verify the effectiveness of our method, on common benchmark datasets, and the results demonstrate that our method outperforms recent state-of-the-arts by large margins (roughly 19% absolute increase in average attack success rate on ImageNet), and, by combining with these recent methods, further performance gain can be obtained. Our code: https:
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#39034;&#24207;&#26680;&#29420;&#31435;&#24615;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#25209;&#37327;&#27979;&#35797;&#22312;&#27969;&#25968;&#25454;&#19978;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26681;&#25454;&#20219;&#21153;&#22797;&#26434;&#24615;&#33258;&#36866;&#24212;&#35843;&#25972;&#26679;&#26412;&#22823;&#23567;&#65292;&#24182;&#22312;&#25910;&#38598;&#26032;&#25968;&#25454;&#21518;&#25345;&#32493;&#30417;&#27979;&#21644;&#25511;&#21046;&#35823;&#25253;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.07383</link><description>&lt;p&gt;
&#39034;&#24207;&#26680;&#29420;&#31435;&#24615;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Sequential Kernelized Independence Testing. (arXiv:2212.07383v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07383
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#39034;&#24207;&#26680;&#29420;&#31435;&#24615;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#25209;&#37327;&#27979;&#35797;&#22312;&#27969;&#25968;&#25454;&#19978;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#26681;&#25454;&#20219;&#21153;&#22797;&#26434;&#24615;&#33258;&#36866;&#24212;&#35843;&#25972;&#26679;&#26412;&#22823;&#23567;&#65292;&#24182;&#22312;&#25910;&#38598;&#26032;&#25968;&#25454;&#21518;&#25345;&#32493;&#30417;&#27979;&#21644;&#25511;&#21046;&#35823;&#25253;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29420;&#31435;&#24615;&#27979;&#35797;&#26159;&#19968;&#20010;&#32463;&#20856;&#30340;&#32479;&#35745;&#38382;&#39064;&#65292;&#22312;&#22266;&#23450;&#37319;&#38598;&#25968;&#25454;&#20043;&#21069;&#30340;&#25209;&#37327;&#35774;&#32622;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#23454;&#36341;&#32773;&#20204;&#24448;&#24448;&#26356;&#21916;&#27426;&#33021;&#22815;&#26681;&#25454;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#36827;&#34892;&#33258;&#36866;&#24212;&#30340;&#31243;&#24207;&#65292;&#32780;&#19981;&#26159;&#20107;&#20808;&#35774;&#23450;&#26679;&#26412;&#22823;&#23567;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#36825;&#26679;&#30340;&#31243;&#24207;&#24212;&#35813;&#65288;a&#65289;&#22312;&#31616;&#21333;&#20219;&#21153;&#19978;&#23613;&#26089;&#20572;&#27490;&#65288;&#22312;&#22256;&#38590;&#20219;&#21153;&#19978;&#31245;&#21518;&#20572;&#27490;&#65289;&#65292;&#22240;&#27492;&#26356;&#22909;&#22320;&#21033;&#29992;&#21487;&#29992;&#36164;&#28304;&#65292;&#20197;&#21450;&#65288;b&#65289;&#22312;&#25910;&#38598;&#26032;&#25968;&#25454;&#20043;&#21518;&#65292;&#25345;&#32493;&#30417;&#27979;&#25968;&#25454;&#24182;&#39640;&#25928;&#22320;&#25972;&#21512;&#32479;&#35745;&#35777;&#25454;&#65292;&#21516;&#26102;&#25511;&#21046;&#35823;&#25253;&#29575;&#12290;&#32463;&#20856;&#30340;&#25209;&#37327;&#27979;&#35797;&#19981;&#36866;&#29992;&#20110;&#27969;&#25968;&#25454;&#65306;&#22312;&#25968;&#25454;&#35266;&#23519;&#21518;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#38656;&#35201;&#23545;&#22810;&#37325;&#27979;&#35797;&#36827;&#34892;&#26657;&#27491;&#65292;&#36825;&#23548;&#33268;&#20102;&#20302;&#21151;&#29575;&#12290;&#36981;&#24490;&#36890;&#36807;&#25237;&#27880;&#36827;&#34892;&#27979;&#35797;&#30340;&#21407;&#21017;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#39034;&#24207;&#26680;&#29420;&#31435;&#24615;&#27979;&#35797;&#65292;&#20811;&#26381;&#20102;&#36825;&#20123;&#32570;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#30001;&#26680;&#30456;&#20851;&#24615;&#27979;&#24230;&#65288;&#22914;Hilbert-&#65289;&#21551;&#21457;&#30340;&#25237;&#27880;&#26469;&#35828;&#26126;&#25105;&#20204;&#30340;&#24191;&#27867;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Independence testing is a classical statistical problem that has been extensively studied in the batch setting when one fixes the sample size before collecting data. However, practitioners often prefer procedures that adapt to the complexity of a problem at hand instead of setting sample size in advance. Ideally, such procedures should (a) stop earlier on easy tasks (and later on harder tasks), hence making better use of available resources, and (b) continuously monitor the data and efficiently incorporate statistical evidence after collecting new data, while controlling the false alarm rate. Classical batch tests are not tailored for streaming data: valid inference after data peeking requires correcting for multiple testing which results in low power. Following the principle of testing by betting, we design sequential kernelized independence tests that overcome such shortcomings. We exemplify our broad framework using bets inspired by kernelized dependence measures, e.g., the Hilbert-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#24565;&#24615;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#22312;&#22330;&#23398;&#20064;&#32773;&#23398;&#20064;&#26032;&#25216;&#33021;&#12290;&#36890;&#36807;&#36873;&#25321;&#19982;&#39044;&#27979;&#31034;&#20363;&#20849;&#20139;&#21487;&#33021;&#20449;&#24687;&#30340;&#28436;&#31034;&#65292;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#22312;&#27169;&#22411;&#35760;&#24518;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#30340;&#22312;&#22330;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.01692</link><description>&lt;p&gt;
&#22312;&#22330;&#23398;&#20064;&#32773;&#33021;&#21542;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#25512;&#29702;&#27010;&#24565;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can In-context Learners Learn a Reasoning Concept from Demonstrations?. (arXiv:2212.01692v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#24565;&#24615;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#22312;&#22330;&#23398;&#20064;&#32773;&#23398;&#20064;&#26032;&#25216;&#33021;&#12290;&#36890;&#36807;&#36873;&#25321;&#19982;&#39044;&#27979;&#31034;&#20363;&#20849;&#20139;&#21487;&#33021;&#20449;&#24687;&#30340;&#28436;&#31034;&#65292;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#22312;&#27169;&#22411;&#35760;&#24518;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#30340;&#22312;&#22330;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#20174;&#23569;&#37327;&#36755;&#20837;-&#36755;&#20986;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#26032;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#22330;&#23398;&#20064;&#32773;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#20182;&#20204;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#65292;&#22914;&#26631;&#31614;&#30340;&#24773;&#24863;&#65292;&#32780;&#19981;&#26159;&#22312;&#36755;&#20837;&#20013;&#25214;&#21040;&#26032;&#30340;&#20851;&#32852;&#24615;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#23569;&#26679;&#26412;&#35780;&#20272;&#35774;&#32622;&#20351;&#29992;&#38543;&#26426;&#36873;&#25321;&#30340;&#22312;&#22330;&#28436;&#31034;&#26080;&#27861;&#21306;&#20998;&#27169;&#22411;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#25216;&#33021;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#22823;&#37096;&#20998;&#38543;&#26426;&#36873;&#25321;&#30340;&#28436;&#31034;&#24182;&#19981;&#21576;&#29616;&#36229;&#36234;&#26292;&#38706;&#20110;&#26032;&#20219;&#21153;&#20998;&#24067;&#30340;&#39044;&#27979;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#22312;&#27169;&#22411;&#35760;&#24518;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#30340;&#22312;&#22330;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#36873;&#25321;&#19982;&#39044;&#27979;&#31034;&#20363;&#20849;&#20139;&#21487;&#33021;&#20449;&#24687;&#30340;&#28436;&#31034;&#12290;&#25105;&#20204;&#20174;&#27880;&#37322;&#35299;&#37322;&#20013;&#25552;&#21462;&#20102;&#19968;&#32452;&#36825;&#26679;&#30340;&#27010;&#24565;&#65292;&#24182;&#27979;&#37327;&#20102;&#27169;&#22411;&#23637;&#31034;&#36825;&#20123;&#27010;&#24565;&#21487;&#20197;&#33719;&#24471;&#22810;&#23569;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models show an emergent ability to learn a new task from a small number of input-output demonstrations. However, recent work shows that in-context learners largely rely on their pre-trained knowledge, such as the sentiment of the labels, instead of finding new associations in the input. However, the commonly-used few-shot evaluation settings using a random selection of in-context demonstrations can not disentangle models' ability to learn a new skill from demonstrations, as most of the randomly-selected demonstrations do not present relations informative for prediction beyond exposing the new task distribution.  To disentangle models' in-context learning ability independent of models' memory, we introduce a Conceptual few-shot learning method selecting the demonstrations sharing a possibly-informative concept with the predicted sample. We extract a set of such concepts from annotated explanations and measure how much can models benefit from presenting these concepts in f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25351;&#25968;&#22686;&#38271;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#65292;&#33021;&#22815;&#35299;&#20915;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36139;&#30240;&#39640;&#21407;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#37327;&#23376;&#32534;&#30721;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.00736</link><description>&lt;p&gt;
&#19968;&#20010;&#25351;&#25968;&#22686;&#38271;&#30340;&#36890;&#29992;&#37327;&#23376;&#30005;&#36335;&#31995;&#21015;
&lt;/p&gt;
&lt;p&gt;
An exponentially-growing family of universal quantum circuits. (arXiv:2212.00736v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00736
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25351;&#25968;&#22686;&#38271;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#65292;&#33021;&#22815;&#35299;&#20915;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36139;&#30240;&#39640;&#21407;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#37327;&#23376;&#32534;&#30721;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#20010;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#20851;&#27880;&#30340;&#39046;&#22495;&#65292;&#20294;&#23427;&#23384;&#22312;&#19968;&#23450;&#30340;&#29702;&#35770;&#21644;&#30828;&#20214;&#38480;&#21046;&#12290;&#29305;&#21035;&#26159;&#65292;&#28040;&#22833;&#26799;&#24230;&#38382;&#39064;&#25110;&#31216;&#20026;&#36139;&#30240;&#39640;&#21407;&#38382;&#39064;&#65292;&#20351;&#24471;&#23545;&#20110;&#25317;&#26377;&#22823;&#37327;&#37327;&#23376;&#27604;&#29305;&#30340;&#30005;&#36335;&#65292;&#35757;&#32451;&#21464;&#24471;&#19981;&#21487;&#33021;&#65292;&#38480;&#21046;&#20102;&#25968;&#25454;&#31185;&#23398;&#23478;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#21487;&#20197;&#20351;&#29992;&#30340;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#12290;&#21478;&#22806;&#65292;&#29420;&#31435;&#30340;&#35282;&#24230;&#23884;&#20837;&#30417;&#30563;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#34987;&#35777;&#26126;&#33021;&#22815;&#20135;&#29983;&#20855;&#26377;&#19982;&#32534;&#30721;&#28145;&#24230;&#21644;&#32534;&#30721;&#24212;&#29992;&#20110;&#30340;&#24182;&#34892;&#27604;&#29305;&#25968;&#30452;&#25509;&#30456;&#20851;&#30340;&#25130;&#26029;&#20613;&#37324;&#21494;&#32423;&#25968;&#12290;&#20613;&#37324;&#21494;&#32423;&#25968;&#30340;&#27425;&#25968;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#26412;&#24037;&#20316;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#20854;&#20613;&#37324;&#21494;&#32423;&#25968;&#30340;&#27425;&#25968;&#21576;&#25351;&#25968;&#22686;&#38271;&#65306;&#39034;&#24207;&#21644;&#24182;&#34892;&#30340;&#25351;&#25968;&#22686;&#38271;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#12290;&#36890;&#36807;&#22312;&#32534;&#30721;&#26102;&#39640;&#25928;&#22320;&#21033;&#29992;&#21487;&#29992;&#30340;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65292;&#22686;&#21152;&#20102;&#37327;&#23376;&#32534;&#30721;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25351;&#25968;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning has become an area of growing interest but has certain theoretical and hardware-specific limitations. Notably, the problem of vanishing gradients, or barren plateaus, renders the training impossible for circuits with high qubit counts, imposing a limit on the number of qubits that data scientists can use for solving problems. Independently, angle-embedded supervised quantum neural networks were shown to produce truncated Fourier series with a degree directly dependent on two factors: the depth of the encoding and the number of parallel qubits the encoding applied to. The degree of the Fourier series limits the model expressivity. This work introduces two new architectures whose Fourier degrees grow exponentially: the sequential and parallel exponential quantum machine learning architectures. This is done by efficiently using the available Hilbert space when encoding, increasing the expressivity of the quantum encoding. Therefore, the exponential growth allows s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;Softmax&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36817;&#20284;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#22522;&#20110;MC Dropout&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#20998;&#26512;&#21457;&#29616;&#65292;&#23613;&#31649;MC dropout&#20135;&#29983;&#20102;&#26368;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#36817;&#20284;&#65292;&#20294;&#20351;&#29992;softmax&#20063;&#33021;&#20135;&#29983;&#30456;&#23545;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.14037</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;Softmax&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Revisiting Softmax for Uncertainty Approximation in Text Classification. (arXiv:2210.14037v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;Softmax&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36817;&#20284;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#22522;&#20110;MC Dropout&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#20998;&#26512;&#21457;&#29616;&#65292;&#23613;&#31649;MC dropout&#20135;&#29983;&#20102;&#26368;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#36817;&#20284;&#65292;&#20294;&#20351;&#29992;softmax&#20063;&#33021;&#20135;&#29983;&#30456;&#23545;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36817;&#20284;&#26159;&#19968;&#20010;&#22312;&#39046;&#22495;&#36866;&#24212;&#21644;&#21487;&#35299;&#37322;&#24615;&#20013;&#24212;&#29992;&#24191;&#27867;&#30340;&#37325;&#35201;&#39046;&#22495;&#12290;&#20854;&#20013;&#19968;&#31181;&#26368;&#24120;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#36817;&#20284;&#26041;&#27861;&#26159;&#33945;&#29305;&#21345;&#32599;&#65288;MC&#65289;Dropout&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#22810;&#27425;&#21069;&#21521;&#20256;&#36882;&#65292;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#19968;&#31181;&#26356;&#20415;&#23452;&#30340;&#26041;&#27861;&#26159;&#20165;&#20351;&#29992;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#22522;&#20110;softmax&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#39044;&#27979;&#24448;&#24448;&#36807;&#20110;&#33258;&#20449;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#20004;&#31181;&#22522;&#26412;&#31070;&#32463;&#32467;&#26500;&#30340;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24443;&#24213;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#26088;&#22312;&#25506;&#35752;&#36825;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;softmax&#21644;MC Dropout&#30340;&#19981;&#30830;&#23450;&#24615;&#36817;&#20284;&#20197;&#21450;&#19979;&#28216;&#25991;&#26412;&#20998;&#31867;&#24615;&#33021;&#65292;&#21516;&#26102;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#36816;&#34892;&#26102;&#38388;&#65288;&#25104;&#26412;&#65289;&#21644;&#24615;&#33021;&#65288;&#25928;&#30410;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;MC dropout&#20135;&#29983;&#20102;&#26368;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#36817;&#20284;&#65292;&#20294;&#20351;&#29992;softmax&#20063;&#33021;&#20135;&#29983;&#30456;&#23545;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty approximation in text classification is an important area with applications in domain adaptation and interpretability. One of the most widely used uncertainty approximation methods is Monte Carlo (MC) Dropout, which is computationally expensive as it requires multiple forward passes through the model. A cheaper alternative is to simply use the softmax based on a single forward pass without dropout to estimate model uncertainty. However, prior work has indicated that these predictions tend to be overconfident. In this paper, we perform a thorough empirical analysis of these methods on five datasets with two base neural architectures in order to identify the trade-offs between the two. We compare both softmax and an efficient version of MC Dropout on their uncertainty approximations and downstream text classification performance, while weighing their runtime (cost) against performance (benefit). We find that, while MC dropout produces the best uncertainty approximations, usin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#21033;&#26222;&#24076;&#33576;&#38750;&#32447;&#24615;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22312;&#25932;&#23545;&#26631;&#31614;&#22122;&#22768;&#19979;&#25311;&#21512;&#32447;&#24615;&#20989;&#25968;&#65292;&#24182;&#22312;&#36924;&#36817;&#20445;&#35777;&#26041;&#38754;&#20855;&#26377;&#24378;&#26377;&#21147;&#30340;&#21487;&#35777;&#26126;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.13601</link><description>&lt;p&gt;
&#20855;&#26377;&#21033;&#26222;&#24076;&#33576;&#38750;&#32447;&#24615;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning for Single Neuron Models with Lipschitz Non-Linearities. (arXiv:2210.13601v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13601
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#21033;&#26222;&#24076;&#33576;&#38750;&#32447;&#24615;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22312;&#25932;&#23545;&#26631;&#31614;&#22122;&#22768;&#19979;&#25311;&#21512;&#32447;&#24615;&#20989;&#25968;&#65292;&#24182;&#22312;&#36924;&#36817;&#20445;&#35777;&#26041;&#38754;&#20855;&#26377;&#24378;&#26377;&#21147;&#30340;&#21487;&#35777;&#26126;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#25932;&#23545;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#23545;&#21333;&#20010;&#31070;&#32463;&#20803;&#27169;&#22411;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#26377;&#26102;&#20063;&#34987;&#31216;&#20026;&#8220;&#23725;&#20989;&#25968;&#8221;&#12290;&#36825;&#20123;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#24191;&#27867;&#26377;&#25928;&#22320;&#27169;&#25311;&#29289;&#29702;&#29616;&#35937;&#65292;&#24182;&#26500;&#24314;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#20195;&#29702;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#20855;&#26377;&#20219;&#20309;&#21033;&#26222;&#24076;&#33576;&#38750;&#32447;&#24615;&#65288;&#22914;ReLU&#20989;&#25968;&#12289;sigmoid&#20989;&#25968;&#12289;&#32477;&#23545;&#20540;&#20989;&#25968;&#12289;&#20302;&#27425;&#22810;&#39033;&#24335;&#20989;&#25968;&#31561;&#65289;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;&#24050;&#30693;&#30340;&#22312;&#25932;&#23545;&#26631;&#31614;&#22122;&#22768;&#19979;&#25311;&#21512;&#8220;&#32447;&#24615;&#20989;&#25968;&#8221;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#33719;&#24471;&#24378;&#26377;&#21147;&#30340;&#21487;&#35777;&#26126;&#30340;&#36924;&#36817;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of active learning for single neuron models, also sometimes called ``ridge functions'', in the agnostic setting (under adversarial label noise). Such models have been shown to be broadly effective in modeling physical phenomena, and for constructing surrogate data-driven models for partial differential equations.  Surprisingly, we show that for a single neuron model with any Lipschitz non-linearity (such as the ReLU, sigmoid, absolute value, low-degree polynomial, among others), strong provable approximation guarantees can be obtained using a well-known active learning strategy for fitting \emph{linear functions} in the agnostic setting. % -- i.e. for the case when there is no non-linearity. Namely, we can collect samples via statistical \emph{leverage score sampling}, which has been shown to be near-optimal in other active learning scenarios. We support our theoretical results with empirical simulations showing that our proposed active learning strategy based o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SurCo&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32447;&#24615;&#20195;&#29702;&#25104;&#26412;&#65292;&#23558;&#32452;&#21512;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#32447;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#26799;&#24230;&#26041;&#27861;&#21644;&#32447;&#24615;&#32452;&#21512;&#20248;&#21270;&#30340;&#32467;&#26500;&#25552;&#20379;&#20102;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2210.12547</link><description>&lt;p&gt;
SurCo&#65306;&#23398;&#20064;&#29992;&#20110;&#32452;&#21512;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#30340;&#32447;&#24615;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
SurCo: Learning Linear Surrogates For Combinatorial Nonlinear Optimization Problems. (arXiv:2210.12547v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12547
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SurCo&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32447;&#24615;&#20195;&#29702;&#25104;&#26412;&#65292;&#23558;&#32452;&#21512;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#32447;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#26799;&#24230;&#26041;&#27861;&#21644;&#32447;&#24615;&#32452;&#21512;&#20248;&#21270;&#30340;&#32467;&#26500;&#25552;&#20379;&#20102;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20855;&#26377;&#38750;&#32447;&#24615;&#20195;&#20215;&#20989;&#25968;&#21644;&#32452;&#21512;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#19982;&#20854;&#32447;&#24615;&#23545;&#24212;&#29289;&#30456;&#27604;&#65292;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38590;&#20197;&#39640;&#25928;&#27714;&#35299;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SurCo&#65292;&#23427;&#23398;&#20064;&#32447;&#24615;&#20195;&#29702;&#25104;&#26412;&#65292;&#21487;&#29992;&#20110;&#29616;&#26377;&#30340;&#32452;&#21512;&#27714;&#35299;&#22120;&#65292;&#20197;&#36755;&#20986;&#21407;&#22987;&#38750;&#32447;&#24615;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#33391;&#22909;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#23545;&#32447;&#24615;&#20195;&#29702;&#27714;&#35299;&#22120;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#23398;&#20064;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#25439;&#22833;&#20989;&#25968;&#24494;&#20998;&#65292;&#23558;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#19982;&#32447;&#24615;&#32452;&#21512;&#20248;&#21270;&#30340;&#32467;&#26500;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;SurCo&#21464;&#20307;&#65306;SurCo-zero&#29992;&#20110;&#21333;&#20010;&#38750;&#32447;&#24615;&#38382;&#39064;&#65292;SurCo-prior&#29992;&#20110;&#38382;&#39064;&#20998;&#24067;&#65292;SurCo-hybrid&#29992;&#20110;&#32467;&#21512;&#20998;&#24067;&#21644;&#38382;&#39064;&#29305;&#23450;&#20449;&#24687;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#29702;&#35770;&#19978;&#30340;&#30452;&#35273;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Optimization problems with nonlinear cost functions and combinatorial constraints appear in many real-world applications but remain challenging to solve efficiently compared to their linear counterparts. To bridge this gap, we propose $\textbf{SurCo}$ that learns linear $\underline{\text{Sur}}$rogate costs which can be used in existing $\underline{\text{Co}}$mbinatorial solvers to output good solutions to the original nonlinear combinatorial optimization problem. The surrogate costs are learned end-to-end with nonlinear loss by differentiating through the linear surrogate solver, combining the flexibility of gradient-based methods with the structure of linear combinatorial optimization. We propose three $\texttt{SurCo}$ variants: $\texttt{SurCo}-\texttt{zero}$ for individual nonlinear problems, $\texttt{SurCo}-\texttt{prior}$ for problem distributions, and $\texttt{SurCo}-\texttt{hybrid}$ to combine both distribution and problem-specific information. We give theoretical intuition motiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;VR-IWAE&#19979;&#30028;&#65292;&#35813;&#19979;&#30028;&#26159;IWAE&#19979;&#30028;&#30340;&#25512;&#24191;&#65292;&#37319;&#29992;&#26080;&#20559;&#26799;&#24230;&#20272;&#35745;&#22120;&#33021;&#22815;&#23454;&#29616;&#19982;VR&#19979;&#30028;&#30456;&#21516;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#65292;&#23545;&#35813;&#19979;&#30028;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#20854;&#20248;&#21183;&#21644;&#19981;&#36275;&#65292;&#24182;&#36890;&#36807;&#31034;&#20363;&#39564;&#35777;&#20102;&#29702;&#35770;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2210.06226</link><description>&lt;p&gt;
Alpha-divergence&#21464;&#20998;&#25512;&#26029;&#19982;&#37325;&#35201;&#24615;&#21152;&#26435;&#33258;&#32534;&#30721;&#22120;&#30340;&#32467;&#21512;&#65306;&#26041;&#27861;&#21644;&#28176;&#36817;&#24615;
&lt;/p&gt;
&lt;p&gt;
Alpha-divergence Variational Inference Meets Importance Weighted Auto-Encoders: Methodology and Asymptotics. (arXiv:2210.06226v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;VR-IWAE&#19979;&#30028;&#65292;&#35813;&#19979;&#30028;&#26159;IWAE&#19979;&#30028;&#30340;&#25512;&#24191;&#65292;&#37319;&#29992;&#26080;&#20559;&#26799;&#24230;&#20272;&#35745;&#22120;&#33021;&#22815;&#23454;&#29616;&#19982;VR&#19979;&#30028;&#30456;&#21516;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#65292;&#23545;&#35813;&#19979;&#30028;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#20854;&#20248;&#21183;&#21644;&#19981;&#36275;&#65292;&#24182;&#36890;&#36807;&#31034;&#20363;&#39564;&#35777;&#20102;&#29702;&#35770;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#30446;&#26631;&#21518;&#39564;&#20998;&#24067;&#21644;&#21464;&#20998;&#20998;&#24067;&#20043;&#38388;&#30340;alpha&#25955;&#24230;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#20010;&#28041;&#21450;&#21464;&#20998;R&#233;nyi (VR)&#19979;&#30028;&#30340;&#31639;&#27861;&#12290;&#23613;&#31649;&#26377;&#20196;&#20154;&#28385;&#24847;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#31639;&#27861;&#37117;&#37319;&#29992;&#20102;&#26377;&#20559;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#65292;&#22240;&#27492;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#12290;&#26412;&#25991;&#23545;VR-IWAE&#19979;&#30028;&#36827;&#34892;&#20102;&#27491;&#24335;&#21270;&#21644;&#30740;&#31350;&#65292;&#35813;&#19979;&#30028;&#26159;&#37325;&#35201;&#24615;&#21152;&#26435;&#33258;&#32534;&#30721;&#22120;(IWAE)&#19979;&#30028;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;VR-IWAE&#19979;&#30028;&#20855;&#26377;&#20960;&#20010;&#21487;&#21462;&#30340;&#29305;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#19982;VR&#19979;&#30028;&#23548;&#33268;&#30456;&#21516;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#65292;&#20294;&#36825;&#27425;&#26159;&#20381;&#38752;&#26080;&#20559;&#26799;&#24230;&#20272;&#35745;&#22120;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;VR-IWAE&#19979;&#30028;&#20197;&#21450;&#26631;&#20934;IWAE&#19979;&#30028;&#30340;&#20004;&#31181;&#20114;&#34917;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#36825;&#20123;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20123;&#19979;&#30028;&#30340;&#22909;&#22788;&#21644;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#29609;&#20855;&#21644;&#30495;&#23454;&#25968;&#25454;&#31034;&#20363;&#26469;&#35828;&#26126;&#25105;&#20204;&#30340;&#29702;&#35770;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several algorithms involving the Variational R\'enyi (VR) bound have been proposed to minimize an alpha-divergence between a target posterior distribution and a variational distribution. Despite promising empirical results, those algorithms resort to biased stochastic gradient descent procedures and thus lack theoretical guarantees. In this paper, we formalize and study the VR-IWAE bound, a generalization of the Importance Weighted Auto-Encoder (IWAE) bound. We show that the VR-IWAE bound enjoys several desirable properties and notably leads to the same stochastic gradient descent procedure as the VR bound in the reparameterized case, but this time by relying on unbiased gradient estimators. We then provide two complementary theoretical analyses of the VR-IWAE bound and thus of the standard IWAE bound. Those analyses shed light on the benefits or lack thereof of these bounds. Lastly, we illustrate our theoretical claims over toy and real-data examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21152;&#26435;&#19981;&#23545;&#31216;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#21487;&#38752;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#36866;&#29992;&#20110;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#24773;&#22659;&#65292;&#21487;&#25193;&#23637;&#20026;&#21442;&#25968;&#21270;&#20989;&#25968;&#30340;PI&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2210.04318</link><description>&lt;p&gt;
&#20351;&#29992;&#21152;&#26435;&#19981;&#23545;&#31216;&#25439;&#22833;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Prediction intervals for neural network models using weighted asymmetric loss functions. (arXiv:2210.04318v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21152;&#26435;&#19981;&#23545;&#31216;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#21487;&#38752;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#36866;&#29992;&#20110;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#24773;&#22659;&#65292;&#21487;&#25193;&#23637;&#20026;&#21442;&#25968;&#21270;&#20989;&#25968;&#30340;PI&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#36817;&#20284;&#21644;&#39044;&#27979;&#36235;&#21183;&#30340;&#39044;&#27979;&#21306;&#38388;&#65288;PIs&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#21152;&#26435;&#19981;&#23545;&#31216;&#25439;&#22833;&#20989;&#25968;&#26469;&#20272;&#35745;PI&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#65292;&#26435;&#37325;&#30001;&#21306;&#38388;&#23485;&#24230;&#30830;&#23450;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35813;&#26041;&#27861;&#30340;&#31616;&#27905;&#25968;&#23398;&#35777;&#26126;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#25193;&#23637;&#21040;&#20026;&#21442;&#25968;&#21270;&#20989;&#25968;&#25512;&#23548;PI&#65292;&#24182;&#35770;&#35777;&#20102;&#35813;&#26041;&#27861;&#20026;&#39044;&#27979;&#30456;&#20851;&#21464;&#37327;&#30340;PI&#32780;&#26377;&#25928;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#30340;&#30495;&#23454;&#19990;&#30028;&#39044;&#27979;&#20219;&#21153;&#19978;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#22312;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#24773;&#22659;&#19979;&#21487;&#20197;&#20135;&#29983;&#21487;&#38752;&#30340;PI&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple and efficient approach to generate prediction intervals (PIs) for approximated and forecasted trends. Our method leverages a weighted asymmetric loss function to estimate the lower and upper bounds of the PIs, with the weights determined by the interval width. We provide a concise mathematical proof of the method, show how it can be extended to derive PIs for parametrised functions and argue why the method works for predicting PIs of dependent variables. The presented tests of the method on a real-world forecasting task using a neural network-based model show that it can produce reliable PIs in complex machine learning scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#28155;&#21152;&#28151;&#21512;&#25104;&#20998;&#30340;&#22909;&#22788;&#65292;&#24182;&#35777;&#26126;&#20102;&#28151;&#21512;&#25104;&#20998;&#30340;&#22686;&#21152;&#33021;&#22815;&#25552;&#39640;&#20854;&#22312;&#22270;&#20687;&#21644;&#21333;&#32454;&#32990;&#25968;&#25454;&#38598;&#19978;&#30340;&#28508;&#22312;&#34920;&#31034;&#33021;&#21147;&#12290;&#36825;&#34920;&#26126;&#20351;&#29992;&#28151;&#21512;VAE&#26159;&#33719;&#21462;&#26356;&#28789;&#27963;&#21464;&#20998;&#36924;&#36817;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.15514</link><description>&lt;p&gt;
&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#21512;&#20316;&#65306;&#22312;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#28155;&#21152;&#28151;&#21512;&#25104;&#20998;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
Cooperation in the Latent Space: The Benefits of Adding Mixture Components in Variational Autoencoders. (arXiv:2209.15514v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#28155;&#21152;&#28151;&#21512;&#25104;&#20998;&#30340;&#22909;&#22788;&#65292;&#24182;&#35777;&#26126;&#20102;&#28151;&#21512;&#25104;&#20998;&#30340;&#22686;&#21152;&#33021;&#22815;&#25552;&#39640;&#20854;&#22312;&#22270;&#20687;&#21644;&#21333;&#32454;&#32990;&#25968;&#25454;&#38598;&#19978;&#30340;&#28508;&#22312;&#34920;&#31034;&#33021;&#21147;&#12290;&#36825;&#34920;&#26126;&#20351;&#29992;&#28151;&#21512;VAE&#26159;&#33719;&#21462;&#26356;&#28789;&#27963;&#21464;&#20998;&#36924;&#36817;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#28151;&#21512;&#25104;&#20998;&#22312;&#20849;&#21516;&#36866;&#24212;&#26368;&#22823;&#21270;ELBO&#26102;&#30340;&#21512;&#20316;&#26041;&#24335;&#12290;&#25105;&#20204;&#20511;&#37492;&#20102;&#26368;&#36817;&#22312;&#22810;&#20010;&#21644;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#37319;&#26679;&#25991;&#29486;&#20013;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#20351;&#29992;&#21333;&#29420;&#30340;&#32534;&#30721;&#22120;&#32593;&#32476;&#23545;&#28151;&#21512;&#25104;&#20998;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#22312;&#23454;&#35777;&#19978;&#35777;&#26126;ELBO&#38543;&#28151;&#21512;&#25104;&#20998;&#25968;&#37327;&#30340;&#22686;&#21152;&#26159;&#21333;&#35843;&#38750;&#20943;&#30340;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;MNIST&#12289;FashionMNIST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#30340;&#19981;&#21516;VAE&#26550;&#26500;&#12290;&#26412;&#24037;&#20316;&#36824;&#34920;&#26126;&#22686;&#21152;&#28151;&#21512;&#25104;&#20998;&#30340;&#25968;&#37327;&#33021;&#22815;&#25913;&#21892;VAE&#22312;&#22270;&#20687;&#21644;&#21333;&#32454;&#32990;&#25968;&#25454;&#38598;&#19978;&#30340;&#28508;&#22312;&#34920;&#31034;&#33021;&#21147;&#12290;&#36825;&#31181;&#21512;&#20316;&#34892;&#20026;&#34920;&#26126;&#65292;&#20351;&#29992;&#28151;&#21512;VAE&#24212;&#34987;&#35270;&#20026;&#33719;&#21462;&#26356;&#28789;&#27963;&#30340;&#21464;&#20998;&#36817;&#20284;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#39318;&#27425;&#22312;&#22823;&#33539;&#22260;&#30340;&#28040;&#34701;&#23454;&#39564;&#20013;&#23558;&#28151;&#21512;VAE&#19982;&#24402;&#19968;&#21270;&#27969;&#12289;&#23618;&#27425;&#27169;&#22411;&#21644;/&#25110;VampPrior&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we show how the mixture components cooperate when they jointly adapt to maximize the ELBO. We build upon recent advances in the multiple and adaptive importance sampling literature. We then model the mixture components using separate encoder networks and show empirically that the ELBO is monotonically non-decreasing as a function of the number of mixture components. These results hold for a range of different VAE architectures on the MNIST, FashionMNIST, and CIFAR-10 datasets. In this work, we also demonstrate that increasing the number of mixture components improves the latent-representation capabilities of the VAE on both image and single-cell datasets. This cooperative behavior motivates that using Mixture VAEs should be considered a standard approach for obtaining more flexible variational approximations. Finally, Mixture VAEs are here, for the first time, compared and combined with normalizing flows, hierarchical models and/or the VampPrior in an extensive ablation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33258;&#30417;&#30563;&#26041;&#27861;&#39044;&#35757;&#32451;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#26102;&#38388;&#39034;&#24207;&#39564;&#35777;&#20219;&#21153;&#26469;&#25429;&#25417;&#35266;&#27979;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23398;&#20064;&#26377;&#29992;&#30340;&#34920;&#31034;&#21644;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#37117;&#24456;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2209.10901</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#30417;&#30563;&#26041;&#27861;&#39044;&#35757;&#32451;&#35270;&#35273;&#36716;&#25442;&#22120;&#29992;&#20110;&#22522;&#20110;&#35270;&#35273;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pretraining the Vision Transformer using self-supervised methods for vision based Deep Reinforcement Learning. (arXiv:2209.10901v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33258;&#30417;&#30563;&#26041;&#27861;&#39044;&#35757;&#32451;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#26102;&#38388;&#39034;&#24207;&#39564;&#35777;&#20219;&#21153;&#26469;&#25429;&#25417;&#35266;&#27979;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23398;&#20064;&#26377;&#29992;&#30340;&#34920;&#31034;&#21644;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#37117;&#24456;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#36716;&#25442;&#22120;&#26550;&#26500;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#20195;&#20102;&#22522;&#20110;&#21367;&#31215;&#30340;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20173;&#28982;&#26159;&#34920;&#31034;&#27169;&#22359;&#30340;&#39318;&#36873;&#26550;&#26500;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#26469;&#39044;&#35757;&#32451;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#24182;&#35780;&#20272;&#25152;&#23398;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#23637;&#31034;&#22312;&#36825;&#20010;&#19978;&#19979;&#25991;&#20013;&#26102;&#38388;&#32500;&#24230;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VICReg&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#28155;&#21152;&#19968;&#20010;&#26102;&#38388;&#39034;&#24207;&#39564;&#35777;&#20219;&#21153;&#26469;&#26356;&#22909;&#22320;&#25429;&#25417;&#35266;&#27979;&#20043;&#38388;&#30340;&#26102;&#38388;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#26377;&#26041;&#27861;&#22312;&#23398;&#20064;&#26377;&#29992;&#30340;&#34920;&#31034;&#21644;&#36991;&#20813;&#22312;Atari Learning Environment (ALE)&#20013;&#35266;&#27979;&#25968;&#25454;&#19978;&#20986;&#29616;&#37325;&#22797;&#34920;&#31034;&#26041;&#38754;&#37117;&#26159;&#26377;&#25928;&#30340;&#65292;&#36825;&#23548;&#33268;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25928;&#29575;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#35201;&#27604;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Vision Transformer architecture has shown to be competitive in the computer vision (CV) space where it has dethroned convolution-based networks in several benchmarks. Nevertheless, convolutional neural networks (CNN) remain the preferential architecture for the representation module in reinforcement learning. In this work, we study pretraining a Vision Transformer using several state-of-the-art self-supervised methods and assess the quality of the learned representations. To show the importance of the temporal dimension in this context we propose an extension of VICReg to better capture temporal relations between observations by adding a temporal order verification task. Our results show that all methods are effective in learning useful representations and avoiding representational collapse for observations from Atari Learning Environment (ALE) which leads to improvements in data efficiency when we evaluated in reinforcement learning (RL). Moreover, the encoder pretrained with the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#20351;&#29992;&#22870;&#21169;&#20989;&#25968;&#35780;&#20998;&#36712;&#36857;&#26102;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#20215;&#20540;&#30340;&#25240;&#25187;&#21644;&#27714;&#21644;&#26469;&#25552;&#39640;MPC-based&#24378;&#21270;&#23398;&#20064;&#30340;&#23398;&#20064;&#25928;&#29575;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.08169</link><description>&lt;p&gt;
&#22522;&#20110;MPC&#30340;&#27169;&#22411;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#35780;&#20998;&#20989;&#25968;&#65306;&#20215;&#20540;&#27714;&#21644;
&lt;/p&gt;
&lt;p&gt;
Value Summation: A Novel Scoring Function for MPC-based Model-based Reinforcement Learning. (arXiv:2209.08169v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#20351;&#29992;&#22870;&#21169;&#20989;&#25968;&#35780;&#20998;&#36712;&#36857;&#26102;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#20215;&#20540;&#30340;&#25240;&#25187;&#21644;&#27714;&#21644;&#26469;&#25552;&#39640;MPC-based&#24378;&#21270;&#23398;&#20064;&#30340;&#23398;&#20064;&#25928;&#29575;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;MPC-based&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35268;&#21010;&#27169;&#22359;&#30340;&#26032;&#22411;&#35780;&#20998;&#20989;&#25968;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#22870;&#21169;&#20989;&#25968;&#26469;&#35780;&#20998;&#36712;&#36857;&#26102;&#30340;&#22266;&#26377;&#20559;&#24046;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20215;&#20540;&#30340;&#25240;&#25187;&#21644;&#27714;&#21644;&#26469;&#25552;&#39640;&#29616;&#26377;MPC-based MBRL&#26041;&#27861;&#30340;&#23398;&#20064;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26368;&#20248;&#36712;&#36857;&#26469;&#25351;&#23548;&#31574;&#30053;&#23398;&#20064;&#65292;&#24182;&#26681;&#25454;&#23454;&#38469;&#19990;&#30028;&#21644;&#22686;&#24378;&#22411;&#26495;&#36733;&#25968;&#25454;&#26356;&#26032;&#20854;&#29366;&#24577;-&#21160;&#20316;&#20540;&#20989;&#25968;&#12290;&#36890;&#36807;&#22312;&#36873;&#23450;&#30340;MuJoCo Gym&#29615;&#22659;&#20013;&#35780;&#20272;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#23398;&#20064;&#25928;&#29575;&#65292;&#24182;&#22312;&#23398;&#20064;Cassie&#26426;&#22120;&#20154;&#27169;&#22411;&#30340;&#36816;&#21160;&#25216;&#33021;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#23398;&#20064;&#25928;&#29575;&#21644;&#24179;&#22343;&#22870;&#21169;&#22238;&#25253;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel scoring function for the planning module of MPC-based reinforcement learning methods to address the inherent bias of using the reward function to score trajectories. The proposed method enhances the learning efficiency of existing MPC-based MBRL methods using the discounted sum of values. The method utilizes optimal trajectories to guide policy learning and updates its state-action value function based on real-world and augmented onboard data. The learning efficiency of the proposed method is evaluated in selected MuJoCo Gym environments as well as in learning locomotion skills for a simulated model of the Cassie robot. The results demonstrate that the proposed method outperforms the current state-of-the-art algorithms in terms of learning efficiency and average reward return.
&lt;/p&gt;</description></item><item><title>&#19981;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#21487;&#20197;&#23545;&#20219;&#21153;&#30340;&#27867;&#21270;&#35823;&#24046;&#20135;&#29983;&#38750;&#21333;&#35843;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#23569;&#37327;&#19981;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;</title><link>http://arxiv.org/abs/2208.10967</link><description>&lt;p&gt;
&#19981;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
The Value of Out-of-Distribution Data. (arXiv:2208.10967v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10967
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#21487;&#20197;&#23545;&#20219;&#21153;&#30340;&#27867;&#21270;&#35823;&#24046;&#20135;&#29983;&#38750;&#21333;&#35843;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#23569;&#37327;&#19981;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26399;&#26395;&#38543;&#30528;&#31867;&#20284;&#20219;&#21153;&#26679;&#26412;&#30340;&#22686;&#21152;&#65292;&#27867;&#21270;&#35823;&#24046;&#20250;&#20943;&#23567;&#65307;&#32780;&#38543;&#30528;&#26469;&#33258;&#19981;&#21516;&#20998;&#24067;&#65288;OOD&#65289;&#20219;&#21153;&#26679;&#26412;&#30340;&#22686;&#21152;&#65292;&#27867;&#21270;&#35823;&#24046;&#20250;&#22686;&#22823;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#21453;&#30452;&#35273;&#30340;&#29616;&#35937;&#65306;&#20219;&#21153;&#30340;&#27867;&#21270;&#35823;&#24046;&#21487;&#20197;&#26159;&#26679;&#26412;&#20174;OOD&#20219;&#21153;&#20013;&#30340;&#25968;&#37327;&#30340;&#38750;&#21333;&#35843;&#20989;&#25968;&#12290;&#38543;&#30528;OOD&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#30446;&#26631;&#20219;&#21153;&#30340;&#27867;&#21270;&#35823;&#24046;&#22312;&#36229;&#36807;&#19968;&#20010;&#38408;&#20540;&#20043;&#21069;&#20250;&#20808;&#20943;&#23567;&#21518;&#22686;&#22823;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#20351;&#29992;&#23569;&#37327;OOD&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;Fisher&#32447;&#24615;&#21028;&#21035;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;&#22914;MNIST&#12289;CIFAR-10&#12289;CINIC-10&#12289;PACS&#21644;DomainNet&#65289;&#19978;&#30340;&#28145;&#24230;&#32593;&#32476;&#26469;&#23637;&#31034;&#21644;&#20998;&#26512;&#36825;&#19968;&#29616;&#35937;&#12290;&#22312;&#25105;&#20204;&#30693;&#36947;&#21738;&#20123;&#26679;&#26412;&#23646;&#20110;OOD&#30340;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#21033;&#29992;&#30446;&#26631;&#21644;OOD&#32463;&#39564;&#39118;&#38505;&#30340;&#36866;&#24403;&#21152;&#26435;&#30446;&#26631;&#26469;&#21033;&#29992;&#36825;&#20123;&#38750;&#21333;&#35843;&#36235;&#21183;&#12290;&#23613;&#31649;&#23454;&#38469;&#24212;&#29992;&#26377;&#38480;&#65292;&#20294;&#36825;&#34920;&#26126;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#26816;&#27979;&#21040;OOD&#26679;&#26412;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We expect the generalization error to improve with more samples from a similar task, and to deteriorate with more samples from an out-of-distribution (OOD) task. In this work, we show a counter-intuitive phenomenon: the generalization error of a task can be a non-monotonic function of the number of OOD samples. As the number of OOD samples increases, the generalization error on the target task improves before deteriorating beyond a threshold. In other words, there is value in training on small amounts of OOD data. We use Fisher's Linear Discriminant on synthetic datasets and deep networks on computer vision benchmarks such as MNIST, CIFAR-10, CINIC-10, PACS and DomainNet to demonstrate and analyze this phenomenon. In the idealistic setting where we know which samples are OOD, we show that these non-monotonic trends can be exploited using an appropriately weighted objective of the target and OOD empirical risk. While its practical utility is limited, this does suggest that if we can det
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#19982;&#24322;&#24120;&#29983;&#25104;&#26426;&#21046;&#20043;&#38388;&#30340;&#23545;&#40784;&#26159;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#24182;&#19988;&#22312;&#32570;&#20047;&#23545;&#40784;&#26102;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#29978;&#33267;&#21487;&#33021;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.07734</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#19968;&#20010;&#36229;&#21442;&#25968;&#65306;&#31934;&#24515;&#31579;&#36873;&#30340;&#33258;&#30417;&#30563;&#23545;&#20110;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#25104;&#21151;&#20135;&#29983;&#20102;&#24187;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation is a Hyperparameter: Cherry-picked Self-Supervision for Unsupervised Anomaly Detection is Creating the Illusion of Success. (arXiv:2208.07734v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07734
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#19982;&#24322;&#24120;&#29983;&#25104;&#26426;&#21046;&#20043;&#38388;&#30340;&#23545;&#40784;&#26159;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#24182;&#19988;&#22312;&#32570;&#20047;&#23545;&#40784;&#26102;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#29978;&#33267;&#21487;&#33021;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#21019;&#24314;&#30417;&#30563;&#20449;&#21495;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#24040;&#22823;&#25104;&#26412;&#12290;&#23545;&#20110;&#26631;&#35760;&#24322;&#24120;&#31232;&#32570;&#25110;&#20960;&#20046;&#19981;&#23384;&#22312;&#30340;&#26080;&#30417;&#30563;&#20219;&#21153;&#65288;&#22914;&#24322;&#24120;&#26816;&#27979;&#65289;&#65292;SSL&#29305;&#21035;&#26377;&#21560;&#24341;&#21147;&#12290;&#36807;&#21435;&#24050;&#32463;&#20351;&#29992;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#22686;&#24378;&#20989;&#25968;&#26469;&#36827;&#34892;&#22522;&#20110;SSL&#30340;&#24322;&#24120;&#26816;&#27979;&#65288;SSAD&#65289;&#30340;&#22270;&#20687;&#25968;&#25454;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#25968;&#25454;&#22686;&#24378;&#30340;&#31867;&#22411;&#23545;&#20934;&#30830;&#24615;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#19977;&#31181;&#19981;&#21516;&#26816;&#27979;&#27169;&#22411;&#21644;420&#20010;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#25968;&#23383;&#21644;&#21487;&#35270;&#35777;&#25454;&#65292;&#35777;&#26126;&#25968;&#25454;&#22686;&#24378;&#19982;&#24322;&#24120;&#29983;&#25104;&#26426;&#21046;&#20043;&#38388;&#30340;&#23545;&#40784;&#26159;SSAD&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#32780;&#22312;&#32570;&#20047;&#23545;&#40784;&#30340;&#24773;&#20917;&#19979;&#65292;SSL&#29978;&#33267;&#21487;&#33021;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#20851;&#20110;&#22270;&#20687;&#22411;SSAD&#30340;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has emerged as a promising alternative to create supervisory signals to real-world problems, avoiding the extensive cost of manual labeling. SSL is particularly attractive for unsupervised tasks such as anomaly detection (AD), where labeled anomalies are rare or often nonexistent. A large catalog of augmentation functions has been used for SSL-based AD (SSAD) on image data, and recent works have reported that the type of augmentation has a significant impact on accuracy. Motivated by those, this work sets out to put image-based SSAD under a larger lens and investigate the role of data augmentation in SSAD. Through extensive experiments on 3 different detector models and across 420 AD tasks, we provide comprehensive numerical and visual evidences that the alignment between data augmentation and anomaly-generating mechanism is the key to the success of SSAD, and in the lack thereof, SSL may even impair accuracy. To the best of our knowledge, this is the fir
&lt;/p&gt;</description></item><item><title>&#21487;&#20449;&#24230;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#24050;&#32463;&#20174;&#20197;&#20934;&#30830;&#24615;&#20026;&#23548;&#21521;&#36716;&#21464;&#20026;&#20197;&#36879;&#26126;&#12289;&#20844;&#27491;&#12289;&#31283;&#20581;&#24615;&#20026;&#29305;&#28857;&#30340;&#21487;&#20449;&#24230;&#25512;&#33616;&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#21487;&#20449;&#24230;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#30340;&#25991;&#29486;&#32508;&#36848;&#21644;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2208.06265</link><description>&lt;p&gt;
&#21487;&#20449;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Recommender Systems. (arXiv:2208.06265v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06265
&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#24230;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#24050;&#32463;&#20174;&#20197;&#20934;&#30830;&#24615;&#20026;&#23548;&#21521;&#36716;&#21464;&#20026;&#20197;&#36879;&#26126;&#12289;&#20844;&#27491;&#12289;&#31283;&#20581;&#24615;&#20026;&#29305;&#28857;&#30340;&#21487;&#20449;&#24230;&#25512;&#33616;&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#21487;&#20449;&#24230;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#30340;&#25991;&#29486;&#32508;&#36848;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#20174;&#24222;&#22823;&#30340;&#30446;&#24405;&#20013;&#26377;&#25928;&#22320;&#26816;&#32034;&#24863;&#20852;&#36259;&#30340;&#29289;&#21697;&#12290;&#38271;&#26399;&#20197;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#33268;&#21147;&#20110;&#24320;&#21457;&#20934;&#30830;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#36817;&#24180;&#26469;&#65292;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#36234;&#26469;&#36234;&#22810;&#30340;&#23041;&#32961;&#65292;&#21253;&#25324;&#26469;&#33258;&#25915;&#20987;&#12289;&#31995;&#32479;&#21644;&#29992;&#25143;&#20135;&#29983;&#30340;&#24178;&#25200;&#20197;&#21450;&#31995;&#32479;&#30340;&#20559;&#35265;&#12290;&#22240;&#27492;&#65292;&#20165;&#20165;&#20851;&#27880;&#20934;&#30830;&#24615;&#24050;&#32463;&#19981;&#22815;&#65292;&#30740;&#31350;&#24517;&#39035;&#32771;&#34385;&#20854;&#20182;&#37325;&#35201;&#22240;&#32032;&#65292;&#22914;&#21487;&#20449;&#24230;&#12290;&#23545;&#20110;&#32456;&#31471;&#29992;&#25143;&#26469;&#35828;&#65292;&#19968;&#20010;&#20540;&#24471;&#20449;&#36182;&#30340;&#25512;&#33616;&#31995;&#32479;&#19981;&#20165;&#35201;&#20934;&#30830;&#65292;&#32780;&#19988;&#36824;&#35201;&#36879;&#26126;&#12289;&#26080;&#20559;&#35265;&#12289;&#20844;&#27491;&#65292;&#24182;&#19988;&#23545;&#24178;&#25200;&#25110;&#25915;&#20987;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#36825;&#20123;&#35266;&#23519;&#23454;&#38469;&#19978;&#23548;&#33268;&#20102;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#30340;&#33539;&#24335;&#36716;&#21464;: &#20174;&#20197;&#20934;&#30830;&#24615;&#20026;&#23548;&#21521;&#30340;&#25512;&#33616;&#31995;&#32479;&#36716;&#21521;&#20102;&#20197;&#21487;&#20449;&#24230;&#20026;&#23548;&#21521;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#32570;&#20047;&#23545;&#21487;&#20449;&#24230;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#30340;&#25991;&#29486;&#30340;&#31995;&#32479;&#27010;&#36848;&#21644;&#35752;&#35770;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#21487;&#20449;&#24230;&#25512;&#33616;&#31995;&#32479;&#30340;&#27010;&#36848;&#65292;&#21253;&#25324;&#23545;&#35813;&#26032;&#20852;&#19988;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#25991;&#29486;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems (RSs) aim to help users to effectively retrieve items of their interests from a large catalogue. For a quite long period of time, researchers and practitioners have been focusing on developing accurate RSs. Recent years have witnessed an increasing number of threats to RSs, coming from attacks, system and user generated noise, system bias. As a result, it has become clear that a strict focus on RS accuracy is limited and the research must consider other important factors, e.g., trustworthiness. For end users, a trustworthy RS (TRS) should not only be accurate, but also transparent, unbiased and fair as well as robust to noise or attacks. These observations actually led to a paradigm shift of the research on RSs: from accuracy-oriented RSs to TRSs. However, researchers lack a systematic overview and discussion of the literature in this novel and fast developing field of TRSs. To this end, in this paper, we provide an overview of TRSs, including a discussion of the mo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#23454;&#29616;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#65292;&#24182;&#35780;&#20272;&#25104;&#26412;&#25935;&#24863;&#30340;PEGASOS SVM&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#23558;&#26680;&#20989;&#25968;&#32435;&#20837;SVM&#20013;&#25193;&#23637;Ding&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2206.09311</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#22987;&#20272;&#35745;&#20122;&#26799;&#24230;&#27714;&#35299;&#22120;&#30340;&#19981;&#24179;&#34913;&#20998;&#31867;SVM
&lt;/p&gt;
&lt;p&gt;
Primal Estimated Subgradient Solver for SVM for Imbalanced Classification. (arXiv:2206.09311v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#23454;&#29616;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#65292;&#24182;&#35780;&#20272;&#25104;&#26412;&#25935;&#24863;&#30340;PEGASOS SVM&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#23558;&#26680;&#20989;&#25968;&#32435;&#20837;SVM&#20013;&#25193;&#23637;Ding&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#25104;&#26412;&#25935;&#24863;PEGASOS SVM&#22312;&#20027;&#22810;&#27425;&#35201;&#27604;&#20174;8.6&#65306;1&#21040;130&#65306;1&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#21253;&#25324;&#25130;&#36317;&#65288;&#20559;&#35265;&#65289;&#12289;&#27491;&#21017;&#21270;&#21644;&#21442;&#25968;&#26159;&#21542;&#20250;&#24433;&#21709;&#25105;&#20204;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#35768;&#22810;&#20154;&#37319;&#29992;SMOTE&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#26088;&#22312;&#37319;&#29992;&#19968;&#31181;&#35745;&#31639;&#37327;&#36739;&#23567;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#26816;&#26597;&#23398;&#20064;&#26354;&#32447;&#26469;&#35780;&#20272;&#24615;&#33021;&#65292;&#36825;&#20123;&#26354;&#32447;&#21487;&#20197;&#35786;&#26029;&#25105;&#20204;&#26159;&#36807;&#24230;&#25311;&#21512;&#36824;&#26159;&#27424;&#25311;&#21512;&#65292;&#25110;&#32773;&#25105;&#20204;&#36873;&#25321;&#20102;&#36807;&#24230;&#20195;&#34920;&#24615;&#25110;&#27424;&#20195;&#34920;&#24615;&#30340;&#35757;&#32451;/&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#23558;&#22312;&#39564;&#35777;&#26354;&#32447;&#20013;&#26597;&#30475;&#36229;&#21442;&#25968;&#30340;&#32972;&#26223;&#19982;&#27979;&#35797;&#21644;&#35757;&#32451;&#35823;&#24046;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#23558;&#22522;&#20934;&#21270;&#25105;&#20204;&#30340;PEGASOS&#25104;&#26412;&#25935;&#24863;SVM&#19982;Ding&#30340;LINEAR SVM DECIDL&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;&#20182;&#22312;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#20102;0.5&#30340;ROC-AUC&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#36890;&#36807;&#23558;&#26680;&#20989;&#25968;&#32435;&#20837;SVM&#26469;&#25193;&#23637;Ding&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#23558;&#20351;&#29992;Python&#32780;&#19981;&#26159;MATLAB&#65292;&#22240;&#20026;Python&#20855;&#26377;&#26356;&#26377;&#25928;&#22320;&#23384;&#20648;&#25105;&#30340;&#25968;&#25454;&#38598;&#30340;&#23383;&#20856;&#12290;
&lt;/p&gt;
&lt;p&gt;
We aim to demonstrate in experiments that our cost sensitive PEGASOS SVM achieves good performance on imbalanced data sets with a Majority to Minority Ratio ranging from 8.6:1 to 130:1 and to ascertain whether the including intercept (bias), regularization and parameters affects performance on our selection of datasets. Although many resort to SMOTE methods, we aim for a less computationally intensive method. We evaluate the performance by examining the learning curves. These curves diagnose whether we overfit or underfit or we choose over representative or under representative training/test data. We will also see the background of the hyperparameters versus the test and train error in validation curves. We benchmark our PEGASOS Cost-Sensitive SVM's results of Ding's LINEAR SVM DECIDL method. He obtained an ROC-AUC of .5 in one dataset. Our work will extend the work of Ding by incorporating kernels into SVM. We will use Python rather than MATLAB as python has dictionaries for storing m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#29992;&#20110;&#35299;&#20915;&#22270;&#19978;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#20132;&#26367;&#20248;&#21270;&#31639;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#21487;&#20197;&#36798;&#21040;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.03638</link><description>&lt;p&gt;
&#20132;&#26367;&#20248;&#21270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Alternately Optimized Graph Neural Networks. (arXiv:2206.03638v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#29992;&#20110;&#35299;&#20915;&#22270;&#19978;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#20132;&#26367;&#20248;&#21270;&#31639;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#21487;&#20197;&#36798;&#21040;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22270;&#19978;&#30340;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;GNN&#37117;&#26159;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#35299;&#20915;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20010;&#36807;&#31243;&#22312;&#35745;&#31639;&#21644;&#20869;&#23384;&#20351;&#29992;&#19978;&#36890;&#24120;&#25928;&#29575;&#20302;&#19979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26694;&#26550;&#29992;&#20110;&#22270;&#19978;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#36890;&#36807;&#20132;&#26367;&#20248;&#21270;&#31639;&#27861;&#26041;&#20415;&#22320;&#35299;&#20915;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#25928;&#29575;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#22522;&#32447;&#65292;&#24182;&#19988;&#21487;&#20197;&#36798;&#21040;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have greatly advanced the semi-supervised node classification task on graphs. The majority of existing GNNs are trained in an end-to-end manner that can be viewed as tackling a bi-level optimization problem. This process is often inefficient in computation and memory usage. In this work, we propose a new optimization framework for semi-supervised learning on graphs. The proposed framework can be conveniently solved by the alternating optimization algorithms, resulting in significantly improved efficiency. Extensive experiments demonstrate that the proposed method can achieve comparable or better performance with state-of-the-art baselines while it has significantly better computation and memory efficiency.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#21442;&#25968;&#21270;&#25216;&#33021;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21270;&#25216;&#33021;&#24182;&#23558;&#20854;&#32508;&#21512;&#21040;&#26032;&#30340;&#21160;&#20316;&#31354;&#38388;&#20013;&#65292;&#23454;&#29616;&#20102;&#38271;&#26399;&#20219;&#21153;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20351;&#26234;&#33021;&#20307;&#22312;&#38590;&#20197;&#35299;&#20915;&#30340;&#38271;&#26399;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2206.03597</link><description>&lt;p&gt;
&#20803;&#23398;&#20064;&#21442;&#25968;&#21270;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning Parameterized Skills. (arXiv:2206.03597v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03597
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#21442;&#25968;&#21270;&#25216;&#33021;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21270;&#25216;&#33021;&#24182;&#23558;&#20854;&#32508;&#21512;&#21040;&#26032;&#30340;&#21160;&#20316;&#31354;&#38388;&#20013;&#65292;&#23454;&#29616;&#20102;&#38271;&#26399;&#20219;&#21153;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20351;&#26234;&#33021;&#20307;&#22312;&#38590;&#20197;&#35299;&#20915;&#30340;&#38271;&#26399;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#21270;&#25216;&#33021;&#23398;&#20064;&#31639;&#27861;&#65292;&#26088;&#22312;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21270;&#25216;&#33021;&#24182;&#23558;&#23427;&#20204;&#32508;&#21512;&#21040;&#25903;&#25345;&#38271;&#26399;&#20219;&#21153;&#39640;&#25928;&#23398;&#20064;&#30340;&#26032;&#21160;&#20316;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#31163;&#31574;&#30053;&#20803;&#24378;&#21270;&#23398;&#20064;&#19982;&#20197;&#36712;&#36857;&#20026;&#20013;&#24515;&#30340;&#24179;&#28369;&#39033;&#30456;&#32467;&#21512;&#65292;&#23398;&#20064;&#19968;&#32452;&#21442;&#25968;&#21270;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#25216;&#33021;&#26500;&#24314;&#19968;&#20010;&#19977;&#32423;&#23618;&#27425;&#32467;&#26500;&#26694;&#26550;&#65292;&#27169;&#25311;&#26102;&#38388;&#25193;&#23637;&#21442;&#25968;&#21270;&#21160;&#20316;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#35299;&#20915;&#19968;&#32452;&#22256;&#38590;&#30340;&#38271;&#26399;&#20219;&#21153;&#65288;&#38556;&#30861;&#35838;&#31243;&#21644;&#26426;&#22120;&#20154;&#25805;&#32437;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel parameterized skill-learning algorithm that aims to learn transferable parameterized skills and synthesize them into a new action space that supports efficient learning in long-horizon tasks. We propose to leverage off-policy Meta-RL combined with a trajectory-centric smoothness term to learn a set of parameterized skills. Our agent can use these learned skills to construct a three-level hierarchical framework that models a Temporally-extended Parameterized Action Markov Decision Process. We empirically demonstrate that the proposed algorithms enable an agent to solve a set of difficult long-horizon (obstacle-course and robot manipulation) tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22788;&#29702;&#26102;&#38388;&#30456;&#20851;&#30340;&#27969;&#24335;&#25968;&#25454;&#30340;&#22312;&#32447;&#38543;&#26426;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#38750;&#28176;&#36827;&#20998;&#26512;&#24314;&#31435;&#20102;&#26032;&#39062;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21152;&#36895;&#25910;&#25947;&#12290;&#23454;&#39564;&#35777;&#26126;&#26102;&#38388;&#21464;&#21270;&#30340;&#23567;&#25209;&#37327;SGD&#26041;&#27861;&#21487;&#20197;&#25171;&#30772;&#20381;&#36182;&#32467;&#26500;&#65292;&#26377;&#20559;&#20506;&#30340;SGD&#26041;&#27861;&#20855;&#26377;&#19982;&#26080;&#20559;&#20506;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20351;&#29992;Polyak-Ruppert&#24179;&#22343;&#21270;&#26041;&#27861;&#33021;&#22815;&#21152;&#24555;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2205.12549</link><description>&lt;p&gt;
&#23398;&#20064;&#22788;&#29702;&#26102;&#38388;&#30456;&#20851;&#30340;&#27969;&#24335;&#25968;&#25454;&#30340;&#22312;&#32447;&#38543;&#26426;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning from time-dependent streaming data with online stochastic algorithms. (arXiv:2205.12549v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22788;&#29702;&#26102;&#38388;&#30456;&#20851;&#30340;&#27969;&#24335;&#25968;&#25454;&#30340;&#22312;&#32447;&#38543;&#26426;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#38750;&#28176;&#36827;&#20998;&#26512;&#24314;&#31435;&#20102;&#26032;&#39062;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21152;&#36895;&#25910;&#25947;&#12290;&#23454;&#39564;&#35777;&#26126;&#26102;&#38388;&#21464;&#21270;&#30340;&#23567;&#25209;&#37327;SGD&#26041;&#27861;&#21487;&#20197;&#25171;&#30772;&#20381;&#36182;&#32467;&#26500;&#65292;&#26377;&#20559;&#20506;&#30340;SGD&#26041;&#27861;&#20855;&#26377;&#19982;&#26080;&#20559;&#20506;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20351;&#29992;Polyak-Ruppert&#24179;&#22343;&#21270;&#26041;&#27861;&#33021;&#22815;&#21152;&#24555;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#26102;&#38388;&#30456;&#20851;&#19988;&#26377;&#20559;&#20506;&#26799;&#24230;&#20272;&#35745;&#19979;&#30340;&#27969;&#24335;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20123;&#19968;&#38454;&#26041;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#12289;&#23567;&#25209;&#37327;SGD&#21644;&#26102;&#38388;&#21464;&#21270;&#30340;&#23567;&#25209;&#37327;SGD&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;Polyak-Ruppert&#24179;&#22343;&#20540;&#12290;&#25105;&#20204;&#30340;&#38750;&#28176;&#36827;&#20998;&#26512;&#24314;&#31435;&#20102;&#26032;&#39062;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#23558;&#20381;&#36182;&#24615;&#12289;&#20559;&#20506;&#21644;&#20984;&#24615;&#27700;&#24179;&#32852;&#31995;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25910;&#25947;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;i&#65289;&#26102;&#38388;&#21464;&#21270;&#30340;&#23567;&#25209;&#37327;SGD&#26041;&#27861;&#33021;&#22815;&#25171;&#30772;&#38271;&#26399;&#21644;&#30701;&#26399;&#30340;&#20381;&#36182;&#32467;&#26500;&#65307;&#65288;ii&#65289;&#26377;&#20559;&#20506;&#30340;SGD&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#19982;&#26080;&#20559;&#20506;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65307;&#65288;iii&#65289;&#20351;&#29992;Polyak-Ruppert&#24179;&#22343;&#21270;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#25910;&#25947;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#29616;&#23454;&#30340;&#26102;&#38388;&#30456;&#20851;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses stochastic optimization in a streaming setting with time-dependent and biased gradient estimates. We analyze several first-order methods, including Stochastic Gradient Descent (SGD), mini-batch SGD, and time-varying mini-batch SGD, along with their Polyak-Ruppert averages. Our non-asymptotic analysis establishes novel heuristics that link dependence, biases, and convexity levels, enabling accelerated convergence. Specifically, our findings demonstrate that (i) time-varying mini-batch SGD methods have the capability to break long- and short-range dependence structures, (ii) biased SGD methods can achieve comparable performance to their unbiased counterparts, and (iii) incorporating Polyak-Ruppert averaging can accelerate the convergence of the stochastic optimization algorithms. To validate our theoretical findings, we conduct a series of experiments using both simulated and real-life time-dependent data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23646;&#24615;&#22270;&#30340;&#20195;&#34920;&#24615;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#32570;&#20047;&#22270;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;RS&#30340;&#23398;&#20064;&#22256;&#38590;&#24615;&#12290;&#21516;&#26102;&#65292;&#21457;&#29616;&#24403;&#23384;&#22312;&#25110;&#26500;&#24314;&#20102;&#21516;&#36136;&#22270;&#32467;&#26500;&#26102;&#65292;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#24314;&#27169;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#19968;&#22256;&#38590;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.10403</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#21487;&#35777;&#26126;&#22256;&#38590;&#30340;&#20195;&#34920;&#24615;&#36873;&#25321;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling Provably Hard Representative Selection via Graph Neural Networks. (arXiv:2205.10403v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23646;&#24615;&#22270;&#30340;&#20195;&#34920;&#24615;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#32570;&#20047;&#22270;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;RS&#30340;&#23398;&#20064;&#22256;&#38590;&#24615;&#12290;&#21516;&#26102;&#65292;&#21457;&#29616;&#24403;&#23384;&#22312;&#25110;&#26500;&#24314;&#20102;&#21516;&#36136;&#22270;&#32467;&#26500;&#26102;&#65292;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#24314;&#27169;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#19968;&#22256;&#38590;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#34920;&#24615;&#36873;&#25321;&#65288;RS&#65289;&#26159;&#20174;&#25968;&#25454;&#38598;&#20013;&#25214;&#20986;&#20195;&#34920;&#25968;&#25454;&#23376;&#38598;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23646;&#24615;&#22270;&#30340;RS&#65292;&#24182;&#19987;&#27880;&#20110;&#25214;&#21040;&#33021;&#22815;&#20248;&#21270;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#20195;&#34920;&#24615;&#33410;&#28857;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#22270;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;RS&#30340;&#22256;&#38590;&#24615;&#65292;&#36890;&#36807;&#35777;&#26126;&#19968;&#20010;&#20855;&#26377;&#39640;&#24230;&#23454;&#38469;&#24847;&#20041;&#30340;&#21464;&#20307;&#65288;&#29992;&#20110;&#23398;&#20064;&#30340;RS&#65289;&#22312;&#20219;&#20309;&#21512;&#29702;&#22240;&#23376;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#26080;&#27861;&#36817;&#20284;&#65292;&#36825;&#24847;&#21619;&#30528;&#24191;&#27867;&#20351;&#29992;&#30340;&#26367;&#20195;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#19982;&#27169;&#22411;&#30340;&#23454;&#38469;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#24046;&#36317;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#65288;&#21516;&#36136;&#65289;&#22270;&#32467;&#26500;&#30340;&#24773;&#20917;&#65292;&#25110;&#32773;&#21487;&#20197;&#22312;&#25968;&#25454;&#28857;&#20043;&#38388;&#26500;&#24314;&#22270;&#32467;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#21512;&#36866;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#36825;&#31181;&#32467;&#26500;&#30340;&#23384;&#22312;&#21487;&#20197;&#23558;&#19968;&#20010;&#22256;&#38590;&#30340;RS&#65288;&#29992;&#20110;&#23398;&#20064;&#65289;&#38382;&#39064;&#26377;&#25928;&#22320;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representative Selection (RS) is the problem of finding a small subset of exemplars from a dataset that is representative of the dataset. In this paper, we study RS for attributed graphs, and focus on finding representative nodes that optimize the accuracy of a model trained on the selected representatives. Theoretically, we establish a new hardness result forRS (in the absence of a graph structure) by proving that a particular, highly practical variant of it (RS for Learning) is hard to approximate in polynomial time within any reasonable factor, which implies a significant potential gap between the optimum solution of widely-used surrogate functions and the actual accuracy of the model. We then study the setting where a (homophilous) graph structure is available, or can be constructed, between the data points.We show that with an appropriate modeling approach, the presence of such a structure can turn a hard RS (for learning) problem into one that can be effectively solved. To this e
&lt;/p&gt;</description></item><item><title>ConceptEvo&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25581;&#31034;&#27010;&#24565;&#30340;&#20135;&#29983;&#21644;&#28436;&#21464;&#65292;&#24182;&#36890;&#36807;&#20154;&#26426;&#35780;&#20272;&#21644;&#23454;&#39564;&#35777;&#26126;&#20854;&#21457;&#29616;&#23545;&#27169;&#22411;&#21644;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2203.16475</link><description>&lt;p&gt;
ConceptEvo&#65306;&#35299;&#35835;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#20013;&#30340;&#27010;&#24565;&#28436;&#21464;
&lt;/p&gt;
&lt;p&gt;
ConceptEvo: Interpreting Concept Evolution in Deep Learning Training. (arXiv:2203.16475v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16475
&lt;/p&gt;
&lt;p&gt;
ConceptEvo&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25581;&#31034;&#27010;&#24565;&#30340;&#20135;&#29983;&#21644;&#28436;&#21464;&#65292;&#24182;&#36890;&#36807;&#20154;&#26426;&#35780;&#20272;&#21644;&#23454;&#39564;&#35777;&#26126;&#20854;&#21457;&#29616;&#23545;&#27169;&#22411;&#21644;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ConceptEvo&#65292;&#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#32479;&#19968;&#35299;&#37322;&#26694;&#26550;&#65292;&#21487;&#20197;&#25581;&#31034;&#35757;&#32451;&#36807;&#31243;&#20013;&#23398;&#20064;&#27010;&#24565;&#30340;&#20135;&#29983;&#21644;&#28436;&#21464;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22635;&#34917;&#20102;DNN&#35299;&#37322;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#31354;&#30333;&#65292;&#22240;&#20026;&#29616;&#26377;&#26041;&#27861;&#20165;&#20851;&#27880;&#35757;&#32451;&#21518;&#30340;&#35299;&#37322;&#12290;ConceptEvo&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#25216;&#26415;&#36129;&#29486;&#65306;&#65288;1&#65289;&#19968;&#31181;&#29983;&#25104;&#32479;&#19968;&#35821;&#20041;&#31354;&#38388;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#19981;&#21516;&#27169;&#22411;&#30340;&#24182;&#34892;&#27604;&#36739;&#65307;&#65288;2&#65289;&#19968;&#31181;&#21457;&#29616;&#21644;&#37327;&#21270;&#31867;&#21035;&#39044;&#27979;&#20013;&#37325;&#35201;&#27010;&#24565;&#28436;&#21464;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#19982;260&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#22823;&#35268;&#27169;&#20154;&#26426;&#35780;&#20272;&#21644;&#23450;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ConceptEvo&#21487;&#20197;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#26377;&#24847;&#20041;&#19988;&#23545;&#39044;&#27979;&#37325;&#35201;&#30340;&#28436;&#21464;&#12290;ConceptEvo&#36866;&#29992;&#20110;&#29616;&#20195;&#65288;ConvNeXt&#65289;&#21644;&#32463;&#20856;&#30340;DNNs&#65288;&#20363;&#22914;VGGs&#65292;InceptionV3&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ConceptEvo, a unified interpretation framework for deep neural networks (DNNs) that reveals the inception and evolution of learned concepts during training. Our work fills a critical gap in DNN interpretation research, as existing methods focus on post-hoc interpretation after training. ConceptEvo presents two novel technical contributions: (1) an algorithm that generates a unified semantic space that enables side-by-side comparison of different models during training; and (2) an algorithm that discovers and quantifies important concept evolutions for class predictions. Through a large-scale human evaluation with 260 participants and quantitative experiments, we show that ConceptEvo discovers evolutions across different models that are meaningful to humans and important for predictions. ConceptEvo works for both modern (ConvNeXt) and classic DNNs (e.g., VGGs, InceptionV3).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22270;&#30340;&#36317;&#31163;&#30697;&#38453;&#23884;&#20837;&#21040;Hilbert Simplex&#20960;&#20309;&#20013;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#21457;&#29616;&#35813;&#20960;&#20309;&#32467;&#26500;&#22312;&#23884;&#20837;&#20219;&#21153;&#20013;&#19982;&#20854;&#20182;&#20960;&#20309;&#32467;&#26500;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#20855;&#26377;&#24555;&#36895;&#21644;&#25968;&#20540;&#31283;&#20581;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2203.11434</link><description>&lt;p&gt;
Hilbert Simplex&#20960;&#20309;&#20013;&#30340;&#38750;&#32447;&#24615;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Non-linear Embeddings in Hilbert Simplex Geometry. (arXiv:2203.11434v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22270;&#30340;&#36317;&#31163;&#30697;&#38453;&#23884;&#20837;&#21040;Hilbert Simplex&#20960;&#20309;&#20013;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#21457;&#29616;&#35813;&#20960;&#20309;&#32467;&#26500;&#22312;&#23884;&#20837;&#20219;&#21153;&#20013;&#19982;&#20854;&#20182;&#20960;&#20309;&#32467;&#26500;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#20855;&#26377;&#24555;&#36895;&#21644;&#25968;&#20540;&#31283;&#20581;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#26159;&#23558;&#31163;&#25955;&#21152;&#26435;&#22270;&#23884;&#20837;&#21040;&#36830;&#32493;&#31354;&#38388;&#20013;&#36827;&#34892;&#21518;&#32493;&#22788;&#29702;&#12290;&#22312;&#20998;&#23618;&#32467;&#26500;&#23884;&#20837;&#21040;&#21452;&#26354;&#20960;&#20309;&#20013;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#65292;&#22240;&#20026;&#24050;&#32463;&#35777;&#26126;&#20219;&#20309;&#21152;&#26435;&#26641;&#37117;&#21487;&#20197;&#20197;&#20219;&#24847;&#20302;&#30340;&#25197;&#26354;&#31243;&#24230;&#23884;&#20837;&#21040;&#35813;&#20960;&#20309;&#20013;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#22270;&#30340;&#36317;&#31163;&#30697;&#38453;&#23884;&#20837;&#21040;Hilbert Simplex&#20960;&#20309;&#20013;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;Hilbert Simplex&#20960;&#20309;&#22312;&#23884;&#20837;&#20219;&#21153;&#20013;&#19982;&#20854;&#20182;&#20960;&#20309;&#32467;&#26500;&#65288;&#22914;Poincar&#233;&#21452;&#26354;&#29699;&#25110;&#27431;&#20960;&#37324;&#24503;&#20960;&#20309;&#65289;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#20855;&#26377;&#24555;&#36895;&#21644;&#25968;&#20540;&#31283;&#20581;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key technique of machine learning and computer vision is to embed discrete weighted graphs into continuous spaces for further downstream processing. Embedding discrete hierarchical structures in hyperbolic geometry has proven very successful since it was shown that any weighted tree can be embedded in that geometry with arbitrary low distortion. Various optimization methods for hyperbolic embeddings based on common models of hyperbolic geometry have been studied. In this paper, we consider Hilbert geometry for the standard simplex which is isometric to a vector space equipped with the variation polytope norm. We study the representation power of this Hilbert simplex geometry by embedding distance matrices of graphs. Our findings demonstrate that Hilbert simplex geometry is competitive to alternative geometries such as the Poincar\'e hyperbolic ball or the Euclidean geometry for embedding tasks while being fast and numerically robust.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#37096;&#20998;&#35266;&#23519;&#30340;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#19979;&#33258;&#28982;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#30340;&#26377;&#38480;&#26102;&#38388;&#29305;&#24615;&#65292;&#24182;&#23545;&#20351;&#29992;&#26377;&#38480;&#29366;&#24577;&#25511;&#21046;&#22120;&#20135;&#29983;&#30340;&#38169;&#35823;&#36827;&#34892;&#20102;&#26126;&#30830;&#30340;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2202.09753</link><description>&lt;p&gt;
&#37096;&#20998;&#35266;&#23519;&#30340;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#30340;&#33258;&#28982;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Finite-Time Analysis of Natural Actor-Critic for POMDPs. (arXiv:2202.09753v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.09753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#37096;&#20998;&#35266;&#23519;&#30340;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#19979;&#33258;&#28982;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#30340;&#26377;&#38480;&#26102;&#38388;&#29305;&#24615;&#65292;&#24182;&#23545;&#20351;&#29992;&#26377;&#38480;&#29366;&#24577;&#25511;&#21046;&#22120;&#20135;&#29983;&#30340;&#38169;&#35823;&#36827;&#34892;&#20102;&#26126;&#30830;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#26377;&#38480;&#25110;&#21487;&#25968;&#26080;&#38480;&#29366;&#24577;&#31354;&#38388;&#30340;&#37096;&#20998;&#35266;&#23519;&#30340;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#25511;&#21046;&#22120;&#21482;&#33021;&#35775;&#38382;&#22522;&#30784;&#25511;&#21046;&#39532;&#23572;&#31185;&#22827;&#38142;&#30340;&#22122;&#22768;&#35266;&#27979;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#26377;&#38480;&#30340;&#20869;&#37096;&#23384;&#20648;&#22120;&#36827;&#34892;&#31574;&#30053;&#21442;&#25968;&#21270;&#65292;&#24182;&#20351;&#29992;&#22810;&#27493;&#26102;&#24207;&#24046;&#24322;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#31574;&#30053;&#35780;&#20272;&#12290;&#20973;&#20511;&#25105;&#20204;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#39318;&#27425;&#30830;&#31435;&#20102;&#37096;&#20998;&#35266;&#23519;&#31995;&#32479;&#19979;&#22522;&#20110;&#20989;&#25968;&#36924;&#36817;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#30340;&#38750;&#28176;&#36817;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#38500;&#20102;&#22312;MDPs&#20013;&#20986;&#29616;&#30340;&#20989;&#25968;&#36924;&#36817;&#21644;&#32479;&#35745;&#35823;&#24046;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#26126;&#30830;&#22320;&#34920;&#24449;&#20102;&#30001;&#20110;&#20351;&#29992;&#26377;&#38480;&#29366;&#24577;&#25511;&#21046;&#22120;&#32780;&#20135;&#29983;&#30340;&#38169;&#35823;&#12290;&#36825;&#31181;&#39069;&#22806;&#30340;&#38169;&#35823;&#26159;&#20197;&#20256;&#32479;&#30340;POMDPs&#20013;&#30340;&#20449;&#24515;&#29366;&#24577;&#21644;&#20351;&#29992;&#26377;&#38480;&#29366;&#24577;&#26102;&#30340;&#38544;&#34255;&#29366;&#24577;&#30340;&#21518;&#39564;&#20998;&#24067;&#20043;&#38388;&#30340;&#24635;&#21464;&#24046;&#36317;&#31163;&#26469;&#34920;&#31034;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the reinforcement learning problem for partially observed Markov decision processes (POMDPs) with large or even countably infinite state spaces, where the controller has access to only noisy observations of the underlying controlled Markov chain. We consider a natural actor-critic method that employs a finite internal memory for policy parameterization, and a multi-step temporal difference learning algorithm for policy evaluation. We establish, to the best of our knowledge, the first non-asymptotic global convergence of actor-critic methods for partially observed systems under function approximation. In particular, in addition to the function approximation and statistical errors that also arise in MDPs, we explicitly characterize the error due to the use of finite-state controllers. This additional error is stated in terms of the total variation distance between the traditional belief state in POMDPs and the posterior distribution of the hidden state when using a finite-sta
&lt;/p&gt;</description></item><item><title>Weisfeiler-Leman&#31639;&#27861;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22788;&#29702;&#22270;&#21644;&#20851;&#31995;&#25968;&#25454;&#12290;&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#35813;&#31639;&#27861;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#29702;&#35770;&#32972;&#26223;&#12289;&#25193;&#23637;&#12289;&#19982;&#31561;&#21464;&#31070;&#32463;&#32593;&#26684;&#30340;&#32852;&#31995;&#12289;&#24182;&#21015;&#20986;&#20102;&#24403;&#21069;&#24212;&#29992;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2112.09992</link><description>&lt;p&gt;
Weisfeiler&#21644;Leman&#26469;&#20570;&#26426;&#22120;&#23398;&#20064;&#20102;&#65306;&#30446;&#21069;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weisfeiler and Leman go Machine Learning: The Story so far. (arXiv:2112.09992v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.09992
&lt;/p&gt;
&lt;p&gt;
Weisfeiler-Leman&#31639;&#27861;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22788;&#29702;&#22270;&#21644;&#20851;&#31995;&#25968;&#25454;&#12290;&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#35813;&#31639;&#27861;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#29702;&#35770;&#32972;&#26223;&#12289;&#25193;&#23637;&#12289;&#19982;&#31561;&#21464;&#31070;&#32463;&#32593;&#26684;&#30340;&#32852;&#31995;&#12289;&#24182;&#21015;&#20986;&#20102;&#24403;&#21069;&#24212;&#29992;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;Weisfeiler-Leman&#31639;&#27861;&#30340;&#31639;&#27861;&#21644;&#31070;&#32463;&#26550;&#26500;&#24050;&#25104;&#20026;&#22788;&#29702;&#22270;&#21644;&#20851;&#31995;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#31639;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#37325;&#28857;&#20851;&#27880;&#30417;&#30563;&#23398;&#20064;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#29702;&#35770;&#32972;&#26223;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#29992;&#20110;&#30417;&#30563;&#22270;&#24418;&#21644;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#65292;&#35752;&#35770;&#20102;&#26368;&#36817;&#30340;&#25193;&#23637;&#65292;&#24182;&#27010;&#36848;&#20102;&#31639;&#27861;&#19982;&#65288;&#32622;&#25442;&#65289;&#31561;&#21464;&#31070;&#32463;&#32593;&#26684;&#30340;&#32852;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#24403;&#21069;&#30340;&#24212;&#29992;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#20197;&#21050;&#28608;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, algorithms and neural architectures based on the Weisfeiler-Leman algorithm, a well-known heuristic for the graph isomorphism problem, have emerged as a powerful tool for machine learning with graphs and relational data. Here, we give a comprehensive overview of the algorithm's use in a machine-learning setting, focusing on the supervised regime. We discuss the theoretical background, show how to use it for supervised graph and node representation learning, discuss recent extensions, and outline the algorithm's connection to (permutation-)equivariant neural architectures. Moreover, we give an overview of current applications and future directions to stimulate further research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#24863;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;&#34701;&#21512;&#21644;&#20999;&#29255;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#21152;&#36895;&#25512;&#26029;&#12290;&#36890;&#36807;&#23558;&#32593;&#32476;&#32454;&#20998;&#20026;&#22810;&#20010;&#29420;&#31435;&#30340;&#21367;&#31215;&#23618;&#32452;&#21512;&#65292;&#24182;&#36827;&#34892;&#34701;&#21512;&#21644;&#20999;&#29255;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#38477;&#20302;&#20869;&#23384;&#21344;&#29992;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2107.06960</link><description>&lt;p&gt;
MAFAT: &#20869;&#23384;&#24863;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;&#34701;&#21512;&#21644;&#20999;&#29255;&#21152;&#36895;&#36793;&#32536;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
MAFAT: Memory-Aware Fusing and Tiling of Neural Networks for Accelerated Edge Inference. (arXiv:2107.06960v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.06960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#24863;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;&#34701;&#21512;&#21644;&#20999;&#29255;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#21152;&#36895;&#25512;&#26029;&#12290;&#36890;&#36807;&#23558;&#32593;&#32476;&#32454;&#20998;&#20026;&#22810;&#20010;&#29420;&#31435;&#30340;&#21367;&#31215;&#23618;&#32452;&#21512;&#65292;&#24182;&#36827;&#34892;&#34701;&#21512;&#21644;&#20999;&#29255;&#25805;&#20316;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#38477;&#20302;&#20869;&#23384;&#21344;&#29992;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#30740;&#31350;&#25361;&#25112;&#26159;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#26412;&#22320;&#36816;&#34892;&#26114;&#36149;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#32593;&#32476;&#12290;&#20855;&#26377;&#22823;&#22411;&#21367;&#31215;&#23618;&#30340;ML&#32593;&#32476;&#24456;&#23481;&#26131;&#36229;&#20986;&#21487;&#29992;&#20869;&#23384;&#65292;&#23548;&#33268;&#30001;&#20110;&#36807;&#22810;&#30340;&#25805;&#20316;&#31995;&#32479;&#20132;&#25442;&#32780;&#22686;&#21152;&#24310;&#36831;&#12290;&#20808;&#21069;&#30340;&#20869;&#23384;&#20943;&#23569;&#25216;&#26415;&#65292;&#22914;&#20462;&#21098;&#21644;&#37327;&#21270;&#65292;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;&#21478;&#22806;&#65292;&#20998;&#24067;&#24335;&#26041;&#27861;&#23558;&#21367;&#31215;&#23618;&#21010;&#20998;&#20026;&#31561;&#25928;&#30340;&#36739;&#23567;&#23376;&#35745;&#31639;&#65292;&#20294;&#23454;&#26045;&#24341;&#20837;&#20102;&#36890;&#20449;&#25104;&#26412;&#65292;&#24182;&#19988;&#38656;&#35201;&#19968;&#20010;&#35774;&#22791;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#20998;&#24067;&#24335;&#21010;&#20998;&#26041;&#27861;&#20063;&#21487;&#20197;&#29992;&#20110;&#22312;&#21333;&#20010;&#35774;&#22791;&#19978;&#20197;&#20943;&#23569;&#30340;&#20869;&#23384;&#21344;&#29992;&#36816;&#34892;&#65292;&#36890;&#36807;&#23558;&#32593;&#32476;&#32454;&#20998;&#20026;&#26356;&#23567;&#30340;&#25805;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#20808;&#21069;&#30340;&#20998;&#24067;&#24335;&#21010;&#20998;&#24037;&#20316;&#25193;&#23637;&#20026;&#22312;&#21333;&#20010;&#35774;&#22791;&#19978;&#20869;&#23384;&#24863;&#30693;&#30340;&#25191;&#34892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#20197;&#20801;&#35768;&#22810;&#20010;&#21367;&#31215;&#23618;&#30340;&#32452;&#25104;&#37096;&#20998;&#29420;&#31435;&#34701;&#21512;&#21644;&#20999;&#29255;&#12290;
&lt;/p&gt;
&lt;p&gt;
A rising research challenge is running costly machine learning (ML) networks locally on resource-constrained edge devices. ML networks with large convolutional layers can easily exceed available memory, increasing latency due to excessive OS swapping. Previous memory reduction techniques such as pruning and quantization reduce model accuracy and often require retraining. Alternatively, distributed methods partition the convolutions into equivalent smaller sub-computations, but the implementations introduce communication costs and require a network of devices. Distributed partitioning approaches can, however, also be used to run in a reduced memory footprint on a single device by subdividing the network into smaller operations. In this paper, we extend prior work on distributed partitioning into a memory-aware execution on a single device. Our approach extends prior fusing strategies to allow for multiple groups of convolutional layers that are fused and tiled independently. This enable
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#27010;&#29575;&#20844;&#24179;&#31574;&#30053;ProbFair&#65292;&#22914;&#20309;&#22312;&#19981;&#30830;&#23450;&#24615;&#36138;&#23146;&#36172;&#21338;&#38382;&#39064;&#20013;&#36827;&#34892;&#36164;&#28304;&#20998;&#37197;&#65292;&#24182;&#22312;&#28385;&#36275;&#39044;&#31639;&#32422;&#26463;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#24635;&#26399;&#26395;&#22870;&#21169;&#12290;&#23454;&#39564;&#35777;&#26126;ProbFair&#22312;&#20445;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#25552;&#20379;&#20844;&#24179;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2106.07677</link><description>&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#24615;&#36138;&#23146;&#36172;&#21338;&#38382;&#39064;&#20013;&#30340;&#20844;&#24179;&#20998;&#37197;&#35268;&#21010;&#65306;&#27010;&#29575;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Planning to Fairly Allocate: Probabilistic Fairness in the Restless Bandit Setting. (arXiv:2106.07677v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#27010;&#29575;&#20844;&#24179;&#31574;&#30053;ProbFair&#65292;&#22914;&#20309;&#22312;&#19981;&#30830;&#23450;&#24615;&#36138;&#23146;&#36172;&#21338;&#38382;&#39064;&#20013;&#36827;&#34892;&#36164;&#28304;&#20998;&#37197;&#65292;&#24182;&#22312;&#28385;&#36275;&#39044;&#31639;&#32422;&#26463;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#24635;&#26399;&#26395;&#22870;&#21169;&#12290;&#23454;&#39564;&#35777;&#26126;ProbFair&#22312;&#20445;&#25345;&#25928;&#29992;&#30340;&#21516;&#26102;&#25552;&#20379;&#20844;&#24179;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#31639;&#21463;&#38480;&#30340;&#36164;&#28304;&#20998;&#37197;&#20013;&#65292;&#24120;&#24120;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#21644;&#23849;&#28291;&#36172;&#21338;&#27169;&#22411;&#26469;&#25551;&#36848;&#20855;&#26377;&#21160;&#20316;&#30456;&#20851;&#36716;&#31227;&#27010;&#29575;&#30340;&#24773;&#22659;&#65292;&#20363;&#22914;&#22312;&#24739;&#32773;&#20043;&#38388;&#20998;&#37197;&#20581;&#24247;&#24178;&#39044;&#25514;&#26045;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#36825;&#20010;&#35268;&#21010;&#38382;&#39064;&#30340;&#26368;&#20808;&#36827;&#30340;Whittle&#25351;&#25968;&#26041;&#27861;&#35201;&#20040;&#19981;&#32771;&#34385;&#36172;&#21338;&#20043;&#38388;&#30340;&#20844;&#24179;&#24615;&#65292;&#35201;&#20040;&#22312;&#20445;&#35777;&#20844;&#24179;&#24615;&#26102;&#28608;&#21169;&#20844;&#24179;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;ProbFair&#65292;&#19968;&#31181;&#27010;&#29575;&#20844;&#24179;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#22312;&#28385;&#36275;&#39044;&#31639;&#32422;&#26463;&#30340;&#21516;&#26102;&#65292;&#26368;&#22823;&#21270;&#24635;&#26399;&#26395;&#22870;&#21169;&#65292;&#24182;&#30830;&#20445;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#19978;&#34987;&#36873;&#25321;&#30340;&#27010;&#29575;&#20855;&#26377;&#20005;&#26684;&#30340;&#27491;&#19979;&#30028;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#22312;&#36825;&#20010;&#24212;&#29992;&#20013;&#65292;&#20171;&#20837;&#25514;&#26045;&#25903;&#25345;&#24739;&#32773;&#38388;&#25345;&#32493;&#30340;&#27491;&#21387;&#36890;&#27668;&#65288;CPAP&#65289;&#30103;&#27861;&#30340;&#20381;&#20174;&#24615;&#65292;&#20197;&#21450;&#22312;&#26356;&#24191;&#27867;&#30340;&#21512;&#25104;&#36716;&#31227;&#30697;&#38453;&#31867;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;ProbFair&#22312;&#25552;&#20379;&#20844;&#24179;&#24615;&#20445;&#35777;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restless and collapsing bandits are often used to model budget-constrained resource allocation in settings where arms have action-dependent transition probabilities, such as the allocation of health interventions among patients. However, state-of-the-art Whittle-index-based approaches to this planning problem either do not consider fairness among arms, or incentivize fairness without guaranteeing it. We thus introduce ProbFair, a probabilistically fair policy that maximizes total expected reward and satisfies the budget constraint while ensuring a strictly positive lower bound on the probability of being pulled at each timestep. We evaluate our algorithm on a real-world application, where interventions support continuous positive airway pressure (CPAP) therapy adherence among patients, as well as on a broader class of synthetic transition matrices. We find that ProbFair preserves utility while providing fairness guarantees.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#30340;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#24418;&#24335;&#26469;&#23398;&#20064;&#26368;&#20248;&#20108;&#36827;&#21046;&#20998;&#31867;&#26641;&#65292;&#35813;&#24418;&#24335;&#20855;&#26377;&#26356;&#24378;&#30340;&#32447;&#24615;&#20248;&#21270;&#33021;&#21147;&#65292;&#24182;&#33021;&#22788;&#29702;&#20391;&#32422;&#26463;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#21644;&#20844;&#24179;&#30340;&#20915;&#31574;&#26641;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2103.15965</link><description>&lt;p&gt;
&#24378;&#20248;&#21270;&#20998;&#31867;&#26641;
&lt;/p&gt;
&lt;p&gt;
Strong Optimal Classification Trees. (arXiv:2103.15965v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.15965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#30340;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#24418;&#24335;&#26469;&#23398;&#20064;&#26368;&#20248;&#20108;&#36827;&#21046;&#20998;&#31867;&#26641;&#65292;&#35813;&#24418;&#24335;&#20855;&#26377;&#26356;&#24378;&#30340;&#32447;&#24615;&#20248;&#21270;&#33021;&#21147;&#65292;&#24182;&#33021;&#22788;&#29702;&#20391;&#32422;&#26463;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#21644;&#20844;&#24179;&#30340;&#20915;&#31574;&#26641;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#26159;&#26368;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20043;&#19968;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#20174;&#25910;&#30410;&#31649;&#29702;&#21644;&#21307;&#23398;&#21040;&#29983;&#29289;&#20449;&#24687;&#23398;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#23398;&#20064;&#20855;&#26377;&#21333;&#21464;&#37327;&#20998;&#21106;&#30340;&#26368;&#20248;&#20108;&#36827;&#21046;&#20998;&#31867;&#26641;&#30340;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#35813;&#39046;&#22495;&#30340;&#25991;&#29486;&#22823;&#37327;&#28044;&#29616;&#65292;&#26082;&#21463;&#21040;&#21551;&#21457;&#24335;&#26041;&#27861;&#32463;&#39564;&#24615;&#27425;&#20248;&#24615;&#30340;&#25512;&#21160;&#65292;&#20063;&#21463;&#21040;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#65288;MIO&#65289;&#25216;&#26415;&#30340;&#24040;&#22823;&#25913;&#36827;&#30340;&#25512;&#21160;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;MIO&#30340;&#26041;&#27861;&#27809;&#26377;&#20805;&#20998;&#21457;&#25381;MIO&#30340;&#23041;&#21147;&#65306;&#23427;&#20204;&#20381;&#36182;&#20110;&#24369;&#30340;&#24418;&#24335;&#65292;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#24930;&#19988;&#23384;&#22312;&#36739;&#22823;&#20248;&#21270;&#38388;&#38553;&#12290;&#20026;&#20102;&#22635;&#34917;&#25991;&#29486;&#20013;&#30340;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#22522;&#20110;&#27969;&#30340;MIO&#24418;&#24335;&#26469;&#23398;&#20064;&#26368;&#20248;&#20108;&#36827;&#21046;&#20998;&#31867;&#26641;&#12290;&#25105;&#20204;&#30340;&#24418;&#24335;&#21487;&#20197;&#36866;&#24212;&#20391;&#32422;&#26463;&#65292;&#20197;&#23454;&#29616;&#21487;&#35299;&#37322;&#21644;&#20844;&#24179;&#30340;&#20915;&#31574;&#26641;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24418;&#24335;&#20855;&#26377;&#26356;&#24378;&#30340;&#32447;&#24615;&#20248;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees are among the most popular machine learning models and are used routinely in applications ranging from revenue management and medicine to bioinformatics. In this paper, we consider the problem of learning optimal binary classification trees with univariate splits. Literature on the topic has burgeoned in recent years, motivated both by the empirical suboptimality of heuristic approaches and the tremendous improvements in mixed-integer optimization (MIO) technology. Yet, existing MIO-based approaches from the literature do not leverage the power of MIO to its full extent: they rely on weak formulations, resulting in slow convergence and large optimality gaps. To fill this gap in the literature, we propose an intuitive flow-based MIO formulation for learning optimal binary classification trees. Our formulation can accommodate side constraints to enable the design of interpretable and fair decision trees. Moreover, we show that our formulation has a stronger linear optimiza
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#35843;&#26597;&#20102;25&#20010;&#22797;&#26434;&#24230;&#24230;&#37327;&#19982;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#26222;&#36866;&#24615;&#33021;&#21147;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21457;&#29616;&#22522;&#20110;PAC-Bayes&#24179;&#22374;&#24615;&#21644;&#36335;&#24452;&#33539;&#25968;&#30340;&#24230;&#37327;&#26041;&#27861;&#20135;&#29983;&#20102;&#26368;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2103.03328</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#22797;&#26434;&#24230;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Complexity Measures for Deep Learning Generalization in Medical Image Analysis. (arXiv:2103.03328v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.03328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#35843;&#26597;&#20102;25&#20010;&#22797;&#26434;&#24230;&#24230;&#37327;&#19982;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#26222;&#36866;&#24615;&#33021;&#21147;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21457;&#29616;&#22522;&#20110;PAC-Bayes&#24179;&#22374;&#24615;&#21644;&#36335;&#24452;&#33539;&#25968;&#30340;&#24230;&#37327;&#26041;&#27861;&#20135;&#29983;&#20102;&#26368;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#36890;&#24120;&#22312;&#20351;&#29992;&#19981;&#21516;&#35774;&#22791;&#36827;&#34892;&#25968;&#25454;&#37319;&#38598;&#12289;&#35774;&#22791;&#35774;&#32622;&#25110;&#24739;&#32773;&#32676;&#20307;&#26102;&#38477;&#20302;&#12290;&#23545;&#26032;&#22270;&#20687;&#30340;&#26222;&#36866;&#24615;&#33021;&#21147;&#30340;&#26356;&#22909;&#29702;&#35299;&#23545;&#20020;&#24202;&#21307;&#29983;&#23545;&#28145;&#24230;&#23398;&#20064;&#30340;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#26469;&#24314;&#31435;&#26222;&#36866;&#24615;&#30028;&#38480;&#21644;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#20294;&#26159;&#39044;&#27979;&#21644;&#23454;&#38469;&#30340;&#26222;&#36866;&#24615;&#34920;&#29616;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#30456;&#20851;&#30340;&#22823;&#22411;&#32463;&#39564;&#30740;&#31350;&#20027;&#35201;&#22522;&#20110;&#36890;&#29992;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#39564;&#35777;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#32463;&#39564;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;25&#20010;&#22797;&#26434;&#24230;&#24230;&#37327;&#19982;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#26222;&#36866;&#24615;&#33021;&#21147;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;PAC-Bayes&#24179;&#22374;&#24615;&#21644;&#36335;&#24452;&#33539;&#25968;&#30340;&#24230;&#37327;&#26041;&#27861;&#20135;&#29983;&#20102;&#26368;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalization performance of deep learning models for medical image analysis often decreases on images collected with different devices for data acquisition, device settings, or patient population. A better understanding of the generalization capacity on new images is crucial for clinicians' trustworthiness in deep learning. Although significant research efforts have been recently directed toward establishing generalization bounds and complexity measures, still, there is often a significant discrepancy between the predicted and actual generalization performance. As well, related large empirical studies have been primarily based on validation with general-purpose image datasets. This paper presents an empirical study that investigates the correlation between 25 complexity measures and the generalization abilities of supervised deep learning classifiers for breast ultrasound images. The results indicate that PAC-Bayes flatness-based and path norm-based measures produce the most cons
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MixPath&#30340;&#32479;&#19968;&#30340;&#19968;&#27425;&#24615;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#27425;&#24615;&#30340;&#22810;&#36335;&#24452;&#36229;&#32593;&#32476;&#26469;&#20934;&#30830;&#35780;&#20272;&#20505;&#36873;&#26550;&#26500;&#12290;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#21046;&#31216;&#20026;Shadow Batch Normalization&#65288;SBN&#65289;&#26469;&#35299;&#20915;&#22810;&#36335;&#24452;&#32467;&#26500;&#30340;&#29305;&#24449;&#24046;&#24322;&#38382;&#39064;&#65292;&#31283;&#23450;&#20248;&#21270;&#24182;&#25552;&#39640;&#25490;&#21517;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2001.05887</link><description>&lt;p&gt;
MixPath: &#19968;&#31181;&#32479;&#19968;&#30340;&#19968;&#27425;&#24615;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MixPath: A Unified Approach for One-shot Neural Architecture Search. (arXiv:2001.05887v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2001.05887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MixPath&#30340;&#32479;&#19968;&#30340;&#19968;&#27425;&#24615;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#27425;&#24615;&#30340;&#22810;&#36335;&#24452;&#36229;&#32593;&#32476;&#26469;&#20934;&#30830;&#35780;&#20272;&#20505;&#36873;&#26550;&#26500;&#12290;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#21046;&#31216;&#20026;Shadow Batch Normalization&#65288;SBN&#65289;&#26469;&#35299;&#20915;&#22810;&#36335;&#24452;&#32467;&#26500;&#30340;&#29305;&#24449;&#24046;&#24322;&#38382;&#39064;&#65292;&#31283;&#23450;&#20248;&#21270;&#24182;&#25552;&#39640;&#25490;&#21517;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#26550;&#26500;&#35774;&#35745;&#20013;&#65292;&#28151;&#21512;&#22810;&#20010;&#21367;&#31215;&#26680;&#34987;&#35777;&#26126;&#26159;&#26377;&#20248;&#21183;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20004;&#38454;&#27573;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#20027;&#35201;&#23616;&#38480;&#20110;&#21333;&#36335;&#24452;&#25628;&#32034;&#31354;&#38388;&#12290;&#22914;&#20309;&#39640;&#25928;&#22320;&#25628;&#32034;&#22810;&#36335;&#24452;&#32467;&#26500;&#30340;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30340;&#21160;&#26426;&#26159;&#35757;&#32451;&#19968;&#20010;&#19968;&#27425;&#24615;&#30340;&#22810;&#36335;&#24452;&#36229;&#32593;&#32476;&#26469;&#20934;&#30830;&#35780;&#20272;&#20505;&#36873;&#26550;&#26500;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#25152;&#30740;&#31350;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#65292;&#20174;&#22810;&#20010;&#36335;&#24452;&#20013;&#27714;&#21644;&#30340;&#29305;&#24449;&#21521;&#37327;&#20960;&#20046;&#26159;&#21333;&#20010;&#36335;&#24452;&#30340;&#20493;&#25968;&#12290;&#36825;&#31181;&#24046;&#24322;&#25200;&#20081;&#20102;&#36229;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#25490;&#21517;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#21046;&#65292;&#31216;&#20026;Shadow Batch Normalization&#65288;SBN&#65289;&#65292;&#26469;&#35268;&#33539;&#24046;&#24322;&#30340;&#29305;&#24449;&#32479;&#35745;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;SBN&#33021;&#22815;&#31283;&#23450;&#20248;&#21270;&#21644;&#25552;&#39640;&#25490;&#21517;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#32479;&#19968;&#22810;&#36335;&#24452;&#19968;&#27425;&#24615;&#26041;&#27861;&#31216;&#20026;MixPath&#65292;&#21487;&#20197;&#29983;&#25104;&#19968;&#31995;&#21015;&#33021;&#36798;&#21040;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blending multiple convolutional kernels is proved advantageous in neural architecture design. However, current two-stage neural architecture search methods are mainly limited to single-path search spaces. How to efficiently search models of multi-path structures remains a difficult problem. In this paper, we are motivated to train a one-shot multi-path supernet to accurately evaluate the candidate architectures. Specifically, we discover that in the studied search spaces, feature vectors summed from multiple paths are nearly multiples of those from a single path. Such disparity perturbs the supernet training and its ranking ability. Therefore, we propose a novel mechanism called Shadow Batch Normalization (SBN) to regularize the disparate feature statistics. Extensive experiments prove that SBNs are capable of stabilizing the optimization and improving ranking performance. We call our unified multi-path one-shot approach as MixPath, which generates a series of models that achieve state
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22768;&#26126;&#24615;&#26426;&#21046;&#35774;&#35745;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#26426;&#26500;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#19968;&#31181;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#24341;&#36215;&#20154;&#20204;&#23545;&#20154;&#24037;&#25945;&#23398;&#30340;&#20851;&#27880;&#65292;&#24182;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/1912.13122</link><description>&lt;p&gt;
&#22768;&#26126;&#24615;&#26426;&#21046;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Declarative Mechanism Design. (arXiv:1912.13122v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.13122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22768;&#26126;&#24615;&#26426;&#21046;&#35774;&#35745;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#26426;&#26500;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#19968;&#31181;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#24341;&#36215;&#20154;&#20204;&#23545;&#20154;&#24037;&#25945;&#23398;&#30340;&#20851;&#27880;&#65292;&#24182;&#25552;&#20379;&#20102;&#21021;&#27493;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65288;MAS&#65289;&#21644;&#22768;&#26126;&#24615;&#30005;&#23376;&#26426;&#26500;&#65288;DEIs&#65289;&#30340;&#35843;&#25511;&#26159;&#36807;&#21435;&#21313;&#24180;&#28041;&#21450;&#29289;&#29702;&#21644;&#36719;&#20214;&#26234;&#33021;&#20307;&#20197;&#21450;&#27861;&#24459;&#30340;&#22810;&#23398;&#31185;&#30740;&#31350;&#35838;&#39064;&#65292;&#20294;&#36817;&#24180;&#26469;&#36880;&#28176;&#28436;&#21464;&#20026;2016&#24180;&#36215;&#34987;&#31216;&#20026;&#26032;&#38395;&#30340;&#26426;&#22120;&#24459;&#24072;&#12290;&#20854;&#20013;&#19968;&#31181;&#39318;&#27425;&#25552;&#20986;&#38480;&#21046;&#36719;&#20214;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#26041;&#26696;&#26159;&#30005;&#23376;&#26426;&#26500;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#34987;&#37325;&#26032;&#23450;&#20041;&#20026;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#65292;&#26377;&#20851;DL&#20351;&#29992;&#30340;&#23433;&#20840;&#12289;&#38544;&#31169;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#38382;&#39064;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31038;&#21306;&#30340;&#20851;&#27880;&#12290;&#29616;&#22312;&#65292;MAS&#30340;&#35268;&#33539;&#20960;&#20046;&#24471;&#21040;&#27491;&#30830;&#22788;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#33539;&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#31867;&#22411;&#30340;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20043;&#20026;&#26426;&#26500;&#31070;&#32463;&#32593;&#32476;&#65288;INN&#65289;&#12290;&#26412;&#25991;&#30340;&#20027;&#26088;&#26159;&#24341;&#36215;&#20154;&#20204;&#23545;&#20154;&#24037;&#25945;&#23398;&#65288;AT&#65289;&#30340;&#20851;&#27880;&#65292;&#24182;&#32473;&#20986;&#19968;&#20010;&#21021;&#27493;&#30340;&#31572;&#26696;&#65292;&#23637;&#31034;&#20102;&#19968;&#31181;&#35777;&#26126;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regulation of Multi-Agent Systems (MAS) and Declarative Electronic Institutions (DEIs) was a multidisciplinary research topic of the past decade involving (Physical and Software) Agents and Law since the beginning, but recently evolved towards News-claimed Robot Lawyer since 2016. One of these first proposals of restricting the behaviour of Software Agentswas Electronic Institutions.However, with the recent reformulation of Artificial Neural Networks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legal issues regarding the use of DL has raised concerns in the Artificial Intelligence (AI) Community. Now that the Regulation of MAS is almost correctly addressed, we propose the Regulation of Artificial Neural Networks as Agent-based Training of a special type of regulated Artificial Neural Network that we call Institutional Neural Network (INN).The main purpose of this paper is to bring attention to Artificial Teaching (AT) and to give a tentative answer showing a proof-of-con
&lt;/p&gt;</description></item></channel></rss>