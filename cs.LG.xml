<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20102;&#22522;&#20110;&#38598;&#25104;VAE&#30340;&#20998;&#32780;&#27835;&#20043;&#26041;&#27861;&#65292;&#22312;&#39640;&#32500;&#24230;&#12289;&#20302;&#26679;&#26412;&#25968;&#37327;&#30340;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#26356;&#22909;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#22312;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15661</link><description>&lt;p&gt;
&#22312;&#39640;&#32500;&#24230;&#12289;&#23567;&#26679;&#26412;&#34920;&#26684;&#25968;&#25454;&#19978;&#22686;&#24378;&#34920;&#31034;&#23398;&#20064;&#65306;&#19968;&#31181;&#22522;&#20110;&#38598;&#25104;VAE&#30340;&#20998;&#32780;&#27835;&#20043;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Representation Learning on High-Dimensional, Small-Size Tabular Data: A Divide and Conquer Method with Ensembled VAEs. (arXiv:2306.15661v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15661
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20102;&#22522;&#20110;&#38598;&#25104;VAE&#30340;&#20998;&#32780;&#27835;&#20043;&#26041;&#27861;&#65292;&#22312;&#39640;&#32500;&#24230;&#12289;&#20302;&#26679;&#26412;&#25968;&#37327;&#30340;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#26356;&#22909;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#22312;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21450;&#20854;&#21508;&#31181;&#21464;&#20307;&#23637;&#29616;&#20102;&#22312;&#38477;&#32500;&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#24615;&#33021;&#65292;&#36890;&#24120;&#33021;&#36798;&#21040;&#26368;&#26032;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#24230;&#12289;&#20302;&#26679;&#26412;&#25968;&#37327;(HDLSS)&#20219;&#21153;&#20013;&#65292;&#35768;&#22810;&#24403;&#21069;&#26041;&#27861;&#22312;&#23398;&#20064;&#33391;&#22909;&#30340;&#34920;&#31034;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#26159;&#19968;&#20010;&#22266;&#26377;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;VAE&#30340;&#38598;&#25104;&#26469;&#23398;&#20064;&#29305;&#24449;&#31354;&#38388;&#23376;&#38598;&#19978;&#30340;&#21518;&#39564;&#27010;&#29575;&#65292;&#20174;&#32780;&#22312;&#26032;&#39062;&#30340;&#20998;&#32780;&#27835;&#20043;&#26041;&#27861;&#20013;&#32858;&#21512;&#21040;&#19968;&#20010;&#32852;&#21512;&#21518;&#39564;&#20013;&#65292;&#20174;&#32780;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;&#21518;&#39564;&#30340;&#19968;&#31181;&#26367;&#20195;&#20998;&#35299;&#26041;&#24335;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38544;&#24335;&#25968;&#25454;&#22686;&#24378;&#24418;&#24335;&#65292;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#36890;&#36807;&#23545;&#20843;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;HDLSS&#35774;&#32622;&#20013;&#23398;&#20064;&#21040;&#26356;&#22909;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#36825;&#23548;&#33268;&#20102;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#20013;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#35299;&#32544;&#32538;&#25928;&#26524;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational Autoencoders and their many variants have displayed impressive ability to perform dimensionality reduction, often achieving state-of-the-art performance. Many current methods however, struggle to learn good representations in High Dimensional, Low Sample Size (HDLSS) tasks, which is an inherently challenging setting. We address this challenge by using an ensemble of lightweight VAEs to learn posteriors over subsets of the feature-space, which get aggregated into a joint posterior in a novel divide-and-conquer approach. Specifically, we present an alternative factorisation of the joint posterior that induces a form of implicit data augmentation that yields greater sample efficiency. Through a series of experiments on eight real-world datasets, we show that our method learns better latent representations in HDLSS settings, which leads to higher accuracy in a downstream classification task. Furthermore, we verify that our approach has a positive effect on disentanglement and a
&lt;/p&gt;</description></item><item><title>SparseOptimizer&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#23427;&#37319;&#29992;&#23884;&#20837;&#30340;&#25910;&#32553;&#25805;&#20316;&#31526;&#65292;&#26080;&#38656;&#23545;&#20195;&#30721;&#36827;&#34892;&#20462;&#25913;&#21363;&#21487;&#36866;&#24212;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#19982;&#23494;&#38598;&#22411;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.15656</link><description>&lt;p&gt;
SparseOptimizer: &#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#26469;&#38477;&#20302;&#35821;&#35328;&#27169;&#22411;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#36890;&#36807;&#32534;&#35793;&#22120;&#20849;&#21516;&#35774;&#35745;&#26469;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design. (arXiv:2306.15656v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15656
&lt;/p&gt;
&lt;p&gt;
SparseOptimizer&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#23427;&#37319;&#29992;&#23884;&#20837;&#30340;&#25910;&#32553;&#25805;&#20316;&#31526;&#65292;&#26080;&#38656;&#23545;&#20195;&#30721;&#36827;&#34892;&#20462;&#25913;&#21363;&#21487;&#36866;&#24212;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#19982;&#23494;&#38598;&#22411;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SparseOptimizer&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65292;ALBERT&#21644;GPT&#65289;&#20013;&#33258;&#28982;&#22320;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;SparseOptimizer&#35774;&#35745;&#30340;&#20851;&#38190;&#26159;&#23884;&#20837;&#30340;&#25910;&#32553;&#25805;&#20316;&#31526;&#65292;&#23427;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#30452;&#25509;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#36825;&#20010;&#25805;&#20316;&#31526;&#36890;&#36807;&#22362;&#23454;&#30340;&#29702;&#35770;&#26694;&#26550;&#25903;&#25345;&#65292;&#24182;&#21253;&#21547;&#20102;&#19968;&#20010;&#20998;&#26512;&#35299;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#20248;&#21270;&#22120;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#26524;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;SparseOptimizer&#30340;&#21363;&#25554;&#21363;&#29992;&#21151;&#33021;&#28040;&#38500;&#20102;&#23545;&#20195;&#30721;&#20462;&#25913;&#30340;&#38656;&#27714;&#65292;&#20351;&#20854;&#25104;&#20026;&#36866;&#29992;&#20110;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#36866;&#24212;&#24037;&#20855;&#12290;&#22312;GLUE&#12289;RACE&#12289;SQuAD1&#21644;SQuAD2&#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#36890;&#36807;SparseOptimizer&#31232;&#30095;&#21270;&#21518;&#30340;SparseBERT&#21644;SparseALBERT&#22312;&#24615;&#33021;&#19978;&#19982;&#23494;&#38598;&#22411;&#30340;BERT&#21644;ALBERT&#30456;&#24403;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces SparseOptimizer, a novel deep learning optimizer that exploits Moreau-Yosida regularization to naturally induce sparsity in large language models such as BERT, ALBERT and GPT. Key to the design of SparseOptimizer is an embedded shrinkage operator, which imparts sparsity directly within the optimization process. This operator, backed by a sound theoretical framework, includes an analytical solution, thereby reinforcing the optimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-play functionality eradicates the need for code modifications, making it a universally adaptable tool for a wide array of large language models. Empirical evaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2 confirm that SparseBERT and SparseALBERT, when sparsified using SparseOptimizer, achieve performance comparable to their dense counterparts, BERT and ALBERT, while significantly reducing their parameter count. Further, this work proposes an innovati
&lt;/p&gt;</description></item><item><title>Dental CLAIRES&#26159;&#19968;&#20010;&#29992;&#20110;&#21475;&#33108;&#30740;&#31350;&#30340;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#26816;&#32034;&#25628;&#32034;&#24037;&#20855;&#65292;&#21033;&#29992;&#26681;&#23574;X&#23556;&#32447;&#29255;&#21644;&#30456;&#20851;&#20020;&#24202;&#32454;&#33410;&#26469;&#26816;&#32034;&#26368;&#21305;&#37197;&#30340;&#22270;&#20687;&#12290;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#22312;hit@3&#27604;&#29575;&#21644;&#24179;&#22343;&#20498;&#25968;&#25490;&#21517;&#26041;&#38754;&#22343;&#21462;&#24471;&#20102;&#38750;&#24120;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.15651</link><description>&lt;p&gt;
Dental CLAIRES: &#21475;&#33108;&#30740;&#31350;&#30340;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#26816;&#32034;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Dental CLAIRES: Contrastive LAnguage Image REtrieval Search for Dental Research. (arXiv:2306.15651v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15651
&lt;/p&gt;
&lt;p&gt;
Dental CLAIRES&#26159;&#19968;&#20010;&#29992;&#20110;&#21475;&#33108;&#30740;&#31350;&#30340;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#26816;&#32034;&#25628;&#32034;&#24037;&#20855;&#65292;&#21033;&#29992;&#26681;&#23574;X&#23556;&#32447;&#29255;&#21644;&#30456;&#20851;&#20020;&#24202;&#32454;&#33410;&#26469;&#26816;&#32034;&#26368;&#21305;&#37197;&#30340;&#22270;&#20687;&#12290;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#22312;hit@3&#27604;&#29575;&#21644;&#24179;&#22343;&#20498;&#25968;&#25490;&#21517;&#26041;&#38754;&#22343;&#21462;&#24471;&#20102;&#38750;&#24120;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#29273;&#31185;&#25918;&#23556;&#22270;&#23398;&#20013;&#23398;&#20064;&#35786;&#26029;&#29305;&#24449;&#21644;&#30456;&#20851;&#20020;&#24202;&#20449;&#24687;&#23545;&#20110;&#29273;&#31185;&#30740;&#31350;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19987;&#23478;&#27880;&#37322;&#30340;&#25968;&#25454;&#21644;&#20415;&#25463;&#30340;&#25628;&#32034;&#24037;&#20855;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#21033;&#29992;&#29992;&#25143;&#30340;&#21475;&#33108;&#30456;&#20851;&#30740;&#31350;&#26597;&#35810;&#30340;&#25628;&#32034;&#24037;&#20855;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#26816;&#32034;&#25628;&#32034;&#29992;&#20110;&#29273;&#31185;&#30740;&#31350;&#65292;&#21363;Dental CLAIRES&#65292;&#21033;&#29992;&#26681;&#23574;X&#23556;&#32447;&#29255;&#21644;&#30456;&#20851;&#20020;&#24202;&#32454;&#33410;&#65288;&#22914;&#29273;&#21608;&#35786;&#26029;&#12289;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65289;&#26681;&#25454;&#25991;&#26412;&#26597;&#35810;&#26816;&#32034;&#26368;&#21305;&#37197;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#24212;&#29992;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#27491;&#23545;&#21305;&#37197;&#23545;&#65288;&#30495;&#23454;&#23545;&#65289;&#30340;&#30456;&#20284;&#24615;&#20998;&#25968;&#24182;&#26368;&#23567;&#21270;&#36127;&#23545;&#65288;&#38543;&#26426;&#23545;&#65289;&#30340;&#20998;&#25968;&#26469;&#25214;&#21040;&#30001;&#29992;&#25143;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;96%&#30340;hit@3&#27604;&#29575;&#21644;0.82&#30340;&#24179;&#22343;&#20498;&#25968;&#25490;&#21517;&#65288;MRR&#65289;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65292;&#35753;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#39564;&#35777;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning about diagnostic features and related clinical information from dental radiographs is important for dental research. However, the lack of expert-annotated data and convenient search tools poses challenges. Our primary objective is to design a search tool that uses a user's query for oral-related research. The proposed framework, Contrastive LAnguage Image REtrieval Search for dental research, Dental CLAIRES, utilizes periapical radiographs and associated clinical details such as periodontal diagnosis, demographic information to retrieve the best-matched images based on the text query. We applied a contrastive representation learning method to find images described by the user's text by maximizing the similarity score of positive pairs (true pairs) and minimizing the score of negative pairs (random pairs). Our model achieved a hit@3 ratio of 96% and a Mean Reciprocal Rank (MRR) of 0.82. We also designed a graphical user interface that allows researchers to verify the model's pe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32771;&#34385;&#21306;&#22495;&#38388;&#30340;&#26377;&#25928;&#30005;&#38459;&#65292;&#32780;&#19981;&#26159;&#28857;&#23545;&#38388;&#30340;&#26377;&#25928;&#30005;&#38459;&#65292;&#24182;&#26681;&#25454;&#27599;&#20010;&#21306;&#22495;&#30340;&#23494;&#24230;&#36866;&#24403;&#32553;&#25918;&#36793;&#26435;&#37325;&#65292;&#35299;&#20915;&#20102;&#22312;&#24230;&#37327;&#31354;&#38388;&#20013;&#26377;&#25928;&#30005;&#38459;&#25910;&#25947;&#21040;&#24494;&#19981;&#36275;&#36947;&#37327;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.15649</link><description>&lt;p&gt;
&#22312;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#26377;&#25928;&#30005;&#38459;
&lt;/p&gt;
&lt;p&gt;
Effective resistance in metric spaces. (arXiv:2306.15649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32771;&#34385;&#21306;&#22495;&#38388;&#30340;&#26377;&#25928;&#30005;&#38459;&#65292;&#32780;&#19981;&#26159;&#28857;&#23545;&#38388;&#30340;&#26377;&#25928;&#30005;&#38459;&#65292;&#24182;&#26681;&#25454;&#27599;&#20010;&#21306;&#22495;&#30340;&#23494;&#24230;&#36866;&#24403;&#32553;&#25918;&#36793;&#26435;&#37325;&#65292;&#35299;&#20915;&#20102;&#22312;&#24230;&#37327;&#31354;&#38388;&#20013;&#26377;&#25928;&#30005;&#38459;&#25910;&#25947;&#21040;&#24494;&#19981;&#36275;&#36947;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30005;&#38459;&#65288;ER&#65289;&#26159;&#19968;&#31181;&#25506;&#27979;&#22270;&#32467;&#26500;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#23427;&#26159;&#35745;&#31639;&#22270;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#29305;&#24449;&#21521;&#37327;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;ER&#30340;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#24212;&#29992;&#26159;&#23545;&#28857;&#20113;&#36827;&#34892;&#20998;&#26512;&#65292;&#21363;&#22270;&#30340;&#39030;&#28857;&#23545;&#24212;&#20110;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;IID&#26679;&#26412;&#30340;&#20998;&#24067;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;&#38543;&#30528;&#26679;&#26412;&#22823;&#23567;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#65292;&#20219;&#24847;&#20004;&#28857;&#20043;&#38388;&#30340;ER&#25910;&#25947;&#21040;&#19968;&#20010;&#27809;&#26377;&#20851;&#20110;&#22270;&#32467;&#26500;&#30340;&#20449;&#24687;&#30340;&#24494;&#19981;&#36275;&#36947;&#30340;&#37327;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#23567;&#21306;&#22495;&#20043;&#38388;&#30340;&#22522;&#20110;&#21306;&#22495;&#30340;ER&#65292;&#32780;&#19981;&#26159;&#28857;&#23545;&#20043;&#38388;&#30340;ER&#65292;&#24182;&#26681;&#25454;&#27599;&#20010;&#21306;&#22495;&#20013;&#30340;&#28508;&#22312;&#23494;&#24230;&#36866;&#24403;&#22320;&#32553;&#25918;&#36793;&#26435;&#37325;&#65292;&#35777;&#26126;&#20102;&#36825;&#20010;&#24494;&#19981;&#36275;&#36947;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#34987;&#32469;&#36807;&#12290;&#36890;&#36807;&#20445;&#25345;&#21306;&#22495;&#19981;&#21464;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#22522;&#20110;&#21306;&#22495;&#30340;ER&#22312;&#28857;&#30340;&#25968;&#37327;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#25910;&#25947;&#21040;&#19968;&#20010;&#38750;&#24494;&#19981;&#36275;&#36947;&#30340;&#26497;&#38480;&#65292;&#21363;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;ER&#12290;&#25105;&#20204;&#29992;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective resistance (ER) is an attractive way to interrogate the structure of graphs. It is an alternative to computing the eigenvectors of the graph Laplacian.  One attractive application of ER is to point clouds, i.e. graphs whose vertices correspond to IID samples from a distribution over a metric space. Unfortunately, it was shown that the ER between any two points converges to a trivial quantity that holds no information about the graph's structure as the size of the sample increases to infinity.  In this study, we show that this trivial solution can be circumvented by considering a region-based ER between pairs of small regions rather than pairs of points and by scaling the edge weights appropriately with respect to the underlying density in each region. By keeping the regions fixed, we show analytically that the region-based ER converges to a non-trivial limit as the number of points increases to infinity. Namely the ER on a metric space. We support our theoretical findings wit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#22312;&#25968;&#25454;&#20849;&#20139;&#12289;&#25968;&#25454;&#22686;&#24378;&#12289;&#31867;&#21035;&#24179;&#34913;&#21644;&#25968;&#25454;&#27719;&#24635;&#31561;&#29992;&#20363;&#20013;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#23545;&#20110;ML&#35757;&#32451;&#30340;&#23454;&#29992;&#24615;&#30446;&#21069;&#23578;&#32570;&#20047;&#36275;&#22815;&#30340;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.15636</link><description>&lt;p&gt;
&#20851;&#20110;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#30340;&#23454;&#29992;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Usefulness of Synthetic Tabular Data Generation. (arXiv:2306.15636v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#22312;&#25968;&#25454;&#20849;&#20139;&#12289;&#25968;&#25454;&#22686;&#24378;&#12289;&#31867;&#21035;&#24179;&#34913;&#21644;&#25968;&#25454;&#27719;&#24635;&#31561;&#29992;&#20363;&#20013;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#23545;&#20110;ML&#35757;&#32451;&#30340;&#23454;&#29992;&#24615;&#30446;&#21069;&#23578;&#32570;&#20047;&#36275;&#22815;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#31185;&#23398;&#30028;&#23545;&#20854;&#23454;&#29992;&#24615;&#20173;&#32570;&#20047;&#19968;&#33268;&#30340;&#20849;&#35782;&#12290;&#26222;&#36941;&#35748;&#20026;&#65292;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#25968;&#25454;&#20132;&#25442;&#21644;&#21152;&#24378;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#35757;&#32451;&#12290;&#38544;&#31169;&#20445;&#25252;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21487;&#20197;&#21152;&#36895;&#19979;&#28216;&#20219;&#21153;&#30340;&#25968;&#25454;&#20132;&#25442;&#65292;&#20294;&#30446;&#21069;&#23578;&#32570;&#20047;&#36275;&#22815;&#30340;&#35777;&#25454;&#26469;&#35828;&#26126;&#21512;&#25104;&#25968;&#25454;&#22914;&#20309;&#25110;&#20026;&#20309;&#33021;&#22815;&#25552;&#21319;ML&#35757;&#32451;&#12290;&#26412;&#30740;&#31350;&#23545;&#22235;&#31181;&#29992;&#20363;&#20351;&#29992;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#23545;ML&#24615;&#33021;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65306;&#25968;&#25454;&#20849;&#20139;&#12289;&#25968;&#25454;&#22686;&#24378;&#12289;&#31867;&#21035;&#24179;&#34913;&#21644;&#25968;&#25454;&#27719;&#24635;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#24179;&#34913;&#29992;&#20363;&#26377;&#36731;&#24494;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#30446;&#21069;&#24182;&#27809;&#26377;&#36275;&#22815;&#30340;&#35777;&#25454;&#26469;&#22768;&#31216;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#23545;ML&#35757;&#32451;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances in synthetic data generation, the scientific community still lacks a unified consensus on its usefulness. It is commonly believed that synthetic data can be used for both data exchange and boosting machine learning (ML) training. Privacy-preserving synthetic data generation can accelerate data exchange for downstream tasks, but there is not enough evidence to show how or why synthetic data can boost ML training. In this study, we benchmarked ML performance using synthetic tabular data for four use cases: data sharing, data augmentation, class balancing, and data summarization. We observed marginal improvements for the balancing use case on some datasets. However, we conclude that there is not enough evidence to claim that synthetic tabular data is useful for ML training.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33410;&#28857;&#29366;&#24577;&#26356;&#26032;&#21644;&#28040;&#24687;&#20989;&#25968;&#35843;&#29992;&#20998;&#31163;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#24322;&#27493;&#35745;&#31639;&#65292;&#24182;&#20197;&#27492;&#20316;&#20026;&#22522;&#30784;&#65292;&#36827;&#34892;&#20102;&#24322;&#27493;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2306.15632</link><description>&lt;p&gt;
&#24322;&#27493;&#31639;&#27861;&#19982;Cocycles&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Algorithmic Alignment with Cocycles. (arXiv:2306.15632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15632
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#33410;&#28857;&#29366;&#24577;&#26356;&#26032;&#21644;&#28040;&#24687;&#20989;&#25968;&#35843;&#29992;&#20998;&#31163;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#24322;&#27493;&#35745;&#31639;&#65292;&#24182;&#20197;&#27492;&#20316;&#20026;&#22522;&#30784;&#65292;&#36827;&#34892;&#20102;&#24322;&#27493;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#22120;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#30340;&#28040;&#24687;&#20256;&#36882;&#12290;&#20294;&#26159;&#65292;&#20856;&#22411;&#30340;GNN&#22312;&#23450;&#20041;&#21644;&#35843;&#29992;&#28040;&#24687;&#20989;&#25968;&#20043;&#38388;&#27169;&#31946;&#20102;&#21306;&#21035;&#65292;&#36843;&#20351;&#33410;&#28857;&#22312;&#27599;&#19968;&#23618;&#37117;&#21521;&#20854;&#37051;&#23621;&#21457;&#36865;&#28040;&#24687;&#65292;&#21516;&#27493;&#22320;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;GNN&#24212;&#29992;&#20110;&#23398;&#20064;&#25191;&#34892;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#26102;&#65292;&#22823;&#22810;&#25968;&#27493;&#39588;&#21482;&#26377;&#23569;&#25968;&#20960;&#20010;&#33410;&#28857;&#20250;&#26377;&#26377;&#24847;&#20041;&#30340;&#26356;&#26032;&#35201;&#21457;&#36865;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#22312;&#22270;&#20013;&#21457;&#36865;&#22826;&#22810;&#26080;&#20851;&#30340;&#25968;&#25454;&#65292;&#21487;&#33021;&#23548;&#33268;&#20302;&#25928;&#29575;&#65292;&#32780;&#35768;&#22810;&#20013;&#38388;&#30340;GNN&#27493;&#39588;&#24517;&#39035;&#23398;&#20064;&#36523;&#20221;&#20989;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#20998;&#31163;&#20102;&#33410;&#28857;&#29366;&#24577;&#26356;&#26032;&#21644;&#28040;&#24687;&#20989;&#25968;&#35843;&#29992;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#36825;&#31181;&#20998;&#31163;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#25968;&#23398;&#34920;&#36798;&#65292;&#21487;&#20197;&#35753;&#25105;&#20204;&#24605;&#32771;&#31639;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24322;&#27493;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art neural algorithmic reasoners make use of message passing in graph neural networks (GNNs). But typical GNNs blur the distinction between the definition and invocation of the message function, forcing a node to send messages to its neighbours at every layer, synchronously. When applying GNNs to learn to execute dynamic programming algorithms, however, on most steps only a handful of the nodes would have meaningful updates to send. One, hence, runs the risk of inefficiencies by sending too much irrelevant data across the graph -- with many intermediate GNN steps having to learn identity functions. In this work, we explicitly separate the concepts of node state update and message function invocation. With this separation, we obtain a mathematical formulation that allows us to reason about asynchronous computation in both algorithms and neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31070;&#32463;Galerkin&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#20998;&#24067;&#30340;&#25968;&#25454;&#26469;&#20272;&#35745;&#35757;&#32451;&#25439;&#22833;&#65292;&#20197;&#24212;&#23545;&#20855;&#26377;&#23616;&#37096;&#29305;&#24449;&#21644;&#39640;&#26041;&#24046;&#30340;&#36755;&#36816;&#20027;&#23548;&#30340;&#39640;&#32500;&#38382;&#39064;&#12290;&#31890;&#23376;&#38598;&#21512;&#36890;&#36807;&#19982;&#35299;&#22330;&#30340;&#38750;&#32447;&#24615;&#21442;&#25968;&#21270;&#32806;&#21512;&#30340;&#21160;&#21147;&#23398;&#28436;&#21270;&#26469;&#20027;&#21160;&#35843;&#25972;&#65292;&#20197;&#20445;&#25345;&#20449;&#24687;&#20016;&#23500;&#12290;</title><link>http://arxiv.org/abs/2306.15630</link><description>&lt;p&gt;
&#31070;&#32463;Galerkin&#26041;&#26696;&#20013;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#32806;&#21512;&#21442;&#25968;&#21644;&#31890;&#23376;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Coupling parameter and particle dynamics for adaptive sampling in Neural Galerkin schemes. (arXiv:2306.15630v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31070;&#32463;Galerkin&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#20998;&#24067;&#30340;&#25968;&#25454;&#26469;&#20272;&#35745;&#35757;&#32451;&#25439;&#22833;&#65292;&#20197;&#24212;&#23545;&#20855;&#26377;&#23616;&#37096;&#29305;&#24449;&#21644;&#39640;&#26041;&#24046;&#30340;&#36755;&#36816;&#20027;&#23548;&#30340;&#39640;&#32500;&#38382;&#39064;&#12290;&#31890;&#23376;&#38598;&#21512;&#36890;&#36807;&#19982;&#35299;&#22330;&#30340;&#38750;&#32447;&#24615;&#21442;&#25968;&#21270;&#32806;&#21512;&#30340;&#21160;&#21147;&#23398;&#28436;&#21270;&#26469;&#20027;&#21160;&#35843;&#25972;&#65292;&#20197;&#20445;&#25345;&#20449;&#24687;&#20016;&#23500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#38750;&#32447;&#24615;&#21442;&#25968;&#21270;&#65292;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#25968;&#20540;&#36924;&#36817;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#36890;&#24120;&#22522;&#20110;&#26368;&#23567;&#21270;&#21253;&#25324;&#27531;&#24046;&#30340;&#25439;&#22833;&#65292;&#35813;&#27531;&#24046;&#22312;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#26159;&#21487;&#20197;&#35299;&#26512;&#24471;&#21040;&#30340;&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#23384;&#22312;&#23616;&#37096;&#29305;&#24449;(&#22914;&#27874;&#21644;&#30456;&#24178;&#32467;&#26500;)&#30340;&#20197;&#36755;&#36816;&#20026;&#20027;&#30340;&#39640;&#32500;&#38382;&#39064;&#30340;&#27531;&#24046;&#21644;&#30456;&#20851;&#37327;&#21487;&#33021;&#20855;&#26377;&#36739;&#39640;&#30340;&#26041;&#24046;&#65292;&#22240;&#27492;&#32463;&#39564;&#20272;&#35745;&#35757;&#32451;&#25439;&#22833;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#26469;&#33258;&#26080;&#20449;&#24687;&#22343;&#21248;&#20998;&#24067;&#30340;&#25968;&#25454;&#26679;&#26412;&#30340;&#20272;&#35745;&#22120;&#26159;&#20302;&#25928;&#30340;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31070;&#32463;Galerkin&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20351;&#29992;&#26469;&#33258;&#33258;&#36866;&#24212;&#20998;&#24067;&#30340;&#25968;&#25454;&#26469;&#20272;&#35745;&#35757;&#32451;&#25439;&#22833;&#65292;&#36825;&#20123;&#20998;&#24067;&#36890;&#36807;&#31890;&#23376;&#38598;&#21512;&#36827;&#34892;&#32463;&#39564;&#34920;&#31034;&#12290;&#31890;&#23376;&#38598;&#21512;&#36890;&#36807;&#19982;&#35299;&#22330;&#30340;&#38750;&#32447;&#24615;&#21442;&#25968;&#21270;&#32806;&#21512;&#30340;&#21160;&#21147;&#23398;&#28436;&#21270;&#26469;&#20027;&#21160;&#35843;&#25972;&#65292;&#20351;&#24471;&#31890;&#23376;&#38598;&#21512;&#20445;&#25345;&#20449;&#24687;&#20016;&#23500;&#20197;&#29992;&#20110;&#20272;&#35745;...
&lt;/p&gt;
&lt;p&gt;
Training nonlinear parametrizations such as deep neural networks to numerically approximate solutions of partial differential equations is often based on minimizing a loss that includes the residual, which is analytically available in limited settings only. At the same time, empirically estimating the training loss is challenging because residuals and related quantities can have high variance, especially for transport-dominated and high-dimensional problems that exhibit local features such as waves and coherent structures. Thus, estimators based on data samples from un-informed, uniform distributions are inefficient. This work introduces Neural Galerkin schemes that estimate the training loss with data from adaptive distributions, which are empirically represented via ensembles of particles. The ensembles are actively adapted by evolving the particles with dynamics coupled to the nonlinear parametrizations of the solution fields so that the ensembles remain informative for estimating t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#34920;&#24449;&#21644;&#26657;&#27491;&#20013;&#24615;&#21407;&#23376;NISQ&#35774;&#22791;&#19978;&#30340;&#22122;&#22768;&#21442;&#25968;&#65292;&#20026;&#36827;&#19968;&#27493;&#25552;&#39640;&#35745;&#31639;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2306.15628</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20013;&#24615;&#21407;&#23376;NISQ&#35774;&#22791;&#22122;&#22768;&#29305;&#24615;&#21644;&#26657;&#27491;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Machine-learning based noise characterization and correction on neutral atoms NISQ devices. (arXiv:2306.15628v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#34920;&#24449;&#21644;&#26657;&#27491;&#20013;&#24615;&#21407;&#23376;NISQ&#35774;&#22791;&#19978;&#30340;&#22122;&#22768;&#21442;&#25968;&#65292;&#20026;&#36827;&#19968;&#27493;&#25552;&#39640;&#35745;&#31639;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#24615;&#21407;&#23376;&#22120;&#20214;&#21033;&#29992;&#20809;&#38218;&#25490;&#21015;&#21407;&#23376;&#21644;&#35843;&#21046;&#28608;&#20809;&#33033;&#20914;&#25511;&#21046;&#37327;&#23376;&#24577;&#65292;&#20195;&#34920;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#12290;Pasqal&#21457;&#23637;&#20102;&#19968;&#31181;&#20351;&#29992;&#38135;&#21407;&#23376;&#30340;&#20013;&#24615;&#21407;&#23376;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#35774;&#22791;&#65292;&#21487;&#22788;&#29702;&#22810;&#36798;100&#20010;&#37327;&#23376;&#27604;&#29305;&#12290;&#25152;&#26377;NISQ&#35774;&#22791;&#37117;&#21463;&#21040;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#36825;&#23545;&#35745;&#31639;&#32467;&#26524;&#26377;&#30528;&#19968;&#23450;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#26356;&#22909;&#22320;&#20102;&#35299;&#21644;&#34920;&#24449;&#22122;&#22768;&#28304;&#24182;&#21487;&#33021;&#32416;&#27491;&#23427;&#20204;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#34920;&#24449;&#21644;&#26657;&#27491;&#20013;&#24615;&#21407;&#23376;NISQ&#35774;&#22791;&#19978;&#30340;&#22122;&#22768;&#21442;&#25968;&#12290;&#29305;&#21035;&#20851;&#27880;Pasqal&#35774;&#22791;&#65292;&#24182;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#26469;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#12290;&#20026;&#20102;&#34920;&#24449;&#22122;&#22768;&#21442;&#25968;&#65292;&#35757;&#32451;&#20102;&#22810;&#20010;ML&#27169;&#22411;&#65292;&#21482;&#20351;&#29992;&#21407;&#23376;&#26368;&#32456;&#37327;&#23376;&#24577;&#30340;&#27979;&#37327;&#32467;&#26524;&#20316;&#20026;&#36755;&#20837;&#65292;&#26469;&#39044;&#27979;&#28608;&#20809;&#24378;&#24230;&#27874;&#21160;&#21644;&#33136;&#22260;&#12289;&#28201;&#24230;&#20197;&#21450;&#20551;&#38451;&#24615;&#21644;&#20551;&#38452;&#24615;&#27979;&#37327;&#20540;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neutral atoms devices represent a promising technology that uses optical tweezers to geometrically arrange atoms and modulated laser pulses to control the quantum states. A neutral atoms Noisy Intermediate Scale Quantum (NISQ) device is developed by Pasqal with rubidium atoms that will allow to work with up to 100 qubits. All NISQ devices are affected by noise that have an impact on the computations results. Therefore it is important to better understand and characterize the noise sources and possibly to correct them. Here, two approaches are proposed to characterize and correct noise parameters on neutral atoms NISQ devices. In particular the focus is on Pasqal devices and Machine Learning (ML) techniques are adopted to pursue those objectives. To characterize the noise parameters, several ML models are trained, using as input only the measurements of the final quantum state of the atoms, to predict laser intensity fluctuation and waist, temperature and false positive and negative mea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;LeanDojo&#65292;&#35813;&#24037;&#20855;&#36890;&#36807;&#25552;&#21462;Lean&#30340;&#25968;&#25454;&#65292;&#20026;&#23450;&#29702;&#35777;&#26126;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#24179;&#21488;&#12290;&#21033;&#29992;LeanDojo&#30340;&#25968;&#25454;&#65292;&#24320;&#21457;&#20102;ReProver&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#26126;&#22120;&#65292;&#21487;&#20197;&#20174;&#24222;&#22823;&#30340;&#25968;&#23398;&#24211;&#20013;&#36873;&#25321;&#21629;&#39064;&#65292;&#35757;&#32451;&#25104;&#26412;&#20302;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#19968;&#21608;&#30340;GPU&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.15626</link><description>&lt;p&gt;
LeanDojo: &#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#29702;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
LeanDojo: Theorem Proving with Retrieval-Augmented Language Models. (arXiv:2306.15626v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;LeanDojo&#65292;&#35813;&#24037;&#20855;&#36890;&#36807;&#25552;&#21462;Lean&#30340;&#25968;&#25454;&#65292;&#20026;&#23450;&#29702;&#35777;&#26126;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#24179;&#21488;&#12290;&#21033;&#29992;LeanDojo&#30340;&#25968;&#25454;&#65292;&#24320;&#21457;&#20102;ReProver&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#26126;&#22120;&#65292;&#21487;&#20197;&#20174;&#24222;&#22823;&#30340;&#25968;&#23398;&#24211;&#20013;&#36873;&#25321;&#21629;&#39064;&#65292;&#35757;&#32451;&#25104;&#26412;&#20302;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#19968;&#21608;&#30340;GPU&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20351;&#29992;Lean&#31561;&#35777;&#26126;&#21161;&#25163;&#35777;&#26126;&#24418;&#24335;&#23450;&#29702;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31169;&#26377;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#22823;&#37327;&#35745;&#31639;&#35201;&#27714;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24456;&#38590;&#22797;&#21046;&#25110;&#24314;&#31435;&#22312;&#20854;&#22522;&#30784;&#19978;&#65292;&#36825;&#32473;&#23450;&#29702;&#35777;&#26126;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#30740;&#31350;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#38556;&#30861;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;LeanDojo&#26469;&#28040;&#38500;&#36825;&#20123;&#38556;&#30861;&#65306;&#19968;&#20010;&#21253;&#21547;&#24037;&#20855;&#21253;&#12289;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;Lean&#28216;&#20048;&#22330;&#12290;LeanDojo&#20174;Lean&#20013;&#25552;&#21462;&#25968;&#25454;&#65292;&#24182;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#32534;&#31243;&#19982;&#35777;&#26126;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#23427;&#21253;&#21547;&#35777;&#26126;&#20013;&#21629;&#39064;&#30340;&#32454;&#31890;&#24230;&#27880;&#37322;&#65292;&#20026;&#21629;&#39064;&#36873;&#25321;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#65306;&#36825;&#26159;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#24320;&#21457;&#20986;&#20102;ReProver&#65288;&#26816;&#32034;&#22686;&#24378;&#30340;&#35777;&#26126;&#22120;&#65289;&#65306;&#23427;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;LLM&#30340;&#35777;&#26126;&#22120;&#65292;&#36890;&#36807;&#26816;&#32034;&#20174;&#24222;&#22823;&#30340;&#25968;&#23398;&#24211;&#20013;&#36873;&#25321;&#21629;&#39064;&#12290;&#23427;&#25104;&#26412;&#20302;&#24265;&#65292;&#21482;&#38656;&#35201;&#19968;&#21608;&#30340;GPU&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#26816;&#32034;&#22120;&#21033;&#29992;&#20102;LeanDojo&#30340;pro&#30456;&#20851;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine learning methods for theorem proving. This paper removes these barriers by introducing LeanDojo: an open-source Lean playground consisting of toolkits, data, models, and benchmarks. LeanDojo extracts data from Lean and enables interaction with the proof environment programmatically. It contains fine-grained annotations of premises in proofs, providing valuable data for premise selection: a key bottleneck in theorem proving. Using this data, we develop ReProver (Retrieval-Augmented Prover): the first LLM-based prover that is augmented with retrieval for selecting premises from a vast math library. It is inexpensive and needs only one GPU week of training. Our retriever leverages LeanDojo's prog
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20215;&#20540;&#24863;&#30693;&#37325;&#35201;&#24615;&#26435;&#37325;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#27491;&#31163;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#37325;&#35201;&#24615;&#37319;&#26679;&#26435;&#37325;&#30340;&#26041;&#24046;&#24182;&#20445;&#25345;&#26080;&#20559;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#23454;&#36341;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.15625</link><description>&lt;p&gt;
&#38024;&#23545;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#30340;&#20215;&#20540;&#24863;&#30693;&#37325;&#35201;&#24615;&#21152;&#26435;
&lt;/p&gt;
&lt;p&gt;
Value-aware Importance Weighting for Off-policy Reinforcement Learning. (arXiv:2306.15625v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20215;&#20540;&#24863;&#30693;&#37325;&#35201;&#24615;&#26435;&#37325;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#27491;&#31163;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#37325;&#35201;&#24615;&#37319;&#26679;&#26435;&#37325;&#30340;&#26041;&#24046;&#24182;&#20445;&#25345;&#26080;&#20559;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#23454;&#36341;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#35201;&#24615;&#37319;&#26679;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#31163;&#31574;&#30053;&#39044;&#27979;&#30340;&#19968;&#20010;&#26680;&#24515;&#24605;&#24819;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#19968;&#20010;&#20998;&#24067;&#20013;&#30340;&#26679;&#26412;&#36827;&#34892;&#37325;&#26032;&#21152;&#26435;&#65292;&#20174;&#32780;&#22312;&#21478;&#19968;&#20010;&#20998;&#24067;&#19979;&#33719;&#24471;&#26080;&#20559;&#30340;&#20272;&#35745;&#20540;&#12290;&#28982;&#32780;&#65292;&#37325;&#35201;&#24615;&#37319;&#26679;&#26435;&#37325;&#24448;&#24448;&#20250;&#34920;&#29616;&#20986;&#26497;&#31471;&#30340;&#26041;&#24046;&#65292;&#24120;&#24120;&#23548;&#33268;&#23454;&#36341;&#20013;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31867;&#26356;&#24191;&#27867;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#26469;&#26657;&#27491;&#31163;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#8220;&#20215;&#20540;&#24863;&#30693;&#37325;&#35201;&#24615;&#26435;&#37325;&#8221;&#65292;&#23427;&#32771;&#34385;&#20102;&#26679;&#26412;&#31354;&#38388;&#65292;&#20174;&#32780;&#22312;&#30446;&#26631;&#20998;&#24067;&#19979;&#25552;&#20379;&#26356;&#20302;&#30340;&#26041;&#24046;&#65292;&#20294;&#20173;&#28982;&#26159;&#26080;&#20559;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#22914;&#20309;&#35745;&#31639;&#36825;&#26679;&#30340;&#26435;&#37325;&#65292;&#24182;&#35814;&#32454;&#35828;&#26126;&#20102;&#32467;&#26524;&#37325;&#35201;&#24615;&#26435;&#37325;&#30340;&#20851;&#38190;&#23646;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20960;&#20010;&#24378;&#21270;&#23398;&#20064;&#39044;&#27979;&#31639;&#27861;&#25193;&#23637;&#21040;&#20351;&#29992;&#36825;&#20123;&#26435;&#37325;&#30340;&#31163;&#31574;&#30053;&#35774;&#32622;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Importance sampling is a central idea underlying off-policy prediction in reinforcement learning. It provides a strategy for re-weighting samples from a distribution to obtain unbiased estimates under another distribution. However, importance sampling weights tend to exhibit extreme variance, often leading to stability issues in practice. In this work, we consider a broader class of importance weights to correct samples in off-policy learning. We propose the use of $\textit{value-aware importance weights}$ which take into account the sample space to provide lower variance, but still unbiased, estimates under a target distribution. We derive how such weights can be computed, and detail key properties of the resulting importance weights. We then extend several reinforcement learning prediction algorithms to the off-policy setting with these weights, and evaluate them empirically.
&lt;/p&gt;</description></item><item><title>SCENEREPLICA&#26159;&#19968;&#20010;&#22522;&#20110;YCB&#23545;&#35937;&#30340;&#21487;&#37325;&#22797;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#33021;&#21147;&#12290;&#27492;&#22522;&#20934;&#27979;&#35797;&#26131;&#20110;&#37325;&#22797;&#24182;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#27604;&#36739;&#19981;&#21516;&#30340;&#25216;&#26415;&#21644;&#31639;&#27861;&#65292;&#26377;&#21161;&#20110;&#21152;&#24555;&#26426;&#22120;&#20154;&#25805;&#32437;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.15620</link><description>&lt;p&gt;
SCENEREPLICA&#65306;&#36890;&#36807;&#21019;&#24314;&#21487;&#37325;&#22797;&#30340;&#22330;&#26223;&#26469;&#35780;&#20272;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
SCENEREPLICA: Benchmarking Real-World Robot Manipulation by Creating Reproducible Scenes. (arXiv:2306.15620v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15620
&lt;/p&gt;
&lt;p&gt;
SCENEREPLICA&#26159;&#19968;&#20010;&#22522;&#20110;YCB&#23545;&#35937;&#30340;&#21487;&#37325;&#22797;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#33021;&#21147;&#12290;&#27492;&#22522;&#20934;&#27979;&#35797;&#26131;&#20110;&#37325;&#22797;&#24182;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#27604;&#36739;&#19981;&#21516;&#30340;&#25216;&#26415;&#21644;&#31639;&#27861;&#65292;&#26377;&#21161;&#20110;&#21152;&#24555;&#26426;&#22120;&#20154;&#25805;&#32437;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#37325;&#22797;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#33021;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#25235;&#21462;&#21644;&#25918;&#32622;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20351;&#29992;&#20102;YCB&#23545;&#35937;&#65292;&#36825;&#26159;&#26426;&#22120;&#20154;&#23398;&#30028;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#30830;&#20445;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#20197;&#19982;&#20854;&#20182;&#30740;&#31350;&#36827;&#34892;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#27492;&#22522;&#20934;&#27979;&#35797;&#36824;&#34987;&#35774;&#35745;&#20026;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26131;&#20110;&#37325;&#22797;&#65292;&#20351;&#20854;&#21487;&#20379;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#20351;&#29992;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23545;&#22522;&#20934;&#27979;&#35797;&#20013;&#22522;&#20110;&#27169;&#22411;&#21644;&#26080;&#27169;&#22411;&#30340;6D&#26426;&#22120;&#20154;&#25235;&#21462;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#20998;&#26512;&#65292;&#20854;&#20013;&#35780;&#20272;&#20102;&#20195;&#34920;&#24615;&#31639;&#27861;&#22312;&#29289;&#20307;&#24863;&#30693;&#12289;&#25235;&#21462;&#35268;&#21010;&#21644;&#36816;&#21160;&#35268;&#21010;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#23558;&#25104;&#20026;&#25512;&#21160;&#26426;&#22120;&#20154;&#25805;&#32437;&#39046;&#22495;&#21457;&#23637;&#30340;&#23453;&#36149;&#24037;&#20855;&#12290;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#26356;&#23481;&#26131;&#22320;&#27604;&#36739;&#19981;&#21516;&#30340;&#25216;&#26415;&#21644;&#31639;&#27861;&#65292;&#20174;&#32780;&#21152;&#24555;&#21457;&#23637;&#26426;&#22120;&#20154;&#25805;&#32437;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new reproducible benchmark for evaluating robot manipulation in the real world, specifically focusing on pick-and-place. Our benchmark uses the YCB objects, a commonly used dataset in the robotics community, to ensure that our results are comparable to other studies. Additionally, the benchmark is designed to be easily reproducible in the real world, making it accessible to researchers and practitioners. We also provide our experimental results and analyzes for model-based and model-free 6D robotic grasping on the benchmark, where representative algorithms are evaluated for object perception, grasping planning, and motion planning. We believe that our benchmark will be a valuable tool for advancing the field of robot manipulation. By providing a standardized evaluation framework, researchers can more easily compare different techniques and algorithms, leading to faster progress in developing robot manipulation methods.
&lt;/p&gt;</description></item><item><title>DCID&#26159;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#19968;&#32500;&#30446;&#26631;&#21464;&#37327;&#20043;&#38388;&#20849;&#20139;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#20256;&#32479;&#26041;&#27861;&#22312;&#19968;&#20803;&#24773;&#20917;&#19979;&#25928;&#26524;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ICM&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#23398;&#20064;&#21040;&#30340;&#20849;&#20139;&#29305;&#24449;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#28145;&#24230;&#35268;&#33539;&#20449;&#24687;&#20998;&#35299;&#65288;DCID&#65289;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.15619</link><description>&lt;p&gt;
DCID: &#28145;&#24230;&#35268;&#33539;&#20449;&#24687;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
DCID: Deep Canonical Information Decomposition. (arXiv:2306.15619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15619
&lt;/p&gt;
&lt;p&gt;
DCID&#26159;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#19968;&#32500;&#30446;&#26631;&#21464;&#37327;&#20043;&#38388;&#20849;&#20139;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#20256;&#32479;&#26041;&#27861;&#22312;&#19968;&#20803;&#24773;&#20917;&#19979;&#25928;&#26524;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ICM&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#23398;&#20064;&#21040;&#30340;&#20849;&#20139;&#29305;&#24449;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#28145;&#24230;&#35268;&#33539;&#20449;&#24687;&#20998;&#35299;&#65288;DCID&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#23384;&#22312;&#39069;&#22806;&#30340;&#22810;&#21464;&#37327;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#35782;&#21035;&#20004;&#20010;&#19968;&#32500;&#30446;&#26631;&#21464;&#37327;&#20043;&#38388;&#20849;&#20139;&#30340;&#20449;&#21495;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#19978;&#65292;&#22522;&#20110;&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#30340;&#26041;&#27861;&#34987;&#29992;&#20110;&#35782;&#21035;&#20849;&#20139;&#21464;&#37327;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#34987;&#35774;&#35745;&#29992;&#20110;&#22810;&#21464;&#37327;&#30446;&#26631;&#65292;&#24182;&#19988;&#23545;&#20110;&#19968;&#20803;&#24773;&#20917;&#21482;&#25552;&#20379;&#24494;&#19981;&#36275;&#36947;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#27169;&#22411;&#26469;&#23398;&#20064;&#31232;&#30095;&#19988;&#20849;&#20139;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#20182;&#20204;&#30340;&#39044;&#27979;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#27809;&#26377;&#31995;&#32479;&#22320;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#33021;&#27491;&#30830;&#22320;&#24674;&#22797;&#20849;&#20139;&#30340;&#20449;&#21495;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#19968;&#20803;&#20849;&#20139;&#20449;&#24687;&#26816;&#32034;&#30340;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;ICM&#65292;&#19968;&#31181;&#21487;&#20197;&#22312;&#23384;&#22312;&#30495;&#23454;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#35780;&#20272;&#23398;&#20064;&#21040;&#30340;&#20849;&#20139;&#29305;&#24449;&#30340;&#19977;&#20010;&#26041;&#38754;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#28145;&#24230;&#35268;&#33539;&#20449;&#24687;&#20998;&#35299;&#65288;DCID&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;&#22810;&#21464;&#37327;&#30446;&#26631;&#19968;&#33268;&#25193;&#23637;&#21040;&#19968;&#32500;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of identifying the signal shared between two one-dimensional target variables, in the presence of additional multivariate observations. Canonical Correlation Analysis (CCA)-based methods have traditionally been used to identify shared variables, however, they were designed for multivariate targets and only offer trivial solutions for univariate cases. In the context of Multi-Task Learning (MTL), various models were postulated to learn features that are sparse and shared across multiple tasks. However, these methods were typically evaluated by their predictive performance. To the best of our knowledge, no prior studies systematically evaluated models in terms of correctly recovering the shared signal. Here, we formalize the setting of univariate shared information retrieval, and propose ICM, an evaluation metric which can be used in the presence of ground-truth labels, quantifying 3 aspects of the learned shared features. We further propose Deep Canonical Informa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26410;&#30693;&#38750;&#33258;&#27835;&#31995;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20462;&#25913;&#31995;&#32479;&#21644;&#38477;&#32500;&#25554;&#20540;&#26500;&#24314;&#21442;&#25968;&#21270;&#20195;&#29702;&#27169;&#22411;&#26469;&#35299;&#20915;&#26102;&#38388;&#30456;&#20851;&#24211;&#26222;&#26364;&#31639;&#23376;&#30340;&#36817;&#20284;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.15618</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#23398;&#20064;&#38750;&#33258;&#27835;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Learning Nonautonomous Systems via Dynamic Mode Decomposition. (arXiv:2306.15618v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15618
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26410;&#30693;&#38750;&#33258;&#27835;&#31995;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20462;&#25913;&#31995;&#32479;&#21644;&#38477;&#32500;&#25554;&#20540;&#26500;&#24314;&#21442;&#25968;&#21270;&#20195;&#29702;&#27169;&#22411;&#26469;&#35299;&#20915;&#26102;&#38388;&#30456;&#20851;&#24211;&#26222;&#26364;&#31639;&#23376;&#30340;&#36817;&#20284;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65288;DMD&#65289;&#30340;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26410;&#30693;&#30340;&#38750;&#33258;&#27835;&#21160;&#21147;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#30340;&#36755;&#20837;&#12290;&#20026;&#20102;&#35268;&#36991;&#23545;&#38750;&#33258;&#27835;&#31995;&#32479;&#20013;&#30340;&#26102;&#38388;&#30456;&#20851;&#24211;&#26222;&#26364;&#31639;&#23376;&#36827;&#34892;&#36817;&#20284;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20174;&#22806;&#37096;&#26102;&#38388;&#30456;&#20851;&#36755;&#20837;&#30340;&#23616;&#37096;&#21442;&#25968;&#21270;&#23548;&#20986;&#30340;&#20462;&#25913;&#31995;&#32479;&#20316;&#20026;&#23545;&#21407;&#22987;&#38750;&#33258;&#27835;&#31995;&#32479;&#30340;&#36817;&#20284;&#12290;&#20462;&#25913;&#21518;&#30340;&#31995;&#32479;&#30001;&#19968;&#31995;&#21015;&#23616;&#37096;&#21442;&#25968;&#21270;&#31995;&#32479;&#32452;&#25104;&#65292;&#21487;&#20197;&#36890;&#36807;&#25105;&#20204;&#20808;&#21069;&#25552;&#20986;&#30340;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#36827;&#34892;&#38477;&#32500;&#21644;&#25554;&#20540;&#30340;&#26694;&#26550;&#65288;DRIPS&#65289;&#20351;&#29992;&#21442;&#25968;&#21270;&#20195;&#29702;&#27169;&#22411;&#36827;&#34892;&#24456;&#22909;&#30340;&#36817;&#20284;&#12290;DRIPS&#30340;&#31163;&#32447;&#27493;&#39588;&#20381;&#36182;&#20110;DMD&#26469;&#26500;&#24314;&#32447;&#24615;&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#36171;&#20104;&#20174;&#35757;&#32451;&#25968;&#25454;&#26144;&#23556;&#32780;&#26469;&#30340;&#21487;&#35266;&#27979;&#20540;&#38477;&#24207;&#22522;&#65288;ROBs&#65289;&#12290;&#28982;&#21518;&#65292;&#31163;&#32447;&#27493;&#39588;&#20174;&#36866;&#24403;&#27969;&#24418;&#19978;&#30340;&#25554;&#20540;&#26500;&#36896;&#19968;&#31995;&#21015;&#36845;&#20195;&#21442;&#25968;&#20195;&#29702;&#27169;&#22411;&#65292;&#20854;&#20013;&#30446;&#26631;/&#27979;&#35797;&#21442;&#25968;&#28857;&#21487;&#20197;&#20174;&#25554;&#20540;&#21644;&#27491;&#20132;&#25805;&#20316;&#20013;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a data-driven learning approach for unknown nonautonomous dynamical systems with time-dependent inputs based on dynamic mode decomposition (DMD). To circumvent the difficulty of approximating the time-dependent Koopman operators for nonautonomous systems, a modified system derived from local parameterization of the external time-dependent inputs is employed as an approximation to the original nonautonomous system. The modified system comprises a sequence of local parametric systems, which can be well approximated by a parametric surrogate model using our previously proposed framework for dimension reduction and interpolation in parameter space (DRIPS). The offline step of DRIPS relies on DMD to build a linear surrogate model, endowed with reduced-order bases (ROBs), for the observables mapped from training data. Then the offline step constructs a sequence of iterative parametric surrogate models from interpolations on suitable manifolds, where the target/test parameter point
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20301;&#32622;&#25554;&#20540;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#26368;&#23567;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;RoPE-based&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#26368;&#22810;32768&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#33719;&#24471;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#36890;&#36807;&#32447;&#24615;&#38477;&#20302;&#36755;&#20837;&#20301;&#32622;&#32034;&#24341;&#30340;&#22823;&#23567;&#65292;&#25105;&#20204;&#20445;&#25345;&#20102;&#25193;&#23637;&#27169;&#22411;&#22312;&#21407;&#22987;&#19978;&#19979;&#25991;&#31383;&#21475;&#20869;&#20219;&#21153;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.15595</link><description>&lt;p&gt;
&#36890;&#36807;&#20301;&#32622;&#25554;&#20540;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;
&lt;/p&gt;
&lt;p&gt;
Extending Context Window of Large Language Models via Positional Interpolation. (arXiv:2306.15595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15595
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20301;&#32622;&#25554;&#20540;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#26368;&#23567;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;RoPE-based&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#26368;&#22810;32768&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#33719;&#24471;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#36890;&#36807;&#32447;&#24615;&#38477;&#20302;&#36755;&#20837;&#20301;&#32622;&#32034;&#24341;&#30340;&#22823;&#23567;&#65292;&#25105;&#20204;&#20445;&#25345;&#20102;&#25193;&#23637;&#27169;&#22411;&#22312;&#21407;&#22987;&#19978;&#19979;&#25991;&#31383;&#21475;&#20869;&#20219;&#21153;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20301;&#32622;&#25554;&#20540;&#65288;PI&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26368;&#23567;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;RoPE-based&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;LLaMA&#27169;&#22411;&#65289;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#25193;&#23637;&#21040;&#26368;&#22810;32768&#65292;&#24182;&#19988;&#22312;&#38656;&#35201;&#38271;&#19978;&#19979;&#25991;&#30340;&#21508;&#31181;&#20219;&#21153;&#65288;&#21253;&#25324;&#23494;&#38053;&#26816;&#32034;&#12289;&#35821;&#35328;&#24314;&#27169;&#21644;&#38271;&#31687;&#25991;&#26723;&#25688;&#35201;&#31561;&#65289;&#19978;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20301;&#32622;&#25554;&#20540;&#25193;&#23637;&#30340;&#27169;&#22411;&#22312;&#21407;&#22987;&#19978;&#19979;&#25991;&#31383;&#21475;&#20869;&#30340;&#20219;&#21153;&#20013;&#30456;&#23545;&#20445;&#25345;&#33391;&#22909;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20301;&#32622;&#25554;&#20540;&#32447;&#24615;&#22320;&#38477;&#20302;&#36755;&#20837;&#20301;&#32622;&#32034;&#24341;&#30340;&#22823;&#23567;&#65292;&#20197;&#21305;&#37197;&#21407;&#22987;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#65292;&#32780;&#19981;&#26159;&#36229;&#36807;&#35757;&#32451;&#26102;&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#39640;&#27880;&#24847;&#21147;&#20998;&#25968;&#65292;&#23436;&#20840;&#30772;&#22351;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#30740;&#31350;&#34920;&#26126;&#65292;&#25554;&#20540;&#30340;&#19978;&#30028;&#33267;&#23569;&#26159;&#25512;&#26029;&#30340;&#19978;&#30028;&#30340;$\sim 600 \times$&#35201;&#23567;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#20854;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Position Interpolation (PI) that extends the context window sizes of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B. Meanwhile, the extended model by Position Interpolation preserve quality relatively well on tasks within its original context window. To achieve this goal, Position Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond the trained context length which may lead to catastrophically high attention scores that completely ruin the self-attention mechanism. Our theoretical study shows that the upper bound of interpolation is at least $\sim 600 \times$ smaller than that of extrapolation, further demonstrating its stability. Models extend
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;MARLIN&#65292;&#29992;&#20110;&#22312;&#25112;&#26415;&#29615;&#22659;&#20013;&#36827;&#34892;&#25317;&#22622;&#25511;&#21046;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20934;&#30830;&#19988;&#21487;&#24182;&#34892;&#21270;&#30340;&#20223;&#30495;&#29615;&#22659;&#26469;&#27169;&#25311;&#25112;&#26415;&#32593;&#32476;&#26465;&#20214;&#65292;&#24182;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#20195;&#29702;&#30340;&#31934;&#32454;&#21270;&#24378;&#21270;&#23398;&#20064;&#20844;&#24335;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;&#36890;&#36807;&#35757;&#32451;MARLIN&#26234;&#33021;&#20307;&#22312;&#27169;&#25311;&#26465;&#20214;&#19979;&#36827;&#34892;&#29942;&#39048;&#38142;&#36335;&#36716;&#25442;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15591</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#21160;&#24577;&#32593;&#32476;&#20013;&#33322;&#34892;&#65306;MARLIN&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#22312;&#25112;&#26415;&#29615;&#22659;&#20013;&#30340;&#25317;&#22622;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Learning to Sail Dynamic Networks: The MARLIN Reinforcement Learning Framework for Congestion Control in Tactical Environments. (arXiv:2306.15591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;MARLIN&#65292;&#29992;&#20110;&#22312;&#25112;&#26415;&#29615;&#22659;&#20013;&#36827;&#34892;&#25317;&#22622;&#25511;&#21046;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20934;&#30830;&#19988;&#21487;&#24182;&#34892;&#21270;&#30340;&#20223;&#30495;&#29615;&#22659;&#26469;&#27169;&#25311;&#25112;&#26415;&#32593;&#32476;&#26465;&#20214;&#65292;&#24182;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#20195;&#29702;&#30340;&#31934;&#32454;&#21270;&#24378;&#21270;&#23398;&#20064;&#20844;&#24335;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;&#36890;&#36807;&#35757;&#32451;MARLIN&#26234;&#33021;&#20307;&#22312;&#27169;&#25311;&#26465;&#20214;&#19979;&#36827;&#34892;&#29942;&#39048;&#38142;&#36335;&#36716;&#25442;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25317;&#22622;&#25511;&#21046;&#31639;&#27861;&#65292;&#22914;TCP Cubic&#65292;&#22312;&#25112;&#26415;&#29615;&#22659;&#20013;&#36935;&#21040;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#38169;&#35823;&#22320;&#23558;&#20002;&#21253;&#21644;&#32593;&#32476;&#24615;&#33021;&#27874;&#21160;&#35270;&#20026;&#25317;&#22622;&#30151;&#29366;&#12290;&#26368;&#36817;&#30340;&#21162;&#21147;&#65292;&#21253;&#25324;&#25105;&#20204;&#33258;&#24049;&#30340;MARLIN&#65292;&#24050;&#32463;&#25506;&#32034;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#25317;&#22622;&#25511;&#21046;&#65292;&#20294;&#22312;&#31454;&#20105;&#28608;&#28872;&#12289;&#19981;&#31283;&#23450;&#21644;&#24847;&#22806;&#24773;&#20917;&#19979;&#24448;&#24448;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;RL&#26694;&#26550;&#65292;&#21033;&#29992;&#20934;&#30830;&#19988;&#21487;&#24182;&#34892;&#21270;&#30340;&#20223;&#30495;&#29615;&#22659;&#37325;&#29616;&#25112;&#26415;&#32593;&#32476;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;&#22312;&#36825;&#31181;&#22797;&#26434;&#22330;&#26223;&#20013;&#36816;&#34892;&#30340;&#20195;&#29702;&#30340;&#31934;&#32454;&#21270;RL&#20844;&#24335;&#21644;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#22312;&#21355;&#26143;&#36890;&#20449;&#65288;SATCOM&#65289;&#21644;UHF&#23485;&#24102;&#65288;UHF&#65289;&#26080;&#32447;&#30005;&#38142;&#36335;&#20043;&#38388;&#27169;&#25311;&#29942;&#39048;&#38142;&#36335;&#36716;&#25442;&#26465;&#20214;&#30340;MARLIN&#26234;&#33021;&#20307;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;RL&#23398;&#20064;&#26694;&#26550;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#20854;&#22312;&#25991;&#20214;&#20256;&#36755;&#20219;&#21153;&#20013;&#19982;Transm&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional Congestion Control (CC) algorithms,such as TCP Cubic, struggle in tactical environments as they misinterpret packet loss and fluctuating network performance as congestion symptoms. Recent efforts, including our own MARLIN, have explored the use of Reinforcement Learning (RL) for CC, but they often fall short of generalization, particularly in competitive, unstable, and unforeseen scenarios. To address these challenges, this paper proposes an RL framework that leverages an accurate and parallelizable emulation environment to reenact the conditions of a tactical network. We also introduce refined RL formulation and performance evaluation methods tailored for agents operating in such intricate scenarios. We evaluate our RL learning framework by training a MARLIN agent in conditions replicating a bottleneck link transition between a Satellite Communication (SATCOM) and an UHF Wide Band (UHF) radio link. Finally, we compared its performance in file transfer tasks against Transm
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#24179;&#34913;&#26368;&#22823;&#21270;&#25237;&#36164;&#32452;&#21512;&#25910;&#20837;&#21644;&#26368;&#23567;&#21270;&#20934;&#22791;&#37329;&#30340;&#23545;&#25239;&#30446;&#26631;&#65292;&#33258;&#21160;&#21270;&#23547;&#25214;&#26368;&#20248;&#20449;&#29992;&#21345;&#39069;&#24230;&#35843;&#25972;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.15585</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#23545;&#25239;&#30446;&#26631;&#19979;&#30340;&#20449;&#29992;&#39069;&#24230;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Optimizing Credit Limit Adjustments Under Adversarial Goals Using Reinforcement Learning. (arXiv:2306.15585v1 [q-fin.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#24179;&#34913;&#26368;&#22823;&#21270;&#25237;&#36164;&#32452;&#21512;&#25910;&#20837;&#21644;&#26368;&#23567;&#21270;&#20934;&#22791;&#37329;&#30340;&#23545;&#25239;&#30446;&#26631;&#65292;&#33258;&#21160;&#21270;&#23547;&#25214;&#26368;&#20248;&#20449;&#29992;&#21345;&#39069;&#24230;&#35843;&#25972;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#24456;&#22810;&#38382;&#39064;&#20013;&#24471;&#21040;&#24212;&#29992;&#65292;&#20174;&#20855;&#26377;&#30830;&#23450;&#24615;&#29615;&#22659;&#30340;&#35270;&#39057;&#28216;&#25103;&#21040;&#20855;&#26377;&#38543;&#26426;&#22330;&#26223;&#30340;&#25237;&#36164;&#32452;&#21512;&#21644;&#36816;&#33829;&#31649;&#29702;&#65307;&#28982;&#32780;&#65292;&#22312;&#38134;&#34892;&#38382;&#39064;&#20013;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#27979;&#35797;&#23581;&#35797;&#24456;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#25214;&#21040;&#24182;&#33258;&#21160;&#21270;&#26368;&#20248;&#20449;&#29992;&#21345;&#39069;&#24230;&#35843;&#25972;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30001;&#20110;&#26377;&#21382;&#21490;&#25968;&#25454;&#21487;&#29992;&#65292;&#25105;&#20204;&#32771;&#34385;&#27599;&#20010;&#23458;&#25143;&#30340;&#20004;&#31181;&#21487;&#33021;&#25805;&#20316;&#65292;&#21363;&#22686;&#21152;&#25110;&#20445;&#25345;&#20010;&#20154;&#24403;&#21069;&#30340;&#20449;&#29992;&#39069;&#24230;&#12290;&#20026;&#20102;&#25214;&#21040;&#36825;&#20010;&#31574;&#30053;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#36825;&#20010;&#20915;&#31574;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;&#20854;&#20013;&#26368;&#22823;&#21270;&#39044;&#26399;&#21033;&#28070;&#65307;&#22240;&#27492;&#65292;&#25105;&#20204;&#24179;&#34913;&#20102;&#20004;&#20010;&#23545;&#25239;&#30446;&#26631;&#65306;&#26368;&#22823;&#21270;&#25237;&#36164;&#32452;&#21512;&#30340;&#25910;&#20837;&#21644;&#26368;&#23567;&#21270;&#25237;&#36164;&#32452;&#21512;&#30340;&#20934;&#22791;&#37329;&#12290;&#20854;&#27425;&#65292;&#32771;&#34385;&#21040;&#25105;&#20204;&#38382;&#39064;&#30340;&#29305;&#27530;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#31163;&#32447;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#27169;&#25311;&#34892;&#21160;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning has been explored for many problems, from video games with deterministic environments to portfolio and operations management in which scenarios are stochastic; however, there have been few attempts to test these methods in banking problems. In this study, we sought to find and automatize an optimal credit card limit adjustment policy by employing reinforcement learning techniques. In particular, because of the historical data available, we considered two possible actions per customer, namely increasing or maintaining an individual's current credit limit. To find this policy, we first formulated this decision-making question as an optimization problem in which the expected profit was maximized; therefore, we balanced two adversarial goals: maximizing the portfolio's revenue and minimizing the portfolio's provisions. Second, given the particularities of our problem, we used an offline learning strategy to simulate the impact of the action based on historical data f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#30697;&#38453;&#24352;&#37327;&#20056;&#31215;&#27169;&#22411;&#30340;&#36817;&#20284;&#20256;&#36882;&#28040;&#24687;&#65288;AMP&#65289;&#31639;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#24615;&#33021;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#26435;&#34913;&#21644;&#32452;&#21512;&#22810;&#20010;&#20272;&#35745;&#26469;&#20248;&#21270;&#31639;&#27861;&#36845;&#20195;&#36807;&#31243;&#12290;&#36890;&#36807;&#23545;&#38750;&#21487;&#20998;&#20989;&#25968;&#30340;AMP&#25910;&#25947;&#23450;&#29702;&#21644;&#29366;&#24577;&#28436;&#21270;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#24674;&#22797;&#30446;&#26631;&#20449;&#21495;&#25152;&#38656;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#22810;&#31181;&#31867;&#22411;&#30340;&#20004;&#20004;&#35266;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.15580</link><description>&lt;p&gt;
&#36817;&#20284;&#20256;&#36882;&#28040;&#24687;&#22312;&#30697;&#38453;&#24352;&#37327;&#20056;&#31215;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Approximate Message Passing for the Matrix Tensor Product Model. (arXiv:2306.15580v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#30697;&#38453;&#24352;&#37327;&#20056;&#31215;&#27169;&#22411;&#30340;&#36817;&#20284;&#20256;&#36882;&#28040;&#24687;&#65288;AMP&#65289;&#31639;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#24615;&#33021;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#26435;&#34913;&#21644;&#32452;&#21512;&#22810;&#20010;&#20272;&#35745;&#26469;&#20248;&#21270;&#31639;&#27861;&#36845;&#20195;&#36807;&#31243;&#12290;&#36890;&#36807;&#23545;&#38750;&#21487;&#20998;&#20989;&#25968;&#30340;AMP&#25910;&#25947;&#23450;&#29702;&#21644;&#29366;&#24577;&#28436;&#21270;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#24674;&#22797;&#30446;&#26631;&#20449;&#21495;&#25152;&#38656;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#22810;&#31181;&#31867;&#22411;&#30340;&#20004;&#20004;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#30697;&#38453;&#24352;&#37327;&#20056;&#31215;&#27169;&#22411;&#30340;&#36817;&#20284;&#20256;&#36882;&#28040;&#24687;&#65288;AMP&#65289;&#31639;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#35813;&#27169;&#22411;&#26159;&#26631;&#20934;&#23574;&#23792;&#30697;&#38453;&#27169;&#22411;&#30340;&#25512;&#24191;&#65292;&#20801;&#35768;&#23545;&#19968;&#32452;&#28508;&#22312;&#21464;&#37327;&#36827;&#34892;&#22810;&#31181;&#31867;&#22411;&#30340;&#20004;&#20004;&#35266;&#27979;&#12290;&#35813;&#31639;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#21019;&#26032;&#26159;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#36890;&#36807;&#19968;&#31181;&#26041;&#27861;&#23545;&#22810;&#20010;&#20272;&#35745;&#36827;&#34892;&#26368;&#20248;&#26435;&#37325;&#21644;&#32452;&#21512;&#12290;&#20511;&#21161;&#38750;&#21487;&#20998;&#20989;&#25968;&#30340;AMP&#25910;&#25947;&#23450;&#29702;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#38750;&#21487;&#20998;&#20989;&#25968;&#30340;&#29366;&#24577;&#28436;&#21270;&#65292;&#20174;&#32780;&#22312;&#39640;&#32500;&#26497;&#38480;&#19979;&#25552;&#20379;&#20102;&#20854;&#24615;&#33021;&#30340;&#28176;&#36817;&#31934;&#30830;&#25551;&#36848;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#29366;&#24577;&#28436;&#21270;&#32467;&#26524;&#32473;&#20986;&#20102;&#24674;&#22797;&#30446;&#26631;&#20449;&#21495;&#25152;&#38656;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#36825;&#20123;&#26465;&#20214;&#21462;&#20915;&#20110;&#20174;&#25105;&#20204;&#27169;&#22411;&#30340;&#36866;&#24403;&#25512;&#24191;&#20013;&#23548;&#20986;&#30340;&#32447;&#24615;&#31639;&#23376;&#30340;&#22855;&#24322;&#20540;&#65292;&#36825;&#20010;&#32447;&#24615;&#31639;&#23376;&#26159;&#19968;&#20010;&#36866;&#24403;&#25512;&#24191;&#30340;&#20449;&#22122;&#27604;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#28085;&#30422;&#20102;&#19968;&#20123;&#26368;&#36817;&#25552;&#20986;&#30340;&#32972;&#26223;&#27169;&#22411;&#26041;&#27861;&#65288;&#20363;&#22914;&#65292;covariance-&#65289;
&lt;/p&gt;
&lt;p&gt;
We propose and analyze an approximate message passing (AMP) algorithm for the matrix tensor product model, which is a generalization of the standard spiked matrix models that allows for multiple types of pairwise observations over a collection of latent variables. A key innovation for this algorithm is a method for optimally weighing and combining multiple estimates in each iteration. Building upon an AMP convergence theorem for non-separable functions, we prove a state evolution for non-separable functions that provides an asymptotically exact description of its performance in the high-dimensional limit. We leverage this state evolution result to provide necessary and sufficient conditions for recovery of the signal of interest. Such conditions depend on the singular values of a linear operator derived from an appropriate generalization of a signal-to-noise ratio for our model. Our results recover as special cases a number of recently proposed methods for contextual models (e.g., cova
&lt;/p&gt;</description></item><item><title>PyBADS&#26159;Python&#20013;&#19968;&#31181;&#24555;&#36895;&#32780;&#31283;&#20581;&#30340;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#35299;&#20915;&#30446;&#26631;&#20989;&#25968;&#31895;&#31961;&#12289;&#35745;&#31639;&#20195;&#20215;&#39640;&#12289;&#21487;&#33021;&#23384;&#22312;&#22122;&#22768;&#19988;&#26799;&#24230;&#20449;&#24687;&#19981;&#21487;&#29992;&#30340;&#22256;&#38590;&#20248;&#21270;&#38382;&#39064;&#12290;&#23427;&#25903;&#25345;&#39640;&#36798;20&#20010;&#36830;&#32493;&#36755;&#20837;&#21442;&#25968;&#30340;&#40657;&#30418;&#20989;&#25968;&#65292;&#24182;&#25552;&#20379;&#26131;&#20110;&#20351;&#29992;&#30340;Python&#25509;&#21475;&#12290;</title><link>http://arxiv.org/abs/2306.15576</link><description>&lt;p&gt;
PyBADS&#65306;Python&#20013;&#24555;&#36895;&#32780;&#31283;&#20581;&#30340;&#40657;&#30418;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
PyBADS: Fast and robust black-box optimization in Python. (arXiv:2306.15576v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15576
&lt;/p&gt;
&lt;p&gt;
PyBADS&#26159;Python&#20013;&#19968;&#31181;&#24555;&#36895;&#32780;&#31283;&#20581;&#30340;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#35299;&#20915;&#30446;&#26631;&#20989;&#25968;&#31895;&#31961;&#12289;&#35745;&#31639;&#20195;&#20215;&#39640;&#12289;&#21487;&#33021;&#23384;&#22312;&#22122;&#22768;&#19988;&#26799;&#24230;&#20449;&#24687;&#19981;&#21487;&#29992;&#30340;&#22256;&#38590;&#20248;&#21270;&#38382;&#39064;&#12290;&#23427;&#25903;&#25345;&#39640;&#36798;20&#20010;&#36830;&#32493;&#36755;&#20837;&#21442;&#25968;&#30340;&#40657;&#30418;&#20989;&#25968;&#65292;&#24182;&#25552;&#20379;&#26131;&#20110;&#20351;&#29992;&#30340;Python&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PyBADS&#26159;Bayesian Adaptive Direct Search&#65288;BADS&#65289;&#31639;&#27861;&#30340;Python&#23454;&#29616;&#65292;&#29992;&#20110;&#24555;&#36895;&#32780;&#31283;&#20581;&#30340;&#40657;&#30418;&#20248;&#21270;&#65288;Acerbi&#21644;Ma&#65292;2017&#65289;&#12290;BADS&#26159;&#19968;&#31181;&#38024;&#23545;&#30446;&#26631;&#20989;&#25968;&#31895;&#31961;&#65288;&#38750;&#20984;&#12289;&#38750;&#20809;&#28369;&#65289;&#12289;&#35745;&#31639;&#20195;&#20215;&#36739;&#39640;&#65288;&#20363;&#22914;&#20989;&#25968;&#35780;&#20272;&#38656;&#35201;&#36229;&#36807;0.1&#31186;&#65289;&#12289;&#21487;&#33021;&#23384;&#22312;&#22122;&#22768;&#19988;&#26799;&#24230;&#20449;&#24687;&#19981;&#21487;&#29992;&#30340;&#22256;&#38590;&#20248;&#21270;&#38382;&#39064;&#32780;&#35774;&#35745;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#36890;&#36807;BADS&#65292;&#36825;&#20123;&#38382;&#39064;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#35299;&#20915;&#65292;&#20351;&#20854;&#25104;&#20026;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#31561;&#26041;&#27861;&#25311;&#21512;&#35745;&#31639;&#27169;&#22411;&#30340;&#20248;&#31168;&#36873;&#25321;&#12290;&#35813;&#31639;&#27861;&#22312;&#20855;&#26377;&#39640;&#36798;$D \approx 20$&#20010;&#36830;&#32493;&#36755;&#20837;&#21442;&#25968;&#30340;&#40657;&#30418;&#20989;&#25968;&#19978;&#20855;&#26377;&#39640;&#25928;&#30340;&#25193;&#23637;&#24615;&#65292;&#24182;&#25903;&#25345;&#36793;&#30028;&#32422;&#26463;&#25110;&#26080;&#32422;&#26463;&#12290;PyBADS&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;Python&#25509;&#21475;&#65292;&#29992;&#20110;&#36816;&#34892;&#31639;&#27861;&#21644;&#26816;&#26597;&#20854;&#32467;&#26524;&#12290;PyBADS&#21482;&#38656;&#35201;&#29992;&#25143;&#25552;&#20379;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30446;&#26631;&#20989;&#25968;&#30340;Python&#20989;&#25968;&#65292;&#20197;&#21450;&#20854;&#20182;&#32422;&#26463;&#65288;&#21487;&#36873;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
PyBADS is a Python implementation of the Bayesian Adaptive Direct Search (BADS) algorithm for fast and robust black-box optimization (Acerbi and Ma 2017). BADS is an optimization algorithm designed to efficiently solve difficult optimization problems where the objective function is rough (non-convex, non-smooth), mildly expensive (e.g., the function evaluation requires more than 0.1 seconds), possibly noisy, and gradient information is unavailable. With BADS, these issues are well addressed, making it an excellent choice for fitting computational models using methods such as maximum-likelihood estimation. The algorithm scales efficiently to black-box functions with up to $D \approx 20$ continuous input parameters and supports bounds or no constraints. PyBADS comes along with an easy-to-use Pythonic interface for running the algorithm and inspecting its results. PyBADS only requires the user to provide a Python function for evaluating the target function, and optionally other constraint
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26377;&#25928;&#22788;&#29702;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#36974;&#25377;&#24773;&#20917;&#12290;&#36890;&#36807;&#36880;&#27493;&#24341;&#20837;&#36974;&#25377;&#65292;&#27169;&#22411;&#39318;&#20808;&#23398;&#20064;&#31616;&#21333;&#12289;&#21487;&#36776;&#21035;&#30340;&#27169;&#24335;&#65292;&#28982;&#21518;&#36880;&#28176;&#29702;&#35299;&#26356;&#22797;&#26434;&#30340;&#36974;&#25377;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.15574</link><description>&lt;p&gt;
&#36879;&#36807;&#36855;&#38654;&#30475;&#28165;&#26970;&#65306;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#37319;&#29992;&#28176;&#36827;&#36974;&#25377;&#30340;&#35838;&#31243;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
See Through the Fog: Curriculum Learning with Progressive Occlusion in Medical Imaging. (arXiv:2306.15574v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26377;&#25928;&#22788;&#29702;&#21307;&#23398;&#22270;&#20687;&#20013;&#30340;&#36974;&#25377;&#24773;&#20917;&#12290;&#36890;&#36807;&#36880;&#27493;&#24341;&#20837;&#36974;&#25377;&#65292;&#27169;&#22411;&#39318;&#20808;&#23398;&#20064;&#31616;&#21333;&#12289;&#21487;&#36776;&#21035;&#30340;&#27169;&#24335;&#65292;&#28982;&#21518;&#36880;&#28176;&#29702;&#35299;&#26356;&#22797;&#26434;&#30340;&#36974;&#25377;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#35299;&#37322;&#26041;&#38754;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20855;&#26377;&#37096;&#20998;&#25110;&#23436;&#20840;&#36974;&#25377;&#30340;&#22797;&#26434;&#22270;&#20687;&#19978;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#36825;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#38750;&#24120;&#24120;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#30340;&#20840;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26377;&#25928;&#22788;&#29702;&#36974;&#25377;&#21307;&#23398;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36880;&#27493;&#24341;&#20837;&#36974;&#25377;&#65292;&#20174;&#28165;&#26224;&#12289;&#26080;&#36974;&#25377;&#30340;&#22270;&#20687;&#24320;&#22987;&#65292;&#36880;&#28176;&#36807;&#28193;&#21040;&#36974;&#25377;&#31243;&#24230;&#19981;&#26029;&#22686;&#21152;&#30340;&#22270;&#20687;&#12290;&#36825;&#31181;&#26377;&#24207;&#30340;&#23398;&#20064;&#36807;&#31243;&#31867;&#20284;&#20110;&#20154;&#31867;&#23398;&#20064;&#30340;&#36807;&#31243;&#65292;&#20351;&#27169;&#22411;&#39318;&#20808;&#25484;&#25569;&#31616;&#21333;&#12289;&#21487;&#36776;&#21035;&#30340;&#27169;&#24335;&#65292;&#28982;&#21518;&#22312;&#27492;&#22522;&#30784;&#19978;&#36880;&#28176;&#29702;&#35299;&#26356;&#22797;&#26434;&#30340;&#36974;&#25377;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19977;&#31181;&#20840;&#26032;&#30340;&#36974;&#25377;&#29983;&#25104;&#26041;&#27861;&#65292;&#20998;&#21035;&#26159;Wasserstein&#35838;&#31243;&#23398;&#20064;&#65288;WCL&#65289;&#12289;&#20449;&#24687;&#33258;&#36866;&#24212;&#23398;&#20064;&#65288;IAL&#65289;&#21644;&#27979;&#22320;&#32447;&#35838;&#31243;&#23398;&#20064;&#65288;Geodesic Curriculum Learn&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning models have revolutionized medical image interpretation, offering substantial improvements in diagnostic accuracy. However, these models often struggle with challenging images where critical features are partially or fully occluded, which is a common scenario in clinical practice. In this paper, we propose a novel curriculum learning-based approach to train deep learning models to handle occluded medical images effectively. Our method progressively introduces occlusion, starting from clear, unobstructed images and gradually moving to images with increasing occlusion levels. This ordered learning process, akin to human learning, allows the model to first grasp simple, discernable patterns and subsequently build upon this knowledge to understand more complicated, occluded scenarios. Furthermore, we present three novel occlusion synthesis methods, namely Wasserstein Curriculum Learning (WCL), Information Adaptive Learning (IAL), and Geodesic Curriculum Learn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;Risch&#31639;&#27861;&#29983;&#25104;&#21021;&#31561;&#21487;&#31215;&#34920;&#36798;&#24335;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#24335;&#29983;&#25104;&#30340;&#25968;&#25454;&#21487;&#20197;&#20943;&#36731;&#26089;&#26399;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#19968;&#20123;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2306.15572</link><description>&lt;p&gt;
&#29983;&#25104;&#21021;&#31561;&#21487;&#31215;&#34920;&#36798;&#24335;
&lt;/p&gt;
&lt;p&gt;
Generating Elementary Integrable Expressions. (arXiv:2306.15572v1 [cs.SC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;Risch&#31639;&#27861;&#29983;&#25104;&#21021;&#31561;&#21487;&#31215;&#34920;&#36798;&#24335;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#24335;&#29983;&#25104;&#30340;&#25968;&#25454;&#21487;&#20197;&#20943;&#36731;&#26089;&#26399;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#19968;&#20123;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#22312;&#35745;&#31639;&#26426;&#20195;&#25968;&#39046;&#22495;&#30340;&#24212;&#29992;&#25968;&#37327;&#19981;&#26029;&#22686;&#38271;&#65292;&#21253;&#25324;&#22312;&#31526;&#21495;&#31215;&#20998;&#30340;&#37325;&#35201;&#23376;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#25165;&#33021;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#23427;&#20204;&#32780;&#35328;&#65292;&#30446;&#21069;&#23384;&#22312;&#30528;&#24456;&#23569;&#31526;&#21512;&#23454;&#38469;&#38656;&#27714;&#30340;&#22522;&#20934;&#12290;&#34429;&#28982;&#24050;&#32463;&#23384;&#22312;&#19968;&#20123;&#29983;&#25104;&#26032;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#19968;&#20123;&#32570;&#38519;&#65292;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#22312;&#20854;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20986;&#29616;&#20559;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;Risch&#31639;&#27861;&#36827;&#34892;&#31526;&#21495;&#31215;&#20998;&#65292;&#20174;&#32780;&#21019;&#24314;&#19968;&#20010;&#21021;&#31561;&#21487;&#31215;&#34920;&#36798;&#24335;&#30340;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20197;&#36825;&#31181;&#26041;&#24335;&#29983;&#25104;&#30340;&#25968;&#25454;&#21487;&#20197;&#20943;&#36731;&#26089;&#26399;&#26041;&#27861;&#20013;&#21457;&#29616;&#30340;&#19968;&#20123;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been an increasing number of applications of machine learning to the field of Computer Algebra in recent years, including to the prominent sub-field of Symbolic Integration. However, machine learning models require an abundance of data for them to be successful and there exist few benchmarks on the scale required. While methods to generate new data already exist, they are flawed in several ways which may lead to bias in machine learning models trained upon them. In this paper, we describe how to use the Risch Algorithm for symbolic integration to create a dataset of elementary integrable expressions. Further, we show that data generated this way alleviates some of the flaws found in earlier methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#24179;&#34913;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20248;&#21270;&#30456;&#20114;&#24433;&#21709;&#65292;&#20026;&#26410;&#26469;&#23433;&#20840;&#24212;&#29992;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.15567</link><description>&lt;p&gt;
&#19968;&#20010;&#19977;&#37325;&#32416;&#32467;&#65306;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
A Three-Way Knot: Privacy, Fairness, and Predictive Performance Dynamics. (arXiv:2306.15567v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#24179;&#34913;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20248;&#21270;&#30456;&#20114;&#24433;&#21709;&#65292;&#20026;&#26410;&#26469;&#23433;&#20840;&#24212;&#29992;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#36793;&#30028;&#36827;&#19968;&#27493;&#21521;&#20154;&#31867;&#20114;&#21160;&#39046;&#22495;&#25512;&#36827;&#65292;&#20851;&#20110;&#33258;&#21160;&#20915;&#31574;&#30340;&#22810;&#20010;&#38382;&#39064;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#20004;&#20010;&#38382;&#39064;&#26159;&#20844;&#24179;&#24615;&#21644;&#25968;&#25454;&#38544;&#31169;&#12290;&#19968;&#26041;&#38754;&#65292;&#24517;&#39035;&#30830;&#20445;&#33258;&#21160;&#20915;&#31574;&#27809;&#26377;&#23545;&#29305;&#23450;&#32676;&#20307;&#65292;&#23588;&#20854;&#26159;&#37027;&#20123;&#27809;&#26377;&#20445;&#25252;&#25110;&#34987;&#36793;&#32536;&#21270;&#30340;&#32676;&#20307;&#20135;&#29983;&#20559;&#35265;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24517;&#39035;&#30830;&#20445;&#20010;&#20154;&#20449;&#24687;&#30340;&#20351;&#29992;&#23436;&#20840;&#36981;&#23432;&#38544;&#31169;&#27861;&#35268;&#65292;&#24182;&#30830;&#20445;&#29992;&#25143;&#36523;&#20221;&#30340;&#23433;&#20840;&#12290;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#38750;&#24120;&#22797;&#26434;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#21487;&#33021;&#23545;&#31038;&#20250;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#20248;&#21270;&#21521;&#37327;&#20043;&#38388;&#30340;&#21160;&#24577;&#20851;&#31995;&#20173;&#28982;&#20102;&#35299;&#29978;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#19977;&#37325;&#24352;&#21147;&#20197;&#21450;&#27599;&#20010;&#21521;&#37327;&#30340;&#20248;&#21270;&#22914;&#20309;&#24433;&#21709;&#20854;&#20182;&#21521;&#37327;&#65292;&#26088;&#22312;&#20026;&#26410;&#26469;&#23433;&#20840;&#24212;&#29992;&#30340;&#21457;&#23637;&#25552;&#20379;&#20449;&#24687;&#12290;&#22312;&#32771;&#34385;&#21040;&#39044;&#27979;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#21487;&#20197;&#20849;&#21516;&#20248;&#21270;&#30340;&#35266;&#28857;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#21482;&#26159;&#37096;&#20998;&#23454;&#29616;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the frontier of machine learning applications moves further into human interaction, multiple concerns arise regarding automated decision-making. Two of the most critical issues are fairness and data privacy. On the one hand, one must guarantee that automated decisions are not biased against certain groups, especially those unprotected or marginalized. On the other hand, one must ensure that the use of personal information fully abides by privacy regulations and that user identities are kept safe. The balance between privacy, fairness, and predictive performance is complex. However, despite their potential societal impact, we still demonstrate a poor understanding of the dynamics between these optimization vectors. In this paper, we study this three-way tension and how the optimization of each vector impacts others, aiming to inform the future development of safe applications. In light of claims that predictive performance and fairness can be jointly optimized, we find this is only p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RansomAI&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#38544;&#34109;&#21152;&#23494;&#21202;&#32034;&#36719;&#20214;&#65292;&#23427;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#36866;&#24212;&#21152;&#23494;&#34892;&#20026;&#65292;&#26368;&#23567;&#21270;&#34987;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;&#21152;&#23494;&#26102;&#26368;&#22823;&#21270;&#25439;&#23475;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15559</link><description>&lt;p&gt;
RansomAI: &#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#38544;&#34109;&#21152;&#23494;&#21202;&#32034;&#36719;&#20214;
&lt;/p&gt;
&lt;p&gt;
RansomAI: AI-powered Ransomware for Stealthy Encryption. (arXiv:2306.15559v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RansomAI&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#38544;&#34109;&#21152;&#23494;&#21202;&#32034;&#36719;&#20214;&#65292;&#23427;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#36866;&#24212;&#21152;&#23494;&#34892;&#20026;&#65292;&#26368;&#23567;&#21270;&#34987;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;&#21152;&#23494;&#26102;&#26368;&#22823;&#21270;&#25439;&#23475;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#29190;&#21457;, &#21202;&#32034;&#36719;&#20214;&#65288;&#21253;&#25324;&#24694;&#24847;&#36719;&#20214;&#65289;&#23558;&#20250;&#36816;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#26234;&#33021;&#32780;&#21160;&#24577;&#22320;&#36866;&#24212;&#20854;&#21152;&#23494;&#34892;&#20026;&#20197;&#36991;&#20813;&#34987;&#26816;&#27979;&#12290;&#36825;&#23558;&#23548;&#33268;&#29616;&#26377;&#30340;&#32593;&#32476;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#21464;&#24471;&#26080;&#25928;&#21644;&#36807;&#26102;&#65292;&#20294;&#26159;&#25991;&#29486;&#20013;&#32570;&#20047;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21202;&#32034;&#36719;&#20214;&#26469;&#39564;&#35777;&#27492;&#35266;&#28857;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RansomAI&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#21202;&#32034;&#36719;&#20214;&#26679;&#26412;&#20013;&#65292;&#20351;&#20854;&#21152;&#23494;&#34892;&#20026;&#20855;&#26377;&#36866;&#24212;&#24615;&#65292;&#24182;&#20445;&#25345;&#38544;&#34109;&#24615;&#12290;RansomAI&#24341;&#20837;&#20102;&#19968;&#20010;&#26234;&#33021;&#20195;&#29702;&#65292;&#36890;&#36807;&#22870;&#21169;&#26426;&#21046;&#21644;&#25351;&#32441;&#26234;&#33021;&#26816;&#27979;&#31995;&#32479;&#26469;&#23398;&#20064;&#26368;&#20339;&#30340;&#21152;&#23494;&#31639;&#27861;&#12289;&#36895;&#29575;&#21644;&#25345;&#32493;&#26102;&#38388;&#65292;&#20197;&#38477;&#20302;&#34987;&#26816;&#27979;&#30340;&#27010;&#29575;&#21516;&#26102;&#26368;&#22823;&#21270;&#20854;&#25439;&#23475;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cybersecurity solutions have shown promising performance when detecting ransomware samples that use fixed algorithms and encryption rates. However, due to the current explosion of Artificial Intelligence (AI), sooner than later, ransomware (and malware in general) will incorporate AI techniques to intelligently and dynamically adapt its encryption behavior to be undetected. It might result in ineffective and obsolete cybersecurity solutions, but the literature lacks AI-powered ransomware to verify it. Thus, this work proposes RansomAI, a Reinforcement Learning-based framework that can be integrated into existing ransomware samples to adapt their encryption behavior and stay stealthy while encrypting files. RansomAI presents an agent that learns the best encryption algorithm, rate, and duration that minimizes its detection (using a reward mechanism and a fingerprinting intelligent detection system) while maximizing its damage function. The proposed framework was validated in a ransomwar
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#31639;&#27861;&#34917;&#20607;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#27969;&#24418;&#20013;&#25552;&#20379;&#29992;&#25143;&#21487;&#20197;&#37319;&#21462;&#30340;&#26041;&#21521;&#26469;&#25913;&#21464;&#20854;&#39044;&#27979;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#24615;&#12289;&#21487;&#35777;&#26126;&#30340;&#38544;&#31169;&#21644;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2306.15557</link><description>&lt;p&gt;
&#31616;&#21333;&#25104;&#21151;&#30340;&#27493;&#39588;&#65306;&#22522;&#20110;&#36317;&#31163;&#30340;&#31639;&#27861;&#34917;&#20607;&#30340;&#20844;&#29702;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Simple Steps to Success: Axiomatics of Distance-Based Algorithmic Recourse. (arXiv:2306.15557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15557
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#31639;&#27861;&#34917;&#20607;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#27969;&#24418;&#20013;&#25552;&#20379;&#29992;&#25143;&#21487;&#20197;&#37319;&#21462;&#30340;&#26041;&#21521;&#26469;&#25913;&#21464;&#20854;&#39044;&#27979;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#25928;&#24615;&#12289;&#21487;&#35777;&#26126;&#30340;&#38544;&#31169;&#21644;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#29992;&#20110;&#31639;&#27861;&#34917;&#20607;&#65292;&#25552;&#20379;&#32473;&#29992;&#25143;&#25913;&#21464;&#20854;&#39044;&#27979;&#32467;&#26524;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#29616;&#26377;&#30340;&#35745;&#31639;&#34917;&#20607;&#26041;&#27861;&#25214;&#21040;&#28385;&#36275;&#26576;&#20123;&#26399;&#26395;&#30340;&#28857;&#38598;&#65292;&#20363;&#22914;&#22312;&#22522;&#30784;&#22240;&#26524;&#22270;&#20013;&#30340;&#24178;&#39044;&#65292;&#25110;&#32773;&#26368;&#23567;&#21270;&#20195;&#20215;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#28385;&#36275;&#36825;&#20123;&#26631;&#20934;&#38656;&#35201;&#23545;&#22522;&#30784;&#27169;&#22411;&#32467;&#26500;&#26377;&#24191;&#27867;&#30340;&#20102;&#35299;&#65292;&#22312;&#20960;&#20010;&#39046;&#22495;&#20013;&#24448;&#24448;&#38656;&#35201;&#22823;&#37327;&#30340;&#19981;&#20999;&#23454;&#38469;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#35745;&#31639;&#39640;&#25928;&#30340;&#31639;&#27861;&#34917;&#20607;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#27969;&#24418;&#20013;&#25552;&#20379;&#29992;&#25143;&#21487;&#20197;&#37319;&#21462;&#30340;&#26041;&#21521;&#26469;&#25913;&#21464;&#20854;&#39044;&#27979;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#29702;&#21270;&#21512;&#29702;&#21270;&#30340;&#26041;&#27861;&#65292;Stepwise Explainable Paths (StEP)&#65292;&#29992;&#20110;&#35745;&#31639;&#22522;&#20110;&#26041;&#21521;&#30340;&#31639;&#27861;&#34917;&#20607;&#12290;&#25105;&#20204;&#23545;StEP&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#35777;&#21644;&#29702;&#35770;&#30740;&#31350;&#12290;StEP&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#38544;&#31169;&#21644;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#24182;&#22312;&#20960;&#20010;&#24050;&#24314;&#31435;&#30340;&#34917;&#20607;&#26041;&#27861;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel data-driven framework for algorithmic recourse that offers users interventions to change their predicted outcome. Existing approaches to compute recourse find a set of points that satisfy some desiderata -- e.g. an intervention in the underlying causal graph, or minimizing a cost function. Satisfying these criteria, however, requires extensive knowledge of the underlying model structure, often an unrealistic amount of information in several domains. We propose a data-driven, computationally efficient approach to computing algorithmic recourse. We do so by suggesting directions in the data manifold that users can take to change their predicted outcome. We present Stepwise Explainable Paths (StEP), an axiomatically justified framework to compute direction-based algorithmic recourse. We offer a thorough empirical and theoretical investigation of StEP. StEP offers provable privacy and robustness guarantees, and outperforms the state-of-the-art on several established reco
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#38754;&#21521;&#24322;&#26500;HPC&#24179;&#21488;&#30340;&#28145;&#24230;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#21253;&#25324;GPU&#12289;TPU&#12289;FPGA&#12289;ASIC&#12289;&#31070;&#32463;&#22788;&#29702;&#21333;&#20803;&#21644;RISC-V&#31561;&#65292;&#21516;&#26102;&#20063;&#28085;&#30422;&#20102;&#26032;&#20852;&#20869;&#23384;&#25216;&#26415;&#21644;&#35745;&#31639;&#33539;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.15552</link><description>&lt;p&gt;
&#38754;&#21521;&#24322;&#26500;HPC&#24179;&#21488;&#30340;&#28145;&#24230;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#22120;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms. (arXiv:2306.15552v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#38754;&#21521;&#24322;&#26500;HPC&#24179;&#21488;&#30340;&#28145;&#24230;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#21253;&#25324;GPU&#12289;TPU&#12289;FPGA&#12289;ASIC&#12289;&#31070;&#32463;&#22788;&#29702;&#21333;&#20803;&#21644;RISC-V&#31561;&#65292;&#21516;&#26102;&#20063;&#28085;&#30422;&#20102;&#26032;&#20852;&#20869;&#23384;&#25216;&#26415;&#21644;&#35745;&#31639;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#24212;&#29992;&#20013;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#35821;&#38899;&#35782;&#21035;&#20013;&#25104;&#20026;&#30828;&#20214;&#21152;&#36895;&#22120;&#26368;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#32508;&#36848;&#24635;&#32467;&#21644;&#20998;&#31867;&#20102;&#35774;&#35745;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#22120;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20197;&#28385;&#36275;HPC&#24212;&#29992;&#30340;&#24615;&#33021;&#35201;&#27714;&#12290;&#29305;&#21035;&#22320;&#65292;&#23427;&#24378;&#35843;&#20102;&#25903;&#25345;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#21253;&#25324;&#19981;&#20165;&#38480;&#20110;&#22522;&#20110;GPU&#21644;TPU&#30340;&#21152;&#36895;&#22120;&#65292;&#36824;&#21253;&#25324;&#22522;&#20110;FPGA&#21644;ASIC&#30340;&#29305;&#23450;&#35774;&#35745;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#12289;&#31070;&#32463;&#22788;&#29702;&#21333;&#20803;&#12289;&#22522;&#20110;&#24320;&#25918;&#30828;&#20214;RISC-V&#30340;&#21152;&#36895;&#22120;&#21644;&#21327;&#22788;&#29702;&#22120;&#12290;&#26412;&#32508;&#36848;&#36824;&#25551;&#36848;&#20102;&#22522;&#20110;&#26032;&#20852;&#20869;&#23384;&#25216;&#26415;&#21644;&#35745;&#31639;&#33539;&#24335;&#30340;&#21152;&#36895;&#22120;&#65292;&#20363;&#22914;3D&#22534;&#21472;&#22788;&#29702;&#22120;&#20869;&#23384;&#12289;&#38750;&#26131;&#22833;&#24615;&#23384;&#20648;&#22120;&#65288;&#20027;&#35201;&#26159;&#30005;&#38459;&#24335;&#38543;&#26426;&#23384;&#21462;&#23384;&#20648;&#22120;&#21644;&#30456;&#21464;&#23384;&#20648;&#22120;&#65289;&#23454;&#29616;&#20869;&#23384;&#35745;&#31639;&#65292;&#31070;&#32463;&#24418;&#24577;&#23398;&#22788;&#29702;&#21333;&#20803;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent trends in deep learning (DL) imposed hardware accelerators as the most viable solution for several classes of high-performance computing (HPC) applications such as image classification, computer vision, and speech recognition. This survey summarizes and classifies the most recent advances in designing DL accelerators suitable to reach the performance requirements of HPC applications. In particular, it highlights the most advanced approaches to support deep learning accelerations including not only GPU and TPU-based accelerators but also design-specific hardware accelerators such as FPGA-based and ASIC-based accelerators, Neural Processing Units, open hardware RISC-V-based accelerators and co-processors. The survey also describes accelerators based on emerging memory technologies and computing paradigms, such as 3D-stacked Processor-In-Memory, non-volatile memories (mainly, Resistive RAM and Phase Change Memories) to implement in-memory computing, Neuromorphic Processing Units, a
&lt;/p&gt;</description></item><item><title>CrunchGPT&#26159;&#19968;&#20010;&#22522;&#20110;ChatGPT&#30340;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#29992;&#25143;&#25552;&#31034;&#26469;&#21327;&#35843;&#25972;&#20010;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#23454;&#29616;&#26080;&#32541;&#38598;&#25104;&#25968;&#25454;&#21644;&#29289;&#29702;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;SciML&#22312;&#39044;&#22788;&#29702;&#12289;&#38382;&#39064;&#24314;&#27169;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#21518;&#22788;&#29702;&#21644;&#20998;&#26512;&#31561;&#26041;&#38754;&#30340;&#32791;&#26102;&#38382;&#39064;&#65292;&#25299;&#23637;&#20102;&#20854;&#24037;&#19994;&#24212;&#29992;&#21644;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15551</link><description>&lt;p&gt;
CrunchGPT&#65306;&#22522;&#20110;ChatGPT&#30340;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CrunchGPT: A chatGPT assisted framework for scientific machine learning. (arXiv:2306.15551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15551
&lt;/p&gt;
&lt;p&gt;
CrunchGPT&#26159;&#19968;&#20010;&#22522;&#20110;ChatGPT&#30340;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#29992;&#25143;&#25552;&#31034;&#26469;&#21327;&#35843;&#25972;&#20010;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#23454;&#29616;&#26080;&#32541;&#38598;&#25104;&#25968;&#25454;&#21644;&#29289;&#29702;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;SciML&#22312;&#39044;&#22788;&#29702;&#12289;&#38382;&#39064;&#24314;&#27169;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#21518;&#22788;&#29702;&#21644;&#20998;&#26512;&#31561;&#26041;&#38754;&#30340;&#32791;&#26102;&#38382;&#39064;&#65292;&#25299;&#23637;&#20102;&#20854;&#24037;&#19994;&#24212;&#29992;&#21644;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#36817;&#24180;&#26469;&#22312;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#30340;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#20854;&#30446;&#26631;&#26159;&#22312;&#19981;&#38656;&#35201;&#22797;&#26434;&#21644;&#35745;&#31639;&#23494;&#38598;&#30340;&#25968;&#25454;&#21516;&#21270;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#32541;&#22320;&#23558;&#25968;&#25454;&#21644;&#29289;&#29702;&#30693;&#35782;&#38598;&#25104;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#39044;&#22788;&#29702;&#12289;&#38382;&#39064;&#24314;&#27169;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#21518;&#22788;&#29702;&#21644;&#20998;&#26512;&#20173;&#28982;&#26159;&#32791;&#26102;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#38480;&#21046;SciML&#22312;&#24037;&#19994;&#24212;&#29992;&#21644;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;SciML&#30340;&#21508;&#20010;&#38454;&#27573;&#25972;&#21512;&#21040;ChatGPT&#30340;&#20254;&#19979;&#65292;&#24418;&#25104;CrunchGPT&#65292;&#23427;&#36890;&#36807;&#29992;&#25143;&#31616;&#21333;&#30340;&#25552;&#31034;&#26469;&#21327;&#35843;&#25972;&#20010;SciML&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#31034;&#20363;&#65292;&#28436;&#31034;&#20102;CrunchGPT&#22312;&#27668;&#21160;&#23398;&#20013;&#20248;&#21270;&#26426;&#32764;&#21644;&#22312;&#21508;&#31181;&#20960;&#20309;&#24418;&#29366;&#20013;&#33719;&#24471;&#27969;&#22330;&#30340;&#28508;&#22312;&#29992;&#36884;&#65292;&#24182;&#24378;&#35843;&#20102;&#39564;&#35777;&#38454;&#27573;&#12290;&#20026;&#20102;&#28436;&#31034;CrunchGPT&#30340;&#27969;&#31243;&#21644;
&lt;/p&gt;
&lt;p&gt;
Scientific Machine Learning (SciML) has advanced recently across many different areas in computational science and engineering. The objective is to integrate data and physics seamlessly without the need of employing elaborate and computationally taxing data assimilation schemes. However, preprocessing, problem formulation, code generation, postprocessing and analysis are still time consuming and may prevent SciML from wide applicability in industrial applications and in digital twin frameworks. Here, we integrate the various stages of SciML under the umbrella of ChatGPT, to formulate CrunchGPT, which plays the role of a conductor orchestrating the entire workflow of SciML based on simple prompts by the user. Specifically, we present two examples that demonstrate the potential use of CrunchGPT in optimizing airfoils in aerodynamics, and in obtaining flow fields in various geometries in interactive mode, with emphasis on the validation stage. To demonstrate the flow of the CrunchGPT, and
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#26694;&#26550;&#30340;&#36229;&#22768;&#23450;&#20301;&#26174;&#24494;&#38236;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20381;&#36182;&#21040;&#36798;&#26102;&#38388;&#24046;&#20449;&#24687;&#23454;&#29616;&#24494;&#27668;&#27873;&#30340;&#23450;&#20301;&#65292;&#24182;&#22312;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.15548</link><description>&lt;p&gt;
&#20960;&#20309;&#36229;&#22768;&#23450;&#20301;&#26174;&#24494;&#38236;
&lt;/p&gt;
&lt;p&gt;
Geometric Ultrasound Localization Microscopy. (arXiv:2306.15548v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15548
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#26694;&#26550;&#30340;&#36229;&#22768;&#23450;&#20301;&#26174;&#24494;&#38236;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20381;&#36182;&#21040;&#36798;&#26102;&#38388;&#24046;&#20449;&#24687;&#23454;&#29616;&#24494;&#27668;&#27873;&#30340;&#23450;&#20301;&#65292;&#24182;&#22312;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#22686;&#24378;&#36229;&#22768;&#65288;CEUS&#65289;&#24050;&#25104;&#20026;&#26080;&#21019;&#21160;&#24577;&#21487;&#35270;&#21270;&#21307;&#23398;&#35786;&#26029;&#26041;&#27861;&#65292;&#28982;&#32780;&#36229;&#22768;&#23450;&#20301;&#26174;&#24494;&#38236;&#65288;ULM&#65289;&#36890;&#36807;&#25552;&#20379;&#21313;&#20493;&#26356;&#39640;&#30340;&#20998;&#36776;&#29575;&#23454;&#29616;&#20102;&#31361;&#30772;&#24615;&#36827;&#23637;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#24310;&#36831;&#21644;&#27714;&#21644;&#65288;DAS&#65289;&#27874;&#26463;&#24418;&#25104;&#22120;&#34987;&#29992;&#20110;&#28210;&#26579;ULM&#24103;&#65292;&#26368;&#32456;&#30830;&#23450;&#22270;&#20687;&#30340;&#20998;&#36776;&#29575;&#33021;&#21147;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;ULM&#65292;&#26412;&#30740;&#31350;&#36136;&#30097;&#27874;&#26463;&#24418;&#25104;&#26159;&#21542;&#26159;ULM&#26368;&#26377;&#25928;&#30340;&#22788;&#29702;&#27493;&#39588;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20381;&#36182;&#21040;&#36798;&#26102;&#38388;&#24046;&#65288;TDoA&#65289;&#20449;&#24687;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36807;&#26925;&#22278;&#20132;&#28857;&#23450;&#20301;&#24494;&#27668;&#27873;&#30340;&#20960;&#20309;&#26694;&#26550;&#65292;&#20197;&#20811;&#26381;&#29616;&#26377;&#27874;&#26463;&#24418;&#25104;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#22522;&#20110;&#19968;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22522;&#20934;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#20960;&#20309;ULM&#22312;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#20165;&#21033;&#29992;&#20102;&#37096;&#20998;&#21487;&#29992;&#30340;&#25442;&#33021;&#22120;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrast-Enhanced Ultra-Sound (CEUS) has become a viable method for non-invasive, dynamic visualization in medical diagnostics, yet Ultrasound Localization Microscopy (ULM) has enabled a revolutionary breakthrough by offering ten times higher resolution. To date, Delay-And-Sum (DAS) beamformers are used to render ULM frames, ultimately determining the image resolution capability. To take full advantage of ULM, this study questions whether beamforming is the most effective processing step for ULM, suggesting an alternative approach that relies solely on Time-Difference-of-Arrival (TDoA) information. To this end, a novel geometric framework for micro bubble localization via ellipse intersections is proposed to overcome existing beamforming limitations. We present a benchmark comparison based on a public dataset for which our geometric ULM outperforms existing baseline methods in terms of accuracy and reliability while only utilizing a portion of the available transducer data.
&lt;/p&gt;</description></item><item><title>&#22522;&#30784;&#27169;&#22411;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#20132;&#21449;&#25552;&#20379;&#20102;&#35299;&#38145;&#26032;&#21487;&#33021;&#24615;&#30340;&#29420;&#29305;&#26426;&#20250;&#65292;&#25193;&#23637;&#20102;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#20419;&#36827;&#20102;&#21327;&#20316;&#24335;&#27169;&#22411;&#21457;&#23637;&#65292;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2306.15546</link><description>&lt;p&gt;
&#24403;&#22522;&#30784;&#27169;&#22411;&#36935;&#21040;&#32852;&#37030;&#23398;&#20064;&#65306;&#21160;&#26426;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions. (arXiv:2306.15546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15546
&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#20132;&#21449;&#25552;&#20379;&#20102;&#35299;&#38145;&#26032;&#21487;&#33021;&#24615;&#30340;&#29420;&#29305;&#26426;&#20250;&#65292;&#25193;&#23637;&#20102;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#20419;&#36827;&#20102;&#21327;&#20316;&#24335;&#27169;&#22411;&#21457;&#23637;&#65292;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65288;FM&#65289;&#19982;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#20132;&#21449;&#25552;&#20379;&#20102;&#30456;&#20114;&#30340;&#22909;&#22788;&#65292;&#22312;AI&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#35299;&#38145;&#26032;&#21487;&#33021;&#24615;&#30340;&#29420;&#29305;&#26426;&#20250;&#65292;&#35299;&#20915;&#20102;AI&#21644;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;FL&#25193;&#23637;&#20102;FM&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#35745;&#31639;&#20849;&#20139;&#65292;&#20998;&#25955;&#20102;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20943;&#36731;&#20102;FL&#21442;&#19982;&#32773;&#30340;&#36127;&#25285;&#12290;&#23427;&#20419;&#36827;&#20102;&#21327;&#20316;&#24335;FM&#21457;&#23637;&#65292;&#27665;&#20027;&#21270;&#20102;&#36825;&#19968;&#36807;&#31243;&#65292;&#20419;&#36827;&#20102;&#21253;&#23481;&#24615;&#21644;&#21019;&#26032;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;FM&#20197;&#20854;&#24222;&#22823;&#30340;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#30340;&#30693;&#35782;&#21644;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20026;FL&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#36215;&#28857;&#65292;&#20419;&#36827;&#20102;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19979;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;FM&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#20016;&#23500;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#20943;&#23569;&#36807;&#25311;&#21512;&#65292;&#20445;&#25252;&#38544;&#31169;&#12290;&#36890;&#36807;&#30740;&#31350;FL&#21644;FM&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26412;&#25991;&#26088;&#22312;&#21152;&#28145;&#23545;&#23427;&#20204;&#21327;&#21516;&#20851;&#31995;&#30340;&#29702;&#35299;&#65292;&#24378;&#35843;&#21160;&#26426;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The intersection of the Foundation Model (FM) and Federated Learning (FL) provides mutual benefits, presents a unique opportunity to unlock new possibilities in AI research, and address critical challenges in AI and real-world applications. FL expands the availability of data for FMs and enables computation sharing, distributing the training process and reducing the burden on FL participants. It promotes collaborative FM development, democratizing the process and fostering inclusivity and innovation. On the other hand, FM, with its enormous size, pre-trained knowledge, and exceptional performance, serves as a robust starting point for FL, facilitating faster convergence and better performance under non-iid data. Additionally, leveraging FM to generate synthetic data enriches data diversity, reduces overfitting, and preserves privacy. By examining the interplay between FL and FM, this paper aims to deepen the understanding of their synergistic relationship, highlighting the motivations,
&lt;/p&gt;</description></item><item><title>DataCI&#26159;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;&#65292;&#19987;&#20026;&#27969;&#25968;&#25454;&#20013;&#30340;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#32780;&#35774;&#35745;&#65292;&#25552;&#20379;&#20016;&#23500;&#30340;API&#21644;&#29256;&#26412;&#25511;&#21046;&#21151;&#33021;&#65292;&#20855;&#26377;&#26131;&#20110;&#20351;&#29992;&#21644;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#25913;&#21464;&#27969;&#25968;&#25454;&#32972;&#26223;&#19979;&#30340;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2306.15538</link><description>&lt;p&gt;
DataCI: &#19968;&#20010;&#29992;&#20110;&#27969;&#25968;&#25454;&#20013;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
DataCI: A Platform for Data-Centric AI on Streaming Data. (arXiv:2306.15538v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15538
&lt;/p&gt;
&lt;p&gt;
DataCI&#26159;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;&#65292;&#19987;&#20026;&#27969;&#25968;&#25454;&#20013;&#30340;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#32780;&#35774;&#35745;&#65292;&#25552;&#20379;&#20016;&#23500;&#30340;API&#21644;&#29256;&#26412;&#25511;&#21046;&#21151;&#33021;&#65292;&#20855;&#26377;&#26131;&#20110;&#20351;&#29992;&#21644;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#25913;&#21464;&#27969;&#25968;&#25454;&#32972;&#26223;&#19979;&#30340;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;DataCI&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#21160;&#24577;&#27969;&#25968;&#25454;&#22330;&#26223;&#20013;&#30340;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#32508;&#21512;&#24320;&#28304;&#24179;&#21488;&#12290;DataCI&#25552;&#20379;&#20102;&#22522;&#30784;&#35774;&#26045;&#65292;&#20855;&#26377;&#20016;&#23500;&#30340;API&#65292;&#29992;&#20110;&#26080;&#32541;&#27969;&#25968;&#25454;&#38598;&#31649;&#29702;&#12289;&#25968;&#25454;&#20013;&#24515;&#27969;&#31243;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#31934;&#24515;&#35774;&#35745;&#30340;&#29256;&#26412;&#25511;&#21046;&#21151;&#33021;&#65292;&#20197;&#36319;&#36394;&#27969;&#31243;&#30340;&#34893;&#29983;&#12290;&#21478;&#22806;&#65292;DataCI&#36824;&#25552;&#20379;&#20102;&#30452;&#35266;&#30340;&#22270;&#24418;&#30028;&#38754;&#65292;&#25552;&#20379;&#26356;&#22909;&#30340;&#20132;&#20114;&#20307;&#39564;&#12290;&#21021;&#27493;&#30740;&#31350;&#21644;&#28436;&#31034;&#35777;&#26126;&#20102;DataCI&#26131;&#20110;&#20351;&#29992;&#21644;&#26377;&#25928;&#24615;&#65292;&#20984;&#26174;&#20102;&#23427;&#22312;&#27969;&#25968;&#25454;&#32972;&#26223;&#19979;&#25913;&#21464;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#23454;&#36341;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce DataCI, a comprehensive open-source platform designed specifically for data-centric AI in dynamic streaming data settings. DataCI provides 1) an infrastructure with rich APIs for seamless streaming dataset management, data-centric pipeline development and evaluation on streaming scenarios, 2) an carefully designed versioning control function to track the pipeline lineage, and 3) an intuitive graphical interface for a better interactive user experience. Preliminary studies and demonstrations attest to the easy-to-use and effectiveness of DataCI, highlighting its potential to revolutionize the practice of data-centric AI in streaming data contexts.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#38454;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#32852;&#21512;&#20998;&#26512;&#30340;&#32929;&#31080;&#36873;&#32929;&#26041;&#27861;&#65288;H-GAT&#65289;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#28041;&#21450;&#36229;&#36807;&#20004;&#20010;&#33410;&#28857;&#30340;&#22797;&#26434;&#39640;&#38454;&#32467;&#26500;&#65292;&#24182;&#21516;&#26102;&#32467;&#21512;&#22522;&#26412;&#20998;&#26512;&#22240;&#32032;&#21644;&#25216;&#26415;&#20998;&#26512;&#22240;&#32032;&#65292;&#23545;&#32929;&#31080;&#36873;&#32929;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.15526</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#38454;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#32929;&#31080;&#36873;&#32929;&#26041;&#27861;&#21450;&#32852;&#21512;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Higher-order Graph Attention Network for Stock Selection with Joint Analysis. (arXiv:2306.15526v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15526
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#38454;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#32852;&#21512;&#20998;&#26512;&#30340;&#32929;&#31080;&#36873;&#32929;&#26041;&#27861;&#65288;H-GAT&#65289;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#28041;&#21450;&#36229;&#36807;&#20004;&#20010;&#33410;&#28857;&#30340;&#22797;&#26434;&#39640;&#38454;&#32467;&#26500;&#65292;&#24182;&#21516;&#26102;&#32467;&#21512;&#22522;&#26412;&#20998;&#26512;&#22240;&#32032;&#21644;&#25216;&#26415;&#20998;&#26512;&#22240;&#32032;&#65292;&#23545;&#32929;&#31080;&#36873;&#32929;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32929;&#31080;&#36873;&#32929;&#23545;&#20110;&#25237;&#36164;&#32773;&#26500;&#24314;&#26377;&#21033;&#21487;&#22270;&#30340;&#25237;&#36164;&#32452;&#21512;&#33267;&#20851;&#37325;&#35201;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30001;&#20110;&#20854;&#22312;&#20851;&#31995;&#24314;&#27169;&#21644;&#27867;&#21270;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#36234;&#26469;&#36234;&#21463;&#21040;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GNN&#26041;&#27861;&#20165;&#20851;&#27880;&#31616;&#21333;&#30340;&#32929;&#31080;&#37197;&#23545;&#20851;&#31995;&#65292;&#24182;&#19981;&#33021;&#25429;&#25417;&#28041;&#21450;&#36229;&#36807;&#20004;&#20010;&#33410;&#28857;&#30340;&#22797;&#26434;&#39640;&#38454;&#32467;&#26500;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#21482;&#32771;&#34385;&#25216;&#26415;&#20998;&#26512;&#22240;&#32032;&#65292;&#24573;&#35270;&#20102;&#21487;&#33021;&#23545;&#32929;&#31080;&#36235;&#21183;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#30340;&#22522;&#26412;&#20998;&#26512;&#22240;&#32032;&#12290;&#22522;&#20110;&#36825;&#20123;&#21160;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#38454;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#21644;&#32852;&#21512;&#20998;&#26512;&#30340;&#32929;&#31080;&#36873;&#32929;&#26041;&#27861;&#65288;H-GAT&#65289;&#12290;H-GAT&#33021;&#22815;&#25429;&#25417;&#39640;&#38454;&#32467;&#26500;&#65292;&#24182;&#21516;&#26102;&#32467;&#21512;&#22522;&#26412;&#20998;&#26512;&#22240;&#32032;&#21644;&#25216;&#26415;&#20998;&#26512;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;H-GAT&#30340;&#39034;&#24207;&#23618;&#23558;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#22240;&#32032;&#20316;&#20026;&#38271;&#30701;&#26399;&#35760;&#24518;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;H-GAT&#30340;&#20851;&#31995;&#23884;&#20837;&#23618;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#38454;&#22270;&#19988;&#23398;&#20064;&#20102;
&lt;/p&gt;
&lt;p&gt;
Stock selection is important for investors to construct profitable portfolios. Graph neural networks (GNNs) are increasingly attracting researchers for stock prediction due to their strong ability of relation modelling and generalisation. However, the existing GNN methods only focus on simple pairwise stock relation and do not capture complex higher-order structures modelling relations more than two nodes. In addition, they only consider factors of technical analysis and overlook factors of fundamental analysis that can affect the stock trend significantly. Motivated by them, we propose higher-order graph attention network with joint analysis (H-GAT). H-GAT is able to capture higher-order structures and jointly incorporate factors of fundamental analysis with factors of technical analysis. Specifically, the sequential layer of H-GAT take both types of factors as the input of a long-short term memory model. The relation embedding layer of H-GAT constructs a higher-order graph and learn 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#30340;&#22238;&#25918;&#35760;&#24518;&#26041;&#27861;&#65292;&#23558;&#25968;&#25454;&#37319;&#26679;&#30340;&#35270;&#35282;&#25193;&#23637;&#21040;&#36712;&#36857;&#20013;&#65292;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#26356;&#20840;&#38754;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#21453;&#21521;&#37319;&#26679;&#36712;&#36857;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#24182;&#21033;&#29992;&#21152;&#26435;&#35780;&#35770;&#30446;&#26631;&#36991;&#20813;&#37319;&#26679;&#26410;&#35265;&#36807;&#30340;&#21160;&#20316;&#12290;&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#36824;&#33021;&#26681;&#25454;&#19981;&#21516;&#30340;&#20248;&#20808;&#24230;&#25351;&#26631;&#20248;&#20808;&#37319;&#26679;&#25928;&#29575;&#26356;&#39640;&#30340;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2306.15503</link><description>&lt;p&gt;
&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#65306;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#30340;&#22238;&#25918;&#35760;&#24518;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prioritized Trajectory Replay: A Replay Memory for Data-driven Reinforcement Learning. (arXiv:2306.15503v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#30340;&#22238;&#25918;&#35760;&#24518;&#26041;&#27861;&#65292;&#23558;&#25968;&#25454;&#37319;&#26679;&#30340;&#35270;&#35282;&#25193;&#23637;&#21040;&#36712;&#36857;&#20013;&#65292;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#26356;&#20840;&#38754;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#21453;&#21521;&#37319;&#26679;&#36712;&#36857;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#24182;&#21033;&#29992;&#21152;&#26435;&#35780;&#35770;&#30446;&#26631;&#36991;&#20813;&#37319;&#26679;&#26410;&#35265;&#36807;&#30340;&#21160;&#20316;&#12290;&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#36824;&#33021;&#26681;&#25454;&#19981;&#21516;&#30340;&#20248;&#20808;&#24230;&#25351;&#26631;&#20248;&#20808;&#37319;&#26679;&#25928;&#29575;&#26356;&#39640;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#20063;&#31216;&#20026;&#31163;&#32447;RL&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#20855;&#26377;&#25552;&#21319;&#22312;&#32447;RL&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20294;&#31163;&#32447;RL&#20013;&#30340;&#25968;&#25454;&#37319;&#26679;&#25216;&#26415;&#30340;&#20316;&#29992;&#21364;&#34987;&#24573;&#35270;&#20102;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30452;&#25509;&#23558;&#37319;&#26679;&#25216;&#26415;&#24212;&#29992;&#20110;&#29366;&#24577;&#36716;&#25442;&#24182;&#19981;&#33021;&#22987;&#32456;&#25552;&#39640;&#31163;&#32447;RL&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#25216;&#26415;&#8212;&#8212;&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#65288;TR/PTR&#65289;&#65292;&#23427;&#23558;&#37319;&#26679;&#30340;&#35270;&#35282;&#25193;&#23637;&#21040;&#36712;&#36857;&#20013;&#65292;&#20197;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#26356;&#20840;&#38754;&#30340;&#20449;&#24687;&#12290;TR&#36890;&#36807;&#21453;&#21521;&#37319;&#26679;&#36712;&#36857;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#20248;&#21270;&#21518;&#32493;&#29366;&#24577;&#20449;&#24687;&#30340;&#20351;&#29992;&#12290;&#22312;TR&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21152;&#26435;&#35780;&#35770;&#30446;&#26631;&#65292;&#20197;&#36991;&#20813;&#22312;&#31163;&#32447;&#35757;&#32451;&#20013;&#37319;&#26679;&#26410;&#35265;&#36807;&#30340;&#21160;&#20316;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#65288;PTR&#65289;&#26469;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#36712;&#36857;&#37319;&#26679;&#65292;&#26681;&#25454;&#19981;&#21516;&#30340;&#36712;&#36857;&#20248;&#20808;&#24230;&#25351;&#26631;&#36827;&#34892;&#20248;&#20808;&#35774;&#32622;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
In recent years, data-driven reinforcement learning (RL), also known as offline RL, have gained significant attention. However, the role of data sampling techniques in offline RL has been overlooked despite its potential to enhance online RL performance. Recent research suggests applying sampling techniques directly to state-transitions does not consistently improve performance in offline RL. Therefore, in this study, we propose a memory technique, (Prioritized) Trajectory Replay (TR/PTR), which extends the sampling perspective to trajectories for more comprehensive information extraction from limited data. TR enhances learning efficiency by backward sampling of trajectories that optimizes the use of subsequent state information. Building on TR, we build the weighted critic target to avoid sampling unseen actions in offline training, and Prioritized Trajectory Replay (PTR) that enables more efficient trajectory sampling, prioritized by various trajectory priority metrics. We demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#21270;&#35770;&#35777;&#26694;&#26550;$xADG$&#65292;&#36890;&#36807;&#20351;&#29992;&#24067;&#23572;&#36923;&#36753;&#36816;&#31639;&#31526;&#21644;&#22810;&#20010;&#25903;&#25345;&#26469;&#26500;&#24314;&#31616;&#27905;&#19988;&#21487;&#29702;&#35299;&#30340;&#35770;&#35777;&#22270;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20998;&#31867;&#20219;&#21153;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.15500</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;&#20998;&#31867;&#20219;&#21153;&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#22411;&#32467;&#26500;&#21270;&#35770;&#35777;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A novel structured argumentation framework for improved explainability of classification tasks. (arXiv:2306.15500v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#21270;&#35770;&#35777;&#26694;&#26550;$xADG$&#65292;&#36890;&#36807;&#20351;&#29992;&#24067;&#23572;&#36923;&#36753;&#36816;&#31639;&#31526;&#21644;&#22810;&#20010;&#25903;&#25345;&#26469;&#26500;&#24314;&#31616;&#27905;&#19988;&#21487;&#29702;&#35299;&#30340;&#35770;&#35777;&#22270;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20998;&#31867;&#20219;&#21153;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#23637;&#35770;&#35777;&#20915;&#31574;&#22270;&#65288;$xADG$&#65289;&#30340;&#26032;&#22411;&#32467;&#26500;&#21270;&#35770;&#35777;&#26694;&#26550;&#12290;&#23427;&#26159;&#22522;&#20110;Dung&#30340;&#25277;&#35937;&#35770;&#35777;&#22270;&#26500;&#24314;&#30340;&#35770;&#35777;&#20915;&#31574;&#22270;&#30340;&#25299;&#23637;&#12290;$xADG$&#26694;&#26550;&#20801;&#35768;&#35770;&#35777;&#20351;&#29992;&#24067;&#23572;&#36923;&#36753;&#36816;&#31639;&#31526;&#21644;&#22810;&#20010;&#21069;&#25552;&#65288;&#25903;&#25345;&#65289;&#22312;&#20869;&#37096;&#32467;&#26500;&#20013;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#31616;&#27905;&#30340;&#35770;&#35777;&#22270;&#65292;&#20351;&#29992;&#25143;&#26356;&#23481;&#26131;&#29702;&#35299;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;$xADG$&#30340;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#35268;&#27169;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#22823;&#23567;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#24471;&#21040;&#30340;$xADG$&#20855;&#26377;&#24378;&#65288;&#24179;&#34913;&#30340;&#65289;&#20934;&#30830;&#24615;&#65292;&#36825;&#26159;&#36890;&#36807;&#36755;&#20837;&#20915;&#31574;&#26641;&#23454;&#29616;&#30340;&#65292;&#21516;&#26102;&#36824;&#20943;&#23569;&#20102;&#36798;&#21040;&#32467;&#35770;&#25152;&#38656;&#30340;&#24179;&#22343;&#25903;&#25345;&#25968;&#12290;&#32467;&#26524;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#21487;&#20197;&#26500;&#24314;&#20986;&#22312;&#39044;&#27979;&#33021;&#21147;&#21644;&#24635;&#20307;&#22823;&#23567;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26500;&#24314;$ADG$&#25216;&#26415;&#30340;&#21487;&#29702;&#35299;&#30340;$xADG$&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel framework for structured argumentation, named extend argumentative decision graph ($xADG$). It is an extension of argumentative decision graphs built upon Dung's abstract argumentation graphs. The $xADG$ framework allows for arguments to use boolean logic operators and multiple premises (supports) within their internal structure, resulting in more concise argumentation graphs that may be easier for users to understand. The study presents a methodology for construction of $xADGs$ and evaluates their size and predictive capacity for classification tasks of varying magnitudes. Resulting $xADGs$ achieved strong (balanced) accuracy, which was accomplished through an input decision tree, while also reducing the average number of supports needed to reach a conclusion. The results further indicated that it is possible to construct plausibly understandable $xADGs$ that outperform other techniques for building $ADGs$ in terms of predictive capacity and overall size. I
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;&#40065;&#26834;&#24615;&#20013;&#30340;&#21512;&#20316;&#19982;&#31454;&#20105;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#23545;&#25163;&#39044;&#31639;&#26469;&#36991;&#20813;&#29609;&#23478;&#20027;&#23548;&#30340;&#26032;&#26694;&#26550;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#30446;&#26631;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15482</link><description>&lt;p&gt;
&#21512;&#20316;&#36824;&#26159;&#31454;&#20105;&#65306;&#36890;&#36807;&#33258;&#36866;&#24212;&#39044;&#31639;&#36991;&#20813;&#22810;&#30446;&#26631;&#40065;&#26834;&#24615;&#20013;&#30340;&#29609;&#23478;&#20027;&#23548;
&lt;/p&gt;
&lt;p&gt;
Cooperation or Competition: Avoiding Player Domination for Multi-Target Robustness via Adaptive Budgets. (arXiv:2306.15482v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#30446;&#26631;&#40065;&#26834;&#24615;&#20013;&#30340;&#21512;&#20316;&#19982;&#31454;&#20105;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35843;&#25972;&#23545;&#25163;&#39044;&#31639;&#26469;&#36991;&#20813;&#29609;&#23478;&#20027;&#23548;&#30340;&#26032;&#26694;&#26550;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#30446;&#26631;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21462;&#24471;&#20102;&#20196;&#20154;&#38590;&#20197;&#32622;&#20449;&#30340;&#36827;&#23637;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#20197;&#32463;&#39564;&#21644;&#21487;&#35777;&#26126;&#30340;&#26041;&#24335;&#35757;&#32451;&#40065;&#26834;&#32593;&#32476;&#12290;&#20294;&#26159;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#21482;&#33021;&#38450;&#24481;&#19968;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#65292;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#38450;&#24481;&#22810;&#31181;&#25915;&#20987;&#26041;&#38754;&#26377;&#25152;&#36827;&#23637;&#12290;&#26412;&#25991;&#23558;&#22810;&#30446;&#26631;&#40065;&#26834;&#24615;&#38382;&#39064;&#35270;&#20026;&#19968;&#20010;&#21338;&#24328;&#36807;&#31243;&#65292;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#19981;&#21516;&#30340;&#29609;&#23478;&#65288;&#23545;&#25163;&#65289;&#36890;&#36807;&#21327;&#21830;&#22312;&#21442;&#25968;&#26356;&#26032;&#30340;&#26041;&#21521;&#19978;&#36798;&#25104;&#19968;&#33268;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;&#29609;&#23478;&#20027;&#23548;&#30340;&#29616;&#35937;&#65292;&#22312;&#36825;&#20010;&#21338;&#24328;&#20013;&#65292;&#29616;&#26377;&#30340;&#26368;&#22823;&#20540;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#65292;&#22914;MAX&#21644;MSD&#65292;&#26080;&#27861;&#25910;&#25947;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35843;&#25972;&#19981;&#21516;&#23545;&#25163;&#30340;&#39044;&#31639;&#26469;&#36991;&#20813;&#20219;&#20309;&#29609;&#23478;&#30340;&#20027;&#23548;&#12290;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;&#25552;&#20986;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#29616;&#26377;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#30446;&#26631;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite incredible advances, deep learning has been shown to be susceptible to adversarial attacks. Numerous approaches have been proposed to train robust networks both empirically and certifiably. However, most of them defend against only a single type of attack, while recent work takes steps forward in defending against multiple attacks. In this paper, to understand multi-target robustness, we view this problem as a bargaining game in which different players (adversaries) negotiate to reach an agreement on a joint direction of parameter updating. We identify a phenomenon named player domination in the bargaining game, namely that the existing max-based approaches, such as MAX and MSD, do not converge. Based on our theoretical analysis, we design a novel framework that adjusts the budgets of different adversaries to avoid any player dominance. Experiments on standard benchmarks show that employing the proposed framework to the existing approaches significantly advances multi-target ro
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#30340;&#25216;&#26415;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#22240;&#26524;&#22270;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#22240;&#26524;&#25512;&#29702;&#21644;&#22240;&#26524;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.15479</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Causal Inference via Predictive Coding. (arXiv:2306.15479v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15479
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#30340;&#25216;&#26415;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;&#22240;&#26524;&#22270;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#22240;&#26524;&#25512;&#29702;&#21644;&#22240;&#26524;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#29702;&#21644;&#22240;&#26524;&#25512;&#29702;&#26159;&#26234;&#33021;&#30340;&#22522;&#26412;&#36807;&#31243;&#12290;&#36125;&#21494;&#26031;&#25512;&#29702;&#27169;&#22411;&#35266;&#23519;&#65306;&#22914;&#26524;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#20010;&#30456;&#20851;&#21464;&#37327;x&#65292;&#25105;&#20204;&#33021;&#25512;&#26029;&#20986;&#20851;&#20110;y&#30340;&#20449;&#24687;&#21527;&#65311;&#22240;&#26524;&#25512;&#29702;&#27169;&#22411;&#24178;&#39044;&#65306;&#22914;&#26524;&#25105;&#20204;&#30452;&#25509;&#25913;&#21464;x&#65292;y&#20250;&#22914;&#20309;&#21464;&#21270;&#65311;&#39044;&#27979;&#32534;&#30721;&#26159;&#19968;&#31181;&#21463;&#31070;&#32463;&#31185;&#23398;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#20165;&#23616;&#37096;&#20449;&#24687;&#23545;&#36830;&#32493;&#29366;&#24577;&#21464;&#37327;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36229;&#36234;&#20102;&#36125;&#21494;&#26031;&#25512;&#29702;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23545;&#39044;&#27979;&#32534;&#30721;&#30340;&#25512;&#29702;&#36807;&#31243;&#36827;&#34892;&#31616;&#21333;&#26356;&#25913;&#65292;&#23454;&#29616;&#24050;&#30693;&#22240;&#26524;&#22270;&#30340;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#25512;&#29702;&#12290;&#28982;&#21518;&#25105;&#20204;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#39044;&#27979;&#32534;&#30721;&#25512;&#24191;&#21040;&#22240;&#26524;&#22270;&#26410;&#30693;&#19988;&#38656;&#35201;&#36890;&#36807;&#25968;&#25454;&#25512;&#26029;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#12290;&#32467;&#26524;&#26159;&#19968;&#31181;&#26032;&#39062;&#19988;&#30452;&#25509;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#22522;&#20110;&#39044;&#27979;&#32534;&#30721;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#22240;&#26524;&#25512;&#29702;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian and causal inference are fundamental processes for intelligence. Bayesian inference models observations: what can be inferred about y if we observe a related variable x? Causal inference models interventions: if we directly change x, how will y change? Predictive coding is a neuroscience-inspired method for performing Bayesian inference on continuous state variables using local information only. In this work, we go beyond Bayesian inference, and show how a simple change in the inference process of predictive coding enables interventional and counterfactual inference in scenarios where the causal graph is known. We then extend our results, and show how predictive coding can be generalized to cases where this graph is unknown, and has to be inferred from data, hence performing causal discovery. What results is a novel and straightforward technique that allows us to perform end-to-end causal inference on predictive-coding-based structural causal models, and demonstrate its utilit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#35268;&#27169;&#26080;&#30417;&#30563;&#38899;&#39057;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35270;&#39057;&#21040;&#35821;&#38899;&#21512;&#25104;&#12290;&#36890;&#36807;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#35270;&#39057;&#23545;&#24212;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#20016;&#23500;&#30340;&#20165;&#38899;&#39057;&#25968;&#25454;&#38598;&#36827;&#34892;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.15464</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#26080;&#30417;&#30563;&#38899;&#39057;&#39044;&#35757;&#32451;&#29992;&#20110;&#35270;&#39057;&#21040;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Large-scale unsupervised audio pre-training for video-to-speech synthesis. (arXiv:2306.15464v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#35268;&#27169;&#26080;&#30417;&#30563;&#38899;&#39057;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35270;&#39057;&#21040;&#35821;&#38899;&#21512;&#25104;&#12290;&#36890;&#36807;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#35270;&#39057;&#23545;&#24212;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#20016;&#23500;&#30340;&#20165;&#38899;&#39057;&#25968;&#25454;&#38598;&#36827;&#34892;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#21040;&#35821;&#38899;&#21512;&#25104;&#26159;&#20174;&#26080;&#22768;&#35270;&#39057;&#20013;&#37325;&#24314;&#35821;&#38899;&#20449;&#21495;&#30340;&#20219;&#21153;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#24050;&#24314;&#31435;&#30340;&#26041;&#27861;&#37117;&#37319;&#29992;&#20102;&#19968;&#20010;&#20004;&#27493;&#27861;&#65292;&#39318;&#20808;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#20013;&#38388;&#34920;&#31034;&#65292;&#22914;&#35889;&#22270;&#65292;&#28982;&#21518;&#20256;&#36882;&#32473;&#22768;&#30721;&#22120;&#29983;&#25104;&#21407;&#22987;&#38899;&#39057;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#24037;&#20316;&#19987;&#27880;&#20110;&#31471;&#21040;&#31471;&#21512;&#25104;&#65292;&#21363;&#21516;&#26102;&#29983;&#25104;&#21407;&#22987;&#38899;&#39057;&#21644;&#20219;&#20309;&#20013;&#38388;&#34920;&#31034;&#12290;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#37117;&#38656;&#35201;&#22312;&#20960;&#20046;&#23436;&#20840;&#26159;&#38899;&#39057;-&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21363;&#27599;&#20010;&#38899;&#39057;&#26679;&#26412;&#37117;&#26377;&#23545;&#24212;&#30340;&#35270;&#39057;&#26679;&#26412;&#12290;&#36825;&#25490;&#38500;&#20102;&#20351;&#29992;&#20016;&#23500;&#30340;&#20165;&#38899;&#39057;&#25968;&#25454;&#38598;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21487;&#33021;&#27809;&#26377;&#23545;&#24212;&#30340;&#35270;&#35273;&#27169;&#24577;&#65288;&#20363;&#22914;&#26377;&#22768;&#35835;&#29289;&#12289;&#24191;&#25773;&#25773;&#23458;&#12289;&#35821;&#38899;&#35782;&#21035;&#25968;&#25454;&#38598;&#31561;&#65289;&#65292;&#20197;&#21450;&#22810;&#24180;&#26469;&#30001;&#38899;&#39057;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#24320;&#21457;&#30340;&#20165;&#38899;&#39057;&#26550;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#36229;&#36807;3500&#23567;&#26102;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video-to-speech synthesis is the task of reconstructing the speech signal from a silent video of a speaker. Most established approaches to date involve a two-step process, whereby an intermediate representation from the video, such as a spectrogram, is extracted first and then passed to a vocoder to produce the raw audio. Some recent work has focused on end-to-end synthesis, whereby the generation of raw audio and any intermediate representations is performed jointly. All such approaches involve training on data from almost exclusively audio-visual datasets, i.e. every audio sample has a corresponding video sample. This precludes the use of abundant audio-only datasets which may not have a corresponding visual modality (e.g. audiobooks, radio podcasts, speech recognition datasets etc.), as well as audio-only architectures that have been developed by the audio machine learning community over the years. In this paper we propose to train encoder-decoder models on more than 3,500 hours of 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#23545;&#25239;&#29992;&#25143;&#26500;&#24314;&#30340;&#23545;&#25239;&#24615;&#36755;&#20837;&#26102;&#26159;&#21542;&#20173;&#33021;&#20445;&#25345;&#23545;&#40784;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#25915;&#20987;&#25163;&#27861;&#19981;&#36275;&#20197;&#21487;&#38752;&#25915;&#20987;&#23545;&#40784;&#25991;&#26412;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#34542;&#21147;&#26041;&#27861;&#25214;&#21040;&#20102;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2306.15447</link><description>&lt;p&gt;
&#23545;&#40784;&#30340;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#23545;&#25239;&#23545;&#40784;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are aligned neural networks adversarially aligned?. (arXiv:2306.15447v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15447
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#23545;&#25239;&#29992;&#25143;&#26500;&#24314;&#30340;&#23545;&#25239;&#24615;&#36755;&#20837;&#26102;&#26159;&#21542;&#20173;&#33021;&#20445;&#25345;&#23545;&#40784;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#25915;&#20987;&#25163;&#27861;&#19981;&#36275;&#20197;&#21487;&#38752;&#25915;&#20987;&#23545;&#40784;&#25991;&#26412;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#34542;&#21147;&#26041;&#27861;&#25214;&#21040;&#20102;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29616;&#22312;&#34987;&#35843;&#25972;&#20026;&#19982;&#20854;&#21019;&#24314;&#32773;&#30340;&#30446;&#26631;&#20445;&#25345;&#19968;&#33268;&#65292;&#21363;"&#26377;&#30410;&#19988;&#26080;&#23475;"&#12290;&#36825;&#20123;&#27169;&#22411;&#24212;&#35813;&#23545;&#29992;&#25143;&#30340;&#38382;&#39064;&#32473;&#20986;&#26377;&#30410;&#30340;&#22238;&#31572;&#65292;&#20294;&#25298;&#32477;&#22238;&#31572;&#21487;&#33021;&#20250;&#36896;&#25104;&#20260;&#23475;&#30340;&#35831;&#27714;&#12290;&#28982;&#32780;&#65292;&#23545;&#25239;&#29992;&#25143;&#21487;&#20197;&#26500;&#24314;&#32469;&#36807;&#23545;&#40784;&#23581;&#35797;&#30340;&#36755;&#20837;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19982;&#26500;&#36896;&#26368;&#22351;&#24773;&#20917;&#36755;&#20837;&#65288;&#23545;&#25239;&#24615;&#26679;&#26412;&#65289;&#30340;&#23545;&#25239;&#29992;&#25143;&#20132;&#20114;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#20445;&#25345;&#22810;&#22823;&#31243;&#24230;&#30340;&#23545;&#40784;&#12290;&#36825;&#20123;&#36755;&#20837;&#34987;&#35774;&#35745;&#25104;&#23548;&#33268;&#27169;&#22411;&#21457;&#20986;&#26412;&#24212;&#31105;&#27490;&#30340;&#26377;&#23475;&#20869;&#23481;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20248;&#21270;&#25915;&#20987;&#25163;&#27861;&#22312;&#21487;&#38752;&#25915;&#20987;&#23545;&#40784;&#25991;&#26412;&#27169;&#22411;&#26041;&#38754;&#30340;&#19981;&#36275;&#20043;&#22788;&#65306;&#21363;&#20351;&#22312;&#24403;&#21069;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25915;&#20987;&#22833;&#36133;&#26102;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#34542;&#21147;&#26041;&#27861;&#25214;&#21040;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;&#22240;&#27492;&#65292;&#24403;&#21069;&#25915;&#20987;&#30340;&#22833;&#36133;&#19981;&#24212;&#34987;&#35270;&#20026;&#23545;&#40784;&#25991;&#26412;&#27169;&#22411;&#22312;&#38754;&#23545;&#23545;&#25239;&#24615;&#36755;&#20837;&#26102;&#20173;&#28982;&#20445;&#25345;&#23545;&#40784;&#30340;&#35777;&#26126;&#12290;&#20294;&#26159;&#36817;&#26399;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36235;&#21183;&#26159;&#22810;&#27169;&#24577;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are now tuned to align with the goals of their creators, namely to be "helpful and harmless." These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs.  However the recent trend in large-scale ML models is multim
&lt;/p&gt;</description></item><item><title>&#26377;&#38480;&#20869;&#23384;&#36138;&#23146;&#25311;&#29275;&#39039;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26631;&#20934;&#25311;&#29275;&#39039;&#26041;&#27861;&#35745;&#31639;&#25104;&#26412;&#21644;&#20869;&#23384;&#38656;&#27714;&#36807;&#39640;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#26377;&#20855;&#26377;&#38750;&#28176;&#36827;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.15444</link><description>&lt;p&gt;
&#26377;&#38480;&#20869;&#23384;&#36138;&#23146;&#25311;&#29275;&#39039;&#26041;&#27861;&#19982;&#38750;&#28176;&#36827;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Limited-Memory Greedy Quasi-Newton Method with Non-asymptotic Superlinear Convergence Rate. (arXiv:2306.15444v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15444
&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#20869;&#23384;&#36138;&#23146;&#25311;&#29275;&#39039;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26631;&#20934;&#25311;&#29275;&#39039;&#26041;&#27861;&#35745;&#31639;&#25104;&#26412;&#21644;&#20869;&#23384;&#38656;&#27714;&#36807;&#39640;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#26377;&#20855;&#26377;&#38750;&#28176;&#36827;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#28176;&#36827;&#25910;&#25947;&#20998;&#26512;&#34920;&#26126;&#65292;&#25311;&#29275;&#39039;&#26041;&#27861;&#30340;&#26174;&#24335;&#36229;&#32447;&#24615;&#36895;&#29575;&#20026;O$((1/\sqrt{t})^t)$&#12290;&#28982;&#32780;&#65292;&#33719;&#24471;&#36825;&#19968;&#36895;&#29575;&#30340;&#26041;&#27861;&#23384;&#22312;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#32570;&#28857;&#65306;&#23427;&#20204;&#38656;&#35201;&#23384;&#20648;&#20808;&#21069;&#30340;&#40657;&#22622;&#36817;&#20284;&#30697;&#38453;&#65292;&#25110;&#32773;&#23384;&#20648;&#25152;&#26377;&#36807;&#21435;&#30340;&#26354;&#29575;&#20449;&#24687;&#20197;&#24418;&#25104;&#24403;&#21069;&#30340;&#40657;&#22622;&#36870;&#36817;&#20284;&#12290;&#26377;&#38480;&#20869;&#23384;&#30340;&#25311;&#29275;&#39039;&#26041;&#27861;&#65288;&#22914;&#33879;&#21517;&#30340;L-BFGS&#65289;&#36890;&#36807;&#21033;&#29992;&#26377;&#38480;&#31383;&#21475;&#30340;&#36807;&#21435;&#26354;&#29575;&#20449;&#24687;&#26469;&#26500;&#36896;&#40657;&#22622;&#36870;&#36817;&#20284;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#21644;&#23384;&#20648;&#38656;&#27714;&#20026;O$(\tau d)$&#65292;&#20854;&#20013;$\tau \le d$ &#26159;&#31383;&#21475;&#30340;&#22823;&#23567;&#65292;$d$ &#26159;&#38382;&#39064;&#30340;&#32500;&#25968;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#26631;&#20934;&#25311;&#29275;&#39039;&#26041;&#27861;&#30340;O$(d^2)$ &#35745;&#31639;&#25104;&#26412;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#27809;&#26377;&#32467;&#26524;&#34920;&#26126;&#26377;&#38480;&#20869;&#23384;&#25311;&#29275;&#39039;&#26041;&#27861;&#23384;&#22312;&#38750;&#28176;&#36827;&#36229;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-asymptotic convergence analysis of quasi-Newton methods has gained attention with a landmark result establishing an explicit superlinear rate of O$((1/\sqrt{t})^t)$. The methods that obtain this rate, however, exhibit a well-known drawback: they require the storage of the previous Hessian approximation matrix or instead storing all past curvature information to form the current Hessian inverse approximation. Limited-memory variants of quasi-Newton methods such as the celebrated L-BFGS alleviate this issue by leveraging a limited window of past curvature information to construct the Hessian inverse approximation. As a result, their per iteration complexity and storage requirement is O$(\tau d)$ where $\tau \le d$ is the size of the window and $d$ is the problem dimension reducing the O$(d^2)$ computational cost and memory requirement of standard quasi-Newton methods. However, to the best of our knowledge, there is no result showing a non-asymptotic superlinear convergence rate for a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23884;&#20837;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#22312;&#35774;&#22791;&#19978;&#23545;&#29992;&#25143;&#30340;&#31038;&#20132;&#29615;&#22659;&#21644;&#29087;&#24713;&#22320;&#28857;&#36827;&#34892;&#24314;&#27169;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#20013;&#24515;&#21270;&#26550;&#26500;&#22788;&#29702;&#19978;&#19979;&#25991;&#20449;&#24687;&#23384;&#22312;&#38544;&#31169;&#27844;&#38706;&#21644;&#32570;&#20047;&#20010;&#24615;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.15437</link><description>&lt;p&gt;
&#21033;&#29992;&#23884;&#20837;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#29992;&#25143;&#31038;&#20132;&#29615;&#22659;&#21644;&#29087;&#24713;&#22320;&#28857;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
On-device modeling of user's social context and familiar places from smartphone-embedded sensor data. (arXiv:2306.15437v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23884;&#20837;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#22312;&#35774;&#22791;&#19978;&#23545;&#29992;&#25143;&#30340;&#31038;&#20132;&#29615;&#22659;&#21644;&#29087;&#24713;&#22320;&#28857;&#36827;&#34892;&#24314;&#27169;&#30340;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#20013;&#24515;&#21270;&#26550;&#26500;&#22788;&#29702;&#19978;&#19979;&#25991;&#20449;&#24687;&#23384;&#22312;&#38544;&#31169;&#27844;&#38706;&#21644;&#32570;&#20047;&#20010;&#24615;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#35782;&#21035;&#23545;&#20110;&#33258;&#36866;&#24212;&#31227;&#21160;&#21644;&#26222;&#36866;&#35745;&#31639;&#33267;&#20851;&#37325;&#35201;&#12290;&#31227;&#21160;&#29615;&#22659;&#20013;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#20381;&#36182;&#20110;&#23545;&#19978;&#19979;&#25991;&#21464;&#21270;&#30340;&#21450;&#26102;&#21453;&#24212;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#20851;&#27880;&#22312;&#38598;&#20013;&#24335;&#26550;&#26500;&#19978;&#22788;&#29702;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#23384;&#22312;&#38544;&#31169;&#27844;&#38706;&#21644;&#32570;&#20047;&#20010;&#24615;&#21270;&#30340;&#39118;&#38505;&#12290;&#35774;&#22791;&#19978;&#30340;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#35782;&#21035;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#36235;&#21183;&#65292;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#31038;&#20132;&#20114;&#21160;&#21644;&#35775;&#38382;&#20301;&#32622;&#22312;&#25551;&#36848;&#26085;&#24120;&#29983;&#27963;&#22330;&#26223;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#12289;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#65292;&#30452;&#25509;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#24314;&#27169;&#29992;&#25143;&#30340;&#31038;&#20132;&#29615;&#22659;&#21644;&#20301;&#32622;&#12290;&#21033;&#29992;ego-network&#27169;&#22411;&#65292;&#31995;&#32479;&#20174;&#25163;&#26426;&#23884;&#20837;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#25552;&#21462;&#39640;&#32423;&#12289;&#35821;&#20041;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#12290;&#23545;&#20110;&#31038;&#20132;&#29615;&#22659;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29992;&#25143;&#21644;&#35774;&#22791;&#20043;&#38388;&#30340;&#29289;&#29702;&#21644;&#32593;&#32476;&#31038;&#20132;&#20114;&#21160;&#25968;&#25454;&#12290;&#22312;&#20301;&#32622;&#26041;&#38754;&#65292;&#23427;&#20248;&#20808;&#32771;&#34385;&#24314;&#27169;&#29087;&#24713;&#30340;&#22320;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context modeling and recognition are crucial for adaptive mobile and ubiquitous computing. Context-awareness in mobile environments relies on prompt reactions to context changes. However, current solutions focus on limited context information processed on centralized architectures, risking privacy leakage and lacking personalization. On-device context modeling and recognition are emerging research trends, addressing these concerns. Social interactions and visited locations play significant roles in characterizing daily life scenarios. This paper proposes an unsupervised and lightweight approach to model the user's social context and locations directly on the mobile device. Leveraging the ego-network model, the system extracts high-level, semantic-rich context features from smartphone-embedded sensor data. For the social context, the approach utilizes data on physical and cyber social interactions among users and their devices. Regarding location, it prioritizes modeling the familiarity
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20811;&#26381;&#22270;&#23398;&#20064;&#35774;&#32622;&#30340;&#38480;&#21046;&#65292;&#24341;&#20837;&#21487;&#23398;&#20064;&#22270;&#25193;&#25955;&#30340;&#28789;&#27963;GNNs&#20197;&#21450;&#38024;&#23545;&#32467;&#26500;&#25200;&#21160;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#23545;&#25239;&#35757;&#32451;&#26159;&#38024;&#23545;&#23545;&#25239;&#32467;&#26500;&#25200;&#21160;&#30340;&#26368;&#20808;&#36827;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.15427</link><description>&lt;p&gt;
&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Adversarial Training for Graph Neural Networks. (arXiv:2306.15427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15427
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20811;&#26381;&#22270;&#23398;&#20064;&#35774;&#32622;&#30340;&#38480;&#21046;&#65292;&#24341;&#20837;&#21487;&#23398;&#20064;&#22270;&#25193;&#25955;&#30340;&#28789;&#27963;GNNs&#20197;&#21450;&#38024;&#23545;&#32467;&#26500;&#25200;&#21160;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#23545;&#25239;&#35757;&#32451;&#26159;&#38024;&#23545;&#23545;&#25239;&#32467;&#26500;&#25200;&#21160;&#30340;&#26368;&#20808;&#36827;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22270;&#20687;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23545;&#25239;&#35757;&#32451;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#23545;&#25239;&#22270;&#32467;&#26500;&#25200;&#21160;&#26041;&#38754;&#24182;&#27809;&#26377;&#26126;&#26174;&#25928;&#26524;&#12290;&#22312;&#20462;&#22797;&#23545;&#25239;&#35757;&#32451;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#20811;&#26381;&#20102;&#20043;&#21069;&#24037;&#20316;&#20013;&#37319;&#29992;&#30340;&#22270;&#23398;&#20064;&#35774;&#32622;&#30340;&#22522;&#26412;&#29702;&#35770;&#21644;&#23454;&#38469;&#38480;&#21046;&#65307;&#25105;&#20204;&#25581;&#31034;&#20102;&#22522;&#20110;&#21487;&#23398;&#20064;&#22270;&#25193;&#25955;&#30340;&#26356;&#28789;&#27963; GNNs &#33021;&#22815;&#36866;&#24212;&#23545;&#25239;&#25200;&#21160;&#65292;&#21516;&#26102;&#23398;&#20064;&#21040;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#20855;&#26377;&#33258;&#28982;&#30340;&#21487;&#35299;&#37322;&#24615;&#65307;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31532;&#19968;&#31181;&#38024;&#23545;&#32467;&#26500;&#25200;&#21160;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#23545;&#22810;&#20010;&#33410;&#28857;&#36827;&#34892;&#25915;&#20987;&#65292;&#24182;&#33021;&#22788;&#29702;&#20840;&#23616;&#65288;&#22270;&#32423;&#21035;&#65289;&#21644;&#23616;&#37096;&#65288;&#33410;&#28857;&#32423;&#21035;&#65289;&#30340;&#32422;&#26463;&#12290;&#36890;&#36807;&#36825;&#20123;&#36129;&#29486;&#65292;&#25105;&#20204;&#35777;&#26126;&#23545;&#25239;&#35757;&#32451;&#26159;&#23545;&#25239;&#32467;&#26500;&#25200;&#21160;&#30340;&#26368;&#20808;&#36827;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its success in the image domain, adversarial training does not (yet) stand out as an effective defense for Graph Neural Networks (GNNs) against graph structure perturbations. In the pursuit of fixing adversarial training (1) we show and overcome fundamental theoretical as well as practical limitations of the adopted graph learning setting in prior work; (2) we reveal that more flexible GNNs based on learnable graph diffusion are able to adjust to adversarial perturbations, while the learned message passing scheme is naturally interpretable; (3) we introduce the first attack for structure perturbations that, while targeting multiple nodes at once, is capable of handling global (graph-level) as well as local (node-level) constraints. Including these contributions, we demonstrate that adversarial training is a state-of-the-art defense against adversarial structure perturbations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25299;&#25169;&#35282;&#24230;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#23433;&#20840;&#24615;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#32986;&#24615;&#36136;&#21644;&#24320;&#26144;&#23556;&#24615;&#36136;&#24314;&#31435;&#20102;&#36755;&#20837;&#38598;&#21644;&#36755;&#20986;&#38598;&#20043;&#38388;&#30340;&#20005;&#26684;&#20445;&#35777;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#23433;&#20840;&#39564;&#35777;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.15403</link><description>&lt;p&gt;
&#20174;&#25299;&#25169;&#35282;&#24230;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Verifying Safety of Neural Networks from Topological Perspectives. (arXiv:2306.15403v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25299;&#25169;&#35282;&#24230;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#23433;&#20840;&#24615;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#32986;&#24615;&#36136;&#21644;&#24320;&#26144;&#23556;&#24615;&#36136;&#24314;&#31435;&#20102;&#36755;&#20837;&#38598;&#21644;&#36755;&#20986;&#38598;&#20043;&#38388;&#30340;&#20005;&#26684;&#20445;&#35777;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#23433;&#20840;&#39564;&#35777;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#65292;&#28982;&#32780;&#23427;&#20204;&#26131;&#21463;&#25439;&#24182;&#19988;&#24120;&#24120;&#34920;&#29616;&#19981;&#33391;&#12290;&#22240;&#27492;&#65292;&#22312;&#23454;&#38469;&#37096;&#32626;&#20043;&#21069;&#65292;&#23427;&#20204;&#30340;&#34892;&#20026;&#24212;&#35813;&#32463;&#36807;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25299;&#25169;&#35282;&#24230;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#23433;&#20840;&#39564;&#35777;&#38382;&#39064;&#30340;&#38598;&#36793;&#30028;&#21487;&#36798;&#24615;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#20010;&#20855;&#26377;&#36755;&#20837;&#38598;&#21644;&#23433;&#20840;&#38598;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23433;&#20840;&#39564;&#35777;&#38382;&#39064;&#26159;&#30830;&#23450;&#25152;&#26377;&#26469;&#33258;&#36755;&#20837;&#38598;&#30340;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#26159;&#21542;&#33853;&#22312;&#23433;&#20840;&#38598;&#20013;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#20027;&#35201;&#21033;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#32986;&#24615;&#36136;&#21644;&#24320;&#26144;&#23556;&#24615;&#36136;&#65292;&#36825;&#20123;&#24615;&#36136;&#22312;&#36755;&#20837;&#38598;&#30340;&#36793;&#30028;&#21644;&#36755;&#20986;&#38598;&#30340;&#36793;&#30028;&#20043;&#38388;&#24314;&#31435;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#21033;&#29992;&#36825;&#20004;&#20010;&#24615;&#36136;&#21487;&#20197;&#36890;&#36807;&#25552;&#21462;&#36755;&#20837;&#38598;&#30340;&#23376;&#38598;&#32780;&#19981;&#26159;&#25972;&#20010;&#36755;&#20837;&#38598;&#26469;&#36827;&#34892;&#21487;&#36798;&#24615;&#35745;&#31639;&#65292;&#20174;&#32780;&#25511;&#21046;&#21487;&#36798;&#24615;&#20998;&#26512;&#20013;&#30340;&#21253;&#35065;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks (NNs) are increasingly applied in safety-critical systems such as autonomous vehicles. However, they are fragile and are often ill-behaved. Consequently, their behaviors should undergo rigorous guarantees before deployment in practice. In this paper, we propose a set-boundary reachability method to investigate the safety verification problem of NNs from a topological perspective. Given an NN with an input set and a safe set, the safety verification problem is to determine whether all outputs of the NN resulting from the input set fall within the safe set. In our method, the homeomorphism property and the open map property of NNs are mainly exploited, which establish rigorous guarantees between the boundaries of the input set and the boundaries of the output set. The exploitation of these two properties facilitates reachability computations via extracting subsets of the input set rather than the entire input set, thus controlling the wrapping effect in reachability analy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#30456;&#23545;&#20301;&#32622;&#23884;&#20837;&#21644;&#35757;&#32451;&#38598;&#24341;&#23548;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#25972;&#25968;&#31639;&#26415;&#21644;&#25512;&#24191;&#21040;&#26356;&#38271;&#24207;&#21015;&#19978;&#30340;&#24615;&#33021;&#65292;&#22312;&#31616;&#21333;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65307;&#21516;&#26102;&#23637;&#31034;&#20102;&#22312;&#20056;&#27861;&#20219;&#21153;&#19978;&#24341;&#23548;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#24341;&#23548;&#26041;&#27861;&#22312;&#20854;&#20182;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.15400</link><description>&lt;p&gt;
&#31639;&#26415;Transformer&#20013;&#30340;&#38271;&#24230;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
Length Generalization in Arithmetic Transformers. (arXiv:2306.15400v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#30456;&#23545;&#20301;&#32622;&#23884;&#20837;&#21644;&#35757;&#32451;&#38598;&#24341;&#23548;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#22312;&#23398;&#20064;&#25972;&#25968;&#31639;&#26415;&#21644;&#25512;&#24191;&#21040;&#26356;&#38271;&#24207;&#21015;&#19978;&#30340;&#24615;&#33021;&#65292;&#22312;&#31616;&#21333;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65307;&#21516;&#26102;&#23637;&#31034;&#20102;&#22312;&#20056;&#27861;&#20219;&#21153;&#19978;&#24341;&#23548;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#24341;&#23548;&#26041;&#27861;&#22312;&#20854;&#20182;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;Transformer&#22312;&#20004;&#20010;&#25361;&#25112;&#20013;&#30340;&#34920;&#29616;&#65306;&#23398;&#20064;&#22522;&#26412;&#25972;&#25968;&#36816;&#31639;&#21644;&#25512;&#24191;&#21040;&#35757;&#32451;&#20013;&#26410;&#35265;&#36807;&#30340;&#26356;&#38271;&#24207;&#21015;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30456;&#23545;&#20301;&#32622;&#23884;&#20837;&#21487;&#20197;&#20351;&#31616;&#21333;&#20219;&#21153;&#65288;&#22914;&#21152;&#27861;&#65289;&#30340;&#38271;&#24230;&#25512;&#24191;&#65306;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;5&#20301;&#25968;&#30340;&#27169;&#22411;&#21487;&#20197;&#25191;&#34892;15&#20301;&#25968;&#30340;&#27714;&#21644;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20056;&#27861;&#19978;&#22833;&#25928;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#38598;&#24341;&#23548;&#65306;&#23558;&#20960;&#20010;&#65288;10&#21040;50&#20010;&#65289;&#38271;&#24207;&#21015;&#28155;&#21152;&#21040;&#35757;&#32451;&#38598;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24341;&#23548;&#21487;&#20197;&#20351;&#35757;&#32451;&#22312;5&#20301;&#25968;&#20056;&#20197;3&#20301;&#25968;&#30340;&#27169;&#22411;&#25512;&#24191;&#21040;35&#20056;&#20197;3&#30340;&#20363;&#23376;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#27169;&#22411;&#21487;&#20197;&#38024;&#23545;&#19981;&#21516;&#30340;&#25512;&#24191;&#38271;&#24230;&#36827;&#34892;&#24341;&#23548;&#65292;&#32780;&#24341;&#23548;&#26679;&#26412;&#37327;&#30340;&#32553;&#25918;&#19982;&#35757;&#32451;&#38598;&#22823;&#23567;&#30340;&#23545;&#25968;&#25104;&#27604;&#20363;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24341;&#23548;&#22312;&#31639;&#26415;&#20043;&#22806;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine how transformers cope with two challenges: learning basic integer arithmetic, and generalizing to longer sequences than seen during training. We find that relative position embeddings enable length generalization for simple tasks, such as addition: models trained on $5$-digit numbers can perform $15$-digit sums. However, this method fails for multiplication, and we propose train set priming: adding a few ($10$ to $50$) long sequences to the training set. We show that priming allows models trained on $5$-digit $\times$ $3$-digit multiplications to generalize to $35\times 3$ examples. We also show that models can be primed for different generalization lengths, and that the priming sample size scales as the logarithm of the training set size. Finally, we discuss potential applications of priming beyond arithmetic.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#20219;&#21153;&#20013;&#25968;&#25454;&#38598;&#36136;&#37327;&#35780;&#20272;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#36890;&#36807;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#25968;&#25454;&#38598;&#36136;&#37327;&#23545;&#27169;&#22411;&#35757;&#32451;&#21644;&#24615;&#33021;&#30340;&#28145;&#36828;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#36136;&#37327;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2306.15392</link><description>&lt;p&gt;
&#36890;&#36807;&#20915;&#31574;&#26641;&#29305;&#24449;&#22312;&#33258;&#21160;&#32534;&#30721;&#22120;&#22788;&#29702;&#31354;&#38388;&#20013;&#35780;&#20272;&#25968;&#25454;&#38598;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Assessing Dataset Quality Through Decision Tree Characteristics in Autoencoder-Processed Spaces. (arXiv:2306.15392v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#20219;&#21153;&#20013;&#25968;&#25454;&#38598;&#36136;&#37327;&#35780;&#20272;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#36890;&#36807;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#25968;&#25454;&#38598;&#36136;&#37327;&#23545;&#27169;&#22411;&#35757;&#32451;&#21644;&#24615;&#33021;&#30340;&#28145;&#36828;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#36136;&#37327;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#20219;&#21153;&#20013;&#25968;&#25454;&#38598;&#36136;&#37327;&#35780;&#20272;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#20511;&#21161;&#20061;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#32463;&#36807;&#20998;&#31867;&#20219;&#21153;&#38656;&#27714;&#30340;&#31934;&#24515;&#35774;&#35745;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#25454;&#38598;&#36136;&#37327;&#23545;&#27169;&#22411;&#35757;&#32451;&#21644;&#24615;&#33021;&#30340;&#28145;&#36828;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20004;&#20010;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#29992;&#20110;&#34920;&#31034;&#20855;&#20307;&#30340;&#25968;&#25454;&#26465;&#20214; - &#19968;&#20010;&#26368;&#22823;&#21270;&#29109;&#65292;&#21478;&#19968;&#20010;&#23637;&#31034;&#39640;&#20887;&#20313;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#36866;&#24403;&#30340;&#29305;&#24449;&#36873;&#25321;&#12289;&#20805;&#36275;&#30340;&#25968;&#25454;&#37327;&#21644;&#25968;&#25454;&#36136;&#37327;&#22312;&#23454;&#29616;&#39640;&#24615;&#33021;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#36136;&#37327;&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#35780;&#20272;&#25163;&#22836;&#30340;&#25968;&#25454;&#38598;&#26159;&#21542;&#36275;&#22815;&#19988;&#20855;&#22791;&#29305;&#23450;&#20219;&#21153;&#25152;&#38656;&#30340;&#36136;&#37327;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#25968;&#25454;&#35780;&#20272;&#23454;&#36341;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#65292;&#24182;&#26377;&#21161;&#20110;&#24320;&#21457;&#26356;&#20934;&#30830;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we delve into the critical aspect of dataset quality assessment in machine learning classification tasks. Leveraging a variety of nine distinct datasets, each crafted for classification tasks with varying complexity levels, we illustrate the profound impact of dataset quality on model training and performance. We further introduce two additional datasets designed to represent specific data conditions - one maximizing entropy and the other demonstrating high redundancy. Our findings underscore the importance of appropriate feature selection, adequate data volume, and data quality in achieving high-performing machine learning models. To aid researchers and practitioners, we propose a comprehensive framework for dataset quality assessment, which can help evaluate if the dataset at hand is sufficient and of the required quality for specific tasks. This research offers valuable insights into data assessment practices, contributing to the development of more accurate and robus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#35282;&#24230;&#20449;&#24687;&#34701;&#21512;Res2Net&#19982;&#38543;&#26426;Specmix&#30340;&#31995;&#32479;&#29992;&#20110;&#20551;&#20882;&#35821;&#38899;&#26816;&#27979;&#65292;&#22312;&#20302;&#36136;&#37327;&#22330;&#26223;&#19979;&#25552;&#39640;&#20102;&#27169;&#22411;&#23398;&#20064;&#20934;&#30830;&#20266;&#36896;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20943;&#23569;&#20887;&#20313;&#30340;&#24178;&#25200;&#20449;&#24687;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15389</link><description>&lt;p&gt;
&#22810;&#35282;&#24230;&#20449;&#24687;&#34701;&#21512;Res2Net&#19982;&#38543;&#26426;Specmix&#29992;&#20110;&#20551;&#20882;&#35821;&#38899;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-perspective Information Fusion Res2Net with RandomSpecmix for Fake Speech Detection. (arXiv:2306.15389v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#35282;&#24230;&#20449;&#24687;&#34701;&#21512;Res2Net&#19982;&#38543;&#26426;Specmix&#30340;&#31995;&#32479;&#29992;&#20110;&#20551;&#20882;&#35821;&#38899;&#26816;&#27979;&#65292;&#22312;&#20302;&#36136;&#37327;&#22330;&#26223;&#19979;&#25552;&#39640;&#20102;&#27169;&#22411;&#23398;&#20064;&#20934;&#30830;&#20266;&#36896;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20943;&#23569;&#20887;&#20313;&#30340;&#24178;&#25200;&#20449;&#24687;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#35282;&#24230;&#20449;&#24687;&#34701;&#21512;Res2Net&#19982;&#38543;&#26426;Specmix&#29992;&#20110;&#20551;&#20882;&#35821;&#38899;&#26816;&#27979;&#12290;&#35813;&#31995;&#32479;&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#25552;&#39640;&#27169;&#22411;&#22312;&#20302;&#36136;&#37327;&#22330;&#26223;&#19979;&#23398;&#20064;&#20934;&#30830;&#30340;&#20266;&#36896;&#20449;&#24687;&#20197;&#23436;&#25104;&#20551;&#20882;&#35821;&#38899;&#26816;&#27979;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#38543;&#26426;Specmix&#26159;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20854;&#30446;&#30340;&#26159;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#22686;&#24378;&#27169;&#22411;&#23450;&#20301;&#36776;&#21035;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;Specmix&#22312;&#21516;&#19968;&#25209;&#27425;&#30340;&#26679;&#26412;&#20013;&#21098;&#20999;&#21644;&#31896;&#36148;&#39057;&#29575;&#32500;&#24230;&#30340;&#22768;&#35889;&#22270;&#20449;&#24687;&#65292;&#19981;&#24341;&#20837;&#20854;&#20182;&#25968;&#25454;&#65292;&#26377;&#21161;&#20110;&#27169;&#22411;&#23450;&#20301;&#30495;&#27491;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#38543;&#26426;&#36873;&#25321;&#26679;&#26412;&#36827;&#34892;&#22686;&#24378;&#65292;&#20197;&#20943;&#23569;&#25968;&#25454;&#22686;&#24378;&#30452;&#25509;&#25913;&#21464;&#25152;&#26377;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#19968;&#26086;&#36798;&#21040;&#24110;&#21161;&#27169;&#22411;&#23450;&#20301;&#20449;&#24687;&#30340;&#30446;&#30340;&#65292;&#20943;&#23569;&#19981;&#24517;&#35201;&#30340;&#20449;&#24687;&#20063;&#21464;&#24471;&#37325;&#35201;&#12290;MPIF-Res2Net&#30340;&#20316;&#29992;&#26159;&#20943;&#23569;&#20887;&#20313;&#30340;&#24178;&#25200;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose the multi-perspective information fusion (MPIF) Res2Net with random Specmix for fake speech detection (FSD). The main purpose of this system is to improve the model's ability to learn precise forgery information for FSD task in low-quality scenarios. The task of random Specmix, a data augmentation, is to improve the generalization ability of the model and enhance the model's ability to locate discriminative information. Specmix cuts and pastes the frequency dimension information of the spectrogram in the same batch of samples without introducing other data, which helps the model to locate the really useful information. At the same time, we randomly select samples for augmentation to reduce the impact of data augmentation directly changing all the data. Once the purpose of helping the model to locate information is achieved, it is also important to reduce unnecessary information. The role of MPIF-Res2Net is to reduce redundant interference information. Deceptiv
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#33258;&#21160;&#28040;&#38500;&#24207;&#21015;&#20887;&#20313;&#20197;&#23454;&#29616;&#20986;&#33394;&#30340;&#21387;&#32553;&#27604;&#21644;&#35299;&#21387;&#32553;&#24615;&#33021;&#30340;LeCo&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#21387;&#32553;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#24207;&#21015;&#30456;&#20851;&#24615;&#65292;&#23427;&#33021;&#22815;&#22312;&#21387;&#32553;&#27604;&#21644;&#38543;&#26426;&#35775;&#38382;&#36895;&#24230;&#19978;&#23454;&#29616;&#24085;&#32047;&#25176;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.15374</link><description>&lt;p&gt;
LeCo&#65306;&#36890;&#36807;&#23398;&#20064;&#24207;&#21015;&#30456;&#20851;&#24615;&#23454;&#29616;&#36731;&#37327;&#32423;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
LeCo: Lightweight Compression via Learning Serial Correlations. (arXiv:2306.15374v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15374
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#33258;&#21160;&#28040;&#38500;&#24207;&#21015;&#20887;&#20313;&#20197;&#23454;&#29616;&#20986;&#33394;&#30340;&#21387;&#32553;&#27604;&#21644;&#35299;&#21387;&#32553;&#24615;&#33021;&#30340;LeCo&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#21387;&#32553;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#24207;&#21015;&#30456;&#20851;&#24615;&#65292;&#23427;&#33021;&#22815;&#22312;&#21387;&#32553;&#27604;&#21644;&#38543;&#26426;&#35775;&#38382;&#36895;&#24230;&#19978;&#23454;&#29616;&#24085;&#32047;&#25176;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36731;&#37327;&#32423;&#25968;&#25454;&#21387;&#32553;&#26159;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#65292;&#23427;&#20351;&#24471;&#21015;&#24335;&#23384;&#20648;&#22312;&#20998;&#26512;&#26597;&#35810;&#26041;&#38754;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#20043;&#21069;&#26377;&#20851;&#22522;&#20110;&#23383;&#20856;&#32534;&#30721;&#26469;&#36924;&#36817;Shannon&#29109;&#30340;&#30740;&#31350;&#24050;&#32463;&#24456;&#20840;&#38754;&#65292;&#20294;&#40092;&#26377;&#20043;&#21069;&#30340;&#24037;&#20316;&#31995;&#32479;&#22320;&#21033;&#29992;&#21015;&#30340;&#24207;&#21015;&#30456;&#20851;&#24615;&#26469;&#36827;&#34892;&#21387;&#32553;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LeCo&#65288;&#21363;&#23398;&#20064;&#21387;&#32553;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#33258;&#21160;&#28040;&#38500;&#24207;&#21015;&#20887;&#20313;&#20197;&#23454;&#29616;&#20986;&#33394;&#30340;&#21387;&#32553;&#27604;&#21644;&#35299;&#21387;&#32553;&#24615;&#33021;&#30340;&#26694;&#26550;&#12290;LeCo&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19979;&#65292;&#29616;&#26377;&#30340;&#65288;&#20020;&#26102;&#30340;&#65289;&#31639;&#27861;&#65292;&#22914;&#21442;&#32771;&#24103;&#65288;Frame-of-Reference&#65289;&#65292;Delta&#32534;&#30721;&#21644;&#28216;&#31243;&#32534;&#30721;&#65288;Run-Length Encoding&#65289;&#37117;&#26159;&#29305;&#20363;&#12290;&#25105;&#20204;&#23545;&#19977;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20845;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#24494;&#22522;&#20934;&#27979;&#35797;&#26174;&#31034;&#65292;LeCo&#21407;&#22411;&#22312;&#21387;&#32553;&#27604;&#21644;&#38543;&#26426;&#35775;&#38382;&#36895;&#24230;&#19978;&#30456;&#27604;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21462;&#24471;&#20102;&#24085;&#32047;&#25176;&#25913;&#36827;&#12290;&#24403;&#23558;LeCo&#38598;&#25104;&#26102;
&lt;/p&gt;
&lt;p&gt;
Lightweight data compression is a key technique that allows column stores to exhibit superior performance for analytical queries. Despite a comprehensive study on dictionary-based encodings to approach Shannon's entropy, few prior works have systematically exploited the serial correlation in a column for compression. In this paper, we propose LeCo (i.e., Learned Compression), a framework that uses machine learning to remove the serial redundancy in a value sequence automatically to achieve an outstanding compression ratio and decompression performance simultaneously. LeCo presents a general approach to this end, making existing (ad-hoc) algorithms such as Frame-of-Reference (FOR), Delta Encoding, and Run-Length Encoding (RLE) special cases under our framework. Our microbenchmark with three synthetic and six real-world data sets shows that a prototype of LeCo achieves a Pareto improvement on both compression ratio and random access speed over the existing solutions. When integrating LeC
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31995;&#32479;&#24615;&#25991;&#29486;&#22238;&#39038;&#21644;&#20803;&#20998;&#26512;&#65292;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#26420;&#32032;&#36125;&#21494;&#26031;&#21644;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#22312;&#36719;&#20214;&#32570;&#38519;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27809;&#26377;&#32479;&#35745;&#35777;&#25454;&#34920;&#26126;&#26420;&#32032;&#36125;&#21494;&#26031;&#22312;&#21484;&#22238;&#29575;&#12289;F-&#24230;&#37327;&#21644;&#31934;&#30830;&#24230;&#26041;&#38754;&#19982;&#38543;&#26426;&#26862;&#26519;&#26377;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.15369</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#20998;&#26512;&#30340;&#26420;&#32032;&#36125;&#21494;&#26031;&#21644;&#38543;&#26426;&#26862;&#26519;&#22312;&#36719;&#20214;&#32570;&#38519;&#39044;&#27979;&#20013;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Meta-analytical Comparison of Naive Bayes and Random Forest for Software Defect Prediction. (arXiv:2306.15369v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15369
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31995;&#32479;&#24615;&#25991;&#29486;&#22238;&#39038;&#21644;&#20803;&#20998;&#26512;&#65292;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#26420;&#32032;&#36125;&#21494;&#26031;&#21644;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#22312;&#36719;&#20214;&#32570;&#38519;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27809;&#26377;&#32479;&#35745;&#35777;&#25454;&#34920;&#26126;&#26420;&#32032;&#36125;&#21494;&#26031;&#22312;&#21484;&#22238;&#29575;&#12289;F-&#24230;&#37327;&#21644;&#31934;&#30830;&#24230;&#26041;&#38754;&#19982;&#38543;&#26426;&#26862;&#26519;&#26377;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#32570;&#38519;&#39044;&#27979;&#20013;&#65292;&#26420;&#32032;&#36125;&#21494;&#26031;&#21644;&#38543;&#26426;&#26862;&#26519;&#22312;&#21484;&#22238;&#29575;&#12289;F-&#24230;&#37327;&#21644;&#31934;&#30830;&#24230;&#26041;&#38754;&#26159;&#21542;&#23384;&#22312;&#32479;&#35745;&#24046;&#24322;&#65311;&#36890;&#36807;&#21033;&#29992;&#31995;&#32479;&#24615;&#25991;&#29486;&#22238;&#39038;&#21644;&#20803;&#20998;&#26512;&#65292;&#25105;&#20204;&#27491;&#22312;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#25628;&#32034;&#21644;&#36873;&#25321;&#35770;&#25991;&#30340;&#26631;&#20934;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#25991;&#29486;&#22238;&#39038;&#65292;&#20849;&#21253;&#25324;&#20102;&#20116;&#20010;&#30740;&#31350;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20116;&#20010;&#36873;&#25321;&#30340;&#35770;&#25991;&#30340;&#20803;&#25968;&#25454;&#21644;&#26862;&#26519;&#22270;&#36827;&#34892;&#20102;&#20803;&#20998;&#26512;&#26469;&#27604;&#36739;&#20004;&#20010;&#27169;&#22411;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#27809;&#26377;&#26174;&#33879;&#30340;&#32479;&#35745;&#35777;&#25454;&#34920;&#26126;&#26420;&#32032;&#36125;&#21494;&#26031;&#22312;&#21484;&#22238;&#29575;&#12289;F-&#24230;&#37327;&#21644;&#31934;&#30830;&#24230;&#26041;&#38754;&#19982;&#38543;&#26426;&#26862;&#26519;&#34920;&#29616;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is there a statistical difference between Naive Bayes and Random Forest in terms of recall, f-measure, and precision for predicting software defects? By utilizing systematic literature review and meta-analysis, we are answering this question. We conducted a systematic literature review by establishing criteria to search and choose papers, resulting in five studies. After that, using the meta-data and forest-plots of five chosen papers, we conducted a meta-analysis to compare the two models. The results have shown that there is no significant statistical evidence that Naive Bayes perform differently from Random Forest in terms of recall, f-measure, and precision.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#22343;&#22330;&#29702;&#35770;&#24212;&#29992;&#20110;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#24230;&#30340;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#20998;&#31867;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#35757;&#32451;&#22797;&#26434;&#24230;&#65292;&#22312;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.15368</link><description>&lt;p&gt;
&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#20013;&#30340;&#22343;&#22330;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Mean Field Theory in Deep Metric Learning. (arXiv:2306.15368v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22343;&#22330;&#29702;&#35770;&#24212;&#29992;&#20110;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#24230;&#30340;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#20998;&#31867;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#35757;&#32451;&#22797;&#26434;&#24230;&#65292;&#22312;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#20013;&#24212;&#29992;&#32479;&#35745;&#29289;&#29702;&#20013;&#30340;&#22343;&#22330;&#29702;&#35770;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;&#24230;&#37327;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#25152;&#26222;&#36941;&#23384;&#22312;&#30340;&#39640;&#35757;&#32451;&#22797;&#26434;&#24230;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#22343;&#22330;&#29702;&#35770;&#24212;&#29992;&#20110;&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22522;&#20110;&#23545;&#27604;&#24230;&#30340;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#20998;&#31867;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36825;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#23545;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#30340;&#34917;&#20805;&#12290;&#23558;&#22343;&#22330;&#29702;&#35770;&#24212;&#29992;&#20110;&#20004;&#31181;&#22522;&#20110;&#23545;&#27604;&#24230;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;MeanFieldContrastive&#21644;MeanFieldClassWiseMultiSimilarity&#25439;&#22833;&#20989;&#25968;&#65292;&#20854;&#35757;&#32451;&#22797;&#26434;&#24230;&#24471;&#21040;&#20102;&#38477;&#20302;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22270;&#20687;&#26816;&#32034;&#25968;&#25454;&#38598;&#19978;&#23545;&#36825;&#20123;&#34893;&#29983;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#20004;&#20010;&#19978;&#32988;&#36807;&#20102;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the application of mean field theory, a technique from statistical physics, to deep metric learning and address the high training complexity commonly associated with conventional metric learning loss functions. By adapting mean field theory for deep metric learning, we develop an approach to design classification-based loss functions from pair-based ones, which can be considered complementary to the proxy-based approach. Applying the mean field theory to two pair-based loss functions, we derive two new loss functions, MeanFieldContrastive and MeanFieldClassWiseMultiSimilarity losses, with reduced training complexity. We extensively evaluate these derived loss functions on three image-retrieval datasets and demonstrate that our loss functions outperform baseline methods in two out of the three datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DUMB&#25915;&#20987;&#32773;&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#26512;&#22312;&#26367;&#20195;&#21697;&#21644;&#21463;&#23475;&#27169;&#22411;&#30340;&#35757;&#32451;&#26465;&#20214;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#25915;&#20987;&#35268;&#36991;&#26159;&#21542;&#22833;&#36133;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2306.15363</link><description>&lt;p&gt;
&#20320;&#30340;&#25915;&#20987;&#22826;&#24858;&#34850;&#20102;: &#23545;&#25932;&#23545;&#36801;&#31227;&#30340;&#25915;&#20987;&#32773;&#22330;&#26223;&#36827;&#34892;&#24418;&#24335;&#21270;&#12290;(arXiv:2306.15363v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
Your Attack Is Too DUMB: Formalizing Attacker Scenarios for Adversarial Transferability. (arXiv:2306.15363v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;DUMB&#25915;&#20987;&#32773;&#27169;&#22411;&#65292;&#36890;&#36807;&#20998;&#26512;&#22312;&#26367;&#20195;&#21697;&#21644;&#21463;&#23475;&#27169;&#22411;&#30340;&#35757;&#32451;&#26465;&#20214;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#25915;&#20987;&#35268;&#36991;&#26159;&#21542;&#22833;&#36133;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25915;&#20987;&#35268;&#36991;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26500;&#25104;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#35797;&#22270;&#36890;&#36807;&#27880;&#20837;&#24694;&#24847;&#26679;&#26412;&#26469;&#24433;&#21709;&#20998;&#31867;&#22120;&#12290;&#25915;&#20987;&#35268;&#36991;&#30340;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#21103;&#20316;&#29992;&#26159;&#20854;&#33021;&#22815;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#36801;&#31227;&#65292;&#36825;&#34987;&#31216;&#20026;&#36801;&#31227;&#24615;&#12290;&#22240;&#27492;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#33258;&#23450;&#20041;&#27169;&#22411;&#65288;&#26367;&#20195;&#21697;&#65289;&#19978;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#28982;&#21518;&#22312;&#21463;&#23475;&#32452;&#32455;&#19978;&#36827;&#34892;&#25915;&#20987;&#12290;&#23613;&#31649;&#25991;&#29486;&#24191;&#27867;&#35752;&#35770;&#20102;&#25932;&#23545;&#26041;&#22914;&#20309;&#36716;&#31227;&#20182;&#20204;&#30340;&#25915;&#20987;&#65292;&#20294;&#20182;&#20204;&#30340;&#23454;&#39564;&#35774;&#32622;&#26377;&#38480;&#19988;&#19982;&#29616;&#23454;&#30456;&#36317;&#29978;&#36828;&#12290;&#20363;&#22914;&#65292;&#35768;&#22810;&#23454;&#39564;&#32771;&#34385;&#25915;&#20987;&#32773;&#21644;&#38450;&#24481;&#32773;&#20849;&#20139;&#30456;&#21516;&#30340;&#25968;&#25454;&#38598;&#12289;&#24179;&#34913;&#32423;&#21035;&#65288;&#21363;&#65292;&#22522;&#26412;&#20107;&#23454;&#20998;&#24067;&#65289;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24858;&#34850;&#30340;&#25915;&#20987;&#32773;&#27169;&#22411;&#65288;DUMB&#65289;&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#20998;&#26512;&#24403;&#26367;&#20195;&#21697;&#21644;&#21463;&#23475;&#27169;&#22411;&#30340;&#35757;&#32451;&#26465;&#20214;&#19981;&#21516;&#26102;&#65292;&#25915;&#20987;&#35268;&#36991;&#26159;&#21542;&#22833;&#36133;&#36716;&#31227;&#12290;&#24858;&#34850;&#25915;&#20987;&#32773;&#27169;&#22411;&#32771;&#34385;&#20197;&#19979;&#26465;&#20214;&#65306;&#25968;&#25454;&#38598;&#26469;&#28304;&#65292;&#27169;&#22411;&#26550;&#26500;&#65292;&#25932;&#23545;&#26041;&#36873;&#25321;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evasion attacks are a threat to machine learning models, where adversaries attempt to affect classifiers by injecting malicious samples. An alarming side-effect of evasion attacks is their ability to transfer among different models: this property is called transferability. Therefore, an attacker can produce adversarial samples on a custom model (surrogate) to conduct the attack on a victim's organization later. Although literature widely discusses how adversaries can transfer their attacks, their experimental settings are limited and far from reality. For instance, many experiments consider both attacker and defender sharing the same dataset, balance level (i.e., how the ground truth is distributed), and model architecture.  In this work, we propose the DUMB attacker model. This framework allows analyzing if evasion attacks fail to transfer when the training conditions of surrogate and victim models differ. DUMB considers the following conditions: Dataset soUrces, Model architecture, a
&lt;/p&gt;</description></item><item><title>CellViT&#26159;&#19968;&#31181;&#22522;&#20110;Vision Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#21160;&#23454;&#20363;&#20998;&#21106;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#30340;&#32452;&#32455;&#26679;&#26412;&#20013;&#30340;&#32454;&#32990;&#26680;&#12290;&#36890;&#36807;&#22312;PanNuke&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;CellViT&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15350</link><description>&lt;p&gt;
CellViT:&#29992;&#20110;&#31934;&#30830;&#32454;&#32990;&#20998;&#21106;&#21644;&#20998;&#31867;&#30340;&#35270;&#35273;Transformer
&lt;/p&gt;
&lt;p&gt;
CellViT: Vision Transformers for Precise Cell Segmentation and Classification. (arXiv:2306.15350v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15350
&lt;/p&gt;
&lt;p&gt;
CellViT&#26159;&#19968;&#31181;&#22522;&#20110;Vision Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#21160;&#23454;&#20363;&#20998;&#21106;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#30340;&#32452;&#32455;&#26679;&#26412;&#20013;&#30340;&#32454;&#32990;&#26680;&#12290;&#36890;&#36807;&#22312;PanNuke&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;CellViT&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#26680;&#22312;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#30340;&#32452;&#32455;&#22270;&#20687;&#20013;&#30340;&#26816;&#27979;&#21644;&#20998;&#21106;&#26159;&#37325;&#35201;&#30340;&#20020;&#24202;&#20219;&#21153;&#65292;&#24182;&#19988;&#23545;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32454;&#32990;&#26680;&#26579;&#33394;&#21644;&#22823;&#23567;&#30340;&#24046;&#24322;&#12289;&#36793;&#30028;&#37325;&#21472;&#21644;&#26680;&#32858;&#38598;&#65292;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#24050;&#32463;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#25105;&#20204;&#25506;&#32034;&#20102;Transformer-based&#32593;&#32476;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;Vision Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;CellViT&#23545;&#25968;&#23383;&#21270;&#32452;&#32455;&#26679;&#26412;&#20013;&#30340;&#32454;&#32990;&#26680;&#36827;&#34892;&#33258;&#21160;&#23454;&#20363;&#20998;&#21106;&#30340;&#26032;&#26041;&#27861;&#12290;CellViT&#22312;PanNuke&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#32454;&#32990;&#26680;&#23454;&#20363;&#20998;&#21106;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#36817;20&#19975;&#20010;&#27880;&#37322;&#30340;&#32454;&#32990;&#26680;&#65292;&#20998;&#20026;19&#31181;&#32452;&#32455;&#31867;&#22411;&#30340;5&#20010;&#20020;&#24202;&#37325;&#35201;&#31867;&#21035;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;&#22823;&#35268;&#27169;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#39044;&#35757;&#32451;&#30340;Vision Transformer&#65292;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nuclei detection and segmentation in hematoxylin and eosin-stained (H&amp;E) tissue images are important clinical tasks and crucial for a wide range of applications. However, it is a challenging task due to nuclei variances in staining and size, overlapping boundaries, and nuclei clustering. While convolutional neural networks have been extensively used for this task, we explore the potential of Transformer-based networks in this domain. Therefore, we introduce a new method for automated instance segmentation of cell nuclei in digitized tissue samples using a deep learning architecture based on Vision Transformer called CellViT. CellViT is trained and evaluated on the PanNuke dataset, which is one of the most challenging nuclei instance segmentation datasets, consisting of nearly 200,000 annotated Nuclei into 5 clinically important classes in 19 tissue types. We demonstrate the superiority of large-scale in-domain and out-of-domain pre-trained Vision Transformers by leveraging the recently
&lt;/p&gt;</description></item><item><title>FedET&#26159;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#31867;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22686;&#24378;Transformer&#21644;&#22686;&#24378;&#22120;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36890;&#20449;&#25104;&#26412;&#38382;&#39064;&#65292;&#20445;&#35777;&#20102;&#39640;&#31934;&#24230;&#21644;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2306.15347</link><description>&lt;p&gt;
FedET: &#19968;&#31181;&#22522;&#20110;&#22686;&#24378;Transformer&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#31867;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedET: A Communication-Efficient Federated Class-Incremental Learning Framework Based on Enhanced Transformer. (arXiv:2306.15347v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15347
&lt;/p&gt;
&lt;p&gt;
FedET&#26159;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#31867;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22686;&#24378;Transformer&#21644;&#22686;&#24378;&#22120;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#36890;&#20449;&#25104;&#26412;&#38382;&#39064;&#65292;&#20445;&#35777;&#20102;&#39640;&#31934;&#24230;&#21644;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Federated Learning, FL)&#30001;&#20110;&#33021;&#22815;&#22312;&#20445;&#35777;&#25968;&#25454;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#36827;&#34892;&#20998;&#25955;&#23398;&#20064;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#19981;&#20999;&#23454;&#38469;&#22320;&#20551;&#35774;&#26412;&#22320;&#23458;&#25143;&#31471;&#36935;&#21040;&#30340;&#31867;&#21035;&#38543;&#26102;&#38388;&#22266;&#23450;&#12290;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#21518;&#65292;&#36825;&#20010;&#20551;&#35774;&#20250;&#23548;&#33268;&#27169;&#22411;&#23545;&#26087;&#31867;&#21035;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#27492;&#22806;&#65292;&#21463;&#36890;&#20449;&#25104;&#26412;&#30340;&#38480;&#21046;&#65292;&#22312;FL&#20013;&#20351;&#29992;&#22823;&#35268;&#27169;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#36825;&#23558;&#24433;&#21709;&#39044;&#27979;&#31934;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;FedET&#65292;&#23427;&#21516;&#26102;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#21644;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FedET&#20351;&#29992;&#22686;&#24378;&#22120;(Enhancer)&#36825;&#20010;&#23567;&#22411;&#27169;&#22359;&#26469;&#21560;&#25910;&#21644;&#20256;&#36882;&#26032;&#30340;&#30693;&#35782;&#65292;&#24182;&#23558;&#39044;&#35757;&#32451;&#30340;Transformer&#19982;&#19981;&#21516;&#30340;&#22686;&#24378;&#22120;&#32467;&#21512;&#20351;&#29992;&#65292;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#20445;&#35777;&#39640;&#31934;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#26032;&#20219;&#21153;&#30340;&#26032;&#31867;&#21035;&#24341;&#36215;&#30340;&#26412;&#22320;&#36951;&#24536;&#38382;&#39064;&#21644;&#38750;i.i.d&#65288;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65289;&#25152;&#24102;&#26469;&#30340;&#20840;&#23616;&#36951;&#24536;&#38382;&#39064;&#65292;FedET&#20351;&#29992;&#20102;&#21160;&#24577;&#25193;&#23637;&#26041;&#27861;&#21644;&#37325;&#25918;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has been widely concerned for it enables decentralized learning while ensuring data privacy. However, most existing methods unrealistically assume that the classes encountered by local clients are fixed over time. After learning new classes, this assumption will make the model's catastrophic forgetting of old classes significantly severe. Moreover, due to the limitation of communication cost, it is challenging to use large-scale models in FL, which will affect the prediction accuracy. To address these challenges, we propose a novel framework, Federated Enhanced Transformer (FedET), which simultaneously achieves high accuracy and low communication cost. Specifically, FedET uses Enhancer, a tiny module, to absorb and communicate new knowledge, and applies pre-trained Transformers combined with different Enhancers to ensure high precision on various tasks. To address local forgetting caused by new classes of new tasks and global forgetting brought by non-i.i.d (non
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;numpy&#30340;&#21306;&#38388;&#20998;&#26512;&#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#26500;&#24314;&#33258;&#28982;&#21253;&#21547;&#20989;&#25968;&#26469;&#35745;&#31639;&#24191;&#27867;&#26144;&#23556;&#31867;&#30340;&#21306;&#38388;&#30028;&#38480;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#24418;&#24335;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.15340</link><description>&lt;p&gt;
&#20351;&#29992;numpy&#30340;&#24555;&#36895;&#21306;&#38388;&#31639;&#26415;&#24037;&#20855;&#31665;&#21450;&#20854;&#22312;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#24418;&#24335;&#39564;&#35777;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Toolbox for Fast Interval Arithmetic in numpy with an Application to Formal Verification of Neural Network Controlled Systems. (arXiv:2306.15340v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;numpy&#30340;&#21306;&#38388;&#20998;&#26512;&#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#26500;&#24314;&#33258;&#28982;&#21253;&#21547;&#20989;&#25968;&#26469;&#35745;&#31639;&#24191;&#27867;&#26144;&#23556;&#31867;&#30340;&#21306;&#38388;&#30028;&#38480;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#24418;&#24335;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22312;numpy&#20013;&#36827;&#34892;&#21306;&#38388;&#20998;&#26512;&#30340;&#24037;&#20855;&#31665;&#65292;&#24182;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#24418;&#24335;&#39564;&#35777;&#12290;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#21253;&#21547;&#20989;&#25968;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#26500;&#24314;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#26144;&#23556;&#31867;&#30340;&#21306;&#38388;&#30028;&#38480;&#12290;&#35813;&#24037;&#20855;&#31665;&#25552;&#20379;&#20102;&#20351;&#29992;&#32534;&#35793;&#30340;C&#20195;&#30721;&#39640;&#25928;&#35745;&#31639;&#33258;&#28982;&#21253;&#21547;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;numpy&#20013;&#25552;&#20379;&#20102;&#29087;&#24713;&#30340;&#25509;&#21475;&#21644;&#20854;&#32463;&#20856;&#21151;&#33021;&#65292;&#20363;&#22914;n&#32500;&#25968;&#32452;&#12289;&#30697;&#38453;/&#21521;&#37327;&#25805;&#20316;&#21644;&#21521;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a toolbox for interval analysis in numpy, with an application to formal verification of neural network controlled systems. Using the notion of natural inclusion functions, we systematically construct interval bounds for a general class of mappings. The toolbox offers efficient computation of natural inclusion functions using compiled C code, as well as a familiar interface in numpy with its canonical features, such as n-dimensional arrays, matrix/vector operations, and vectorization. We then use this toolbox in formal verification of dynamical systems with neural network controllers, through the composition of their inclusion functions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21516;&#35843;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24212;&#29992;&#32593;&#32476;&#36807;&#28388;&#25216;&#26415;&#26500;&#24314;&#31232;&#30095;&#30340;&#39640;&#38454;&#22270;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#33021;&#28304;&#25928;&#29575;&#25361;&#25112;&#65292;&#24182;&#22312;&#34920;&#26684;&#25968;&#25454;&#21644;&#26102;&#38388;&#24207;&#21015;&#22238;&#24402;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15337</link><description>&lt;p&gt;
&#21516;&#35843;&#31070;&#32463;&#32593;&#32476;&#65306;&#22810;&#20803;&#22797;&#26434;&#24615;&#30340;&#31232;&#30095;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Homological Neural Networks: A Sparse Architecture for Multivariate Complexity. (arXiv:2306.15337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21516;&#35843;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24212;&#29992;&#32593;&#32476;&#36807;&#28388;&#25216;&#26415;&#26500;&#24314;&#31232;&#30095;&#30340;&#39640;&#38454;&#22270;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#33021;&#28304;&#25928;&#29575;&#25361;&#25112;&#65292;&#24182;&#22312;&#34920;&#26684;&#25968;&#25454;&#21644;&#26102;&#38388;&#24207;&#21015;&#22238;&#24402;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24320;&#21457;&#24102;&#26469;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#12289;&#33021;&#28304;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#26085;&#30410;&#22686;&#38271;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#20808;&#36827;&#30340;&#22522;&#20110;&#32593;&#32476;&#30340;&#20449;&#24687;&#36807;&#28388;&#25216;&#26415;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#65292;&#35813;&#21333;&#20803;&#20197;&#22522;&#30784;&#25968;&#25454;&#30340;&#21516;&#35843;&#32467;&#26500;&#20026;&#22522;&#30784;&#26500;&#24314;&#20102;&#19968;&#20010;&#31232;&#30095;&#30340;&#39640;&#38454;&#22270;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20256;&#32479;&#19978;&#23545;&#28145;&#24230;&#23398;&#20064;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24212;&#29992;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65306;&#34920;&#26684;&#25968;&#25454;&#21644;&#26102;&#38388;&#24207;&#21015;&#22238;&#24402;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#39062;&#35774;&#35745;&#30340;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#20165;&#20351;&#29992;&#37096;&#20998;&#21442;&#25968;&#19982;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24182;&#21015;&#25110;&#36229;&#36234;&#20854;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid progress of Artificial Intelligence research came with the development of increasingly complex deep learning models, leading to growing challenges in terms of computational complexity, energy efficiency and interpretability. In this study, we apply advanced network-based information filtering techniques to design a novel deep neural network unit characterized by a sparse higher-order graphical architecture built over the homological structure of underlying data. We demonstrate its effectiveness in two application domains which are traditionally challenging for deep learning: tabular data and time series regression problems. Results demonstrate the advantages of this novel design which can tie or overcome the results of state-of-the-art machine learning and deep learning models using only a fraction of parameters.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#27169;&#25311;&#21453;&#20107;&#23454;&#20998;&#24067;&#20013;&#30340;&#20540;&#65292;&#21487;&#23545;&#31163;&#25955;&#21644;&#36830;&#32493;&#21464;&#37327;&#35774;&#23450;&#26465;&#20214;&#65292;&#24182;&#24212;&#29992;&#20110;&#20449;&#29992;&#35780;&#20998;&#20013;&#30340;&#20844;&#24179;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.15328</link><description>&lt;p&gt;
&#27169;&#25311;&#21453;&#20107;&#23454;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Simulating counterfactuals. (arXiv:2306.15328v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15328
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#27169;&#25311;&#21453;&#20107;&#23454;&#20998;&#24067;&#20013;&#30340;&#20540;&#65292;&#21487;&#23545;&#31163;&#25955;&#21644;&#36830;&#32493;&#21464;&#37327;&#35774;&#23450;&#26465;&#20214;&#65292;&#24182;&#24212;&#29992;&#20110;&#20449;&#29992;&#35780;&#20998;&#20013;&#30340;&#20844;&#24179;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#25512;&#26029;&#32771;&#34385;&#20102;&#22312;&#19982;&#23454;&#38469;&#19990;&#30028;&#23384;&#22312;&#19968;&#20123;&#35777;&#25454;&#30340;&#24179;&#34892;&#19990;&#30028;&#20013;&#36827;&#34892;&#30340;&#20551;&#35774;&#24615;&#24178;&#39044;&#12290;&#22914;&#26524;&#35777;&#25454;&#22312;&#27969;&#24418;&#19978;&#25351;&#23450;&#20102;&#26465;&#20214;&#20998;&#24067;&#65292;&#21453;&#20107;&#23454;&#21487;&#33021;&#26159;&#35299;&#26512;&#38590;&#35299;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#21453;&#20107;&#23454;&#20998;&#24067;&#20013;&#27169;&#25311;&#20540;&#65292;&#20854;&#20013;&#21487;&#20197;&#23545;&#31163;&#25955;&#21644;&#36830;&#32493;&#21464;&#37327;&#35774;&#23450;&#26465;&#20214;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#34987;&#21576;&#29616;&#20026;&#31890;&#23376;&#28388;&#27874;&#22120;&#65292;&#20174;&#32780;&#23548;&#33268;&#28176;&#36817;&#26377;&#25928;&#30340;&#25512;&#26029;&#12290;&#35813;&#31639;&#27861;&#34987;&#24212;&#29992;&#20110;&#20449;&#29992;&#35780;&#20998;&#20013;&#30340;&#20844;&#24179;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual inference considers a hypothetical intervention in a parallel world that shares some evidence with the factual world. If the evidence specifies a conditional distribution on a manifold, counterfactuals may be analytically intractable. We present an algorithm for simulating values from a counterfactual distribution where conditions can be set on both discrete and continuous variables. We show that the proposed algorithm can be presented as a particle filter leading to asymptotically valid inference. The algorithm is applied to fairness analysis in credit scoring.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23567;&#35268;&#27169;&#22270;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.15324</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Anomaly Detection in Networks via Score-Based Generative Models. (arXiv:2306.15324v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15324
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23567;&#35268;&#27169;&#22270;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24102;&#23646;&#24615;&#30340;&#22270;&#20013;&#23545;&#33410;&#28857;&#24322;&#24120;&#36827;&#34892;&#26816;&#27979;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#21463;&#22270;&#29983;&#25104;&#24314;&#27169;&#20013;&#22522;&#20110;&#24471;&#20998;&#27169;&#22411;&#30340;&#26368;&#26032;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#20854;&#24212;&#29992;&#20110;&#19978;&#36848;&#38382;&#39064;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23567;&#35268;&#27169;&#22270;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23545;&#29380;&#21033;&#20811;&#38647;&#33021;&#37327;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#24182;&#34920;&#26126;&#29983;&#25104;&#27169;&#22411;&#21487;&#33021;&#38590;&#20197;&#20934;&#30830;&#37325;&#26500;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node outlier detection in attributed graphs is a challenging problem for which there is no method that would work well across different datasets. Motivated by the state-of-the-art results of score-based models in graph generative modeling, we propose to incorporate them into the aforementioned problem. Our method achieves competitive results on small-scale graphs. We provide an empirical analysis of the Dirichlet energy, and show that generative models might struggle to accurately reconstruct it.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#20174;&#20915;&#31574;&#21512;&#29702;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#65292;&#24182;&#23450;&#20041;&#20102;&#21442;&#25968;&#24179;&#31561;&#24471;&#20998;&#26469;&#34920;&#24449;&#20844;&#24179;&#20915;&#31574;&#36807;&#31243;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#29616;&#26377;&#30340;&#20844;&#24179;&#24615;&#35268;&#33539;&#39033;&#26080;&#27861;&#23454;&#29616;&#20915;&#31574;&#21512;&#29702;&#24615;&#30340;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2306.15299</link><description>&lt;p&gt;
FAIRER: &#20844;&#24179;&#20316;&#20026;&#20915;&#31574;&#21512;&#29702;&#24615;&#23545;&#40784;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
FAIRER: Fairness as Decision Rationale Alignment. (arXiv:2306.15299v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#20174;&#20915;&#31574;&#21512;&#29702;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#65292;&#24182;&#23450;&#20041;&#20102;&#21442;&#25968;&#24179;&#31561;&#24471;&#20998;&#26469;&#34920;&#24449;&#20844;&#24179;&#20915;&#31574;&#36807;&#31243;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#29616;&#26377;&#30340;&#20844;&#24179;&#24615;&#35268;&#33539;&#39033;&#26080;&#27861;&#23454;&#29616;&#20915;&#31574;&#21512;&#29702;&#24615;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#24448;&#24448;&#23384;&#22312;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#28145;&#24230;&#27169;&#22411;&#36890;&#24120;&#22312;&#26576;&#20123;&#23376;&#32676;&#20307;&#65288;&#20363;&#22914;&#30007;&#24615;&#21644;&#22899;&#24615;&#65289;&#20043;&#38388;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#20934;&#30830;&#24615;&#24046;&#24322;&#12290;&#29616;&#26377;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20844;&#24179;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#26469;&#32422;&#26463;&#26368;&#21518;&#19968;&#23618;&#30340;&#36755;&#20986;&#24182;&#30452;&#25509;&#35268;&#33539;&#21270;DNNs&#26469;&#35299;&#20915;&#36825;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#34429;&#28982;DNN&#30340;&#20844;&#24179;&#24615;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#20294;&#19981;&#28165;&#26970;&#32463;&#36807;&#35757;&#32451;&#30340;&#32593;&#32476;&#22914;&#20309;&#36827;&#34892;&#20844;&#24179;&#39044;&#27979;&#65292;&#36825;&#38480;&#21046;&#20102;&#26410;&#26469;&#30340;&#20844;&#24179;&#24615;&#25913;&#36827;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20915;&#31574;&#21512;&#29702;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20844;&#24179;&#24615;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#21508;&#20010;&#23376;&#32676;&#20307;&#20013;&#30340;&#31070;&#32463;&#20803;&#24433;&#21709;&#26469;&#23450;&#20041;&#21442;&#25968;&#24179;&#31561;&#24471;&#20998;&#26469;&#34920;&#24449;&#32593;&#32476;&#30340;&#20844;&#24179;&#20915;&#31574;&#36807;&#31243;&#12290;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#20844;&#24179;&#38382;&#39064;&#21487;&#33021;&#28304;&#20110;&#23376;&#32676;&#20307;&#30340;&#19981;&#23545;&#40784;&#20915;&#31574;&#21512;&#29702;&#24615;&#12290;&#29616;&#26377;&#30340;&#20844;&#24179;&#24615;&#35268;&#33539;&#39033;&#26080;&#27861;&#23454;&#29616;&#20915;&#31574;&#21512;&#29702;&#24615;&#30340;&#23545;&#40784;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#32422;&#26463;&#26368;&#21518;&#19968;&#23618;&#30340;&#36755;&#20986;&#65292;&#32780;&#24573;&#35270;&#20102;&#20043;&#21069;&#30340;&#23618;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have made significant progress, but often suffer from fairness issues, as deep models typically show distinct accuracy differences among certain subgroups (e.g., males and females). Existing research addresses this critical issue by employing fairness-aware loss functions to constrain the last-layer outputs and directly regularize DNNs. Although the fairness of DNNs is improved, it is unclear how the trained network makes a fair prediction, which limits future fairness improvements. In this paper, we investigate fairness from the perspective of decision rationale and define the parameter parity score to characterize the fair decision process of networks by analyzing neuron influence in various subgroups. Extensive empirical studies show that the unfair issue could arise from the unaligned decision rationales of subgroups. Existing fairness regularization terms fail to achieve decision rationale alignment because they only constrain last-layer outputs while i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#20559;&#35265;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#22312;&#19968;&#20010;&#29616;&#23454;&#30340;IMDB&#30005;&#24433;&#20998;&#31867;&#22120;&#30340;&#20363;&#23376;&#20013;&#23545;BERT&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20844;&#24320;&#30340;BERT&#27169;&#22411;&#20013;&#23384;&#22312;&#30528;&#26174;&#33879;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#24378;&#35843;&#20102;&#36127;&#36131;&#20219;&#20351;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15298</link><description>&lt;p&gt;
BERT&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#8212;&#8212;&#36890;&#36807;&#24773;&#24863;&#35780;&#20998;&#22312;&#29616;&#23454;&#30340;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#20013;&#27979;&#37327;&#21644;&#20998;&#26512;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Gender Bias in BERT -- Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task. (arXiv:2306.15298v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15298
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#20559;&#35265;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#22312;&#19968;&#20010;&#29616;&#23454;&#30340;IMDB&#30005;&#24433;&#20998;&#31867;&#22120;&#30340;&#20363;&#23376;&#20013;&#23545;BERT&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20844;&#24320;&#30340;BERT&#27169;&#22411;&#20013;&#23384;&#22312;&#30528;&#26174;&#33879;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#24378;&#35843;&#20102;&#36127;&#36131;&#20219;&#20351;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#29616;&#23454;&#24212;&#29992;&#20013;&#20844;&#24320;&#21487;&#29992;&#65292;&#24182;&#19981;&#26029;&#36827;&#34892;&#24494;&#35843;&#12290;&#38543;&#30528;&#23427;&#20204;&#20855;&#22791;&#25235;&#21462;&#22797;&#26434;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#26377;&#23475;&#20559;&#35265;&#24456;&#21487;&#33021;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#36825;&#20123;&#27169;&#22411;&#20132;&#32455;&#22312;&#19968;&#36215;&#12290;&#26412;&#25991;&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#26469;&#20998;&#26512;BERT&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65306;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#35265;&#24230;&#37327;&#26041;&#27861;&#65292;&#23558;&#20559;&#35265;&#23450;&#20041;&#20026;&#22899;&#24615;&#21644;&#30007;&#24615;&#26679;&#26412;&#29256;&#26412;&#22312;&#24773;&#24863;&#35780;&#20272;&#19978;&#30340;&#24046;&#24322;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#29616;&#23454;&#30340;IMDB&#30005;&#24433;&#20998;&#31867;&#22120;&#30340;&#20363;&#23376;&#20013;BERT&#30340;&#20559;&#35265;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#36890;&#36807;&#31995;&#32479;&#22320;&#21464;&#21270;&#35757;&#32451;&#27969;&#31243;&#30340;&#21508;&#20010;&#20803;&#32032;&#65292;&#25105;&#20204;&#21487;&#20197;&#23545;&#26368;&#32456;&#27169;&#22411;&#20559;&#35265;&#30340;&#24433;&#21709;&#20570;&#20986;&#32467;&#35770;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19971;&#20010;&#19981;&#21516;&#30340;&#20844;&#24320;BERT&#27169;&#22411;&#30340;&#20061;&#31181;&#35757;&#32451;&#26465;&#20214;&#65292;&#21363;&#24635;&#20849;63&#20010;&#27169;&#22411;&#12290;&#20960;&#20046;&#25152;&#26377;&#30340;&#26465;&#20214;&#37117;&#20135;&#29983;&#20102;&#26174;&#33879;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21453;&#26144;&#30340;&#20559;&#35265;&#28304;&#20110;&#20844;&#24320;&#30340;BERT&#27169;&#22411;&#32780;&#19981;&#26159;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#65292;&#24378;&#35843;&#20102;&#36127;&#36131;&#20219;&#20351;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models are publicly available and constantly finetuned for various real-life applications. As they become capable of grasping complex contextual information, harmful biases are likely increasingly intertwined with those models. This paper analyses gender bias in BERT models with two main contributions: First, a novel bias measure is introduced, defining biases as the difference in sentiment valuation of female and male sample versions. Second, we comprehensively analyse BERT's biases on the example of a realistic IMDB movie classifier. By systematically varying elements of the training pipeline, we can conclude regarding their impact on the final model bias. Seven different public BERT models in nine training conditions, i.e. 63 models in total, are compared. Almost all conditions yield significant gender biases. Results indicate that reflected biases stem from public BERT models rather than task-specific data, emphasising the weight of responsible usage.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36864;&#28779;&#37325;&#35201;&#24615;&#37319;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#24658;&#23450;&#36895;&#29575;&#31163;&#25955;&#21270;&#36827;&#23637;&#35745;&#21010;&#65292;&#23454;&#29616;&#20102;&#22312;&#36864;&#28779;&#36807;&#31243;&#20013;&#26679;&#26412;&#22312;&#21508;&#20010;&#20998;&#24067;&#20043;&#38388;&#30340;&#33258;&#36866;&#24212;&#31227;&#21160;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#37319;&#26679;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.15283</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#36864;&#28779;&#37325;&#35201;&#24615;&#37319;&#26679;&#19982;&#24658;&#23450;&#36895;&#29575;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Adaptive Annealed Importance Sampling with Constant Rate Progress. (arXiv:2306.15283v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36864;&#28779;&#37325;&#35201;&#24615;&#37319;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#24658;&#23450;&#36895;&#29575;&#31163;&#25955;&#21270;&#36827;&#23637;&#35745;&#21010;&#65292;&#23454;&#29616;&#20102;&#22312;&#36864;&#28779;&#36807;&#31243;&#20013;&#26679;&#26412;&#22312;&#21508;&#20010;&#20998;&#24067;&#20043;&#38388;&#30340;&#33258;&#36866;&#24212;&#31227;&#21160;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#37319;&#26679;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36864;&#28779;&#37325;&#35201;&#24615;&#37319;&#26679;(AIS)&#33021;&#22815;&#22312;&#32473;&#23450;&#38590;&#20197;&#35745;&#31639;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#38750;&#35268;&#33539;&#21270;&#23494;&#24230;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#21512;&#25104;&#21152;&#26435;&#26679;&#26412;&#12290;&#35813;&#31639;&#27861;&#20381;&#36182;&#20110;&#19968;&#31995;&#21015;&#25554;&#20540;&#20998;&#24067;&#65292;&#23558;&#30446;&#26631;&#20998;&#24067;&#19982;&#21021;&#22987;&#26131;&#20110;&#35745;&#31639;&#30340;&#20998;&#24067;&#36827;&#34892;&#34900;&#25509;&#65292;&#20854;&#20013;&#33879;&#21517;&#30340;&#20960;&#20309;&#24179;&#22343;&#36335;&#24452;&#34987;&#35748;&#20026;&#36890;&#24120;&#26159;&#27425;&#20248;&#30340;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#20960;&#20309;&#36864;&#28779;&#26159;&#24403;&#21069;&#31890;&#23376;&#20998;&#24067;&#19982;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;KL&#25955;&#24230;&#26368;&#23567;&#30340;&#36335;&#24452;&#12290;&#25105;&#20204;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25512;&#23548;&#20986;&#20102;&#36825;&#19968;&#36864;&#28779;&#24207;&#21015;&#30340;&#24658;&#23450;&#36895;&#29575;&#31163;&#25955;&#21270;&#36827;&#23637;&#35745;&#21010;&#65292;&#26681;&#25454;&#26679;&#26412;&#22312;&#21021;&#22987;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#31227;&#21160;&#30340;&#38590;&#24230;&#35843;&#25972;&#36827;&#23637;&#35745;&#21010;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#32467;&#26524;&#25512;&#24191;&#33267;f-&#25955;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#36864;&#28779;&#24207;&#21015;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annealed Importance Sampling (AIS) synthesizes weighted samples from an intractable distribution given its unnormalized density function. This algorithm relies on a sequence of interpolating distributions bridging the target to an initial tractable distribution such as the well-known geometric mean path of unnormalized distributions which is assumed to be suboptimal in general. In this paper, we prove that the geometric annealing corresponds to the distribution path that minimizes the KL divergence between the current particle distribution and the desired target when the feasible change in the particle distribution is constrained. Following this observation, we derive the constant rate discretization schedule for this annealing sequence, which adjusts the schedule to the difficulty of moving samples between the initial and the target distributions. We further extend our results to $f$-divergences and present the respective dynamics of annealing sequences based on which we propose the C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22840;&#22823;&#30340;&#35299;&#37322;&#65292;&#23427;&#26159;&#20026;&#20102;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20915;&#31574;&#21407;&#22240;&#25152;&#23450;&#20041;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#35299;&#37322;&#26041;&#27861;&#21482;&#26174;&#31034;&#36873;&#25321;&#30340;&#29305;&#24449;&#23545;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#32780;&#22840;&#22823;&#30340;&#35299;&#37322;&#32771;&#34385;&#20102;&#26356;&#22810;&#29305;&#24449;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15272</link><description>&lt;p&gt;
&#25552;&#20379;&#22840;&#22823;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Delivering Inflated Explanations. (arXiv:2306.15272v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22840;&#22823;&#30340;&#35299;&#37322;&#65292;&#23427;&#26159;&#20026;&#20102;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20915;&#31574;&#21407;&#22240;&#25152;&#23450;&#20041;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#35299;&#37322;&#26041;&#27861;&#21482;&#26174;&#31034;&#36873;&#25321;&#30340;&#29305;&#24449;&#23545;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#32780;&#22840;&#22823;&#30340;&#35299;&#37322;&#32771;&#34385;&#20102;&#26356;&#22810;&#29305;&#24449;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36861;&#27714;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#36807;&#31243;&#20013;&#65292;&#32463;&#24120;&#20986;&#29616;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363; AI &#31995;&#32479;&#20570;&#20986;&#20915;&#31574;&#30340;&#21407;&#22240;&#26159;&#20160;&#20040;&#12290;&#35299;&#37322;&#24615;&#30340;&#27491;&#24335;&#26041;&#27861;&#24314;&#31435;&#20102; AI &#31995;&#32479;&#30340;&#24418;&#24335;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#35813;&#27169;&#22411;&#25512;&#29702;&#31995;&#32479;&#30340;&#29305;&#24615;&#12290;&#32473;&#23450;&#19968;&#20010;&#35201;&#35299;&#37322;&#30340;&#23454;&#20363;&#30340;&#29305;&#24449;&#20540;&#38598;&#21644;&#30456;&#24212;&#30340;&#20915;&#31574;&#65292;&#19968;&#20010;&#24418;&#24335;&#25512;&#29702;&#35299;&#37322;&#26159;&#19968;&#32452;&#29305;&#24449;&#65292;&#22914;&#26524;&#23427;&#20204;&#37319;&#21462;&#32473;&#23450;&#20540;&#65292;&#23558;&#22987;&#32456;&#23548;&#33268;&#21516;&#26679;&#30340;&#20915;&#31574;&#12290;&#36825;&#31181;&#35299;&#37322;&#26159;&#26377;&#29992;&#30340;&#65292;&#23427;&#26174;&#31034;&#21482;&#26377;&#19968;&#20123;&#29305;&#24449;&#34987;&#29992;&#20110;&#20570;&#20986;&#26368;&#32456;&#20915;&#31574;&#12290;&#20294;&#23427;&#26159;&#29421;&#20041;&#30340;&#65292;&#23427;&#21482;&#26174;&#31034;&#22914;&#26524;&#36873;&#25321;&#30340;&#29305;&#24449;&#37319;&#21462;&#23427;&#20204;&#32473;&#23450;&#30340;&#20540;&#65292;&#20915;&#31574;&#23601;&#19981;&#20250;&#25913;&#21464;&#12290;&#21487;&#33021;&#26377;&#20123;&#29305;&#24449;&#30340;&#20540;&#20250;&#25913;&#21464;&#65292;&#20294;&#20173;&#28982;&#23548;&#33268;&#30456;&#21516;&#30340;&#20915;&#31574;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#22840;&#22823;&#30340;&#35299;&#37322;&#65292;&#23427;&#26159;&#19968;&#32452;&#29305;&#24449;&#65292;&#23545;&#20110;&#27599;&#20010;&#29305;&#24449;&#26377;&#19968;&#32452;&#20540;&#65288;&#22987;&#32456;&#21253;&#25324;&#35813;&#29305;&#24449;&#30340;&#20540;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the quest for Explainable Artificial Intelligence (XAI) one of the questions that frequently arises given a decision made by an AI system is, ``why was the decision made in this way?'' Formal approaches to explainability build a formal model of the AI system and use this to reason about the properties of the system. Given a set of feature values for an instance to be explained, and a resulting decision, a formal abductive explanation is a set of features, such that if they take the given value will always lead to the same decision. This explanation is useful, it shows that only some features were used in making the final decision. But it is narrow, it only shows that if the selected features take their given values the decision is unchanged. It's possible that some features may change values and still lead to the same decision. In this paper we formally define inflated explanations which is a set of features, and for each feature of set of values (always including the value of the i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Librispeech&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;Conformer ASR&#31995;&#32479;&#30340;&#36229;&#21442;&#25968;&#36866;&#24212;&#24615;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;DementiaBank&#32769;&#24180;&#20154;&#21644;UASpeech&#21457;&#38899;&#38556;&#30861;&#32773;&#35821;&#38899;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36229;&#21442;&#25968;&#36866;&#24212;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#35789;&#38169;&#35823;&#29575;&#65292;&#24182;&#21457;&#29616;&#20102;&#36229;&#21442;&#25968;&#36866;&#24212;&#24615;&#33021;&#25913;&#36827;&#19982;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#25968;&#25454;&#30340;&#35805;&#35821;&#38271;&#24230;&#27604;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15265</link><description>&lt;p&gt;
&#36866;&#24212;&#32769;&#24180;&#21644;&#21457;&#38899;&#38556;&#30861;&#32773;&#35821;&#38899;&#35782;&#21035;&#30340;Conformer ASR&#31995;&#32479;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Hyper-parameter Adaptation of Conformer ASR Systems for Elderly and Dysarthric Speech Recognition. (arXiv:2306.15265v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Librispeech&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;Conformer ASR&#31995;&#32479;&#30340;&#36229;&#21442;&#25968;&#36866;&#24212;&#24615;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;DementiaBank&#32769;&#24180;&#20154;&#21644;UASpeech&#21457;&#38899;&#38556;&#30861;&#32773;&#35821;&#38899;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36229;&#21442;&#25968;&#36866;&#24212;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#35789;&#38169;&#35823;&#29575;&#65292;&#24182;&#21457;&#29616;&#20102;&#36229;&#21442;&#25968;&#36866;&#24212;&#24615;&#33021;&#25913;&#36827;&#19982;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#25968;&#25454;&#30340;&#35805;&#35821;&#38271;&#24230;&#27604;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35782;&#21035;&#28151;&#20081;&#21644;&#32769;&#24180;&#20154;&#30340;&#35821;&#38899;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#25968;&#25454;&#31232;&#32570;&#12290;&#36890;&#24120;&#20351;&#29992;&#21442;&#25968;&#24494;&#35843;&#26469;&#21033;&#29992;&#22823;&#37327;&#30340;&#38750;&#32769;&#24180;&#21644;&#20581;&#24247;&#35821;&#38899;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#36229;&#21442;&#25968;&#21017;&#20351;&#29992;&#19987;&#23478;&#30693;&#35782;&#35774;&#32622;&#24182;&#20445;&#25345;&#19981;&#21464;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Librispeech&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;Conformer ASR&#31995;&#32479;&#30340;&#36229;&#21442;&#25968;&#36866;&#24212;&#24615;&#65292;&#28982;&#21518;&#23558;&#20854;&#39046;&#22495;&#36866;&#24212;&#21040;DementiaBank&#32769;&#24180;&#20154;&#21644;UASpeech&#21457;&#38899;&#38556;&#30861;&#32773;&#35821;&#38899;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20165;&#21442;&#25968;&#24494;&#35843;&#30456;&#27604;&#65292;&#36229;&#21442;&#25968;&#36866;&#24212;&#22312;DBank&#21644;UASpeech&#20219;&#21153;&#20013;&#20998;&#21035;&#20943;&#23569;&#20102;0.45%&#21644;0.67%&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#12290;&#22312;&#36229;&#21442;&#25968;&#39046;&#22495;&#36866;&#24212;&#30340;&#24615;&#33021;&#25913;&#36827;&#21644;&#28304;&#22495;&#25968;&#25454;&#19982;&#30446;&#26631;&#22495;&#25968;&#25454;&#30340;&#30456;&#23545;&#35805;&#35821;&#38271;&#24230;&#27604;&#20043;&#38388;&#23384;&#22312;&#30452;&#35266;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic recognition of disordered and elderly speech remains highly challenging tasks to date due to data scarcity. Parameter fine-tuning is often used to exploit the large quantities of non-aged and healthy speech pre-trained models, while neural architecture hyper-parameters are set using expert knowledge and remain unchanged. This paper investigates hyper-parameter adaptation for Conformer ASR systems that are pre-trained on the Librispeech corpus before being domain adapted to the DementiaBank elderly and UASpeech dysarthric speech datasets. Experimental results suggest that hyper-parameter adaptation produced word error rate (WER) reductions of 0.45% and 0.67% over parameter-only fine-tuning on DBank and UASpeech tasks respectively. An intuitive correlation is found between the performance improvements by hyper-parameter domain adaptation and the relative utterance length ratio between the source and target domain data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#25277;&#26679;&#38543;&#26426;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#30340;&#24179;&#28369;&#20998;&#24067;&#25913;&#21892;&#20102;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#35748;&#35777;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15221</link><description>&lt;p&gt;
[Re]&#21452;&#25277;&#26679;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
[Re] Double Sampling Randomized Smoothing. (arXiv:2306.15221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15221
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#25277;&#26679;&#38543;&#26426;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#30340;&#24179;&#28369;&#20998;&#24067;&#25913;&#21892;&#20102;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#35748;&#35777;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21487;&#37325;&#22797;&#24615;&#25361;&#25112;&#30340;&#19968;&#20010;&#36129;&#29486;&#65292;&#29305;&#21035;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;(NNs)&#23545;&#25239;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#35748;&#35777;&#38382;&#39064;&#12290;&#25552;&#20986;&#30340;&#21452;&#25277;&#26679;&#38543;&#26426;&#24179;&#28369;(DSRS)&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#30340;&#24179;&#28369;&#20998;&#24067;&#26469;&#25913;&#21892;&#40065;&#26834;&#24615;&#35748;&#35777;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#28165;&#26224;&#22320;&#38416;&#36848;&#20102;DSRS&#22312;&#24191;&#20041;&#39640;&#26031;&#24179;&#28369;&#26063;&#30340;&#26696;&#20363;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#23454;&#29616;&#26041;&#27861;&#12290;&#22312;MNIST&#21644;CIFAR-10&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;DSRS&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#65292;&#22987;&#32456;&#35777;&#26126;&#20102;&#26356;&#22823;&#30340;&#40065;&#26834;&#21322;&#24452;&#12290;&#27492;&#22806;&#65292;&#36824;&#36827;&#34892;&#20102;&#21508;&#31181;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#36827;&#19968;&#27493;&#20998;&#26512;&#36229;&#21442;&#25968;&#21644;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#23545;&#25152;&#25552;&#20986;&#26694;&#26550;&#25152;&#35748;&#35777;&#21322;&#24452;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is a contribution to the reproducibility challenge in the field of machine learning, specifically addressing the issue of certifying the robustness of neural networks (NNs) against adversarial perturbations. The proposed Double Sampling Randomized Smoothing (DSRS) framework overcomes the limitations of existing methods by using an additional smoothing distribution to improve the robustness certification. The paper provides a clear manifestation of DSRS for a generalized family of Gaussian smoothing and a computationally efficient method for implementation. The experiments on MNIST and CIFAR-10 demonstrate the effectiveness of DSRS, consistently certifying larger robust radii compared to other methods. Also various ablations studies are conducted to further analyze the hyperparameters and effect of adversarial training methods on the certified radius by the proposed framework.
&lt;/p&gt;</description></item><item><title>S-TLLR&#26159;&#19968;&#20010;&#21463;&#21040;STDP&#26426;&#21046;&#21551;&#21457;&#30340;&#26102;&#38388;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#65292;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#20102;&#22240;&#26524;&#21644;&#38750;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.15220</link><description>&lt;p&gt;
S-TLLR: &#21463;&#21040;&#26102;&#38388;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#30340;STDP&#21551;&#21457;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
S-TLLR: STDP-inspired Temporal Local Learning Rule for Spiking Neural Networks. (arXiv:2306.15220v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15220
&lt;/p&gt;
&lt;p&gt;
S-TLLR&#26159;&#19968;&#20010;&#21463;&#21040;STDP&#26426;&#21046;&#21551;&#21457;&#30340;&#26102;&#38388;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#65292;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#20102;&#22240;&#26524;&#21644;&#38750;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26159;&#21487;&#29992;&#20110;&#36793;&#32536;&#26234;&#33021;&#30340;&#29983;&#29289;&#23398;&#21512;&#29702;&#27169;&#22411;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#39034;&#24207;&#23398;&#20064;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;SNN&#30340;&#35757;&#32451;&#38754;&#20020;&#30528;&#31934;&#30830;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#29992;&#20998;&#37197;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;BPTT&#31639;&#27861;&#26159;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#20854;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#23427;&#20135;&#29983;&#20102;&#36739;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;BPTT&#21450;&#20854;&#36817;&#20284;&#20165;&#21033;&#29992;&#20174;&#33033;&#20914;&#27963;&#21160;&#20013;&#23548;&#20986;&#30340;&#22240;&#26524;&#20449;&#24687;&#26469;&#35745;&#31639;&#31361;&#35302;&#26356;&#26032;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#38750;&#22240;&#26524;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;S-TLLR&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;Spike-Timing Dependent Plasticity&#65288;STDP&#65289;&#26426;&#21046;&#21551;&#21457;&#30340;&#26032;&#22411;&#19977;&#22240;&#32032;&#26102;&#38388;&#23616;&#37096;&#23398;&#20064;&#35268;&#21017;&#65292;&#26088;&#22312;&#29992;&#20110;&#20107;&#20214;&#39537;&#21160;&#23398;&#20064;&#20219;&#21153;&#30340;SNN&#35757;&#32451;&#12290;S-TLLR&#21516;&#26102;&#32771;&#34385;&#20102;&#21069;&#21518;&#31361;&#35302;&#20043;&#38388;&#30340;&#22240;&#26524;&#21644;&#38750;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) are biologically plausible models that have been identified as potentially apt for the deployment for energy-efficient intelligence at the edge, particularly for sequential learning tasks. However, training of SNNs poses a significant challenge due to the necessity for precise temporal and spatial credit assignment. Back-propagation through time (BPTT) algorithm, whilst being the most widely used method for addressing these issues, incurs a high computational cost due to its temporal dependency. Moreover, BPTT and its approximations solely utilize causal information derived from the spiking activity to compute the synaptic updates, thus neglecting non-causal relationships. In this work, we propose S-TLLR, a novel three-factor temporal local learning rule inspired by the Spike-Timing Dependent Plasticity (STDP) mechanism, aimed at training SNNs on event-based learning tasks. S-TLLR considers both causal and non-causal relationships between pre and post-syn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#35299;&#20915;&#27809;&#26377;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#23427;&#20204;&#20805;&#20998;&#21033;&#29992;&#25152;&#26377;&#33410;&#28857;&#20449;&#24687;&#65292;&#24182;&#19988;&#36890;&#36807;&#27867;&#21270;&#33021;&#21147;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15217</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#29992;&#20110;&#22270;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Episode Generation for Graph Meta-learning. (arXiv:2306.15217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#35299;&#20915;&#27809;&#26377;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#23427;&#20204;&#20805;&#20998;&#21033;&#29992;&#25152;&#26377;&#33410;&#28857;&#20449;&#24687;&#65292;&#24182;&#19988;&#36890;&#36807;&#27867;&#21270;&#33021;&#21147;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#26469;&#35299;&#20915;&#27809;&#26377;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#20027;&#27969;&#30340;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#26159;&#22312;&#23384;&#22312;&#22823;&#37327;&#26377;&#26631;&#31614;&#33410;&#28857;&#29992;&#20110;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#24320;&#21457;&#30340;&#65292;&#28982;&#32780;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#36825;&#26679;&#30340;&#25968;&#25454;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#35299;&#20915;&#26631;&#31614;&#31232;&#32570;&#24615;&#38382;&#39064;&#30340;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#20381;&#36182;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#26377;&#26631;&#31614;&#25968;&#25454;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#22270;&#20013;&#25152;&#26377;&#33410;&#28857;&#20449;&#24687;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#23613;&#31649;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#19978;&#24456;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#23398;&#20064;&#36890;&#29992;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#27809;&#26377;&#32771;&#34385;&#35201;&#35299;&#20915;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#20854;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#30417;&#30563;&#30340;&#21095;&#38598;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#21033;&#29992;&#23427;&#20204;&#22312;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#35299;&#20915;&#26631;&#31614;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#22686;&#24378;&#26041;&#27861;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate Unsupervised Episode Generation methods to solve Few-Shot Node-Classification (FSNC) problem via Meta-learning without labels. Dominant meta-learning methodologies for FSNC were developed under the existence of abundant labeled nodes for training, which however may not be possible to obtain in the real-world. Although few studies have been proposed to tackle the label-scarcity problem, they still rely on a limited amount of labeled data, which hinders the full utilization of the information of all nodes in a graph. Despite the effectiveness of Self-Supervised Learning (SSL) approaches on FSNC without labels, they mainly learn generic node embeddings without consideration on the downstream task to be solved, which may limit its performance. In this work, we propose unsupervised episode generation methods to benefit from their generalization ability for FSNC tasks while resolving label-scarcity problem. We first propose a method that utilizes graph augmentat
&lt;/p&gt;</description></item><item><title>TranssionADD&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#24103;&#24378;&#21270;&#23398;&#20064;&#30340;&#24207;&#21015;&#26631;&#27880;&#27169;&#22411;&#65292;&#29992;&#20110;&#38899;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#65292;&#36890;&#36807;&#36866;&#24212;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#12289;&#25913;&#36827;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#21644;&#24341;&#20837;&#22810;&#24103;&#26816;&#27979;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#38899;&#39057;&#29255;&#27573;&#24322;&#24120;&#20540;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.15212</link><description>&lt;p&gt;
TranssionADD:&#19968;&#31181;&#22522;&#20110;&#22810;&#24103;&#24378;&#21270;&#23398;&#20064;&#30340;&#24207;&#21015;&#26631;&#27880;&#27169;&#22411;&#29992;&#20110;&#38899;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
TranssionADD: A multi-frame reinforcement based sequence tagging model for audio deepfake detection. (arXiv:2306.15212v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15212
&lt;/p&gt;
&lt;p&gt;
TranssionADD&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#24103;&#24378;&#21270;&#23398;&#20064;&#30340;&#24207;&#21015;&#26631;&#27880;&#27169;&#22411;&#65292;&#29992;&#20110;&#38899;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#65292;&#36890;&#36807;&#36866;&#24212;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#12289;&#25913;&#36827;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#21644;&#24341;&#20837;&#22810;&#24103;&#26816;&#27979;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#38899;&#39057;&#29255;&#27573;&#24322;&#24120;&#20540;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26368;&#36817;&#31471;&#21040;&#31471;&#35821;&#38899;&#24314;&#27169;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#27169;&#20223;&#21644;&#20811;&#38534;&#29992;&#25143;&#30340;&#22768;&#38899;&#21464;&#24471;&#36234;&#26469;&#36234;&#21487;&#34892;&#12290;&#36825;&#23548;&#33268;&#21306;&#20998;&#30495;&#23454;&#21644;&#20266;&#36896;&#38899;&#39057;&#29255;&#27573;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#29992;&#25143;&#22768;&#38899;&#28389;&#29992;&#21644;&#35823;&#29992;&#30340;&#38382;&#39064;&#65292;&#31532;&#20108;&#23626;&#38899;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#25361;&#25112;&#65288;ADD 2023&#65289;&#26088;&#22312;&#26816;&#27979;&#21644;&#20998;&#26512;&#20266;&#36896;&#30340;&#35821;&#38899;&#34920;&#36798;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Track 2&#65292;&#21517;&#20026;&#25805;&#32437;&#21306;&#22495;&#23450;&#20301;&#65288;RL&#65289;&#65292;&#26088;&#22312;&#20934;&#30830;&#23450;&#20301;&#38899;&#39057;&#20013;&#34987;&#25805;&#32437;&#30340;&#21306;&#22495;&#65292;&#36825;&#20123;&#21306;&#22495;&#21487;&#33021;&#23384;&#22312;&#20110;&#30495;&#23454;&#21644;&#29983;&#25104;&#30340;&#38899;&#39057;&#29255;&#27573;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TranssionADD&#30340;&#26032;&#39062;&#31995;&#32479;&#65292;&#20316;&#20026;&#23545;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#38899;&#39057;&#29255;&#27573;&#24322;&#24120;&#20540;&#30340;&#36861;&#36394;&#31454;&#36187;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#25552;&#20379;&#20102;&#19977;&#20010;&#29420;&#29305;&#30340;&#36129;&#29486;&#65306;1&#65289;&#25105;&#20204;&#23558;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#24212;&#29992;&#20110;&#38899;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#65307;2&#65289;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65307;3&#65289;&#25105;&#20204;&#32467;&#21512;&#22810;&#24103;&#26816;&#27979;&#65288;MFD&#65289;&#25913;&#36827;&#20102;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thanks to recent advancements in end-to-end speech modeling technology, it has become increasingly feasible to imitate and clone a user`s voice. This leads to a significant challenge in differentiating between authentic and fabricated audio segments. To address the issue of user voice abuse and misuse, the second Audio Deepfake Detection Challenge (ADD 2023) aims to detect and analyze deepfake speech utterances. Specifically, Track 2, named the Manipulation Region Location (RL), aims to pinpoint the location of manipulated regions in audio, which can be present in both real and generated audio segments. We propose our novel TranssionADD system as a solution to the challenging problem of model robustness and audio segment outliers in the trace competition. Our system provides three unique contributions: 1) we adapt sequence tagging task for audio deepfake detection; 2) we improve model generalization by various data augmentation techniques; 3) we incorporate multi-frame detection (MFD) 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#38745;&#24687;&#24577;&#21407;&#22987;&#33041;&#30005;&#20449;&#21495;&#20013;&#26816;&#27979;&#24930;&#24615;&#30140;&#30171;&#65292;&#35813;&#31639;&#27861;&#22312;&#31867;&#21035;&#21487;&#20998;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#24182;&#19988;&#22312;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.15194</link><description>&lt;p&gt;
&#20351;&#29992;&#25913;&#36827;&#30340;&#29305;&#24449;&#36873;&#25321;&#20174;&#38745;&#24687;&#24577;&#21407;&#22987;&#33041;&#30005;&#20449;&#21495;&#20013;&#26816;&#27979;&#24930;&#24615;&#30140;&#30171;
&lt;/p&gt;
&lt;p&gt;
Chronic pain detection from resting-state raw EEG signals using improved feature selection. (arXiv:2306.15194v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#38745;&#24687;&#24577;&#21407;&#22987;&#33041;&#30005;&#20449;&#21495;&#20013;&#26816;&#27979;&#24930;&#24615;&#30140;&#30171;&#65292;&#35813;&#31639;&#27861;&#22312;&#31867;&#21035;&#21487;&#20998;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#24182;&#19988;&#22312;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#38745;&#24687;&#24577;&#21407;&#22987;&#33041;&#30005;&#25968;&#25454;&#20013;&#26816;&#27979;&#24930;&#24615;&#30140;&#30171;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;-&#25913;&#36827;&#30340;&#39034;&#24207;&#28014;&#21160;&#21069;&#21521;&#36873;&#25321;&#65288;mSFFS&#65289;&#12290;&#25913;&#36827;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#26696;&#34429;&#28982;&#36739;&#20026;&#31616;&#27905;&#65292;&#20294;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#36825;&#30001;Bhattacharyya&#36317;&#31163;&#24230;&#37327;&#21644;&#26356;&#22909;&#30340;&#21487;&#35270;&#21270;&#32467;&#26524;&#25152;&#31034;&#12290;&#23427;&#20063;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#29983;&#25104;&#30340;&#36873;&#25321;&#65292;&#23558;&#27979;&#35797;&#20934;&#30830;&#29575;&#25552;&#39640;&#21040;97.5&#65285;&#65292;&#24182;&#22312;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#24930;&#24615;&#30140;&#30171;&#30340;&#22806;&#37096;&#25968;&#25454;&#38598;&#19978;&#20135;&#29983;81.4&#65285;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an automatic approach that works on resting-state raw EEG data for chronic pain detection. A new feature selection algorithm - modified Sequential Floating Forward Selection (mSFFS) - is proposed. The improved feature selection scheme is rather compact but displays better class separability as indicated by the Bhattacharyya distance measures and better visualization results. It also outperforms selections generated by other benchmark methods, boosting the test accuracy to 97.5% and yielding a test accuracy of 81.4% on an external dataset that contains different types of chronic pain
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21069;&#21521;&#35757;&#32451;&#26041;&#24335;&#19979;&#35757;&#32451;&#28145;&#24230;&#21333;&#31867;&#30446;&#26631;&#20989;&#25968;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#36825;&#20123;&#20989;&#25968;&#21487;&#20197;&#22788;&#29702;&#21160;&#24577;&#32593;&#32476;&#22823;&#23567;&#65292;&#20026;&#26080;&#32541;&#22312;&#32447;&#35757;&#32451;&#24102;&#26469;&#20102;&#35768;&#22810;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2306.15188</link><description>&lt;p&gt;
&#19968;&#31867;&#31995;&#32479;&#23436;&#32654;&#36866;&#29992;&#20110;&#21069;&#21521;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
One-class systems seamlessly fit in the forward-forward algorithm. (arXiv:2306.15188v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21069;&#21521;&#35757;&#32451;&#26041;&#24335;&#19979;&#35757;&#32451;&#28145;&#24230;&#21333;&#31867;&#30446;&#26631;&#20989;&#25968;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#36825;&#20123;&#20989;&#25968;&#21487;&#20197;&#22788;&#29702;&#21160;&#24577;&#32593;&#32476;&#22823;&#23567;&#65292;&#20026;&#26080;&#32541;&#22312;&#32447;&#35757;&#32451;&#24102;&#26469;&#20102;&#35768;&#22810;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#21521;&#31639;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26356;&#26032;&#26435;&#37325;&#65292;&#36880;&#23618;&#36827;&#34892;&#21442;&#25968;&#26356;&#26032;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#31435;&#21363;&#38477;&#20302;&#20102;&#20869;&#23384;&#38656;&#27714;&#65292;&#21487;&#33021;&#24102;&#26469;&#26356;&#22810;&#22909;&#22788;&#65292;&#27604;&#22914;&#26080;&#32541;&#22312;&#32447;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#25439;&#22833;&#65288;&#8220;&#22909;&#24230;&#8221;&#65289;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#21487;&#20197;&#22312;&#27599;&#20010;&#23618;&#30340;&#28608;&#27963;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#36825;&#20123;&#23618;&#30340;&#21442;&#25968;&#22823;&#23567;&#21487;&#20197;&#26681;&#25454;&#32593;&#32476;&#30340;&#36229;&#21442;&#25968;&#21270;&#32780;&#21464;&#21270;&#12290;&#22312;&#24320;&#21019;&#24615;&#35770;&#25991;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22909;&#24230;&#20989;&#25968;&#26469;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65307;&#28982;&#32780;&#65292;&#22914;&#26524;&#23558;&#20854;&#32622;&#20110;&#19968;&#20010;&#21333;&#31867;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#65292;&#23601;&#26080;&#38656;&#24320;&#21019;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22240;&#20026;&#36825;&#20123;&#20989;&#25968;&#26412;&#36523;&#21487;&#20197;&#22788;&#29702;&#21160;&#24577;&#32593;&#32476;&#22823;&#23567;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21069;&#21521;&#35757;&#32451;&#26041;&#24335;&#19979;&#35757;&#32451;&#28145;&#24230;&#21333;&#31867;&#30446;&#26631;&#20989;&#25968;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#20197;&#22312; \url{https://github.com/MichaelHopwood/ForwardForwardOneclass} &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The forward-forward algorithm presents a new method of training neural networks by updating weights during an inference, performing parameter updates for each layer individually. This immediately reduces memory requirements during training and may lead to many more benefits, like seamless online training. This method relies on a loss ("goodness") function that can be evaluated on the activations of each layer, of which can have a varied parameter size, depending on the hyperparamaterization of the network. In the seminal paper, a goodness function was proposed to fill this need; however, if placed in a one-class problem context, one need not pioneer a new loss because these functions can innately handle dynamic network sizes. In this paper, we investigate the performance of deep one-class objective functions when trained in a forward-forward fashion. The code is available at \url{https://github.com/MichaelHopwood/ForwardForwardOneclass}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;AutoTruss&#26694;&#26550;&#65292;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#39640;&#25928;&#29983;&#25104;&#36731;&#22411;&#21644;&#21512;&#27861;&#30340;&#26689;&#26550;&#24067;&#23616;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;AutoTruss&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#25253;&#36947;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.15182</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#26689;&#26550;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Automatic Truss Design with Reinforcement Learning. (arXiv:2306.15182v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;AutoTruss&#26694;&#26550;&#65292;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#39640;&#25928;&#29983;&#25104;&#36731;&#22411;&#21644;&#21512;&#27861;&#30340;&#26689;&#26550;&#24067;&#23616;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;AutoTruss&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#25253;&#36947;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26689;&#26550;&#24067;&#23616;&#35774;&#35745;&#26159;&#24314;&#31569;&#34892;&#19994;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#23547;&#25214;&#19968;&#20010;&#28385;&#36275;&#25152;&#26377;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#30340;&#36731;&#22411;&#26689;&#26550;&#24067;&#23616;&#12290;&#29983;&#25104;&#26368;&#20248;&#24067;&#23616;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#31351;&#20030;&#25628;&#32034;&#27714;&#35299;&#21487;&#33021;&#38750;&#24120;&#26114;&#36149;&#12290;&#23558;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#26689;&#26550;&#24067;&#23616;&#35774;&#35745;&#20063;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#22312;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#19979;&#21482;&#26377;&#19968;&#20010;&#24456;&#23567;&#30340;&#24067;&#23616;&#31354;&#38388;&#26159;&#26377;&#25928;&#30340;&#65292;&#23548;&#33268;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#22870;&#21169;&#38750;&#24120;&#31232;&#30095;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;AutoTruss&#65292;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#29983;&#25104;&#36731;&#22411;&#21644;&#21512;&#27861;&#30340;&#26689;&#26550;&#24067;&#23616;&#12290;AutoTruss&#39318;&#20808;&#37319;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#26469;&#21457;&#29616;&#22810;&#26679;&#21270;&#30340;&#21512;&#27861;&#24067;&#23616;&#65292;&#28982;&#21518;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#36880;&#27493;&#20248;&#21270;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;2D&#21644;3D&#35774;&#32622;&#19979;&#30340;&#27969;&#34892;&#26689;&#26550;&#24067;&#23616;&#35774;&#35745;&#27979;&#35797;&#26696;&#20363;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#12290;AutoTruss&#22312;&#34920;&#29616;&#19978;&#36229;&#36807;&#20102;&#26368;&#20339;&#25253;&#36947;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Truss layout design, namely finding a lightweight truss layout satisfying all the physical constraints, is a fundamental problem in the building industry. Generating the optimal layout is a challenging combinatorial optimization problem, which can be extremely expensive to solve by exhaustive search. Directly applying end-to-end reinforcement learning (RL) methods to truss layout design is infeasible either, since only a tiny portion of the entire layout space is valid under the physical constraints, leading to particularly sparse rewards for RL training. In this paper, we develop AutoTruss, a two-stage framework to efficiently generate both lightweight and valid truss layouts. AutoTruss first adopts Monte Carlo tree search to discover a diverse collection of valid layouts. Then RL is applied to iteratively refine the valid solutions. We conduct experiments and ablation studies in popular truss layout design test cases in both 2D and 3D settings. AutoTruss outperforms the best-reported
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#25512;&#29702;&#32467;&#26500;&#22312;&#31070;&#32463;&#36807;&#31243;&#20013;&#36827;&#34892;&#25193;&#23637;&#12290;&#25105;&#20204;&#20026;NPs&#30340;&#28508;&#21464;&#37327;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#20808;&#39564;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#24403;&#30340;&#19978;&#19979;&#25991;&#38598;&#32858;&#21512;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#19968;&#31181;&#28040;&#24687;&#20256;&#36882;&#36807;&#31243;&#65292;&#21487;&#20197;&#36827;&#34892;&#31471;&#21040;&#31471;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#28151;&#21512;&#21644;&#23398;&#29983;-t&#20551;&#35774;&#25913;&#21892;&#20102;&#20989;&#25968;&#24314;&#27169;&#21644;&#27979;&#35797;&#26102;&#38388;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15169</link><description>&lt;p&gt;
&#21033;&#29992;&#25512;&#29702;&#32467;&#26500;&#22312;&#31070;&#32463;&#36807;&#31243;&#20013;&#36827;&#34892;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Exploiting Inferential Structure in Neural Processes. (arXiv:2306.15169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15169
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#25512;&#29702;&#32467;&#26500;&#22312;&#31070;&#32463;&#36807;&#31243;&#20013;&#36827;&#34892;&#25193;&#23637;&#12290;&#25105;&#20204;&#20026;NPs&#30340;&#28508;&#21464;&#37327;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#20808;&#39564;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#24403;&#30340;&#19978;&#19979;&#25991;&#38598;&#32858;&#21512;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#19968;&#31181;&#28040;&#24687;&#20256;&#36882;&#36807;&#31243;&#65292;&#21487;&#20197;&#36827;&#34892;&#31471;&#21040;&#31471;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#28151;&#21512;&#21644;&#23398;&#29983;-t&#20551;&#35774;&#25913;&#21892;&#20102;&#20989;&#25968;&#24314;&#27169;&#21644;&#27979;&#35797;&#26102;&#38388;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36807;&#31243;(NPs)&#30001;&#20110;&#20854;&#33021;&#22815;&#22522;&#20110;&#19978;&#19979;&#25991;&#38598;&#25191;&#34892;&#24555;&#36895;&#36866;&#24212;&#32780;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;&#36825;&#20010;&#38598;&#21512;&#30001;&#19968;&#20010;&#28508;&#21464;&#37327;&#32534;&#30721;&#65292;&#36890;&#24120;&#20551;&#35774;&#35813;&#21464;&#37327;&#36981;&#24490;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#19978;&#19979;&#25991;&#38598;&#21487;&#33021;&#26469;&#33258;&#20855;&#26377;&#22810;&#20010;&#27169;&#24335;&#12289;&#37325;&#23614;&#31561;&#20016;&#23500;&#20998;&#24067;&#30340;&#25277;&#26679;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20801;&#35768;&#32473;&#20104;NPs&#28508;&#21464;&#37327;&#19968;&#20010;&#30001;&#22270;&#27169;&#22411;&#23450;&#20041;&#30340;&#20016;&#23500;&#20808;&#39564;&#12290;&#36825;&#20123;&#20998;&#24067;&#20551;&#35774;&#30452;&#25509;&#36716;&#21270;&#20026;&#36866;&#21512;&#19978;&#19979;&#25991;&#38598;&#30340;&#36866;&#24403;&#32858;&#21512;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#28040;&#24687;&#20256;&#36882;&#36807;&#31243;&#65292;&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#36827;&#34892;&#31471;&#21040;&#31471;&#20248;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#28151;&#21512;&#21644;&#23398;&#29983;-t&#20551;&#35774;&#26469;&#35777;&#26126;&#25105;&#20204;&#26694;&#26550;&#30340;&#26222;&#36866;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#20989;&#25968;&#24314;&#27169;&#21644;&#27979;&#35797;&#26102;&#38388;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Processes (NPs) are appealing due to their ability to perform fast adaptation based on a context set. This set is encoded by a latent variable, which is often assumed to follow a simple distribution. However, in real-word settings, the context set may be drawn from richer distributions having multiple modes, heavy tails, etc. In this work, we provide a framework that allows NPs' latent variable to be given a rich prior defined by a graphical model. These distributional assumptions directly translate into an appropriate aggregation strategy for the context set. Moreover, we describe a message-passing procedure that still allows for end-to-end optimization with stochastic gradients. We demonstrate the generality of our framework by using mixture and Student-t assumptions that yield improvements in function modelling and test-time robustness.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26426;&#21046;&#65292;&#21033;&#29992;&#21253;&#21547;&#26080;&#25928;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#32467;&#26524;&#30340;&#31934;&#24230;&#21644;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#21482;&#20351;&#29992;&#26377;&#25928;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#30340;&#26631;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#22522;&#20110;&#26080;&#25928;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#26631;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.15166</link><description>&lt;p&gt;
&#20174;&#26080;&#25928;&#25968;&#25454;&#20013;&#23398;&#20064;&#65306;&#20851;&#20110;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning from Invalid Data: On Constraint Satisfaction in Generative Models. (arXiv:2306.15166v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26426;&#21046;&#65292;&#21033;&#29992;&#21253;&#21547;&#26080;&#25928;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#32467;&#26524;&#30340;&#31934;&#24230;&#21644;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#21482;&#20351;&#29992;&#26377;&#25928;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#30340;&#26631;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#22522;&#20110;&#26080;&#25928;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#26631;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#35821;&#38899;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26377;&#22823;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#23427;&#20204;&#20173;&#28982;&#22312;&#31934;&#24230;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#29983;&#25104;&#20986;&#29289;&#29702;&#19978;&#26080;&#25928;&#25110;&#20107;&#23454;&#19978;&#19981;&#27491;&#30830;&#30340;&#25968;&#25454;&#12290;&#24403;&#29983;&#25104;&#30340;&#25968;&#25454;&#24517;&#39035;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#26102;&#65292;&#36825;&#19968;&#38382;&#39064;&#23588;&#20026;&#20005;&#37325;&#65292;&#20363;&#22914;&#65292;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#28385;&#36275;&#20135;&#21697;&#35268;&#26684;&#25110;&#32773;&#22312;&#33258;&#28982;&#22330;&#26223;&#20013;&#36981;&#23432;&#29289;&#29702;&#23450;&#24459;&#12290;&#20026;&#20102;&#25552;&#39640;&#31934;&#24230;&#24182;&#20445;&#25345;&#22810;&#26679;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26426;&#21046;&#65292;&#21033;&#29992;&#21253;&#21547;&#26080;&#25928;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26368;&#23567;&#21270;&#20102;&#29983;&#25104;&#20998;&#24067;&#19982;&#26377;&#25928;&#20808;&#39564;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#20102;&#19982;&#26080;&#25928;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#23558;GAN&#21644;DDPM&#31561;&#29983;&#25104;&#27169;&#22411;&#19982;&#26080;&#25928;&#25968;&#25454;&#19968;&#36215;&#35757;&#32451;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#20165;&#20351;&#29992;&#26377;&#25928;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#30340;&#26631;&#20934;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#35757;&#32451;&#36807;&#31243;&#29983;&#25104;&#20102;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Generative models have demonstrated impressive results in vision, language, and speech. However, even with massive datasets, they struggle with precision, generating physically invalid or factually incorrect data. This is particularly problematic when the generated data must satisfy constraints, for example, to meet product specifications in engineering design or to adhere to the laws of physics in a natural scene. To improve precision while preserving diversity and fidelity, we propose a novel training mechanism that leverages datasets of constraint-violating data points, which we consider invalid. Our approach minimizes the divergence between the generative distribution and the valid prior while maximizing the divergence with the invalid distribution. We demonstrate how generative models like GANs and DDPMs that we augment to train with invalid data vastly outperform their standard counterparts which solely train on valid data points. For example, our training procedure generates up 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;DSRM&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20998;&#24067;&#20559;&#31227;&#39118;&#38505;&#32780;&#19981;&#26159;&#20351;&#29992;&#23545;&#25239;&#26679;&#26412;&#26469;&#23545;&#25239;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20943;&#23569;&#20102;&#26102;&#38388;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2306.15164</link><description>&lt;p&gt;
&#29992;&#20998;&#24067;&#20559;&#31227;&#39118;&#38505;&#26368;&#23567;&#21270;&#22686;&#24378;&#25991;&#26412;&#23545;&#25239;&#35757;&#32451;&#65288;DSRM&#65289;
&lt;/p&gt;
&lt;p&gt;
DSRM: Boost Textual Adversarial Training with Distribution Shift Risk Minimization. (arXiv:2306.15164v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;DSRM&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20998;&#24067;&#20559;&#31227;&#39118;&#38505;&#32780;&#19981;&#26159;&#20351;&#29992;&#23545;&#25239;&#26679;&#26412;&#26469;&#23545;&#25239;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20943;&#23569;&#20102;&#26102;&#38388;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#25913;&#21892;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#26368;&#20339;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#40065;&#26834;&#27169;&#22411;&#30340;&#20195;&#20215;&#26159;&#26102;&#38388;&#28040;&#32791;&#39640;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#22810;&#27493;&#26799;&#24230;&#19978;&#21319;&#25110;&#21333;&#35789;&#26367;&#25442;&#26469;&#33719;&#21462;&#23545;&#25239;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#29983;&#25104;&#30340;&#26679;&#26412;&#22312;&#35821;&#27861;&#36136;&#37327;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#65292;&#24433;&#21709;&#20102;&#23545;&#25239;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#26377;&#25928;&#30340;&#36807;&#31243;&#65292;&#23558;&#23545;&#25239;&#35757;&#32451;&#25913;&#20026;&#21482;&#20351;&#29992;&#24178;&#20928;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;DSRM&#36890;&#36807;&#25200;&#21160;&#36755;&#20837;&#25968;&#25454;&#30340;&#27010;&#29575;&#20998;&#24067;&#32780;&#19981;&#26159;&#23427;&#20204;&#30340;&#23884;&#20837;&#26469;&#20272;&#35745;&#23545;&#25239;&#25439;&#22833;&#12290;&#36825;&#31181;&#20844;&#24335;&#21270;&#32467;&#26524;&#23548;&#33268;&#20102;&#19968;&#20010;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#26368;&#23567;&#21270;&#26399;&#26395;&#20840;&#23616;&#25439;&#22833;&#30340;&#40065;&#26834;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#23545;&#25239;&#26679;&#26412;&#65292;&#24182;&#19988;&#19982;&#24403;&#21069;&#26368;&#20339;&#23545;&#25239;&#35757;&#32451;&#30456;&#27604;&#65292;&#20943;&#23569;&#20102;&#39640;&#36798;70\%&#30340;&#26102;&#38388;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training is one of the best-performing methods in improving the robustness of deep language models. However, robust models come at the cost of high time consumption, as they require multi-step gradient ascents or word substitutions to obtain adversarial samples. In addition, these generated samples are deficient in grammatical quality and semantic consistency, which impairs the effectiveness of adversarial training. To address these problems, we introduce a novel, effective procedure for instead adversarial training with only clean data. Our procedure, distribution shift risk minimization (DSRM), estimates the adversarial loss by perturbing the input data's probability distribution rather than their embeddings. This formulation results in a robust model that minimizes the expected global loss under adversarial attacks. Our approach requires zero adversarial samples for training and reduces time consumption by up to 70\% compared to current best-performing adversarial traini
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#19968;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#21644;&#26465;&#20214;&#20998;&#24067;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23398;&#20064;&#26694;&#26550;&#21516;&#26102;&#20272;&#35745;&#22238;&#24402;&#20989;&#25968;&#21644;&#26465;&#20214;&#29983;&#25104;&#22120;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#26465;&#20214;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#22810;&#20803;&#36755;&#20986;&#21644;&#21327;&#21464;&#37327;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#26500;&#24314;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20445;&#35777;&#21644;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15163</link><description>&lt;p&gt;
Wasserstein&#29983;&#25104;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Generative Regression. (arXiv:2306.15163v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15163
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#19968;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#21644;&#26465;&#20214;&#20998;&#24067;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23398;&#20064;&#26694;&#26550;&#21516;&#26102;&#20272;&#35745;&#22238;&#24402;&#20989;&#25968;&#21644;&#26465;&#20214;&#29983;&#25104;&#22120;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#26465;&#20214;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#22810;&#20803;&#36755;&#20986;&#21644;&#21327;&#21464;&#37327;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#26500;&#24314;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20445;&#35777;&#21644;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#19968;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#21644;&#26465;&#20214;&#20998;&#24067;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#20272;&#35745;&#22238;&#24402;&#20989;&#25968;&#21644;&#26465;&#20214;&#29983;&#25104;&#22120;&#65292;&#20351;&#29992;&#29983;&#25104;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#26465;&#20214;&#29983;&#25104;&#22120;&#26159;&#19968;&#20010;&#21487;&#20197;&#20174;&#26465;&#20214;&#20998;&#24067;&#29983;&#25104;&#26679;&#26412;&#30340;&#20989;&#25968;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#20272;&#35745;&#19968;&#20010;&#28385;&#36275;&#20135;&#29983;&#33391;&#22909;&#22238;&#24402;&#20989;&#25968;&#20272;&#35745;&#30340;&#32422;&#26463;&#26465;&#20214;&#30340;&#26465;&#20214;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#26465;&#20214;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#22810;&#20803;&#36755;&#20986;&#21644;&#21327;&#21464;&#37327;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#26469;&#26500;&#24314;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#36890;&#36807;&#24471;&#20986;&#38750;&#28176;&#36817;&#35823;&#24046;&#30028;&#21644;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#25105;&#20204;&#26041;&#27861;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#26469;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26576;&#20123;&#29616;&#26377;&#26041;&#27861;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a new and unified approach for nonparametric regression and conditional distribution learning. Our approach simultaneously estimates a regression function and a conditional generator using a generative learning framework, where a conditional generator is a function that can generate samples from a conditional distribution. The main idea is to estimate a conditional generator that satisfies the constraint that it produces a good regression function estimator. We use deep neural networks to model the conditional generator. Our approach can handle problems with multivariate outcomes and covariates, and can be used to construct prediction intervals. We provide theoretical guarantees by deriving non-asymptotic error bounds and the distributional consistency of our approach under suitable assumptions. We also perform numerical experiments with simulated and real data to demonstrate the effectiveness and superiority of our approach over some existing approaches in va
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#30340;UQ&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#38477;&#20302;&#35757;&#32451;&#25968;&#25454;&#38598;&#35268;&#27169;&#21644;&#23433;&#20840;&#22240;&#23376;&#25552;&#20379;&#20102;&#25104;&#26412;&#33410;&#32422;&#30340;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2306.15159</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#30340;UQ&#20013;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#23545;&#20851;&#20110;&#35748;&#30693;&#21644;&#27979;&#35797;&#35823;&#24046;&#30340;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Evaluation of machine learning architectures on the quantification of epistemic and aleatoric uncertainties in complex dynamical systems. (arXiv:2306.15159v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15159
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#30340;UQ&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#38477;&#20302;&#35757;&#32451;&#25968;&#25454;&#38598;&#35268;&#27169;&#21644;&#23433;&#20840;&#22240;&#23376;&#25552;&#20379;&#20102;&#25104;&#26412;&#33410;&#32422;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#30340;&#38477;&#38454;&#27169;&#22411;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#31243;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#20316;&#20026;&#26114;&#36149;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#23545;&#20110;&#27169;&#22411;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#26816;&#39564;&#26159;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#65292;&#23427;&#26159;&#23545;&#27169;&#22411;&#35823;&#24046;&#30340;&#33258;&#25105;&#35780;&#20272;&#12290;&#20934;&#30830;&#30340;UQ&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#23433;&#20840;&#22240;&#23376;&#26469;&#23454;&#29616;&#25104;&#26412;&#33410;&#32422;&#65292;&#32780;&#36739;&#24046;&#30340;UQ&#21017;&#20250;&#38459;&#30861;&#29992;&#25143;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#20449;&#20219;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21253;&#25324;&#39640;&#26031;&#36807;&#31243;&#21644;&#19968;&#31995;&#21015;&#22686;&#24378;UQ&#30340;&#31070;&#32463;&#32593;&#32476;&#65306;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#65288;ENN&#65289;&#12289;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#12289;Dropout&#31070;&#32463;&#32593;&#32476;&#65288;D-NN&#65289;&#21644;&#39640;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;G-NN&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#25351;&#26631;&#26469;&#35780;&#20272;UQ&#20934;&#30830;&#24615;&#65288;&#19982;&#27169;&#22411;&#20934;&#30830;&#24615;&#19981;&#21516;&#65289;&#65306;&#39564;&#35777;&#25968;&#25454;&#19978;&#24402;&#19968;&#21270;&#27531;&#24046;&#30340;&#20998;&#24067;&#21644;&#20272;&#35745;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning methods for the construction of data-driven reduced order model models are used in an increasing variety of engineering domains, especially as a supplement to expensive computational fluid dynamics for design problems. An important check on the reliability of surrogate models is Uncertainty Quantification (UQ), a self assessed estimate of the model error. Accurate UQ allows for cost savings by reducing both the required size of training data sets and the required safety factors, while poor UQ prevents users from confidently relying on model predictions. We examine several machine learning techniques, including both Gaussian processes and a family UQ-augmented neural networks: Ensemble neural networks (ENN), Bayesian neural networks (BNN), Dropout neural networks (D-NN), and Gaussian neural networks (G-NN). We evaluate UQ accuracy (distinct from model accuracy) using two metrics: the distribution of normalized residuals on validation data, and the distribution of estima
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#28909;&#24102;&#22810;&#39033;&#24335;&#38500;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#31616;&#21270;&#20013;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20855;&#26377;&#23454;&#31995;&#25968;&#30340;&#28909;&#24102;&#22810;&#39033;&#24335;&#65292;&#35777;&#26126;&#20102;&#21807;&#19968;&#21830;&#20313;&#23545;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20986;&#20102;&#31934;&#30830;&#31639;&#27861;&#21644;&#36817;&#20284;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.15157</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#28909;&#24102;&#22810;&#39033;&#24335;&#38500;&#27861;&#65306;&#29702;&#35770;&#12289;&#31639;&#27861;&#19982;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Revisiting Tropical Polynomial Division: Theory, Algorithms and Application to Neural Networks. (arXiv:2306.15157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#28909;&#24102;&#22810;&#39033;&#24335;&#38500;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#31616;&#21270;&#20013;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20855;&#26377;&#23454;&#31995;&#25968;&#30340;&#28909;&#24102;&#22810;&#39033;&#24335;&#65292;&#35777;&#26126;&#20102;&#21807;&#19968;&#21830;&#20313;&#23545;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20986;&#20102;&#31934;&#30830;&#31639;&#27861;&#21644;&#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28909;&#24102;&#20960;&#20309;&#22312;&#20998;&#26512;&#20855;&#26377;&#20998;&#27573;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#21457;&#29616;&#20102;&#20960;&#20010;&#24212;&#29992;&#12290;&#26412;&#25991;&#23545;&#28909;&#24102;&#22810;&#39033;&#24335;&#38500;&#27861;&#38382;&#39064;&#21450;&#20854;&#22312;&#31070;&#32463;&#32593;&#32476;&#31616;&#21270;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#26032;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20855;&#26377;&#23454;&#31995;&#25968;&#30340;&#28909;&#24102;&#22810;&#39033;&#24335;&#65292;&#25193;&#23637;&#20102;&#26089;&#26399;&#23545;&#25972;&#25968;&#31995;&#25968;&#22810;&#39033;&#24335;&#30340;&#24605;&#24819;&#21644;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#21807;&#19968;&#21830;&#20313;&#23545;&#30340;&#23384;&#22312;&#65292;&#24182;&#20197;&#19968;&#20010;&#30456;&#20851;&#20989;&#25968;&#30340;&#20984;&#21452;&#20849;&#36717;&#26469;&#34920;&#24449;&#21830;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#20855;&#26377;&#25972;&#25968;&#31995;&#25968;&#30340;&#28909;&#24102;&#22810;&#39033;&#24335;&#30340;&#21830;&#19981;&#19968;&#23450;&#20855;&#26377;&#25972;&#25968;&#31995;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#28909;&#24102;&#22810;&#39033;&#24335;&#38500;&#27861;&#19982;&#35745;&#31639;&#20984;&#22810;&#38754;&#20307;&#24182;&#38598;&#30340;&#20984;&#21253;&#30340;&#20851;&#31995;&#65292;&#24182;&#29992;&#23427;&#26469;&#25512;&#23548;&#20102;&#28909;&#24102;&#22810;&#39033;&#24335;&#38500;&#27861;&#30340;&#31934;&#30830;&#31639;&#27861;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#20132;&#26367;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tropical geometry has recently found several applications in the analysis of neural networks with piecewise linear activation functions. This paper presents a new look at the problem of tropical polynomial division and its application to the simplification of neural networks. We analyze tropical polynomials with real coefficients, extending earlier ideas and methods developed for polynomials with integer coefficients. We first prove the existence of a unique quotient-remainder pair and characterize the quotient in terms of the convex bi-conjugate of a related function. Interestingly, the quotient of tropical polynomials with integer coefficients does not necessarily have integer coefficients. Furthermore, we develop a relationship of tropical polynomial division with the computation of the convex hull of unions of convex polyhedra and use it to derive an exact algorithm for tropical polynomial division. An approximate algorithm is also presented, based on an alternation between data pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20165;&#29366;&#24577;&#24207;&#21015;&#23398;&#20064;&#38750;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#23454;&#29616;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#20223;&#12290;&#23398;&#20064;&#30340;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#8220;&#25512;&#29702;&#24335;&#20915;&#31574;&#8221;&#65292;&#24182;&#22312;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15156</link><description>&lt;p&gt;
&#20174;&#20165;&#29366;&#24577;&#24207;&#21015;&#23398;&#20064;&#38750;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Learning non-Markovian Decision-Making from State-only Sequences. (arXiv:2306.15156v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20165;&#29366;&#24577;&#24207;&#21015;&#23398;&#20064;&#38750;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#23454;&#29616;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#20223;&#12290;&#23398;&#20064;&#30340;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#8220;&#25512;&#29702;&#24335;&#20915;&#31574;&#8221;&#65292;&#24182;&#22312;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#27169;&#20223;&#23398;&#20064;&#20551;&#35774;&#33021;&#22815;&#33719;&#24471;&#23637;&#31034;&#32773;&#30340;&#21160;&#20316;&#65292;&#20294;&#26159;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#36825;&#20123;&#21160;&#20316;&#36890;&#24120;&#26080;&#27861;&#35266;&#27979;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#30340;&#24207;&#21015;&#20915;&#31574;&#34892;&#20026;&#21487;&#33021;&#20559;&#31163;&#26631;&#20934;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#38750;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;nMDP&#65289;&#20013;&#20165;&#29366;&#24577;&#24207;&#21015;&#30340;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#65292;&#20854;&#20013;&#31574;&#30053;&#26159;&#28508;&#22312;&#29366;&#24577;&#36716;&#31227;&#29983;&#25104;&#22120;&#30340;&#33021;&#37327;&#20808;&#39564;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26469;&#23454;&#29616;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#20223;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#20808;&#39564;&#36827;&#34892;&#30701;&#26399;MCMC&#37319;&#26679;&#21644;&#23545;&#21518;&#39564;&#36827;&#34892;&#37325;&#35201;&#24615;&#37319;&#26679;&#12290;&#23398;&#20064;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#8220;&#25512;&#29702;&#24335;&#20915;&#31574;&#8221;&#65292;&#21363;&#26080;&#27169;&#22411;&#31574;&#30053;&#25191;&#34892;&#31561;&#20215;&#20110;&#20808;&#39564;&#37319;&#26679;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#21017;&#26159;&#20174;&#31574;&#30053;&#21021;&#22987;&#21270;&#30340;&#21518;&#39564;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20855;&#26377;&#38750;&#39532;&#23572;&#31185;&#22827;&#29305;&#24449;&#30340;&#21407;&#22411;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional imitation learning assumes access to the actions of demonstrators, but these motor signals are often non-observable in naturalistic settings. Additionally, sequential decision-making behaviors in these settings can deviate from the assumptions of a standard Markov Decision Process (MDP). To address these challenges, we explore deep generative modeling of state-only sequences with non-Markov Decision Process (nMDP), where the policy is an energy-based prior in the latent space of the state transition generator. We develop maximum likelihood estimation to achieve model-based imitation, which involves short-run MCMC sampling from the prior and importance sampling for the posterior. The learned model enables \textit{decision-making as inference}: model-free policy execution is equivalent to prior sampling, model-based planning is posterior sampling initialized from the policy. We demonstrate the efficacy of the proposed method in a prototypical path planning task with non-Mark
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#21516;&#30340;&#36755;&#20837;&#22270;&#21644;GNN&#23884;&#20837;&#22823;&#23567;&#19978;&#20351;&#29992;&#20195;&#25968;&#37325;&#32452;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#20339;&#32452;&#21512;&#26469;&#25552;&#39640;GNN&#21152;&#36895;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15155</link><description>&lt;p&gt;
&#38024;&#23545;GNN&#21152;&#36895;&#30340;&#36755;&#20837;&#25935;&#24863;&#30340;&#31264;&#23494;-&#31232;&#30095;&#22522;&#26412;&#32452;&#25104;&#20803;&#32032;
&lt;/p&gt;
&lt;p&gt;
Input-sensitive dense-sparse primitive compositions for GNN acceleration. (arXiv:2306.15155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#21516;&#30340;&#36755;&#20837;&#22270;&#21644;GNN&#23884;&#20837;&#22823;&#23567;&#19978;&#20351;&#29992;&#20195;&#25968;&#37325;&#32452;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#20339;&#32452;&#21512;&#26469;&#25552;&#39640;GNN&#21152;&#36895;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#19968;&#31867;&#37325;&#35201;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22312;&#31038;&#20132;&#21644;&#37329;&#34701;&#32593;&#32476;&#20998;&#26512;&#31561;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;GNN&#35745;&#31639;&#30340;&#19981;&#21516;&#38454;&#27573;&#21487;&#20197;&#20351;&#29992;&#31264;&#23494;&#21644;&#31232;&#30095;&#30697;&#38453;&#36816;&#31639;&#26469;&#24314;&#27169;&#12290;&#25991;&#29486;&#20013;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26694;&#26550;&#21644;&#20248;&#21270;&#25216;&#26415;&#26469;&#21152;&#36895;GNN&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#20855;&#26377;&#19981;&#21516;&#31232;&#30095;&#27169;&#24335;&#21644;GNN&#23884;&#20837;&#22823;&#23567;&#30340;&#36755;&#20837;&#22270;&#19978;&#23454;&#29616;&#19968;&#33268;&#39640;&#24615;&#33021;&#20173;&#28982;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;GNN&#35745;&#31639;&#36827;&#34892;&#19981;&#21516;&#30340;&#20195;&#25968;&#37325;&#32452;&#65292;&#23548;&#33268;&#20102;&#26032;&#30340;&#23494;&#38598;&#21644;&#31232;&#30095;&#30697;&#38453;&#22522;&#26412;&#36873;&#25321;&#21644;&#32452;&#21512;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20123;&#32452;&#21512;&#30340;&#30408;&#21033;&#33021;&#21147;&#21462;&#20915;&#20110;&#36755;&#20837;&#22270;&#12289;&#23884;&#20837;&#22823;&#23567;&#21644;&#30446;&#26631;&#30828;&#20214;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;SENSEi&#30340;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#26469;&#36873;&#25321;&#32473;&#23450;&#36755;&#20837;&#22270;&#21644;GNN&#23884;&#20837;&#22823;&#23567;&#30340;&#26368;&#20339;&#32452;&#21512;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#33539;&#22260;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNN) have become an important class of neural network models that have gained popularity in domains such as social and financial network analysis. Different phases of GNN computations can be modeled using both dense and sparse matrix operations. There have been many frameworks and optimization techniques proposed in the literature to accelerate GNNs. However, getting consistently high performance across many input graphs with different sparsity patterns and GNN embedding sizes has remained difficult.  In this paper, we propose different algebraic reassociations of GNN computations that lead to novel dense and sparse matrix primitive selections and compositions. We show that the profitability of these compositions depends on the input graph, embedding size, and the target hardware. We developed SENSEi, a system that uses a data-driven adaptive strategy to select the best composition given the input graph and GNN embedding sizes. Our evaluations on a wide range of 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20174;&#22823;&#37327;&#26377;&#26631;&#35760;&#33410;&#28857;&#30340;&#31867;&#21035;&#20013;&#25277;&#21462;&#21487;&#36801;&#31227;&#30340;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#20855;&#26377;&#26377;&#38480;&#26631;&#35760;&#33410;&#28857;&#30340;&#20854;&#20182;&#31867;&#21035;&#65292;&#23398;&#20064;&#36890;&#29992;&#30340;&#33410;&#28857;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2306.15154</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#23545;&#27604;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Meta-Learning for Few-shot Node Classification. (arXiv:2306.15154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15154
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20174;&#22823;&#37327;&#26377;&#26631;&#35760;&#33410;&#28857;&#30340;&#31867;&#21035;&#20013;&#25277;&#21462;&#21487;&#36801;&#31227;&#30340;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#20855;&#26377;&#26377;&#38480;&#26631;&#35760;&#33410;&#28857;&#30340;&#20854;&#20182;&#31867;&#21035;&#65292;&#23398;&#20064;&#36890;&#29992;&#30340;&#33410;&#28857;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#26159;&#22312;&#21482;&#26377;&#26377;&#38480;&#26631;&#35760;&#33410;&#28857;&#20316;&#20026;&#21442;&#32771;&#30340;&#22270;&#19978;&#20026;&#33410;&#28857;&#39044;&#27979;&#26631;&#31614;&#30340;&#20219;&#21153;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#25366;&#25496;&#20219;&#21153;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#38024;&#23545;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#26694;&#26550;&#21033;&#29992;&#22823;&#37327;&#30340;episode&#20174;&#26377;&#22823;&#37327;&#26631;&#35760;&#33410;&#28857;&#30340;&#31867;&#21035;&#20013;&#25277;&#21462;&#21487;&#36801;&#31227;&#30340;&#30693;&#35782;&#65292;&#24182;&#23558;&#36825;&#20123;&#30693;&#35782;&#25512;&#24191;&#21040;&#20855;&#26377;&#26377;&#38480;&#26631;&#35760;&#33410;&#28857;&#30340;&#20854;&#20182;&#31867;&#21035;&#12290;&#26412;&#36136;&#19978;&#65292;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23398;&#20064;&#21487;&#25512;&#24191;&#21040;&#19981;&#21516;&#31867;&#21035;&#30340;&#33410;&#28857;&#23884;&#20837;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;GNN&#32534;&#30721;&#22120;&#24517;&#39035;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#21516;&#26102;&#36824;&#35201;&#23545;&#21516;&#19968;&#31867;&#21035;&#20013;&#30340;&#33410;&#28857;&#23884;&#20837;&#36827;&#34892;&#23545;&#40784;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#32771;&#34385;&#31867;&#20869;&#21644;&#31867;&#38388;&#30340;&#33410;&#28857;&#23884;&#20837;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot node classification, which aims to predict labels for nodes on graphs with only limited labeled nodes as references, is of great significance in real-world graph mining tasks. Particularly, in this paper, we refer to the task of classifying nodes in classes with a few labeled nodes as the few-shot node classification problem. To tackle such a label shortage issue, existing works generally leverage the meta-learning framework, which utilizes a number of episodes to extract transferable knowledge from classes with abundant labeled nodes and generalizes the knowledge to other classes with limited labeled nodes. In essence, the primary aim of few-shot node classification is to learn node embeddings that are generalizable across different classes. To accomplish this, the GNN encoder must be able to distinguish node embeddings between different classes, while also aligning embeddings for nodes in the same class. Thus, in this work, we propose to consider both the intra-class and int
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#33258;&#25105;&#24341;&#23548;&#21644;&#22359;&#23545;&#35282;&#34920;&#31034;&#30340;&#37325;&#21551;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#35889;&#32858;&#31867;&#20013;&#39318;&#27425;&#24212;&#29992;&#37325;&#21551;&#31574;&#30053;&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;&#27599;&#20010;&#21608;&#26399;&#20013;&#37325;&#26032;&#20998;&#31867;&#26679;&#26412;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.15138</link><description>&lt;p&gt;
&#19968;&#20010;&#20855;&#26377;&#33258;&#25105;&#24341;&#23548;&#21644;&#22359;&#23545;&#35282;&#34920;&#31034;&#30340;&#37325;&#21551;&#22823;&#35268;&#27169;&#35889;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Restarted Large-Scale Spectral Clustering with Self-Guiding and Block Diagonal Representation. (arXiv:2306.15138v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#33258;&#25105;&#24341;&#23548;&#21644;&#22359;&#23545;&#35282;&#34920;&#31034;&#30340;&#37325;&#21551;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#35889;&#32858;&#31867;&#20013;&#39318;&#27425;&#24212;&#29992;&#37325;&#21551;&#31574;&#30053;&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;&#27599;&#20010;&#21608;&#26399;&#20013;&#37325;&#26032;&#20998;&#31867;&#26679;&#26412;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#32858;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35889;&#32858;&#31867;&#26159;&#26368;&#27969;&#34892;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20043;&#19968;&#12290;&#26500;&#24314;&#30456;&#20284;&#24615;&#30697;&#38453;&#23545;&#20110;&#36825;&#31867;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#30456;&#20284;&#24615;&#30697;&#38453;&#21482;&#35745;&#31639;&#19968;&#27425;&#25110;&#32773;&#26159;&#20132;&#26367;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#24456;&#38590;&#21453;&#26144;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20840;&#38754;&#20851;&#31995;&#65292;&#32780;&#21518;&#32773;&#32791;&#26102;&#19988;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#20013;&#29978;&#33267;&#38590;&#20197;&#23454;&#26045;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#33258;&#25105;&#24341;&#23548;&#21644;&#22359;&#23545;&#35282;&#34920;&#31034;&#30340;&#37325;&#21551;&#32858;&#31867;&#26694;&#26550;&#12290;&#35813;&#31574;&#30053;&#30340;&#20248;&#21183;&#22312;&#20110;&#23613;&#21487;&#33021;&#20445;&#30041;&#20174;&#20808;&#21069;&#21608;&#26399;&#20013;&#33719;&#24471;&#30340;&#26377;&#29992;&#32858;&#31867;&#20449;&#24687;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#37325;&#21551;&#31574;&#30053;&#24212;&#29992;&#20110;&#35889;&#32858;&#31867;&#30340;&#24037;&#20316;&#12290;&#20851;&#38190;&#21306;&#21035;&#22312;&#20110;&#25105;&#20204;&#22312;&#26041;&#27861;&#30340;&#27599;&#20010;&#21608;&#26399;&#20013;&#37325;&#26032;&#23545;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#65292;&#32780;&#29616;&#26377;&#26041;&#27861;&#21482;&#36827;&#34892;&#19968;&#27425;&#20998;&#31867;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20943;&#23569;&#24320;&#38144;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22359;&#23545;&#35282;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral clustering is one of the most popular unsupervised machine learning methods. Constructing similarity matrix is crucial to this type of method. In most existing works, the similarity matrix is computed once for all or is updated alternatively. However, the former is difficult to reflect comprehensive relationships among data points, and the latter is time-consuming and is even infeasible for large-scale problems. In this work, we propose a restarted clustering framework with self-guiding and block diagonal representation. An advantage of the strategy is that some useful clustering information obtained from previous cycles could be preserved as much as possible. To the best of our knowledge, this is the first work that applies restarting strategy to spectral clustering. The key difference is that we reclassify the samples in each cycle of our method, while they are classified only once in existing methods. To further release the overhead, we introduce a block diagonal representa
&lt;/p&gt;</description></item><item><title>MIMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#19981;&#38656;&#35201;&#20219;&#20309;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22810;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#20248;&#20110;&#20351;&#29992;&#27880;&#37322;&#25366;&#25496;&#30340;&#34920;&#31034;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.15128</link><description>&lt;p&gt;
MIMIC: &#22522;&#20110;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MIMIC: Masked Image Modeling with Image Correspondences. (arXiv:2306.15128v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15128
&lt;/p&gt;
&lt;p&gt;
MIMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#23545;&#24212;&#20851;&#31995;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#25366;&#25496;&#19981;&#38656;&#35201;&#20219;&#20309;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#22810;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#20248;&#20110;&#20351;&#29992;&#27880;&#37322;&#25366;&#25496;&#30340;&#34920;&#31034;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20687;&#32032;&#32423;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#8212;&#8212;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#28145;&#24230;&#20272;&#35745;&#21644;&#35821;&#20041;&#20998;&#21106;&#8212;&#8212;&#22914;&#20170;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#31579;&#36873;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#33267;&#20851;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20165;&#36890;&#36807;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#24102;&#26377;&#27880;&#37322;&#30340;3D&#32593;&#26684;&#12289;&#28857;&#20113;&#21644;&#30456;&#26426;&#21442;&#25968;&#31579;&#36873;&#32780;&#26469;&#65292;&#24182;&#19981;&#20855;&#22791;&#22810;&#35270;&#35282;&#22330;&#26223;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#20219;&#20309;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#31579;&#36873;&#26426;&#21046;&#12290;&#25105;&#20204;&#20174;&#24320;&#28304;&#35270;&#39057;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#30340;3D&#29615;&#22659;&#20013;&#25366;&#25496;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;MIMIC-1M(&#21253;&#21547;1.3M&#20010;&#22810;&#35270;&#35282;&#22270;&#20687;&#23545;)&#21644;MIMIC-3M(&#21253;&#21547;3.1M&#20010;&#22810;&#35270;&#35282;&#22270;&#20687;&#23545;)&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#30446;&#26631;&#65292;&#23637;&#31034;&#20102;&#20197;&#19979;&#21457;&#29616;&#65306;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;MIMIC-3M&#35757;&#32451;&#30340;&#34920;&#31034;&#20248;&#20110;&#20351;&#29992;&#27880;&#37322;&#25366;&#25496;&#30340;&#34920;&#31034;&#65292;&#21253;&#25324;&#28145;&#24230;&#20272;&#35745;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#34920;&#38754;&#27861;&#32447;&#21644;&#23039;&#24577;&#20272;&#35745;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many pixelwise dense prediction tasks-depth estimation and semantic segmentation in computer vision today rely on pretrained image representations. Therefore, curating effective pretraining datasets is vital. Unfortunately, the effective pretraining datasets are those with multi-view scenes and have only been curated using annotated 3D meshes, point clouds, and camera parameters from simulated environments. We propose a dataset-curation mechanism that does not require any annotations. We mine two datasets: MIMIC-1M with 1.3M and MIMIC-3M with 3.1M multi-view image pairs from open-sourced video datasets and from synthetic 3D environments. We train multiple self-supervised models with different masked image modeling objectives to showcase the following findings: Representations trained on MIMIC-3M outperform those mined using annotations on multiple downstream tasks, including depth estimation, semantic segmentation, surface normals, and pose estimation. They also outperform representati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;Adaptive IPS (AIPS)&#65292;&#38024;&#23545;&#25490;&#21517;&#31574;&#30053;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#36890;&#36807;&#32771;&#34385;&#29992;&#25143;&#34892;&#20026;&#30340;&#22810;&#26679;&#24615;&#21644;&#19978;&#19979;&#25991;&#30340;&#21464;&#21270;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#20272;&#35745;&#20013;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.15098</link><description>&lt;p&gt;
&#19981;&#21516;&#29992;&#25143;&#34892;&#20026;&#19979;&#30340;&#25490;&#21517;&#31574;&#30053;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Evaluation of Ranking Policies under Diverse User Behavior. (arXiv:2306.15098v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#26041;&#27861;Adaptive IPS (AIPS)&#65292;&#38024;&#23545;&#25490;&#21517;&#31574;&#30053;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#36890;&#36807;&#32771;&#34385;&#29992;&#25143;&#34892;&#20026;&#30340;&#22810;&#26679;&#24615;&#21644;&#19978;&#19979;&#25991;&#30340;&#21464;&#21270;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#20272;&#35745;&#20013;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24179;&#21488;&#19978;&#21040;&#22788;&#37117;&#26159;&#25490;&#21517;&#30028;&#38754;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#20351;&#29992;&#35760;&#24405;&#25968;&#25454;&#36827;&#34892;&#25490;&#21517;&#31574;&#30053;&#30340;&#20934;&#30830;&#24615;&#33021;&#35780;&#20272;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;OPE&#30340;&#19968;&#31181;&#20107;&#23454;&#19978;&#30340;&#26041;&#27861;&#26159;&#20498;&#25968;&#20542;&#21521;&#24471;&#20998;&#27861;&#65288;IPS&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#26080;&#20559;&#19988;&#19968;&#33268;&#30340;&#20540;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#22312;&#25490;&#21517;&#35774;&#32622;&#20013;&#65292;&#30001;&#20110;&#22312;&#22823;&#22411;&#34892;&#20026;&#31354;&#38388;&#19979;&#20855;&#26377;&#36739;&#39640;&#30340;&#26041;&#24046;&#65292;&#23427;&#21464;&#24471;&#26497;&#20854;&#19981;&#20934;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20551;&#35774;&#29992;&#25143;&#34892;&#20026;&#26159;&#29420;&#31435;&#30340;&#25110;&#32423;&#32852;&#30340;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20123;IPS&#30340;&#25490;&#21517;&#29256;&#26412;&#12290;&#23613;&#31649;&#36825;&#20123;&#20272;&#35745;&#22120;&#22312;&#20943;&#23569;&#26041;&#24046;&#26041;&#38754;&#26377;&#19968;&#23450;&#30340;&#25928;&#26524;&#65292;&#20294;&#25152;&#26377;&#29616;&#26377;&#30340;&#20272;&#35745;&#22120;&#37117;&#23545;&#27599;&#20010;&#29992;&#25143;&#24212;&#29992;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#36890;&#29992;&#20551;&#35774;&#65292;&#23548;&#33268;&#36807;&#24230;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#12290;&#22240;&#27492;&#65292;&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#26356;&#36890;&#29992;&#30340;&#20844;&#24335;&#65292;&#20854;&#20013;&#29992;&#25143;&#34892;&#20026;&#26159;&#22810;&#26679;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#19978;&#19979;&#25991;&#30340;&#19981;&#21516;&#32780;&#21464;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20986;&#30001;&#27492;&#20135;&#29983;&#30340;&#20272;&#35745;&#22120;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#33258;&#36866;&#24212;IPS&#65288;AIPS&#65289;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#36827;&#34892;&#31163;&#31574;&#30053;&#35780;&#20272;&#21644;&#25490;&#21517;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ranking interfaces are everywhere in online platforms. There is thus an ever growing interest in their Off-Policy Evaluation (OPE), aiming towards an accurate performance evaluation of ranking policies using logged data. A de-facto approach for OPE is Inverse Propensity Scoring (IPS), which provides an unbiased and consistent value estimate. However, it becomes extremely inaccurate in the ranking setup due to its high variance under large action spaces. To deal with this problem, previous studies assume either independent or cascade user behavior, resulting in some ranking versions of IPS. While these estimators are somewhat effective in reducing the variance, all existing estimators apply a single universal assumption to every user, causing excessive bias and variance. Therefore, this work explores a far more general formulation where user behavior is diverse and can vary depending on the user context. We show that the resulting estimator, which we call Adaptive IPS (AIPS), can be unb
&lt;/p&gt;</description></item><item><title>C3S&#24494;&#20307;&#31995;&#32467;&#26500;&#22686;&#24378;&#30340;&#21019;&#26032;&#26159;&#36890;&#36807;&#24341;&#20837;&#21050;&#28608;&#32534;&#30721;&#22120;&#27169;&#22359;&#21644;&#25918;&#26494;&#20285;&#39532;&#26102;&#38047;&#26469;&#25913;&#36827;&#29616;&#26377;&#30340;&#30382;&#23618;&#21015;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2306.15093</link><description>&lt;p&gt;
C3S&#24494;&#20307;&#31995;&#32467;&#26500;&#22686;&#24378;&#65306;&#21050;&#28608;&#32534;&#30721;&#22120;&#27169;&#22359;&#21644;&#25918;&#26494;&#20285;&#39532;&#26102;&#38047;&#65288;&#24322;&#27493;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
C3S Micro-architectural Enhancement: Spike Encoder Block and Relaxing Gamma Clock (Asynchronous). (arXiv:2306.15093v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15093
&lt;/p&gt;
&lt;p&gt;
C3S&#24494;&#20307;&#31995;&#32467;&#26500;&#22686;&#24378;&#30340;&#21019;&#26032;&#26159;&#36890;&#36807;&#24341;&#20837;&#21050;&#28608;&#32534;&#30721;&#22120;&#27169;&#22359;&#21644;&#25918;&#26494;&#20285;&#39532;&#26102;&#38047;&#26469;&#25913;&#36827;&#29616;&#26377;&#30340;&#30382;&#23618;&#21015;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#39046;&#22495;&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#12290;&#38543;&#30528;&#23545;&#29983;&#29289;&#20934;&#30830;&#24615;&#21644;&#23454;&#38469;&#23454;&#29616;&#30340;&#25506;&#32034;&#65292;&#29616;&#26377;&#26550;&#26500;&#34987;&#20462;&#25913;&#21644;&#25913;&#36827;&#20197;&#36866;&#24212;&#36825;&#20004;&#20010;&#30446;&#30340;&#12290;&#26102;&#38388;&#31070;&#32463;&#32593;&#32476;&#65288;TNN&#65289;&#24335;&#30340;&#26550;&#26500;&#26159;&#36817;&#20284;&#29983;&#29289;&#31070;&#32463;&#20803;&#30340;&#22909;&#22522;&#30784;&#65292;&#22240;&#20026;&#23427;&#20351;&#29992;&#23450;&#26102;&#33033;&#20914;&#26469;&#32534;&#30721;&#25968;&#25454;&#21644;&#30005;&#21387;&#38408;&#20540;&#31995;&#32479;&#12290;&#22522;&#20110;&#26102;&#38388;&#31070;&#32463;&#32593;&#32476;&#30382;&#23618;&#27169;&#22411;C3S&#26550;&#26500;&#35774;&#35745;&#65292;&#26412;&#39033;&#30446;&#26088;&#22312;&#22686;&#24378;&#32593;&#32476;&#35774;&#35745;&#12290;&#26412;&#39033;&#30446;&#27880;&#24847;&#21040;&#20004;&#20010;&#24819;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#23427;&#20204;&#30340;&#35774;&#35745;&#65292;&#20197;&#25913;&#21892;&#29616;&#26377;&#30340;&#30382;&#23618;&#21015;&#26550;&#26500;&#12290;&#35813;&#39046;&#22495;&#30340;&#19968;&#20010;&#38656;&#35201;&#26159;&#25317;&#26377;&#19968;&#31181;&#33021;&#22815;&#22312;&#24120;&#35265;&#25968;&#23383;&#26684;&#24335;&#21644;&#23450;&#26102;&#31070;&#32463;&#33033;&#20914;&#20043;&#38388;&#36716;&#25442;&#30340;&#32534;&#30721;&#22120;&#65292;&#22240;&#20026;&#29983;&#29289;&#20934;&#30830;&#32593;&#32476;&#30340;&#26412;&#36136;&#26159;&#26102;&#38388;&#24615;&#30340;&#12290;&#20026;&#27492;&#65292;&#26412;&#39033;&#30446;&#25552;&#20986;&#20102;&#19968;&#31181;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#23558;&#20108;&#36827;&#21046;&#32534;&#30721;&#20540;&#21644;&#23450;&#26102;&#33033;&#20914;&#36827;&#34892;&#36716;&#25442;&#65292;&#20197;&#20379;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#12290;&#21478;&#19968;&#20010;&#38656;&#27714;&#26159;&#25918;&#26494;&#20285;&#39532;&#26102;&#38047;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of neuromorphic computing is rapidly evolving. As both biological accuracy and practical implementations are explored, existing architectures are modified and improved for both purposes. The Temporal Neural Network(TNN) style of architecture is a good basis for approximating biological neurons due to its use of timed pulses to encode data and a voltage-threshold-like system. Using the Temporal Neural Network cortical column C3S architecture design as a basis, this project seeks to augment the network's design. This project takes note of two ideas and presents their designs with the goal of improving existing cortical column architecture. One need in this field is for an encoder that could convert between common digital formats and timed neuronal spikes, as biologically accurate networks are temporal in nature. To this end, this project presents an encoder to translate between binary encoded values and timed spikes to be processed by the neural network. Another need is for the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#39640;&#38454;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#37327;&#24314;&#27169;&#21644;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#20262;&#25958;&#22320;&#19979;&#20892;&#19994;&#20892;&#22330;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#21463;&#22122;&#22768;&#21644;&#30636;&#24577;&#26465;&#20214;&#24433;&#21709;&#30340;&#22797;&#26434;&#25968;&#25454;&#65292;&#24182;&#20998;&#26512;&#21644;&#39044;&#27979;&#20892;&#22330;&#30340;&#29615;&#22659;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.15089</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#38454;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#26041;&#27861;&#23545;&#22320;&#19979;&#20892;&#19994;&#20892;&#22330;&#36827;&#34892;&#33021;&#37327;&#24314;&#27169;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Energy Modelling and Forecasting for an Underground Agricultural Farm using a Higher Order Dynamic Mode Decomposition Approach. (arXiv:2306.15089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#39640;&#38454;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#37327;&#24314;&#27169;&#21644;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#20262;&#25958;&#22320;&#19979;&#20892;&#19994;&#20892;&#22330;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#21463;&#22122;&#22768;&#21644;&#30636;&#24577;&#26465;&#20214;&#24433;&#21709;&#30340;&#22797;&#26434;&#25968;&#25454;&#65292;&#24182;&#20998;&#26512;&#21644;&#39044;&#27979;&#20892;&#22330;&#30340;&#29615;&#22659;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#38454;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65288;HODMD&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#20301;&#20110;&#20262;&#25958;&#22320;&#19979;&#38567;&#36947;&#20013;&#30340;&#25913;&#36896;&#20892;&#19994;&#20892;&#22330;&#30340;&#33021;&#37327;&#34892;&#20026;&#36827;&#34892;&#24314;&#27169;&#12289;&#20998;&#26512;&#21644;&#39044;&#27979;&#12290;&#22312;&#36825;&#20010;&#20892;&#22330;&#20013;&#65292;&#35266;&#27979;&#21040;&#30340;&#27979;&#37327;&#25968;&#25454;&#21463;&#21040;&#22122;&#22768;&#21644;&#20598;&#23572;&#30340;&#30636;&#24577;&#26465;&#20214;&#30340;&#24433;&#21709;&#12290;HODMD &#26159;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#38477;&#38454;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#24120;&#29992;&#20110;&#20998;&#26512;&#21644;&#39044;&#27979;&#27969;&#20307;&#21160;&#21147;&#23398;&#20013;&#39640;&#22122;&#22768;&#21644;&#22797;&#26434;&#27969;&#21160;&#65292;&#25110;&#32773;&#26469;&#33258;&#21160;&#21147;&#31995;&#32479;&#30340;&#20219;&#20309;&#31867;&#22411;&#30340;&#22797;&#26434;&#25968;&#25454;&#12290;HODMD &#26159;&#32463;&#20856;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#26041;&#27861;&#65288;DMD&#65289;&#30340;&#26368;&#26032;&#25193;&#23637;&#65292;&#19987;&#38376;&#29992;&#20110;&#22788;&#29702;&#27979;&#37327;&#25968;&#25454;&#30340;&#35889;&#22797;&#26434;&#24230;&#39640;&#20110;&#20854;&#31354;&#38388;&#22797;&#26434;&#24230;&#30340;&#22330;&#26223;&#65292;&#20363;&#22914;&#20892;&#22330;&#30340;&#29615;&#22659;&#34892;&#20026;&#12290;HODMD&#23558;&#26102;&#38388;&#25968;&#25454;&#20998;&#35299;&#20026;&#29289;&#29702;&#21547;&#20041;&#30340;DMD&#27169;&#24577;&#30340;&#32447;&#24615;&#23637;&#24320;&#65292;&#37319;&#29992;&#26102;&#24310;&#23884;&#20837;&#26041;&#27861;&#36827;&#34892;&#21322;&#33258;&#21160;&#22788;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#38469;&#27979;&#37327;&#30340;&#25968;&#25454;&#23558;HODMD&#24212;&#29992;&#20110;&#19977;&#20010;&#23395;&#33410;&#30340;&#24773;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an approach based on higher order dynamic mode decomposition (HODMD) to model, analyse, and forecast energy behaviour in an urban agriculture farm situated in a retrofitted London underground tunnel, where observed measurements are influenced by noisy and occasionally transient conditions. HODMD is a data-driven reduced order modelling method typically used to analyse and predict highly noisy and complex flows in fluid dynamics or any type of complex data from dynamical systems. HODMD is a recent extension of the classical dynamic mode decomposition method (DMD), customised to handle scenarios where the spectral complexity underlying the measurement data is higher than its spatial complexity, such as is the environmental behaviour of the farm. HODMD decomposes temporal data as a linear expansion of physically-meaningful DMD-modes in a semi-automatic approach, using a time-delay embedded approach. We apply HODMD to three seasonal scenarios using real data measured by
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32676;&#32452;&#25104;&#21592;&#36164;&#26684;&#19981;&#21487;&#29992;&#25110;&#34987;&#31105;&#27490;&#20351;&#29992;&#26102;&#65292;&#22914;&#20309;&#20197;&#38750;&#27844;&#38706;&#26041;&#24335;&#25910;&#38598;&#24179;&#34913;&#30340;&#25968;&#25454;&#26679;&#26412;&#12290;&#36890;&#36807;&#20351;&#29992;&#20195;&#29702;&#20989;&#25968;&#21644;&#25277;&#26679;&#27010;&#29575;&#65292;&#23454;&#29616;&#20102;&#23545;&#20010;&#20307;&#26679;&#26412;&#30340;&#20998;&#31867;&#21644;&#36873;&#25321;&#65292;&#21516;&#26102;&#20445;&#25252;&#20010;&#20307;&#26679;&#26412;&#30340;&#25935;&#24863;&#32676;&#32452;&#25104;&#21592;&#36164;&#26684;&#19981;&#34987;&#27844;&#38706;&#12290;</title><link>http://arxiv.org/abs/2306.15083</link><description>&lt;p&gt;
&#24179;&#34913;&#36807;&#28388;&#20351;&#29992;&#38750;&#27844;&#38706;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Balanced Filtering via Non-Disclosive Proxies. (arXiv:2306.15083v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32676;&#32452;&#25104;&#21592;&#36164;&#26684;&#19981;&#21487;&#29992;&#25110;&#34987;&#31105;&#27490;&#20351;&#29992;&#26102;&#65292;&#22914;&#20309;&#20197;&#38750;&#27844;&#38706;&#26041;&#24335;&#25910;&#38598;&#24179;&#34913;&#30340;&#25968;&#25454;&#26679;&#26412;&#12290;&#36890;&#36807;&#20351;&#29992;&#20195;&#29702;&#20989;&#25968;&#21644;&#25277;&#26679;&#27010;&#29575;&#65292;&#23454;&#29616;&#20102;&#23545;&#20010;&#20307;&#26679;&#26412;&#30340;&#20998;&#31867;&#21644;&#36873;&#25321;&#65292;&#21516;&#26102;&#20445;&#25252;&#20010;&#20307;&#26679;&#26412;&#30340;&#25935;&#24863;&#32676;&#32452;&#25104;&#21592;&#36164;&#26684;&#19981;&#34987;&#27844;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#32676;&#32452;&#25104;&#21592;&#36164;&#26684;&#22312;&#25910;&#38598;&#26102;&#19981;&#21487;&#29992;&#25110;&#34987;&#31105;&#27490;&#20351;&#29992;&#26102;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#27844;&#38706;&#26041;&#24335;&#25910;&#38598;&#19982;&#25935;&#24863;&#32676;&#32452;&#24179;&#34913;&#30340;&#25968;&#25454;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#25910;&#38598;&#26426;&#21046;&#19981;&#20250;&#27604;&#22522;&#26412;&#27604;&#29575;&#33021;&#22815;&#30830;&#23450;&#30340;&#20219;&#20309;&#20010;&#20307;&#26679;&#26412;&#30340;&#32676;&#32452;&#25104;&#21592;&#36164;&#26684;&#26356;&#22810;&#22320;&#36879;&#38706;&#30456;&#20851;&#20449;&#24687;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20844;&#24179;&#27969;&#31243;&#30340;&#35266;&#28857;&#65292;&#21363;&#23398;&#20064;&#32773;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#20195;&#29702;&#20989;&#25968;&#65292;&#36825;&#20010;&#20195;&#29702;&#20989;&#25968;&#20197;&#21518;&#21487;&#20197;&#29992;&#20110;&#36825;&#20010;&#36807;&#28388;&#20219;&#21153;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20195;&#29702;&#20989;&#25968;&#30340;&#33539;&#22260;&#19982;&#25277;&#26679;&#27010;&#29575;&#30456;&#20851;&#32852;&#65307;&#32473;&#23450;&#19968;&#20010;&#26032;&#30340;&#20505;&#36873;&#26679;&#26412;&#65292;&#25105;&#20204;&#20351;&#29992;&#20195;&#29702;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#28982;&#21518;&#26681;&#25454;&#19982;&#20854;&#20195;&#29702;&#20998;&#31867;&#23545;&#24212;&#30340;&#25277;&#26679;&#27010;&#29575;&#36873;&#25321;&#23427;&#20316;&#20026;&#25105;&#20204;&#30340;&#26679;&#26412;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35201;&#27714;&#20195;&#29702;&#20998;&#31867;&#26412;&#36523;&#19981;&#20250;&#36879;&#38706;&#20219;&#20309;&#20010;&#20307;&#26679;&#26412;&#30340;&#25935;&#24863;&#32676;&#32452;&#25104;&#21592;&#36164;&#26684;&#30340;&#37325;&#35201;&#20449;&#24687;&#65288;&#21363;&#65292;&#23427;&#24212;&#35813;&#26159;&#38750;&#27844;&#38706;&#30340;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of non-disclosively collecting a sample of data that is balanced with respect to sensitive groups when group membership is unavailable or prohibited from use at collection time. Specifically, our collection mechanism does not reveal significantly more about group membership of any individual sample than can be ascertained from base rates alone. To do this, we adopt a fairness pipeline perspective, in which a learner can use a small set of labeled data to train a proxy function that can later be used for this filtering task. We then associate the range of the proxy function with sampling probabilities; given a new candidate, we classify it using our proxy function, and then select it for our sample with probability proportional to the sampling probability corresponding to its proxy classification. Importantly, we require that the proxy classification itself not reveal significant information about the sensitive group membership of any individual sample (i.e., it sho
&lt;/p&gt;</description></item><item><title>&#20998;&#23376;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65288;Mol-GDL&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#36890;&#29992;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#38750;&#20849;&#20215;&#38190;&#26500;&#24314;&#30340;&#20998;&#23376;&#22270;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#19982;&#22522;&#20110;&#20849;&#20215;&#38190;&#27169;&#22411;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#23637;&#29616;&#20102;&#36229;&#36234;&#22522;&#20110;&#20849;&#20215;&#38190;&#30340;&#20998;&#23376;&#22270;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.15065</link><description>&lt;p&gt;
&#20998;&#23376;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Molecular geometric deep learning. (arXiv:2306.15065v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15065
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65288;Mol-GDL&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#36890;&#29992;&#30340;&#20998;&#23376;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#38750;&#20849;&#20215;&#38190;&#26500;&#24314;&#30340;&#20998;&#23376;&#22270;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#19982;&#22522;&#20110;&#20849;&#20215;&#38190;&#27169;&#22411;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#23637;&#29616;&#20102;&#36229;&#36234;&#22522;&#20110;&#20849;&#20215;&#38190;&#30340;&#20998;&#23376;&#22270;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65288;GDL&#65289;&#22312;&#20998;&#23376;&#25968;&#25454;&#20998;&#26512;&#20013;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#23041;&#21147;&#21644;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#39640;&#25928;&#30340;&#20998;&#23376;&#34920;&#31034;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#65292;&#22522;&#20110;&#20849;&#20215;&#38190;&#30340;&#20998;&#23376;&#22270;&#24050;&#25104;&#20026;&#34920;&#31034;&#21407;&#23376;&#23618;&#27425;&#30340;&#20998;&#23376;&#25299;&#25169;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#65292;&#20165;&#30001;&#38750;&#20849;&#20215;&#38190;&#26500;&#24314;&#30340;&#20998;&#23376;&#22270;&#21487;&#20197;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20013;&#21462;&#24471;&#19982;&#22522;&#20110;&#20849;&#20215;&#38190;&#27169;&#22411;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#35777;&#26126;&#20102;&#36229;&#36234;&#22522;&#20110;&#20849;&#20215;&#38190;&#30340;&#20998;&#23376;&#22270;&#30340;&#26032;&#22411;&#20998;&#23376;&#34920;&#31034;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#23376;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65288;Mol-GDL&#65289;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#26356;&#36890;&#29992;&#30340;&#20998;&#23376;&#34920;&#31034;&#34701;&#20837;GDL&#27169;&#22411;&#20013;&#12290;&#22312;&#25105;&#20204;&#30340;Mol-GDL&#20013;&#65292;&#20998;&#23376;&#25299;&#25169;&#34987;&#24314;&#27169;&#20026;&#19968;&#31995;&#21015;&#30340;&#20998;&#23376;&#22270;&#65292;&#27599;&#20010;&#20998;&#23376;&#22270;&#32858;&#28966;&#20110;&#19981;&#21516;&#23610;&#24230;&#30340;&#21407;&#23376;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geometric deep learning (GDL) has demonstrated huge power and enormous potential in molecular data analysis. However, a great challenge still remains for highly efficient molecular representations. Currently, covalent-bond-based molecular graphs are the de facto standard for representing molecular topology at the atomic level. Here we demonstrate, for the first time, that molecular graphs constructed only from non-covalent bonds can achieve similar or even better results than covalent-bond-based models in molecular property prediction. This demonstrates the great potential of novel molecular representations beyond the de facto standard of covalent-bond-based molecular graphs. Based on the finding, we propose molecular geometric deep learning (Mol-GDL). The essential idea is to incorporate a more general molecular representation into GDL models. In our Mol-GDL, molecular topology is modeled as a series of molecular graphs, each focusing on a different scale of atomic interactions. In th
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;&#30340;transformer&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#29616;&#20102;&#38750;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20854;&#22312;&#20219;&#21153;&#22810;&#26679;&#24615;&#38408;&#20540;&#20197;&#19979;&#34920;&#29616;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#65292;&#32780;&#22312;&#38408;&#20540;&#20197;&#19978;&#26126;&#26174;&#20248;&#20110;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#65292;&#19982;&#23725;&#22238;&#24402;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2306.15063</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#20219;&#21153;&#22810;&#26679;&#24615;&#19982;&#22238;&#24402;&#38382;&#39064;&#20013;&#38750;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression. (arXiv:2306.15063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15063
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;transformer&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#29616;&#20102;&#38750;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20854;&#22312;&#20219;&#21153;&#22810;&#26679;&#24615;&#38408;&#20540;&#20197;&#19979;&#34920;&#29616;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#65292;&#32780;&#22312;&#38408;&#20540;&#20197;&#19978;&#26126;&#26174;&#20248;&#20110;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#65292;&#19982;&#23725;&#22238;&#24402;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;transformer&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#38054;&#20329;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65288;ICL&#65289;&#65306;&#23427;&#20204;&#21487;&#20197;&#20174;&#20165;&#25552;&#20379;&#22312;&#25552;&#31034;&#20013;&#30340;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#26356;&#26032;&#20219;&#20309;&#26435;&#37325;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;ICL&#33021;&#22815;&#35299;&#20915;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#12289;&#22312;&#26412;&#36136;&#19978;&#19982;&#20043;&#21069;&#20219;&#21153;&#38750;&#24120;&#19981;&#21516;&#30340;&#26032;&#20219;&#21153;&#21527;&#65311;&#20026;&#20102;&#25506;&#32034;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#25913;&#21464;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#65292;&#30740;&#31350;&#20102;ICL&#22312;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#20986;&#29616;ICL&#30340;&#20219;&#21153;&#22810;&#26679;&#24615;&#38408;&#20540;&#12290;&#22312;&#36825;&#20010;&#38408;&#20540;&#20197;&#19979;&#65292;&#39044;&#35757;&#32451;&#30340;transformer&#26080;&#27861;&#35299;&#20915;&#26410;&#35265;&#30340;&#22238;&#24402;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#30340;&#34892;&#20026;&#31867;&#20284;&#20110;&#20855;&#26377;&#38750;&#22810;&#26679;&#24615;&#39044;&#35757;&#32451;&#20219;&#21153;&#20998;&#24067;&#20316;&#20026;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#12290;&#36229;&#36807;&#36825;&#20010;&#38408;&#20540;&#21518;&#65292;transformer&#26126;&#26174;&#20248;&#20110;&#36825;&#20010;&#20272;&#35745;&#22120;&#65307;&#23427;&#30340;&#34892;&#20026;&#19982;&#23725;&#22238;&#24402;&#19968;&#33268;&#65292;&#23545;$\textit{&#25152;&#26377;&#20219;&#21153;}$&#65292;&#21253;&#25324;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#20855;&#26377;&#39640;&#26031;&#20808;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained transformers exhibit the remarkable ability of in-context learning (ICL): they can learn tasks from just a few examples provided in the prompt without updating any weights. This raises a foundational question: can ICL solve fundamentally $\textit{new}$ tasks that are very different from those seen during pretraining? To probe this question, we examine ICL's performance on linear regression while varying the diversity of tasks in the pretraining dataset. We empirically demonstrate a $\textit{task diversity threshold}$ for the emergence of ICL. Below this threshold, the pretrained transformer cannot solve unseen regression tasks as it behaves like a Bayesian estimator with the $\textit{non-diverse pretraining task distribution}$ as the prior. Beyond this threshold, the transformer significantly outperforms this estimator; its behavior aligns with that of ridge regression, corresponding to a Gaussian prior over $\textit{all tasks}$, including those not seen during pretraining. 
&lt;/p&gt;</description></item><item><title>BatchGFN&#26159;&#19968;&#31181;&#29992;&#20110;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#26681;&#25454;&#25209;&#37327;&#22870;&#21169;&#37319;&#26679;&#25968;&#25454;&#28857;&#38598;&#21512;&#65292;&#33021;&#22815;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#26500;&#24314;&#39640;&#24230;&#20449;&#24687;&#37327;&#30340;&#25209;&#37327;&#65292;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#12290;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#38388;&#20869;&#36827;&#34892;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#26469;&#37319;&#26679;&#36817;&#20046;&#26368;&#20248;&#25928;&#29992;&#30340;&#25209;&#37327;&#65292;&#20943;&#36731;&#20102;&#38754;&#21521;&#25209;&#37327;&#30340;&#31639;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#28040;&#38500;&#20102;&#36138;&#23146;&#36817;&#20284;&#30340;&#38656;&#27714;&#12290;&#25552;&#20986;&#20102;&#36328;&#33719;&#21462;&#27493;&#39588;&#20998;&#25674;&#35757;&#32451;&#30340;&#26089;&#26399;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#23545;&#23454;&#38469;&#20219;&#21153;&#30340;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.15058</link><description>&lt;p&gt;
BatchGFN: &#29992;&#20110;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#30340;&#29983;&#25104;&#27969;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
BatchGFN: Generative Flow Networks for Batch Active Learning. (arXiv:2306.15058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15058
&lt;/p&gt;
&lt;p&gt;
BatchGFN&#26159;&#19968;&#31181;&#29992;&#20110;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#26681;&#25454;&#25209;&#37327;&#22870;&#21169;&#37319;&#26679;&#25968;&#25454;&#28857;&#38598;&#21512;&#65292;&#33021;&#22815;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#26500;&#24314;&#39640;&#24230;&#20449;&#24687;&#37327;&#30340;&#25209;&#37327;&#65292;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#12290;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#38388;&#20869;&#36827;&#34892;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#26469;&#37319;&#26679;&#36817;&#20046;&#26368;&#20248;&#25928;&#29992;&#30340;&#25209;&#37327;&#65292;&#20943;&#36731;&#20102;&#38754;&#21521;&#25209;&#37327;&#30340;&#31639;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#28040;&#38500;&#20102;&#36138;&#23146;&#36817;&#20284;&#30340;&#38656;&#27714;&#12290;&#25552;&#20986;&#20102;&#36328;&#33719;&#21462;&#27493;&#39588;&#20998;&#25674;&#35757;&#32451;&#30340;&#26089;&#26399;&#32467;&#26524;&#65292;&#23454;&#29616;&#20102;&#23545;&#23454;&#38469;&#20219;&#21153;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;BatchGFN - &#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27744;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#26681;&#25454;&#25209;&#37327;&#22870;&#21169;&#37319;&#26679;&#25968;&#25454;&#28857;&#38598;&#21512;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#37327;&#21270;&#33719;&#21462;&#25209;&#37327;&#30340;&#25928;&#29992;&#65292;&#22914;&#25209;&#37327;&#19982;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#30340;&#32852;&#21512;&#20114;&#20449;&#24687;&#65292;BatchGFN&#33021;&#22815;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#26500;&#24314;&#39640;&#24230;&#20449;&#24687;&#37327;&#30340;&#25209;&#37327;&#65292;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29609;&#20855;&#22238;&#24402;&#38382;&#39064;&#20013;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#38388;&#20869;&#36890;&#36807;&#23545;&#25209;&#37327;&#20013;&#27599;&#20010;&#28857;&#36827;&#34892;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#26469;&#37319;&#26679;&#36817;&#20046;&#26368;&#20248;&#25928;&#29992;&#30340;&#25209;&#37327;&#65292;&#36825;&#20943;&#36731;&#20102;&#38754;&#21521;&#25209;&#37327;&#30340;&#31639;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#28040;&#38500;&#20102;&#23547;&#25214;&#25209;&#37327;&#22870;&#21169;&#26368;&#22823;&#21270;&#22120;&#30340;&#36138;&#23146;&#36817;&#20284;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#36328;&#33719;&#21462;&#27493;&#39588;&#20998;&#25674;&#35757;&#32451;&#30340;&#26089;&#26399;&#32467;&#26524;&#65292;&#36825;&#23558;&#23454;&#29616;&#23545;&#23454;&#38469;&#20219;&#21153;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce BatchGFN -- a novel approach for pool-based active learning that uses generative flow networks to sample sets of data points proportional to a batch reward. With an appropriate reward function to quantify the utility of acquiring a batch, such as the joint mutual information between the batch and the model parameters, BatchGFN is able to construct highly informative batches for active learning in a principled way. We show our approach enables sampling near-optimal utility batches at inference time with a single forward pass per point in the batch in toy regression problems. This alleviates the computational complexity of batch-aware algorithms and removes the need for greedy approximations to find maximizers for the batch reward. We also present early results for amortizing training across acquisition steps, which will enable scaling to real-world tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;&#26368;&#20248;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#35757;&#32451;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#26102;&#22914;&#20309;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.15056</link><description>&lt;p&gt;
&#20855;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;&#26368;&#20248;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal Differentially Private Learning with Public Data. (arXiv:2306.15056v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;&#26368;&#20248;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#35757;&#32451;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#26102;&#22914;&#20309;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#33021;&#22815;&#30830;&#20445;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19981;&#27844;&#28431;&#31169;&#23494;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24046;&#20998;&#38544;&#31169;&#30340;&#20195;&#20215;&#26159;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#25110;&#26679;&#26412;&#22797;&#26434;&#24230;&#22686;&#21152;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#21487;&#33021;&#21487;&#20197;&#35775;&#38382;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#30340;&#36741;&#21161;&#20844;&#20849;&#25968;&#25454;&#12290;&#36825;&#20419;&#20351;&#20102;&#26368;&#36817;&#30740;&#31350;&#20844;&#20849;&#25968;&#25454;&#22312;&#25552;&#39640;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#26377;&#19968;&#23450;&#25968;&#37327;&#30340;&#20844;&#20849;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#20197;&#19979;&#22522;&#26412;&#24320;&#25918;&#38382;&#39064;&#65306;1.&#22312;&#26377;&#20844;&#20849;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#22522;&#20110;&#31169;&#26377;&#25968;&#25454;&#38598;&#30340;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#30340;&#26368;&#20248;&#65288;&#26368;&#22351;&#24773;&#20917;&#65289;&#35823;&#24046;&#26159;&#22810;&#23569;&#65311;&#21738;&#20123;&#31639;&#27861;&#26159;&#26368;&#20248;&#30340;&#65311;2.&#22914;&#20309;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#22312;&#23454;&#36341;&#20013;&#25913;&#36827;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#35757;&#32451;&#65311;&#25105;&#20204;&#22312;&#26412;&#22320;&#27169;&#22411;&#21644;&#20013;&#24515;&#27169;&#22411;&#30340;&#24046;&#20998;&#38544;&#31169;&#38382;&#39064;&#19979;&#32771;&#34385;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#22238;&#31572;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#19977;&#20010;&#22522;&#26412;&#38382;&#39064;&#30340;&#26368;&#20248;&#35823;&#24046;&#29575;&#30340;&#32039;&#23494;&#65288;&#26368;&#39640;&#24120;&#25968;&#22240;&#23376;&#65289;&#19979;&#30028;&#21644;&#19978;&#30028;&#12290;&#36825;&#19977;&#20010;&#38382;&#39064;&#26159;&#65306;&#22343;&#20540;&#20272;&#35745;&#65292;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21644;&#20984;&#22855;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential Privacy (DP) ensures that training a machine learning model does not leak private data. However, the cost of DP is lower model accuracy or higher sample complexity. In practice, we may have access to auxiliary public data that is free of privacy concerns. This has motivated the recent study of what role public data might play in improving the accuracy of DP models. In this work, we assume access to a given amount of public data and settle the following fundamental open questions: 1. What is the optimal (worst-case) error of a DP model trained over a private data set while having access to side public data? What algorithms are optimal? 2. How can we harness public data to improve DP model training in practice? We consider these questions in both the local and central models of DP. To answer the first question, we prove tight (up to constant factors) lower and upper bounds that characterize the optimal error rates of three fundamental problems: mean estimation, empirical ris
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#30340;Sybil&#27602;&#21270;&#25915;&#20987;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SybilWall&#30340;&#21019;&#26032;&#31639;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#23545;&#36825;&#31867;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15044</link><description>&lt;p&gt;
&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#20013;&#30340;Sybil&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Sybil Resilience in Decentralized Learning. (arXiv:2306.15044v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#30340;Sybil&#27602;&#21270;&#25915;&#20987;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SybilWall&#30340;&#21019;&#26032;&#31639;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#23545;&#36825;&#31867;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#21463;&#21040;&#21487;&#25193;&#23637;&#24615;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#38480;&#21046;&#20027;&#35201;&#26469;&#33258;&#20110;&#20013;&#22830;&#21442;&#25968;&#26381;&#21153;&#22120;&#30340;&#20114;&#32852;&#32593;&#36830;&#25509;&#21644;&#20869;&#23384;&#23481;&#37327;&#65292;&#20197;&#21450;&#27169;&#22411;&#32858;&#21512;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#20316;&#20026;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26367;&#20195;&#26041;&#26696;&#36880;&#28176;&#20852;&#36215;&#12290;&#36825;&#31181;&#26032;&#25216;&#26415;&#36890;&#36807;&#23558;&#27169;&#22411;&#32858;&#21512;&#20998;&#25955;&#21040;&#25152;&#26377;&#21442;&#19982;&#33410;&#28857;&#20013;&#65292;&#28040;&#38500;&#20102;&#23545;&#20013;&#22830;&#21442;&#25968;&#26381;&#21153;&#22120;&#30340;&#38656;&#27714;&#12290;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#38024;&#23545;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#23545;&#27602;&#21270;&#25915;&#20987;&#21644;Sybil&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#32780;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#20173;&#28982;&#40092;&#26377;&#30740;&#31350;&#12290;&#36825;&#20010;&#30740;&#31350;&#31354;&#30333;&#26159;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#21160;&#26426;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#39640;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#30340;Sybil&#27602;&#21270;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SybilWall&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#27880;&#20110;&#22686;&#24378;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#23545;Sybil&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#30340;&#21019;&#26032;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a privacy-enforcing machine learning technology but suffers from limited scalability. This limitation mostly originates from the internet connection and memory capacity of the central parameter server, and the complexity of the model aggregation function. Decentralized learning has recently been emerging as a promising alternative to federated learning. This novel technology eliminates the need for a central parameter server by decentralizing the model aggregation across all participating nodes. Numerous studies have been conducted on improving the resilience of federated learning against poisoning and Sybil attacks, whereas the resilience of decentralized learning remains largely unstudied. This research gap serves as the main motivator for this study, in which our objective is to improve the Sybil poisoning resilience of decentralized learning.  We present SybilWall, an innovative algorithm focused on increasing the resilience of decentralized learning against t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#27969;&#21305;&#37197;&#30340;&#31561;&#21464;CNF&#35757;&#32451;&#30446;&#26631;&#65292;&#21487;&#20197;&#25552;&#39640;&#31561;&#21464;CNF&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.15030</link><description>&lt;p&gt;
&#31561;&#21464;&#27969;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Equivariant flow matching. (arXiv:2306.15030v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#27969;&#21305;&#37197;&#30340;&#31561;&#21464;CNF&#35757;&#32451;&#30446;&#26631;&#65292;&#21487;&#20197;&#25552;&#39640;&#31561;&#21464;CNF&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#21270;&#27969;&#26159;&#19968;&#31867;&#29305;&#21035;&#36866;&#29992;&#20110;&#29289;&#29702;&#23398;&#20013;&#27010;&#29575;&#20998;&#24067;&#24314;&#27169;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290;&#20854;&#20013;&#65292;&#27969;&#30340;&#20934;&#30830;&#20284;&#28982;&#24615;&#36136;&#21487;&#20197;&#23454;&#29616;&#23545;&#24050;&#30693;&#30446;&#26631;&#33021;&#37327;&#20989;&#25968;&#30340;&#21152;&#26435;&#37325;&#37325;&#21644;&#26080;&#20559;&#35266;&#27979;&#37327;&#30340;&#35745;&#31639;&#12290;&#20363;&#22914;&#65292;Boltzmann&#29983;&#25104;&#22120;&#36890;&#36807;&#35757;&#32451;&#27969;&#29983;&#25104;&#22788;&#20110;&#24179;&#34913;&#29366;&#24577;&#30340;&#22810;&#20307;&#31995;&#32479;&#65288;&#22914;&#23567;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#65289;&#26679;&#26412;&#65292;&#35299;&#20915;&#20102;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#37319;&#26679;&#38382;&#39064;&#12290;&#20026;&#20102;&#26500;&#24314;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#20063;&#24456;&#20851;&#38190;&#23558;&#30446;&#26631;&#33021;&#37327;&#30340;&#23545;&#31216;&#24615;&#32435;&#20837;&#27169;&#22411;&#20013;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#31561;&#21464;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#65288;CNF&#65289;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;CNF&#30340;&#35757;&#32451;&#21644;&#26679;&#26412;&#29983;&#25104;&#30340;&#35745;&#31639;&#24320;&#38144;&#36739;&#22823;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31561;&#21464;&#27969;&#21305;&#37197;&#65292;&#19968;&#31181;&#26032;&#30340;&#31561;&#21464;CNF&#35757;&#32451;&#30446;&#26631;&#65292;&#20854;&#22522;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#26368;&#20248;&#36755;&#36816;&#27969;&#21305;&#37197;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalizing flows are a class of deep generative models that are especially interesting for modeling probability distributions in physics, where the exact likelihood of flows allows reweighting to known target energy functions and computing unbiased observables. For instance, Boltzmann generators tackle the long-standing sampling problem in statistical physics by training flows to produce equilibrium samples of many-body systems such as small molecules and proteins. To build effective models for such systems, it is crucial to incorporate the symmetries of the target energy into the model, which can be achieved by equivariant continuous normalizing flows (CNFs). However, CNFs can be computationally expensive to train and generate samples from, which has hampered their scalability and practical application. In this paper, we introduce equivariant flow matching, a new training objective for equivariant CNFs that is based on the recently proposed optimal transport flow matching. Equivarian
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#24471;&#20998;&#23551;&#21629;&#35268;&#21010;&#65292;&#29992;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#26041;&#27861;&#21487;&#20197;&#25628;&#32034;&#38750;&#31283;&#24577;&#31574;&#30053;&#20989;&#25968;&#65292;&#24182;&#30452;&#25509;&#35745;&#31639;&#26368;&#20248;&#26080;&#38480;&#26102;&#38388;&#21306;&#38388;&#21160;&#20316;&#24207;&#21015;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#24314;&#31435;&#21160;&#20316;&#24207;&#21015;&#21644;&#23454;&#25968;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#35745;&#31639;&#26368;&#20248;&#21160;&#20316;&#24207;&#21015;&#12290;&#26041;&#27861;&#22312;&#38750;&#32447;&#24615;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15029</link><description>&lt;p&gt;
&#36229;&#36234;&#21160;&#24577;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Beyond dynamic programming. (arXiv:2306.15029v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15029
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#24471;&#20998;&#23551;&#21629;&#35268;&#21010;&#65292;&#29992;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#26041;&#27861;&#21487;&#20197;&#25628;&#32034;&#38750;&#31283;&#24577;&#31574;&#30053;&#20989;&#25968;&#65292;&#24182;&#30452;&#25509;&#35745;&#31639;&#26368;&#20248;&#26080;&#38480;&#26102;&#38388;&#21306;&#38388;&#21160;&#20316;&#24207;&#21015;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#24314;&#31435;&#21160;&#20316;&#24207;&#21015;&#21644;&#23454;&#25968;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#35745;&#31639;&#26368;&#20248;&#21160;&#20316;&#24207;&#21015;&#12290;&#26041;&#27861;&#22312;&#38750;&#32447;&#24615;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#24471;&#20998;&#23551;&#21629;&#35268;&#21010;&#65292;&#29992;&#20110;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#21160;&#24577;&#35268;&#21010;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25628;&#32034;&#38750;&#31283;&#24577;&#31574;&#30053;&#20989;&#25968;&#65292;&#24182;&#19988;&#21487;&#20197;&#30452;&#25509;&#35745;&#31639;&#32473;&#23450;&#29366;&#24577;&#19979;&#30340;&#26368;&#20248;&#26080;&#38480;&#26102;&#38388;&#21306;&#38388;&#21160;&#20316;&#24207;&#21015;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#24314;&#31435;&#26080;&#38480;&#26102;&#38388;&#21306;&#38388;&#21160;&#20316;&#24207;&#21015;&#21644;&#26377;&#30028;&#21306;&#38388;&#20869;&#23454;&#25968;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#36825;&#31181;&#26500;&#36896;&#20351;&#25105;&#20204;&#33021;&#22815;&#21046;&#23450;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#30452;&#25509;&#35745;&#31639;&#26368;&#20248;&#26080;&#38480;&#26102;&#38388;&#21306;&#38388;&#21160;&#20316;&#24207;&#21015;&#65292;&#26080;&#38656;&#31574;&#30053;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#38750;&#32447;&#24615;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#20026;&#21046;&#23450;&#21644;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present Score-life programming, a novel theoretical approach for solving reinforcement learning problems. In contrast with classical dynamic programming-based methods, our method can search over non-stationary policy functions, and can directly compute optimal infinite horizon action sequences from a given state. The central idea in our method is the construction of a mapping between infinite horizon action sequences and real numbers in a bounded interval. This construction enables us to formulate an optimization problem for directly computing optimal infinite horizon action sequences, without requiring a policy function. We demonstrate the effectiveness of our approach by applying it to nonlinear optimal control problems. Overall, our contributions provide a novel theoretical framework for formulating and solving reinforcement learning problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20851;&#20110;&#21069;&#39304;&#32593;&#32476;&#20013;&#30340;&#32553;&#25918;&#21644;&#35843;&#25972;&#23545;&#31216;&#24615;&#30340;&#21457;&#29616;&#65292;&#24182;&#35828;&#26126;&#20102;&#22312;&#20020;&#30028;&#28857;&#19978;&#30340;&#21021;&#22987;&#21270;&#26041;&#24335;&#23545;&#20110;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.15015</link><description>&lt;p&gt;
&#22312;&#21069;&#39304;&#32593;&#32476;&#20013;&#30340;&#32553;&#25918;&#21644;&#35843;&#25972;&#23545;&#31216;&#24615;
&lt;/p&gt;
&lt;p&gt;
Scaling and Resizing Symmetry in Feedforward Networks. (arXiv:2306.15015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20851;&#20110;&#21069;&#39304;&#32593;&#32476;&#20013;&#30340;&#32553;&#25918;&#21644;&#35843;&#25972;&#23545;&#31216;&#24615;&#30340;&#21457;&#29616;&#65292;&#24182;&#35828;&#26126;&#20102;&#22312;&#20020;&#30028;&#28857;&#19978;&#30340;&#21021;&#22987;&#21270;&#26041;&#24335;&#23545;&#20110;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#23545;&#23398;&#20064;&#26144;&#23556;&#30340;&#25910;&#25947;&#36895;&#24230;&#26377;&#24456;&#22823;&#24433;&#21709;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#26435;&#37325;&#21644;&#20559;&#24046;&#30340;&#26041;&#24046;&#31354;&#38388;&#20250;&#21457;&#29983;&#28151;&#27788;/&#26377;&#24207;&#30456;&#21464;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22914;&#26524;&#22312;&#20020;&#30028;&#32447;&#19978;&#30340;&#25968;&#20540;&#19978;&#21021;&#22987;&#21270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#24456;&#22823;&#25913;&#36827;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#34920;&#26126;&#22312;&#20020;&#30028;&#32447;&#19978;&#30340;&#26410;&#35757;&#32451;&#21069;&#39304;&#32593;&#32476;&#20013;&#20063;&#23384;&#22312;&#29289;&#29702;&#31995;&#32479;&#22312;&#20020;&#30028;&#28857;&#25152;&#34920;&#29616;&#20986;&#30340;&#32553;&#25918;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#25968;&#25454;&#35843;&#25972;&#23545;&#31216;&#24615;&#65292;&#30452;&#25509;&#32487;&#25215;&#33258;&#20020;&#30028;&#28857;&#30340;&#32553;&#25918;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weights initialization in deep neural networks have a strong impact on the speed of converge of the learning map. Recent studies have shown that in the case of random initializations, a chaos/order phase transition occur in the space of variances of random weights and biases. Experiments then had shown that large improvements can be made, in terms of the training speed, if a neural network is initialized on values along the critical line of such phase transition. In this contribution, we show evidence that the scaling property exhibited by physical systems at criticality, is also present in untrained feedforward networks with random weights initialization at the critical line. Additionally, we suggest an additional data-resizing symmetry, which is directly inherited from the scaling symmetry at criticality.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#22122;&#22768;&#28151;&#21512;&#29289;&#20013;&#24674;&#22797;&#30446;&#26631;&#20449;&#21495;&#30340;&#32479;&#35745;&#20998;&#37327;&#20998;&#31163;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#22270;&#20687;&#38477;&#22122;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#26631;&#20934;&#38477;&#22122;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.15012</link><description>&lt;p&gt;
&#29992;&#20110;&#22122;&#22768;&#28151;&#21512;&#29289;&#20013;&#30446;&#26631;&#20449;&#21495;&#24674;&#22797;&#30340;&#32479;&#35745;&#20998;&#37327;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Statistical Component Separation for Targeted Signal Recovery in Noisy Mixtures. (arXiv:2306.15012v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#22122;&#22768;&#28151;&#21512;&#29289;&#20013;&#24674;&#22797;&#30446;&#26631;&#20449;&#21495;&#30340;&#32479;&#35745;&#20998;&#37327;&#20998;&#31163;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#22270;&#20687;&#38477;&#22122;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#26631;&#20934;&#38477;&#22122;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21482;&#23545;&#32473;&#23450;&#20449;&#21495;&#30340;&#29305;&#23450;&#23646;&#24615;&#24863;&#20852;&#36259;&#26102;&#65292;&#20174;&#19968;&#20010;&#21152;&#24615;&#28151;&#21512;&#29289;&#20013;&#20998;&#31163;&#20449;&#21495;&#21487;&#33021;&#26159;&#19968;&#20010;&#19981;&#24517;&#35201;&#22320;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#26356;&#31616;&#21333;&#30340;&#8220;&#32479;&#35745;&#20998;&#37327;&#20998;&#31163;&#8221;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#19987;&#27880;&#20110;&#20174;&#22122;&#22768;&#28151;&#21512;&#29289;&#20013;&#24674;&#22797;&#30446;&#26631;&#20449;&#21495;&#30340;&#39044;&#23450;&#20041;&#32479;&#35745;&#25551;&#36848;&#37327;&#12290;&#20551;&#35774;&#21487;&#20197;&#33719;&#24471;&#22122;&#22768;&#36807;&#31243;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#20351;&#21463;&#22122;&#22768;&#26679;&#26412;&#27745;&#26579;&#30340;&#35299;&#20915;&#26041;&#26696;&#20505;&#36873;&#30340;&#32479;&#35745;&#29305;&#24615;&#19982;&#35266;&#27979;&#30340;&#28151;&#21512;&#29289;&#30340;&#32479;&#35745;&#29305;&#24615;&#21305;&#37197;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#35299;&#26512;&#21487;&#36861;&#36394;&#35745;&#31639;&#30340;&#31616;&#21333;&#31034;&#20363;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#30340;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#22270;&#20687;&#38477;&#22122;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#20102;1&#65289;&#22522;&#20110;&#23567;&#27874;&#30340;&#25551;&#36848;&#31526;&#65292;2&#65289;&#38024;&#23545;&#22825;&#20307;&#29289;&#29702;&#21644;ImageNet&#25968;&#25454;&#30340;ConvNet-based&#25551;&#36848;&#31526;&#12290;&#22312;&#31532;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#27604;&#26631;&#20934;&#38477;&#22122;&#26041;&#27861;&#26356;&#22909;&#22320;&#24674;&#22797;&#20102;&#30446;&#26631;&#25968;&#25454;&#30340;&#25551;&#36848;&#31526;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#19981;&#26159;&#20026;&#27492;&#30446;&#30340;&#26500;&#24314;&#30340;&#65292;&#23427;&#20063;&#34920;&#29616;&#20986;&#23545;&#30446;&#26631;&#20449;&#21495;&#25551;&#36848;&#31526;&#24674;&#22797;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Separating signals from an additive mixture may be an unnecessarily hard problem when one is only interested in specific properties of a given signal. In this work, we tackle simpler "statistical component separation" problems that focus on recovering a predefined set of statistical descriptors of a target signal from a noisy mixture. Assuming access to samples of the noise process, we investigate a method devised to match the statistics of the solution candidate corrupted by noise samples with those of the observed mixture. We first analyze the behavior of this method using simple examples with analytically tractable calculations. Then, we apply it in an image denoising context employing 1) wavelet-based descriptors, 2) ConvNet-based descriptors on astrophysics and ImageNet data. In the case of 1), we show that our method better recovers the descriptors of the target data than a standard denoising method in most situations. Additionally, despite not constructed for this purpose, it pe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26080;&#30417;&#30563;&#20998;&#31867;&#26041;&#27861;&#23545;&#27169;&#25311;&#21644;&#35266;&#27979;&#30340;Sentinel-2/MSI&#22270;&#20687;&#20013;&#30340;&#28023;&#27915;&#22403;&#22334;&#36827;&#34892;&#20809;&#35889;&#20998;&#26512;&#65292;&#21457;&#29616;&#27745;&#26579;&#29289;&#30340;&#20809;&#35889;&#34892;&#20026;&#21463;&#22810;&#31181;&#22240;&#32032;&#24433;&#21709;&#65292;&#24182;&#25581;&#31034;&#20102;&#20803;&#32032;&#20043;&#38388;&#30340;&#20809;&#35889;&#29305;&#24449;&#21644;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.15008</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#30417;&#30563;&#20998;&#31867;&#23545;&#27169;&#25311;&#21644;&#35266;&#27979;&#30340;Sentinel-2/MSI&#22270;&#20687;&#20013;&#30340;&#28023;&#27915;&#22403;&#22334;&#36827;&#34892;&#20809;&#35889;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Spectral Analysis of Marine Debris in Simulated and Observed Sentinel-2/MSI Images using Unsupervised Classification. (arXiv:2306.15008v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26080;&#30417;&#30563;&#20998;&#31867;&#26041;&#27861;&#23545;&#27169;&#25311;&#21644;&#35266;&#27979;&#30340;Sentinel-2/MSI&#22270;&#20687;&#20013;&#30340;&#28023;&#27915;&#22403;&#22334;&#36827;&#34892;&#20809;&#35889;&#20998;&#26512;&#65292;&#21457;&#29616;&#27745;&#26579;&#29289;&#30340;&#20809;&#35889;&#34892;&#20026;&#21463;&#22810;&#31181;&#22240;&#32032;&#24433;&#21709;&#65292;&#24182;&#25581;&#31034;&#20102;&#20803;&#32032;&#20043;&#38388;&#30340;&#20809;&#35889;&#29305;&#24449;&#21644;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#22403;&#22334;&#23545;&#28023;&#27915;&#21644;&#27839;&#28023;&#29615;&#22659;&#36896;&#25104;&#37325;&#22823;&#23041;&#32961;&#65292;&#20854;&#24433;&#21709;&#19981;&#26029;&#22686;&#38271;&#12290;&#36965;&#24863;&#25216;&#26415;&#36890;&#36807;&#24191;&#27867;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#39057;&#32321;&#30340;&#35266;&#27979;&#25552;&#20379;&#20102;&#20256;&#32479;&#32531;&#35299;&#25216;&#26415;&#30340;&#26377;&#21033;&#34917;&#20805;&#65292;&#22914;&#24403;&#22320;&#28165;&#29702;&#34892;&#21160;&#21644;&#25302;&#32593;&#35843;&#26597;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#36752;&#23556;&#20256;&#36755;&#27169;&#22411;&#65288;RTM&#65289;&#27169;&#25311;&#25968;&#25454;&#21644;Sentinel-2&#20219;&#21153;&#30340;&#22810;&#20809;&#35889;&#20202;&#65288;MSI&#65289;&#30340;&#25968;&#25454;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30740;&#31350;&#28023;&#27915;&#22609;&#26009;&#27745;&#26579;&#30340;&#20809;&#35889;&#34892;&#20026;&#65292;&#24182;&#35780;&#20272;RTM&#22312;&#35813;&#30740;&#31350;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#12290;&#25506;&#32034;&#24615;&#20998;&#26512;&#21644;&#20351;&#29992;KMeans&#31639;&#27861;&#30340;&#26080;&#30417;&#30563;&#20998;&#31867;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27745;&#26579;&#29289;&#30340;&#20809;&#35889;&#34892;&#20026;&#21463;&#21040;&#32858;&#21512;&#29289;&#31867;&#22411;&#21644;&#20687;&#32032;&#35206;&#30422;&#30334;&#20998;&#27604;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#25581;&#31034;&#20102;&#20803;&#32032;&#20043;&#38388;&#30340;&#20809;&#35889;&#29305;&#24449;&#12289;&#20851;&#32852;&#21644;&#24046;&#24322;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Marine litter poses significant threats to marine and coastal environments, with its impacts ever-growing. Remote sensing provides an advantageous supplement to traditional mitigation techniques, such as local cleaning operations and trawl net surveys, due to its capabilities for extensive coverage and frequent observation. In this study, we used Radiative Transfer Model (RTM) simulated data and data from the Multispectral Instrument (MSI) of the Sentinel-2 mission in combination with machine learning algorithms. Our aim was to study the spectral behavior of marine plastic pollution and evaluate the applicability of RTMs within this research area. The results from the exploratory analysis and unsupervised classification using the KMeans algorithm indicate that the spectral behavior of pollutants is influenced by factors such as the type of polymer and pixel coverage percentage. The findings also reveal spectral characteristics and trends of association and differentiation among element
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37319;&#35775;&#23454;&#36341;&#32773;&#21644;&#36827;&#34892;&#35843;&#26597;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#36719;&#20214;&#31995;&#32479;&#20013;&#30340;&#36136;&#37327;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20010;&#36136;&#37327;&#38382;&#39064;&#30446;&#24405;&#12290;</title><link>http://arxiv.org/abs/2306.15007</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36719;&#20214;&#31995;&#32479;&#20013;&#30340;&#36136;&#37327;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Quality Issues in Machine Learning Software Systems. (arXiv:2306.15007v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37319;&#35775;&#23454;&#36341;&#32773;&#21644;&#36827;&#34892;&#35843;&#26597;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#36719;&#20214;&#31995;&#32479;&#20013;&#30340;&#36136;&#37327;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20010;&#36136;&#37327;&#38382;&#39064;&#30446;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#65306;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#38656;&#27714;&#26159;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;ML&#27169;&#22411;&#34987;&#23454;&#29616;&#20026;&#36719;&#20214;&#32452;&#20214;&#24182;&#37096;&#32626;&#22312;&#26426;&#22120;&#23398;&#20064;&#36719;&#20214;&#31995;&#32479;&#65288;MLSSs&#65289;&#20013;&#12290;&#38382;&#39064;&#65306;&#26377;&#24517;&#35201;&#30830;&#20445;MLSSs&#30340;&#26381;&#21153;&#36136;&#37327;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#38169;&#35823;&#25110;&#19981;&#33391;&#20915;&#31574;&#21487;&#33021;&#23548;&#33268;&#20854;&#20182;&#31995;&#32479;&#30340;&#25925;&#38556;&#65292;&#36896;&#25104;&#24040;&#22823;&#30340;&#36130;&#21153;&#25439;&#22833;&#65292;&#29978;&#33267;&#23545;&#20154;&#31867;&#29983;&#21629;&#26500;&#25104;&#23041;&#32961;&#12290; MLSSs&#30340;&#36136;&#37327;&#20445;&#35777;&#34987;&#35748;&#20026;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30446;&#21069;&#26159;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#30446;&#26631;&#65306;&#26412;&#25991;&#26088;&#22312;&#20174;&#23454;&#36341;&#32773;&#30340;&#35282;&#24230;&#30740;&#31350;MLSSs&#20013;&#30495;&#23454;&#36136;&#37327;&#38382;&#39064;&#30340;&#29305;&#24449;&#12290;&#36825;&#39033;&#23454;&#35777;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;MLSSs&#20013;&#30340;&#36136;&#37327;&#38382;&#39064;&#30446;&#24405;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#19982;&#23454;&#36341;&#32773;/&#19987;&#23478;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#37319;&#35775;&#65292;&#20197;&#33719;&#21462;&#20182;&#20204;&#22788;&#29702;&#36136;&#37327;&#38382;&#39064;&#26102;&#30340;&#32463;&#39564;&#21644;&#20570;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;ML&#20174;&#19994;&#32773;&#30340;&#35843;&#26597;&#39564;&#35777;&#20102;&#25152;&#30830;&#23450;&#30340;&#36136;&#37327;&#38382;&#39064;&#12290;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Context: An increasing demand is observed in various domains to employ Machine Learning (ML) for solving complex problems. ML models are implemented as software components and deployed in Machine Learning Software Systems (MLSSs). Problem: There is a strong need for ensuring the serving quality of MLSSs. False or poor decisions of such systems can lead to malfunction of other systems, significant financial losses, or even threats to human life. The quality assurance of MLSSs is considered a challenging task and currently is a hot research topic. Objective: This paper aims to investigate the characteristics of real quality issues in MLSSs from the viewpoint of practitioners. This empirical study aims to identify a catalog of quality issues in MLSSs. Method: We conduct a set of interviews with practitioners/experts, to gather insights about their experience and practices when dealing with quality issues. We validate the identified quality issues via a survey with ML practitioners. Result
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;LM4HPC&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;LMs&#26469;&#20998;&#26512;&#21644;&#20248;&#21270;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#36719;&#20214;&#12290;&#36890;&#36807;&#25903;&#25345;HPC&#25968;&#25454;&#38598;&#12289;AI&#27169;&#22411;&#21644;&#27969;&#27700;&#32447;&#65292;LM4HPC&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#24555;&#36895;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#24182;&#29983;&#25104;&#26377;&#35265;&#22320;&#30340;&#25490;&#34892;&#27036;&#12290;</title><link>http://arxiv.org/abs/2306.14979</link><description>&lt;p&gt;
LM4HPC&#65306;&#38754;&#21521;&#39640;&#24615;&#33021;&#35745;&#31639;&#30340;&#26377;&#25928;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
LM4HPC: Towards Effective Language Model Application in High-Performance Computing. (arXiv:2306.14979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;LM4HPC&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;LMs&#26469;&#20998;&#26512;&#21644;&#20248;&#21270;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#36719;&#20214;&#12290;&#36890;&#36807;&#25903;&#25345;HPC&#25968;&#25454;&#38598;&#12289;AI&#27169;&#22411;&#21644;&#27969;&#27700;&#32447;&#65292;LM4HPC&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#24555;&#36895;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#24182;&#29983;&#25104;&#26377;&#35265;&#22320;&#30340;&#25490;&#34892;&#27036;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#35832;&#22914;GPT-4&#20043;&#31867;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#21487;&#35270;&#21270;&#31561;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#29305;&#23450;&#30340;&#25903;&#25345;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#20998;&#26512;&#21644;&#20248;&#21270;HPC&#36719;&#20214;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;LM4HPC&#26694;&#26550;&#65292;&#20197;&#20415;&#21033;&#29992;LMs&#36827;&#34892;HPC&#36719;&#20214;&#20998;&#26512;&#21644;&#20248;&#21270;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19987;&#20026;&#25903;&#25345;HPC&#25968;&#25454;&#38598;&#12289;AI&#27169;&#22411;&#21644;&#27969;&#27700;&#32447;&#32780;&#35774;&#35745;&#65292;&#26500;&#24314;&#22312;&#26426;&#22120;&#23398;&#20064;&#36719;&#20214;&#26632;&#30340;&#19981;&#21516;&#23618;&#32423;&#32452;&#20214;&#20043;&#19978;&#65292;&#24182;&#20855;&#26377;Hugging Face&#20860;&#23481;&#30340;API&#12290;&#36890;&#36807;&#19977;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#21407;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LM4HPC&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#24555;&#36895;&#35780;&#20272;&#19968;&#32452;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#24182;&#29983;&#25104;&#26377;&#35265;&#22320;&#30340;&#25490;&#34892;&#27036;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, language models (LMs), such as GPT-4, have been widely used in multiple domains, including natural language processing, visualization, and so on. However, applying them for analyzing and optimizing high-performance computing (HPC) software is still challenging due to the lack of HPC-specific support. In this paper, we design the LM4HPC framework to facilitate the research and development of HPC software analyses and optimizations using LMs. Tailored for supporting HPC datasets, AI models, and pipelines, our framework is built on top of a range of components from different levels of the machine learning software stack, with Hugging Face-compatible APIs. Using three representative tasks, we evaluated the prototype of our framework. The results show that LM4HPC can help users quickly evaluate a set of state-of-the-art models and generate insightful leaderboards.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#23376;&#32676;&#30340;&#20844;&#24179;&#24863;&#30693;&#21453;&#20107;&#23454;&#35770;&#35777;&#65288;FACTS&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#23457;&#26597;&#23376;&#32676;&#30340;&#20844;&#24179;&#24615;&#12290;&#35813;&#26694;&#26550;&#37325;&#26032;&#23450;&#20041;&#20102;&#23376;&#32676;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#26088;&#22312;&#20174;&#24494;&#35266;&#21644;&#23439;&#35266;&#20004;&#20010;&#35282;&#24230;&#32771;&#34385;&#23454;&#29616;&#26399;&#26395;&#32467;&#26524;&#30340;&#22256;&#38590;&#65292;&#24182;&#24341;&#20837;&#20102;&#23545;&#20110;&#23454;&#29616;&#25104;&#26412;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#23376;&#32676;&#20844;&#24179;&#24615;&#27010;&#24565;&#12290;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12289;&#36866;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.14978</link><description>&lt;p&gt;
&#38024;&#23545;&#23376;&#32676;&#30340;&#20844;&#24179;&#24863;&#30693;&#21453;&#20107;&#23454;&#35770;&#35777;
&lt;/p&gt;
&lt;p&gt;
Fairness Aware Counterfactuals for Subgroups. (arXiv:2306.14978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#23376;&#32676;&#30340;&#20844;&#24179;&#24863;&#30693;&#21453;&#20107;&#23454;&#35770;&#35777;&#65288;FACTS&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#23457;&#26597;&#23376;&#32676;&#30340;&#20844;&#24179;&#24615;&#12290;&#35813;&#26694;&#26550;&#37325;&#26032;&#23450;&#20041;&#20102;&#23376;&#32676;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#26088;&#22312;&#20174;&#24494;&#35266;&#21644;&#23439;&#35266;&#20004;&#20010;&#35282;&#24230;&#32771;&#34385;&#23454;&#29616;&#26399;&#26395;&#32467;&#26524;&#30340;&#22256;&#38590;&#65292;&#24182;&#24341;&#20837;&#20102;&#23545;&#20110;&#23454;&#29616;&#25104;&#26412;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#23376;&#32676;&#20844;&#24179;&#24615;&#27010;&#24565;&#12290;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12289;&#36866;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#23376;&#32676;&#30340;&#20844;&#24179;&#24863;&#30693;&#21453;&#20107;&#23454;&#35770;&#35777;&#65288;FACTS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#21453;&#20107;&#23454;&#35299;&#37322;&#26469;&#23457;&#26597;&#23376;&#32676;&#20844;&#24179;&#24615;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#65288;&#24182;&#27867;&#21270;&#65289;&#29616;&#26377;&#27010;&#24565;&#65292;&#24182;&#24341;&#20837;&#26356;&#31934;&#32454;&#30340;&#23376;&#32676;&#20844;&#24179;&#24615;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#65288;a&#65289;&#22312;&#24494;&#35266;&#23618;&#38754;&#19978;&#32771;&#34385;&#21040;&#23376;&#32676;&#25104;&#21592;&#20010;&#20307;&#65292;&#25110;&#22312;&#23439;&#35266;&#23618;&#38754;&#19978;&#32771;&#34385;&#25972;&#20010;&#23376;&#32676;&#65292;&#26469;&#34920;&#36798;&#26576;&#20123;&#23376;&#32676;&#20010;&#20307;&#22312;&#23454;&#29616;&#34917;&#25937;&#65288;&#21363;&#23454;&#29616;&#26399;&#26395;&#32467;&#26524;&#65289;&#26041;&#38754;&#30340;&#22256;&#38590;&#19981;&#21516;&#26041;&#38754;&#65307;&#20197;&#21450;&#65288;b&#65289;&#24341;&#20837;&#23545;&#20110;&#23454;&#29616;&#34917;&#25937;&#25104;&#26412;&#22362;&#22266;&#30340;&#65288;&#22914;&#26524;&#19981;&#26159;&#23436;&#20840;&#26080;&#35270;&#30340;&#65289;&#23376;&#32676;&#20844;&#24179;&#24615;&#27010;&#24565;&#12290;&#25105;&#20204;&#32467;&#21512;&#36825;&#20123;&#27010;&#24565;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#12289;&#27169;&#22411;&#26080;&#20851;&#12289;&#39640;&#24230;&#21487;&#21442;&#25968;&#21270;&#21644;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#26469;&#35780;&#20272;&#23376;&#32676;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#30340;&#24443;&#24213;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#21183;&#12289;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present Fairness Aware Counterfactuals for Subgroups (FACTS), a framework for auditing subgroup fairness through counterfactual explanations. We start with revisiting (and generalizing) existing notions and introducing new, more refined notions of subgroup fairness. We aim to (a) formulate different aspects of the difficulty of individuals in certain subgroups to achieve recourse, i.e. receive the desired outcome, either at the micro level, considering members of the subgroup individually, or at the macro level, considering the subgroup as a whole, and (b) introduce notions of subgroup fairness that are robust, if not totally oblivious, to the cost of achieving recourse. We accompany these notions with an efficient, model-agnostic, highly parameterizable, and explainable framework for evaluating subgroup fairness. We demonstrate the advantages, the wide applicability, and the efficiency of our approach through a thorough experimental evaluation of different benchmark d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22797;&#26434;&#25968;&#25454;&#38598;&#20013;&#30340;&#24213;&#23618;&#32553;&#25918;&#23450;&#24459;&#21644;&#26222;&#36866;&#32479;&#35745;&#32467;&#26500;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#31867;&#27604;&#20026;&#29289;&#29702;&#31995;&#32479;&#65292;&#24182;&#24212;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#21644;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#29305;&#24449;-&#29305;&#24449;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#20540;&#32479;&#35745;&#37327;&#30340;&#35268;&#24459;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26080;&#20851;&#38543;&#26426;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#38271;&#31243;&#30456;&#20851;&#24615;&#23436;&#20840;&#24674;&#22797;&#32553;&#25918;&#34892;&#20026;&#12290;&#21516;&#26102;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37117;&#23646;&#20110;&#28151;&#27788;&#31995;&#32479;&#65292;&#24182;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#21363;&#21487;&#20307;&#29616;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#32479;&#35745;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.14975</link><description>&lt;p&gt;
&#22797;&#26434;&#25968;&#25454;&#38598;&#30340;&#24213;&#23618;&#32553;&#25918;&#23450;&#24459;&#21644;&#26222;&#36866;&#32479;&#35745;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
The Underlying Scaling Laws and Universal Statistical Structure of Complex Datasets. (arXiv:2306.14975v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22797;&#26434;&#25968;&#25454;&#38598;&#20013;&#30340;&#24213;&#23618;&#32553;&#25918;&#23450;&#24459;&#21644;&#26222;&#36866;&#32479;&#35745;&#32467;&#26500;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#31867;&#27604;&#20026;&#29289;&#29702;&#31995;&#32479;&#65292;&#24182;&#24212;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#21644;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#29305;&#24449;-&#29305;&#24449;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#20540;&#32479;&#35745;&#37327;&#30340;&#35268;&#24459;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#26080;&#20851;&#38543;&#26426;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#38271;&#31243;&#30456;&#20851;&#24615;&#23436;&#20840;&#24674;&#22797;&#32553;&#25918;&#34892;&#20026;&#12290;&#21516;&#26102;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#37117;&#23646;&#20110;&#28151;&#27788;&#31995;&#32479;&#65292;&#24182;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#21363;&#21487;&#20307;&#29616;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#32479;&#35745;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22797;&#26434;&#25968;&#25454;&#38598;&#21644;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#20013;&#37117;&#20986;&#29616;&#30340;&#26222;&#36941;&#29305;&#24449;&#12290;&#25105;&#20204;&#23558;&#25968;&#25454;&#31867;&#27604;&#20026;&#29289;&#29702;&#31995;&#32479;&#65292;&#24182;&#21033;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#21644;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#24037;&#20855;&#25581;&#31034;&#20854;&#24213;&#23618;&#32467;&#26500;&#12290;&#25105;&#20204;&#37325;&#28857;&#20998;&#26512;&#20102;&#29305;&#24449;-&#29305;&#24449;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#20998;&#26512;&#20102;&#20854;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#20540;&#32479;&#35745;&#37327;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#23519;&#32467;&#26524;&#26159;&#65306;(i) &#22823;&#37096;&#20998;&#29305;&#24449;&#20540;&#21576;&#29616;&#30340;&#24130;&#24459;&#32553;&#25918;&#22312;&#26080;&#30456;&#20851;&#38543;&#26426;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;(ii) &#36890;&#36807;&#31616;&#21333;&#22320;&#24341;&#20837;&#38271;&#31243;&#30456;&#20851;&#24615;&#65292;&#21487;&#20197;&#23436;&#20840;&#24674;&#22797;&#36825;&#31181;&#32553;&#25918;&#34892;&#20026;&#21040;&#21512;&#25104;&#25968;&#25454;&#20013;&#65292;(iii) &#20174;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#35282;&#24230;&#30475;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#23646;&#20110;&#21516;&#19968;&#20010;&#26222;&#36866;&#24615;&#31867;&#21035;&#65292;&#37117;&#26159;&#28151;&#27788;&#31995;&#32479;&#32780;&#38750;&#21487;&#31215;&#31995;&#32479;&#65292;(iv) &#39044;&#26399;&#30340;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#32479;&#35745;&#34892;&#20026;&#22312;&#30456;&#23545;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#23601;&#24050;&#32463;&#22312;&#32463;&#39564;&#21327;&#26041;&#24046;&#30697;&#38453;&#20013;&#24471;&#21040;&#20307;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study universal traits which emerge both in real-world complex datasets, as well as in artificially generated ones. Our approach is to analogize data to a physical system and employ tools from statistical physics and Random Matrix Theory (RMT) to reveal their underlying structure. We focus on the feature-feature covariance matrix, analyzing both its local and global eigenvalue statistics. Our main observations are: (i) The power-law scalings that the bulk of its eigenvalues exhibit are vastly different for uncorrelated random data compared to real-world data, (ii) this scaling behavior can be completely recovered by introducing long range correlations in a simple way to the synthetic data, (iii) both generated and real-world datasets lie in the same universality class from the RMT perspective, as chaotic rather than integrable systems, (iv) the expected RMT statistical behavior already manifests for empirical covariance matrices at dataset sizes significantly smaller than those conv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIMF&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#35821;&#20041;&#24863;&#30693;&#30340;&#20132;&#20114;&#24335;&#36816;&#21160;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23454;&#29616;&#22522;&#20110;&#35821;&#20041;&#30340;&#34892;&#20026;&#20307;&#36873;&#25321;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#20840;&#23616;&#32534;&#30721;&#65292;&#33021;&#22815;&#25429;&#25417;&#31354;&#38388;&#20449;&#24687;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#20248;&#36873;&#30456;&#20851;&#30340;&#34892;&#20026;&#20307;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.14941</link><description>&lt;p&gt;
SIMF: &#33258;&#21160;&#39550;&#39542;&#30340;&#35821;&#20041;&#24863;&#30693;&#20132;&#20114;&#24335;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SIMF: Semantics-aware Interactive Motion Forecasting for Autonomous Driving. (arXiv:2306.14941v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIMF&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#35821;&#20041;&#24863;&#30693;&#30340;&#20132;&#20114;&#24335;&#36816;&#21160;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23454;&#29616;&#22522;&#20110;&#35821;&#20041;&#30340;&#34892;&#20026;&#20307;&#36873;&#25321;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#20840;&#23616;&#32534;&#30721;&#65292;&#33021;&#22815;&#25429;&#25417;&#31354;&#38388;&#20449;&#24687;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#20248;&#36873;&#30456;&#20851;&#30340;&#34892;&#20026;&#20307;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38656;&#35201;&#23545;&#21608;&#22260;&#22810;&#20010;&#34892;&#20026;&#20307;&#65288;&#34892;&#20154;&#21644;&#36710;&#36742;&#65289;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#65292;&#20197;&#20570;&#20986;&#26368;&#20248;&#23548;&#33322;&#20915;&#31574;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#34892;&#20026;&#20307;&#30340;&#20301;&#32622;&#21644;&#36895;&#24230;&#65292;&#24182;&#26410;&#33021;&#25429;&#25417;&#21040;&#22330;&#26223;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#19982;&#22330;&#26223;&#20013;&#34892;&#20026;&#20307;&#25968;&#37327;&#22686;&#21152;&#30456;&#20851;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#19968;&#20123;&#26041;&#27861;&#21033;&#29992;&#27431;&#27663;&#36317;&#31163;&#26469;&#21098;&#26525;&#36828;&#31163;&#30340;&#34892;&#20026;&#20307;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#22522;&#20110;&#36317;&#31163;&#30340;&#24230;&#37327;&#26080;&#27861;&#36873;&#25321;&#30456;&#20851;&#30340;&#34892;&#20026;&#20307;&#24182;&#20934;&#30830;&#36827;&#34892;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SIMF&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#31354;&#38388;&#20449;&#24687;&#20197;&#21450;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#20248;&#36873;&#30456;&#20851;&#30340;&#34892;&#20026;&#20307;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#29616;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#30340;&#34892;&#20026;&#20307;&#36873;&#25321;&#26041;&#27861;&#65292;&#23558;&#20854;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#20256;&#36882;&#65292;&#20197;&#25552;&#21462;&#20840;&#23616;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicles require motion forecasting of their surrounding multi-agents (pedestrians and vehicles) to make optimal decisions for navigation. The existing methods focus on techniques to utilize the positions and velocities of these agents and fail to capture semantic information from the scene. Moreover, to mitigate the increase in computational complexity associated with the number of agents in the scene, some works leverage Euclidean distance to prune far-away agents. However, distance-based metric alone is insufficient to select relevant agents and accurately perform their predictions. To resolve these issues, we propose Semantics-aware Interactive Motion Forecasting (SIMF) method to capture semantics along with spatial information, and optimally select relevant agents for motion prediction. Specifically, we achieve this by implementing a semantic-aware selection of relevant agents from the scene and passing them through an attention mechanism to extract global encodings. Th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#20248;&#21270;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#23884;&#20837;&#30340;&#32452;&#21512;&#20250;&#30053;&#24494;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#24182;&#19988;&#32452;&#21512;&#26041;&#24335;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2306.14939</link><description>&lt;p&gt;
&#23884;&#20837;&#34701;&#21512;&#30340;&#33402;&#26415;&#65306;&#20248;&#21270;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
The Art of Embedding Fusion: Optimizing Hate Speech Detection. (arXiv:2306.14939v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14939
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#20248;&#21270;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#23884;&#20837;&#30340;&#32452;&#21512;&#20250;&#30053;&#24494;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#24182;&#19988;&#32452;&#21512;&#26041;&#24335;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#38656;&#35201;&#25429;&#25417;&#35821;&#35328;&#21644;&#35821;&#22659;&#32454;&#24494;&#24046;&#21035;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#25991;&#26412;&#35821;&#20041;&#34920;&#31034;&#65292;&#21487;&#20197;&#25913;&#36827;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26377;&#25928;&#22320;&#32452;&#21512;PLMs&#30340;&#34920;&#31034;&#21644;&#21033;&#29992;&#23427;&#20204;&#30340;&#20114;&#34917;&#20248;&#21183;&#30340;&#26041;&#27861;&#36824;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20960;&#31181;PLMs&#32452;&#21512;&#25216;&#26415;&#30340;&#26041;&#24335;&#65292;&#24182;&#20840;&#38754;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#32452;&#21512;&#23884;&#20837;&#21487;&#20197;&#30053;&#24494;&#25913;&#21892;&#24615;&#33021;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#65292;&#32452;&#21512;&#26041;&#24335;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;&#25105;&#20204;&#36824;&#22312;https://github.com/aflah02/The-Art-of-Embedding-Fusion-Optimizing-Hate-Speech-Detection&#19978;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hate speech detection is a challenging natural language processing task that requires capturing linguistic and contextual nuances. Pre-trained language models (PLMs) offer rich semantic representations of text that can improve this task. However there is still limited knowledge about ways to effectively combine representations across PLMs and leverage their complementary strengths. In this work, we shed light on various combination techniques for several PLMs and comprehensively analyze their effectiveness. Our findings show that combining embeddings leads to slight improvements but at a high computational cost and the choice of combination has marginal effect on the final outcome. We also make our codebase public at https://github.com/aflah02/The-Art-of-Embedding-Fusion-Optimizing-Hate-Speech-Detection .
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#19982;&#23376;&#35789;&#23884;&#20837;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20316;&#32773;&#24402;&#23646;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22788;&#29702;&#25991;&#26412;&#20013;&#30340;&#38544;&#21547;&#35789;&#38382;&#39064;&#30340;&#21516;&#26102;&#20445;&#30041;&#35789;&#30340;&#39034;&#24207;&#19978;&#19979;&#25991;&#12290;</title><link>http://arxiv.org/abs/2306.14933</link><description>&lt;p&gt;
&#23558;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#19982;&#23376;&#35789;&#23884;&#20837;&#30456;&#32467;&#21512;&#29992;&#20110;&#20316;&#32773;&#24402;&#23646;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Integrating Bidirectional Long Short-Term Memory with Subword Embedding for Authorship Attribution. (arXiv:2306.14933v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14933
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#19982;&#23376;&#35789;&#23884;&#20837;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20316;&#32773;&#24402;&#23646;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22788;&#29702;&#25991;&#26412;&#20013;&#30340;&#38544;&#21547;&#35789;&#38382;&#39064;&#30340;&#21516;&#26102;&#20445;&#30041;&#35789;&#30340;&#39034;&#24207;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#32473;&#23450;&#25991;&#26412;&#25991;&#26723;&#30340;&#20316;&#32773;&#36523;&#20221;&#26159;&#20316;&#32773;&#24402;&#23646;&#38382;&#39064;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#25104;&#21151;&#22320;&#20351;&#29992;&#22810;&#26679;&#30340;&#22522;&#20110;&#35789;&#30340;&#39118;&#26684;&#26631;&#35760;&#26469;&#22788;&#29702;&#20316;&#32773;&#24402;&#23646;&#30340;&#20869;&#22312;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#35789;&#30340;&#20316;&#32773;&#24402;&#23646;&#31995;&#32479;&#30340;&#24615;&#33021;&#21463;&#21040;&#35757;&#32451;&#35821;&#26009;&#24211;&#35789;&#27719;&#30340;&#38480;&#21046;&#12290;&#25991;&#29486;&#25512;&#33616;&#20102;&#20197;&#23383;&#31526;&#20026;&#22522;&#30784;&#30340;&#39118;&#26684;&#26631;&#35760;&#20316;&#20026;&#20811;&#26381;&#38544;&#21547;&#35789;&#38382;&#39064;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#23383;&#31526;&#30340;&#26041;&#27861;&#32463;&#24120;&#26080;&#27861;&#25429;&#25417;&#25991;&#26412;&#20013;&#30340;&#35789;&#30340;&#39034;&#24207;&#20851;&#31995;&#65292;&#36825;&#26159;&#36827;&#19968;&#27493;&#25913;&#21892;&#30340;&#38590;&#39064;&#12290;&#26412;&#25991;&#35752;&#35770;&#30340;&#38382;&#39064;&#26159;&#26159;&#21542;&#21487;&#20197;&#35299;&#20915;&#25991;&#26412;&#25991;&#26723;&#20013;&#38544;&#21547;&#35789;&#30340;&#27495;&#20041;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#35789;&#30340;&#39034;&#24207;&#19978;&#19979;&#25991;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;BLSTM&#65289;&#19982;&#20108;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#25429;&#25417;&#39034;&#24207;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of unveiling the author of a given text document from multiple candidate authors is called authorship attribution. Manifold word-based stylistic markers have been successfully used in deep learning methods to deal with the intrinsic problem of authorship attribution. Unfortunately, the performance of word-based authorship attribution systems is limited by the vocabulary of the training corpus. Literature has recommended character-based stylistic markers as an alternative to overcome the hidden word problem. However, character-based methods often fail to capture the sequential relationship of words in texts which is a chasm for further improvement. The question addressed in this paper is whether it is possible to address the ambiguity of hidden words in text documents while preserving the sequential context of words. Consequently, a method based on bidirectional long short-term memory (BLSTM) with a 2-dimensional convolutional neural network (CNN) is proposed to capture sequ
&lt;/p&gt;</description></item><item><title>GloptiNets&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#35777;&#26126;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#20989;&#25968;&#30340;&#27491;&#21017;&#24615;&#21644;&#24182;&#34892;&#35745;&#31639;&#30340;&#20248;&#21183;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.14932</link><description>&lt;p&gt;
GloptiNets&#65306;&#20855;&#26377;&#35777;&#26126;&#30340;&#21487;&#25193;&#23637;&#38750;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
GloptiNets: Scalable Non-Convex Optimization with Certificates. (arXiv:2306.14932v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14932
&lt;/p&gt;
&lt;p&gt;
GloptiNets&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#35777;&#26126;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#20989;&#25968;&#30340;&#27491;&#21017;&#24615;&#21644;&#24182;&#34892;&#35745;&#31639;&#30340;&#20248;&#21183;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#36229;&#31435;&#26041;&#20307;&#25110;&#29615;&#38754;&#19978;&#30340;&#20809;&#28369;&#20989;&#25968;&#30340;&#20855;&#26377;&#35777;&#26126;&#30340;&#38750;&#20984;&#20248;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20381;&#36182;&#20195;&#25968;&#24615;&#36136;&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21033;&#29992;&#20102;&#30446;&#26631;&#20989;&#25968;&#30340;&#27491;&#21017;&#24615;&#65292;&#35813;&#27491;&#21017;&#24615;&#22312;&#20854;&#20613;&#37324;&#21494;&#35889;&#30340;&#34928;&#20943;&#20013;&#20307;&#29616;&#20986;&#26469;&#12290;&#36890;&#36807;&#23450;&#20041;&#19968;&#20010;&#26131;&#22788;&#29702;&#30340;&#27169;&#22411;&#26063;&#65292;&#25105;&#20204;&#26082;&#33021;&#22815;&#33719;&#24471;&#31934;&#30830;&#30340;&#35777;&#26126;&#65292;&#21448;&#33021;&#22815;&#21033;&#29992;&#29992;&#20110;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20808;&#36827;&#21644;&#24378;&#22823;&#30340;&#35745;&#31639;&#25216;&#26415;&#12290;&#36890;&#36807;&#19982;GPU&#30340;&#24182;&#34892;&#35745;&#31639;&#65292;&#25105;&#20204;&#33258;&#28982;&#22320;&#22686;&#24378;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#20855;&#26377;&#25968;&#21315;&#20010;&#31995;&#25968;&#20294;&#32500;&#24230;&#36866;&#20013;&#30340;&#22810;&#39033;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#20248;&#20110;&#22522;&#20110;Lasserre&#23618;&#27425;&#30340;&#35777;&#26126;&#26368;&#20808;&#36827;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#31454;&#20105;&#32773;&#38590;&#20197;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to non-convex optimization with certificates, which handles smooth functions on the hypercube or on the torus. Unlike traditional methods that rely on algebraic properties, our algorithm exploits the regularity of the target function intrinsic in the decay of its Fourier spectrum. By defining a tractable family of models, we allow at the same time to obtain precise certificates and to leverage the advanced and powerful computational techniques developed to optimize neural networks. In this way the scalability of our approach is naturally enhanced by parallel computing with GPUs. Our approach, when applied to the case of polynomials of moderate dimensions but with thousands of coefficients, outperforms the state-of-the-art optimization methods with certificates, as the ones based on Lasserre's hierarchy, addressing problems intractable for the competitors.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20943;&#23569;&#28436;&#32462;&#32534;&#30721;&#25152;&#38656;&#26102;&#38388;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#30041;&#20256;&#32479;&#20869;&#23481;&#20998;&#26512;&#30340;&#28789;&#27963;&#24615;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#21644;&#32463;&#39564;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#22312;&#19981;&#21516;&#28436;&#32462;&#32534;&#30721;&#20219;&#21153;&#19978;&#65292;GPT-3.5&#22312;LLM&#36741;&#21161;&#20869;&#23481;&#20998;&#26512;&#65288;LACA&#65289;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14924</link><description>&lt;p&gt;
LLM&#36741;&#21161;&#20869;&#23481;&#20998;&#26512;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#28436;&#32462;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
LLM-Assisted Content Analysis: Using Large Language Models to Support Deductive Coding. (arXiv:2306.14924v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20943;&#23569;&#28436;&#32462;&#32534;&#30721;&#25152;&#38656;&#26102;&#38388;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#30041;&#20256;&#32479;&#20869;&#23481;&#20998;&#26512;&#30340;&#28789;&#27963;&#24615;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#21644;&#32463;&#39564;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#22312;&#19981;&#21516;&#28436;&#32462;&#32534;&#30721;&#20219;&#21153;&#19978;&#65292;GPT-3.5&#22312;LLM&#36741;&#21161;&#20869;&#23481;&#20998;&#26512;&#65288;LACA&#65289;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#32462;&#32534;&#30721;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#23450;&#24615;&#30740;&#31350;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#25991;&#26723;&#20013;&#20027;&#39064;&#30340;&#26222;&#36941;&#24615;&#12290;&#23613;&#31649;&#26377;&#29992;&#65292;&#28436;&#32462;&#32534;&#30721;&#36890;&#24120;&#26159;&#32321;&#29712;&#19988;&#32791;&#26102;&#30340;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#30740;&#31350;&#20154;&#21592;&#38405;&#35835;&#12289;&#35299;&#37322;&#24182;&#21487;&#38752;&#22320;&#23545;&#22823;&#37327;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#26159;&#19968;&#31867;&#24555;&#36895;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65292;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25512;&#29702;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#26469;&#20943;&#23569;&#28436;&#32462;&#32534;&#30721;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#30041;&#20256;&#32479;&#20869;&#23481;&#20998;&#26512;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;LLM&#36741;&#21161;&#20869;&#23481;&#20998;&#26512;&#65288;LACA&#65289;&#65292;&#24182;&#20351;&#29992;GPT-3.5&#22312;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#28436;&#32462;&#32534;&#30721;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#28145;&#20837;&#26696;&#20363;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#20010;&#32463;&#39564;&#22522;&#20934;&#27979;&#35797;&#65292;&#20351;&#29992;LACA&#22312;4&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;GPT-3.5&#22312;&#19981;&#21516;&#28436;&#32462;&#32534;&#30721;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deductive coding is a widely used qualitative research method for determining the prevalence of themes across documents. While useful, deductive coding is often burdensome and time consuming since it requires researchers to read, interpret, and reliably categorize a large body of unstructured text documents. Large language models (LLMs), like ChatGPT, are a class of quickly evolving AI tools that can perform a range of natural language processing and reasoning tasks. In this study, we explore the use of LLMs to reduce the time it takes for deductive coding while retaining the flexibility of a traditional content analysis. We outline the proposed approach, called LLM-assisted content analysis (LACA), along with an in-depth case study using GPT-3.5 for LACA on a publicly available deductive coding data set. Additionally, we conduct an empirical benchmark using LACA on 4 publicly available data sets to assess the broader question of how well GPT-3.5 performs across a range of deductive co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#31163;&#32676;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#27979;&#35797;&#29305;&#24449;&#21644;&#20869;&#37096;&#25968;&#25454;&#29305;&#24449;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#26469;&#21028;&#26029;&#31163;&#32676;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#31163;&#32676;&#25968;&#25454;&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.14920</link><description>&lt;p&gt;
&#22522;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#31163;&#32676;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Cosine Similarity-based Method for Out-of-Distribution Detection. (arXiv:2306.14920v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#31163;&#32676;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#27979;&#35797;&#29305;&#24449;&#21644;&#20869;&#37096;&#25968;&#25454;&#29305;&#24449;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#26469;&#21028;&#26029;&#31163;&#32676;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#31163;&#32676;&#25968;&#25454;&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#31163;&#32676;&#25968;&#25454;&#30340;&#33021;&#21147;&#26159;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#35201;&#32032;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27979;&#35797;&#29305;&#24449;&#21644;&#20856;&#22411;&#20869;&#37096;&#25968;&#25454;&#29305;&#24449;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#26159;&#26816;&#27979;&#31163;&#32676;&#25968;&#25454;&#30340;&#33391;&#22909;&#25351;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31867;&#20856;&#22411;&#21305;&#37197;&#65288;CTM&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;&#20313;&#24358;&#30456;&#20284;&#24230;&#35780;&#20998;&#20989;&#25968;&#30340;&#20107;&#21518;&#31163;&#32676;&#25968;&#25454;&#26816;&#27979;&#31639;&#27861;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;CTM&#20248;&#20110;&#29616;&#26377;&#30340;&#20107;&#21518;&#31163;&#32676;&#25968;&#25454;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to detect OOD data is a crucial aspect of practical machine learning applications. In this work, we show that cosine similarity between the test feature and the typical ID feature is a good indicator of OOD data. We propose Class Typical Matching (CTM), a post hoc OOD detection algorithm that uses a cosine similarity scoring function. Extensive experiments on multiple benchmarks show that CTM outperforms existing post hoc OOD detection methods.
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#22810;&#27169;&#24577;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#65292;&#32467;&#21512;&#20102;&#21270;&#23398;&#35821;&#35328;&#34920;&#31034;&#21644;&#29289;&#29702;&#21270;&#23398;&#29305;&#24449;&#65292;&#36890;&#36807;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#21644;&#20998;&#23376;&#23884;&#20837;&#21521;&#37327;&#31354;&#38388;&#30340;&#38598;&#25104;&#26469;&#23454;&#29616;&#20934;&#30830;&#39044;&#27979;&#12290;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#29983;&#29289;&#38477;&#35299;&#24615;&#21644;PFAS&#27602;&#24615;&#35780;&#20272;&#31561;&#22797;&#26434;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.14919</link><description>&lt;p&gt;
&#36229;&#36234;&#21270;&#23398;&#35821;&#35328;: &#19968;&#31181;&#22686;&#24378;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Beyond Chemical Language: A Multimodal Approach to Enhance Molecular Property Prediction. (arXiv:2306.14919v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14919
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#22810;&#27169;&#24577;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#65292;&#32467;&#21512;&#20102;&#21270;&#23398;&#35821;&#35328;&#34920;&#31034;&#21644;&#29289;&#29702;&#21270;&#23398;&#29305;&#24449;&#65292;&#36890;&#36807;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#21644;&#20998;&#23376;&#23884;&#20837;&#21521;&#37327;&#31354;&#38388;&#30340;&#38598;&#25104;&#26469;&#23454;&#29616;&#20934;&#30830;&#39044;&#27979;&#12290;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#29983;&#29289;&#38477;&#35299;&#24615;&#21644;PFAS&#27602;&#24615;&#35780;&#20272;&#31561;&#22797;&#26434;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21270;&#23398;&#35821;&#35328;&#34920;&#31034;&#19982;&#29289;&#29702;&#21270;&#23398;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;MULTIMODAL-MOLFORMER&#37319;&#29992;&#22240;&#26524;&#22810;&#38454;&#27573;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#26681;&#25454;&#20854;&#23545;&#29305;&#23450;&#30446;&#26631;&#24615;&#36136;&#30340;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#26469;&#35782;&#21035;&#29289;&#29702;&#21270;&#23398;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#22240;&#26524;&#29305;&#24449;&#19982;&#30001;MOLFORMER&#29983;&#25104;&#30340;&#20998;&#23376;&#23884;&#20837;&#21521;&#37327;&#31354;&#38388;&#30456;&#32467;&#21512;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20351;&#29992;Mordred&#25551;&#36848;&#31526;&#20316;&#20026;&#29289;&#29702;&#21270;&#23398;&#29305;&#24449;&#65292;&#24182;&#35782;&#21035;&#30446;&#26631;&#24615;&#36136;&#30340;&#39532;&#23572;&#21487;&#22827;&#27631;&#65292;&#29702;&#35770;&#19978;&#21253;&#21547;&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#20197;&#23454;&#29616;&#20934;&#30830;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#19994;&#30028;&#39046;&#20808;&#31639;&#27861;&#65288;&#21253;&#25324;&#22522;&#20110;&#21270;&#23398;&#35821;&#35328;&#30340;MOLFORMER&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#29983;&#29289;&#38477;&#35299;&#24615;&#21644;PFAS&#27602;&#24615;&#35780;&#20272;&#31561;&#22797;&#26434;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel multimodal language model approach for predicting molecular properties by combining chemical language representation with physicochemical features. Our approach, MULTIMODAL-MOLFORMER, utilizes a causal multistage feature selection method that identifies physicochemical features based on their direct causal effect on a specific target property. These causal features are then integrated with the vector space generated by molecular embeddings from MOLFORMER. In particular, we employ Mordred descriptors as physicochemical features and identify the Markov blanket of the target property, which theoretically contains the most relevant features for accurate prediction. Our results demonstrate a superior performance of our proposed approach compared to existing state-of-the-art algorithms, including the chemical language-based MOLFORMER and graph neural networks, in predicting complex tasks such as biodegradability and PFAS toxicity estimation. Moreover, we demonstrate the ef
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#32454;&#21270;&#26631;&#20934;&#24471;&#20998;&#65292;&#23454;&#29616;&#23545;&#35838;&#22530;&#35752;&#35770;&#36136;&#37327;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#65292;&#21516;&#26102;&#25351;&#20986;&#26631;&#20934;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;NLP&#26041;&#27861;&#23545;&#19981;&#21516;&#30340;&#26631;&#20934;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.14918</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36827;&#34892;&#35838;&#22530;&#35752;&#35770;&#30340;&#33258;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Utilizing Natural Language Processing for Automated Assessment of Classroom Discussion. (arXiv:2306.14918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#32454;&#21270;&#26631;&#20934;&#24471;&#20998;&#65292;&#23454;&#29616;&#23545;&#35838;&#22530;&#35752;&#35770;&#36136;&#37327;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#65292;&#21516;&#26102;&#25351;&#20986;&#26631;&#20934;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;NLP&#26041;&#27861;&#23545;&#19981;&#21516;&#30340;&#26631;&#20934;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20005;&#26684;&#32780;&#20114;&#21160;&#30340;&#35838;&#22530;&#35752;&#35770;&#23545;&#20110;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#65292;&#21516;&#26102;&#20063;&#26159;&#22823;&#22810;&#25968;&#25945;&#23398;&#24178;&#39044;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23545;&#35752;&#35770;&#36136;&#37327;&#36827;&#34892;&#35268;&#27169;&#21270;&#30340;&#27491;&#24335;&#35780;&#20272;&#23545;&#20110;&#22823;&#22810;&#25968;&#30740;&#31350;&#32773;&#26469;&#35828;&#26159;&#26114;&#36149;&#19988;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#21508;&#31181;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#65292;&#20197;&#33258;&#21160;&#29983;&#25104;&#35838;&#22530;&#25991;&#26412;&#35752;&#35770;&#36136;&#37327;&#30340;&#32454;&#21270;&#26631;&#20934;&#24471;&#20998;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21253;&#21547;&#36229;&#36807;18000&#36718;&#27425;&#12289;&#27880;&#37322;&#26377;&#35814;&#32454;&#30340;&#25945;&#23398;&#20998;&#26512;&#36816;&#21160;&#65288;ATM&#65289;&#20195;&#30721;&#30340;90&#20010;&#35838;&#22530;&#35752;&#35770;&#35760;&#24405;&#25968;&#25454;&#38598;&#65292;&#24182;&#32858;&#28966;&#20110;&#22235;&#20010;&#25945;&#23398;&#36136;&#37327;&#35780;&#20272;&#65288;IQA&#65289;&#26631;&#20934;&#12290;&#23613;&#31649;&#25968;&#25454;&#37327;&#26377;&#38480;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#19968;&#20123;&#26631;&#20934;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20063;&#26263;&#31034;&#20854;&#20182;&#26631;&#20934;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#26576;&#20123;NLP&#26041;&#27861;&#22312;&#26576;&#20123;&#26631;&#20934;&#19978;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rigorous and interactive class discussions that support students to engage in high-level thinking and reasoning are essential to learning and are a central component of most teaching interventions. However, formally assessing discussion quality 'at scale' is expensive and infeasible for most researchers. In this work, we experimented with various modern natural language processing (NLP) techniques to automatically generate rubric scores for individual dimensions of classroom text discussion quality. Specifically, we worked on a dataset of 90 classroom discussion transcripts consisting of over 18000 turns annotated with fine-grained Analyzing Teaching Moves (ATM) codes and focused on four Instructional Quality Assessment (IQA) rubrics. Despite the limited amount of data, our work shows encouraging results in some of the rubrics while suggesting that there is room for improvement in the others. We also found that certain NLP approaches work better for certain rubrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24341;&#23548;&#23646;&#24615;&#65288;&#38382;&#39064;&#26126;&#30830;&#24615;&#65289;&#26469;&#20016;&#23500;&#25945;&#32946;&#38382;&#39064;&#29983;&#25104;&#30340;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#25511;&#21046;&#29983;&#25104;&#26126;&#30830;&#21644;&#38544;&#21547;wh-&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#36890;&#36807;&#38382;&#39064;&#26126;&#30830;&#24615;&#21644;&#21465;&#20107;&#35201;&#32032;&#21516;&#26102;&#25511;&#21046;&#38382;&#39064;&#29983;&#25104;&#30340;&#21021;&#27493;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.14917</link><description>&lt;p&gt;
&#36808;&#21521;&#25945;&#32946;&#38382;&#39064;&#29983;&#25104;&#30340;&#20016;&#23500;&#21487;&#25511;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Enriched Controllability for Educational Question Generation. (arXiv:2306.14917v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24341;&#23548;&#23646;&#24615;&#65288;&#38382;&#39064;&#26126;&#30830;&#24615;&#65289;&#26469;&#20016;&#23500;&#25945;&#32946;&#38382;&#39064;&#29983;&#25104;&#30340;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#25511;&#21046;&#29983;&#25104;&#26126;&#30830;&#21644;&#38544;&#21547;wh-&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#36890;&#36807;&#38382;&#39064;&#26126;&#30830;&#24615;&#21644;&#21465;&#20107;&#35201;&#32032;&#21516;&#26102;&#25511;&#21046;&#38382;&#39064;&#29983;&#25104;&#30340;&#21021;&#27493;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#38382;&#39064;&#65288;QG&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#19968;&#20010;&#20219;&#21153;&#65292;&#23427;&#28041;&#21450;&#26681;&#25454;&#36755;&#20837;&#65288;&#36890;&#24120;&#30001;&#25991;&#26412;&#21644;&#30446;&#26631;&#31572;&#26696;&#32452;&#25104;&#65289;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#12290;&#36817;&#26399;&#20851;&#20110;QG&#30340;&#30740;&#31350;&#26088;&#22312;&#25511;&#21046;&#29983;&#25104;&#38382;&#39064;&#30340;&#31867;&#22411;&#65292;&#20197;&#28385;&#36275;&#25945;&#32946;&#38656;&#27714;&#12290;&#25945;&#32946;QG&#20013;&#21487;&#25511;&#24615;&#30340;&#19968;&#20010;&#26174;&#33879;&#20363;&#23376;&#26159;&#29983;&#25104;&#28041;&#21450;&#29305;&#23450;&#21465;&#20107;&#35201;&#32032;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#22240;&#26524;&#20851;&#31995;&#12289;&#32467;&#26524;&#35299;&#20915;&#25110;&#39044;&#27979;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24341;&#23548;&#23646;&#24615;&#65288;&#38382;&#39064;&#26126;&#30830;&#24615;&#65289;&#26469;&#20016;&#23500;QG&#30340;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#25552;&#35758;&#36890;&#36807;&#25511;&#21046;&#20174;&#36866;&#21512;&#20799;&#31461;&#30340;&#25925;&#20107;&#20013;&#29983;&#25104;&#26126;&#30830;&#21644;&#38544;&#21547;&#30340;wh-&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#36890;&#36807;&#38382;&#39064;&#26126;&#30830;&#24615;&#20197;&#21450;&#19982;&#21478;&#19968;&#20010;&#30446;&#26631;&#23646;&#24615;&#65288;&#38382;&#39064;&#30340;&#21465;&#20107;&#35201;&#32032;&#65289;&#21516;&#26102;&#25511;&#21046;QG&#30340;&#21021;&#27493;&#35777;&#25454;&#12290;&#20195;&#30721;&#20844;&#24320;&#21487;&#22312;github.com/bernardoleite/question-generation-control&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question Generation (QG) is a task within Natural Language Processing (NLP) that involves automatically generating questions given an input, typically composed of a text and a target answer. Recent work on QG aims to control the type of generated questions so that they meet educational needs. A remarkable example of controllability in educational QG is the generation of questions underlying certain narrative elements, e.g., causal relationship, outcome resolution, or prediction. This study aims to enrich controllability in QG by introducing a new guidance attribute: question explicitness. We propose to control the generation of explicit and implicit wh-questions from children-friendly stories. We show preliminary evidence of controlling QG via question explicitness alone and simultaneously with another target attribute: the question's narrative element. The code is publicly available at github.com/bernardoleite/question-generation-control.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20998;&#23376;&#21147;&#22330;&#20013;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#35201;&#27714;&#21644;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20808;&#21069;&#30340;&#30740;&#31350;&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#25152;&#26377;&#26631;&#20934;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#31070;&#32463;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65288;LNK&#65289;&#12290;</title><link>http://arxiv.org/abs/2306.14916</link><description>&lt;p&gt;
&#20998;&#23376;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;: &#26399;&#26395;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Estimation for Molecules: Desiderata and Methods. (arXiv:2306.14916v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14916
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20998;&#23376;&#21147;&#22330;&#20013;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#35201;&#27714;&#21644;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20808;&#21069;&#30340;&#30740;&#31350;&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#25152;&#26377;&#26631;&#20934;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#31070;&#32463;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65288;LNK&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20316;&#20026;&#37327;&#23376;&#21147;&#23398;&#35745;&#31639;&#30340;&#26377;&#24076;&#26395;&#26367;&#20195;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#36712;&#36857;&#38598;&#21512;&#19978;&#24314;&#31435;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20302;&#35823;&#24046;&#12290;&#30001;&#20110;&#23427;&#20204;&#24555;&#36895;&#30340;&#25512;&#29702;&#26102;&#38388;&#65292;&#23427;&#20204;&#25215;&#35834;&#21152;&#36895;&#35745;&#31639;&#21270;&#23398;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#20998;&#24067;&#65288;ID&#65289;&#35823;&#24046;&#19978;&#21487;&#20302;&#65292;&#20294;&#36825;&#20123;GNNs&#22312;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#26679;&#26412;&#19978;&#21487;&#33021;&#23436;&#20840;&#38169;&#35823;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65288;UE&#65289;&#21487;&#20197;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#24110;&#21161;&#36890;&#36807;&#20256;&#36798;&#27169;&#22411;&#23545;&#20854;&#39044;&#27979;&#30340;&#30830;&#23450;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30528;&#30524;&#20110;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#30830;&#23450;&#20102;&#20998;&#23376;&#21147;&#22330;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20013;&#30340;&#20845;&#20010;&#20851;&#38190;&#35201;&#27714;&#65292;&#19977;&#20010;&#26159;&#8220;&#29289;&#29702;&#20449;&#24687;&#8221;&#26041;&#38754;&#30340;&#65292;&#19977;&#20010;&#26159;&#8220;&#24212;&#29992;&#28966;&#28857;&#8221;&#26041;&#38754;&#30340;&#12290;&#20026;&#20102;&#27010;&#36848;&#35813;&#39046;&#22495;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29616;&#26377;&#30340;UE&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#23427;&#20204;&#22914;&#20309;&#31526;&#21512;&#36825;&#20123;&#35201;&#27714;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#37117;&#26080;&#27861;&#28385;&#36275;&#25152;&#26377;&#26631;&#20934;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23616;&#37096;&#31070;&#32463;&#26680;&#65288;LNK&#65289;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are promising surrogates for quantum mechanical calculations as they establish unprecedented low errors on collections of molecular dynamics (MD) trajectories. Thanks to their fast inference times they promise to accelerate computational chemistry applications. Unfortunately, despite low in-distribution (ID) errors, such GNNs might be horribly wrong for out-of-distribution (OOD) samples. Uncertainty estimation (UE) may aid in such situations by communicating the model's certainty about its prediction. Here, we take a closer look at the problem and identify six key desiderata for UE in molecular force fields, three 'physics-informed' and three 'application-focused' ones. To overview the field, we survey existing methods from the field of UE and analyze how they fit to the set desiderata. By our analysis, we conclude that none of the previous works satisfies all criteria. To fill this gap, we propose Localized Neural Kernel (LNK) a Gaussian Process (GP)-based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35770;&#36848;&#20102;&#22312;LLMs&#26102;&#20195;&#65292;&#20154;&#26631;&#35760;&#25968;&#25454;&#20173;&#28982;&#20855;&#26377;&#37325;&#35201;&#24615;&#30340;&#35770;&#25454;&#21644;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.14910</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#20013;&#20154;&#26631;&#35760;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Importance of Human-Labeled Data in the Era of LLMs. (arXiv:2306.14910v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35770;&#36848;&#20102;&#22312;LLMs&#26102;&#20195;&#65292;&#20154;&#26631;&#35760;&#25968;&#25454;&#20173;&#28982;&#20855;&#26377;&#37325;&#35201;&#24615;&#30340;&#35770;&#25454;&#21644;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22312;&#23450;&#21046;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24320;&#21457;&#26041;&#38754;&#24102;&#26469;&#20102;&#19968;&#22330;&#38761;&#21629;&#65292;&#24182;&#24341;&#21457;&#20102;&#20851;&#20110;&#37325;&#26032;&#23450;&#20041;&#25968;&#25454;&#35201;&#27714;&#30340;&#35752;&#35770;&#12290;LLMs&#30340;&#35757;&#32451;&#21644;&#23454;&#26045;&#25152;&#24102;&#26469;&#30340;&#33258;&#21160;&#21270;&#24341;&#21457;&#20102;&#23545;&#20154;&#24037;&#26631;&#35760;&#24178;&#39044;&#21487;&#33021;&#19981;&#20877;&#20855;&#26377;&#19982;&#30417;&#30563;&#23398;&#20064;&#26102;&#20195;&#30456;&#21516;&#37325;&#35201;&#24615;&#30340;&#35752;&#35770;&#21644;&#26399;&#26395;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26377;&#21147;&#30340;&#35770;&#25454;&#65292;&#25903;&#25345;&#22312;LLMs&#26102;&#20195;&#20154;&#26631;&#35760;&#25968;&#25454;&#30340;&#25345;&#32493;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models (LLMs) has brought about a revolution in the development of tailored machine learning models and sparked debates on redefining data requirements. The automation facilitated by the training and implementation of LLMs has led to discussions and aspirations that human-level labeling interventions may no longer hold the same level of importance as in the era of supervised learning. This paper presents compelling arguments supporting the ongoing relevance of human-labeled data in the era of LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#26469;&#35774;&#35745;&#20855;&#26377;&#25152;&#38656;&#21270;&#23398;&#21644;&#29983;&#29289;&#24615;&#36136;&#30340;&#20998;&#23376;&#65292;&#24182;&#36890;&#36807;&#36880;&#27493;&#20998;&#24067;&#21464;&#25442;&#37319;&#26679;&#31639;&#27861;&#25628;&#32034;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20998;&#23376;&#35774;&#35745;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.14902</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#33021;&#37327;&#24314;&#27169;&#21644;&#20998;&#24067;&#36880;&#27493;&#21464;&#25442;&#36827;&#34892;&#20998;&#23376;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Molecule Design by Latent Space Energy-Based Modeling and Gradual Distribution Shifting. (arXiv:2306.14902v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#26469;&#35774;&#35745;&#20855;&#26377;&#25152;&#38656;&#21270;&#23398;&#21644;&#29983;&#29289;&#24615;&#36136;&#30340;&#20998;&#23376;&#65292;&#24182;&#36890;&#36807;&#36880;&#27493;&#20998;&#24067;&#21464;&#25442;&#37319;&#26679;&#31639;&#27861;&#25628;&#32034;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20998;&#23376;&#35774;&#35745;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#33647;&#29289;&#30740;&#21457;&#65292;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#21270;&#23398;&#21644;&#29983;&#29289;&#24615;&#36136;&#65288;&#22914;&#39640;&#33647;&#29289;&#26679;&#24615;&#65292;&#39640;&#20146;&#21644;&#21147;&#65289;&#30340;&#20998;&#23376;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#25429;&#25417;&#20998;&#23376;&#21644;&#20854;&#24615;&#36136;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#22522;&#20110;&#33021;&#37327;&#24314;&#27169;&#65288;EBM&#65289;&#12290;&#22312;&#28508;&#22312;&#21521;&#37327;&#30340;&#26465;&#20214;&#19979;&#65292;&#20998;&#23376;&#21450;&#20854;&#24615;&#36136;&#34987;&#20998;&#21035;&#24314;&#27169;&#20026;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#21644;&#24615;&#36136;&#22238;&#24402;&#27169;&#22411;&#12290;&#20026;&#20102;&#25628;&#32034;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36880;&#27493;&#20998;&#24067;&#21464;&#25442;&#37319;&#26679;&#65288;SGDS&#65289;&#31639;&#27861;&#65292;&#20351;&#24471;&#22312;&#26368;&#21021;&#23398;&#20064;&#29616;&#26377;&#20998;&#23376;&#21450;&#20854;&#24615;&#36136;&#30340;&#35757;&#32451;&#25968;&#25454;&#21518;&#65292;&#35813;&#31639;&#27861;&#36880;&#28176;&#23558;&#27169;&#22411;&#20998;&#24067;&#36716;&#31227;&#21040;&#25903;&#25345;&#25152;&#38656;&#24615;&#36136;&#20540;&#30340;&#20998;&#23376;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#20998;&#23376;&#35774;&#35745;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generation of molecules with desired chemical and biological properties such as high drug-likeness, high binding affinity to target proteins, is critical for drug discovery. In this paper, we propose a probabilistic generative model to capture the joint distribution of molecules and their properties. Our model assumes an energy-based model (EBM) in the latent space. Conditional on the latent vector, the molecule and its properties are modeled by a molecule generation model and a property regression model respectively. To search for molecules with desired properties, we propose a sampling with gradual distribution shifting (SGDS) algorithm, so that after learning the model initially on the training data of existing molecules and their properties, the proposed algorithm gradually shifts the model distribution towards the region supported by molecules with desired values of properties. Our experiments show that our method achieves very strong performances on various molecule design tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22768;&#23376;&#27874;&#21253;&#27169;&#25311;&#30740;&#31350;&#21457;&#29616;&#65292;&#38750;&#26230;&#24615;&#20013;&#38388;&#23618;&#26174;&#33879;&#38459;&#30861;&#22768;&#23376;&#22312;GaN/AlN&#30028;&#38754;&#19978;&#30340;&#20256;&#36755;&#65292;&#23548;&#33268;&#22768;&#23376;&#27169;&#24335;&#36716;&#25442;&#21644;&#39640;&#39057;&#22768;&#23376;&#37096;&#20998;&#31359;&#36807;&#38750;&#26230;&#24615;&#20013;&#38388;&#23618;&#65292;&#36825;&#20026;&#30028;&#38754;&#28909;&#20256;&#23548;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#28909;&#20256;&#36755;&#36890;&#36947;&#12290;</title><link>http://arxiv.org/abs/2306.14901</link><description>&lt;p&gt;
&#21463;&#38750;&#26230;&#24615;&#20013;&#38388;&#23618;&#24433;&#21709;&#30340;&#24322;&#36136;&#30028;&#38754;&#22768;&#23376;&#21160;&#24577;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Phonon dynamic behaviors induced by amorphous interlayer at heterointerfaces. (arXiv:2306.14901v1 [physics.app-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22768;&#23376;&#27874;&#21253;&#27169;&#25311;&#30740;&#31350;&#21457;&#29616;&#65292;&#38750;&#26230;&#24615;&#20013;&#38388;&#23618;&#26174;&#33879;&#38459;&#30861;&#22768;&#23376;&#22312;GaN/AlN&#30028;&#38754;&#19978;&#30340;&#20256;&#36755;&#65292;&#23548;&#33268;&#22768;&#23376;&#27169;&#24335;&#36716;&#25442;&#21644;&#39640;&#39057;&#22768;&#23376;&#37096;&#20998;&#31359;&#36807;&#38750;&#26230;&#24615;&#20013;&#38388;&#23618;&#65292;&#36825;&#20026;&#30028;&#38754;&#28909;&#20256;&#23548;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#28909;&#20256;&#36755;&#36890;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30028;&#38754;&#38459;&#30861;&#24322;&#36136;&#32467;&#26500;&#20013;&#30340;&#28909;&#27969;&#65292;&#24182;&#19988;&#30028;&#38754;&#28909;&#38459;&#24050;&#25104;&#20026;&#30005;&#23376;&#22120;&#20214;&#28909;&#32791;&#25955;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#20102;&#25506;&#32034;&#30028;&#38754;&#28909;&#38459;&#30340;&#26426;&#21046;&#65292;&#26412;&#30740;&#31350;&#20351;&#29992;&#22768;&#23376;&#27874;&#21253;&#27169;&#25311;&#30740;&#31350;&#20102;&#20855;&#26377;&#38750;&#26230;&#24615;&#20013;&#38388;&#23618;&#30340;GaN/AlN&#30028;&#38754;&#19978;&#36890;&#36807;&#30340;&#22768;&#23376;&#30340;&#21160;&#24577;&#34892;&#20026;&#12290;&#21457;&#29616;&#38750;&#26230;&#24615;&#20013;&#38388;&#23618;&#26174;&#33879;&#38459;&#30861;&#22768;&#23376;&#22312;&#30028;&#38754;&#19978;&#30340;&#20256;&#36755;&#65292;&#24182;&#23548;&#33268;&#26174;&#33879;&#30340;&#22768;&#23376;&#27169;&#24335;&#36716;&#25442;&#65292;&#22914;LA&#8594;TA&#12289;TA&#8594;LA&#21644;LA&#8594;TO&#36716;&#25442;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#24335;&#36716;&#25442;&#21644;&#38750;&#24377;&#24615;&#25955;&#23556;&#65292;&#25105;&#20204;&#21457;&#29616;&#37096;&#20998;&#39640;&#39057;TA&#22768;&#23376;&#65292;&#39640;&#20110;&#25130;&#27490;&#39057;&#29575;&#19988;&#26080;&#27861;&#31359;&#36807;&#29702;&#24819;&#23574;&#38160;&#30028;&#38754;&#65292;&#21487;&#20197;&#37096;&#20998;&#31359;&#36807;&#38750;&#26230;&#24615;&#20013;&#38388;&#23618;&#65292;&#36825;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#28909;&#20256;&#36755;&#36890;&#36947;&#24182;&#23545;&#30028;&#38754;&#28909;&#20256;&#23548;&#36215;&#21040;&#31215;&#26497;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Interface impedes heat flow in heterostructures and the interfacial thermal resistance (ITR) has become a critical issue for thermal dissipation in electronic devices. To explore the mechanism leading to the ITR, in this work, the dynamic behaviors of phonons passing through the GaN/AlN interface with an amorphous interlayer is investigated by using phonon wave packet simulation. It is found the amorphous interlayer significantly impedes phonon transport across the interface, and leads to remarkable phonon mode conversions, such as LA$\rightarrow$TA, TA$\rightarrow$LA, and LA$\rightarrow$TO conversion. However, due to mode conversion and inelastic scattering, we found a portion of high-frequency TA phonons, which are higher than the cut-off frequency and cannot transmit across the ideal sharp interface, can partially transmit across the amorphous interlayer, which introduces additional thermal transport channels through the interface and has positive effect on interfacial thermal condu
&lt;/p&gt;</description></item><item><title>InterCode&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#32534;&#30721;&#30340;&#26631;&#20934;&#21270;&#21644;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#25191;&#34892;&#21453;&#39304;&#20316;&#20026;&#35266;&#23519;&#65292;&#24182;&#25552;&#20379;&#20102;&#23433;&#20840;&#21487;&#37325;&#29616;&#30340;&#25191;&#34892;&#29615;&#22659;&#65292;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#26032;&#30340;&#20132;&#20114;&#24335;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.14898</link><description>&lt;p&gt;
InterCode:&#26631;&#20934;&#21270;&#21644;&#22522;&#20934;&#27979;&#35797;&#20855;&#26377;&#25191;&#34892;&#21453;&#39304;&#30340;&#20132;&#20114;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback. (arXiv:2306.14898v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14898
&lt;/p&gt;
&lt;p&gt;
InterCode&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#32534;&#30721;&#30340;&#26631;&#20934;&#21270;&#21644;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#25191;&#34892;&#21453;&#39304;&#20316;&#20026;&#35266;&#23519;&#65292;&#24182;&#25552;&#20379;&#20102;&#23433;&#20840;&#21487;&#37325;&#29616;&#30340;&#25191;&#34892;&#29615;&#22659;&#65292;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#26032;&#30340;&#20132;&#20114;&#24335;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20197;&#22522;&#26412;&#20132;&#20114;&#26041;&#24335;&#32534;&#20889;&#20195;&#30721;&#65292;&#24182;&#20381;&#36182;&#20110;&#25345;&#32493;&#30340;&#25191;&#34892;&#21453;&#39304;&#26469;&#32416;&#27491;&#38169;&#35823;&#65292;&#35299;&#20915;&#27495;&#20041;&#21644;&#20998;&#35299;&#20219;&#21153;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;LLM&#23637;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32534;&#30721;&#33021;&#21147;&#65292;&#20294;&#30446;&#21069;&#30340;&#32534;&#30721;&#22522;&#20934;&#20027;&#35201;&#32771;&#34385;&#38745;&#24577;&#30340;&#25351;&#20196;&#21040;&#20195;&#30721;&#24207;&#21015;&#36716;&#25442;&#36807;&#31243;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#20256;&#25773;&#21644;&#29983;&#25104;&#30340;&#20195;&#30721;&#19982;&#20854;&#26368;&#32456;&#25191;&#34892;&#29615;&#22659;&#20043;&#38388;&#30340;&#33073;&#33410;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;InterCode&#65292;&#36825;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#12289;&#28789;&#27963;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#20132;&#20114;&#24335;&#32534;&#30721;&#26694;&#26550;&#65292;&#20316;&#20026;&#19968;&#20010;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#29615;&#22659;&#65292;&#20351;&#29992;&#20195;&#30721;&#20316;&#20026;&#34892;&#21160;&#65292;&#25191;&#34892;&#21453;&#39304;&#20316;&#20026;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#35821;&#35328;&#21644;&#24179;&#21488;&#26080;&#20851;&#65292;&#20351;&#29992;&#29420;&#31435;&#30340;Docker&#29615;&#22659;&#25552;&#20379;&#23433;&#20840;&#21644;&#21487;&#37325;&#29616;&#30340;&#25191;&#34892;&#65292;&#24182;&#19988;&#19982;&#20256;&#32479;&#30340;seq2seq&#32534;&#30721;&#26041;&#27861;&#24320;&#31665;&#21363;&#29992;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#24320;&#21457;&#26032;&#30340;&#20132;&#20114;&#24335;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;InterCode&#21019;&#24314;...
&lt;/p&gt;
&lt;p&gt;
Humans write code in a fundamentally interactive manner and rely on constant execution feedback to correct errors, resolve ambiguities, and decompose tasks. While LLMs have recently exhibited promising coding capabilities, current coding benchmarks mostly consider a static instruction-to-code sequence transduction process, which has the potential for error propagation and a disconnect between the generated code and its final execution environment. To address this gap, we introduce InterCode, a lightweight, flexible, and easy-to-use framework of interactive coding as a standard reinforcement learning (RL) environment, with code as actions and execution feedback as observations. Our framework is language and platform agnostic, uses self-contained Docker environments to provide safe and reproducible execution, and is compatible out-of-the-box with traditional seq2seq coding methods, while enabling the development of new methods for interactive code generation. We use InterCode to create t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;PMaF&#26694;&#26550;&#65292;&#20351;&#29992;&#22768;&#26126;&#24615;&#30340;&#28145;&#24230;&#23618;&#26469;&#23398;&#20064;&#20027;&#35201;&#30697;&#38453;&#29305;&#24449;&#65292;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#35299;&#20915;&#38382;&#39064;&#24182;&#24212;&#29992;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#65292;&#20174;&#32780;&#25552;&#39640;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.14759</link><description>&lt;p&gt;
PMaF:&#29992;&#20110;&#20027;&#35201;&#30697;&#38453;&#29305;&#24449;&#30340;&#28145;&#24230;&#22768;&#26126;&#24615;&#23618;
&lt;/p&gt;
&lt;p&gt;
PMaF: Deep Declarative Layers for Principal Matrix Features. (arXiv:2306.14759v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PMaF&#26694;&#26550;&#65292;&#20351;&#29992;&#22768;&#26126;&#24615;&#30340;&#28145;&#24230;&#23618;&#26469;&#23398;&#20064;&#20027;&#35201;&#30697;&#38453;&#29305;&#24449;&#65292;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#35299;&#20915;&#38382;&#39064;&#24182;&#24212;&#29992;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#65292;&#20174;&#32780;&#25552;&#39640;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20004;&#31181;&#19981;&#21516;iable&#30340;&#28145;&#24230;&#22768;&#26126;&#24615;&#23618;&#65292;&#21363;&#29699;&#19978;&#30340;&#26368;&#23567;&#20108;&#20056;&#27861;(LESS)&#21644;&#38544;&#24335;&#29305;&#24449;&#20540;&#20998;&#35299;(IED)&#65292;&#29992;&#20110;&#23398;&#20064;&#20027;&#35201;&#30697;&#38453;&#29305;&#24449;(PMaF)&#12290;&#36825;&#21487;&#20197;&#29992;&#19968;&#20010;&#20302;&#32500;&#21521;&#37327;&#34920;&#31034;&#21253;&#21547;&#26469;&#33258;&#39640;&#32500;&#30697;&#38453;&#30340;&#20027;&#35201;&#20449;&#24687;&#30340;&#25968;&#25454;&#29305;&#24449;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#21069;&#21521;&#20256;&#36882;&#30340;&#36845;&#20195;&#20248;&#21270;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#28982;&#21518;&#22312;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#19979;&#21453;&#21521;&#20256;&#25773;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#38544;&#24335;&#26799;&#24230;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#36866;&#24212;&#19979;&#38477;&#27493;&#39588;&#21644;&#21453;&#21521;&#32447;&#25628;&#32034;&#26041;&#27861;&#20197;&#21450;&#20999;&#32447;&#31354;&#38388;&#20013;&#30340;&#19979;&#38477;&#34928;&#20943;&#65292;&#20197;&#25552;&#39640;LESS&#30340;&#21069;&#21521;&#36890;&#36807;&#31243;&#30340;&#25928;&#29575;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22312;LESS&#21644;IED&#30340;&#21453;&#21521;&#20256;&#36882;&#20013;&#20351;&#29992;&#20102;&#20248;&#21270;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#22312;&#32463;&#39564;&#19978;&#65292;&#36890;&#36807;&#27604;&#36739;&#35299;&#20915;&#26041;&#26696;&#30340;&#26368;&#20248;&#24615;&#21644;&#35745;&#31639;&#35201;&#27714;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23618;&#20248;&#20110;&#29616;&#25104;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore two differentiable deep declarative layers, namely least squares on sphere (LESS) and implicit eigen decomposition (IED), for learning the principal matrix features (PMaF). This can be used to represent data features with a low-dimension vector containing dominant information from a high-dimension matrix. We first solve the problems with iterative optimization in the forward pass and then backpropagate the solution for implicit gradients under a bi-level optimization framework. Particularly, adaptive descent steps with the backtracking line search method and descent decay in the tangent space are studied to improve the forward pass efficiency of LESS. Meanwhile, exploited data structures are used to greatly reduce the computational complexity in the backward pass of LESS and IED. Empirically, we demonstrate the superiority of our layers over the off-the-shelf baselines by comparing the solution optimality and computational requirements.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#22312;&#36866;&#24212;&#24615;&#32422;&#26463;&#19979;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;Eluder Condition&#31867;&#65292;&#24182;&#38024;&#23545;&#31232;&#32570;&#31574;&#30053;&#20999;&#25442;&#21644;&#25209;&#27425;&#23398;&#20064;&#32422;&#26463;&#20998;&#21035;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;&#27492;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#32771;&#34385;&#36890;&#29992;&#20989;&#25968;&#31867;&#21035;&#19979;&#31232;&#32570;&#31574;&#30053;&#20999;&#25442;&#21644;&#25209;&#27425;&#23398;&#20064;&#30340;&#24037;&#20316;&#65292;&#28085;&#30422;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#30340;&#22823;&#37096;&#20998;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.14468</link><description>&lt;p&gt;
&#36890;&#29992;&#26694;&#26550;&#19979;&#36866;&#24212;&#24615;&#32422;&#26463;&#19979;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A General Framework for Sequential Decision-Making under Adaptivity Constraints. (arXiv:2306.14468v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#22312;&#36866;&#24212;&#24615;&#32422;&#26463;&#19979;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;Eluder Condition&#31867;&#65292;&#24182;&#38024;&#23545;&#31232;&#32570;&#31574;&#30053;&#20999;&#25442;&#21644;&#25209;&#27425;&#23398;&#20064;&#32422;&#26463;&#20998;&#21035;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;&#27492;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#32771;&#34385;&#36890;&#29992;&#20989;&#25968;&#31867;&#21035;&#19979;&#31232;&#32570;&#31574;&#30053;&#20999;&#25442;&#21644;&#25209;&#27425;&#23398;&#20064;&#30340;&#24037;&#20316;&#65292;&#28085;&#30422;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#30340;&#22823;&#37096;&#20998;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#30740;&#31350;&#36890;&#29992;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#19979;&#23545;&#20004;&#20010;&#36866;&#24212;&#24615;&#32422;&#26463;&#36827;&#34892;&#20102;&#39318;&#27425;&#25506;&#32034;&#65306;&#31574;&#30053;&#20999;&#25442;&#31232;&#32570;&#21644;&#25209;&#27425;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31216;&#20026;Eluder Condition&#31867;&#30340;&#36890;&#29992;&#31867;&#21035;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#24191;&#27867;&#30340;&#24378;&#21270;&#23398;&#20064;&#31867;&#21035;&#12290;&#28982;&#21518;&#65292;&#23545;&#20110;&#31574;&#30053;&#20999;&#25442;&#31232;&#32570;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#31639;&#27861;&#65292;&#22312;EC&#31867;&#21035;&#19978;&#23454;&#29616;&#20102;&#22823;&#32422;$ \widetilde{\mathcal{O}}(\log K)$&#30340;&#20999;&#25442;&#20195;&#20215;&#21644;$\widetilde{\mathcal{O}}(\sqrt{K})$&#30340;&#21518;&#24724;&#20195;&#20215;&#12290;&#23545;&#20110;&#25209;&#27425;&#23398;&#20064;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#22312;$B$&#20010;&#25209;&#27425;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#22823;&#32422;$\widetilde{\mathcal{O}}(\sqrt{K}+K/B)$&#30340;&#21518;&#24724;&#20195;&#20215;&#12290;&#36825;&#31687;&#35770;&#25991;&#26159;&#31532;&#19968;&#31687;&#32771;&#34385;&#36890;&#29992;&#20989;&#25968;&#31867;&#21035;&#19979;&#31232;&#32570;&#31574;&#30053;&#20999;&#25442;&#21644;&#25209;&#27425;&#23398;&#20064;&#30340;&#24037;&#20316;&#65292;&#28085;&#30422;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#20960;&#20046;&#25152;&#26377;&#30340;&#27169;&#22411;&#65292;&#22914;&#34920;&#26684;MDP (Bai et al. 2019; Zhang et al. 2020)&#12289;&#32447;&#24615;MDP (Wang et al. 2021; Gao et al. 2021)&#12289;&#20302;Eluder&#32500;&#24230;MDP (Kong et al. 2021; Gao et al. 2021)&#12289;&#24191;&#20041;&#32447;&#24615;&#20989;&#25968;&#31867;&#21035;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
We take the first step in studying general sequential decision-making under two adaptivity constraints: rare policy switch and batch learning. First, we provide a general class called the Eluder Condition class, which includes a wide range of reinforcement learning classes. Then, for the rare policy switch constraint, we provide a generic algorithm to achieve a $\widetilde{\mathcal{O}}(\log K) $ switching cost with a $\widetilde{\mathcal{O}}(\sqrt{K})$ regret on the EC class. For the batch learning constraint, we provide an algorithm that provides a $\widetilde{\mathcal{O}}(\sqrt{K}+K/B)$ regret with the number of batches $B.$ This paper is the first work considering rare policy switch and batch learning under general function classes, which covers nearly all the models studied in the previous works such as tabular MDP (Bai et al. 2019; Zhang et al. 2020), linear MDP (Wang et al. 2021; Gao et al. 2021), low eluder dimension MDP (Kong et al. 2021; Gao et al. 2021), generalized linear fu
&lt;/p&gt;</description></item><item><title>DragDiffusion&#26159;&#19968;&#20010;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24335;&#28857;&#22522;&#22270;&#20687;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25193;&#25955;&#28508;&#22312;&#23454;&#29616;&#31934;&#30830;&#30340;&#31354;&#38388;&#25511;&#21046;&#65292;&#20197;&#25552;&#39640;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14435</link><description>&lt;p&gt;
DragDiffusion: &#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24335;&#28857;&#22522;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing. (arXiv:2306.14435v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14435
&lt;/p&gt;
&lt;p&gt;
DragDiffusion&#26159;&#19968;&#20010;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24335;&#28857;&#22522;&#22270;&#20687;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#25193;&#25955;&#28508;&#22312;&#23454;&#29616;&#31934;&#30830;&#30340;&#31354;&#38388;&#25511;&#21046;&#65292;&#20197;&#25552;&#39640;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#21487;&#25511;&#30340;&#22270;&#20687;&#32534;&#36753;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;DragGAN&#23454;&#29616;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#22522;&#20110;&#28857;&#30340;&#22270;&#20687;&#32534;&#36753;&#26694;&#26550;&#65292;&#24182;&#20197;&#20687;&#32032;&#32423;&#31934;&#24230;&#23454;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32534;&#36753;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35813;&#26041;&#27861;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#20854;&#36890;&#29992;&#24615;&#21463;&#38480;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;GAN&#27169;&#22411;&#30340;&#23481;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#32534;&#36753;&#26694;&#26550;&#25193;&#23637;&#21040;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;DragDiffusion&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#20132;&#20114;&#24335;&#22522;&#20110;&#28857;&#30340;&#32534;&#36753;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#22522;&#20110;&#25991;&#26412;&#23884;&#20837;&#65292;DragDiffusion&#20248;&#21270;&#25193;&#25955;&#28508;&#22312;&#26469;&#23454;&#29616;&#31934;&#30830;&#30340;&#31354;&#38388;&#25511;&#21046;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#20197;&#36845;&#20195;&#26041;&#24335;&#29983;&#25104;&#22270;&#20687;&#65292;&#20294;&#25105;&#20204;&#20973;&#32463;&#39564;&#34920;&#26126;&#65292;&#22312;&#19968;&#20010;&#21333;&#29420;&#30340;&#27493;&#39588;&#20013;&#20248;&#21270;&#25193;&#25955;&#28508;&#22312;&#24050;&#36275;&#20197;&#29983;&#25104;&#36830;&#36143;&#30340;&#32467;&#26524;&#65292;&#20174;&#32780;&#20351;&#24471;&#35813;&#26041;&#27861;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise and controllable image editing is a challenging task that has attracted significant attention. Recently, DragGAN enables an interactive point-based image editing framework and achieves impressive editing results with pixel-level precision. However, since this method is based on generative adversarial networks (GAN), its generality is upper-bounded by the capacity of the pre-trained GAN models. In this work, we extend such an editing framework to diffusion models and propose DragDiffusion. By leveraging large-scale pretrained diffusion models, we greatly improve the applicability of interactive point-based editing in real world scenarios. While most existing diffusion-based image editing methods work on text embeddings, DragDiffusion optimizes the diffusion latent to achieve precise spatial control. Although diffusion models generate images in an iterative manner, we empirically show that optimizing diffusion latent at one single step suffices to generate coherent results, enabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#23545;&#25239;&#33976;&#39311;&#26041;&#27861;&#65288;FAD&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#28857;&#20113;&#20998;&#31867;&#20013;&#30693;&#35782;&#20256;&#36882;&#30340;&#20449;&#24687;&#25439;&#22833;&#38382;&#39064;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#22411;&#21387;&#32553;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.14221</link><description>&lt;p&gt;
&#29305;&#24449;&#23545;&#25239;&#33976;&#39311;&#29992;&#20110;&#28857;&#20113;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Feature Adversarial Distillation for Point Cloud Classification. (arXiv:2306.14221v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#23545;&#25239;&#33976;&#39311;&#26041;&#27861;&#65288;FAD&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#28857;&#20113;&#20998;&#31867;&#20013;&#30693;&#35782;&#20256;&#36882;&#30340;&#20449;&#24687;&#25439;&#22833;&#38382;&#39064;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#22411;&#21387;&#32553;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#28857;&#20113;&#30340;&#19981;&#35268;&#21017;&#21644;&#26080;&#24207;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#20256;&#32479;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#22312;&#30452;&#25509;&#24212;&#29992;&#20110;&#28857;&#20113;&#20219;&#21153;&#26102;&#20002;&#22833;&#20102;&#24456;&#22810;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#23545;&#25239;&#33976;&#39311;&#65288;Feature Adversarial Distillation&#65292;&#31616;&#31216;FAD&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#28857;&#20113;&#33976;&#39311;&#20013;&#20351;&#29992;&#30340;&#36890;&#29992;&#23545;&#25239;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20943;&#23569;&#30693;&#35782;&#20256;&#36882;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#25439;&#22833;&#12290;&#22312;&#29305;&#24449;&#25552;&#21462;&#38454;&#27573;&#65292;&#21033;&#29992;&#25945;&#24072;&#25552;&#21462;&#30340;&#29305;&#24449;&#20316;&#20026;&#37492;&#21035;&#22120;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#23398;&#29983;&#19981;&#26029;&#29983;&#25104;&#26032;&#29305;&#24449;&#12290;&#23398;&#29983;&#30340;&#29305;&#24449;&#36890;&#36807;&#25915;&#20987;&#25945;&#24072;&#30340;&#21453;&#39304;&#24471;&#21040;&#24471;&#20998;&#65292;&#20197;&#21028;&#26029;&#23398;&#29983;&#26159;&#21542;&#23398;&#20064;&#20102;&#30693;&#35782;&#12290;&#22312;ModelNet40&#21644;ScanObjectNN&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#26631;&#20934;&#28857;&#20113;&#20998;&#31867;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;40&#20493;&#27169;&#22411;&#21387;&#32553;&#30340;&#21516;&#26102;&#38477;&#20302;&#20102;&#30693;&#35782;&#20256;&#36882;&#20013;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#20445;&#25345;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the point cloud's irregular and unordered geometry structure, conventional knowledge distillation technology lost a lot of information when directly used on point cloud tasks. In this paper, we propose Feature Adversarial Distillation (FAD) method, a generic adversarial loss function in point cloud distillation, to reduce loss during knowledge transfer. In the feature extraction stage, the features extracted by the teacher are used as the discriminator, and the students continuously generate new features in the training stage. The feature of the student is obtained by attacking the feedback from the teacher and getting a score to judge whether the student has learned the knowledge well or not. In experiments on standard point cloud classification on ModelNet40 and ScanObjectNN datasets, our method reduced the information loss of knowledge transfer in distillation in 40x model compression while maintaining competitive performance.
&lt;/p&gt;</description></item><item><title>QNNRepair &#26159;&#19968;&#31181;&#29992;&#20110;&#20462;&#22797;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#31070;&#32463;&#20803;&#26435;&#37325;&#21442;&#25968;&#20197;&#20462;&#22797;&#22312;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#36807;&#31243;&#20013;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#31070;&#32463;&#20803;&#65292;&#20174;&#32780;&#22312;&#19981;&#24433;&#21709;&#36890;&#36807;&#27979;&#35797;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#22312;&#22833;&#36133;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13793</link><description>&lt;p&gt;
QNNRepair&#65306;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#20462;&#22797;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
QNNRepair: Quantized Neural Network Repair. (arXiv:2306.13793v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13793
&lt;/p&gt;
&lt;p&gt;
QNNRepair &#26159;&#19968;&#31181;&#29992;&#20110;&#20462;&#22797;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#31070;&#32463;&#20803;&#26435;&#37325;&#21442;&#25968;&#20197;&#20462;&#22797;&#22312;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#36807;&#31243;&#20013;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#31070;&#32463;&#20803;&#65292;&#20174;&#32780;&#22312;&#19981;&#24433;&#21709;&#36890;&#36807;&#27979;&#35797;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#22312;&#22833;&#36133;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;QNNRepair&#65292;&#36825;&#26159;&#25991;&#29486;&#20013;&#31532;&#19968;&#20010;&#20462;&#27491;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#30340;&#26041;&#27861;&#12290;&#20854;&#26088;&#22312;&#25552;&#39640;&#22312;&#37327;&#21270;&#20043;&#21518;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#20854;&#25509;&#21463;&#20840;&#31934;&#24230;&#21644;&#26435;&#37325;&#37327;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#19968;&#20010;&#20462;&#22797;&#25968;&#25454;&#38598;&#12290;QNNRepair&#23558;&#20462;&#22797;&#38382;&#39064;&#36716;&#21270;&#20026;&#32447;&#24615;&#35268;&#21010;&#65292;&#36890;&#36807;&#35299;&#20915;&#31070;&#32463;&#20803;&#26435;&#37325;&#21442;&#25968;&#20197;&#20462;&#22797;&#22312;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#36807;&#31243;&#20013;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#31070;&#32463;&#20803;&#65292;&#20174;&#32780;&#22312;&#19981;&#24433;&#21709;&#36890;&#36807;&#27979;&#35797;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#22312;&#22833;&#36133;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present QNNRepair, the first method in the literature for repairing quantized neural networks (QNNs). QNNRepair aims to improve the accuracy of a neural network model after quantization. It accepts the full-precision and weight-quantized neural networks and a repair dataset of passing and failing tests. At first, QNNRepair applies a software fault localization method to identify the neurons that cause performance degradation during neural network quantization. Then, it formulates the repair problem into a linear programming problem of solving neuron weights parameters, which corrects the QNN's performance on failing tests while not compromising its performance on passing tests. We evaluate QNNRepair with widely used neural network architectures such as MobileNetV2, ResNet, and VGGNet on popular datasets, including high-resolution images. We also compare QNNRepair with the state-of-the-art data-free quantization method SQuant. According to the experiment results, we conclude that QNN
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;softmax-attention&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#22312;p&#25110;&#31561;&#20215;&#30340;W&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;&#26368;&#22823;&#36793;&#32536;&#35299;&#65292;&#36825;&#23558;&#23616;&#37096;&#26368;&#20248;&#30340;&#26631;&#35760;&#19982;&#38750;&#26368;&#20248;&#30340;&#26631;&#35760;&#20998;&#38548;&#24320;&#12290;&#36825;&#26126;&#30830;&#22320;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24418;&#24335;&#21270;&#20026;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.13596</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#30340;&#36793;&#32536;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Margin Maximization in Attention Mechanism. (arXiv:2306.13596v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;softmax-attention&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#22312;p&#25110;&#31561;&#20215;&#30340;W&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;&#26368;&#22823;&#36793;&#32536;&#35299;&#65292;&#36825;&#23558;&#23616;&#37096;&#26368;&#20248;&#30340;&#26631;&#35760;&#19982;&#38750;&#26368;&#20248;&#30340;&#26631;&#35760;&#20998;&#38548;&#24320;&#12290;&#36825;&#26126;&#30830;&#22320;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24418;&#24335;&#21270;&#20026;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;Transformer&#26550;&#26500;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#20063;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#24778;&#20154;&#25104;&#21151;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#32972;&#21518;&#30340;&#29702;&#35770;&#21407;&#21017;&#23578;&#19981;&#28165;&#26970;&#65292;&#29305;&#21035;&#26159;&#23427;&#30340;&#38750;&#20984;&#20248;&#21270;&#21160;&#21147;&#23398;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#21019;&#24615;&#30340;softmax-attention&#27169;&#22411;$f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$&#65292;&#20854;&#20013;$\boldsymbol{X}$&#26159;&#26631;&#35760;&#24207;&#21015;&#65292;$(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$&#26159;&#21487;&#35843;&#21442;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;$\boldsymbol{p}$&#25110;&#31561;&#20215;&#30340;$\boldsymbol{W}$&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#20250;&#27839;&#30528;&#26041;&#21521;&#25910;&#25947;&#21040;&#20998;&#38548;&#8220;&#23616;&#37096;&#26368;&#20248;&#8221;&#26631;&#35760;&#21644;&#8220;&#38750;&#26368;&#20248;&#8221;&#26631;&#35760;&#30340;&#26368;&#22823;&#36793;&#32536;&#35299;&#12290;&#36825;&#26126;&#30830;&#22320;&#24418;&#24335;&#21270;&#20102;&#27880;&#24847;&#21147;&#20316;&#20026;&#19968;&#31181;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#19968;&#33324;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#23884;&#20837;$\boldsymbol{Xv}$&#21644;$\texttt{softmax}(\boldsymbol{XWp})$&#31934;&#32454;&#22320;&#34920;&#24449;&#26631;&#35760;&#30340;&#8220;&#26368;&#20248;&#24615;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention mechanism is a central component of the transformer architecture which led to the phenomenal success of large language models. However, the theoretical principles underlying the attention mechanism are poorly understood, especially its nonconvex optimization dynamics. In this work, we explore the seminal softmax-attention model $f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$, where, $\boldsymbol{X}$ is the token sequence and $(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$ are tunable parameters. We prove that running gradient descent on $\boldsymbol{p}$, or equivalently $\boldsymbol{W}$, converges in direction to a max-margin solution that separates $\textit{locally-optimal}$ tokens from non-optimal ones. This clearly formalizes attention as a token separation mechanism. Remarkably, our results are applicable to general data and precisely characterize $\textit{optimality}$ of tokens in terms of the value embeddings $\boldsymbol{Xv}$ and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36712;&#36857;&#37319;&#26679;&#19979;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#23567;&#25209;&#37327;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;MNIST&#25968;&#25454;&#38598;&#23454;&#39564;&#65292;&#21457;&#29616;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#21516;&#26102;&#36824;&#25552;&#39640;&#20102;&#25512;&#26029;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13442</link><description>&lt;p&gt;
&#36712;&#36857;&#37319;&#26679;&#19979;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#23567;&#25209;&#37327;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Minibatch training of neural network ensembles via trajectory sampling. (arXiv:2306.13442v1 [cond-mat.stat-mech])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36712;&#36857;&#37319;&#26679;&#19979;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;&#23567;&#25209;&#37327;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;MNIST&#25968;&#25454;&#38598;&#23454;&#39564;&#65292;&#21457;&#29616;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#21516;&#26102;&#36824;&#25552;&#39640;&#20102;&#25512;&#26029;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#36845;&#20195;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#20351;&#29992;&#25968;&#25454;&#30340;&#23567;&#38543;&#26426;&#23376;&#38598;&#65288;&#25110;&#23567;&#25209;&#37327;&#65289;&#30340;&#25439;&#22833;&#20989;&#25968;&#20272;&#35745;&#26469;&#26356;&#26032;&#21442;&#25968;&#65292;&#22312;&#35757;&#32451;&#26102;&#38388;&#19982;&#24222;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#20043;&#38388;&#35299;&#32806;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#23567;&#25209;&#37327;&#26041;&#27861;&#21487;&#20197;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#36890;&#36807;&#36712;&#36857;&#26041;&#27861;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#38598;&#21512;(NNEs)&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;NNE&#26469;&#20998;&#31867;MNIST&#25968;&#25454;&#38598;&#20013;&#30340;&#22270;&#20687;&#26469;&#35828;&#26126;&#36825;&#31181;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#35757;&#32451;&#26102;&#38388;&#65292;&#20351;&#20854;&#33021;&#22815;&#32553;&#25918;&#20026;&#25968;&#25454;&#38598;&#22823;&#23567;&#19982;&#24179;&#22343;&#23567;&#25209;&#37327;&#22823;&#23567;&#20043;&#27604;&#65292;&#23545;&#20110;MNIST&#26469;&#35828;&#65292;&#35745;&#31639;&#25928;&#29575;&#36890;&#24120;&#25552;&#39640;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#24378;&#35843;&#20351;&#29992;&#36739;&#38271;&#30340;&#36712;&#36857;&#26469;&#34920;&#31034;NNE&#30340;&#20248;&#28857;&#65292;&#26082;&#21487;&#20197;&#25552;&#39640;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#65292;&#21448;&#21487;&#20197;&#22312;&#23567;&#25209;&#37327;&#26356;&#26032;&#25152;&#38656;&#30340;&#26679;&#26412;&#26041;&#38754;&#38477;&#20302;&#26356;&#26032;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most iterative neural network training methods use estimates of the loss function over small random subsets (or minibatches) of the data to update the parameters, which aid in decoupling the training time from the (often very large) size of the training datasets. Here, we show that a minibatch approach can also be used to train neural network ensembles (NNEs) via trajectory methods in a highly efficent manner. We illustrate this approach by training NNEs to classify images in the MNIST datasets. This method gives an improvement to the training times, allowing it to scale as the ratio of the size of the dataset to that of the average minibatch size which, in the case of MNIST, gives a computational improvement typically of two orders of magnitude. We highlight the advantage of using longer trajectories to represent NNEs, both for improved accuracy in inference and reduced update cost in terms of the samples needed in minibatch updates.
&lt;/p&gt;</description></item><item><title>&#36880;&#27493;&#32423;&#32852;&#27169;&#22411;FuXi&#22312;15&#22825;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#30340;&#31215;&#32047;&#36798;&#21040;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.12873</link><description>&lt;p&gt;
FuXi: &#19968;&#20010;15&#22825;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#32423;&#32852;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FuXi: A cascade machine learning forecasting system for 15-day global weather forecast. (arXiv:2306.12873v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12873
&lt;/p&gt;
&lt;p&gt;
&#36880;&#27493;&#32423;&#32852;&#27169;&#22411;FuXi&#22312;15&#22825;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#30340;&#31215;&#32047;&#36798;&#21040;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22825;&#27668;&#39044;&#25253;&#20013;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;0.25&#24230;&#31354;&#38388;&#20998;&#36776;&#29575;&#19979;&#30340;10&#22825;&#22825;&#27668;&#39044;&#25253;&#20013;&#24050;&#32463;&#34920;&#29616;&#20986;&#27604;&#27431;&#27954;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#20013;&#24515;(ECMWF)&#30340;&#39640;&#20998;&#36776;&#29575;&#39044;&#25253;(HRES)&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#22312;15&#22825;&#39044;&#25253;&#20013;&#34920;&#29616;&#19982;ECMWF&#38598;&#21512;&#24179;&#22343;(EM)&#30456;&#24403;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32531;&#35299;&#39044;&#25253;&#35823;&#24046;&#30340;&#31215;&#32047;&#23545;&#20110;&#26377;&#25928;&#30340;&#38271;&#26399;&#39044;&#25253;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#20943;&#23569;&#31215;&#32047;&#35823;&#24046;&#30340;&#21162;&#21147;&#65292;&#21253;&#25324;&#33258;&#22238;&#24402;&#22810;&#26102;&#38388;&#27493;&#38271;&#25439;&#22833;&#65292;&#20294;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#21457;&#29616;&#26080;&#27861;&#22312;&#30701;&#21644;&#38271;&#23548;&#20986;&#26102;&#38388;&#19978;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FuXi&#65292;&#36825;&#26159;&#19968;&#20010;&#32423;&#32852;&#26426;&#22120;&#23398;&#20064;&#22825;&#27668;&#39044;&#27979;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#20998;&#36776;&#29575;&#20026;0.25&#24230;&#12289;&#26102;&#38388;&#20998;&#36776;&#29575;&#20026;6&#23567;&#26102;&#30340;15&#22825;&#20840;&#29699;&#39044;&#27979;&#12290;FuXi&#22522;&#20110;&#32423;&#32852;&#38598;&#21512;&#27169;&#22411;&#24320;&#21457;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#24182;&#20943;&#23569;&#20102;&#39044;&#27979;&#35823;&#24046;&#30340;&#31215;&#32047;&#12290;&#20351;&#29992;&#31354;&#27668;&#28201;&#24230;&#65292;&#27604;&#28287;&#24230;&#21644;&#20301;&#21183;&#39640;&#24230;&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;(RMSE)&#21644;&#24322;&#24120;&#30456;&#20851;&#31995;&#25968;(ACC)&#35780;&#20272;&#20102;FuXi&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;ECMWF HRES&#30456;&#27604;&#65292;FuXi&#22312;15&#22825;&#39044;&#25253;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#31215;&#32047;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, due to the rapid development of machine learning (ML) models for weather forecasting, state-of-the-art ML models have shown superior performance compared to the European Centre for Medium-Range Weather Forecasts (ECMWF)'s high-resolution forecast (HRES) in 10-day forecasts at a spatial resolution of 0.25 degree. However, the challenge remains to perform comparably to the ECMWF ensemble mean (EM) in 15-day forecasts. Previous studies have demonstrated the importance of mitigating the accumulation of forecast errors for effective long-term forecasts. Despite numerous efforts to reduce accumulation errors, including autoregressive multi-time step loss, using a single model is found to be insufficient to achieve optimal performance in both short and long lead times. Therefore, we present FuXi, a cascaded ML weather forecasting system that provides 15-day global forecasts with a temporal resolution of 6 hours and a spatial resolution of 0.25 degree. FuXi is develope
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#37319;&#26679;&#32593;&#32476;&#32467;&#26500;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#22312;&#24674;&#22797;&#32593;&#32476;&#30340;&#22270;&#24418;&#32467;&#26500;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21518;&#39564;&#27010;&#29575;&#30340;&#20934;&#30830;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2306.11380</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Take on Gaussian Process Networks. (arXiv:2306.11380v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11380
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#37319;&#26679;&#32593;&#32476;&#32467;&#26500;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#22312;&#24674;&#22797;&#32593;&#32476;&#30340;&#22270;&#24418;&#32467;&#26500;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21518;&#39564;&#27010;&#29575;&#30340;&#20934;&#30830;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#32593;&#32476;&#65288;GPNs&#65289;&#26159;&#19968;&#31867;&#26377;&#21521;&#22270;&#27169;&#22411;&#65292;&#20854;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#20316;&#20026;&#32593;&#32476;&#20013;&#27599;&#20010;&#21464;&#37327;&#32473;&#23450;&#20854;&#29238;&#21464;&#37327;&#30340;&#26465;&#20214;&#26399;&#26395;&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;&#35813;&#27169;&#22411;&#20801;&#35768;&#20197;&#32039;&#20945;&#20294;&#28789;&#27963;&#30340;&#26041;&#24335;&#25551;&#36848;&#36830;&#32493;&#32852;&#21512;&#20998;&#24067;&#65292;&#23545;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#20165;&#20570;&#26368;&#23569;&#30340;&#21442;&#25968;&#20551;&#35774;&#12290;GPNs&#30340;&#36125;&#21494;&#26031;&#32467;&#26500;&#23398;&#20064;&#38656;&#35201;&#35745;&#31639;&#32593;&#32476;&#32467;&#26500;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#21363;&#20351;&#22312;&#20302;&#32500;&#24773;&#20917;&#19979;&#65292;&#36825;&#20063;&#26159;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#23454;&#29616;&#20102;&#33945;&#29305;&#21345;&#32599;&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#26469;&#20174;&#32593;&#32476;&#32467;&#26500;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#22240;&#27492;&#65292;&#35813;&#26041;&#27861;&#36981;&#24490;&#36125;&#21494;&#26031;&#33539;&#24335;&#65292;&#36890;&#36807;&#36793;&#32536;&#20284;&#28982;&#27604;&#36739;&#27169;&#22411;&#65292;&#24182;&#35745;&#31639;GPN&#29305;&#24449;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#27169;&#25311;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24674;&#22797;&#32593;&#32476;&#30340;&#22270;&#24418;&#32467;&#26500;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20854;&#21518;&#39564;&#30340;&#20934;&#30830;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian Process Networks (GPNs) are a class of directed graphical models which employ Gaussian processes as priors for the conditional expectation of each variable given its parents in the network. The model allows describing continuous joint distributions in a compact but flexible manner with minimal parametric assumptions on the dependencies between variables. Bayesian structure learning of GPNs requires computing the posterior over graphs of the network and is computationally infeasible even in low dimensions. This work implements Monte Carlo and Markov Chain Monte Carlo methods to sample from the posterior distribution of network structures. As such, the approach follows the Bayesian paradigm, comparing models via their marginal likelihood and computing the posterior probability of the GPN features. Simulation studies show that our method outperforms state-of-the-art algorithms in recovering the graphical structure of the network and provides an accurate approximation of its poste
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#21644;&#21487;&#24494;&#20998;&#36712;&#36857;&#37325;&#21152;&#26435;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21183;&#33021;&#65292;&#23454;&#29616;&#20102;&#33258;&#19978;&#32780;&#19979;&#30340;&#31895;&#31890;&#21270;&#34507;&#30333;&#36136;&#21147;&#22330;&#24314;&#27169;&#65292;&#20165;&#38656;&#34507;&#30333;&#36136;&#30340;&#22825;&#28982;&#26500;&#35937;&#21363;&#21487;&#23637;&#31034;&#20854;&#22806;&#25512;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.11375</link><description>&lt;p&gt;
&#33258;&#19978;&#32780;&#19979;&#30340;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#31895;&#31890;&#21270;&#34507;&#30333;&#36136;&#21147;&#22330;
&lt;/p&gt;
&lt;p&gt;
Top-down machine learning of coarse-grained protein force-fields. (arXiv:2306.11375v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11375
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#21644;&#21487;&#24494;&#20998;&#36712;&#36857;&#37325;&#21152;&#26435;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21183;&#33021;&#65292;&#23454;&#29616;&#20102;&#33258;&#19978;&#32780;&#19979;&#30340;&#31895;&#31890;&#21270;&#34507;&#30333;&#36136;&#21147;&#22330;&#24314;&#27169;&#65292;&#20165;&#38656;&#34507;&#30333;&#36136;&#30340;&#22825;&#28982;&#26500;&#35937;&#21363;&#21487;&#23637;&#31034;&#20854;&#22806;&#25512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#34507;&#30333;&#36136;&#31895;&#31890;&#21270;&#34920;&#24449;&#23545;&#20110;&#29702;&#35299;&#23427;&#20204;&#30340;&#25240;&#21472;&#12289;&#21151;&#33021;&#21644;&#22312;&#38271;&#26102;&#38388;&#23610;&#24230;&#19979;&#30340;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#34507;&#30333;&#36136;&#65292;&#24182;&#21033;&#29992;&#24471;&#21040;&#30340;&#36712;&#36857;&#36890;&#36807;&#21487;&#24494;&#20998;&#36712;&#36857;&#37325;&#21152;&#26435;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21183;&#33021;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#20165;&#38656;&#35201;&#34507;&#30333;&#36136;&#30340;&#22825;&#28982;&#26500;&#35937;&#65292;&#28040;&#38500;&#20102;&#20174;&#24191;&#27867;&#27169;&#25311;&#25110;&#20869;&#23384;&#23494;&#38598;&#22411;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#27169;&#25311;&#23548;&#20986;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#24182;&#34892;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#65292;&#24182;&#23545;&#35757;&#32451;&#20998;&#24067;&#20869;&#22806;&#30340;&#34507;&#30333;&#36136;&#36827;&#34892;&#25240;&#21472;&#20107;&#20214;&#37319;&#26679;&#65292;&#23637;&#31034;&#20854;&#22806;&#25512;&#33021;&#21147;&#12290;&#36890;&#36807;&#24212;&#29992;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#27169;&#25311;&#34507;&#30333;&#36136;&#30340;&#19982;&#22825;&#28982;&#26500;&#35937;&#30456;&#20284;&#30340;&#26500;&#35937;&#12290;&#30001;&#20110;&#20854;&#29702;&#35770;&#21487;&#36716;&#31227;&#24615;&#21644;&#20165;&#20351;&#29992;&#34507;&#30333;&#36136;&#30340;&#22825;&#28982;&#26500;&#35937;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#24212;&#29992;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing accurate and efficient coarse-grained representations of proteins is crucial for understanding their folding, function, and interactions over extended timescales. Our methodology involves simulating proteins with molecular dynamics and utilizing the resulting trajectories to train a neural network potential through differentiable trajectory reweighting. Remarkably, this method requires only the native conformation of proteins, eliminating the need for labeled data derived from extensive simulations or memory-intensive end-to-end differentiable simulations. Once trained, the model can be employed to run parallel molecular dynamics simulations and sample folding events for proteins both within and beyond the training distribution, showcasing its extrapolation capabilities. By applying Markov State Models, native-like conformations of the simulated proteins can be predicted from the coarse-grained simulations. Owing to its theoretical transferability and ability to use solely e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;FPGA&#30340;GNN&#26550;&#26500;&#65292;&#22312;&#31890;&#23376;&#36319;&#36394;&#20013;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#21644;&#36164;&#28304;&#39640;&#25928;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#29616;&#26377;CPU&#21644;GPU&#30340;&#25968;&#21315;&#20493;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.11330</link><description>&lt;p&gt;
&#22522;&#20110;FPGA&#30340;&#31890;&#23376;&#36712;&#36857;&#36319;&#36394;&#30340;&#20302;&#24310;&#36831;&#36793;&#32536;&#20998;&#31867;GNN
&lt;/p&gt;
&lt;p&gt;
Low Latency Edge Classification GNN for Particle Trajectory Tracking on FPGAs. (arXiv:2306.11330v1 [cs.AR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;FPGA&#30340;GNN&#26550;&#26500;&#65292;&#22312;&#31890;&#23376;&#36319;&#36394;&#20013;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#21644;&#36164;&#28304;&#39640;&#25928;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#23545;&#29616;&#26377;CPU&#21644;GPU&#30340;&#25968;&#21315;&#20493;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#20013;&#23454;&#26102;&#31890;&#23376;&#36712;&#36857;&#37325;&#24314;&#38754;&#20020;&#39640;&#30896;&#25758;&#29575;&#21644;&#20247;&#22810;&#31890;&#23376;&#25758;&#20987;&#30340;&#25361;&#25112;&#12290;&#20351;&#29992;FPGA&#19978;&#30340;GNN&#65288;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#28789;&#27963;&#30340;&#36712;&#36857;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GNN&#26550;&#26500;&#22312;&#36793;&#32536;&#20998;&#31867;&#26041;&#38754;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#20302;&#19988;&#24182;&#34892;&#24615;&#19981;&#36275;&#12290;&#26412;&#25991;&#22312;FPGA&#19978;&#24341;&#20837;&#20102;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;GNN&#26550;&#26500;&#65292;&#29992;&#20110;&#20302;&#24310;&#36831;&#31890;&#23376;&#36319;&#36394;&#12290;&#27169;&#22359;&#21270;&#30340;&#26550;&#26500;&#20415;&#20110;&#35774;&#35745;&#25193;&#23637;&#20197;&#25903;&#25345;&#22823;&#22411;&#22270;&#24418;&#12290;&#21033;&#29992;&#20987;&#20013;&#25506;&#27979;&#22120;&#30340;&#20960;&#20309;&#29305;&#24615;&#36827;&#19968;&#27493;&#20943;&#23569;&#20102;&#22270;&#24418;&#30340;&#22797;&#26434;&#24615;&#21644;&#36164;&#28304;&#21033;&#29992;&#29575;&#12290;&#25105;&#20204;&#22312;Xilinx UltraScale+ VU9P&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;CPU&#21644;GPU&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;1625&#20493;&#21644;1574&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-time particle trajectory reconstruction in the Large Hadron Collider is challenging due to the high collision rate and numerous particle hits. Using GNN (Graph Neural Network) on FPGA has enabled superior accuracy with flexible trajectory classification. However, existing GNN architectures have inefficient resource usage and insufficient parallelism for edge classification. This paper introduces a resource-efficient GNN architecture on FPGAs for low latency particle tracking. The modular architecture facilitates design scalability to support large graphs. Leveraging the geometric properties of hit detectors further reduces graph complexity and resource usage. Our results on Xilinx UltraScale+ VU9P demonstrate 1625x and 1574x performance improvement over CPU and GPU respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;FPGA&#19978;&#30340;&#21521;&#37327;&#25628;&#32034;&#26694;&#26550;FANNS&#65292;&#23454;&#29616;&#20102;&#30828;&#20214;&#21644;&#31639;&#27861;&#30340;&#20849;&#21516;&#35774;&#35745;&#65292;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#21644;&#30828;&#20214;&#39044;&#31639;&#29983;&#25104;&#30456;&#24212;&#30340;&#21152;&#36895;&#22120;&#12290;&#19982;FPGA&#21644;CPU&#22522;&#20934;&#30456;&#27604;&#65292;FANNS&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#65292;&#24182;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11182</link><description>&lt;p&gt;
&#20026;&#21521;&#37327;&#25628;&#32034;&#36827;&#34892;&#30828;&#20214;&#21644;&#31639;&#27861;&#30340;&#20849;&#21516;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Co-design Hardware and Algorithm for Vector Search. (arXiv:2306.11182v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;FPGA&#19978;&#30340;&#21521;&#37327;&#25628;&#32034;&#26694;&#26550;FANNS&#65292;&#23454;&#29616;&#20102;&#30828;&#20214;&#21644;&#31639;&#27861;&#30340;&#20849;&#21516;&#35774;&#35745;&#65292;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#21644;&#30828;&#20214;&#39044;&#31639;&#29983;&#25104;&#30456;&#24212;&#30340;&#21152;&#36895;&#22120;&#12290;&#19982;FPGA&#21644;CPU&#22522;&#20934;&#30456;&#27604;&#65292;FANNS&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#65292;&#24182;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21521;&#37327;&#25628;&#32034;&#24050;&#25104;&#20026;&#22823;&#35268;&#27169;&#20449;&#24687;&#26816;&#32034;&#21644;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#22522;&#30784;&#65292;&#20687;Google&#21644;Bing&#36825;&#26679;&#30340;&#25628;&#32034;&#24341;&#25806;&#36890;&#36807;&#35780;&#20272;&#32534;&#30721;&#26597;&#35810;&#25991;&#26412;&#21644;&#32593;&#32476;&#25991;&#26723;&#20043;&#38388;&#30340;&#21521;&#37327;&#30456;&#20284;&#24230;&#65292;&#27599;&#31186;&#22788;&#29702;&#25968;&#19975;&#20010;&#26597;&#35810;&#65292;&#22312;&#25317;&#26377;PB&#32423;&#25991;&#26723;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#12290;&#38543;&#30528;&#23545;&#21521;&#37327;&#25628;&#32034;&#31995;&#32479;&#24615;&#33021;&#30340;&#38656;&#27714;&#28608;&#22686;&#65292;&#22312;&#25705;&#23572;&#23450;&#24459;&#26102;&#20195;&#21518;&#65292;&#21152;&#36895;&#30828;&#20214;&#25104;&#20026;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22312;FPGA&#19978;&#30340;&#31471;&#21040;&#31471;&#21487;&#25193;&#23637;&#21521;&#37327;&#25628;&#32034;&#26694;&#26550;FANNS&#12290;&#32473;&#23450;&#29992;&#25143;&#25552;&#20379;&#30340;&#23545;&#25968;&#25454;&#38598;&#30340;&#21484;&#22238;&#35201;&#27714;&#21644;&#30828;&#20214;&#36164;&#28304;&#39044;&#31639;&#65292;FANNS&#33258;&#21160;&#36827;&#34892;&#30828;&#20214;&#21644;&#31639;&#27861;&#30340;&#20849;&#21516;&#35774;&#35745;&#65292;&#38543;&#21518;&#29983;&#25104;&#30456;&#24212;&#30340;&#21152;&#36895;&#22120;&#12290;&#35813;&#26694;&#26550;&#36824;&#36890;&#36807;&#22312;&#21152;&#36895;&#22120;&#20013;&#24341;&#20837;&#30828;&#20214;TCP/IP&#22534;&#26632;&#26469;&#25903;&#25345;&#35268;&#27169;&#25193;&#23637;&#12290;&#19982;FPGA&#21644;CPU&#22522;&#20934;&#30456;&#27604;&#65292;FANNS&#20998;&#21035;&#23454;&#29616;&#20102;23.0&#20493;&#21644;37.2&#20493;&#30340;&#21152;&#36895;&#65292;&#24182;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vector search has emerged as the foundation for large-scale information retrieval and machine learning systems, with search engines like Google and Bing processing tens of thousands of queries per second on petabyte-scale document datasets by evaluating vector similarities between encoded query texts and web documents. As performance demands for vector search systems surge, accelerated hardware offers a promising solution in the post-Moore's Law era. We introduce \textit{FANNS}, an end-to-end and scalable vector search framework on FPGAs. Given a user-provided recall requirement on a dataset and a hardware resource budget, \textit{FANNS} automatically co-designs hardware and algorithm, subsequently generating the corresponding accelerator. The framework also supports scale-out by incorporating a hardware TCP/IP stack in the accelerator. \textit{FANNS} attains up to 23.0$\times$ and 37.2$\times$ speedup compared to FPGA and CPU baselines, respectively, and demonstrates superior scalabil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26494;&#24347;&#20102;&#23436;&#20840;&#19981;&#21464;&#24615;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#25928;&#26524;&#19981;&#21464;&#24615;&#65292;&#35777;&#26126;&#23427;&#36275;&#20197;&#36827;&#34892;&#38646;&#26679;&#26412;&#31574;&#30053;&#27010;&#25324;&#65292;&#24182;&#35752;&#35770;&#20102;&#22522;&#20110;&#23569;&#37327;&#26679;&#26412;&#30340;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.10983</link><description>&lt;p&gt;
&#31574;&#30053;&#27010;&#25324;&#20013;&#30340;&#25928;&#26524;&#19981;&#21464;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Effect-Invariant Mechanisms for Policy Generalization. (arXiv:2306.10983v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26494;&#24347;&#20102;&#23436;&#20840;&#19981;&#21464;&#24615;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#25928;&#26524;&#19981;&#21464;&#24615;&#65292;&#35777;&#26126;&#23427;&#36275;&#20197;&#36827;&#34892;&#38646;&#26679;&#26412;&#31574;&#30053;&#27010;&#25324;&#65292;&#24182;&#35752;&#35770;&#20102;&#22522;&#20110;&#23569;&#37327;&#26679;&#26412;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#23398;&#20064;&#26159;&#35768;&#22810;&#23454;&#38469;&#23398;&#20064;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#31574;&#30053;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22914;&#20309;&#26377;&#25928;&#22320;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#29615;&#22659;&#25110;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#26377;&#20154;&#24314;&#35758;&#21033;&#29992;&#19981;&#21464;&#30340;&#26465;&#20214;&#20998;&#24067;&#26469;&#23398;&#20064;&#26356;&#22909;&#22320;&#27010;&#25324;&#26410;&#35265;&#36807;&#29615;&#22659;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20551;&#35774;&#25972;&#20010;&#26465;&#20214;&#20998;&#24067;&#26159;&#19981;&#21464;&#30340;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#23436;&#20840;&#19981;&#21464;&#24615;&#65289;&#65292;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#26159;&#19968;&#20010;&#22826;&#24378;&#30340;&#20551;&#35774;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26494;&#24347;&#23436;&#20840;&#19981;&#21464;&#24615;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#25928;&#26524;&#19981;&#21464;&#24615;&#65288;&#31616;&#31216;e-&#19981;&#21464;&#24615;&#65289;&#65292;&#24182;&#35777;&#26126;&#23427;&#26159;&#36275;&#22815;&#30340;&#65288;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#65289;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#31574;&#30053;&#27010;&#25324;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#19968;&#31181;&#25193;&#23637;&#65292;&#23427;&#22312;&#27979;&#35797;&#29615;&#22659;&#20013;&#21482;&#26377;&#23569;&#37327;&#26679;&#26412;&#26102;&#21033;&#29992;e-&#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23569;&#26679;&#26412;&#31574;&#30053;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#19981;&#20551;&#35774;&#23384;&#22312;&#19968;&#20010;&#22522;&#30784;&#22240;&#26524;&#22270;&#65292;&#20063;&#19981;&#20551;&#35774;&#25968;&#25454;&#26159;&#30001;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#29983;&#25104;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#27979;&#35797;&#36807;&#31243;&#26469;&#27979;&#35797;e-&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy learning is an important component of many real-world learning systems. A major challenge in policy learning is how to adapt efficiently to unseen environments or tasks. Recently, it has been suggested to exploit invariant conditional distributions to learn models that generalize better to unseen environments. However, assuming invariance of entire conditional distributions (which we call full invariance) may be too strong of an assumption in practice. In this paper, we introduce a relaxation of full invariance called effect-invariance (e-invariance for short) and prove that it is sufficient, under suitable assumptions, for zero-shot policy generalization. We also discuss an extension that exploits e-invariance when we have a small sample from the test environment, enabling few-shot policy generalization. Our work does not assume an underlying causal graph or that the data are generated by a structural causal model; instead, we develop testing procedures to test e-invariance dir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#35821;&#20041;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#26681;&#25454;&#26053;&#23458;&#30340;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#65292;&#23454;&#39564;&#20013;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.10946</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Tourist Attractions Recommendation based on Attention Knowledge Graph Convolution Network. (arXiv:2306.10946v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26053;&#28216;&#26223;&#28857;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#35821;&#20041;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#26681;&#25454;&#26053;&#23458;&#30340;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#65292;&#23454;&#39564;&#20013;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#25512;&#33616;&#31639;&#27861;&#22312;&#30456;&#23545;&#25104;&#29087;&#38454;&#27573;&#65292;&#20294;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#25512;&#33616;&#20173;&#23384;&#22312;&#38382;&#39064;&#12290;&#20363;&#22914;&#22312;&#26053;&#28216;&#39046;&#22495;&#65292;&#36873;&#25321;&#36866;&#21512;&#30340;&#26053;&#28216;&#26223;&#28857;&#23646;&#24615;&#27969;&#31243;&#20316;&#20026;&#25512;&#33616;&#22522;&#30784;&#36739;&#20026;&#22797;&#26434;&#12290;&#26412;&#25991;&#25552;&#20986;&#25913;&#36827;&#30340;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#27169;&#22411;(Att-KGCN)&#65292;&#33258;&#21160;&#35821;&#20041;&#22320;&#21457;&#25496;&#30446;&#26631;&#26223;&#28857;&#30340;&#30456;&#37051;&#23454;&#20307;&#65292;&#21033;&#29992;&#27880;&#24847;&#21147;&#23618;&#23558;&#30456;&#23545;&#30456;&#20284;&#30340;&#20301;&#32622;&#36827;&#34892;&#32858;&#21512;&#65292;&#24182;&#36890;&#36807;&#25512;&#29702;&#26053;&#23458;&#21916;&#22909;&#36873;&#25321;&#65292;&#39044;&#27979;&#31867;&#20284;&#26223;&#28857;&#30340;&#27010;&#29575;&#20316;&#20026;&#25512;&#33616;&#31995;&#32479;&#12290;&#23454;&#39564;&#20013;&#65292;&#37319;&#29992;&#32034;&#31185;&#29305;&#25289;&#23707;-&#20063;&#38376;&#30340;&#26053;&#28216;&#25968;&#25454;&#65292;&#35777;&#26126;&#20102;&#27880;&#24847;&#21147;&#30693;&#35782;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#26053;&#28216;&#39046;&#22495;&#30340;&#26223;&#28857;&#25512;&#33616;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recommendation algorithm based on knowledge graphs is at a relatively mature stage. However, there are still some problems in the recommendation of specific areas. For example, in the tourism field, selecting suitable tourist attraction attributes process is complicated as the recommendation basis for tourist attractions. In this paper, we propose the improved Attention Knowledge Graph Convolution Network model, named (Att-KGCN), which automatically discovers the neighboring entities of the target scenic spot semantically. The attention layer aggregates relatively similar locations and represents them with an adjacent vector. Then, according to the tourist's preferred choices, the model predicts the probability of similar spots as a recommendation system. A knowledge graph dataset of tourist attractions used based on tourism data on Socotra Island-Yemen. Through experiments, it is verified that the Attention Knowledge Graph Convolution Network has a good effect on the recommendatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32039;&#26680;&#30340;&#31639;&#23376;&#29702;&#35770;&#26041;&#27861;&#26469;&#35299;&#20915;&#26465;&#20214;&#26399;&#26395;&#20272;&#35745;&#38382;&#39064;&#65292;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#23454;&#29616;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#19988;&#25104;&#21151;&#24212;&#29992;&#20110;&#23454;&#38469;&#38382;&#39064;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.10592</link><description>&lt;p&gt;
&#22522;&#20110;&#32039;&#26680;&#30340;&#26465;&#20214;&#26399;&#26395;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Conditional expectation via compact kernels. (arXiv:2306.10592v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32039;&#26680;&#30340;&#31639;&#23376;&#29702;&#35770;&#26041;&#27861;&#26469;&#35299;&#20915;&#26465;&#20214;&#26399;&#26395;&#20272;&#35745;&#38382;&#39064;&#65292;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#23454;&#29616;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#19988;&#25104;&#21151;&#24212;&#29992;&#20110;&#23454;&#38469;&#38382;&#39064;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#12289;&#26465;&#20214;&#26399;&#26395;&#21644;&#27969;&#24418;&#23398;&#20064;&#20219;&#21153;&#36890;&#24120;&#21487;&#20197;&#22312;&#23547;&#25214;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#31215;&#30340;&#26465;&#20214;&#26399;&#26395;&#30340;&#20844;&#20849;&#29615;&#22659;&#19979;&#34920;&#36848;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#20010;&#26356;&#19968;&#33324;&#30340;&#38382;&#39064;&#65292;&#25551;&#36848;&#20102;&#19968;&#31181;&#31639;&#23376;&#29702;&#35770;&#26041;&#27861;&#26469;&#20272;&#35745;&#26465;&#20214;&#26399;&#26395;&#12290;&#26680;&#31215;&#20998;&#31639;&#23376;&#34987;&#29992;&#20316;&#32039;&#33268;&#21270;&#24037;&#20855;&#65292;&#23558;&#20272;&#35745;&#38382;&#39064;&#35774;&#32622;&#20026;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#36870;&#38382;&#39064;&#12290;&#35813;&#26041;&#31243;&#30340;&#35299;&#34987;&#35777;&#26126;&#23545;&#25968;&#20540;&#36924;&#36817;&#26159;&#31283;&#23450;&#30340;&#65292;&#20174;&#32780;&#30830;&#20445;&#20102;&#25968;&#25454;&#39537;&#21160;&#23454;&#29616;&#30340;&#25910;&#25947;&#24615;&#12290;&#24635;&#20307;&#25216;&#26415;&#26131;&#20110;&#23454;&#29616;&#65292;&#36824;&#23637;&#31034;&#20102;&#20854;&#22312;&#19968;&#20123;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#25104;&#21151;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The separate tasks of denoising, conditional expectation and manifold learning can often be posed in a common setting of finding the conditional expectations arising from a product of two random variables. This paper focuses on this more general problem and describes an operator theoretic approach to estimating the conditional expectation. Kernel integral operators are used as a compactification tool, to set up the estimation problem as a linear inverse problem in a reproducing kernel Hilbert space. This equation is shown to have solutions that are stable to numerical approximation, thus guaranteeing the convergence of data-driven implementations. The overall technique is easy to implement, and their successful application to some real-world problems are also shown.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32500;RL&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#35813;&#27169;&#22411;&#30340;&#20856;&#22411;&#21160;&#21147;&#23398;&#20026;&#19968;&#32452;&#38381;&#24335;ODE&#26041;&#31243;&#32452;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#19982;&#31070;&#32463;RL&#20195;&#29702;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#25429;&#25417;&#20102;&#29616;&#23454;&#19990;&#30028;RL&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.10404</link><description>&lt;p&gt;
RL&#24863;&#30693;&#26426;&#65306;&#39640;&#32500;&#31574;&#30053;&#23398;&#20064;&#30340;&#27867;&#21270;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
The RL Perceptron: Generalisation Dynamics of Policy Learning in High Dimensions. (arXiv:2306.10404v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32500;RL&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#35813;&#27169;&#22411;&#30340;&#20856;&#22411;&#21160;&#21147;&#23398;&#20026;&#19968;&#32452;&#38381;&#24335;ODE&#26041;&#31243;&#32452;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#19982;&#31070;&#32463;RL&#20195;&#29702;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#25429;&#25417;&#20102;&#29616;&#23454;&#19990;&#30028;RL&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#20855;&#26377;&#21464;&#38761;&#24615;&#65292;&#20026;&#20102;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#31995;&#32479;&#36890;&#24120;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#20174;&#20687;&#32032;&#25110;&#20854;&#20182;&#39640;&#32500;&#24863;&#23448;&#36755;&#20837;&#20013;&#23398;&#20064;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#37117;&#38598;&#20013;&#20110;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#25110;&#26368;&#22351;&#24773;&#20917;&#20998;&#26512;&#65292;&#20851;&#20110;&#39640;&#32500;&#24773;&#20917;&#19979;&#31574;&#30053;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#22522;&#26412;&#38382;&#39064;&#20173;&#26377;&#24453;&#35299;&#20915;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#30340;&#39640;&#32500;RL&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#25429;&#25417;&#22810;&#31181;&#23398;&#20064;&#21327;&#35758;&#65292;&#24182;&#23558;&#20854;&#20856;&#22411;&#21160;&#21147;&#23398;&#23548;&#20986;&#20026;&#19968;&#32452;&#38381;&#24335;&#24120;&#24494;&#20998;&#26041;&#31243;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#26368;&#20339;&#30340;&#23398;&#20064;&#29575;&#21644;&#20219;&#21153;&#38590;&#24230;&#35843;&#24230;&#65292;&#31867;&#20284;&#20110;&#35757;&#32451;&#20013;&#30340;&#36864;&#28779;&#26041;&#26696;&#21644;&#35838;&#31243;&#34920;&#65292;&#24182;&#34920;&#26126;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#20016;&#23500;&#30340;&#34892;&#20026;&#65292;&#21253;&#25324;&#22312;&#31232;&#30095;&#22870;&#21169;&#19979;&#30340;&#24310;&#36831;&#23398;&#20064;&#65307;&#26681;&#25454;&#22870;&#21169;&#22522;&#32447;&#19981;&#21516;&#30340;&#21508;&#31181;&#23398;&#20064;&#26041;&#26696;&#65307;&#20197;&#21450;&#30001;&#22870;&#21169;&#20005;&#26684;&#31243;&#24230;&#39537;&#21160;&#30340;&#36895;&#24230;-&#20934;&#30830;&#24230;&#26435;&#34913;&#12290;&#19982;&#31070;&#32463;RL&#20195;&#29702;&#36827;&#34892;&#30340;&#23454;&#39564;&#27604;&#36739;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#25429;&#25417;&#20102;&#29616;&#23454;&#19990;&#30028;RL&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) algorithms have proven transformative in a range of domains. To tackle real-world domains, these systems often use neural networks to learn policies directly from pixels or other high-dimensional sensory input. By contrast, much theory of RL has focused on discrete state spaces or worst-case analysis, and fundamental questions remain about the dynamics of policy learning in high-dimensional settings. Here, we propose a solvable high-dimensional model of RL that can capture a variety of learning protocols, and derive its typical dynamics as a set of closed-form ordinary differential equations (ODEs). We derive optimal schedules for the learning rates and task difficulty - analogous to annealing schemes and curricula during training in RL - and show that the model exhibits rich behaviour, including delayed learning under sparse rewards; a variety of learning regimes depending on reward baselines; and a speed-accuracy trade-off driven by reward stringency. Expe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36924;&#36817;&#26230;&#20307;&#26448;&#26009;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35206;&#30422;&#25152;&#26377;&#21407;&#23376;&#23545;&#20043;&#38388;&#30340;&#21183;&#65292;&#20811;&#26381;&#20102;&#30446;&#21069;&#26041;&#27861;&#20013;&#21482;&#32771;&#34385;&#38468;&#36817;&#21407;&#23376;&#38388;&#21183;&#21644;&#26080;&#27861;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#27169;&#24335;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.10045</link><description>&lt;p&gt;
&#39044;&#27979;&#26230;&#20307;&#24615;&#36136;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#39640;&#25928;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction. (arXiv:2306.10045v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36924;&#36817;&#26230;&#20307;&#26448;&#26009;&#30340;&#23436;&#25972;&#30456;&#20114;&#20316;&#29992;&#21183;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35206;&#30422;&#25152;&#26377;&#21407;&#23376;&#23545;&#20043;&#38388;&#30340;&#21183;&#65292;&#20811;&#26381;&#20102;&#30446;&#21069;&#26041;&#27861;&#20013;&#21482;&#32771;&#34385;&#38468;&#36817;&#21407;&#23376;&#38388;&#21183;&#21644;&#26080;&#27861;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#27169;&#24335;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26230;&#20307;&#26448;&#26009;&#30340;&#24615;&#36136;&#39044;&#27979;&#12290;&#26230;&#20307;&#32467;&#26500;&#30001;&#19968;&#20010;&#26368;&#23567;&#30340;&#21333;&#20803;&#26684;&#32452;&#25104;&#65292;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#26080;&#38480;&#37325;&#22797;&#12290;&#22914;&#20309;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20934;&#30830;&#34920;&#31034;&#36825;&#31181;&#37325;&#22797;&#32467;&#26500;&#20173;&#28982;&#27809;&#26377;&#35299;&#20915;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#21482;&#22312;&#38468;&#36817;&#30340;&#33410;&#28857;&#20043;&#38388;&#24314;&#31435;&#36793;&#32536;&#26469;&#26500;&#24314;&#22270;&#24418;&#65292;&#22240;&#27492;&#26080;&#27861;&#24544;&#23454;&#22320;&#25429;&#25417;&#26080;&#38480;&#37325;&#22797;&#30340;&#27169;&#24335;&#21644;&#36828;&#36317;&#31163;&#30340;&#21407;&#23376;&#38388;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#21019;&#26032;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#35758;&#30452;&#25509;&#24314;&#27169;&#29289;&#29702;&#21407;&#29702;&#30340;&#30456;&#20114;&#20316;&#29992;&#21183;&#65292;&#32780;&#19981;&#20165;&#20165;&#20351;&#29992;&#36317;&#31163;&#65292;&#22914;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#25152;&#20570;&#30340;&#12290;&#36825;&#20123;&#21183;&#21253;&#25324;&#24211;&#20177;&#21183;&#65292;&#20262;&#25958;&#20998;&#25955;&#21183;&#21644;Pauli&#26021;&#21147;&#21183;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#27169;&#25152;&#26377;&#21407;&#23376;&#20043;&#38388;&#30340;&#23436;&#25972;&#21183;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38468;&#36817;&#21407;&#23376;&#20043;&#38388;&#30340;&#21183;&#12290;&#36825;&#24471;&#30410;&#20110;&#25105;&#20204;&#29992;&#21487;&#35777;&#26126;&#30340;&#35823;&#24046;&#30028;&#36924;&#36817;&#26080;&#38480;&#21183;&#21644;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We study property prediction for crystal materials. A crystal structure consists of a minimal unit cell that is repeated infinitely in 3D space. How to accurately represent such repetitive structures in machine learning models remains unresolved. Current methods construct graphs by establishing edges only between nearby nodes, thereby failing to faithfully capture infinite repeating patterns and distant interatomic interactions. In this work, we propose several innovations to overcome these limitations. First, we propose to model physics-principled interatomic potentials directly instead of only using distances as in many existing methods. These potentials include the Coulomb potential, London dispersion potential, and Pauli repulsion potential. Second, we model the complete set of potentials among all atoms, instead of only between nearby atoms as in existing methods. This is enabled by our approximations of infinite potential summations with provable error bounds. We further develop 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.08158</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#23398;&#20064;&#21040;&#38750;&#39044;&#26399;&#30340;&#20559;&#35265;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20013;&#20559;&#35265;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#35770;&#25991;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20559;&#35265;&#19982;&#30495;&#23454;&#19990;&#30028;&#30340;&#21361;&#23475;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#25105;&#20204;&#20511;&#37492;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#20559;&#35265;&#31867;&#22411;&#12289;&#37327;&#21270;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#12290;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#23545;&#20110;&#37327;&#21270;&#20559;&#35265;&#30340;&#26041;&#27861;&#23384;&#22312;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#35768;&#22810;&#20559;&#35265;&#24230;&#37327;&#24182;&#19981;&#28041;&#21450;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20559;&#35265;&#65292;&#24403;&#21069;&#30340;&#21435;&#20559;&#35265;&#25216;&#26415;&#26159;&#34920;&#38754;&#30340;&#65292;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#65292;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks often learn unintended biases during training, which might have harmful effects when deployed in real-world settings. This paper surveys 209 papers on bias in NLP models, most of which address sociodemographic bias. To better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. We identify three main categories of NLP bias research: types of bias, quantifying bias, and debiasing. We conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world biases, and that current debiasing techniques are superficial and hide bias rather than removing it. Finally, we provide recommendations for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;IFactor&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#21487;&#36776;&#35782;&#20998;&#35299;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#20197;&#25552;&#21462;&#31283;&#23450;&#19988;&#32039;&#20945;&#30340;&#29615;&#22659;&#34920;&#31034;&#65292;&#24182;&#25581;&#31034;&#20102;&#25152;&#26377;&#19982;&#22870;&#21169;&#30456;&#20851;&#30340;&#22240;&#32032;&#23545;&#31574;&#30053;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06561</link><description>&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;&#21487;&#36776;&#35782;&#20998;&#35299;&#30340;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning World Models with Identifiable Factorization. (arXiv:2306.06561v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;IFactor&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#21487;&#36776;&#35782;&#20998;&#35299;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#20197;&#25552;&#21462;&#31283;&#23450;&#19988;&#32039;&#20945;&#30340;&#29615;&#22659;&#34920;&#31034;&#65292;&#24182;&#25581;&#31034;&#20102;&#25152;&#26377;&#19982;&#22870;&#21169;&#30456;&#20851;&#30340;&#22240;&#32032;&#23545;&#31574;&#30053;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#12289;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#65292;&#25552;&#21462;&#29615;&#22659;&#31283;&#23450;&#32780;&#32039;&#20945;&#30340;&#34920;&#31034;&#23545;&#20110;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#23384;&#22312;&#19981;&#21516;&#31867;&#21035;&#30340;&#20449;&#24687;-&#22914;&#20309;&#26377;&#25928;&#25552;&#21462;&#21644;&#21306;&#20998;&#36825;&#20123;&#20449;&#24687;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;IFactor&#65292;&#29992;&#20110;&#24314;&#27169;&#22235;&#31181;&#19981;&#21516;&#31867;&#21035;&#30340;&#28508;&#22312;&#29366;&#24577;&#21464;&#37327;&#65292;&#36825;&#20123;&#21464;&#37327;&#36890;&#36807;&#19982;&#21160;&#20316;&#21644;&#22870;&#21169;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25429;&#25417;RL&#31995;&#32479;&#20013;&#30340;&#21508;&#20010;&#20449;&#24687;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24314;&#31435;&#20102;&#36825;&#20123;&#28508;&#22312;&#21464;&#37327;&#30340;&#20998;&#22359;&#21487;&#35782;&#21035;&#24615;&#65292;&#36825;&#19981;&#20165;&#25552;&#20379;&#20102;&#31283;&#23450;&#19988;&#32039;&#20945;&#30340;&#34920;&#31034;&#65292;&#36824;&#25581;&#31034;&#20102;&#25152;&#26377;&#19982;&#22870;&#21169;&#30456;&#20851;&#30340;&#22240;&#32032;&#23545;&#31574;&#30053;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20855;&#26377;&#21487;&#36776;&#35782;&#20998;&#22359;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#23454;&#29992;&#26041;&#27861;&#65292;&#30830;&#20445;&#21435;&#38500;&#20887;&#20313;&#20294;&#20445;&#30041;&#26368;&#23567;&#19988;&#36275;&#22815;&#30340;&#20449;&#24687;&#20197;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting a stable and compact representation of the environment is crucial for efficient reinforcement learning in high-dimensional, noisy, and non-stationary environments. Different categories of information coexist in such environments -- how to effectively extract and disentangle these information remains a challenging problem. In this paper, we propose IFactor, a general framework to model four distinct categories of latent state variables that capture various aspects of information within the RL system, based on their interactions with actions and rewards. Our analysis establishes block-wise identifiability of these latent variables, which not only provides a stable and compact representation but also discloses that all reward-relevant factors are significant for policy learning. We further present a practical approach to learning the world model with identifiable blocks, ensuring the removal of redundants but retaining minimal and sufficient information for policy optimization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#22810;&#26679;&#24615;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#35270;&#35282;&#23398;&#20064;&#21644;k-means&#32858;&#31867;&#34701;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#21508;&#31181;&#26631;&#20934;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.05437</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#32858;&#31867;&#26041;&#27861;&#65306;&#19968;&#27493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#22810;&#26679;&#24615;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
One-step Multi-view Clustering with Diverse Representation. (arXiv:2306.05437v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#22810;&#26679;&#24615;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#35270;&#35282;&#23398;&#20064;&#21644;k-means&#32858;&#31867;&#34701;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#21508;&#31181;&#26631;&#20934;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#32858;&#31867;&#22240;&#20854;&#33021;&#22815;&#21033;&#29992;&#19981;&#21516;&#35270;&#35282;&#30340;&#20449;&#24687;&#26469;&#25552;&#39640;&#25928;&#26524;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27493;&#22810;&#35270;&#35282;&#32858;&#31867;&#19982;&#22810;&#26679;&#24615;&#34920;&#24449;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#35270;&#35282;&#23398;&#20064;&#21644;k-means&#32858;&#31867;&#34701;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#26631;&#20934;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#26041;&#38754;&#37117;&#20248;&#20110;&#29616;&#26377;&#30340;&#21516;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view clustering has attracted broad attention due to its capacity to utilize consistent and complementary information among views. Although tremendous progress has been made recently, most existing methods undergo high complexity, preventing them from being applied to large-scale tasks. Multi-view clustering via matrix factorization is a representative to address this issue. However, most of them map the data matrices into a fixed dimension, which limits the expressiveness of the model. Moreover, a range of methods suffer from a two-step process, i.e., multimodal learning and the subsequent $k$-means, inevitably causing a sub-optimal clustering result. In light of this, we propose a one-step multi-view clustering with diverse representation method, which incorporates multi-view learning and $k$-means into a unified framework. Specifically, we first project original data matrices into various latent spaces to attain comprehensive information and auto-weight them in a self-supervis
&lt;/p&gt;</description></item><item><title>EMO&#26159;&#19968;&#31181;&#20803;&#23398;&#20064;&#30340;&#24773;&#33410;&#35760;&#24518;&#20248;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#22806;&#37096;&#23384;&#20648;&#22120;&#20013;&#35760;&#24405;&#36807;&#21435;&#20219;&#21153;&#30340;&#26799;&#24230;&#21382;&#21490;&#65292;&#23454;&#29616;&#23567;&#26679;&#26412;&#23398;&#20064;&#65292;&#26080;&#35770;&#25552;&#20379;&#30340;&#26799;&#24230;&#20449;&#24687;&#26159;&#21542;&#21487;&#38752;&#65292;&#37117;&#21487;&#20197;&#25512;&#21160;&#21442;&#25968;&#26356;&#26032;&#26397;&#30528;&#27491;&#30830;&#30340;&#26041;&#21521;&#21069;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.05189</link><description>&lt;p&gt;
EMO&#65306;&#29992;&#20110;&#23567;&#26679;&#26412;&#20803;&#23398;&#20064;&#30340;&#24773;&#33410;&#35760;&#24518;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
EMO: Episodic Memory Optimization for Few-Shot Meta-Learning. (arXiv:2306.05189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05189
&lt;/p&gt;
&lt;p&gt;
EMO&#26159;&#19968;&#31181;&#20803;&#23398;&#20064;&#30340;&#24773;&#33410;&#35760;&#24518;&#20248;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#22806;&#37096;&#23384;&#20648;&#22120;&#20013;&#35760;&#24405;&#36807;&#21435;&#20219;&#21153;&#30340;&#26799;&#24230;&#21382;&#21490;&#65292;&#23454;&#29616;&#23567;&#26679;&#26412;&#23398;&#20064;&#65292;&#26080;&#35770;&#25552;&#20379;&#30340;&#26799;&#24230;&#20449;&#24687;&#26159;&#21542;&#21487;&#38752;&#65292;&#37117;&#21487;&#20197;&#25512;&#21160;&#21442;&#25968;&#26356;&#26032;&#26397;&#30528;&#27491;&#30830;&#30340;&#26041;&#21521;&#21069;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#26679;&#26412;&#20803;&#23398;&#20064;&#30001;&#20110;&#20219;&#21153;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#30340;&#38480;&#21046;&#23545;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#30340;&#24773;&#33410;&#35760;&#24518;&#20248;&#21270;&#26041;&#26696;&#65292;&#31216;&#20026;EMO&#12290;EMO&#21463;&#21040;&#20154;&#31867;&#20174;&#33041;&#20869;&#35760;&#24518;&#20013;&#22238;&#24518;&#36807;&#21435;&#23398;&#20064;&#32463;&#39564;&#30340;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#23558;&#36807;&#21435;&#20219;&#21153;&#30340;&#26799;&#24230;&#21382;&#21490;&#35760;&#24405;&#22312;&#22806;&#37096;&#23384;&#20648;&#22120;&#20013;&#65292;&#20197;&#22686;&#24378;&#35760;&#24518;&#30340;&#26041;&#24335;&#36827;&#34892;&#23567;&#26679;&#26412;&#23398;&#20064;&#12290;&#36890;&#36807;&#23398;&#20064;&#20445;&#30041;&#21644;&#22238;&#24518;&#36807;&#21435;&#35757;&#32451;&#20219;&#21153;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#21363;&#20351;&#20165;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#31034;&#20363;&#25552;&#20379;&#20102;&#19981;&#21487;&#38752;&#30340;&#26799;&#24230;&#65292;EMO&#20063;&#21487;&#20197;&#25512;&#21160;&#21442;&#25968;&#26356;&#26032;&#26397;&#30528;&#27491;&#30830;&#30340;&#26041;&#21521;&#21069;&#36827;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#23545;&#20110;&#24179;&#28369;&#12289;&#24378;&#20984;&#30446;&#26631;&#20989;&#25968;&#20250;&#25910;&#25947;&#12290;EMO&#26159;&#36890;&#29992;&#30340;&#12289;&#28789;&#27963;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#20248;&#21270;&#22120;&#65292;&#21487;&#26080;&#32541;&#23884;&#20837;&#29616;&#26377;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#23567;&#26679;&#26412;&#20803;&#23398;&#20064;&#26041;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;EMO&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot meta-learning presents a challenge for gradient descent optimization due to the limited number of training samples per task. To address this issue, we propose an episodic memory optimization for meta-learning, we call \emph{EMO}, which is inspired by the human ability to recall past learning experiences from the brain's memory. EMO retains the gradient history of past experienced tasks in external memory, enabling few-shot learning in a memory-augmented way. By learning to retain and recall the learning process of past training tasks, EMO nudges parameter updates in the right direction, even when the gradients provided by a limited number of examples are uninformative. We prove theoretically that our algorithm converges for smooth, strongly convex objectives. EMO is generic, flexible, and model-agnostic, making it a simple plug-and-play optimizer that can be seamlessly embedded into existing optimization-based few-shot meta-learning approaches. Empirical results show that EMO 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProjUnit&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#39640;&#25928;&#30340;&#26412;&#22320;&#38544;&#31169;&#22343;&#20540;&#20272;&#35745;&#65292;&#36890;&#36807;&#38543;&#26426;&#25237;&#24433;&#20302;&#32500;&#31354;&#38388;&#23454;&#29616;&#26368;&#20248;&#35299;&#65292;&#19988;&#20855;&#26377;&#20302;&#36890;&#20449;&#22797;&#26434;&#24230;&#21644;&#24555;&#36895;&#30340;&#26381;&#21153;&#22120;&#36816;&#34892;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.04444</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#25237;&#24433;&#24555;&#36895;&#33719;&#24471;&#26368;&#20248;&#30340;&#26412;&#22320;&#38544;&#31169;&#22343;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Fast Optimal Locally Private Mean Estimation via Random Projections. (arXiv:2306.04444v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04444
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProjUnit&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#39640;&#25928;&#30340;&#26412;&#22320;&#38544;&#31169;&#22343;&#20540;&#20272;&#35745;&#65292;&#36890;&#36807;&#38543;&#26426;&#25237;&#24433;&#20302;&#32500;&#31354;&#38388;&#23454;&#29616;&#26368;&#20248;&#35299;&#65292;&#19988;&#20855;&#26377;&#20302;&#36890;&#20449;&#22797;&#26434;&#24230;&#21644;&#24555;&#36895;&#30340;&#26381;&#21153;&#22120;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#39640;&#32500;&#21521;&#37327;&#30340;&#26412;&#22320;&#38544;&#31169;&#22343;&#20540;&#20272;&#35745;&#38382;&#39064;&#12290;&#29616;&#26377;&#31639;&#27861;&#35201;&#20040;&#20135;&#29983;&#27425;&#20248;&#35823;&#24046;&#65292;&#35201;&#20040;&#20855;&#26377;&#39640;&#36890;&#20449;&#21644;/&#25110;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;ProjUnit&#65292;&#29992;&#20110;&#38544;&#31169;&#22343;&#20540;&#20272;&#35745;&#30340;&#31639;&#27861;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#39640;&#12289;&#36890;&#20449;&#22797;&#26434;&#24230;&#20302;&#19988;&#35823;&#24046;&#19982;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#26368;&#22823;&#20026;1 + o(1)&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#36215;&#26469;&#38750;&#24120;&#31616;&#21333;&#65306;&#27599;&#20010;&#38543;&#26426;&#21270;&#22120;&#23558;&#20854;&#36755;&#20837;&#25237;&#24433;&#21040;&#19968;&#20010;&#38543;&#26426;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#65292;&#23545;&#32467;&#26524;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#28982;&#21518;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#36816;&#34892;&#19968;&#20010;&#26368;&#20248;&#31639;&#27861;&#65292;&#20363;&#22914;PrivUnitG&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36866;&#24403;&#22320;&#21327;&#35843;&#35774;&#22791;&#20043;&#38388;&#30340;&#38543;&#26426;&#25237;&#24433;&#30697;&#38453;&#65292;&#21487;&#20197;&#23454;&#29616;&#24555;&#36895;&#30340;&#26381;&#21153;&#22120;&#36816;&#34892;&#26102;&#38388;&#12290;&#25105;&#20204;&#36890;&#36807;&#38543;&#26426;&#25237;&#24433;&#30340;&#24615;&#36136;&#20998;&#26512;&#20102;&#31639;&#27861;&#30340;&#35823;&#24046;&#65292;&#24182;&#30740;&#31350;&#20102;&#20004;&#31181;&#23454;&#20363;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ProjUnit&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#39640;&#26031;&#28151;&#21512;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of locally private mean estimation of high-dimensional vectors in the Euclidean ball. Existing algorithms for this problem either incur sub-optimal error or have high communication and/or run-time complexity. We propose a new algorithmic framework, ProjUnit, for private mean estimation that yields algorithms that are computationally efficient, have low communication complexity, and incur optimal error up to a $1+o(1)$-factor. Our framework is deceptively simple: each randomizer projects its input to a random low-dimensional subspace, normalizes the result, and then runs an optimal algorithm such as PrivUnitG in the lower-dimensional space. In addition, we show that, by appropriately correlating the random projection matrices across devices, we can achieve fast server run-time. We mathematically analyze the error of the algorithm in terms of properties of the random projections, and study two instantiations. Lastly, our experiments for private mean estimation and pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#24322;&#36136;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;PEGFAN&#65292;&#23427;&#20351;&#29992;&#32622;&#25442;&#31561;&#21464;&#22270;&#26694;&#26550;&#23454;&#29616;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#65292;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#30456;&#23545;&#36739;&#22823;&#21644;&#23494;&#38598;&#36830;&#25509;&#30340;&#25968;&#25454;&#38598;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.04265</link><description>&lt;p&gt;
&#32622;&#25442;&#31561;&#21464;&#22270;&#26694;&#26550;&#22312;&#24322;&#36136;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Permutaion Equivariant Graph Framelets for Heterophilous Semi-supervised Learning. (arXiv:2306.04265v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#24322;&#36136;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;PEGFAN&#65292;&#23427;&#20351;&#29992;&#32622;&#25442;&#31561;&#21464;&#22270;&#26694;&#26550;&#23454;&#29616;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#25552;&#21462;&#65292;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#30456;&#23545;&#36739;&#22823;&#21644;&#23494;&#38598;&#36830;&#25509;&#30340;&#25968;&#25454;&#38598;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#36136;&#22270;&#30340;&#26412;&#36136;&#19982;&#21516;&#36136;&#22270;&#26174;&#33879;&#19981;&#21516;&#65292;&#36825;&#34920;&#26126;1-hop&#20197;&#22806;&#30340;&#32858;&#21512;&#26041;&#24335;&#24182;&#24341;&#36215;&#26089;&#26399;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22256;&#38590;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#23610;&#24230;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#32622;&#25442;&#31561;&#21464;&#24615;&#65292;&#39640;&#25928;&#24615;&#21644;&#31232;&#30095;&#24615;&#30340;Haar-type&#22270;&#26694;&#26550;&#65292;&#22312;&#22270;&#19978;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#23454;&#29616;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#25105;&#20204;&#26500;&#24314;&#30340;&#22270;&#26694;&#26550;&#35774;&#35745;&#20102;&#22270;&#26694;&#26550;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;PEGFAN&#12290;&#23454;&#39564;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;9&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26576;&#20123;&#24322;&#36136;&#22270;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#30456;&#23545;&#36739;&#22823;&#21644;&#26356;&#23494;&#38598;&#30340;&#36830;&#25509;&#30340;&#22823;&#37096;&#20998;&#24322;&#36136;&#25968;&#25454;&#38598;&#65289;&#19978;&#21487;&#20197;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#22312;&#20854;&#20313;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The nature of heterophilous graphs is significantly different with that of homophilous graphs, which suggests aggregations beyond 1-hop neighborhood and causes difficulties in early graph neural network models. In this paper, we develop a new way to implement multi-scale extraction via constructing Haar-type graph framelets with desired properties of permutation equivariance, efficiency, and sparsity, for deep learning tasks on graphs. We further deisgn a graph framelet neural network model PEGFAN using our constructed graph framelets. The experiments are conducted on a synthetic dataset and 9 benchmark datasets to compare performance with other state-of-the-art models. The result shows that our model can achieve best performance on certain datasets of heterophilous graphs (including the majority of heterophilous datasets with relatively larger sizes and denser connections) and competitive performance on the remaining.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#39044;&#27979;&#21644;&#26080;&#25439;&#21387;&#32553;&#26041;&#26696;&#30340;&#33521;&#25991;&#25991;&#26412;&#21387;&#32553;&#31639;&#27861;&#65292;&#24182;&#22312;&#21021;&#27493;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21387;&#32553;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.04050</link><description>&lt;p&gt;
LLMZip&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#25439;&#25991;&#26412;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
LLMZip: Lossless Text Compression using Large Language Models. (arXiv:2306.04050v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#39044;&#27979;&#21644;&#26080;&#25439;&#21387;&#32553;&#26041;&#26696;&#30340;&#33521;&#25991;&#25991;&#26412;&#21387;&#32553;&#31639;&#27861;&#65292;&#24182;&#22312;&#21021;&#27493;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21387;&#32553;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLaMA-7B&#23545;&#33521;&#35821;&#29109;&#30340;&#28176;&#36817;&#19978;&#30028;&#25552;&#20986;&#20102;&#26032;&#20272;&#35745;&#20540;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#19982;&#26080;&#25439;&#21387;&#32553;&#26041;&#26696;&#30340;&#33521;&#25991;&#25991;&#26412;&#21387;&#32553;&#31639;&#27861;&#12290;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21387;&#32553;&#26041;&#26696;&#65292;&#22914;BSC&#12289;ZPAQ&#21644;paq8h&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B as a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available estimates in \cite{cover1978convergent}, \cite{lutati2023focus}. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction from the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our scheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#27492;&#30740;&#31350;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.17760</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;
&lt;/p&gt;
&lt;p&gt;
Language Models are Bounded Pragmatic Speakers. (arXiv:2305.17760v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#27492;&#30740;&#31350;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Ouyang&#31561;&#20154;&#65292;2022&#65289;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#65288;Kahneman&#65292;2011&#65289;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24515;&#29702;&#23398;&#23478;&#20204;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#25193;&#23637;&#36825;&#20010;&#26694;&#26550;&#30340;&#36884;&#24452;&#12290;&#26412;&#30740;&#31350;&#23454;&#36136;&#19978;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#26469;&#33719;&#24471;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#26041;&#38754;&#30340;&#28145;&#21051;&#35265;&#35299;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do language models "think"? This paper formulates a probabilistic cognitive model called the bounded pragmatic speaker, which can characterize the operation of different variations of language models. Specifically, we demonstrate that large language models fine-tuned with reinforcement learning from human feedback (Ouyang et al., 2022) embody a model of thought that conceptually resembles a fast-and-slow model (Kahneman, 2011), which psychologists have attributed to humans. We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework. In essence, our research highlights the value of adopting a cognitive probabilistic modeling approach to gain insights into the comprehension, evaluation, and advancement of language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19982;&#31070;&#32463;&#32593;&#32476;&#23436;&#20840;&#23545;&#24212;&#30340;&#26080;&#38480;&#26641;&#29366;PGMs&#26469;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#32570;&#20047;PGMs&#30340;&#31934;&#30830;&#35821;&#20041;&#21644;&#26126;&#30830;&#23450;&#20041;&#30340;&#27010;&#29575;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;DNNs&#22312;&#21069;&#21521;&#20256;&#25773;&#26102;&#30830;&#23454;&#25191;&#34892;PGM&#25512;&#26029;&#30340;&#36817;&#20284;&#65292;&#36825;&#19982;&#29616;&#26377;&#30740;&#31350;&#19981;&#21516;&#65292;&#23427;&#38416;&#26126;&#20102;DNNs&#23545;PGMs&#20013;&#30340;&#31934;&#30830;&#25512;&#29702;&#30340;&#26356;&#30452;&#25509;&#36817;&#20284;&#65292;&#28508;&#22312;&#30340;&#22909;&#22788;&#21253;&#25324;&#25913;&#36827;DNNs&#30340;&#25945;&#23398;&#21644;&#35299;&#37322;&#65292;&#20197;&#21450;&#33021;&#22815;&#21512;&#24182;PGMs&#21644;DNNs&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.17583</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#26080;&#38480;&#26641;&#29366;&#27010;&#29575;&#22270;&#27169;&#22411;&#30340;&#35770;&#25991;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Neural Networks as Infinite Tree-Structured Probabilistic Graphical Models. (arXiv:2305.17583v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19982;&#31070;&#32463;&#32593;&#32476;&#23436;&#20840;&#23545;&#24212;&#30340;&#26080;&#38480;&#26641;&#29366;PGMs&#26469;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#32570;&#20047;PGMs&#30340;&#31934;&#30830;&#35821;&#20041;&#21644;&#26126;&#30830;&#23450;&#20041;&#30340;&#27010;&#29575;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;DNNs&#22312;&#21069;&#21521;&#20256;&#25773;&#26102;&#30830;&#23454;&#25191;&#34892;PGM&#25512;&#26029;&#30340;&#36817;&#20284;&#65292;&#36825;&#19982;&#29616;&#26377;&#30740;&#31350;&#19981;&#21516;&#65292;&#23427;&#38416;&#26126;&#20102;DNNs&#23545;PGMs&#20013;&#30340;&#31934;&#30830;&#25512;&#29702;&#30340;&#26356;&#30452;&#25509;&#36817;&#20284;&#65292;&#28508;&#22312;&#30340;&#22909;&#22788;&#21253;&#25324;&#25913;&#36827;DNNs&#30340;&#25945;&#23398;&#21644;&#35299;&#37322;&#65292;&#20197;&#21450;&#33021;&#22815;&#21512;&#24182;PGMs&#21644;DNNs&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#32570;&#20047;&#27010;&#29575;&#22270;&#27169;&#22411;(PGMs)&#30340;&#31934;&#30830;&#35821;&#20041;&#21644;&#26126;&#30830;&#23450;&#20041;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19982;&#31070;&#32463;&#32593;&#32476;&#23436;&#20840;&#23545;&#24212;&#30340;&#26080;&#38480;&#26641;&#29366;PGMs&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;DNNs&#22312;&#21069;&#21521;&#20256;&#25773;&#26399;&#38388;&#30830;&#23454;&#25191;&#34892;PGM&#25512;&#26029;&#30340;&#36817;&#20284;&#65292;&#36825;&#19982;&#26366;&#32463;&#30340;&#31070;&#32463;&#32593;&#32476;&#25551;&#36848;&#20026;&#26680;&#26426;&#22120;&#25110;&#26080;&#38480;&#22823;&#23567;&#30340;&#39640;&#26031;&#36807;&#31243;&#30340;&#29616;&#26377;&#30740;&#31350;&#19981;&#21516;&#65292;&#23427;&#38416;&#26126;&#20102;DNNs&#23545;PGMs&#20013;&#30340;&#31934;&#30830;&#25512;&#29702;&#30340;&#26356;&#30452;&#25509;&#36817;&#20284;&#12290;&#28508;&#22312;&#30340;&#22909;&#22788;&#21253;&#25324;&#25913;&#36827;DNNs&#30340;&#25945;&#23398;&#21644;&#35299;&#37322;&#65292;&#20197;&#21450;&#33021;&#22815;&#21512;&#24182;PGMs&#21644;DNNs&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) lack the precise semantics and definitive probabilistic interpretation of probabilistic graphical models (PGMs). In this paper, we propose an innovative solution by constructing infinite tree-structured PGMs that correspond exactly to neural networks. Our research reveals that DNNs, during forward propagation, indeed perform approximations of PGM inference that are precise in this alternative PGM structure. Not only does our research complement existing studies that describe neural networks as kernel machines or infinite-sized Gaussian processes, it also elucidates a more direct approximation that DNNs make to exact inference in PGMs. Potential benefits include improved pedagogy and interpretation of DNNs, and algorithms that can merge the strengths of PGMs and DNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#19978;&#19979;&#25991;&#27169;&#22411;&#30340;Levin&#26641;&#25628;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#26367;&#25442;&#20026;&#19978;&#19979;&#25991;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;LTS&#25439;&#22833;&#30340;&#20984;&#20248;&#21270;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#20248;&#20110;LTS+NN&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16945</link><description>&lt;p&gt;
&#20855;&#26377;&#19978;&#19979;&#25991;&#27169;&#22411;&#30340;Levin&#26641;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Levin Tree Search with Context Models. (arXiv:2305.16945v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#19978;&#19979;&#25991;&#27169;&#22411;&#30340;Levin&#26641;&#25628;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#26367;&#25442;&#20026;&#19978;&#19979;&#25991;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;LTS&#25439;&#22833;&#30340;&#20984;&#20248;&#21270;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#20248;&#20110;LTS+NN&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Levin Tree Search&#65288;LTS&#65289;&#26159;&#19968;&#31181;&#21033;&#29992;&#31574;&#30053;&#65288;&#21160;&#20316;&#30340;&#27010;&#29575;&#20998;&#24067;&#65289;&#30340;&#25628;&#32034;&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#20851;&#20110;&#36798;&#21040;&#30446;&#26631;&#33410;&#28857;&#20043;&#21069;&#25193;&#23637;&#27425;&#25968;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#36825;&#21462;&#20915;&#20110;&#31574;&#30053;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#20445;&#35777;&#31216;&#20026;LTS&#25439;&#22833;&#65292;&#21487;&#20197;&#23558;&#20854;&#20316;&#20026;&#20248;&#21270;&#34920;&#31034;&#31574;&#30053;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LTS+NN&#65289;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#29992;&#22312;&#32447;&#21387;&#32553;&#25991;&#29486;&#20013;&#30340;&#21442;&#25968;&#21270;&#19978;&#19979;&#25991;&#27169;&#22411;&#26469;&#26367;&#20195;&#65288;LTS+CM&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36825;&#31181;&#26032;&#27169;&#22411;&#19979;LTS&#25439;&#22833;&#26159;&#20984;&#30340;&#65292;&#20174;&#32780;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#20984;&#20248;&#21270;&#24037;&#20855;&#65292;&#24182;&#19988;&#23545;&#20110;&#32473;&#23450;&#30340;&#35299;&#36712;&#36857;&#38598;&#21512;&#65292;&#22312;&#22312;&#32447;&#35774;&#32622;&#20013;&#21487;&#20197;&#33719;&#24471;&#21040;&#26368;&#20248;&#21442;&#25968;&#30340;&#25910;&#25947;&#20445;&#35777;&#8212;&#8212;&#32780;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#25552;&#20379;&#36825;&#26679;&#30340;&#20445;&#35777;&#12290;&#26032;&#30340;LTS+CM&#31639;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;LTS+NN&#30456;&#27604;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65306;Sokoban&#65288;Boxoban&#65289;&#12289;The Witness&#21644;24-Sliding Tile Puzzle&#65288;STP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Levin Tree Search (LTS) is a search algorithm that makes use of a policy (a probability distribution over actions) and comes with a theoretical guarantee on the number of expansions before reaching a goal node, depending on the quality of the policy. This guarantee can be used as a loss function, which we call the LTS loss, to optimize neural networks representing the policy (LTS+NN). In this work we show that the neural network can be substituted with parameterized context models originating from the online compression literature (LTS+CM). We show that the LTS loss is convex under this new model, which allows for using standard convex optimization tools, and obtain convergence guarantees to the optimal parameters in an online setting for a given set of solution trajectories -- guarantees that cannot be provided for neural networks. The new LTS+CM algorithm compares favorably against LTS+NN on several benchmarks: Sokoban (Boxoban), The Witness, and the 24-Sliding Tile puzzle (STP). The
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#21487;&#22797;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26159;&#25511;&#21046;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#27491;&#24335;&#30340;&#21487;&#22797;&#21046;&#24615;&#32467;&#26524;</title><link>http://arxiv.org/abs/2305.15284</link><description>&lt;p&gt;
&#21487;&#22797;&#29616;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Replicable Reinforcement Learning. (arXiv:2305.15284v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#21487;&#22797;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26159;&#25511;&#21046;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#27491;&#24335;&#30340;&#21487;&#22797;&#21046;&#24615;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20250;&#12289;&#34892;&#20026;&#21644;&#25968;&#25454;&#31185;&#23398;&#20013;&#65292;&#21487;&#37325;&#22797;&#24615;&#21361;&#26426;&#23548;&#33268;&#20102;&#31639;&#27861;&#26694;&#26550;&#30340;&#24418;&#25104;&#65292;&#21363;&#35201;&#27714;&#31639;&#27861;&#22312;&#20174;&#30456;&#21516;&#30340;&#24213;&#23618;&#20998;&#24067;&#25552;&#21462;&#30340;&#20004;&#20010;&#19981;&#21516;&#26679;&#26412;&#19978;&#36816;&#34892;&#26102;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#65288;&#27010;&#29575;&#39640;&#65289;&#12290;&#34429;&#28982;&#20173;&#22788;&#20110;&#21021;&#26399;&#38454;&#27573;&#65292;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#30340;&#35768;&#22810;&#22522;&#26412;&#20219;&#21153;&#65292;&#21253;&#25324;&#32479;&#35745;&#26597;&#35810;&#23398;&#20064;&#12289;&#37325;&#35201;&#39033;&#38382;&#39064;&#21644;&#20998;&#24067;&#27979;&#35797;&#65292;&#37117;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#21487;&#35777;&#26126;&#21487;&#22797;&#29616;&#31639;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#21487;&#22797;&#29616;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#20102;&#24182;&#34892;&#20540;&#36845;&#20195;&#30340;&#21487;&#35777;&#22797;&#21046;&#31639;&#27861;&#20197;&#21450;&#19968;&#20010;&#22312;&#36830;&#32493;&#35774;&#32622;&#20013;&#21487;&#35777;&#22797;&#21046;&#30340;R-max&#12290;&#36825;&#26159;&#25511;&#21046;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#27491;&#24335;&#21487;&#22797;&#21046;&#24615;&#32467;&#26524;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#25209;&#37327;&#23398;&#20064;&#29615;&#22659;&#20013;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#22797;&#21046;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The replicability crisis in the social, behavioral, and data sciences has led to the formulation of algorithm frameworks for replicability -- i.e., a requirement that an algorithm produce identical outputs (with high probability) when run on two different samples from the same underlying distribution. While still in its infancy, provably replicable algorithms have been developed for many fundamental tasks in machine learning and statistics, including statistical query learning, the heavy hitters problem, and distribution testing. In this work we initiate the study of replicable reinforcement learning, providing a provably replicable algorithm for parallel value iteration, and a provably replicable version of R-max in the episodic setting. These are the first formal replicability results for control problems, which present different challenges for replication than batch learning settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DistroFair&#30340;&#20998;&#24067;&#24863;&#30693;&#30340;&#20844;&#24179;&#24615;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#26816;&#27979;&#21040;&#31867;&#32423;&#21035;&#30340;&#20844;&#24179;&#24615;&#36829;&#35268;&#12290;</title><link>http://arxiv.org/abs/2305.13935</link><description>&lt;p&gt;
&#20998;&#24067;&#24863;&#30693;&#30340;&#20844;&#24179;&#24615;&#27979;&#35797;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Distribution-aware Fairness Test Generation. (arXiv:2305.13935v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DistroFair&#30340;&#20998;&#24067;&#24863;&#30693;&#30340;&#20844;&#24179;&#24615;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#26816;&#27979;&#21040;&#31867;&#32423;&#21035;&#30340;&#20844;&#24179;&#24615;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#39564;&#35777;&#22270;&#20687;&#35782;&#21035;&#36719;&#20214;&#20013;&#30340;&#32452;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#30340;&#20844;&#24179;&#24615;&#27979;&#35797;&#26041;&#27861;&#65288;&#31216;&#20026;DistroFair&#65289;&#65292;&#36890;&#36807;&#23558;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#23545;&#35937;&#24341;&#20837;&#21040;&#22270;&#20687;&#35782;&#21035;&#22120;&#20013;&#65292;&#36890;&#36807;&#19977;&#31181;&#35821;&#20041;&#20445;&#30041;&#22270;&#20687;&#21464;&#25442; - &#23545;&#35937;&#21024;&#38500;&#65292;&#23545;&#35937;&#25554;&#20837;&#21644;&#23545;&#35937;&#26059;&#36716;&#26469;&#31995;&#32479;&#24615;&#22320;&#26292;&#38706;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#30340;&#31867;&#32423;&#21035;&#20844;&#24179;&#24615;&#36829;&#35268;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#30693;&#21517;&#25968;&#25454;&#38598;&#65288;CityScapes&#21644;MS-COCO&#65289;&#21644;&#19977;&#20010;&#20027;&#35201;&#30340;&#21830;&#19994;&#22270;&#20687;&#35782;&#21035;&#36719;&#20214;&#65288;&#21363;Amazon Rekognition&#65292;Google Cloud Vision&#21644;Azure&#35745;&#31639;&#26426;&#35270;&#35273;&#65289;&#23545;DistroFair&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;DistroFair&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#65292;&#32422;&#26377;21&#65285;&#36890;&#36807;&#30495;&#23454;&#26631;&#20934;&#25110;&#20803;&#27979;&#35797;&#26631;&#20934;&#26174;&#38706;&#20986;&#20102;&#31867;&#32423;&#21035;&#30340;&#20844;&#24179;&#24615;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses how to validate group fairness in image recognition software. We propose a distribution-aware fairness testing approach (called DistroFair) that systematically exposes class-level fairness violations in image classifiers via a synergistic combination of out-of-distribution (OOD) testing and semantic-preserving image mutation. DistroFair automatically learns the distribution (e.g., number/orientation) of objects in a set of images. Then it systematically mutates objects in the images to become OOD using three semantic-preserving image mutations -- object deletion, object insertion and object rotation. We evaluate DistroFair using two well-known datasets (CityScapes and MS-COCO) and three major, commercial image recognition software (namely, Amazon Rekognition, Google Cloud Vision and Azure Computer Vision). Results show that about 21% of images generated by DistroFair reveal class-level fairness violations using either ground truth or metamorphic oracles. DistroFair 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24402;&#19968;&#21270;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#22312;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#25351;&#25968;&#23614;&#37096;&#25439;&#22833;&#20989;&#25968;&#65292;&#20854;&#25910;&#25947;&#36895;&#29575;&#20026;&#32447;&#24615;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#26377;&#38480;&#26102;&#38388;&#30340;&#27867;&#21270;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2305.13471</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#20998;&#25968;&#25454;&#30340;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Fast Convergence in Learning Two-Layer Neural Networks with Separable Data. (arXiv:2305.13471v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24402;&#19968;&#21270;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#22312;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#25351;&#25968;&#23614;&#37096;&#25439;&#22833;&#20989;&#25968;&#65292;&#20854;&#25910;&#25947;&#36895;&#29575;&#20026;&#32447;&#24615;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#26377;&#38480;&#26102;&#38388;&#30340;&#27867;&#21270;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#21487;&#20998;&#25968;&#25454;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#19978;&#65292;&#24402;&#19968;&#21270;&#26799;&#24230;&#19979;&#38477;&#22312;&#21152;&#36895;&#25351;&#25968;&#23614;&#37096;&#25439;&#22833;&#20989;&#25968;&#65288;&#21253;&#25324;&#25351;&#25968;&#21644;&#36923;&#36753;&#25439;&#22833;&#65289;&#25910;&#25947;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#25104;&#21151;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#24402;&#19968;&#21270; GD &#23545;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20316;&#29992;&#36229;&#36234;&#20102;&#32447;&#24615;&#27169;&#22411;&#12290;&#23545;&#20110;&#25351;&#25968;&#23614;&#37096;&#25439;&#22833;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#24402;&#19968;&#21270; GD &#23548;&#33268;&#35757;&#32451;&#25439;&#22833;&#23545;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#32447;&#24615;&#25910;&#25947;&#36895;&#29575;&#12290;&#36825;&#26159;&#36890;&#36807;&#23637;&#31034;&#19968;&#23450;&#30340;&#26799;&#24230;&#33258;&#38480;&#21046;&#26465;&#20214;&#21644;&#23545;&#25968;&#21033;&#26222;&#24076;&#33576;&#29305;&#24615;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#31639;&#27861;&#31283;&#23450;&#24615;&#20998;&#26512;&#30740;&#31350;&#20102;&#29992;&#20110;&#20984;&#30446;&#26631;&#30340;&#24402;&#19968;&#21270; GD &#30340;&#27867;&#21270;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#26377;&#38480;&#26102;&#38388;&#30340;&#27867;&#21270;&#36793;&#30028;&#35777;&#26126;&#20102;&#35757;&#32451;&#26399;&#38388;&#24402;&#19968;&#21270; GD &#19981;&#20250;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalized gradient descent has shown substantial success in speeding up the convergence of exponentially-tailed loss functions (which includes exponential and logistic losses) on linear classifiers with separable data. In this paper, we go beyond linear models by studying normalized GD on two-layer neural nets. We prove for exponentially-tailed losses that using normalized GD leads to linear rate of convergence of the training loss to the global optimum. This is made possible by showing certain gradient self-boundedness conditions and a log-Lipschitzness property. We also study generalization of normalized GD for convex objectives via an algorithmic-stability analysis. In particular, we show that normalized GD does not overfit during training by establishing finite-time generalization bounds.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22797;&#21046;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#32593;&#32476;&#65288;CRNNet&#65289;&#30340;&#26032;&#22411;&#31895;&#21040;&#32454;&#30340;ICD&#36335;&#24452;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;RNN&#29983;&#25104;&#39034;&#24207;&#36755;&#20986;&#24182;&#32467;&#21512;&#22797;&#21046;&#27169;&#22359;&#65292;&#26377;&#25928;&#35782;&#21035;&#22797;&#26434;&#30142;&#30149;&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#21644;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#22312;&#39044;&#27979;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22797;&#26434;&#30142;&#30149;&#27604;&#29575;&#65288;57.30%&#65289;&#12290;</title><link>http://arxiv.org/abs/2305.13250</link><description>&lt;p&gt;
&#22797;&#21046;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Copy Recurrent Neural Network Structure Network. (arXiv:2305.13250v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22797;&#21046;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#32593;&#32476;&#65288;CRNNet&#65289;&#30340;&#26032;&#22411;&#31895;&#21040;&#32454;&#30340;ICD&#36335;&#24452;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;RNN&#29983;&#25104;&#39034;&#24207;&#36755;&#20986;&#24182;&#32467;&#21512;&#22797;&#21046;&#27169;&#22359;&#65292;&#26377;&#25928;&#35782;&#21035;&#22797;&#26434;&#30142;&#30149;&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#21644;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#22312;&#39044;&#27979;&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22797;&#26434;&#30142;&#30149;&#27604;&#29575;&#65288;57.30%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#32534;&#30721;&#28041;&#21450;&#23558;EHR&#33258;&#21160;&#20998;&#31867;&#20026;&#35786;&#26029;&#20195;&#30721;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#23558;&#20854;&#35270;&#20026;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#65292;&#29983;&#25104;&#27599;&#20010;&#20195;&#30721;&#30340;&#27010;&#29575;&#24182;&#36873;&#25321;&#36229;&#36807;&#19968;&#23450;&#38408;&#20540;&#30340;&#26631;&#31614;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#24573;&#35270;&#20102;&#35782;&#21035;&#22797;&#26434;&#30142;&#30149;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#22312;EHR&#20013;&#26816;&#27979;&#24182;&#21457;&#30142;&#30149;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22797;&#21046;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#32593;&#32476;&#65288;CRNNet&#65289;&#30340;&#26032;&#22411;&#31895;&#21040;&#32454;&#30340;ICD&#36335;&#24452;&#29983;&#25104;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#36335;&#24452;&#29983;&#25104;&#22120;&#65288;PG&#65289;&#21644;&#36335;&#24452;&#37492;&#21035;&#22120;&#65288;PD&#65289;&#36827;&#34892;EHR&#32534;&#30721;&#12290;&#36890;&#36807;&#20351;&#29992;RNN&#29983;&#25104;&#39034;&#24207;&#36755;&#20986;&#24182;&#32467;&#21512;&#22797;&#21046;&#27169;&#22359;&#65292;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#22797;&#26434;&#30142;&#30149;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#20013;&#23454;&#29616;&#20102;57.30&#65285;&#30340;&#22797;&#26434;&#30142;&#30149;&#27604;&#29575;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21644;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#22797;&#21046;&#26426;&#21046;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Record (EHR) coding involves automatically classifying EHRs into diagnostic codes. While most previous research treats this as a multi-label classification task, generating probabilities for each code and selecting those above a certain threshold as labels, these approaches often overlook the challenge of identifying complex diseases. In this study, our focus is on detecting complication diseases within EHRs.  We propose a novel coarse-to-fine ICD path generation framework called the Copy Recurrent Neural Network Structure Network (CRNNet), which employs a Path Generator (PG) and a Path Discriminator (PD) for EHR coding. By using RNNs to generate sequential outputs and incorporating a copy module, we efficiently identify complication diseases. Our method achieves a 57.30\% ratio of complex diseases in predictions, outperforming state-of-the-art and previous approaches.  Additionally, through an ablation study, we demonstrate that the copy mechanism plays a crucial rol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#37325;&#26032;&#21152;&#26435;&#19982;&#26679;&#26412;&#20851;&#32852;&#27979;&#35797;&#65288;Re-SAT&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#22833;&#35821;&#30151;&#24739;&#32773;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#22312;&#19981;&#24433;&#21709;&#20581;&#24247;&#24739;&#32773;&#35821;&#38899;&#30340;ASR&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;ASR&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13108</link><description>&lt;p&gt;
&#36890;&#36807;&#26679;&#26412;&#37325;&#26032;&#21152;&#26435;&#19982;&#26679;&#26412;&#20851;&#32852;&#27979;&#35797;&#36827;&#34892;&#22833;&#35821;&#30151;&#35821;&#38899;&#30340;&#26080;&#20559;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Debiased Automatic Speech Recognition for Dysarthric Speech via Sample Reweighting with Sample Affinity Test. (arXiv:2305.13108v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#37325;&#26032;&#21152;&#26435;&#19982;&#26679;&#26412;&#20851;&#32852;&#27979;&#35797;&#65288;Re-SAT&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#22833;&#35821;&#30151;&#24739;&#32773;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#22312;&#19981;&#24433;&#21709;&#20581;&#24247;&#24739;&#32773;&#35821;&#38899;&#30340;ASR&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;ASR&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20027;&#35201;&#26159;&#36890;&#36807;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#30001;&#20110;ERM&#21033;&#29992;&#25968;&#25454;&#26679;&#26412;&#30340;&#24179;&#22343;&#34920;&#29616;&#32780;&#19981;&#32771;&#34385;&#19968;&#20010;&#32676;&#20307;&#65292;&#20363;&#22914;&#20581;&#24247;&#25110;&#22833;&#35821;&#30151;&#24739;&#32773;&#65292;&#22240;&#27492;ASR&#31995;&#32479;&#26080;&#27861;&#35782;&#21035;&#36328;&#32676;&#20307;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#23548;&#33268;ASR&#31995;&#32479;&#23384;&#22312;&#20559;&#24046;&#19988;&#20854;&#32676;&#20307;&#24615;&#33021;&#24046;&#24322;&#20005;&#37325;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#32676;&#20307;&#31283;&#20581;&#24615;&#65292;&#38024;&#23545;&#22833;&#35821;&#30151;&#24739;&#32773;&#36827;&#34892;&#25913;&#36827;&#12290;&#20026;&#20102;&#23454;&#29616;&#25105;&#20204;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#26679;&#26412;&#37325;&#26032;&#21152;&#26435;&#19982;&#26679;&#26412;&#20851;&#32852;&#27979;&#35797;&#65288;Re-SAT&#65289;&#12290; Re-SAT&#31995;&#32479;&#22320;&#34913;&#37327;&#25152;&#32473;&#25968;&#25454;&#26679;&#26412;&#30340;&#21435;&#20559;&#24110;&#21161;&#24615;&#65292;&#24182;&#36890;&#36807;&#21435;&#20559;&#24110;&#21161;&#24615;&#21152;&#26435;&#26469;&#32531;&#35299;&#20559;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292; Re-SAT&#26377;&#21161;&#20110;&#25913;&#21892;&#22833;&#35821;&#30151;&#35821;&#38899;&#30340;ASR&#24615;&#33021;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#20581;&#24247;&#35821;&#38899;&#30340;ASR&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition systems based on deep learning are mainly trained under empirical risk minimization (ERM). Since ERM utilizes the averaged performance on the data samples regardless of a group such as healthy or dysarthric speakers, ASR systems are unaware of the performance disparities across the groups. This results in biased ASR systems whose performance differences among groups are severe. In this study, we aim to improve the ASR system in terms of group robustness for dysarthric speakers. To achieve our goal, we present a novel approach, sample reweighting with sample affinity test (Re-SAT). Re-SAT systematically measures the debiasing helpfulness of the given data sample and then mitigates the bias by debiasing helpfulness-based sample reweighting. Experimental results demonstrate that Re-SAT contributes to improved ASR performance on dysarthric speech without performance degradation on healthy speech.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HDR&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#21382;&#21490;&#20419;&#38144;&#25968;&#25454;&#65292;&#26469;&#25429;&#25417;&#20419;&#38144;&#36716;&#21270;&#27169;&#24335;&#65292;&#36798;&#21040;&#26356;&#22909;&#22320;&#36866;&#24212;&#20419;&#38144;&#27169;&#24335;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.12837</link><description>&lt;p&gt;
&#25429;&#25417;&#20419;&#38144;&#26399;&#38388;&#30340;&#36716;&#21270;&#29575;&#27874;&#21160;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#21382;&#21490;&#25968;&#25454;&#20877;&#21033;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Capturing Conversion Rate Fluctuation during Sales Promotions: A Novel Historical Data Reuse Approach. (arXiv:2305.12837v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HDR&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#21382;&#21490;&#20419;&#38144;&#25968;&#25454;&#65292;&#26469;&#25429;&#25417;&#20419;&#38144;&#36716;&#21270;&#27169;&#24335;&#65292;&#36798;&#21040;&#26356;&#22909;&#22320;&#36866;&#24212;&#20419;&#38144;&#27169;&#24335;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#21270;&#29575;&#65288;CVR&#65289;&#39044;&#27979;&#26159;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#30340;&#26680;&#24515;&#32452;&#20214;&#20043;&#19968;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#20197;&#33719;&#24471;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;CVR&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21363;&#20351;&#35757;&#32451;&#33391;&#22909;&#30340;CVR&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#20419;&#38144;&#26399;&#38388;&#20063;&#32463;&#24120;&#34920;&#29616;&#20986;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#25968;&#25454;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#20854;&#20013;&#20256;&#32479;&#26041;&#27861;&#19981;&#20877;&#36215;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23547;&#27714;&#24320;&#21457;&#26367;&#20195;&#24314;&#27169;&#25216;&#26415;&#29992;&#20110;CVR&#39044;&#27979;&#12290;&#35266;&#23519;&#21040;&#19981;&#21516;&#20419;&#38144;&#20043;&#38388;&#23384;&#22312;&#30456;&#20284;&#30340;&#36141;&#20080;&#27169;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37325;&#29992;&#21382;&#21490;&#20419;&#38144;&#25968;&#25454;&#20197;&#25429;&#25417;&#20419;&#38144;&#36716;&#21270;&#27169;&#24335;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21382;&#21490;&#25968;&#25454;&#20877;&#21033;&#29992;&#65288;HDR&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#26816;&#32034;&#21382;&#21490;&#19978;&#30456;&#20284;&#30340;&#20419;&#38144;&#25968;&#25454;&#65292;&#28982;&#21518;&#20351;&#29992;&#33719;&#21462;&#30340;&#25968;&#25454;&#24494;&#35843;CVR&#39044;&#27979;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#20419;&#38144;&#27169;&#24335;&#12290;HDR&#30001;&#19977;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#33258;&#21160;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Conversion rate (CVR) prediction is one of the core components in online recommender systems, and various approaches have been proposed to obtain accurate and well-calibrated CVR estimation. However, we observe that a well-trained CVR prediction model often performs sub-optimally during sales promotions. This can be largely ascribed to the problem of the data distribution shift, in which the conventional methods no longer work. To this end, we seek to develop alternative modeling techniques for CVR prediction. Observing similar purchase patterns across different promotions, we propose reusing the historical promotion data to capture the promotional conversion patterns. Herein, we propose a novel \textbf{H}istorical \textbf{D}ata \textbf{R}euse (\textbf{HDR}) approach that first retrieves historically similar promotion data and then fine-tunes the CVR prediction model with the acquired data for better adaptation to the promotion mode. HDR consists of three components: an automated data 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#25311;&#27169;&#22411;&#65292;&#31216;&#20026;GNSTODE&#65292;&#36890;&#36807;&#21033;&#29992;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#25551;&#36848;&#20102;&#31890;&#23376;&#31995;&#32479;&#20013;&#19981;&#21516;&#26102;&#38388;&#21644;&#19981;&#21516;&#31354;&#38388;&#26465;&#20214;&#19979;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.12334</link><description>&lt;p&gt;
&#37319;&#29992;&#22270;&#31070;&#32463;ODE&#27169;&#22411;&#23454;&#29616;&#22797;&#26434;&#21160;&#24577;&#29289;&#29702;&#31995;&#32479;&#30340;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Towards Complex Dynamic Physics System Simulation with Graph Neural ODEs. (arXiv:2305.12334v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#25311;&#27169;&#22411;&#65292;&#31216;&#20026;GNSTODE&#65292;&#36890;&#36807;&#21033;&#29992;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#25551;&#36848;&#20102;&#31890;&#23376;&#31995;&#32479;&#20013;&#19981;&#21516;&#26102;&#38388;&#21644;&#19981;&#21516;&#31354;&#38388;&#26465;&#20214;&#19979;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#24456;&#24378;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#29702;&#35299;&#30495;&#23454;&#30340;&#29289;&#29702;&#19990;&#30028;&#65292;&#22240;&#27492;&#23398;&#20064;&#27169;&#25311;&#22797;&#26434;&#30340;&#31890;&#23376;&#31995;&#32479;&#26159;&#19968;&#20010;&#24456;&#26377;&#21069;&#36884;&#30340;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#29289;&#29702;&#19990;&#30028;&#30340;&#22797;&#26434;&#35268;&#24459;&#32473;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#25311;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#22914;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#20043;&#38388;&#30340;&#19981;&#21516;&#31354;&#38388;&#20381;&#36182;&#24615;&#20197;&#21450;&#19981;&#21516;&#26102;&#38388;&#25139;&#20043;&#38388;&#31890;&#23376;&#31995;&#32479;&#29366;&#24577;&#30340;&#19981;&#21516;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#36825;&#20123;&#22240;&#32032;&#20915;&#23450;&#20102;&#31890;&#23376;&#30340;&#30456;&#20114;&#20316;&#29992;&#34892;&#20026;&#21644;&#29289;&#29702;&#31995;&#32479;&#30340;&#28436;&#21270;&#27169;&#24335;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#25311;&#26041;&#27861;&#26080;&#27861;&#20805;&#20998;&#32771;&#34385;&#36825;&#20123;&#22797;&#26434;&#24615;&#65292;&#22240;&#27492;&#26080;&#27861;&#20135;&#29983;&#20196;&#20154;&#28385;&#24847;&#30340;&#27169;&#25311;&#32467;&#26524;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#22797;&#26434;&#30340;&#29289;&#29702;&#27861;&#21017;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#25311;&#27169;&#22411;&#8212;&#8212;&#20855;&#26377;&#26102;&#31354;&#24314;&#27169;&#30340;&#22270;&#31070;&#32463;ODE&#27169;&#22411;&#65288;GNSTODE&#65289;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#25551;&#36848;&#20102;&#31890;&#23376;&#31995;&#32479;&#20013;&#19981;&#21516;&#26102;&#38388;&#21644;&#19981;&#21516;&#31354;&#38388;&#26465;&#20214;&#19979;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The great learning ability of deep learning models facilitates us to comprehend the real physical world, making learning to simulate complicated particle systems a promising endeavour. However, the complex laws of the physical world pose significant challenges to the learning based simulations, such as the varying spatial dependencies between interacting particles and varying temporal dependencies between particle system states in different time stamps, which dominate particles' interacting behaviour and the physical systems' evolution patterns. Existing learning based simulation methods fail to fully account for the complexities, making them unable to yield satisfactory simulations. To better comprehend the complex physical laws, this paper proposes a novel learning based simulation model- Graph Networks with Spatial-Temporal neural Ordinary Equations (GNSTODE)- that characterizes the varying spatial and temporal dependencies in particle systems using a united end-to-end framework. Th
&lt;/p&gt;</description></item><item><title>Waymo&#24320;&#25918;&#27169;&#25311;&#20195;&#29702;&#25361;&#25112;&#36187;&#25552;&#20986;&#20351;&#29992;&#30495;&#23454;&#12289;&#20114;&#21160;&#30340;&#26234;&#33021;&#20307;&#20223;&#30495;&#20197;&#20419;&#36827;&#33258;&#21160;&#39550;&#39542;&#34892;&#20026;&#27169;&#22411;&#30340;&#35780;&#20272;&#21644;&#35757;&#32451;&#65292;&#26159;&#35813;&#39046;&#22495;&#30340;&#39318;&#20010;&#20844;&#24320;&#25361;&#25112;&#36187;&#65292;&#26088;&#22312;&#25512;&#21160;&#36924;&#30495;&#27169;&#25311;&#22120;&#30340;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.12032</link><description>&lt;p&gt;
Waymo&#24320;&#25918;&#27169;&#25311;&#20195;&#29702;&#25361;&#25112;&#36187;
&lt;/p&gt;
&lt;p&gt;
The Waymo Open Sim Agents Challenge. (arXiv:2305.12032v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12032
&lt;/p&gt;
&lt;p&gt;
Waymo&#24320;&#25918;&#27169;&#25311;&#20195;&#29702;&#25361;&#25112;&#36187;&#25552;&#20986;&#20351;&#29992;&#30495;&#23454;&#12289;&#20114;&#21160;&#30340;&#26234;&#33021;&#20307;&#20223;&#30495;&#20197;&#20419;&#36827;&#33258;&#21160;&#39550;&#39542;&#34892;&#20026;&#27169;&#22411;&#30340;&#35780;&#20272;&#21644;&#35757;&#32451;&#65292;&#26159;&#35813;&#39046;&#22495;&#30340;&#39318;&#20010;&#20844;&#24320;&#25361;&#25112;&#36187;&#65292;&#26088;&#22312;&#25512;&#21160;&#36924;&#30495;&#27169;&#25311;&#22120;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#20041;&#20102;Waymo&#24320;&#25918;&#27169;&#25311;&#20195;&#29702;&#25361;&#25112;&#36187;(WOSAC)&#12290;&#36890;&#36807;&#19982;&#30495;&#23454;&#12289;&#20114;&#21160;&#30340;&#26234;&#33021;&#20307;&#36827;&#34892;&#20223;&#30495;&#26159;&#33258;&#21160;&#39550;&#39542;&#36719;&#20214;&#24320;&#21457;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;WOSAC&#26159;&#31532;&#19968;&#20010;&#20844;&#24320;&#30340;&#25361;&#25112;&#36187;&#65292;&#26088;&#22312;&#35299;&#20915;&#35813;&#20219;&#21153;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#35813;&#25361;&#25112;&#30340;&#30446;&#26631;&#26159;&#28608;&#21457;&#35774;&#35745;&#36924;&#30495;&#27169;&#25311;&#22120;&#30340;&#20852;&#36259;&#65292;&#20197;&#29992;&#20110;&#35780;&#20272;&#21644;&#35757;&#32451;&#33258;&#21160;&#39550;&#39542;&#30340;&#34892;&#20026;&#27169;&#22411;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20960;&#31181;&#22522;&#20934;&#20223;&#30495;&#20195;&#29702;&#26041;&#27861;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we define the Waymo Open Sim Agents Challenge (WOSAC). Simulation with realistic, interactive agents represents a key task for autonomous vehicle software development. WOSAC is the first public challenge to tackle this task and propose corresponding metrics. The goal of the challenge is to stimulate the design of realistic simulators that can be used to evaluate and train a behavior model for autonomous driving. We outline our evaluation methodology and present preliminary results for a number of different baseline simulation agent methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#23398;&#20064;&#24230;&#37327;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#20316;&#65292;&#36890;&#36807;&#35757;&#32451;&#25342;&#21462;&#25104;&#21151;&#39044;&#27979;&#22120;&#21644;&#23398;&#20064;&#25342;&#21462;&#36136;&#37327;&#24230;&#37327;&#65292;&#23454;&#29616;&#20102;&#33021;&#22815;&#22823;&#35268;&#27169;&#37096;&#32626;&#30340;&#24378;&#21147;&#25235;&#25569;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.10272</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#24230;&#37327;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Package Manipulation via Learned Metrics of Pick Success. (arXiv:2305.10272v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#23398;&#20064;&#24230;&#37327;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#20316;&#65292;&#36890;&#36807;&#35757;&#32451;&#25342;&#21462;&#25104;&#21151;&#39044;&#27979;&#22120;&#21644;&#23398;&#20064;&#25342;&#21462;&#36136;&#37327;&#24230;&#37327;&#65292;&#23454;&#29616;&#20102;&#33021;&#22815;&#22823;&#35268;&#27169;&#37096;&#32626;&#30340;&#24378;&#21147;&#25235;&#25569;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#20179;&#20648;&#25805;&#20316;&#21487;&#20197;&#38477;&#20302;&#29289;&#27969;&#25104;&#26412;&#65292;&#26368;&#32456;&#38477;&#20302;&#28040;&#36153;&#21697;&#20215;&#26684;&#65292;&#25552;&#39640;&#20132;&#36135;&#36895;&#24230;&#65292;&#24182;&#22686;&#24378;&#23545;&#21171;&#21160;&#21147;&#27874;&#21160;&#30340;&#25269;&#25239;&#33021;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#33258;&#21160;&#21270;&#37325;&#22797;&#20219;&#21153;&#30340;&#20852;&#36259;&#22686;&#21152;&#65292;&#20294;&#22823;&#22810;&#25968;&#26159;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#12290;&#20174;&#26434;&#20081;&#30340;&#22534;&#22534;&#20013;&#25361;&#36873;&#29289;&#21697;&#31561;&#20219;&#21153;&#30452;&#21040;&#26368;&#36817;&#25165;&#21464;&#24471;&#36275;&#22815;&#24378;&#22823;&#65292;&#21487;&#20197;&#22312;&#26368;&#23567;&#20154;&#24037;&#24178;&#39044;&#19979;&#36827;&#34892;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20122;&#39532;&#36874;&#26426;&#22120;&#20154;&#30340;Robot Induction&#65288;Robin&#65289;&#32676;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#20316;&#65292;&#35813;&#32676;&#21033;&#29992;&#22312;&#23454;&#38469;&#29983;&#20135;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#25342;&#21462;&#25104;&#21151;&#39044;&#27979;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#31995;&#32479;&#22312;&#36229;&#36807;394K&#20010;&#25342;&#21462;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#23427;&#29992;&#20110;&#25226;&#27599;&#22825;&#39640;&#36798;5&#30334;&#19975;&#20010;&#21253;&#35065;&#36827;&#34892;&#20102;&#20998;&#31163;&#65292;&#26412;&#25991;&#30340;&#35780;&#20272;&#26399;&#38388;&#25805;&#20316;&#20102;&#36229;&#36807;2&#20159;&#20010;&#21253;&#35065;&#12290;&#24320;&#21457;&#30340;&#23398;&#20064;&#25342;&#21462;&#36136;&#37327;&#24230;&#37327;&#23454;&#26102;&#25490;&#21517;&#21508;&#31181;&#25342;&#21462;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#37319;&#29992;&#39640;&#25104;&#21151;&#29575;&#30340;&#24378;&#21147;&#25235;&#25569;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automating warehouse operations can reduce logistics overhead costs, ultimately driving down the final price for consumers, increasing the speed of delivery, and enhancing the resiliency to workforce fluctuations. The past few years have seen increased interest in automating such repeated tasks but mostly in controlled settings. Tasks such as picking objects from unstructured, cluttered piles have only recently become robust enough for large-scale deployment with minimal human intervention.  This paper demonstrates a large-scale package manipulation from unstructured piles in Amazon Robotics' Robot Induction (Robin) fleet, which utilizes a pick success predictor trained on real production data. Specifically, the system was trained on over 394K picks. It is used for singulating up to 5~million packages per day and has manipulated over 200~million packages during this paper's evaluation period.  The developed learned pick quality measure ranks various pick alternatives in real-time and p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Molecule-Morphology Contrastive Pretraining (MoCoP)&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#20998;&#23376;&#22270;&#24418;&#21644;&#32454;&#32990;&#24418;&#24577;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MoCoP&#21487;&#20197;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09790</link><description>&lt;p&gt;
&#20998;&#23376;&#24418;&#24577;&#23545;&#27604;&#39044;&#35757;&#32451;&#25552;&#39640;&#20998;&#23376;&#34920;&#31034;&#36801;&#31227;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Molecule-Morphology Contrastive Pretraining for Transferable Molecular Representation. (arXiv:2305.09790v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Molecule-Morphology Contrastive Pretraining (MoCoP)&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#20998;&#23376;&#22270;&#24418;&#21644;&#32454;&#32990;&#24418;&#24577;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MoCoP&#21487;&#20197;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#22522;&#20110;&#22270;&#20687;&#30340;&#20998;&#26512;&#25216;&#26415;&#22240;&#20854;&#22312;&#30446;&#26631;&#37492;&#23450;&#12289;&#20316;&#29992;&#26426;&#21046;&#25512;&#26029;&#21644;&#27979;&#23450;&#21457;&#23637;&#20013;&#30340;&#24212;&#29992;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#25216;&#26415;&#20135;&#29983;&#20102;&#22823;&#37327;&#32454;&#32990;&#24418;&#24577;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#24120;&#29992;&#20110;&#30740;&#31350;&#23567;&#20998;&#23376;&#24178;&#25200;&#29289;&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;Molecule-Morphology Contrastive Pretraining (MoCoP)&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20998;&#23376;&#22270;&#24418;&#21644;&#32454;&#32990;&#24418;&#24577;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#23558;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#25193;&#23637;&#21040;&#20102;&#25913;&#36827;&#37327;&#21270;&#32467;&#26500;-&#27963;&#24615;&#20851;&#31995; (QSAR) &#27169;&#22411;&#12290;&#20351;&#29992;&#26469;&#33258;JUMP-CP&#32852;&#30431;&#30340;&#25968;&#25454;&#65292;&#23558;MoCoP&#25193;&#23637;&#21040;&#20102;&#32422;100K&#30340;&#20998;&#23376;&#21644;&#32422;600K&#30340;&#24418;&#24577;&#25991;&#20214;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;MoCoP&#22312;ChEMBL20&#19978;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#22987;&#32456;&#25552;&#39640;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#30340;&#34920;&#29616;&#65292;&#19988;&#22312;&#20869;&#37096; GSK&#33647;&#20195;&#21160;&#21147;&#23398;&#25968;&#25454;&#21644;&#20854;&#20182;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-based profiling techniques have become increasingly popular over the past decade for their applications in target identification, mechanism-of-action inference, and assay development. These techniques have generated large datasets of cellular morphologies, which are typically used to investigate the effects of small molecule perturbagens. In this work, we extend the impact of such dataset to improving quantitative structure-activity relationship (QSAR) models by introducing Molecule-Morphology Contrastive Pretraining (MoCoP), a framework for learning multi-modal representation of molecular graphs and cellular morphologies. We scale MoCoP to approximately 100K molecules and 600K morphological profiles using data from the JUMP-CP Consortium and show that MoCoP consistently improves performances of graph neural networks (GNNs) on molecular property prediction tasks in ChEMBL20 across all dataset sizes. The pretrained GNNs are also evaluated on internal GSK pharmacokinetic data and s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#37327;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#28508;&#22312;&#31354;&#38388;&#20013;&#35821;&#20041;&#20449;&#24687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25581;&#31034;CNN&#23618;&#20869;&#35821;&#20041;&#20449;&#24687;&#30340;&#27969;&#21160;&#21644;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#19981;&#21516;&#32593;&#32476;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.07663</link><description>&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23450;&#37327;&#35821;&#20041;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Quantified Semantic Comparison of Convolutional Neural Networks. (arXiv:2305.07663v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#37327;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#28508;&#22312;&#31354;&#38388;&#20013;&#35821;&#20041;&#20449;&#24687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25581;&#31034;CNN&#23618;&#20869;&#35821;&#20041;&#20449;&#24687;&#30340;&#27969;&#21160;&#21644;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#19981;&#21516;&#32593;&#32476;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#24212;&#29992;&#22788;&#20110;&#39046;&#20808;&#22320;&#20301;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#24037;&#20316;&#21407;&#29702;&#21364;&#24456;&#38590;&#38416;&#26126;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#36825;&#31867;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#65292;&#27169;&#22411;&#36873;&#25321;&#36824;&#24212;&#32771;&#34385;&#20505;&#36873;&#27169;&#22411;&#22312;&#27169;&#22411;&#36879;&#26126;&#24615;&#26041;&#38754;&#22914;&#20309;&#34920;&#31034;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#37327;&#21270;CNN&#28508;&#22312;&#31354;&#38388;&#20013;&#35821;&#20041;&#20449;&#24687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#26088;&#22312;&#25581;&#31034;CNN&#23618;&#20869;&#35821;&#20041;&#20449;&#24687;&#30340;&#27969;&#21160;&#21644;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#19981;&#21516;&#32593;&#32476;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#31243;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#30340;&#33879;&#21517;&#25216;&#26415;&#20316;&#20026;&#22522;&#30784;&#65292;&#36825;&#20123;&#25216;&#26415;&#29992;&#20110;&#33719;&#24471;&#27599;&#20010;&#28508;&#22312;&#31354;&#38388;&#20013;&#35821;&#20041;&#27010;&#24565;&#30340;&#20840;&#23616;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22522;&#20110;&#23427;&#20204;&#22312;&#27979;&#35797;&#36755;&#20837;&#19978;&#30340;&#28608;&#27963;&#36827;&#34892;&#27604;&#36739;&#12290;&#26412;&#24037;&#20316;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#21644;&#20004;&#20010;&#19981;&#21516;&#33539;&#22260;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The state-of-the-art in convolutional neural networks (CNNs) for computer vision excels in performance, while remaining opaque. But due to safety regulations for safety-critical applications, like perception for automated driving, the choice of model should also take into account how candidate models represent semantic information for model transparency reasons. To tackle this yet unsolved problem, our work proposes two methods for quantifying the similarity between semantic information in CNN latent spaces. These allow insights into both the flow and similarity of semantic information within CNN layers, and into the degree of their similitude between different networks. As a basis, we use renown techniques from the field of explainable artificial intelligence (XAI), which are used to obtain global vector representations of semantic concepts in each latent space. These are compared with respect to their activation on test inputs. When applied to three diverse object detectors and two d
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20914;&#20987;&#21709;&#24212;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38899;&#39057;&#20998;&#31867;&#27169;&#22411;&#27867;&#21270;&#21040;&#26410;&#34987;&#35757;&#32451;&#35774;&#22791;&#19978;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.07499</link><description>&lt;p&gt;
&#22522;&#20110;&#20914;&#20987;&#21709;&#24212;&#22686;&#24378;&#30340;&#35774;&#22791;&#40065;&#26834;&#22768;&#23398;&#22330;&#26223;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Device-Robust Acoustic Scene Classification via Impulse Response Augmentation. (arXiv:2305.07499v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20914;&#20987;&#21709;&#24212;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38899;&#39057;&#20998;&#31867;&#27169;&#22411;&#27867;&#21270;&#21040;&#26410;&#34987;&#35757;&#32451;&#35774;&#22791;&#19978;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#38899;&#39057;&#20998;&#31867;&#27169;&#22411;&#32780;&#35328;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#21508;&#31181;&#24405;&#38899;&#35774;&#22791;&#26159;&#20851;&#38190;&#24615;&#33021;&#22240;&#32032;&#12290;&#19981;&#21516;&#31867;&#22411;&#30340;&#40614;&#20811;&#39118;&#29305;&#24615;&#30001;&#20110;&#20854;&#19981;&#21516;&#30340;&#39057;&#29575;&#21709;&#24212;&#65292;&#20250;&#24341;&#20837;&#25968;&#23383;&#21270;&#38899;&#39057;&#20449;&#21495;&#30340;&#20998;&#24067;&#24046;&#24322;&#12290;&#22914;&#26524;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#32771;&#34385;&#27492;&#39046;&#22495;&#20559;&#31227;&#65292;&#37027;&#20040;&#24403;&#23427;&#29992;&#20110;&#26410;&#35265;&#36807;&#30340;&#35774;&#22791;&#35760;&#24405;&#38899;&#39057;&#26102;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#20005;&#37325;&#19979;&#38477;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#23569;&#25968;&#19981;&#21516;&#40614;&#20811;&#39118;&#19978;&#24405;&#21046;&#38899;&#39057;&#20449;&#21495;&#30340;&#27169;&#22411;&#35757;&#32451;&#21487;&#33021;&#20250;&#20351;&#27867;&#21270;&#21040;&#26410;&#34987;&#35757;&#32451;&#30340;&#35774;&#22791;&#22256;&#38590;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#24405;&#21046;&#35774;&#22791;&#33033;&#20914;&#21709;&#24212;(DIR)&#21367;&#31215;&#35757;&#32451;&#38598;&#20013;&#30340;&#38899;&#39057;&#20449;&#21495;&#65292;&#20174;&#32780;&#20154;&#24037;&#22686;&#21152;&#24405;&#38899;&#35774;&#22791;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20351;&#29992;CNN&#21644;&#38899;&#39057;&#20809;&#35889;&#21464;&#25442;&#36827;&#34892;Acoustic Scene Classification&#20219;&#21153;&#30340;DIR&#22686;&#24378;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;DIR&#22686;&#24378;&#23601;&#33021;&#25552;&#21319;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#35774;&#22791;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to generalize to a wide range of recording devices is a crucial performance factor for audio classification models. The characteristics of different types of microphones introduce distributional shifts in the digitized audio signals due to their varying frequency responses. If this domain shift is not taken into account during training, the model's performance could degrade severely when it is applied to signals recorded by unseen devices. In particular, training a model on audio signals recorded with a small number of different microphones can make generalization to unseen devices difficult. To tackle this problem, we convolve audio signals in the training set with pre-recorded device impulse responses (DIRs) to artificially increase the diversity of recording devices. We systematically study the effect of DIR augmentation on the task of Acoustic Scene Classification using CNNs and Audio Spectrogram Transformers. The results show that DIR augmentation in isolation performs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24418;&#27491;&#21017;&#21270;Tucker&#20998;&#35299;&#30340;&#26102;&#31354;&#20132;&#36890;&#25968;&#25454;&#22635;&#20805;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31232;&#30095;&#27491;&#21017;&#21270;&#39033;&#25913;&#21892;&#20102;Tucker&#26680;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#24341;&#20837;&#27969;&#24418;&#27491;&#21017;&#21270;&#21644;&#26102;&#38388;&#32422;&#26463;&#39033;&#26469;&#20248;&#21270;&#24352;&#37327;&#30340;&#22635;&#20805;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06563</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#24418;&#27491;&#21017;&#21270; Tucker &#20998;&#35299;&#30340;&#26102;&#31354;&#20132;&#36890;&#25968;&#25454;&#22635;&#20805;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Manifold Regularized Tucker Decomposition Approach for Spatiotemporal Traffic Data Imputation. (arXiv:2305.06563v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24418;&#27491;&#21017;&#21270;Tucker&#20998;&#35299;&#30340;&#26102;&#31354;&#20132;&#36890;&#25968;&#25454;&#22635;&#20805;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31232;&#30095;&#27491;&#21017;&#21270;&#39033;&#25913;&#21892;&#20102;Tucker&#26680;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#24341;&#20837;&#27969;&#24418;&#27491;&#21017;&#21270;&#21644;&#26102;&#38388;&#32422;&#26463;&#39033;&#26469;&#20248;&#21270;&#24352;&#37327;&#30340;&#22635;&#20805;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#20132;&#36890;&#25968;&#25454;&#22635;&#20805;(STDI)&#26159;&#25968;&#25454;&#39537;&#21160;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#19981;&#21487;&#36991;&#20813;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22312;&#37096;&#20998;&#35266;&#27979;&#21040;&#30340;&#20132;&#36890;&#25968;&#25454;&#20013;&#20272;&#35745;&#20002;&#22833;&#25968;&#25454;&#12290;&#30001;&#20110;&#20132;&#36890;&#25968;&#25454;&#20855;&#26377;&#22810;&#32500;&#21644;&#26102;&#31354;&#24615;&#36136;&#65292;&#25105;&#20204;&#23558;&#20002;&#22833;&#25968;&#25454;&#22635;&#20805;&#35270;&#20026;&#24352;&#37327;&#23436;&#25104;&#38382;&#39064;&#12290;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#35768;&#22810;&#20851;&#20110;&#22522;&#20110;&#24352;&#37327;&#20998;&#35299;&#30340; STDI &#30340;&#30740;&#31350;&#24050;&#32463;&#23637;&#24320;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#21033;&#29992;&#26102;&#31354;&#30456;&#20851;&#24615;&#21644;&#26680;&#24352;&#37327;&#31232;&#30095;&#24615;&#26469;&#25913;&#21892;&#22635;&#20805;&#24615;&#33021;&#20173;&#28982;&#38656;&#35201;&#35299;&#20915;&#12290;&#26412;&#25991;&#37325;&#26032;&#26500;&#36896;&#20102;3/4&#38454;&#27721;&#20811;&#23572;&#24352;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#27969;&#24418;&#27491;&#21017;&#21270; Tucker &#20998;&#35299;(maniRTD)&#27169;&#22411;&#29992;&#20110;STDI&#12290;&#26126;&#30830;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22810;&#32500;&#24310;&#36831;&#23884;&#20837;&#21464;&#25442;&#23558;&#20256;&#24863;&#20132;&#36890;&#29366;&#24577;&#25968;&#25454;&#34920;&#31034;&#20026;3/4&#38454;&#24352;&#37327;&#12290;&#28982;&#21518;&#65292;ManiRTD&#20351;&#29992;&#31232;&#30095;&#27491;&#21017;&#21270;&#39033;&#25913;&#21892;&#20102;Tucker&#26680;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#20351;&#29992;&#27969;&#24418;&#27491;&#21017;&#21270;&#21644;&#26102;&#38388;&#32422;&#26463;&#39033;&#26469;&#20248;&#21270;&#24352;&#37327;&#30340;&#22635;&#20805;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal traffic data imputation (STDI), estimating the missing data from partially observed traffic data, is an inevitable and challenging task in data-driven intelligent transportation systems (ITS). Due to traffic data's multidimensional and spatiotemporal properties, we treat the missing data imputation as a tensor completion problem. Many studies have been on STDI based on tensor decomposition in the past decade. However, how to use spatiotemporal correlations and core tensor sparsity to improve the imputation performance still needs to be solved. This paper reshapes a 3rd/4th order Hankel tensor and proposes an innovative manifold regularized Tucker decomposition (ManiRTD) model for STDI. Expressly, we represent the sensory traffic state data as the 3rd/4th tensors by introducing Multiway Delay Embedding Transforms. Then, ManiRTD improves the sparsity of the Tucker core using a sparse regularization term and employs manifold regularization and temporal constraint terms of f
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#36716;&#31227;&#23398;&#20064;&#19979;&#27169;&#22411;&#36873;&#25321;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#20854;&#36716;&#31227;&#36317;&#31163;&#20250;&#24433;&#21709;&#33258;&#36866;&#24212;&#36895;&#29575;&#65292;&#21487;&#33021;&#23548;&#33268;&#36895;&#29575;&#36739;&#24930;&#12290;</title><link>http://arxiv.org/abs/2305.00152</link><description>&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#19979;&#30340;&#27169;&#22411;&#36873;&#25321;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Limits of Model Selection under Transfer Learning. (arXiv:2305.00152v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00152
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#36716;&#31227;&#23398;&#20064;&#19979;&#27169;&#22411;&#36873;&#25321;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#20854;&#36716;&#31227;&#36317;&#31163;&#20250;&#24433;&#21709;&#33258;&#36866;&#24212;&#36895;&#29575;&#65292;&#21487;&#33021;&#23548;&#33268;&#36895;&#29575;&#36739;&#24930;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20851;&#20110;&#36716;&#31227;&#23398;&#20064;&#25110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#29702;&#35770;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#24050;&#30693;&#20551;&#35774;&#31867;&#25110;&#27169;&#22411;&#30340;&#24773;&#20917;&#65307;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#24120;&#28041;&#21450;&#19968;&#23450;&#31243;&#24230;&#30340;&#27169;&#22411;&#36873;&#25321;&#65292;&#36825;&#32463;&#24120;&#20986;&#29616;&#22312;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#24635;&#20307;&#33539;&#30068;&#19979;&#65306;&#20363;&#22914;&#65292;&#25105;&#20204;&#21487;&#20197;&#32771;&#34385;&#35843;&#25972;&#38024;&#23545;&#30446;&#26631;&#20219;&#21153;&#30340;&#27491;&#30830;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#21033;&#29992;&#26469;&#33258;&#30456;&#20851;&#28304;&#20219;&#21153;&#30340;&#25968;&#25454;&#12290;&#38500;&#20102;&#19982;&#27169;&#22411;&#36873;&#25321;&#26377;&#20851;&#30340;&#36817;&#20284;&#19982;&#20272;&#35745;&#35823;&#24046;&#30340;&#36890;&#24120;&#26435;&#34913;&#20043;&#22806;&#65292;&#36825;&#20010;&#38382;&#39064;&#36824;&#24102;&#26469;&#20102;&#26032;&#30340;&#22797;&#26434;&#24230;&#65292;&#21363;&#28304;&#20998;&#24067;&#19982;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#30340;&#36716;&#31227;&#36317;&#31163;&#65292;&#36825;&#20010;&#36317;&#31163;&#38543;&#30528;&#20551;&#35774;&#31867;&#30340;&#36873;&#25321;&#32780;&#21457;&#29983;&#21464;&#21270;&#12290;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#20998;&#31867;&#38382;&#39064;&#12290;&#29305;&#21035;&#30340;&#65292;&#20998;&#26512;&#25581;&#31034;&#20102;&#19968;&#20123;&#24341;&#20154;&#27880;&#30446;&#30340;&#29616;&#35937;&#65306;&#33258;&#36866;&#24212;&#36895;&#29575;&#65292;&#21363;&#27809;&#26377;&#20998;&#24067;&#24335;&#20449;&#24687;&#26102;&#21487;&#36798;&#21040;&#30340;&#36895;&#29575;&#65292;&#21487;&#20197;&#20219;&#24847;&#24930;&#20110;oracle&#36895;&#29575;&#65292;&#21363;&#22312;&#32473;&#23450;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theoretical studies on transfer learning or domain adaptation have so far focused on situations with a known hypothesis class or model; however in practice, some amount of model selection is usually involved, often appearing under the umbrella term of hyperparameter-tuning: for example, one may think of the problem of tuning for the right neural network architecture towards a target task, while leveraging data from a related source task.  Now, in addition to the usual tradeoffs on approximation vs estimation errors involved in model selection, this problem brings in a new complexity term, namely, the transfer distance between source and target distributions, which is known to vary with the choice of hypothesis class.  We present a first study of this problem, focusing on classification; in particular, the analysis reveals some remarkable phenomena: adaptive rates, i.e., those achievable with no distributional information, can be arbitrarily slower than oracle rates, i.e., when given kn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#20687;&#32032;&#20013;&#23398;&#20064;&#29289;&#20307;&#20013;&#24515;&#21270;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20174;&#29289;&#20307;&#20013;&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#36716;&#21270;&#20026;&#8220;&#38382;&#39064;&#8221;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#38543;&#21518;&#23398;&#20064;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;&#26469;&#36827;&#34892;&#25511;&#21046;&#65292;&#22312;&#38745;&#24577;&#21644;&#38750;&#38745;&#24577;&#35774;&#32622;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;&#23398;&#21040;&#30340;&#34920;&#31034;&#19981;&#20165;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#32780;&#19988;&#22260;&#32469;&#30528;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#29289;&#20307;&#65292;&#26377;&#21161;&#20110;&#24555;&#36895;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2304.13892</link><description>&lt;p&gt;
&#20174;&#20687;&#32032;&#20013;&#23398;&#20064;&#29289;&#20307;&#20013;&#24515;&#21270;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Discovering Object-Centric Generalized Value Functions From Pixels. (arXiv:2304.13892v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#20687;&#32032;&#20013;&#23398;&#20064;&#29289;&#20307;&#20013;&#24515;&#21270;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20174;&#29289;&#20307;&#20013;&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#36716;&#21270;&#20026;&#8220;&#38382;&#39064;&#8221;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#38543;&#21518;&#23398;&#20064;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;&#26469;&#36827;&#34892;&#25511;&#21046;&#65292;&#22312;&#38745;&#24577;&#21644;&#38750;&#38745;&#24577;&#35774;&#32622;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;&#23398;&#21040;&#30340;&#34920;&#31034;&#19981;&#20165;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#32780;&#19988;&#22260;&#32469;&#30528;&#20855;&#26377;&#19981;&#21464;&#24615;&#30340;&#29289;&#20307;&#65292;&#26377;&#21161;&#20110;&#24555;&#36895;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23637;&#29616;&#20102;&#20174;&#39640;&#32500;&#36755;&#20837;&#20013;&#25552;&#21462;&#26377;&#29992;&#34920;&#31034;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#26159;&#25163;&#24037;&#36741;&#21161;&#20219;&#21153;&#21644;&#20266;&#22870;&#21169;&#12290;&#33258;&#21160;&#21270;&#22320;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#23398;&#20064;&#27492;&#31867;&#34920;&#31034;&#65292;&#20197;&#26399;&#23454;&#29616;&#25511;&#21046;&#21644;&#24555;&#36895;&#36866;&#24212;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35797;&#22270;&#20174;&#29289;&#20307;&#20013;&#21457;&#29616;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#23558;&#23427;&#20204;&#36716;&#21270;&#20026;&#26102;&#38388;&#19978;&#36830;&#36143;&#30340;&#8220;&#38382;&#39064;&#8221;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#38543;&#21518;&#23398;&#20064;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;&#26469;&#36827;&#34892;&#25511;&#21046;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#38745;&#24577;&#21644;&#38750;&#38745;&#24577;&#35774;&#32622;&#19979;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#34987;&#21457;&#29616;&#30340;&#24191;&#20041;&#20540;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#23450;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;&#23398;&#21040;&#30340;&#34920;&#31034;&#19981;&#20165;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#32780;&#19988;&#22260;&#32469;&#30528;&#29289;&#20307;&#65292;&#36825;&#20123;&#29289;&#20307;&#23545;&#20219;&#21153;&#30340;&#21464;&#21270;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#26377;&#21161;&#20110;&#24555;&#36895;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning has shown significant progress in extracting useful representations from high-dimensional inputs albeit using hand-crafted auxiliary tasks and pseudo rewards. Automatically learning such representations in an object-centric manner geared towards control and fast adaptation remains an open research problem. In this paper, we introduce a method that tries to discover meaningful features from objects, translating them to temporally coherent "question" functions and leveraging the subsequent learned general value functions for control. We compare our approach with state-of-the-art techniques alongside other ablations and show competitive performance in both stationary and non-stationary settings. Finally, we also investigate the discovered general value functions and through qualitative analysis show that the learned representations are not only interpretable but also, centered around objects that are invariant to changes across tasks facilitating fast adaptatio
&lt;/p&gt;</description></item><item><title>DLVA&#26159;&#19968;&#31181;&#29992;&#20110;&#20197;&#22826;&#22346;&#26234;&#33021;&#21512;&#32422;&#30340;&#24378;&#22823;&#28145;&#24230;&#23398;&#20064;&#28431;&#27934;&#26816;&#27979;&#24037;&#20855;&#65292;&#20854;&#31639;&#27861;&#28085;&#30422;&#20102;&#28304;&#20195;&#30721;&#21040;&#23383;&#33410;&#30721;&#30340;&#25193;&#23637;&#65292;&#24182;&#19988;&#36895;&#24230;&#27604;&#20256;&#32479;&#28431;&#27934;&#26816;&#27979;&#24037;&#20855;&#25552;&#39640;&#20102;10-500&#20493;&#65292;&#24182;&#25104;&#21151;&#22320;&#21457;&#29616;&#20102;&#19968;&#20123;Slither&#35823;&#26631;&#35760;&#30340;&#26131;&#21463;&#25915;&#20987;&#30340;&#21512;&#32422;&#12290;</title><link>http://arxiv.org/abs/2304.10726</link><description>&lt;p&gt;
&#26234;&#33021;&#23398;&#20064;&#21457;&#29616; &#24858;&#31528;&#21512;&#32422;
&lt;/p&gt;
&lt;p&gt;
Smart Learning to Find Dumb Contracts. (arXiv:2304.10726v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10726
&lt;/p&gt;
&lt;p&gt;
DLVA&#26159;&#19968;&#31181;&#29992;&#20110;&#20197;&#22826;&#22346;&#26234;&#33021;&#21512;&#32422;&#30340;&#24378;&#22823;&#28145;&#24230;&#23398;&#20064;&#28431;&#27934;&#26816;&#27979;&#24037;&#20855;&#65292;&#20854;&#31639;&#27861;&#28085;&#30422;&#20102;&#28304;&#20195;&#30721;&#21040;&#23383;&#33410;&#30721;&#30340;&#25193;&#23637;&#65292;&#24182;&#19988;&#36895;&#24230;&#27604;&#20256;&#32479;&#28431;&#27934;&#26816;&#27979;&#24037;&#20855;&#25552;&#39640;&#20102;10-500&#20493;&#65292;&#24182;&#25104;&#21151;&#22320;&#21457;&#29616;&#20102;&#19968;&#20123;Slither&#35823;&#26631;&#35760;&#30340;&#26131;&#21463;&#25915;&#20987;&#30340;&#21512;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#24378;&#22823;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340; Deep Learning Vulnerability Analyzer &#65288;DLVA&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#20197;&#23383;&#33410;&#30721;&#20026;&#22522;&#30784;&#30340;&#20197;&#22826;&#22346;&#26234;&#33021;&#21512;&#32422;&#30340;&#28431;&#27934;&#26816;&#27979;&#24037;&#20855;&#12290;&#25105;&#20204;&#22312;&#27809;&#26377;&#25163;&#21160;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#23450;&#20041;&#27169;&#24335;&#25110;&#19987;&#23478;&#35268;&#21017;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#28304;&#20195;&#30721;&#20998;&#26512;&#25193;&#23637;&#21040;&#23383;&#33410;&#30721;&#65292;&#35757;&#32451;DLVA&#21028;&#26029;&#23383;&#33410;&#30721;&#12290;DLVA&#35757;&#32451;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#20063;&#24456;&#24378;&#65306;&#23427;&#20811;&#26381;&#20102;1.25%&#35823;&#26631;&#35760;&#21512;&#32422;&#30340;&#38169;&#35823;&#29575;&#65292;&#23398;&#29983;&#36229;&#36234;&#20102;&#32769;&#24072;&#65292;&#24182;&#21457;&#29616;&#20102;Slither&#35823;&#26631;&#35760;&#30340;&#26131;&#21463;&#25915;&#20987;&#30340;&#21512;&#32422;&#12290;DLVA&#27604;&#22522;&#20110;&#24418;&#24335;&#26041;&#27861;&#30340;&#20256;&#32479;&#26234;&#33021;&#21512;&#32422;&#28431;&#27934;&#26816;&#27979;&#24037;&#20855;&#24555;&#24471;&#22810;&#65306;DLVA&#26816;&#26597;&#20102;29&#20010;&#28431;&#27934;&#25152;&#38656;&#30340;&#26102;&#38388;&#20026;0.2&#31186;&#65292;&#36895;&#24230;&#25552;&#39640;&#20102;10-500&#20493;&#12290;DLVA&#26377;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;Smart Contract to Vector&#65288;SC2Vec&#65289;&#23558;&#26234;&#33021;&#21512;&#32422;&#36716;&#25442;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;Bytecode Tokenizer&#65288;BCT&#65289;&#23558;&#24213;&#23618;&#23383;&#33410;&#30721;&#36716;&#25442;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#24847;&#20041;&#30340;&#26631;&#35760;&#65292;DLVA&#26159;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#39044;&#27979;&#26234;&#33021;&#21512;&#32422;&#26159;&#21542;&#21253;&#21547;&#28431;&#27934;&#12290;&#25105;&#20204;&#23545;Etherscan&#30340;28,505&#20010;&#32463;&#36807;&#39564;&#35777;&#30340;&#26234;&#33021;&#21512;&#32422;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;DLVA&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#21462;&#24471;&#20102;0.964&#30340;AUC&#65288;&#30495;&#38451;&#29575;/&#20551;&#38451;&#29575;&#26354;&#32447;&#19979;&#30340;&#38754;&#31215;&#65289;&#24471;&#20998;&#12290;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;DLVA&#22312;F1&#20998;&#25968;&#19978;&#26174;&#31034;&#20102;30.7%&#30340;&#25913;&#36827;&#65292;&#23427;&#26159;&#31934;&#24230;&#21644;&#21484;&#22238;&#30340;&#35843;&#21644;&#24179;&#22343;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Deep Learning Vulnerability Analyzer (DLVA), a vulnerability detection tool for Ethereum smart contracts based on powerful deep learning techniques for sequential data adapted for bytecode. We train DLVA to judge bytecode even though the supervising oracle, Slither, can only judge source code. DLVA's training algorithm is general: we "extend" a source code analysis to bytecode without any manual feature engineering, predefined patterns, or expert rules. DLVA's training algorithm is also robust: it overcame a 1.25% error rate mislabeled contracts, and the student surpassing the teacher; found vulnerable contracts that Slither mislabeled. In addition to extending a source code analyzer to bytecode, DLVA is much faster than conventional tools for smart contract vulnerability detection based on formal methods: DLVA checks contracts for 29 vulnerabilities in 0.2 seconds, a speedup of 10-500x+ compared to traditional tools.  DLVA has three key components. Smart Contract to Vecto
&lt;/p&gt;</description></item><item><title>EPVT&#26159;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#24863;&#30693;&#30340;&#25552;&#31034;&#35270;&#35273;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#30142;&#30149;&#19981;&#30456;&#20851;&#22270;&#20687;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23884;&#20837;&#19968;&#32452;&#39046;&#22495;&#25552;&#31034;&#21644;&#19968;&#20010;&#20849;&#20139;&#25552;&#31034;&#26469;&#36827;&#34892;&#39046;&#22495;&#19968;&#33324;&#21270;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#39046;&#22495;&#25552;&#31034;&#29983;&#25104;&#22120;&#20419;&#36827;&#30693;&#35782;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2304.01508</link><description>&lt;p&gt;
EPVT: &#22522;&#20110;&#29615;&#22659;&#24863;&#30693;&#30340;&#25552;&#31034;&#35270;&#35273;Transformer&#22312;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#39046;&#22495;&#19968;&#33324;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
EPVT: Environment-aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition. (arXiv:2304.01508v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01508
&lt;/p&gt;
&lt;p&gt;
EPVT&#26159;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#24863;&#30693;&#30340;&#25552;&#31034;&#35270;&#35273;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#30142;&#30149;&#19981;&#30456;&#20851;&#22270;&#20687;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23884;&#20837;&#19968;&#32452;&#39046;&#22495;&#25552;&#31034;&#21644;&#19968;&#20010;&#20849;&#20139;&#25552;&#31034;&#26469;&#36827;&#34892;&#39046;&#22495;&#19968;&#33324;&#21270;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#39046;&#22495;&#25552;&#31034;&#29983;&#25104;&#22120;&#20419;&#36827;&#30693;&#35782;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#24050;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#65292;&#32780;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#37096;&#32626;&#36825;&#20123;&#31995;&#32479;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29992;&#20110;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#20110;&#19982;&#30142;&#30149;&#19981;&#30456;&#20851;&#30340;&#22270;&#20687;&#29305;&#24449;&#65288;&#22914;&#26263;&#35282;&#12289;&#27987;&#23494;&#27611;&#21457;&#65289;&#65292;&#23548;&#33268;&#22312;&#30475;&#19981;&#35265;&#30340;&#29615;&#22659;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#19968;&#33324;&#21270;&#26041;&#27861;&#8212;&#8212;EPVT&#65292;&#23427;&#23558;&#25552;&#31034;&#23884;&#20837;&#21040;Vision Transformer&#20013;&#65292;&#20197;&#21327;&#21516;&#23398;&#20064;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;EPVT&#21033;&#29992;&#19968;&#32452;&#39046;&#22495;&#25552;&#31034;&#65292;&#27599;&#20010;&#39046;&#22495;&#25552;&#31034;&#37117;&#25198;&#28436;&#39046;&#22495;&#19987;&#23478;&#30340;&#35282;&#33394;&#65292;&#20197;&#25429;&#33719;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#65307;&#20197;&#21450;&#19968;&#20010;&#20849;&#20139;&#25552;&#31034;&#26469;&#33719;&#24471;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#36890;&#29992;&#30693;&#35782;&#12290;&#20026;&#20102;&#20419;&#36827;&#30693;&#35782;&#20849;&#20139;&#21644;&#19981;&#21516;&#25552;&#31034;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39046;&#22495;&#25552;&#31034;&#29983;&#25104;&#22120;&#65292;&#23427;&#20351;&#24471;&#39046;&#22495;&#25552;&#31034;&#19982;&#20849;&#20139;&#25552;&#31034;&#20043;&#38388;&#21487;&#20197;&#36827;&#34892;&#20302;&#31209;&#20056;&#24615;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skin lesion recognition using deep learning has made remarkable progress, and there is an increasing need for deploying these systems in real-world scenarios. However, recent research has revealed that deep neural networks for skin lesion recognition may overly depend on disease-irrelevant image artifacts (i.e. dark corners, dense hairs), leading to poor generalization in unseen environments. To address this issue, we propose a novel domain generalization method called EPVT, which involves embedding prompts into the vision transformer to collaboratively learn knowledge from diverse domains. Concretely, EPVT leverages a set of domain prompts, each of which plays as a domain expert, to capture domain-specific knowledge; and a shared prompt for general knowledge over the entire dataset. To facilitate knowledge sharing and the interaction of different prompts, we introduce a domain prompt generator that enables low-rank multiplicative updates between domain prompts and the shared prompt. A
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;mCPT&#27169;&#22411;&#29992;&#20110;&#22810;&#35821;&#35328;&#30340;&#12289;&#22810;&#26631;&#31614;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#30340;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#20854;&#20182;8&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#35813;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.09901</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;3&#19978;&#30340;mCPT&#65306;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#26694;&#26550;&#26816;&#27979;&#30340;&#22810;&#35821;&#35328;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection. (arXiv:2303.09901v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;mCPT&#27169;&#22411;&#29992;&#20110;&#22810;&#35821;&#35328;&#30340;&#12289;&#22810;&#26631;&#31614;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#30340;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#20854;&#20182;8&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#35813;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38646;&#26679;&#26412;&#30340;&#35199;&#29677;&#29273;&#35821;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#30340;&#33719;&#32988;&#31995;&#32479;&#65292;&#24182;&#22312;&#21478;&#22806;&#20843;&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#30340;&#25361;&#25112;&#22312;&#20110;&#22312;&#21482;&#26377;&#23569;&#37327;&#25110;&#38646;&#20010;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#19968;&#32452;14&#20010;&#26694;&#26550;&#65292;&#21363;&#22810;&#35821;&#35328;&#22810;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;&#35299;&#20915;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;&#38500;&#20102;&#25551;&#36848;&#31995;&#32479;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23884;&#20837;&#31354;&#38388;&#20998;&#26512;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#22914;&#20309;&#25903;&#25345;&#26694;&#26550;&#26816;&#27979;&#20197;&#25512;&#36827;&#35745;&#31639;&#26694;&#26550;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the winning system for the zero-shot Spanish framing detection task, which also achieves competitive places in eight additional languages. The challenge of the framing detection task lies in identifying a set of 14 frames when only a few or zero samples are available, i.e., a multilingual multi-label few- or zero-shot setting. Our developed solution employs a pre-training procedure based on multilingual Transformers using a label-aware contrastive loss function. In addition to describing the system, we perform an embedding space analysis and ablation study to demonstrate how our pre-training procedure supports framing detection to advance computational framing analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;SpikeGPT&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20108;&#36827;&#21046;&#12289;&#20107;&#20214;&#39537;&#21160;&#33033;&#20914;&#28608;&#27963;&#21333;&#20803;&#36827;&#34892;&#35757;&#32451;&#65292;&#20811;&#26381;&#20102;SNN&#35757;&#32451;&#20013;&#30340;&#25361;&#25112;&#24615;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.13939</link><description>&lt;p&gt;
SpikeGPT&#65306;&#24102;&#26377;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks. (arXiv:2302.13939v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;SpikeGPT&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20108;&#36827;&#21046;&#12289;&#20107;&#20214;&#39537;&#21160;&#33033;&#20914;&#28608;&#27963;&#21333;&#20803;&#36827;&#34892;&#35757;&#32451;&#65292;&#20811;&#26381;&#20102;SNN&#35757;&#32451;&#20013;&#30340;&#25361;&#25112;&#24615;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#19981;&#26029;&#25193;&#22823;&#65292;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#20063;&#38543;&#20043;&#22686;&#21152;&#12290;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#31232;&#30095;&#21644;&#20107;&#20214;&#39537;&#21160;&#28608;&#27963;&#20943;&#23569;&#27169;&#22411;&#25512;&#29702;&#35745;&#31639;&#24320;&#38144;&#30340;&#33410;&#33021;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#34429;&#28982;&#23427;&#20204;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#24050;&#32463;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#20294;SNN&#30340;&#35757;&#32451;&#20063;&#34987;&#35777;&#26126;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#33853;&#21518;&#20110;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#65292;&#25105;&#20204;&#23578;&#26410;&#30475;&#21040;SNN&#22312;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21463;&#21040;Receptance Weighted Key Value&#65288;RWKV&#65289;&#35821;&#35328;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#8220;SpikeGPT&#8221;&#65292;&#23427;&#26159;&#19968;&#31181;&#20855;&#26377;&#20108;&#36827;&#21046;&#12289;&#20107;&#20214;&#39537;&#21160;&#33033;&#20914;&#28608;&#27963;&#21333;&#20803;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;&#27169;&#22411;&#21464;&#20307;&#19978;&#35757;&#32451;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#65306;45M&#21644;216M&#21442;&#25968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SpikeGPT&#26159;&#36804;&#20170;&#26368;&#22823;&#30340;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;SNN&#27169;&#22411;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#38750;&#33033;&#20914;&#27169;&#22411;&#36890;&#24120;&#35299;&#20915;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suita
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphSR&#30340;&#33258;&#21160;&#33410;&#28857;&#20998;&#31867;&#25968;&#25454;&#22686;&#24378;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#26174;&#33879;&#24046;&#24322;&#30340;&#26410;&#26631;&#35760;&#33410;&#28857;&#26469;&#22686;&#24378;&#23569;&#25968;&#31867;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2302.12814</link><description>&lt;p&gt;
GraphSR&#65306;&#19968;&#31181;&#29992;&#20110;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#30340;&#25968;&#25454;&#22686;&#24378;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
GraphSR: A Data Augmentation Algorithm for Imbalanced Node Classification. (arXiv:2302.12814v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphSR&#30340;&#33258;&#21160;&#33410;&#28857;&#20998;&#31867;&#25968;&#25454;&#22686;&#24378;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#26174;&#33879;&#24046;&#24322;&#30340;&#26410;&#26631;&#35760;&#33410;&#28857;&#26469;&#22686;&#24378;&#23569;&#25968;&#31867;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GNN&#33258;&#28982;&#20559;&#21521;&#20110;&#20855;&#26377;&#26356;&#22810;&#26631;&#35760;&#25968;&#25454;&#30340;&#22810;&#25968;&#31867;&#65292;&#24182;&#24573;&#30053;&#37027;&#20123;&#20855;&#26377;&#30456;&#23545;&#36739;&#23569;&#26631;&#35760;&#25968;&#25454;&#30340;&#23569;&#25968;&#31867;&#12290;&#20256;&#32479;&#30340;&#25216;&#26415;&#24448;&#24448;&#37319;&#29992;&#36807;&#37319;&#26679;&#26041;&#27861;&#65292;&#20294;&#21487;&#33021;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#24037;&#20316;&#25552;&#20986;&#20174;&#26631;&#35760;&#33410;&#28857;&#20013;&#21512;&#25104;&#23569;&#25968;&#31867;&#30340;&#38468;&#21152;&#33410;&#28857;&#65292;&#28982;&#32780;&#65292;&#24182;&#27809;&#26377;&#20445;&#35777;&#36825;&#20123;&#29983;&#25104;&#30340;&#33410;&#28857;&#30495;&#27491;&#20195;&#34920;&#30456;&#24212;&#30340;&#23569;&#25968;&#31867;&#12290;&#20107;&#23454;&#19978;&#65292;&#19981;&#24688;&#24403;&#22320;&#21512;&#25104;&#33410;&#28857;&#21487;&#33021;&#23548;&#33268;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20174;&#22270;&#30340;&#22823;&#37327;&#26410;&#26631;&#35760;&#33410;&#28857;&#20013;&#22686;&#24378;&#23569;&#25968;&#31867;&#30340;&#26041;&#27861;&#8212;&#8212;GraphSR&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#26174;&#33879;&#24046;&#24322;&#30340;&#26410;&#26631;&#35760;&#33410;&#28857;&#22686;&#21152;&#23569;&#25968;&#31867;&#65292;&#35813;&#31574;&#30053;&#22522;&#20110;&#30456;&#20284;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have achieved great success in node classification tasks. However, existing GNNs naturally bias towards the majority classes with more labelled data and ignore those minority classes with relatively few labelled ones. The traditional techniques often resort over-sampling methods, but they may cause overfitting problem. More recently, some works propose to synthesize additional nodes for minority classes from the labelled nodes, however, there is no any guarantee if those generated nodes really stand for the corresponding minority classes. In fact, improperly synthesized nodes may result in insufficient generalization of the algorithm. To resolve the problem, in this paper we seek to automatically augment the minority classes from the massive unlabelled nodes of the graph. Specifically, we propose \textit{GraphSR}, a novel self-training strategy to augment the minority classes with significant diversity of unlabelled nodes, which is based on a Similarity-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20026;&#39046;&#22495;&#22806;&#27867;&#21270;&#35774;&#35745;&#25968;&#25454;&#22686;&#24378;&#30340;&#21407;&#21017;&#65292;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#22312;&#20445;&#30041;&#40065;&#26834;&#29305;&#24449;&#30340;&#21516;&#26102;&#38543;&#26426;&#21270;&#34394;&#20551;&#30340;&#39046;&#22495;&#30456;&#20851;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#39046;&#22495;&#22806;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.11861</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#22686;&#24378;&#23454;&#29616;&#39046;&#22495;&#22806;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Out-of-Domain Robustness via Targeted Augmentations. (arXiv:2302.11861v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20026;&#39046;&#22495;&#22806;&#27867;&#21270;&#35774;&#35745;&#25968;&#25454;&#22686;&#24378;&#30340;&#21407;&#21017;&#65292;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#22312;&#20445;&#30041;&#40065;&#26834;&#29305;&#24449;&#30340;&#21516;&#26102;&#38543;&#26426;&#21270;&#34394;&#20551;&#30340;&#39046;&#22495;&#30456;&#20851;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#39046;&#22495;&#22806;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#39046;&#22495;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#24448;&#24448;&#22312;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#19978;&#34920;&#29616;&#19979;&#38477;&#65292;&#27604;&#22914;&#37326;&#29983;&#21160;&#29289;&#30417;&#27979;&#27169;&#22411;&#22312;&#26032;&#30340;&#25668;&#20687;&#26426;&#20301;&#32622;&#19978;&#37096;&#32626;&#26102;&#20250;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20026;&#39046;&#22495;&#22806;&#27867;&#21270;&#35774;&#35745;&#25968;&#25454;&#22686;&#24378;&#30340;&#21407;&#21017;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20851;&#27880;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22330;&#26223;&#65292;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#65292;&#19968;&#20123;&#19982;&#39046;&#22495;&#30456;&#20851;&#30340;&#29305;&#24449;&#26159;&#40065;&#26834;&#30340;&#65292;&#21363;&#19968;&#20123;&#22312;&#19981;&#21516;&#39046;&#22495;&#38388;&#21464;&#21270;&#30340;&#29305;&#24449;&#23545;&#27867;&#21270;&#26377;&#39044;&#27979;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#22312;&#19978;&#36848;&#30340;&#37326;&#29983;&#21160;&#29289;&#30417;&#27979;&#24212;&#29992;&#20013;&#65292;&#22270;&#20687;&#32972;&#26223;&#22312;&#25668;&#20687;&#26426;&#20301;&#32622;&#19978;&#19981;&#21516;&#65292;&#20294;&#21487;&#20197;&#25351;&#31034;&#26646;&#24687;&#22320;&#31867;&#22411;&#65292;&#20174;&#32780;&#24110;&#21161;&#39044;&#27979;&#34987;&#25668;&#21160;&#29289;&#30340;&#29289;&#31181;&#12290;&#22312;&#23545;&#32447;&#24615;&#35774;&#32622;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26377;&#38024;&#23545;&#24615;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21363;&#26377;&#36873;&#25321;&#24615;&#22320;&#38543;&#26426;&#21270;&#34394;&#20551;&#30340;&#39046;&#22495;&#30456;&#20851;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;&#40065;&#26834;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26377;&#38024;&#23545;&#24615;&#30340;&#22686;&#24378;&#21487;&#20197;&#25552;&#39640;&#39046;&#22495;&#22806;&#24615;&#33021;&#65292;&#20351;&#27169;&#22411;&#22312;&#36739;&#23569;&#39046;&#22495;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#29616;&#26377;&#30340;&#36890;&#29992;&#22686;&#24378;&#26041;&#27861;&#26410;&#33021;&#23454;&#29616;&#39046;&#22495;&#22806;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models trained on one set of domains often suffer performance drops on unseen domains, e.g., when wildlife monitoring models are deployed in new camera locations. In this work, we study principles for designing data augmentations for out-of-domain (OOD) generalization. In particular, we focus on real-world scenarios in which some domain-dependent features are robust, i.e., some features that vary across domains are predictive OOD. For example, in the wildlife monitoring application above, image backgrounds vary across camera locations but indicate habitat type, which helps predict the species of photographed animals. Motivated by theoretical analysis on a linear setting, we propose targeted augmentations, which selectively randomize spurious domain-dependent features while preserving robust ones. We prove that targeted augmentations improve OOD performance, allowing models to generalize better with fewer domains. In contrast, existing approaches such as generic augmentations, which fai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#29609;&#25991;&#26412;&#22522;&#30784;&#20882;&#38505;&#28216;&#25103;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22870;&#21169;&#22609;&#24418;&#25216;&#26415;&#65292;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#23494;&#38598;&#30340;&#22870;&#21169;&#20449;&#21495;&#32473;&#24378;&#21270;&#23398;&#20064;&#26426;&#22120;&#20154;&#65292;&#20174;&#32780;&#20351;&#20854;&#22312;&#35768;&#22810;&#28216;&#25103;&#20013;&#36798;&#21040;&#26356;&#39640;&#30340;&#24471;&#20998;&#65292;&#19988;&#21482;&#38656;&#19968;&#21322;&#30340;&#35757;&#32451;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2302.10720</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#29609;&#25991;&#26412;&#22522;&#30784;&#20882;&#38505;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Learning to Play Text-based Adventure Games with Maximum Entropy Reinforcement Learning. (arXiv:2302.10720v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#29609;&#25991;&#26412;&#22522;&#30784;&#20882;&#38505;&#28216;&#25103;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22870;&#21169;&#22609;&#24418;&#25216;&#26415;&#65292;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#23494;&#38598;&#30340;&#22870;&#21169;&#20449;&#21495;&#32473;&#24378;&#21270;&#23398;&#20064;&#26426;&#22120;&#20154;&#65292;&#20174;&#32780;&#20351;&#20854;&#22312;&#35768;&#22810;&#28216;&#25103;&#20013;&#36798;&#21040;&#26356;&#39640;&#30340;&#24471;&#20998;&#65292;&#19988;&#21482;&#38656;&#19968;&#21322;&#30340;&#35757;&#32451;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#22522;&#30784;&#28216;&#25103;&#26159;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;&#30340;&#27969;&#34892;&#27979;&#35797;&#24179;&#21488;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;(SAC)&#36866;&#24212;&#21040;&#25991;&#26412;&#29615;&#22659;&#20013;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#28508;&#21147;&#30340;&#22870;&#21169;&#22609;&#24418;&#25216;&#26415;&#65292;&#20197;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#23494;&#38598;&#30340;&#22870;&#21169;&#20449;&#21495;&#32473;&#24378;&#21270;&#23398;&#20064;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#24212;&#29992;&#35813;&#26041;&#27861;&#26469;&#29609;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25991;&#26412;&#28216;&#25103;&#12290;SAC&#26041;&#27861;&#22312;&#35768;&#22810;&#28216;&#25103;&#20013;&#30340;&#24471;&#20998;&#27604;Q&#23398;&#20064;&#26041;&#27861;&#39640;&#65292;&#24182;&#19988;&#21482;&#38656;&#19968;&#21322;&#30340;&#35757;&#32451;&#27493;&#39588;&#12290;&#36825;&#34920;&#26126;&#35813;&#26041;&#27861;&#38750;&#24120;&#36866;&#29992;&#20110;&#25991;&#26412;&#28216;&#25103;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22870;&#21169;&#22609;&#24418;&#25216;&#26415;&#21487;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#26356;&#24555;&#22320;&#23398;&#20064;&#31574;&#30053;&#24182;&#33719;&#24471;&#26356;&#39640;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based games are a popular testbed for language-based reinforcement learning (RL). In previous work, deep Q-learning is commonly used as the learning agent. Q-learning algorithms are challenging to apply to complex real-world domains due to, for example, their instability in training. Therefore, in this paper, we adapt the soft-actor-critic (SAC) algorithm to the text-based environment. To deal with sparse extrinsic rewards from the environment, we combine it with a potential-based reward shaping technique to provide more informative (dense) reward signals to the RL agent. We apply our method to play difficult text-based games. The SAC method achieves higher scores than the Q-learning methods on many games with only half the number of training steps. This shows that it is well-suited for text-based games. Moreover, we show that the reward shaping technique helps the agent to learn the policy faster and achieve higher scores. In particular, we consider a dynamically learned value fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#20010;&#24615;&#21270;&#26041;&#27861;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26089;&#20572;&#20934;&#21017;&#26469;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;</title><link>http://arxiv.org/abs/2302.04841</link><description>&lt;p&gt;
&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#36890;&#36807;&#36319;&#36394;&#30446;&#26631;&#21160;&#24577;&#26469;&#23454;&#29616;&#26356;&#24555;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#23450;&#21046;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Is This Loss Informative? Faster Text-to-Image Customization by Tracking Objective Dynamics. (arXiv:2302.04841v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#20010;&#24615;&#21270;&#26041;&#27861;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26089;&#20572;&#20934;&#21017;&#26469;&#21152;&#24555;&#35757;&#32451;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20195;&#34920;&#20102;&#22270;&#20687;&#21512;&#25104;&#30340;&#19979;&#19968;&#20010;&#21457;&#23637;&#38454;&#27573;&#65292;&#20026;&#23454;&#29616;&#28789;&#27963;&#20294;&#31934;&#32454;&#30340;&#25511;&#21046;&#32467;&#26524;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#24335;&#12290;&#30740;&#31350;&#30340;&#19968;&#20010;&#26032;&#20852;&#39046;&#22495;&#26159;&#23558;&#22823;&#22411;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#24555;&#36895;&#36866;&#24212;&#21040;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#25110;&#26032;&#30340;&#35270;&#35273;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#39640;&#25928;&#30340;&#36866;&#24212;&#26041;&#27861;&#38656;&#35201;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#38477;&#20302;&#20102;&#30740;&#31350;&#23454;&#39564;&#30340;&#36895;&#24230;&#65292;&#24182;&#28040;&#32791;&#20102;&#36807;&#22810;&#30340;GPU&#36164;&#28304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#20010;&#24615;&#21270;&#26041;&#27861;&#65288;&#22914;&#25991;&#26412;&#20498;&#36716;&#25110;&#26790;&#24187;&#23567;&#23627;&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#26088;&#22312;&#21152;&#36895;&#23427;&#20204;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22823;&#22810;&#25968;&#27010;&#24565;&#22312;&#26089;&#26399;&#38454;&#27573;&#23601;&#24050;&#32463;&#23398;&#20064;&#21040;&#20102;&#65292;&#24182;&#19988;&#36136;&#37327;&#22312;&#21518;&#26399;&#27809;&#26377;&#24471;&#21040;&#25913;&#21892;&#65292;&#20294;&#26159;&#26631;&#20934;&#30340;&#27169;&#22411;&#25910;&#25947;&#25351;&#26631;&#26410;&#33021;&#25351;&#31034;&#36825;&#19968;&#28857;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21363;&#25554;&#21363;&#29992;&#30340;&#26089;&#20572;&#20934;&#21017;&#65292;&#35813;&#20934;&#21017;&#21482;&#38656;&#35201;&#22312;&#25152;&#26377;&#35757;&#32451;&#36845;&#20195;&#20013;&#23545;&#19968;&#32452;&#22266;&#23450;&#36755;&#20837;&#35745;&#31639;&#24120;&#35268;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#23545;...&#36827;&#34892;&#20102;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generation models represent the next step of evolution in image synthesis, offering a natural way to achieve flexible yet fine-grained control over the result. One emerging area of research is the fast adaptation of large text-to-image models to smaller datasets or new visual concepts. However, many efficient methods of adaptation have a long training time, which limits their practical applications, slows down research experiments, and spends excessive GPU resources. In this work, we study the training dynamics of popular text-to-image personalization methods (such as Textual Inversion or DreamBooth), aiming to speed them up. We observe that most concepts are learned at early stages and do not improve in quality later, but standard model convergence metrics fail to indicate that. Instead, we propose a simple drop-in early stopping criterion that only requires computing the regular training objective on a fixed set of inputs for all training iterations. Our experiments on 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#26377;&#30028;f-&#25955;&#24230;&#32422;&#26463;&#19979;&#65292;&#36817;&#20284;&#25298;&#32477;&#37319;&#26679;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21487;&#20197;&#36890;&#36807;&#920;(~(D/f'(n)))&#20989;&#25968;&#26469;&#34920;&#31034;&#65292;&#24182;&#19988;&#24212;&#29992;&#20110;&#24179;&#28369;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#30456;&#20851;&#31639;&#27861;&#30340;&#24615;&#33021;&#20381;&#28982;&#25104;&#31435;&#12290;</title><link>http://arxiv.org/abs/2302.04658</link><description>&lt;p&gt;
&#36817;&#20284;&#25298;&#32477;&#37319;&#26679;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21450;&#20854;&#22312;&#24179;&#28369;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Sample Complexity of Approximate Rejection Sampling with Applications to Smoothed Online Learning. (arXiv:2302.04658v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#26377;&#30028;f-&#25955;&#24230;&#32422;&#26463;&#19979;&#65292;&#36817;&#20284;&#25298;&#32477;&#37319;&#26679;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21487;&#20197;&#36890;&#36807;&#920;(~(D/f'(n)))&#20989;&#25968;&#26469;&#34920;&#31034;&#65292;&#24182;&#19988;&#24212;&#29992;&#20110;&#24179;&#28369;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#30456;&#20851;&#31639;&#27861;&#30340;&#24615;&#33021;&#20381;&#28982;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#25105;&#20204;&#21487;&#20197;&#35775;&#38382;&#26469;&#33258;&#20998;&#24067;&#956;&#30340;n&#20010;&#29420;&#31435;&#26679;&#26412;&#65292;&#24182;&#19988;&#25105;&#20204;&#24076;&#26395;&#36755;&#20986;&#20854;&#20013;&#19968;&#20010;&#26679;&#26412;&#65292;&#20351;&#24471;&#36755;&#20986;&#30340;&#20998;&#24067;&#23613;&#21487;&#33021;&#25509;&#36817;&#30446;&#26631;&#20998;&#24067;&#957;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25152;&#26377;&#20855;&#26377;&#26377;&#30028;f-&#25955;&#24230;Df(&#957;|&#956;)&#8804;D&#30340;&#957;,&#956;&#23545;&#20013;&#65292;&#20851;&#20110;n&#30340;&#26368;&#20248;&#24635;&#21464;&#24046;&#36317;&#31163;&#30001;&#920;(~(D/f'(n)))&#32473;&#20986;&#12290;&#20043;&#21069;&#65292;&#36825;&#20010;&#38382;&#39064;&#21482;&#30740;&#31350;&#20102;&#957;&#30456;&#23545;&#20110;&#956;&#30340;Radon-Nikodym&#23548;&#25968;&#19968;&#33268;&#26377;&#30028;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#20284;&#20046;&#38750;&#24120;&#19981;&#21516;&#30340;&#24179;&#28369;&#22312;&#32447;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#23567;&#21270;&#36951;&#25022;&#21644;&#20855;&#26377;oracle&#25928;&#29575;&#30340;&#31639;&#27861;&#30340;&#36951;&#25022;&#21363;&#20351;&#22312;&#23545;&#25163;&#26377;&#36793;&#30028;f-&#25955;&#24230;&#65288;&#32780;&#19981;&#26159;&#26377;&#30028;Radon-Nikodym&#23548;&#25968;&#65289;&#30340;&#26494;&#24347;&#32422;&#26463;&#19979;&#65292;&#20173;&#28982;&#25104;&#31435;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22312;&#22343;&#21248;&#20272;&#35745;&#20013;&#29992;&#20110;&#24179;&#22343;&#20272;&#35745;&#30340;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Suppose we are given access to $n$ independent samples from distribution $\mu$ and we wish to output one of them with the goal of making the output distributed as close as possible to a target distribution $\nu$. In this work we show that the optimal total variation distance as a function of $n$ is given by $\tilde\Theta(\frac{D}{f'(n)})$ over the class of all pairs $\nu,\mu$ with a bounded $f$-divergence $D_f(\nu\|\mu)\leq D$. Previously, this question was studied only for the case when the Radon-Nikodym derivative of $\nu$ with respect to $\mu$ is uniformly bounded. We then consider an application in the seemingly very different field of smoothed online learning, where we show that recent results on the minimax regret and the regret of oracle-efficient algorithms still hold even under relaxed constraints on the adversary (to have bounded $f$-divergence, as opposed to bounded Radon-Nikodym derivative). Finally, we also study efficacy of importance sampling for mean estimates uniform o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#31639;&#27861;&#26469;&#23398;&#20064;&#25972;&#20010;&#35299;&#26512;&#27169;&#22411;&#30340;&#32467;&#26500;&#21644;&#31995;&#25968;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#36951;&#20256;&#32534;&#31243;&#26041;&#27861;&#22312;&#21464;&#37327;&#21644;&#26679;&#26412;&#25968;&#37327;&#22686;&#21152;&#26102;&#27169;&#22411;&#35268;&#27169;&#21644;&#22797;&#26434;&#24230;&#22686;&#38271;&#24555;&#12289;&#20934;&#30830;&#24615;&#25552;&#21319;&#19981;&#36275;&#20197;&#21450;&#27169;&#22411;&#31995;&#25968;&#35843;&#25972;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.00773</link><description>&lt;p&gt;
&#36808;&#21521;&#29289;&#29702;&#21512;&#29702;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#29992;&#20110;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Toward Physically Plausible Data-Driven Models: A Novel Neural Network Approach to Symbolic Regression. (arXiv:2302.00773v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00773
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#31639;&#27861;&#26469;&#23398;&#20064;&#25972;&#20010;&#35299;&#26512;&#27169;&#22411;&#30340;&#32467;&#26500;&#21644;&#31995;&#25968;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#36951;&#20256;&#32534;&#31243;&#26041;&#27861;&#22312;&#21464;&#37327;&#21644;&#26679;&#26412;&#25968;&#37327;&#22686;&#21152;&#26102;&#27169;&#22411;&#35268;&#27169;&#21644;&#22797;&#26434;&#24230;&#22686;&#38271;&#24555;&#12289;&#20934;&#30830;&#24615;&#25552;&#21319;&#19981;&#36275;&#20197;&#21450;&#27169;&#22411;&#31995;&#25968;&#35843;&#25972;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#31995;&#32479;&#21487;&#20197;&#30001;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#25968;&#23398;&#27169;&#22411;&#25551;&#36848;&#65292;&#36825;&#20123;&#27169;&#22411;&#26131;&#20110;&#20998;&#26512;&#24182;&#26377;&#21161;&#20110;&#35299;&#37322;&#31995;&#32479;&#30340;&#34892;&#20026;&#12290;&#31526;&#21495;&#22238;&#24402;&#26159;&#19968;&#31181;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#33258;&#21160;&#29983;&#25104;&#36825;&#31181;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#21382;&#21490;&#19978;&#65292;&#31526;&#21495;&#22238;&#24402;&#20027;&#35201;&#36890;&#36807;&#36951;&#20256;&#32534;&#31243;&#26469;&#23454;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#36951;&#20256;&#25805;&#20316;&#20132;&#21449;&#21644;&#31361;&#21464;&#23545;&#20505;&#36873;&#35299;&#30340;&#32676;&#20307;&#36827;&#34892;&#36827;&#21270;&#21644;&#20462;&#25913;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65306;&#38543;&#30528;&#35757;&#32451;&#25968;&#25454;&#20013;&#21464;&#37327;&#21644;&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#20250;&#22686;&#38271;&#65292;&#20294;&#20934;&#30830;&#24615;&#25552;&#21319;&#19981;&#36275;&#65307;&#20165;&#36890;&#36807;&#36951;&#20256;&#25805;&#20316;&#24456;&#38590;&#23545;&#27169;&#22411;&#31995;&#25968;&#36827;&#34892;&#24494;&#35843;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#24320;&#22987;&#23558;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#23398;&#20064;&#25972;&#20010;&#35299;&#26512;&#27169;&#22411;&#65292;&#21363;&#23398;&#20064;&#27169;&#22411;&#30340;&#32467;&#26500;&#21644;&#31995;&#25968;&#65292;&#37319;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world systems can be described by mathematical models that are human-comprehensible, easy to analyze and help explain the system's behavior. Symbolic regression is a method that can automatically generate such models from data. Historically, symbolic regression has been predominantly realized by genetic programming, a method that evolves populations of candidate solutions that are subsequently modified by genetic operators crossover and mutation. However, this approach suffers from several deficiencies: it does not scale well with the number of variables and samples in the training data - models tend to grow in size and complexity without an adequate accuracy gain, and it is hard to fine-tune the model coefficients using just genetic operators. Recently, neural networks have been applied to learn the whole analytic model, i.e., its structure and the coefficients, using gradient-based optimization algorithms. This paper proposes a novel neural network-based symbolic regression
&lt;/p&gt;</description></item><item><title>GFlowNets&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#21152;&#24555;&#31185;&#23398;&#25506;&#32034;&#30340;&#36895;&#24230;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#32500;&#25628;&#32034;&#31354;&#38388;&#20013;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#21644;&#29983;&#25104;&#22810;&#26679;&#24615;&#30340;&#23454;&#39564;&#38598;&#12290;</title><link>http://arxiv.org/abs/2302.00615</link><description>&lt;p&gt;
GFlowNets&#29992;&#20110;&#22522;&#20110;AI&#30340;&#31185;&#23398;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
GFlowNets for AI-Driven Scientific Discovery. (arXiv:2302.00615v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00615
&lt;/p&gt;
&lt;p&gt;
GFlowNets&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#21152;&#24555;&#31185;&#23398;&#25506;&#32034;&#30340;&#36895;&#24230;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#32500;&#25628;&#32034;&#31354;&#38388;&#20013;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#21644;&#29983;&#25104;&#22810;&#26679;&#24615;&#30340;&#23454;&#39564;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#20154;&#31867;&#26368;&#32039;&#36843;&#30340;&#38382;&#39064;&#65292;&#22914;&#27668;&#20505;&#21361;&#26426;&#21644;&#20840;&#29699;&#22823;&#27969;&#34892;&#30340;&#23041;&#32961;&#65292;&#38656;&#35201;&#21152;&#24555;&#31185;&#23398;&#25506;&#32034;&#30340;&#36895;&#24230;&#12290;&#23613;&#31649;&#31185;&#23398;&#20256;&#32479;&#19978;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#35797;&#38169;&#21644;&#20598;&#28982;&#24615;&#65292;&#20294;&#36807;&#21435;&#20960;&#21313;&#24180;&#26469;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#31185;&#23398;&#21457;&#29616;&#20986;&#29616;&#20102;&#28608;&#22686;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#30495;&#27491;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#39640;&#36890;&#37327;&#23454;&#39564;&#35013;&#32622;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#24182;&#26356;&#22909;&#22320;&#34701;&#20837;&#31185;&#23398;&#21457;&#29616;&#27969;&#31243;&#12290;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#38754;&#20020;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#39640;&#25928;&#25506;&#32034;&#38750;&#24120;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#36825;&#38656;&#35201;&#20272;&#35745;&#21487;&#20943;&#23569;&#30340;&#65288;&#35748;&#30693;&#65289;&#19981;&#30830;&#23450;&#24615;&#21644;&#29983;&#25104;&#22810;&#26679;&#24615;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#23454;&#39564;&#38598;&#12290;&#36825;&#20419;&#20351;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#31216;&#20026;GFlowNets&#65292;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#24314;&#27169;&#12289;&#20551;&#35774;&#29983;&#25104;&#21644;&#23454;&#39564;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tackling the most pressing problems for humanity, such as the climate crisis and the threat of global pandemics, requires accelerating the pace of scientific discovery. While science has traditionally relied on trial and error and even serendipity to a large extent, the last few decades have seen a surge of data-driven scientific discoveries. However, in order to truly leverage large-scale data sets and high-throughput experimental setups, machine learning methods will need to be further improved and better integrated in the scientific discovery pipeline. A key challenge for current machine learning methods in this context is the efficient exploration of very large search spaces, which requires techniques for estimating reducible (epistemic) uncertainty and generating sets of diverse and informative experiments to perform. This motivated a new probabilistic machine learning framework called GFlowNets, which can be applied in the modeling, hypotheses generation and experimental design s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DiffSTG&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;STGNN&#30340;&#26102;&#31354;&#23398;&#20064;&#33021;&#21147;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23567;STG&#39044;&#27979;&#20013;&#30340;&#25490;&#21517;&#27010;&#29575;&#20998;&#25968;&#21644;&#22343;&#26041;&#26681;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2301.13629</link><description>&lt;p&gt;
DiffSTG: &#24102;&#26377;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#30340;&#27010;&#29575;&#26102;&#31354;&#22270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DiffSTG: Probabilistic Spatio-Temporal Graph Forecasting with Denoising Diffusion Models. (arXiv:2301.13629v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DiffSTG&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;STGNN&#30340;&#26102;&#31354;&#23398;&#20064;&#33021;&#21147;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23567;STG&#39044;&#27979;&#20013;&#30340;&#25490;&#21517;&#27010;&#29575;&#20998;&#25968;&#21644;&#22343;&#26041;&#26681;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STGNN&#65289;&#24050;&#25104;&#20026;&#26102;&#31354;&#22270;&#65288;STG&#65289;&#39044;&#27979;&#30340;&#20027;&#35201;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26080;&#27861;&#23545;STG&#25968;&#25454;&#20013;&#30340;&#20869;&#22312;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#20851;&#27880;&#27010;&#29575;STG&#39044;&#27979;&#65292;&#30001;&#20110;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#21644;&#22797;&#26434;&#30340;ST&#20381;&#36182;&#20851;&#31995;&#30340;&#22256;&#38590;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#23558;&#27969;&#34892;&#30340;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#25512;&#24191;&#21040;STG&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DiffSTG&#30340;&#26032;&#30340;&#38750;&#33258;&#22238;&#24402;&#26694;&#26550;&#65292;&#24182;&#22312;&#35813;&#26694;&#26550;&#20013;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;STG&#21435;&#22122;&#32593;&#32476;UGnet&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;STGNN&#30340;&#26102;&#31354;&#23398;&#20064;&#33021;&#21147;&#19982;&#25193;&#25955;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#30456;&#32467;&#21512;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;DiffSTG&#23558;&#25345;&#32493;&#25490;&#21517;&#27010;&#29575;&#20998;&#25968;&#65288;CRPS&#65289;&#38477;&#20302;&#20102;4%-14%&#65292;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#38477;&#20302;&#20102;2%-7%&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatio-temporal graph neural networks (STGNN) have emerged as the dominant model for spatio-temporal graph (STG) forecasting. Despite their success, they fail to model intrinsic uncertainties within STG data, which cripples their practicality in downstream tasks for decision-making. To this end, this paper focuses on probabilistic STG forecasting, which is challenging due to the difficulty in modeling uncertainties and complex ST dependencies. In this study, we present the first attempt to generalize the popular denoising diffusion probabilistic models to STGs, leading to a novel non-autoregressive framework called DiffSTG, along with the first denoising network UGnet for STG in the framework. Our approach combines the spatio-temporal learning capabilities of STGNNs with the uncertainty measurements of diffusion models. Extensive experiments validate that DiffSTG reduces the Continuous Ranked Probability Score (CRPS) by 4%-14%, and Root Mean Squared Error (RMSE) by 2%-7% over existing 
&lt;/p&gt;</description></item><item><title>GibbsDDRM&#26159;&#19968;&#31181;&#25193;&#23637;&#30340;&#23616;&#37096;&#25240;&#21472;Gibbs&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#36870;&#38382;&#39064;&#20013;&#32447;&#24615;&#31639;&#23376;&#26410;&#30693;&#30340;&#30450;&#22330;&#26223;&#12290;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#20102;&#25968;&#25454;&#12289;&#27979;&#37327;&#21644;&#32447;&#24615;&#31639;&#23376;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#21518;&#39564;&#37319;&#26679;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#30450;&#22270;&#20687;&#21435;&#27169;&#31946;&#21644;&#35821;&#38899;&#21435;&#28151;&#21709;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#39640;&#24615;&#33021;&#65292;&#32780;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#36870;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.12686</link><description>&lt;p&gt;
GibbsDDRM:&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#30450;&#36870;&#38382;&#39064;&#30340;&#23616;&#37096;&#25240;&#21472;Gibbs&#37319;&#26679;&#22120;&#65292;&#24102;&#26377;&#21435;&#22122;&#25193;&#25955;&#24674;&#22797;&#30340;&#20808;&#39564;&#12290;(arXiv:2301.12686v2 [cs.LG] &#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration. (arXiv:2301.12686v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12686
&lt;/p&gt;
&lt;p&gt;
GibbsDDRM&#26159;&#19968;&#31181;&#25193;&#23637;&#30340;&#23616;&#37096;&#25240;&#21472;Gibbs&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#36870;&#38382;&#39064;&#20013;&#32447;&#24615;&#31639;&#23376;&#26410;&#30693;&#30340;&#30450;&#22330;&#26223;&#12290;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#20102;&#25968;&#25454;&#12289;&#27979;&#37327;&#21644;&#32447;&#24615;&#31639;&#23376;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#21518;&#39564;&#37319;&#26679;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#30450;&#22270;&#20687;&#21435;&#27169;&#31946;&#21644;&#35821;&#38899;&#21435;&#28151;&#21709;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#39640;&#24615;&#33021;&#65292;&#32780;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#36870;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#32447;&#24615;&#36870;&#38382;&#39064;&#20013;&#65292;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#21151;&#29992;&#20316;&#20808;&#39564;&#65292;&#30446;&#26631;&#26159;&#20174;&#22122;&#22768;&#32447;&#24615;&#27979;&#37327;&#20013;&#37325;&#26500;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20102;&#35299;&#32447;&#24615;&#31639;&#23376;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GibbsDDRM&#65292;&#23427;&#26159;&#23558;Denoising Diffusion Restoration Models(DDRM)&#25193;&#23637;&#21040;&#32447;&#24615;&#27979;&#37327;&#31639;&#23376;&#26410;&#30693;&#30340;&#30450;&#22330;&#26223;&#12290;GibbsDDRM&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26500;&#24314;&#20102;&#25968;&#25454;&#12289;&#27979;&#37327;&#21644;&#32447;&#24615;&#31639;&#23376;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#19968;&#31181;&#39640;&#25928;&#30340;Gibbs&#37319;&#26679;&#22120;&#30340;&#21464;&#20307;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#38382;&#39064;&#19981;&#21487;&#30693;&#30340;&#65292;&#24847;&#21619;&#30528;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#36870;&#38382;&#39064;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#65292;&#23613;&#31649;&#20351;&#29992;&#20102;&#31616;&#21333;&#30340;&#36890;&#29992;&#20808;&#39564;&#26469;&#22788;&#29702;&#24213;&#23618;&#32447;&#24615;&#31639;&#23376;&#65292;&#35813;&#26041;&#27861;&#22312;&#30450;&#22270;&#20687;&#21435;&#27169;&#31946;&#21644;&#35821;&#38899;&#21435;&#28151;&#21709;&#20219;&#21153;&#19978;&#22343;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained diffusion models have been successfully used as priors in a variety of linear inverse problems, where the goal is to reconstruct a signal from noisy linear measurements. However, existing approaches require knowledge of the linear operator. In this paper, we propose GibbsDDRM, an extension of Denoising Diffusion Restoration Models (DDRM) to a blind setting in which the linear measurement operator is unknown. GibbsDDRM constructs a joint distribution of the data, measurements, and linear operator by using a pre-trained diffusion model for the data prior, and it solves the problem by posterior sampling with an efficient variant of a Gibbs sampler. The proposed method is problem-agnostic, meaning that a pre-trained diffusion model can be applied to various inverse problems without fine-tuning. In experiments, it achieved high performance on both blind image deblurring and vocal dereverberation tasks, despite the use of simple generic priors for the underlying linear operators.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#26089;&#20572;&#19982;conformal&#26657;&#20934;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#26089;&#20572;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#32570;&#20047;&#29420;&#31435;&#26657;&#20934;&#25968;&#25454;&#26102;&#26080;&#27861;&#25552;&#20379;&#20934;&#30830;&#32479;&#35745;&#20445;&#35777;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.11556</link><description>&lt;p&gt;
&#23545;&#20110;&#20351;&#29992;&#26089;&#20572;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;conformal&#25512;&#29702;&#26159;&#20960;&#20046;&#20813;&#36153;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal inference is (almost) free for neural networks trained with early stopping. (arXiv:2301.11556v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#26089;&#20572;&#19982;conformal&#26657;&#20934;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#26089;&#20572;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#32570;&#20047;&#29420;&#31435;&#26657;&#20934;&#25968;&#25454;&#26102;&#26080;&#27861;&#25552;&#20379;&#20934;&#30830;&#32479;&#35745;&#20445;&#35777;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20445;&#30041;&#25968;&#25454;&#30340;&#26089;&#20572;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#20943;&#36731;&#36807;&#25311;&#21512;&#24182;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#20351;&#29992;&#26089;&#20572;&#35757;&#32451;&#30340;&#27169;&#22411;&#36890;&#24120;&#25552;&#20379;&#30456;&#23545;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20294;&#38500;&#38750;&#36827;&#19968;&#27493;&#20351;&#29992;&#29420;&#31435;&#30340;&#20445;&#30041;&#25968;&#25454;&#36827;&#34892;&#26657;&#20934;&#65292;&#21542;&#21017;&#23427;&#20204;&#36890;&#24120;&#20173;&#28982;&#32570;&#20047;&#31934;&#30830;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#26089;&#20572;&#19982;conformal&#26657;&#20934;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#39640;&#25928;&#22320;&#37325;&#22797;&#20351;&#29992;&#30456;&#21516;&#30340;&#20445;&#30041;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#19978;&#36848;&#38480;&#21046;&#12290;&#36825;&#23548;&#33268;&#20102;&#26082;&#20934;&#30830;&#21448;&#33021;&#22815;&#25552;&#20379;&#31934;&#30830;&#39044;&#27979;&#25512;&#26029;&#30340;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#22810;&#27425;&#25968;&#25454;&#25286;&#20998;&#25110;&#36807;&#20110;&#20445;&#23432;&#30340;&#35843;&#25972;&#12290;&#20026;&#19981;&#21516;&#30340;&#23398;&#20064;&#20219;&#21153;&#65288;&#24322;&#24120;&#20540;&#26816;&#27979;&#65292;&#22810;&#31867;&#20998;&#31867;&#65292;&#22238;&#24402;&#65289;&#24320;&#21457;&#20102;&#23454;&#38469;&#23454;&#29616;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early stopping based on hold-out data is a popular regularization technique designed to mitigate overfitting and increase the predictive accuracy of neural networks. Models trained with early stopping often provide relatively accurate predictions, but they generally still lack precise statistical guarantees unless they are further calibrated using independent hold-out data. This paper addresses the above limitation with conformalized early stopping: a novel method that combines early stopping with conformal calibration while efficiently recycling the same hold-out data. This leads to models that are both accurate and able to provide exact predictive inferences without multiple data splits nor overly conservative adjustments. Practical implementations are developed for different learning tasks -- outlier detection, multi-class classification, regression -- and their competitive performance is demonstrated on real data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#21453;&#21521;&#25293;&#21334;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#31227;&#21160;&#20247;&#21253;&#31995;&#32479;&#20013;&#22312;&#25307;&#21215;&#22810;&#20010;&#26410;&#30693;&#21644;&#26377;&#31574;&#30053;&#30340;&#24037;&#20316;&#32773;&#26102;&#20986;&#29616;&#30340;&#25968;&#25454;&#21487;&#20449;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.08563</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#24863;&#30693;&#29575;&#23398;&#20064;&#30340;CMAB&#26041;&#26696;&#65292;&#36890;&#36807;&#21487;&#20449;&#25968;&#25454;&#25910;&#38598;&#22312;&#20154;&#32676;&#20013;&#25239;&#20987;COVID-19
&lt;/p&gt;
&lt;p&gt;
A Semi-supervised Sensing Rate Learning based CMAB Scheme to Combat COVID-19 by Trustful Data Collection in the Crowd. (arXiv:2301.08563v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#21453;&#21521;&#25293;&#21334;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#31227;&#21160;&#20247;&#21253;&#31995;&#32479;&#20013;&#22312;&#25307;&#21215;&#22810;&#20010;&#26410;&#30693;&#21644;&#26377;&#31574;&#30053;&#30340;&#24037;&#20316;&#32773;&#26102;&#20986;&#29616;&#30340;&#25968;&#25454;&#21487;&#20449;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31227;&#21160;&#20247;&#21253;&#31995;&#32479;&#65292;&#25307;&#21215;&#21487;&#20449;&#21644;&#39640;&#36136;&#37327;&#30340;&#24037;&#20316;&#32773;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35201;&#20040;&#20551;&#35774;&#24037;&#20316;&#32773;&#30340;&#33021;&#21147;&#26159;&#20107;&#20808;&#24050;&#30693;&#30340;&#65292;&#35201;&#20040;&#20551;&#35774;&#24179;&#21488;&#19968;&#26086;&#25509;&#25910;&#21040;&#20182;&#20204;&#25910;&#38598;&#30340;&#25968;&#25454;&#23601;&#30693;&#36947;&#20182;&#20204;&#30340;&#33021;&#21147;&#12290;&#23454;&#38469;&#19978;&#65292;&#20026;&#20102;&#38477;&#20302;&#25104;&#26412;&#24182;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#25910;&#20837;&#65292;&#35768;&#22810;&#26377;&#31574;&#30053;&#30340;&#24037;&#20316;&#32773;&#19981;&#35802;&#23454;&#22320;&#25191;&#34892;&#20854;&#24863;&#30693;&#20219;&#21153;&#65292;&#24182;&#21521;&#24179;&#21488;&#25253;&#21578;&#34394;&#20551;&#25968;&#25454;&#65292;&#36825;&#34987;&#31216;&#20026;&#34394;&#20551;&#25968;&#25454;&#25915;&#20987;&#12290;&#23545;&#20110;&#24179;&#21488;&#26469;&#35828;&#65292;&#35780;&#20272;&#25152;&#25910;&#21040;&#30340;&#25968;&#25454;&#30340;&#30495;&#23454;&#24615;&#21313;&#20998;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#21322;&#30417;&#30563;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#21453;&#21521;&#25293;&#21334;&#65288;SCMABA&#65289;&#30340;&#28608;&#21169;&#26426;&#21046;&#26469;&#35299;&#20915;MCS&#20013;&#22810;&#20010;&#26410;&#30693;&#21644;&#26377;&#31574;&#30053;&#30340;&#24037;&#20316;&#32773;&#30340;&#25307;&#32856;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#24037;&#20316;&#32773;&#25307;&#21215;&#24314;&#27169;&#20026;&#22810;&#33218;&#36172;&#21338;&#21453;&#21521;&#25293;&#21334;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;UCB&#30340;&#31639;&#27861;&#26469;&#20998;&#31163;&#25506;&#32034;&#21644;&#24320;&#21457;&#65292;&#23558;&#24050;&#25307;&#21215;&#30340;&#24037;&#20316;&#32773;&#30340;&#24863;&#30693;&#29575;&#35270;&#20026;&#8220;&#33218;&#8220;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recruitment of trustworthy and high-quality workers is an important research issue for MCS. Previous studies either assume that the qualities of workers are known in advance, or assume that the platform knows the qualities of workers once it receives their collected data. In reality, to reduce costs and thus maximize revenue, many strategic workers do not perform their sensing tasks honestly and report fake data to the platform, which is called False data attacks. And it is very hard for the platform to evaluate the authenticity of the received data. In this paper, an incentive mechanism named Semi-supervision based Combinatorial Multi-Armed Bandit reverse Auction (SCMABA) is proposed to solve the recruitment problem of multiple unknown and strategic workers in MCS. First, we model the worker recruitment as a multi-armed bandit reverse auction problem and design an UCB-based algorithm to separate the exploration and exploitation, regarding the Sensing Rates (SRs) of recruited worke
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#22312;&#21355;&#26143;&#19978;&#26500;&#24314;&#22270;&#20687;&#22788;&#29702;&#21333;&#20803;&#65288;IPU&#65289;&#30340;&#24037;&#20316;&#21644;&#32463;&#39564;&#25945;&#35757;&#65292;&#36890;&#36807;&#27604;&#36739;CPU&#12289;GPU&#12289;TPU&#21644;VPU&#31561;&#22810;&#31181;&#36793;&#32536;&#35774;&#22791;&#30340;&#24615;&#33021;&#65292;&#25214;&#21040;&#28385;&#36275;&#21355;&#26143;&#36164;&#28304;&#38480;&#21046;&#24182;&#33021;&#22815;&#22312;&#24037;&#20316;&#36127;&#36733;&#21464;&#21270;&#26102;&#23454;&#29616;&#20934;&#30830;&#32467;&#26524;&#21644;&#28789;&#27963;&#24615;&#30340;&#35774;&#22791;&#12290;</title><link>http://arxiv.org/abs/2301.04954</link><description>&lt;p&gt;
&#36798;&#21040;&#36793;&#32536;&#30340;&#36793;&#32536;&#65306;&#31354;&#38388;&#20013;&#30340;&#22270;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Reaching the Edge of the Edge: Image Analysis in Space. (arXiv:2301.04954v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04954
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#22312;&#21355;&#26143;&#19978;&#26500;&#24314;&#22270;&#20687;&#22788;&#29702;&#21333;&#20803;&#65288;IPU&#65289;&#30340;&#24037;&#20316;&#21644;&#32463;&#39564;&#25945;&#35757;&#65292;&#36890;&#36807;&#27604;&#36739;CPU&#12289;GPU&#12289;TPU&#21644;VPU&#31561;&#22810;&#31181;&#36793;&#32536;&#35774;&#22791;&#30340;&#24615;&#33021;&#65292;&#25214;&#21040;&#28385;&#36275;&#21355;&#26143;&#36164;&#28304;&#38480;&#21046;&#24182;&#33021;&#22815;&#22312;&#24037;&#20316;&#36127;&#36733;&#21464;&#21270;&#26102;&#23454;&#29616;&#20934;&#30830;&#32467;&#26524;&#21644;&#28789;&#27963;&#24615;&#30340;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32452;&#20214;&#30340;&#23610;&#23544;&#21644;&#25104;&#26412;&#30340;&#20943;&#23569;&#65292;&#21355;&#26143;&#21464;&#24471;&#26356;&#21152;&#26222;&#36941;&#21487;&#29992;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#23567;&#22411;&#32452;&#32455;&#26377;&#33021;&#21147;&#37096;&#32626;&#21355;&#26143;&#65292;&#24182;&#22312;&#20854;&#19978;&#36816;&#34892;&#21508;&#31181;&#25968;&#25454;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#12290;&#19968;&#20010;&#27969;&#34892;&#30340;&#24212;&#29992;&#26159;&#22270;&#20687;&#20998;&#26512;&#65292;&#29992;&#20110;&#26816;&#27979;&#22320;&#38754;&#12289;&#20912;&#12289;&#20113;&#31561;&#22320;&#29699;&#35266;&#27979;&#12290;&#28982;&#32780;&#65292;&#21355;&#26143;&#19978;&#37096;&#32626;&#30340;&#35774;&#22791;&#36164;&#28304;&#21463;&#38480;&#65292;&#36825;&#20026;&#36825;&#31181;&#36164;&#28304;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;&#26500;&#24314;&#21355;&#26143;&#22270;&#20687;&#22788;&#29702;&#21333;&#20803;&#65288;IPU&#65289;&#26041;&#38754;&#30340;&#24037;&#20316;&#21644;&#32463;&#39564;&#25945;&#35757;&#12290;&#25105;&#20204;&#39318;&#20808;&#35843;&#26597;&#20102;&#22810;&#31181;&#36793;&#32536;&#35774;&#22791;&#65288;&#27604;&#36739;CPU&#12289;GPU&#12289;TPU&#21644;VPU&#65289;&#22312;&#21355;&#26143;&#19978;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#22788;&#29702;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#33021;&#22815;&#22312;&#28385;&#36275;&#21355;&#26143;&#21151;&#32791;&#21644;&#24310;&#36831;&#38480;&#21046;&#30340;&#21516;&#26102;&#65292;&#22312;&#24037;&#20316;&#36127;&#36733;&#21464;&#21270;&#26102;&#33021;&#22815;&#23454;&#29616;&#20934;&#30830;&#32467;&#26524;&#21644;&#28789;&#27963;&#24615;&#30340;&#35774;&#22791;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
Satellites have become more widely available due to the reduction in size and cost of their components. As a result, there has been an advent of smaller organizations having the ability to deploy satellites with a variety of data-intensive applications to run on them. One popular application is image analysis to detect, for example, land, ice, clouds, etc. for Earth observation. However, the resource-constrained nature of the devices deployed in satellites creates additional challenges for this resource-intensive application.  In this paper, we present our work and lessons-learned on building an Image Processing Unit (IPU) for a satellite. We first investigate the performance of a variety of edge devices (comparing CPU, GPU, TPU, and VPU) for deep-learning-based image processing on satellites. Our goal is to identify devices that can achieve accurate results and are flexible when workload changes while satisfying the power and latency constraints of satellites. Our results demonstrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36755;&#20837;&#24402;&#19968;&#21270;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;INSGD&#65289;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#23398;&#20064;&#29575;&#19978;&#24212;&#29992;&#24402;&#19968;&#21270;&#26469;&#26356;&#26032;&#32593;&#32476;&#26435;&#37325;&#65292;&#20197;&#23454;&#29616;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#22797;&#26434;&#27169;&#22411;&#26102;&#36991;&#20813;&#21457;&#25955;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2212.09921</link><description>&lt;p&gt;
&#36755;&#20837;&#24402;&#19968;&#21270;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Input Normalized Stochastic Gradient Descent Training of Deep Neural Networks. (arXiv:2212.09921v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36755;&#20837;&#24402;&#19968;&#21270;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;INSGD&#65289;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#23398;&#20064;&#29575;&#19978;&#24212;&#29992;&#24402;&#19968;&#21270;&#26469;&#26356;&#26032;&#32593;&#32476;&#26435;&#37325;&#65292;&#20197;&#23454;&#29616;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#22797;&#26434;&#27169;&#22411;&#26102;&#36991;&#20813;&#21457;&#25955;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26032;&#22411;&#20248;&#21270;&#31639;&#27861;&#65292;&#31216;&#20026;&#36755;&#20837;&#24402;&#19968;&#21270;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;INSGD&#65289;&#65292;&#21463;&#21040;&#33258;&#36866;&#24212;&#28388;&#27874;&#20013;&#20351;&#29992;&#30340;&#24402;&#19968;&#21270;&#26368;&#23567;&#22343;&#26041;&#65288;NLMS&#65289;&#31639;&#27861;&#30340;&#21551;&#21457;&#12290;&#24403;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#22797;&#26434;&#27169;&#22411;&#26102;&#65292;&#20248;&#21270;&#22120;&#21442;&#25968;&#30340;&#36873;&#25321;&#65292;&#23588;&#20854;&#26159;&#23398;&#20064;&#29575;&#65292;&#23545;&#20110;&#36991;&#20813;&#21457;&#25955;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#32593;&#32476;&#26435;&#37325;&#65292;&#20854;&#20013;&#23398;&#20064;&#29575;&#20351;&#29992;&#22522;&#20110;$\ell_1$&#21644;$\ell_2$&#30340;&#24402;&#19968;&#21270;&#65292;&#31867;&#20284;&#20110;NLMS&#12290;&#28982;&#32780;&#65292;&#19982;&#29616;&#26377;&#30340;&#24402;&#19968;&#21270;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#24402;&#19968;&#21270;&#36807;&#31243;&#20013;&#25490;&#38500;&#20102;&#35823;&#24046;&#39033;&#65292;&#32780;&#26159;&#20351;&#29992;&#36755;&#20837;&#21521;&#37327;&#23545;&#26356;&#26032;&#39033;&#36827;&#34892;&#24402;&#19968;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#20110;&#19981;&#21516;&#30340;&#21021;&#22987;&#21270;&#35774;&#32622;&#65292;&#25105;&#20204;&#30340;&#20248;&#21270;&#31639;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#27700;&#24179;&#12290;&#25105;&#20204;&#20351;&#29992;ResNet-18&#12289;WResNet-20&#21644;ResNet-50&#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#35757;&#32451;&#31639;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel optimization algorithm for training machine learning models called Input Normalized Stochastic Gradient Descent (INSGD), inspired by the Normalized Least Mean Squares (NLMS) algorithm used in adaptive filtering. When training complex models on large datasets, the choice of optimizer parameters, particularly the learning rate, is crucial to avoid divergence. Our algorithm updates the network weights using stochastic gradient descent with $\ell_1$ and $\ell_2$-based normalizations applied to the learning rate, similar to NLMS. However, unlike existing normalization methods, we exclude the error term from the normalization process and instead normalize the update term using the input vector to the neuron. Our experiments demonstrate that our optimization algorithm achieves higher accuracy levels compared to different initialization settings. We evaluate the efficiency of our training algorithm on benchmark datasets using ResNet-18, WResNet-20, ResNet-50, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#24230;&#20013;&#26356;&#24555;&#30340;&#26368;&#22823;&#20869;&#31215;&#25628;&#32034;&#31639;&#27861; BanditMIPS&#65292;&#35813;&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#19982;&#32500;&#24230;&#26080;&#20851;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#39564;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2212.07551</link><description>&lt;p&gt;
&#22312;&#39640;&#32500;&#24230;&#20013;&#26356;&#24555;&#30340;&#26368;&#22823;&#20869;&#31215;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Faster Maximum Inner Product Search in High Dimensions. (arXiv:2212.07551v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07551
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#24230;&#20013;&#26356;&#24555;&#30340;&#26368;&#22823;&#20869;&#31215;&#25628;&#32034;&#31639;&#27861; BanditMIPS&#65292;&#35813;&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#19982;&#32500;&#24230;&#26080;&#20851;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#20869;&#31215;&#25628;&#32034;&#65288;MIPS&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#20219;&#21153;&#65292;&#22914;&#25512;&#33616;&#31995;&#32479;&#12290;&#32473;&#23450;&#19968;&#20010;&#26597;&#35810;&#21521;&#37327;&#21644;d&#32500;&#31354;&#38388;&#20013;&#30340;n&#20010;&#21407;&#23376;&#21521;&#37327;&#65292;MIPS&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19982;&#26597;&#35810;&#21521;&#37327;&#20855;&#26377;&#26368;&#39640;&#20869;&#31215;&#30340;&#21407;&#23376;&#12290;&#29616;&#26377;&#30340;MIPS&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#33267;&#23569;&#20026;O(&#8730;d)&#65292;&#22312;&#39640;&#32500;&#35774;&#32622;&#19979;&#21464;&#24471;&#35745;&#31639;&#19978;&#31105;&#27490;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BanditMIPS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;MIPS&#31639;&#27861;&#65292;&#20854;&#22797;&#26434;&#24230;&#19982;d&#26080;&#20851;&#12290;BanditMIPS&#36890;&#36807;&#23376;&#37319;&#26679;&#22352;&#26631;&#26469;&#20272;&#35745;&#27599;&#20010;&#21407;&#23376;&#30340;&#20869;&#31215;&#65292;&#24182;&#36866;&#24212;&#24615;&#22320;&#35780;&#20272;&#26356;&#26377;&#21069;&#26223;&#30340;&#21407;&#23376;&#30340;&#22352;&#26631;&#12290;&#20855;&#20307;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#21463;&#21040;&#20102;&#22810;&#33218;&#32769;&#34382;&#26426;&#38382;&#39064;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#65292;BanditMIPS&#20197;&#39640;&#27010;&#29575;&#36820;&#22238;&#27491;&#30830;&#30340;&#31572;&#26696;&#65292;&#21516;&#26102;&#23558;&#22797;&#26434;&#24230;&#20174;O(&#8730;d)&#25913;&#36827;&#21040;O(1)&#12290;&#25105;&#20204;&#36824;&#22312;&#22235;&#20010;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum Inner Product Search (MIPS) is a ubiquitous task in machine learning applications such as recommendation systems. Given a query vector and $n$ atom vectors in $d$-dimensional space, the goal of MIPS is to find the atom that has the highest inner product with the query vector. Existing MIPS algorithms scale at least as $O(\sqrt{d})$, which becomes computationally prohibitive in high-dimensional settings. In this work, we present BanditMIPS, a novel randomized MIPS algorithm whose complexity is independent of $d$. BanditMIPS estimates the inner product for each atom by subsampling coordinates and adaptively evaluates more coordinates for more promising atoms. The specific adaptive sampling strategy is motivated by multi-armed bandits. We provide theoretical guarantees that BanditMIPS returns the correct answer with high probability, while improving the complexity in $d$ from $O(\sqrt{d})$ to $O(1)$. We also perform experiments on four synthetic and real-world datasets and demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#29289;&#26631;&#24535;&#29289;&#28608;&#27963;&#22320;&#22270;&#65288;BAM&#65289;&#30340;&#35786;&#26029;&#26694;&#26550;&#65292;&#20351;&#29992;&#20004;&#20010;U&#24418;&#29983;&#25104;&#22120;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#24847;&#20041;&#26126;&#30830;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#39564;&#35777;&#21644;&#29702;&#35299;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#65292;&#21487;&#24212;&#29992;&#20110;&#33258;&#21160;&#35786;&#26029;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#12290;</title><link>http://arxiv.org/abs/2212.06299</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#29289;&#26631;&#24535;&#29289;&#28608;&#27963;&#22320;&#22270;&#30340;&#21487;&#35299;&#37322;&#24615;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Interpretable Diabetic Retinopathy Diagnosis based on Biomarker Activation Map. (arXiv:2212.06299v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#29289;&#26631;&#24535;&#29289;&#28608;&#27963;&#22320;&#22270;&#65288;BAM&#65289;&#30340;&#35786;&#26029;&#26694;&#26550;&#65292;&#20351;&#29992;&#20004;&#20010;U&#24418;&#29983;&#25104;&#22120;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#24847;&#20041;&#26126;&#30830;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#39564;&#35777;&#21644;&#29702;&#35299;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#65292;&#21487;&#24212;&#29992;&#20110;&#33258;&#21160;&#35786;&#26029;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#22522;&#20110;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#65288;OCT&#65289;&#21450;&#20854;&#34880;&#31649;&#24433;&#20687;&#23398;&#65288;OCTA&#65289;&#33258;&#21160;&#35786;&#26029;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#65288;DR&#65289;&#30340;&#26368;&#20934;&#30830;&#30340;&#25163;&#27573;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#20248;&#21183;&#37096;&#20998;&#24402;&#21151;&#20110;&#21253;&#21547;&#30340;&#38544;&#34255;&#23618;&#25152;&#25552;&#20379;&#30340;&#22797;&#26434;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25152;&#38656;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#38544;&#34255;&#23618;&#20063;&#20351;&#31639;&#27861;&#36755;&#20986;&#38590;&#20197;&#35299;&#37322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#23398;&#20064;&#30340;&#26032;&#22411;&#29983;&#29289;&#26631;&#24535;&#29289;&#28608;&#27963;&#22320;&#22270;&#65288;BAM&#65289;&#26694;&#26550;&#65292;&#20351;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#39564;&#35777;&#21644;&#29702;&#35299;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#12290;&#19968;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;456&#20010;&#40644;&#26001;&#25195;&#25551;&#26681;&#25454;&#24403;&#21069;&#20020;&#24202;&#26631;&#20934;&#34987;&#20998;&#32423;&#20026;&#38750;&#21487;&#36716;&#35786;&#25110;&#21487;&#36716;&#35786;DR&#12290;&#39318;&#20808;&#65292;&#22522;&#20110;&#27492;&#25968;&#25454;&#38598;&#20351;&#29992;DR&#20998;&#31867;&#22120;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;BAM&#12290;BAM&#29983;&#25104;&#26694;&#26550;&#26159;&#36890;&#36807;&#32452;&#21512;&#20004;&#20010;U&#24418;&#29983;&#25104;&#22120;&#26469;&#35774;&#35745;&#30340;&#65292;&#20174;&#32780;&#20026;&#27492;&#20998;&#31867;&#22120;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20027;&#35201;&#29983;&#25104;&#22120;&#26159;&#26681;&#25454;&#21487;&#36716;&#35786;&#25195;&#25551;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning classifiers provide the most accurate means of automatically diagnosing diabetic retinopathy (DR) based on optical coherence tomography (OCT) and its angiography (OCTA). The power of these models is attributable in part to the inclusion of hidden layers that provide the complexity required to achieve a desired task. However, hidden layers also render algorithm outputs difficult to interpret. Here we introduce a novel biomarker activation map (BAM) framework based on generative adversarial learning that allows clinicians to verify and understand classifiers decision-making. A data set including 456 macular scans were graded as non-referable or referable DR based on current clinical standards. A DR classifier that was used to evaluate our BAM was first trained based on this data set. The BAM generation framework was designed by combing two U-shaped generators to provide meaningful interpretability to this classifier. The main generator was trained to take referable scans as
&lt;/p&gt;</description></item><item><title>XPER&#26041;&#27861;&#33021;&#34913;&#37327;&#36755;&#20837;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#20855;&#20307;&#36129;&#29486;&#65292;&#24182;&#21487;&#29992;&#20110;&#22788;&#29702;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#26500;&#24314;&#21516;&#36136;&#21270;&#20010;&#20307;&#32676;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.05866</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#24615;&#33021;&#65306;&#34913;&#37327;&#39044;&#27979;&#24615;&#33021;&#30340;&#39537;&#21160;&#21147;
&lt;/p&gt;
&lt;p&gt;
Explainable Performance: Measuring the Driving Forces of Predictive Performance. (arXiv:2212.05866v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05866
&lt;/p&gt;
&lt;p&gt;
XPER&#26041;&#27861;&#33021;&#34913;&#37327;&#36755;&#20837;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#20855;&#20307;&#36129;&#29486;&#65292;&#24182;&#21487;&#29992;&#20110;&#22788;&#29702;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#26500;&#24314;&#21516;&#36136;&#21270;&#20010;&#20307;&#32676;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;XPER&#65288;eXplainable PERformance&#65289;&#26041;&#27861;&#26469;&#34913;&#37327;&#36755;&#20837;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#20855;&#20307;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#22522;&#20110;Shapley&#20540;&#65292;&#26082;&#19981;&#20381;&#36182;&#20110;&#27169;&#22411;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#24615;&#33021;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;XPER&#21487;&#22312;&#27169;&#22411;&#32423;&#21035;&#25110;&#20010;&#20307;&#32423;&#21035;&#23454;&#29616;&#12290;&#25105;&#20204;&#35777;&#26126;XPER&#20855;&#26377;&#26631;&#20934;&#35299;&#37322;&#24615;&#26041;&#27861;&#65288;SHAP&#65289;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;&#22312;&#36151;&#27454;&#36829;&#32422;&#39044;&#27979;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;XPER&#22788;&#29702;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#22806;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#20010;&#20307;XPER&#20540;&#23545;&#20182;&#20204;&#36827;&#34892;&#32858;&#31867;&#26469;&#26500;&#24314;&#21516;&#36136;&#21270;&#30340;&#20010;&#20307;&#32676;&#20307;&#12290;&#25105;&#20204;&#21457;&#29616;&#20272;&#35745;&#32676;&#20307;&#29305;&#23450;&#30340;&#27169;&#22411;&#27604;&#19968;&#20010;&#27169;&#22411;&#36866;&#29992;&#20110;&#25152;&#26377;&#20010;&#20307;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the XPER (eXplainable PERformance) methodology to measure the specific contribution of the input features to the predictive performance of a model. Our methodology is theoretically grounded on Shapley values and is both model-agnostic and performance metric-agnostic. Furthermore, XPER can be implemented either at the model level or at the individual level. We demonstrate that XPER has as a special case the standard explainability method in machine learning (SHAP). In a loan default forecasting application, we show how XPER can be used to deal with heterogeneity issues and significantly boost out-of-sample performance. To do so, we build homogeneous groups of individuals by clustering them based on their individual XPER values. We find that estimating group-specific models yields a much higher predictive accuracy than with a one-fits-all model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#20998;&#26512;&#19982;&#20272;&#35745;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#36890;&#36807;&#37327;&#21270;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#23545;&#26368;&#32456;&#27169;&#22411;&#30340;&#25913;&#21464;&#31243;&#24230;&#25581;&#31034;&#20102;&#35757;&#32451;&#30340;&#22522;&#26412;&#30456;&#20114;&#20316;&#29992;&#12290;&#35843;&#26597;&#20102;&#24403;&#21069;&#30340;&#24433;&#21709;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2212.04612</link><description>&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#20998;&#26512;&#19982;&#20272;&#35745;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Training Data Influence Analysis and Estimation: A Survey. (arXiv:2212.04612v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#20998;&#26512;&#19982;&#20272;&#35745;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#36890;&#36807;&#37327;&#21270;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#23545;&#26368;&#32456;&#27169;&#22411;&#30340;&#25913;&#21464;&#31243;&#24230;&#25581;&#31034;&#20102;&#35757;&#32451;&#30340;&#22522;&#26412;&#30456;&#20114;&#20316;&#29992;&#12290;&#35843;&#26597;&#20102;&#24403;&#21069;&#30340;&#24433;&#21709;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22909;&#30340;&#27169;&#22411;&#38656;&#35201;&#22909;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23545;&#20110;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#27169;&#22411;&#65292;&#35757;&#32451;&#25968;&#25454;&#19982;&#27169;&#22411;&#39044;&#27979;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#36879;&#26126;&#21644;&#38590;&#20197;&#29702;&#35299;&#12290;&#36890;&#36807;&#37327;&#21270;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#23545;&#26368;&#32456;&#27169;&#22411;&#30340;&#25913;&#21464;&#31243;&#24230;&#65292;&#24433;&#21709;&#20998;&#26512;&#37096;&#20998;&#25581;&#31034;&#20102;&#35757;&#32451;&#30340;&#22522;&#26412;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#27979;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#21147;&#26159;&#26377;&#21487;&#33021;&#35777;&#26126;&#22256;&#38590;&#30340;&#65307;&#22240;&#27492;&#65292;&#21457;&#23637;&#21644;&#20351;&#29992;&#24433;&#21709;&#21147;&#20272;&#35745;&#22120;&#65292;&#20165;&#23545;&#30495;&#23454;&#24433;&#21709;&#36827;&#34892;&#36817;&#20284;&#20272;&#35745;&#12290;&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#35843;&#26597;&#20102;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#20998;&#26512;&#19982;&#20272;&#35745;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#31995;&#32479;&#21270;&#21508;&#31181;&#20851;&#20110;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#30340;&#23450;&#20041;&#65292;&#24182;&#22312;&#26576;&#20123;&#26041;&#38754;&#36827;&#34892;&#35299;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#26368;&#20808;&#36827;&#30340;&#24433;&#21709;&#20998;&#26512;&#26041;&#27861;&#20998;&#31867;&#25972;&#29702;&#65307;&#35814;&#32454;&#25551;&#36848;&#27599;&#31181;&#26041;&#27861;&#65292;&#27604;&#36739;&#20854;&#22522;&#26412;&#20551;&#35774;&#12289;&#28176;&#36817;&#22797;&#26434;&#24230;&#20197;&#21450;&#25972;&#20307;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Good models require good training data. For overparameterized deep models, the causal relationship between training data and model predictions is increasingly opaque and poorly understood. Influence analysis partially demystifies training's underlying interactions by quantifying the amount each training instance alters the final model. Measuring the training data's influence exactly can be provably hard in the worst case; this has led to the development and use of influence estimators, which only approximate the true influence. This paper provides the first comprehensive survey of training data influence analysis and estimation. We begin by formalizing the various, and in places orthogonal, definitions of training data influence. We then organize state-of-the-art influence analysis methods into a taxonomy; we describe each of these methods in detail and compare their underlying assumptions, asymptotic complexities, and overall strengths and weaknesses. Finally, we propose future resear
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24102;&#26377;&#31070;&#32463;&#32593;&#32476;&#21644;&#32034;&#24341;&#30340;&#32858;&#31867;&#27169;&#22411;CNNI&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38750;&#20984;&#24418;&#29366;&#25968;&#25454;&#30340;&#21442;&#25968;&#21270;&#32858;&#31867;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.03853</link><description>&lt;p&gt;
&#24102;&#26377;&#31070;&#32463;&#32593;&#32476;&#21644;&#32034;&#24341;&#30340;&#32858;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Clustering with Neural Network and Index. (arXiv:2212.03853v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03853
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24102;&#26377;&#31070;&#32463;&#32593;&#32476;&#21644;&#32034;&#24341;&#30340;&#32858;&#31867;&#27169;&#22411;CNNI&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38750;&#20984;&#24418;&#29366;&#25968;&#25454;&#30340;&#21442;&#25968;&#21270;&#32858;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#27169;&#22411;&#65292;&#31216;&#20026;&#24102;&#26377;&#31070;&#32463;&#32593;&#32476;&#21644;&#32034;&#24341;&#30340;&#32858;&#31867;&#27169;&#22411;&#65288;CNNI&#65289;&#12290;CNNI&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#32858;&#31867;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#27169;&#20223;&#30417;&#30563;&#23398;&#20064;&#65292;&#20351;&#29992;&#20869;&#37096;&#32858;&#31867;&#35780;&#20272;&#25351;&#26631;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#12290;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#39564;&#26469;&#27979;&#35797;&#26032;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#19982;K&#22343;&#20540;&#21644;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#31561;&#20854;&#20182;&#32858;&#31867;&#27169;&#22411;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;CNNI&#21487;&#20197;&#27491;&#30830;&#22320;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#65307;CNNI&#37197;&#22791;&#20102;MMJ-SC&#65292;&#25104;&#20026;&#31532;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38750;&#20984;&#24418;&#29366;&#65288;&#38750;&#24179;&#38754;&#20960;&#20309;&#65289;&#25968;&#25454;&#30340;&#21442;&#25968;&#21270;&#65288;&#24402;&#32435;&#24335;&#65289;&#32858;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new model called Clustering with Neural Network and Index (CNNI) is introduced. CNNI uses a Neural Network to cluster data points. Training of the Neural Network mimics supervised learning, with an internal clustering evaluation index acting as the loss function. An experiment is conducted to test the feasibility of the new model, and compared with results of other clustering models like K-means and Gaussian Mixture Model (GMM). The result shows CNNI can work properly for clustering data; CNNI equipped with MMJ-SC, achieves the first parametric (inductive) clustering model that can deal with non-convex shaped (non-flat geometry) data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31215;&#27969;&#24418;&#36827;&#34892;&#25512;&#26029;&#30340;&#28508;&#22312;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21160;&#24577;&#23398;&#20064;&#38382;&#39064;&#20869;&#22312;&#30340;&#22270;&#32467;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;Riemannian&#20960;&#20309;&#23398;&#21644;&#29983;&#25104;&#26356;&#22797;&#26434;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#21487;&#20197;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#24182;&#20135;&#29983;&#26356;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2211.16199</link><description>&lt;p&gt;
&#21033;&#29992;&#31215;&#27969;&#24418;&#36827;&#34892;&#28508;&#22312;&#22270;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Latent Graph Inference using Product Manifolds. (arXiv:2211.16199v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31215;&#27969;&#24418;&#36827;&#34892;&#25512;&#26029;&#30340;&#28508;&#22312;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21160;&#24577;&#23398;&#20064;&#38382;&#39064;&#20869;&#22312;&#30340;&#22270;&#32467;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;Riemannian&#20960;&#20309;&#23398;&#21644;&#29983;&#25104;&#26356;&#22797;&#26434;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#21487;&#20197;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#24182;&#20135;&#29983;&#26356;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#20381;&#36182;&#20110;&#22270;&#25299;&#25169;&#32467;&#26500;&#23545;&#32593;&#32476;&#30340;&#21487;&#29992;&#24615;&#20197;&#21450;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#26368;&#20248;&#24615;&#12290;&#32780;&#28508;&#22312;&#22270;&#25512;&#26029;&#20801;&#35768;&#27169;&#22411;&#21160;&#24577;&#23398;&#20064;&#38382;&#39064;&#20869;&#22312;&#30340;&#22270;&#32467;&#26500;&#65292;&#36825;&#20123;&#25968;&#25454;&#30340;&#36830;&#36890;&#24615;&#27169;&#24335;&#21487;&#33021;&#26080;&#27861;&#30452;&#25509;&#35775;&#38382;&#12290;&#26412;&#25991;&#23558;&#31163;&#25955;&#21487;&#24494;&#22270;&#27169;&#22359;&#65288;dDGM&#65289;&#25512;&#24191;&#21040;&#28508;&#22312;&#22270;&#23398;&#20064;&#12290;&#21407;&#22987;&#30340;dDGM&#26550;&#26500;&#20351;&#29992;&#27431;&#20960;&#37324;&#24471;&#24179;&#38754;&#26469;&#32534;&#30721;&#28508;&#22312;&#29305;&#24449;&#65292;&#22522;&#20110;&#27492;&#29983;&#25104;&#28508;&#22312;&#22270;&#12290;&#36890;&#36807;&#23558;&#40654;&#26364;&#20960;&#20309;&#24341;&#20837;&#27169;&#22411;&#24182;&#29983;&#25104;&#26356;&#22797;&#26434;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#28508;&#22312;&#22270;&#25512;&#26029;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#19978;&#21487;&#34892;&#30340;&#26041;&#27861;&#26469;&#20135;&#29983;&#24120;&#26354;&#29575;&#27169;&#22411;&#31354;&#38388;&#30340;&#31215;&#27969;&#24418;&#65292;&#21487;&#20197;&#32534;&#30721;&#19981;&#21516;&#32467;&#26500;&#30340;&#28508;&#22312;&#29305;&#24449;&#12290;&#26144;&#23556;&#21040;&#25512;&#26029;&#20986;&#30340;&#31215;&#27969;&#24418;&#19978;&#30340;&#28508;&#22312;&#34920;&#31034;&#29992;&#20110;&#35745;&#31639;&#26356;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks usually rely on the assumption that the graph topology is available to the network as well as optimal for the downstream task. Latent graph inference allows models to dynamically learn the intrinsic graph structure of problems where the connectivity patterns of data may not be directly accessible. In this work, we generalize the discrete Differentiable Graph Module (dDGM) for latent graph learning. The original dDGM architecture used the Euclidean plane to encode latent features based on which the latent graphs were generated. By incorporating Riemannian geometry into the model and generating more complex embedding spaces, we can improve the performance of the latent graph inference system. In particular, we propose a computationally tractable approach to produce product manifolds of constant curvature model spaces that can encode latent features of varying structure. The latent representations mapped onto the inferred product manifold are used to compute richer s
&lt;/p&gt;</description></item><item><title>SSD-LM&#26159;&#19968;&#31181;&#21322;&#33258;&#22238;&#24402;&#30340;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#26102;&#28789;&#27963;&#29983;&#25104;&#25991;&#26412;&#22359;&#24182;&#23454;&#29616;&#26412;&#22320;&#19978;&#19979;&#25991;&#26356;&#26032;&#65292;&#20197;&#21450;&#22312;&#33258;&#28982;&#35789;&#27719;&#31354;&#38388;&#19978;&#36827;&#34892;&#25193;&#25955;&#65292;&#23454;&#29616;&#20102;&#20998;&#31867;&#22120;&#25351;&#23548;&#21644;&#27169;&#22359;&#21270;&#25511;&#21046;&#12290;&#22312;&#26080;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#22522;&#20934;&#19978;&#65292;SSD-LM&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#26174;&#33879;&#36229;&#36234;&#20102;&#20854;&#20182;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.17432</link><description>&lt;p&gt;
SSD-LM: &#22522;&#20110;&#21322;&#33258;&#22238;&#24402;&#31616;&#21333;&#24418;&#24335;&#30340;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#21644;&#27169;&#22359;&#21270;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control. (arXiv:2210.17432v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17432
&lt;/p&gt;
&lt;p&gt;
SSD-LM&#26159;&#19968;&#31181;&#21322;&#33258;&#22238;&#24402;&#30340;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#26102;&#28789;&#27963;&#29983;&#25104;&#25991;&#26412;&#22359;&#24182;&#23454;&#29616;&#26412;&#22320;&#19978;&#19979;&#25991;&#26356;&#26032;&#65292;&#20197;&#21450;&#22312;&#33258;&#28982;&#35789;&#27719;&#31354;&#38388;&#19978;&#36827;&#34892;&#25193;&#25955;&#65292;&#23454;&#29616;&#20102;&#20998;&#31867;&#22120;&#25351;&#23548;&#21644;&#27169;&#22359;&#21270;&#25511;&#21046;&#12290;&#22312;&#26080;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#22522;&#20934;&#19978;&#65292;SSD-LM&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#26174;&#33879;&#36229;&#36234;&#20102;&#20854;&#20182;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#36830;&#32493;&#20540;&#39046;&#22495;&#65288;&#22914;&#22270;&#20687;&#65289;&#21462;&#24471;&#20102;&#24456;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#31163;&#25955;&#39046;&#22495;&#65288;&#22914;&#25991;&#26412;&#65289;&#20013;&#65292;&#31867;&#20284;&#30340;&#21162;&#21147;&#21364;&#36824;&#27809;&#26377;&#36798;&#21040;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SSD-LM&#65292;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#20004;&#20010;&#20851;&#38190;&#35774;&#35745;&#36873;&#25321;&#12290;&#39318;&#20808;&#65292;SSD-LM&#26159;&#21322;&#33258;&#22238;&#24402;&#30340;&#65292;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#25991;&#26412;&#22359;&#65292;&#22312;&#35299;&#30721;&#26102;&#20801;&#35768;&#28789;&#27963;&#30340;&#36755;&#20986;&#38271;&#24230;&#65292;&#21516;&#26102;&#23454;&#29616;&#26412;&#22320;&#21452;&#21521;&#19978;&#19979;&#25991;&#26356;&#26032;&#12290;&#20854;&#27425;&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;&#21333;&#32431;&#24418;&#30340;&#26041;&#27861;&#65292;&#22312;&#33258;&#28982;&#35789;&#27719;&#31354;&#38388;&#19978;&#36827;&#34892;&#25193;&#25955;&#65292;&#32780;&#19981;&#26159;&#22312;&#23398;&#20064;&#21040;&#30340;&#28508;&#22312;&#31354;&#38388;&#19978;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#20219;&#20309;&#33258;&#36866;&#24212;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#29616;&#25104;&#30340;&#20998;&#31867;&#22120;&#23454;&#29616;&#20998;&#31867;&#22120;&#25351;&#23548;&#21644;&#27169;&#22359;&#21270;&#25511;&#21046;&#12290;&#25105;&#20204;&#22312;&#26080;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102; SSD-LM&#65292;&#24182;&#23637;&#31034;&#23427;&#22312;&#26631;&#20934;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#24230;&#37327;&#19978;&#19982;&#24378;&#22823;&#30340;&#33258;&#22238;&#24402; GPT-2 &#27169;&#22411;&#30456;&#21305;&#37197;&#25110;&#36229;&#36234;&#65292;&#24182;&#19988;&#36828;&#36828;&#20248;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models. In this work, we present SSD-LM -- a diffusion-based language model with two key design choices. First, SSD-LM is semi-autoregressive, iteratively generating blocks of text, allowing for flexible output length at decoding time while enabling local bidirectional context updates. Second, it is simplex-based, performing diffusion on the natural vocabulary space rather than a learned latent space, allowing us to incorporate classifier guidance and modular control using off-the-shelf classifiers without any adaptation. We evaluate SSD-LM on unconstrained text generation benchmarks, and show that it matches or outperforms strong autoregressive GPT-2 models across standard quality and diversity metrics, while vastly outperforming diffusion-based baselines. On controlled text generatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23454;&#29616;&#20102;&#19968;&#20010;&#21517;&#20026;ReLoD&#30340;&#23454;&#26102;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#26412;&#22320;&#21644;&#36828;&#31243;&#35745;&#31639;&#26426;&#26469;&#20998;&#37197;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#35745;&#31639;&#65292;&#24182;&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#25511;&#21046;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;SAC&#30340;&#24615;&#33021;&#19979;&#38477;&#20102;&#12290;</title><link>http://arxiv.org/abs/2210.02317</link><description>&lt;p&gt;
&#23454;&#26102;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#22522;&#20110;&#35270;&#35273;&#30340;&#26426;&#22120;&#20154;&#65292;&#21033;&#29992;&#26412;&#22320;&#21644;&#36828;&#31243;&#35745;&#31639;&#26426;
&lt;/p&gt;
&lt;p&gt;
Real-Time Reinforcement Learning for Vision-Based Robotics Utilizing Local and Remote Computers. (arXiv:2210.02317v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23454;&#29616;&#20102;&#19968;&#20010;&#21517;&#20026;ReLoD&#30340;&#23454;&#26102;&#23398;&#20064;&#31995;&#32479;&#65292;&#21033;&#29992;&#26412;&#22320;&#21644;&#36828;&#31243;&#35745;&#31639;&#26426;&#26469;&#20998;&#37197;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#35745;&#31639;&#65292;&#24182;&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#25511;&#21046;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;SAC&#30340;&#24615;&#33021;&#19979;&#38477;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#23398;&#20064;&#23545;&#20110;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#38750;&#31283;&#23450;&#29615;&#22659;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#35774;&#32622;&#26159;&#21516;&#26102;&#26377;&#20004;&#21488;&#19981;&#21516;&#30340;&#35745;&#31639;&#26426;&#65306;&#19982;&#26426;&#22120;&#20154;&#30456;&#36830;&#30340;&#36164;&#28304;&#21463;&#38480;&#26412;&#22320;&#35745;&#31639;&#26426;&#21644;&#26080;&#32447;&#36830;&#25509;&#30340;&#24378;&#22823;&#36828;&#31243;&#35745;&#31639;&#26426;&#12290;&#22312;&#36825;&#26679;&#30340;&#35774;&#32622;&#19979;&#65292;&#23578;&#19981;&#28165;&#26970;&#23398;&#20064;&#31995;&#32479;&#30340;&#24615;&#33021;&#22312;&#36164;&#28304;&#38480;&#21046;&#26041;&#38754;&#33021;&#21463;&#21040;&#22810;&#22823;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#22914;&#20309;&#39640;&#25928;&#21033;&#29992;&#26080;&#32447;&#36830;&#25509;&#30340;&#24378;&#22823;&#35745;&#31639;&#26426;&#26469;&#24357;&#34917;&#24615;&#33021;&#25439;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#23454;&#26102;&#23398;&#20064;&#31995;&#32479;&#65292;&#31216;&#20026;Remote-Local Distributed (ReLoD)&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#26412;&#22320;&#21644;&#36828;&#31243;&#35745;&#31639;&#26426;&#20043;&#38388;&#20998;&#37197;&#20004;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;Soft Actor-Critic (SAC)&#21644;Proximal Policy Optimization (PPO)&#30340;&#35745;&#31639;&#12290;&#35813;&#31995;&#32479;&#30340;&#24615;&#33021;&#22312;&#20351;&#29992;&#26426;&#26800;&#33218;&#21644;&#31227;&#21160;&#26426;&#22120;&#20154;&#24320;&#21457;&#30340;&#20004;&#20010;&#22522;&#20110;&#35270;&#35273;&#30340;&#25511;&#21046;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;SAC&#30340;&#24615;&#33021;&#19979;&#38477;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time learning is crucial for robotic agents adapting to ever-changing, non-stationary environments. A common setup for a robotic agent is to have two different computers simultaneously: a resource-limited local computer tethered to the robot and a powerful remote computer connected wirelessly. Given such a setup, it is unclear to what extent the performance of a learning system can be affected by resource limitations and how to efficiently use the wirelessly connected powerful computer to compensate for any performance loss. In this paper, we implement a real-time learning system called the Remote-Local Distributed (ReLoD) system to distribute computations of two deep reinforcement learning (RL) algorithms, Soft Actor-Critic (SAC) and Proximal Policy Optimization (PPO), between a local and a remote computer. The performance of the system is evaluated on two vision-based control tasks developed using a robotic arm and a mobile robot. Our results show that SAC's performance degrades
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#24102;&#26377;&#23545;&#27604;&#23398;&#20064;&#30340;&#20840;&#33258;&#20027;&#20998;&#24067;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#26816;&#27979;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#21644;&#23545;&#25239;&#24615;&#25200;&#21160;&#26679;&#26412;&#12290;&#36890;&#36807;&#23558;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#19982;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;CADet&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#21516;&#19968;&#26679;&#26412;&#30340;&#23545;&#27604;&#21464;&#25442;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#36827;&#34892;OOD&#26816;&#27979;&#65292;&#24182;&#22312;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#35782;&#21035;&#26041;&#38754;&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2210.01742</link><description>&lt;p&gt;
CADet:&#20840;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#24102;&#26377;&#23545;&#27604;&#23398;&#20064;&#30340;&#20840;&#33258;&#20027;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CADet: Fully Self-Supervised Out-Of-Distribution Detection With Contrastive Learning. (arXiv:2210.01742v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#24102;&#26377;&#23545;&#27604;&#23398;&#20064;&#30340;&#20840;&#33258;&#20027;&#20998;&#24067;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#26816;&#27979;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#21644;&#23545;&#25239;&#24615;&#25200;&#21160;&#26679;&#26412;&#12290;&#36890;&#36807;&#23558;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#19982;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;CADet&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#21516;&#19968;&#26679;&#26412;&#30340;&#23545;&#27604;&#21464;&#25442;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#36827;&#34892;OOD&#26816;&#27979;&#65292;&#24182;&#22312;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#35782;&#21035;&#26041;&#38754;&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#24102;&#26377;&#20998;&#24067;&#20043;&#22806;&#65288;OOD&#65289;&#26679;&#26412;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#29616;&#23454;&#19990;&#30028;&#37096;&#32626;&#20013;&#30340;&#20027;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21516;&#26102;&#26816;&#27979;&#20004;&#31181;&#31867;&#22411;&#30340;OOD&#26679;&#26412;&#65306;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#21644;&#23545;&#25239;&#24615;&#25200;&#21160;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#19982;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#21452;&#26679;&#26412;&#26816;&#39564;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#40065;&#26834;&#22320;&#27979;&#35797;&#20004;&#20010;&#29420;&#31435;&#26679;&#26412;&#38598;&#26159;&#21542;&#26469;&#33258;&#30456;&#21516;&#30340;&#20998;&#24067;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#20102;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#23427;&#22312;&#21306;&#20998;CIFAR-10&#21644;CIFAR-10.1&#26102;&#20855;&#26377;&#26356;&#39640;&#30340;&#32622;&#20449;&#24230;&#12290;&#22312;&#27492;&#25104;&#21151;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CADet&#65288;&#23545;&#27604;&#24322;&#24120;&#26816;&#27979;&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#21333;&#26679;&#26412;OOD&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;CADet&#20511;&#37492;&#20102;MMD&#30340;&#24605;&#24819;&#65292;&#20294;&#21033;&#29992;&#20102;&#21516;&#19968;&#26679;&#26412;&#30340;&#23545;&#27604;&#21464;&#25442;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;CADet&#22312;&#35782;&#21035;&#34987;&#23545;&#25239;&#24615;&#25200;&#21160;&#24178;&#25200;&#30340;&#26679;&#26412;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#30340;&#23545;&#25239;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Handling out-of-distribution (OOD) samples has become a major stake in the real-world deployment of machine learning systems. This work explores the use of self-supervised contrastive learning to the simultaneous detection of two types of OOD samples: unseen classes and adversarial perturbations. First, we pair self-supervised contrastive learning with the maximum mean discrepancy (MMD) two-sample test. This approach enables us to robustly test whether two independent sets of samples originate from the same distribution, and we demonstrate its effectiveness by discriminating between CIFAR-10 and CIFAR-10.1 with higher confidence than previous work. Motivated by this success, we introduce CADet (Contrastive Anomaly Detection), a novel method for OOD detection of single samples. CADet draws inspiration from MMD, but leverages the similarity between contrastive transformations of a same sample. CADet outperforms existing adversarial detection methods in identifying adversarially perturbed
&lt;/p&gt;</description></item><item><title>MGG&#26159;&#19968;&#31181;&#36719;&#20214;&#27969;&#27700;&#32447;&#35774;&#35745;&#65292;&#21487;&#22312;&#22810;GPU&#24179;&#21488;&#19978;&#21152;&#36895;GNNs&#65292;&#36890;&#36807;&#37319;&#29992;GNN&#29305;&#27530;&#30340;&#27969;&#27700;&#32447;&#26500;&#24314;&#21644;GPU&#24863;&#30693;&#30340;&#27969;&#27700;&#32447;&#26144;&#23556;&#65292;&#23454;&#29616;&#31934;&#32454;&#35745;&#31639;&#36890;&#20449;&#37325;&#21472;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.06800</link><description>&lt;p&gt;
MGG: &#22810;GPU&#24179;&#21488;&#19978;&#36890;&#36807;&#31934;&#32454;&#30340;&#20869;&#26680;&#36890;&#20449;&#35745;&#31639;&#27969;&#27700;&#32447;&#21152;&#36895;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MGG: Accelerating Graph Neural Networks with Fine-grained intra-kernel Communication-Computation Pipelining on Multi-GPU Platforms. (arXiv:2209.06800v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06800
&lt;/p&gt;
&lt;p&gt;
MGG&#26159;&#19968;&#31181;&#36719;&#20214;&#27969;&#27700;&#32447;&#35774;&#35745;&#65292;&#21487;&#22312;&#22810;GPU&#24179;&#21488;&#19978;&#21152;&#36895;GNNs&#65292;&#36890;&#36807;&#37319;&#29992;GNN&#29305;&#27530;&#30340;&#27969;&#27700;&#32447;&#26500;&#24314;&#21644;GPU&#24863;&#30693;&#30340;&#27969;&#27700;&#32447;&#26144;&#23556;&#65292;&#23454;&#29616;&#31934;&#32454;&#35745;&#31639;&#36890;&#20449;&#37325;&#21472;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36755;&#20837;&#22270;&#30340;&#22823;&#23567;&#36234;&#26469;&#36234;&#22823;&#65292;&#38656;&#35201;&#20351;&#29992;&#22810;GPU&#24179;&#21488;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;GPU GNN&#31995;&#32479;&#20165;&#22522;&#20110;&#20256;&#32479;&#20570;&#27861;&#32553;&#25918;&#31264;&#23494;DNN&#65292;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#25805;&#20316;&#65292;&#23545;&#20110;&#19981;&#35268;&#21017;&#31232;&#30095;&#30340;GNN&#24037;&#20316;&#36127;&#36733;&#65292;&#32570;&#22833;&#21516;&#26102;&#35843;&#24230;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#25805;&#20316;&#20197;&#25552;&#39640;&#24615;&#33021;&#30340;&#26426;&#20250;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MGG&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#31995;&#32479;&#35774;&#35745;&#65292;&#21487;&#20197;&#22312;&#22810;GPU&#24179;&#21488;&#19978;&#21152;&#36895;&#20840;&#22270;GNN&#12290;MGG&#30340;&#26680;&#24515;&#26159;&#20854;&#26032;&#39062;&#30340;&#21160;&#24577;&#36719;&#20214;&#27969;&#27700;&#32447;&#65292;&#20197;&#20419;&#36827;GPU&#20869;&#37096;&#31934;&#32454;&#30340;&#35745;&#31639;&#36890;&#20449;&#37325;&#21472;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MGG&#24341;&#20837;&#20102;&#36866;&#29992;&#20110;GNN&#30340;&#27969;&#27700;&#32447;&#26500;&#24314;&#21644;GPU&#24863;&#30693;&#30340;&#27969;&#27700;&#32447;&#26144;&#23556;&#65292;&#20197;&#20419;&#36827;&#24037;&#20316;&#36127;&#36733;&#24179;&#34913;&#21644;&#25805;&#20316;&#37325;&#21472;&#12290;MGG&#36824;&#32467;&#21512;&#20102;&#26234;&#33021;&#30340;&#36816;&#34892;&#26102;&#35774;&#35745;&#21644;&#20998;&#26512;&#24314;&#27169;&#21644;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20197;&#21160;&#24577;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing size of input graphs for graph neural networks (GNNs) highlights the demand for using multi-GPU platforms. However, existing multi-GPU GNN systems optimize the computation and communication individually based on the conventional practice of scaling dense DNNs. For irregularly sparse and fine-grained GNN workloads, such solutions miss the opportunity to jointly schedule/optimize the computation and communication operations for high-performance delivery. To this end, we propose MGG, a novel system design to accelerate full-graph GNNs on multi-GPU platforms. The core of MGG is its novel dynamic software pipeline to facilitate fine-grained computation-communication overlapping within a GPU kernel. Specifically, MGG introduces GNN-tailored pipeline construction and GPU-aware pipeline mapping to facilitate workload balancing and operation overlapping. MGG also incorporates an intelligent runtime design with analytical modeling and optimization heuristics to dynamically improve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20869;&#22312;&#22870;&#21169;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#24403;&#21069;&#35266;&#23519;&#19982;&#21382;&#21490;&#30693;&#35782;&#30340;&#24046;&#24322;&#26469;&#35780;&#20272;&#22909;&#22855;&#24515;&#65292;&#24182;&#21033;&#29992;&#26102;&#38388;&#19981;&#19968;&#33268;&#24615;&#20316;&#20026;&#20869;&#22312;&#22870;&#21169;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#31232;&#30095;&#22806;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#22122;&#22768;&#23481;&#24525;&#24230;&#12290;</title><link>http://arxiv.org/abs/2208.11361</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#19981;&#19968;&#33268;&#30340;&#33258;&#30417;&#30563;&#25506;&#32034;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Exploration via Temporal Inconsistency in Reinforcement Learning. (arXiv:2208.11361v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20869;&#22312;&#22870;&#21169;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#24403;&#21069;&#35266;&#23519;&#19982;&#21382;&#21490;&#30693;&#35782;&#30340;&#24046;&#24322;&#26469;&#35780;&#20272;&#22909;&#22855;&#24515;&#65292;&#24182;&#21033;&#29992;&#26102;&#38388;&#19981;&#19968;&#33268;&#24615;&#20316;&#20026;&#20869;&#22312;&#22870;&#21169;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#31232;&#30095;&#22806;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#22122;&#22768;&#23481;&#24525;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31232;&#30095;&#22806;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#23613;&#31649;&#23545;&#36825;&#20010;&#39046;&#22495;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#21152;&#12290;&#20808;&#21069;&#30340;&#23581;&#35797;&#34920;&#26126;&#65292;&#20869;&#22312;&#22870;&#21169;&#21487;&#20197;&#32531;&#35299;&#31232;&#30095;&#24615;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#20154;&#31867;&#23398;&#20064;&#21551;&#21457;&#30340;&#26032;&#22411;&#20869;&#22312;&#22870;&#21169;&#65292;&#20154;&#31867;&#36890;&#36807;&#23558;&#24403;&#21069;&#35266;&#23519;&#19982;&#21382;&#21490;&#30693;&#35782;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#22909;&#22855;&#24515;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#35757;&#32451;&#19968;&#20010;&#33258;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#65292;&#20445;&#23384;&#27169;&#22411;&#21442;&#25968;&#30340;&#24555;&#29031;&#65292;&#24182;&#20351;&#29992;&#26680;&#33539;&#25968;&#26469;&#35780;&#20272;&#19981;&#21516;&#24555;&#29031;&#20043;&#38388;&#39044;&#27979;&#30340;&#26102;&#38388;&#19981;&#19968;&#33268;&#24615;&#20316;&#20026;&#20869;&#22312;&#22870;&#21169;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#21152;&#26435;&#26426;&#21046;&#65292;&#20197;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#20026;&#19981;&#21516;&#30340;&#24555;&#29031;&#20998;&#37197;&#26435;&#37325;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#29615;&#22659;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#27809;&#26377;&#39069;&#22806;&#35757;&#32451;&#25104;&#26412;&#19988;&#20855;&#26377;&#26356;&#39640;&#22122;&#22768;&#23481;&#24525;&#24230;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;&#20869;&#22312;&#22870;&#21169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Under sparse extrinsic reward settings, reinforcement learning has remained challenging, despite surging interests in this field. Previous attempts suggest that intrinsic reward can alleviate the issue caused by sparsity. In this article, we present a novel intrinsic reward that is inspired by human learning, as humans evaluate curiosity by comparing current observations with historical knowledge. Our method involves training a self-supervised prediction model, saving snapshots of the model parameters, and using nuclear norm to evaluate the temporal inconsistency between the predictions of different snapshots as intrinsic rewards. We also propose a variational weighting mechanism to assign weight to different snapshots in an adaptive manner. Our experimental results on various benchmark environments demonstrate the efficacy of our method, which outperforms other intrinsic reward-based methods without additional training costs and with higher noise tolerance. This work has been submitte
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#24037;&#20316;&#27969;&#20013;&#24322;&#27493;&#20219;&#21153;&#25191;&#34892;&#30340;&#35201;&#27714;&#21644;&#29305;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#30830;&#23450;&#24322;&#27493;&#25191;&#34892;&#25910;&#30410;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#36890;&#36807;&#22312;Summit&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#24322;&#27493;&#25191;&#34892;&#23545;&#24615;&#33021;&#30340;&#22686;&#24378;&#19982;&#27169;&#22411;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2208.11069</link><description>&lt;p&gt;
&#24322;&#26500;&#20219;&#21153;&#22312;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#24037;&#20316;&#27969;&#20013;&#30340;&#24322;&#27493;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Execution of Heterogeneous Tasks in ML-driven HPC Workflows. (arXiv:2208.11069v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11069
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#24037;&#20316;&#27969;&#20013;&#24322;&#27493;&#20219;&#21153;&#25191;&#34892;&#30340;&#35201;&#27714;&#21644;&#29305;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#30830;&#23450;&#24322;&#27493;&#25191;&#34892;&#25910;&#30410;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#36890;&#36807;&#22312;Summit&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#24322;&#27493;&#25191;&#34892;&#23545;&#24615;&#33021;&#30340;&#22686;&#24378;&#19982;&#27169;&#22411;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#31185;&#23398;&#24037;&#20316;&#27969;&#30001;&#35768;&#22810;&#31867;&#22411;&#30340;&#20219;&#21153;&#32452;&#25104;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#22312;&#24322;&#26500;&#36164;&#28304;&#19978;&#25191;&#34892;&#12290;&#24322;&#27493;&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#23545;&#20110;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#12289;&#20219;&#21153;&#21534;&#21520;&#37327;&#21644;&#20943;&#23569;&#24037;&#20316;&#27969;&#30340;&#26102;&#38388;&#38750;&#24120;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#33021;&#22815;&#22312;&#24322;&#26500;&#36164;&#28304;&#19978;&#35843;&#24230;&#21644;&#25191;&#34892;&#19981;&#21516;&#31867;&#22411;&#20219;&#21153;&#30340;&#20013;&#38388;&#20214;&#24517;&#39035;&#25903;&#25345;&#24322;&#27493;&#25191;&#34892;&#20219;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#24037;&#20316;&#27969;&#30340;&#24322;&#27493;&#20219;&#21153;&#25191;&#34892;&#30340;&#35201;&#27714;&#21644;&#29305;&#24615;&#12290;&#25105;&#20204;&#27169;&#22411;&#21270;&#20102;&#20219;&#24847;&#24037;&#20316;&#27969;&#30340;&#24322;&#27493;&#24615;&#31243;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#29992;&#20110;&#30830;&#23450;&#24322;&#27493;&#25191;&#34892;&#24102;&#26469;&#30340;&#23450;&#24615;&#25910;&#30410;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20195;&#34920;&#20102;&#30456;&#20851;&#31185;&#23398;&#39537;&#21160;&#65292;&#25105;&#20204;&#22312;Summit&#19978;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;&#20110;&#24322;&#27493;&#25191;&#34892;&#25152;&#24102;&#26469;&#30340;&#24615;&#33021;&#22686;&#24378;&#19982;&#25105;&#20204;&#30340;&#27169;&#22411;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous scientific workflows consist of numerous types of tasks that require executing on heterogeneous resources. Asynchronous execution of those tasks is crucial to improve resource utilization, task throughput and reduce workflows' makespan. Therefore, middleware capable of scheduling and executing different task types across heterogeneous resources must enable asynchronous execution of tasks. In this paper, we investigate the requirements and properties of the asynchronous task execution of machine learning (ML)-driven high performance computing (HPC) workflows. We model the degree of asynchronicity permitted for arbitrary workflows and propose key metrics that can be used to determine qualitative benefits when employing asynchronous execution. Our experiments represent relevant scientific drivers, we perform them at scale on Summit, and we show that the performance enhancements due to asynchronous execution are consistent with our model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#20214;&#35302;&#21457;&#31639;&#27861;ET-GP-UCB&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#21464;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#27010;&#29575;&#22343;&#21248;&#35823;&#24046;&#30028;&#65292;&#31639;&#27861;&#33021;&#22815;&#22312;&#26410;&#30693;&#21464;&#21270;&#36895;&#29575;&#30340;&#24773;&#20917;&#19979;&#33258;&#36866;&#24212;&#22320;&#36866;&#24212;&#23454;&#38469;&#30340;&#26102;&#38388;&#21464;&#21270;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ET-GP-UCB&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.10790</link><description>&lt;p&gt;
&#20107;&#20214;&#35302;&#21457;&#30340;&#26102;&#21464;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Event-Triggered Time-Varying Bayesian Optimization. (arXiv:2208.10790v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#20214;&#35302;&#21457;&#31639;&#27861;ET-GP-UCB&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#21464;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#27010;&#29575;&#22343;&#21248;&#35823;&#24046;&#30028;&#65292;&#31639;&#27861;&#33021;&#22815;&#22312;&#26410;&#30693;&#21464;&#21270;&#36895;&#29575;&#30340;&#24773;&#20917;&#19979;&#33258;&#36866;&#24212;&#22320;&#36866;&#24212;&#23454;&#38469;&#30340;&#26102;&#38388;&#21464;&#21270;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ET-GP-UCB&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#26102;&#21464;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;TVBO&#65289;&#39034;&#24207;&#20248;&#21270;&#26102;&#21464;&#30446;&#26631;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#20851;&#38190;&#25361;&#25112;&#26159;&#22312;&#26102;&#38388;&#21464;&#21270;&#19979;&#30340;&#21208;&#25506;&#19982;&#24320;&#21457;&#30340;&#26435;&#34913;&#12290;&#24403;&#21069;&#30340;TVBO&#26041;&#27861;&#38656;&#35201;&#23545;&#21464;&#21270;&#36895;&#29575;&#26377;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#21464;&#21270;&#36895;&#29575;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#20214;&#35302;&#21457;&#31639;&#27861;ET-GP-UCB&#65292;&#23427;&#23558;&#20248;&#21270;&#38382;&#39064;&#35270;&#20026;&#38745;&#24577;&#38382;&#39064;&#65292;&#30452;&#21040;&#22312;&#32447;&#26816;&#27979;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340;&#21464;&#21270;&#24182;&#37325;&#32622;&#25968;&#25454;&#38598;&#12290;&#36825;&#20351;&#24471;&#31639;&#27861;&#33021;&#22815;&#36866;&#24212;&#23454;&#38469;&#30340;&#26102;&#38388;&#21464;&#21270;&#65292;&#32780;&#19981;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#12290;&#20107;&#20214;&#35302;&#21457;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20013;&#20351;&#29992;&#30340;&#27010;&#29575;&#22343;&#21248;&#35823;&#24046;&#30028;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;ET-GP-UCB&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;ET-GP-UCB&#21487;&#24191;&#27867;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#35774;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of sequentially optimizing a time-varying objective function using time-varying Bayesian optimization (TVBO). Here, the key challenge is the exploration-exploitation trade-off under time variations. Current approaches to TVBO require prior knowledge of a constant rate of change. However, in practice, the rate of change is usually unknown. We propose an event-triggered algorithm, ET-GP-UCB, that treats the optimization problem as static until it detects changes in the objective function online and then resets the dataset. This allows the algorithm to adapt to realized temporal changes without the need for prior knowledge. The event-trigger is based on probabilistic uniform error bounds used in Gaussian process regression. We provide regret bounds for ET-GP-UCB and show in numerical experiments that it outperforms state-of-the-art algorithms on synthetic and real-world data. Furthermore, these results demonstrate that ET-GP-UCB is readily applicable to various set
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#21387;&#32553;&#26041;&#27861;&#23545;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#32676;&#20307;&#12289;&#24615;&#21035;&#21644;&#35821;&#20041;&#20559;&#24046;&#26041;&#38754;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2205.10828</link><description>&lt;p&gt;
&#21387;&#32553;&#22411;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20250;&#24573;&#30053;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Do Compressed Multilingual Machine Translation Models Forget?. (arXiv:2205.10828v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#21387;&#32553;&#26041;&#27861;&#23545;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#32676;&#20307;&#12289;&#24615;&#21035;&#21644;&#35821;&#20041;&#20559;&#24046;&#26041;&#38754;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38750;&#24120;&#24222;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#22823;&#23567;&#20351;&#24471;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#24212;&#29992;&#23427;&#20204;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#21387;&#32553;&#25216;&#26415;&#21487;&#20197;&#22823;&#24133;&#20943;&#23567;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#20174;&#32780;&#20943;&#23569;&#25512;&#29702;&#26102;&#38388;&#65292;&#24182;&#23545;&#39030;&#32423;&#25351;&#26631;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#23545;&#22810;&#20010;&#20219;&#21153;&#21644;/&#25110;&#35821;&#35328;&#36827;&#34892;&#24179;&#22343;&#30340;&#32508;&#21512;&#24615;&#33021;&#21487;&#33021;&#25513;&#30422;&#20102;&#22312;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#21151;&#33021;&#19978;&#30340;&#20005;&#37325;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#25152;&#32534;&#30721;&#30340;&#20559;&#35265;&#30340;&#25918;&#22823;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#19981;&#21516;&#26426;&#22120;&#32763;&#35793;&#22522;&#20934;&#65288;FLORES-101&#12289;MT-Gender&#21644;DiBiMT&#65289;&#19978;&#30340;&#21387;&#32553;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#21387;&#32553;&#26041;&#27861;&#23545;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65288;MNMT&#65289;&#22312;&#19981;&#21516;&#35821;&#35328;&#32676;&#20307;&#12289;&#24615;&#21035;&#21644;&#35821;&#20041;&#20559;&#24046;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#32780;&#24179;&#22343;BLEU&#24230;&#37327;&#20540;&#21017;&#27809;&#20160;&#20040;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, very large pre-trained models achieve state-of-the-art results in various natural language processing (NLP) tasks, but their size makes it more challenging to apply them in resource-constrained environments. Compression techniques allow to drastically reduce the size of the models and therefore their inference time with negligible impact on top-tier metrics. However, the general performance averaged across multiple tasks and/or languages may hide a drastic performance drop on under-represented features, which could result in the amplification of biases encoded by the models. In this work, we assess the impact of compression methods on Multilingual Neural Machine Translation models (MNMT) for various language groups, gender, and semantic biases by extensive analysis of compressed models on different machine translation benchmarks, i.e. FLORES-101, MT-Gender, and DiBiMT. We show that the performance of under-represented languages drops significantly, while the average BLEU metr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28857;&#31215;&#20302;&#31209;&#36817;&#20284;&#26469;&#34920;&#31034;Q&#20540;&#30340;&#21452;&#32447;&#24615;&#20998;&#35299;&#26041;&#27861;&#65292;&#20854;&#20013;&#31532;&#19968;&#20010;&#21521;&#37327;&#22330;&#25429;&#25417;&#29366;&#24577;&#30340;&#23616;&#37096;&#21160;&#24577;&#65292;&#31532;&#20108;&#20010;&#37096;&#20998;&#25429;&#25417;&#24403;&#21069;&#29366;&#24577;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#20840;&#23616;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.13695</link><description>&lt;p&gt;
&#21452;&#32447;&#24615;&#20215;&#20540;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bilinear value networks. (arXiv:2204.13695v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.13695
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28857;&#31215;&#20302;&#31209;&#36817;&#20284;&#26469;&#34920;&#31034;Q&#20540;&#30340;&#21452;&#32447;&#24615;&#20998;&#35299;&#26041;&#27861;&#65292;&#20854;&#20013;&#31532;&#19968;&#20010;&#21521;&#37327;&#22330;&#25429;&#25417;&#29366;&#24577;&#30340;&#23616;&#37096;&#21160;&#24577;&#65292;&#31532;&#20108;&#20010;&#37096;&#20998;&#25429;&#25417;&#24403;&#21069;&#29366;&#24577;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#20840;&#23616;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#27969;&#26694;&#26550;&#28041;&#21450;&#20272;&#35745;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;Q&#20540;&#20989;&#25968;&#12290;&#24403;&#23398;&#20064;&#23454;&#29616;&#22810;&#20010;&#19981;&#21516;&#30446;&#26631;&#26102;&#65292;&#25968;&#25454;&#25928;&#29575;&#19982;Q&#20989;&#25968;&#23545;&#26032;&#30446;&#26631;&#30340;&#27867;&#21270;&#23494;&#20999;&#30456;&#20851;&#12290;&#30446;&#21069;&#30340;&#33539;&#24335;&#26159;&#20351;&#29992;&#21333;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;Q(s, a, g)&#12290;&#20026;&#20102;&#25913;&#36827;Q&#20989;&#25968;&#30340;&#27867;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#32447;&#24615;&#20998;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#21521;&#37327;&#22330;&#20043;&#38388;&#30340;&#28857;&#31215;&#30340;&#20302;&#31209;&#36817;&#20284;&#26469;&#34920;&#31034;Q&#20540;&#12290;&#31532;&#19968;&#20010;&#21521;&#37327;&#22330;f(s, a)&#25429;&#25417;&#29366;&#24577;s&#22788;&#30340;&#29615;&#22659;&#23616;&#37096;&#21160;&#24577;&#65307;&#32780;&#31532;&#20108;&#20010;&#37096;&#20998;{&#981;}(s, g)&#21017;&#25429;&#25417;&#24403;&#21069;&#29366;&#24577;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#20840;&#23616;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21452;&#32447;&#24615;&#20998;&#35299;&#26041;&#26696;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#30456;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#22788;&#20110;&#20998;&#24067;&#33539;&#22260;&#20043;&#22806;&#30340;&#30446;&#26631;&#20855;&#26377;&#26356;&#22909;&#30340;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#30340;Fetch&#26426;&#22120;&#20154;&#20219;&#21153;&#22871;&#20214;&#21644;DeepMind Control Suite&#19978;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant framework for off-policy multi-goal reinforcement learning involves estimating goal conditioned Q-value function. When learning to achieve multiple goals, data efficiency is intimately connected with the generalization of the Q-function to new goals. The de-facto paradigm is to approximate Q(s, a, g) using monolithic neural networks. To improve the generalization of the Q-function, we propose a bilinear decomposition that represents the Q-value via a low-rank approximation in the form of a dot product between two vector fields. The first vector field, f(s, a), captures the environment's local dynamics at the state s; whereas the second component, {\phi}(s, g), captures the global relationship between the current state and the goal. We show that our bilinear decomposition scheme substantially improves data efficiency, and has superior transfer to out-of-distribution goals compared to prior methods. Empirical evidence is provided on the simulated Fetch robot task-suite and d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25299;&#25169;&#32463;&#39564;&#22238;&#25918;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22270;&#26469;&#26126;&#30830;&#29366;&#24577;&#30340; Q &#20540;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#37319;&#26679;&#31574;&#30053;&#24573;&#35270;&#29366;&#24577;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#28145;&#24230; Q &#20989;&#25968;&#26102;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.15845</link><description>&lt;p&gt;
&#25299;&#25169;&#32463;&#39564;&#22238;&#25918;
&lt;/p&gt;
&lt;p&gt;
Topological Experience Replay. (arXiv:2203.15845v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.15845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25299;&#25169;&#32463;&#39564;&#22238;&#25918;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22270;&#26469;&#26126;&#30830;&#29366;&#24577;&#30340; Q &#20540;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#37319;&#26679;&#31574;&#30053;&#24573;&#35270;&#29366;&#24577;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#28145;&#24230; Q &#20989;&#25968;&#26102;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#28145;&#24230; Q &#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#20174;&#32463;&#39564;&#37325;&#25918;&#32531;&#20914;&#21306;&#20013;&#37319;&#26679;&#30340;&#29366;&#24577;&#36716;&#25442;&#20803;&#32452;&#26356;&#26032; Q &#20540;&#12290;&#36825;&#31181;&#31574;&#30053;&#36890;&#24120;&#22343;&#21248;&#21644;&#38543;&#26426;&#22320;&#37319;&#26679;&#65292;&#25110;&#22522;&#20110;&#35832;&#22914;&#26102;&#38388;&#24046;&#65288;TD&#65289;&#35823;&#24046;&#31561;&#24230;&#37327;&#20248;&#20808;&#12290;&#36825;&#26679;&#30340;&#37319;&#26679;&#31574;&#30053;&#22312;&#23398;&#20064; Q &#20989;&#25968;&#26102;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#19968;&#20010;&#29366;&#24577;&#30340; Q &#20540;&#21462;&#20915;&#20110;&#32487;&#25215;&#29366;&#24577;&#30340; Q &#20540;&#12290;&#22914;&#26524;&#25968;&#25454;&#37319;&#26679;&#31574;&#30053;&#24573;&#30053;&#20102;&#19979;&#19968;&#20010;&#29366;&#24577;&#30340; Q &#20540;&#20272;&#35745;&#30340;&#31934;&#24230;&#65292;&#23427;&#21487;&#33021;&#20250;&#23548;&#33268;&#26080;&#29992;&#21644;&#24120;&#24120;&#19981;&#27491;&#30830;&#30340; Q &#20540;&#26356;&#26032;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26234;&#33021;&#20307;&#30340;&#32463;&#39564;&#32452;&#32455;&#25104;&#19968;&#20010;&#22270;&#65292;&#26126;&#30830;&#36319;&#36394;&#29366;&#24577;&#30340; Q &#20540;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22270;&#20013;&#30340;&#27599;&#26465;&#36793;&#20195;&#34920;&#36890;&#36807;&#25191;&#34892;&#21333;&#20010;&#25805;&#20316;&#22312;&#20004;&#20010;&#29366;&#24577;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#19968;&#32452;&#32456;&#31471;&#29366;&#24577;&#24320;&#22987;&#25193;&#23637;&#22270;&#20013;&#30340;&#39030;&#28857;&#65292;&#24182;&#36880;&#27493;&#21521;&#21518;&#31227;&#21160;&#30340;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#26469;&#25191;&#34892;&#20540;&#22791;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art deep Q-learning methods update Q-values using state transition tuples sampled from the experience replay buffer. This strategy often uniformly and randomly samples or prioritizes data sampling based on measures such as the temporal difference (TD) error. Such sampling strategies can be inefficient at learning Q-function because a state's Q-value depends on the Q-value of successor states. If the data sampling strategy ignores the precision of the Q-value estimate of the next state, it can lead to useless and often incorrect updates to the Q-values. To mitigate this issue, we organize the agent's experience into a graph that explicitly tracks the dependency between Q-values of states. Each edge in the graph represents a transition between two states by executing a single action. We perform value backups via a breadth-first search starting from that expands vertices in the graph starting from the set of terminal states and successively moving backward. We empirically sho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#25551;&#36848;&#28369;&#22369;&#26131;&#21457;&#24615;&#65292;&#24182;&#36890;&#36807;XgBoost&#31639;&#27861;&#21644;TreeSHAP&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20248;&#21270;&#21518;&#30340;XgBoost&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#20998;&#31867;&#22120;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20132;&#21449;&#39564;&#35777;&#21152;&#26435;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2201.03225</link><description>&lt;p&gt;
&#20351;&#29992;TreeSHAP&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#38598;&#25104;&#29305;&#24449;&#36873;&#25321;&#36827;&#34892;&#28369;&#22369;&#26131;&#21457;&#24615;&#21046;&#22270;
&lt;/p&gt;
&lt;p&gt;
Explainable AI Integrated Feature Selection for Landslide Susceptibility Mapping using TreeSHAP. (arXiv:2201.03225v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.03225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#25551;&#36848;&#28369;&#22369;&#26131;&#21457;&#24615;&#65292;&#24182;&#36890;&#36807;XgBoost&#31639;&#27861;&#21644;TreeSHAP&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20248;&#21270;&#21518;&#30340;XgBoost&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#20998;&#31867;&#22120;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20132;&#21449;&#39564;&#35777;&#21152;&#26435;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#20026;&#20840;&#29699;&#21464;&#26262;&#26102;&#20195;&#65292;&#28369;&#22369;&#39057;&#32321;&#21457;&#29983;&#24182;&#23545;&#20154;&#31867;&#29983;&#21629;&#21644;&#36130;&#20135;&#26500;&#25104;&#23041;&#32961;&#12290;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#36827;&#34892;&#26089;&#26399;&#28369;&#22369;&#26131;&#21457;&#24615;&#39044;&#27979;&#26159;&#26102;&#20195;&#30340;&#38656;&#27714;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26368;&#33021;&#25551;&#36848;&#28369;&#22369;&#26131;&#21457;&#24615;&#30340;&#20248;&#31168;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20998;&#26512;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;XgBoost&#12289;LR&#12289;KNN&#12289;SVM&#21644;Adaboost&#31561;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#28369;&#22369;&#26131;&#21457;&#24615;&#39044;&#27979;&#12290;&#20026;&#20102;&#25214;&#21040;&#27599;&#20010;&#20998;&#31867;&#22120;&#30340;&#26368;&#20339;&#36229;&#21442;&#25968;&#20197;&#36798;&#21040;&#26368;&#20248;&#24615;&#33021;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#32593;&#26684;&#25628;&#32034;&#26041;&#27861;&#21644;10&#25240;&#20132;&#21449;&#39564;&#35777;&#12290;&#22312;&#27492;&#24773;&#20917;&#19979;&#65292;&#32463;&#36807;&#20248;&#21270;&#30340;XgBoost&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#25152;&#26377;&#20998;&#31867;&#22120;&#65292;&#20132;&#21449;&#39564;&#35777;&#21152;&#26435;F1&#20998;&#25968;&#36798;&#21040;94.62&#65285;&#12290;&#22312;&#36825;&#20010;&#32463;&#39564;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;TreeSHAP&#26041;&#27861;&#25506;&#32034;&#20102;XgBoost&#20998;&#31867;&#22120;&#65292;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#21338;&#24328;&#35770;&#30340;&#32479;&#35745;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Landslides have been a regular occurrence and an alarming threat to human life and property in the era of anthropogenic global warming. An early prediction of landslide susceptibility using a data-driven approach is a demand of time. In this study, we explored the eloquent features that best describe landslide susceptibility with state-of-the-art machine learning methods. In our study, we employed state-of-the-art machine learning algorithms including XgBoost, LR, KNN, SVM, and Adaboost for landslide susceptibility prediction. To find the best hyperparameters of each individual classifier for optimized performance, we have incorporated the Grid Search method, with 10 Fold Cross-Validation. In this context, the optimized version of XgBoost outperformed all other classifiers with a Cross-validation Weighted F1 score of 94.62 %. Followed by this empirical evidence, we explored the XgBoost classifier by incorporating TreeSHAP, a game-theory-based statistical algorithm used to explain Machi
&lt;/p&gt;</description></item><item><title>&#21452;&#37325;PC&#31639;&#27861;&#36890;&#36807;&#21033;&#29992;&#21327;&#26041;&#24046;&#21644;&#31934;&#24230;&#30697;&#38453;&#20043;&#38388;&#30340;&#21453;&#21521;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;CI&#27979;&#35797;&#65292;&#33021;&#22815;&#24674;&#22797;&#27491;&#30830;&#30340;&#31561;&#20215;&#31867;&#65292;&#24182;&#21487;&#23545;&#20114;&#34917;&#35843;&#33410;&#38598;&#30340;&#20559;&#30456;&#20851;&#36827;&#34892;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2112.09036</link><description>&lt;p&gt;
&#21452;&#37325;PC&#31639;&#27861;&#21450;&#20854;&#23545;&#39640;&#26031;&#24615;&#36136;&#22312;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Dual PC Algorithm and the Role of Gaussianity for Structure Learning of Bayesian Networks. (arXiv:2112.09036v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.09036
&lt;/p&gt;
&lt;p&gt;
&#21452;&#37325;PC&#31639;&#27861;&#36890;&#36807;&#21033;&#29992;&#21327;&#26041;&#24046;&#21644;&#31934;&#24230;&#30697;&#38453;&#20043;&#38388;&#30340;&#21453;&#21521;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;CI&#27979;&#35797;&#65292;&#33021;&#22815;&#24674;&#22797;&#27491;&#30830;&#30340;&#31561;&#20215;&#31867;&#65292;&#24182;&#21487;&#23545;&#20114;&#34917;&#35843;&#33410;&#38598;&#30340;&#20559;&#30456;&#20851;&#36827;&#34892;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#22270;&#24418;&#32467;&#26500;&#26159;&#25551;&#36848;&#35768;&#22810;&#22797;&#26434;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#30340;&#20851;&#38190;&#65292;&#20294;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#27969;&#34892;&#30340;PC&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#36870;&#21521;&#24037;&#31243;&#21464;&#37327;&#20998;&#24067;&#20013;&#25152;&#20855;&#26377;&#30340;&#26465;&#20214;&#29420;&#31435;&#20851;&#31995;&#26469;&#19968;&#33268;&#22320;&#24674;&#22797;&#27491;&#30830;&#30340;&#31561;&#20215;&#31867;&#12290;&#21452;&#37325;PC&#31639;&#27861;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#21327;&#26041;&#24046;&#21644;&#31934;&#24230;&#30697;&#38453;&#20043;&#38388;&#30340;&#21453;&#21521;&#20851;&#31995;&#26469;&#36827;&#34892;PC&#31639;&#27861;&#20013;&#30340;CI&#27979;&#35797;&#12290;&#36890;&#36807;&#21033;&#29992;&#22359;&#30697;&#38453;&#27714;&#36870;&#65292;&#25105;&#20204;&#36824;&#21487;&#20197;&#23545;&#20114;&#34917;&#65288;&#25110;&#21452;&#37325;&#65289;&#35843;&#33410;&#38598;&#30340;&#20559;&#30456;&#20851;&#36827;&#34892;&#27979;&#35797;&#12290;&#21452;&#37325;PC&#31639;&#27861;&#30340;&#22810;&#20010;CI&#27979;&#35797;&#39318;&#20808;&#32771;&#34385;&#36793;&#32536;&#21644;&#23436;&#20840;&#25490;&#24207;CI&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning the graphical structure of Bayesian networks is key to describing data-generating mechanisms in many complex applications but poses considerable computational challenges. Observational data can only identify the equivalence class of the directed acyclic graph underlying a Bayesian network model, and a variety of methods exist to tackle the problem. Under certain assumptions, the popular PC algorithm can consistently recover the correct equivalence class by reverse-engineering the conditional independence (CI) relationships holding in the variable distribution. The dual PC algorithm is a novel scheme to carry out the CI tests within the PC algorithm by leveraging the inverse relationship between covariance and precision matrices. By exploiting block matrix inversions we can also perform tests on partial correlations of complementary (or dual) conditioning sets. The multiple CI tests of the dual PC algorithm proceed by first considering marginal and full-order CI relationships a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#21160;&#37327;&#30340;&#38543;&#26426;&#36817;&#31471;&#28857;&#31639;&#27861;&#65288;SPPAM&#65289;&#30340;&#25910;&#25947;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#23637;&#31034;&#20102;SPPAM&#30456;&#27604;&#20110;&#38543;&#26426;&#36817;&#31471;&#28857;&#31639;&#27861;&#65288;SPPA&#65289;&#20855;&#26377;&#26356;&#24555;&#30340;&#32447;&#24615;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#25910;&#32553;&#22240;&#23376;&#12290;</title><link>http://arxiv.org/abs/2111.06171</link><description>&lt;p&gt;
&#24102;&#26377;&#21160;&#37327;&#30340;&#38543;&#26426;&#36817;&#31471;&#28857;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#21644;&#31283;&#23450;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convergence and Stability of the Stochastic Proximal Point Algorithm with Momentum. (arXiv:2111.06171v5 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.06171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#21160;&#37327;&#30340;&#38543;&#26426;&#36817;&#31471;&#28857;&#31639;&#27861;&#65288;SPPAM&#65289;&#30340;&#25910;&#25947;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#23637;&#31034;&#20102;SPPAM&#30456;&#27604;&#20110;&#38543;&#26426;&#36817;&#31471;&#28857;&#31639;&#27861;&#65288;SPPA&#65289;&#20855;&#26377;&#26356;&#24555;&#30340;&#32447;&#24615;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#25910;&#32553;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#19982;&#21160;&#37327;&#65288;SGDM&#65289;&#26159;&#35768;&#22810;&#20248;&#21270;&#22330;&#26223;&#20013;&#30340;&#20027;&#35201;&#31639;&#27861;&#65292;&#21253;&#25324;&#20984;&#20248;&#21270;&#21644;&#38750;&#20984;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22312;&#38543;&#26426;&#35774;&#32622;&#20013;&#65292;&#21160;&#37327;&#20250;&#24178;&#25200;&#26799;&#24230;&#22122;&#22768;&#65292;&#36890;&#24120;&#38656;&#35201;&#29305;&#23450;&#30340;&#27493;&#38271;&#21644;&#21160;&#37327;&#36873;&#25321;&#25165;&#33021;&#20445;&#35777;&#25910;&#25947;&#65292;&#20294;&#21152;&#36895;&#24448;&#24448;&#34987;&#24573;&#30053;&#12290;&#30456;&#23545;&#32780;&#35328;&#65292;&#36817;&#31471;&#28857;&#26041;&#27861;&#22240;&#20854;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#23545;&#19981;&#23436;&#32654;&#35843;&#25972;&#30340;&#24377;&#24615;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#38543;&#26426;&#21152;&#36895;&#21464;&#20307;&#21364;&#21463;&#21040;&#20102;&#38480;&#21046;&#30340;&#20851;&#27880;&#65306;&#21160;&#37327;&#22914;&#20309;&#24433;&#21709;&#65288;&#38543;&#26426;&#65289;&#36817;&#31471;&#28857;&#26041;&#27861;&#30340;&#31283;&#23450;&#24615;&#20173;&#28982;&#26410;&#32463;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20851;&#27880;&#24102;&#26377;&#21160;&#37327;&#30340;&#38543;&#26426;&#36817;&#31471;&#28857;&#31639;&#27861;&#65288;SPPAM&#65289;&#30340;&#25910;&#25947;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;SPPAM&#30456;&#27604;&#20110;&#24102;&#26377;&#26356;&#22909;&#25910;&#32553;&#22240;&#23376;&#30340;&#38543;&#26426;&#36817;&#31471;&#28857;&#31639;&#27861;&#65288;SPPA&#65289;&#20855;&#26377;&#26356;&#24555;&#30340;&#32447;&#24615;&#25910;&#25947;&#21040;&#19968;&#20010;&#37051;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient descent with momentum (SGDM) is the dominant algorithm in many optimization scenarios, including convex optimization instances and non-convex neural network training. Yet, in the stochastic setting, momentum interferes with gradient noise, often leading to specific step size and momentum choices in order to guarantee convergence, set aside acceleration. Proximal point methods, on the other hand, have gained much attention due to their numerical stability and elasticity against imperfect tuning. Their stochastic accelerated variants though have received limited attention: how momentum interacts with the stability of (stochastic) proximal point methods remains largely unstudied. To address this, we focus on the convergence and stability of the stochastic proximal point algorithm with momentum (SPPAM), and show that SPPAM allows a faster linear convergence to a neighborhood compared to the stochastic proximal point algorithm (SPPA) with a better contraction factor, und
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;&#35821;&#31687;&#24863;&#30693;&#22522;&#20934;&#65292;&#31995;&#32479;&#24615;&#22320;&#30830;&#23450;&#20102;&#38656;&#35201;&#19978;&#19979;&#25991;&#32763;&#35793;&#30340;&#29616;&#35937;&#12290;&#21457;&#29616;&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#23545;&#20110;&#35299;&#20915;&#36825;&#20123;&#29616;&#35937;&#30340;&#22256;&#38590;&#31243;&#24230;&#26377;&#38480;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2109.07446</link><description>&lt;p&gt;
&#20309;&#26102;&#38656;&#35201;&#19978;&#19979;&#25991;&#32763;&#35793;&#65311;&#19968;&#39033;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#22810;&#35821;&#35328;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
When Does Translation Require Context? A Data-driven, Multilingual Exploration. (arXiv:2109.07446v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.07446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;&#35821;&#31687;&#24863;&#30693;&#22522;&#20934;&#65292;&#31995;&#32479;&#24615;&#22320;&#30830;&#23450;&#20102;&#38656;&#35201;&#19978;&#19979;&#25991;&#32763;&#35793;&#30340;&#29616;&#35937;&#12290;&#21457;&#29616;&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#23545;&#20110;&#35299;&#20915;&#36825;&#20123;&#29616;&#35937;&#30340;&#22256;&#38590;&#31243;&#24230;&#26377;&#38480;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#27491;&#30830;&#22788;&#29702;&#35821;&#31687;&#23545;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#26377;&#24456;&#22823;&#24433;&#21709;&#65292;&#20294;&#36825;&#20123;&#25913;&#36827;&#22312;&#24120;&#35265;&#30340;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#20013;&#24471;&#19981;&#21040;&#20805;&#20998;&#34913;&#37327;&#12290;&#26368;&#36817;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#30740;&#31350;&#35797;&#22270;&#38024;&#23545;&#19968;&#23567;&#37096;&#20998;&#35821;&#31687;&#29616;&#35937;&#36827;&#34892;&#35780;&#20272;&#65292;&#20294;&#32570;&#20047;&#23436;&#20840;&#31995;&#32479;&#21270;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;&#35821;&#31687;&#24863;&#30693;&#65288;MuDA&#65289;&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#26631;&#35760;&#22120;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#35780;&#20272;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#30340;&#35821;&#31687;&#29616;&#35937;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#36873;&#25321;&#30340;&#35821;&#31687;&#29616;&#35937;&#21463;&#21040;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#21487;&#31995;&#32479;&#22320;&#30830;&#23450;&#38656;&#35201;&#19978;&#19979;&#25991;&#30340;&#32763;&#35793;&#12290;&#25105;&#20204;&#30830;&#35748;&#20102;&#20043;&#21069;&#30740;&#31350;&#30340;&#35821;&#31687;&#29616;&#35937;&#30340;&#38590;&#24230;&#65292;&#24182;&#25581;&#31034;&#20102;&#20043;&#21069;&#26410;&#35299;&#20915;&#30340;&#20854;&#20182;&#29616;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#24120;&#35265;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20165;&#23545;&#19978;&#19979;&#25991;&#26080;&#20851;&#27169;&#22411;&#36827;&#34892;&#20102;&#36731;&#24494;&#30340;&#25913;&#36827;&#65292;&#36825;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#24182;&#27809;&#26377;&#26377;&#25928;&#22788;&#29702;&#36825;&#20123;&#27495;&#20041;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;14&#31181;&#35821;&#35328;&#23545;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#65292;&#20197;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although proper handling of discourse significantly contributes to the quality of machine translation (MT), these improvements are not adequately measured in common translation quality metrics. Recent works in context-aware MT attempt to target a small set of discourse phenomena during evaluation, however not in a fully systematic way. In this paper, we develop the Multilingual Discourse-Aware (MuDA) benchmark, a series of taggers that identify and evaluate model performance on discourse phenomena in any given dataset. The choice of phenomena is inspired by a novel methodology to systematically identify translations requiring context. We confirm the difficulty of previously studied phenomena while uncovering others that were previously unaddressed. We find that common context-aware MT models make only marginal improvements over context-agnostic models, which suggests these models do not handle these ambiguities effectively. We release code and data for 14 language pairs to encourage th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FedPower&#30340;&#31639;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20869;&#35299;&#20915;&#20102;&#29305;&#24449;&#31354;&#38388;&#20272;&#35745;&#30340;&#38544;&#31169;&#21644;&#36890;&#20449;&#25928;&#29575;&#38382;&#39064;&#12290;&#31639;&#27861;&#21033;&#29992;&#24130;&#27861;&#36827;&#34892;&#26412;&#22320;&#36845;&#20195;&#21644;&#20840;&#23616;&#32858;&#21512;&#65292;&#37319;&#29992;&#27491;&#20132;Procrustes&#21464;&#25442;&#21152;&#26435;&#20197;&#23454;&#29616;&#23545;&#40784;&#65292;&#24182;&#24341;&#20837;&#24046;&#20998;&#38544;&#31169;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2103.00704</link><description>&lt;p&gt;
FedPower: &#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#29305;&#24449;&#31354;&#38388;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
FedPower: Privacy-Preserving Distributed Eigenspace Estimation. (arXiv:2103.00704v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.00704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;FedPower&#30340;&#31639;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20869;&#35299;&#20915;&#20102;&#29305;&#24449;&#31354;&#38388;&#20272;&#35745;&#30340;&#38544;&#31169;&#21644;&#36890;&#20449;&#25928;&#29575;&#38382;&#39064;&#12290;&#31639;&#27861;&#21033;&#29992;&#24130;&#27861;&#36827;&#34892;&#26412;&#22320;&#36845;&#20195;&#21644;&#20840;&#23616;&#32858;&#21512;&#65292;&#37319;&#29992;&#27491;&#20132;Procrustes&#21464;&#25442;&#21152;&#26435;&#20197;&#23454;&#29616;&#23545;&#40784;&#65292;&#24182;&#24341;&#20837;&#24046;&#20998;&#38544;&#31169;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#31354;&#38388;&#20272;&#35745;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#30340;&#22522;&#30784;&#38382;&#39064;&#65292;&#22312;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#32500;&#24230;&#32422;&#31616;&#21644;&#32858;&#31867;&#31561;&#39046;&#22495;&#37117;&#26377;&#24212;&#29992;&#12290;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#36890;&#24120;&#20551;&#35774;&#25968;&#25454;&#26469;&#33258;&#19981;&#21516;&#30340;&#32452;&#32455;&#19988;&#23646;&#20110;&#19981;&#21516;&#32452;&#32455;&#12290;&#25968;&#25454;&#30340;&#20302;&#36890;&#20449;&#33021;&#21147;&#21644;&#21487;&#33021;&#30340;&#38544;&#31169;&#27844;&#28431;&#20351;&#24471;&#29305;&#24449;&#31354;&#38388;&#30340;&#35745;&#31639;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26694;&#26550;&#20869;&#25552;&#20986;&#20102;&#19968;&#31867;&#31639;&#27861;&#65292;&#31216;&#20026;FedPower&#12290;FedPower&#21033;&#29992;&#20102;&#33879;&#21517;&#30340;&#24130;&#27861;&#65292;&#36890;&#36807;&#20132;&#26367;&#36827;&#34892;&#22810;&#20010;&#26412;&#22320;&#24130;&#36845;&#20195;&#21644;&#20840;&#23616;&#32858;&#21512;&#27493;&#39588;&#65292;&#20174;&#32780;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;&#22312;&#32858;&#21512;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#27491;&#20132;Procrustes&#21464;&#25442;&#65288;OPT&#65289;&#23545;&#27599;&#20010;&#26412;&#22320;&#29305;&#24449;&#21521;&#37327;&#30697;&#38453;&#36827;&#34892;&#21152;&#26435;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#23545;&#40784;&#12290;&#20026;&#20102;&#30830;&#20445;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#25105;&#20204;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#28155;&#21152;&#39640;&#26031;&#22122;&#22768;&#65292;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Eigenspace estimation is fundamental in machine learning and statistics, which has found applications in PCA, dimension reduction, and clustering, among others. The modern machine learning community usually assumes that data come from and belong to different organizations. The low communication power and the possible privacy breaches of data make the computation of eigenspace challenging. To address these challenges, we propose a class of algorithms called \textsf{FedPower} within the federated learning (FL) framework. \textsf{FedPower} leverages the well-known power method by alternating multiple local power iterations and a global aggregation step, thus improving communication efficiency. In the aggregation, we propose to weight each local eigenvector matrix with {\it Orthogonal Procrustes Transformation} (OPT) for better alignment. To ensure strong privacy protection, we add Gaussian noise in each iteration by adopting the notion of \emph{differential privacy} (DP). We provide conve
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#29305;&#24449;&#24037;&#20855;&#21464;&#37327;&#22238;&#24402;&#65288;DFIV&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#35266;&#27979;&#25968;&#25454;&#20013;&#24037;&#20855;&#21464;&#37327;&#12289;&#22788;&#29702;&#21464;&#37327;&#21644;&#32467;&#26524;&#21464;&#37327;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#23450;&#20041;&#24037;&#20855;&#21464;&#37327;&#21644;&#22788;&#29702;&#21464;&#37327;&#19978;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20132;&#26367;&#35757;&#32451;&#26426;&#21046;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#31471;&#21040;&#31471;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#35745;&#31639;&#19978;&#33719;&#24471;&#39640;&#24230;&#28789;&#27963;&#30340;&#29305;&#24449;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2010.07154</link><description>&lt;p&gt;
&#22312;&#24037;&#20855;&#21464;&#37327;&#22238;&#24402;&#20013;&#23398;&#20064;&#28145;&#24230;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Learning Deep Features in Instrumental Variable Regression. (arXiv:2010.07154v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.07154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28145;&#24230;&#29305;&#24449;&#24037;&#20855;&#21464;&#37327;&#22238;&#24402;&#65288;DFIV&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#35266;&#27979;&#25968;&#25454;&#20013;&#24037;&#20855;&#21464;&#37327;&#12289;&#22788;&#29702;&#21464;&#37327;&#21644;&#32467;&#26524;&#21464;&#37327;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#23450;&#20041;&#24037;&#20855;&#21464;&#37327;&#21644;&#22788;&#29702;&#21464;&#37327;&#19978;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20132;&#26367;&#35757;&#32451;&#26426;&#21046;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#31471;&#21040;&#31471;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#35745;&#31639;&#19978;&#33719;&#24471;&#39640;&#24230;&#28789;&#27963;&#30340;&#29305;&#24449;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#21464;&#37327;&#65288;IV&#65289;&#22238;&#24402;&#26159;&#19968;&#31181;&#22312;&#35266;&#27979;&#25968;&#25454;&#20013;&#21033;&#29992;&#20165;&#36890;&#36807;&#22788;&#29702;&#24433;&#21709;&#32467;&#26524;&#30340;&#24037;&#20855;&#21464;&#37327;&#26469;&#23398;&#20064;&#28151;&#28102;&#30340;&#22788;&#29702;&#21644;&#32467;&#26524;&#21464;&#37327;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#26631;&#20934;&#31574;&#30053;&#12290;&#22312;&#20256;&#32479;&#30340;IV&#22238;&#24402;&#20013;&#65292;&#23398;&#20064;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#38454;&#27573;1&#20174;&#24037;&#20855;&#21464;&#37327;&#21040;&#22788;&#29702;&#21464;&#37327;&#36827;&#34892;&#32447;&#24615;&#22238;&#24402;&#65307;&#38454;&#27573;2&#22312;&#24037;&#20855;&#21464;&#37327;&#30340;&#26465;&#20214;&#19979;&#65292;&#20174;&#22788;&#29702;&#21464;&#37327;&#21040;&#32467;&#26524;&#21464;&#37327;&#36827;&#34892;&#32447;&#24615;&#22238;&#24402;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#28145;&#24230;&#29305;&#24449;&#24037;&#20855;&#21464;&#37327;&#22238;&#24402;&#65288;DFIV&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#24037;&#20855;&#21464;&#37327;&#12289;&#22788;&#29702;&#21464;&#37327;&#21644;&#32467;&#26524;&#21464;&#37327;&#20043;&#38388;&#21487;&#33021;&#26159;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#24773;&#20917;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#23450;&#20041;&#24037;&#20855;&#21464;&#37327;&#21644;&#22788;&#29702;&#21464;&#37327;&#19978;&#30340;&#20449;&#24687;&#24615;&#38750;&#32447;&#24615;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#26367;&#35757;&#32451;&#26426;&#21046;&#26469;&#35757;&#32451;&#36825;&#20123;&#29305;&#24449;&#65292;&#20197;&#20445;&#35777;&#36890;&#36807;&#32452;&#21512;&#38454;&#27573;1&#21644;&#38454;&#27573;2&#33719;&#24471;&#33391;&#22909;&#30340;&#31471;&#21040;&#31471;&#24615;&#33021;&#65292;&#20174;&#32780;&#22312;&#35745;&#31639;&#19978;&#33719;&#24471;&#39640;&#24230;&#28789;&#27963;&#30340;&#29305;&#24449;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instrumental variable (IV) regression is a standard strategy for learning causal relationships between confounded treatment and outcome variables from observational data by utilizing an instrumental variable, which affects the outcome only through the treatment. In classical IV regression, learning proceeds in two stages: stage 1 performs linear regression from the instrument to the treatment; and stage 2 performs linear regression from the treatment to the outcome, conditioned on the instrument. We propose a novel method, deep feature instrumental variable regression (DFIV), to address the case where relations between instruments, treatments, and outcomes may be nonlinear. In this case, deep neural nets are trained to define informative nonlinear features on the instruments and treatments. We propose an alternating training regime for these features to ensure good end-to-end performance when composing stages 1 and 2, thus obtaining highly flexible feature maps in a computationally eff
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25345;&#32493;&#32456;&#36523;&#23398;&#20064;&#30340;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#25968;&#25454;&#31232;&#30095;&#24615;&#65292;&#24182;&#36890;&#36807;&#30693;&#35782;&#30340;&#25345;&#32493;&#31215;&#32047;&#21644;&#36716;&#31227;&#26469;&#25552;&#39640;&#20027;&#39064;&#24314;&#27169;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2006.10909</link><description>&lt;p&gt;
&#20855;&#26377;&#25345;&#32493;&#32456;&#36523;&#23398;&#20064;&#30340;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Neural Topic Modeling with Continual Lifelong Learning. (arXiv:2006.10909v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.10909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25345;&#32493;&#32456;&#36523;&#23398;&#20064;&#30340;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#25968;&#25454;&#31232;&#30095;&#24615;&#65292;&#24182;&#36890;&#36807;&#30693;&#35782;&#30340;&#25345;&#32493;&#31215;&#32047;&#21644;&#36716;&#31227;&#26469;&#25552;&#39640;&#20027;&#39064;&#24314;&#27169;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#21560;&#24341;&#20102;&#20154;&#20204;&#23545;&#24314;&#31435;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#20851;&#27880;&#65292;&#36825;&#20123;&#31995;&#32479;&#21487;&#20197;&#19981;&#26029;&#31215;&#32047;&#21644;&#20256;&#36882;&#30693;&#35782;&#65292;&#20197;&#24110;&#21161;&#26410;&#26469;&#30340;&#23398;&#20064;&#12290;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#24191;&#27867;&#29992;&#20110;&#20174;&#25991;&#26723;&#38598;&#21512;&#20013;&#21457;&#29616;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#24615;&#65292;&#20363;&#22914;&#22312;&#19968;&#20010;&#23567;&#30340;&#65288;&#30701;&#65289;&#25991;&#26723;&#38598;&#21512;&#20013;&#65292;&#20027;&#39064;&#24314;&#27169;&#30340;&#24212;&#29992;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20174;&#32780;&#20135;&#29983;&#19981;&#36830;&#36143;&#30340;&#20027;&#39064;&#21644;&#27425;&#20248;&#30340;&#25991;&#26723;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#30340;&#32456;&#36523;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#25345;&#32493;&#22788;&#29702;&#25991;&#26723;&#38598;&#21512;&#30340;&#27969;&#65292;&#31215;&#32047;&#20027;&#39064;&#65292;&#24182;&#36890;&#36807;&#20174;&#22810;&#20010;&#28304;&#30340;&#30693;&#35782;&#36716;&#31227;&#25351;&#23548;&#26410;&#26469;&#30340;&#20027;&#39064;&#24314;&#27169;&#20219;&#21153;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#31232;&#30095;&#25968;&#25454;&#12290;&#22312;&#32456;&#36523;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#30740;&#31350;&#20102;&#20849;&#20139;&#30340;&#29983;&#25104;&#21516;&#28304;&#24615;&#65288;&#28508;&#22312;&#20027;&#39064;&#65289;&#20197;&#22312;&#32456;&#36523;&#20013;&#20256;&#36882;&#20808;&#21069;&#30340;&#30693;&#35782;&#65292;&#20197;&#21450;&#36890;&#36807;&#26032;&#30340;&#36873;&#25321;&#24615;&#36951;&#24536;&#26469;&#26368;&#23567;&#21270;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#20197;&#20445;&#30041;&#36807;&#21435;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong learning has recently attracted attention in building machine learning systems that continually accumulate and transfer knowledge to help future learning. Unsupervised topic modeling has been popularly used to discover topics from document collections. However, the application of topic modeling is challenging due to data sparsity, e.g., in a small collection of (short) documents and thus, generate incoherent topics and sub-optimal document representations. To address the problem, we propose a lifelong learning framework for neural topic modeling that can continuously process streams of document collections, accumulate topics and guide future topic modeling tasks by knowledge transfer from several sources to better deal with the sparse data. In the lifelong process, we particularly investigate jointly: (1) sharing generative homologies (latent topics) over lifetime to transfer prior knowledge, and (2) minimizing catastrophic forgetting to retain the past learning via novel sele
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#22797;&#21512;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#35299;&#37322;&#24615;&#30340;&#20027;&#39064;&#34920;&#31034;&#21644;&#21477;&#23376;&#32423;&#30340;&#20027;&#39064;&#23545;&#35805;&#65292;&#23558;&#20027;&#39064;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#65292;&#35813;&#27169;&#22411;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2006.10632</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#12289;&#23545;&#35805;&#20027;&#39064;&#24863;&#30693;&#30340;&#31070;&#32463;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Explainable and Discourse Topic-aware Neural Language Understanding. (arXiv:2006.10632v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.10632
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#22797;&#21512;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#35299;&#37322;&#24615;&#30340;&#20027;&#39064;&#34920;&#31034;&#21644;&#21477;&#23376;&#32423;&#30340;&#20027;&#39064;&#23545;&#35805;&#65292;&#23558;&#20027;&#39064;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#65292;&#35813;&#27169;&#22411;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20027;&#39064;&#27169;&#22411;&#19982;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20027;&#39064;&#23558;&#35821;&#35328;&#29702;&#35299;&#25193;&#23637;&#21040;&#21477;&#23376;&#20043;&#22806;&#30340;&#26356;&#24191;&#27867;&#30340;&#25991;&#26723;&#32423;&#19978;&#19979;&#25991;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#20027;&#39064;&#35821;&#20041;&#65292;&#20294;&#24573;&#30053;&#20102;&#25991;&#26723;&#20013;&#21477;&#23376;&#30340;&#20027;&#39064;&#23545;&#35805;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20026;&#27599;&#20010;&#28508;&#22312;&#20027;&#39064;&#30340;&#27604;&#20363;&#30456;&#23545;&#24212;&#22320;&#24341;&#20837;&#21487;&#35299;&#37322;&#24615;&#20027;&#39064;&#34920;&#31034;&#26469;&#25193;&#23637;&#30740;&#31350;&#39046;&#22495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20026;&#25991;&#26723;&#20013;&#30340;&#27599;&#20010;&#21477;&#23376;&#24314;&#27169;&#20027;&#39064;&#23545;&#35805;&#65292;&#20445;&#30041;&#21477;&#23376;-&#20027;&#39064;&#20851;&#32852;&#20197;&#21450;&#25991;&#26723;-&#20027;&#39064;&#20851;&#32852;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#22797;&#21512;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#20027;&#39064;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#20013;&#21516;&#26102;&#21033;&#29992;&#28508;&#22312;&#20027;&#39064;&#21644;&#21487;&#35299;&#37322;&#20027;&#39064;&#20197;&#21450;&#21477;&#23376;&#32423;&#30340;&#20027;&#39064;&#23545;&#35805;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#22914;&#35821;&#35328;&#24314;&#27169;&#12289;&#35789;&#20041;&#28040;&#27495;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Marrying topic models and language models exposes language understanding to a broader source of document-level context beyond sentences via topics. While introducing topical semantics in language models, existing approaches incorporate latent document topic proportions and ignore topical discourse in sentences of the document. This work extends the line of research by additionally introducing an explainable topic representation in language understanding, obtained from a set of key terms correspondingly for each latent topic of the proportion. Moreover, we retain sentence-topic associations along with document-topic association by modeling topical discourse for every sentence in the document. We present a novel neural composite language model that exploits both the latent and explainable topics along with topical discourse at sentence-level in a joint learning framework of topic and language models. Experiments over a range of tasks such as language modeling, word sense disambiguation, 
&lt;/p&gt;</description></item></channel></rss>