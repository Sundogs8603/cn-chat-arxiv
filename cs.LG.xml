<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#65292;&#23545;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;Lipschitz&#24120;&#25968;&#30340;&#31934;&#30830;&#21051;&#30011;&#65292;&#23545;&#20110;&#36275;&#22815;&#23485;&#24230;&#30340;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19978;&#19979;&#30028;&#65292;&#24182;&#21305;&#37197;&#19968;&#20010;&#20381;&#36182;&#20110;&#28145;&#24230;&#30340;&#23545;&#25968;&#22240;&#23376;&#12290;</title><link>http://arxiv.org/abs/2311.01356</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;
&lt;/p&gt;
&lt;p&gt;
On the Lipschitz constant of random neural networks. (arXiv:2311.01356v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#65292;&#23545;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;Lipschitz&#24120;&#25968;&#30340;&#31934;&#30830;&#21051;&#30011;&#65292;&#23545;&#20110;&#36275;&#22815;&#23485;&#24230;&#30340;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19978;&#19979;&#30028;&#65292;&#24182;&#21305;&#37197;&#19968;&#20010;&#20381;&#36182;&#20110;&#28145;&#24230;&#30340;&#23545;&#25968;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#35777;&#30740;&#31350;&#24191;&#27867;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#23545;&#36755;&#20837;&#30340;&#24494;&#23567;&#23545;&#25239;&#24615;&#25200;&#21160;&#38750;&#24120;&#25935;&#24863;&#12290;&#36825;&#20123;&#25152;&#35859;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#26368;&#22351;&#24773;&#20917;&#40065;&#26834;&#24615;&#21487;&#20197;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#26469;&#37327;&#21270;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#20010;&#37327;&#30340;&#29702;&#35770;&#32467;&#26524;&#22312;&#25991;&#29486;&#20013;&#20165;&#26377;&#23569;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#38543;&#26426;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#65292;&#21363;&#36873;&#25321;&#38543;&#26426;&#26435;&#37325;&#24182;&#37319;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#23545;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#23558;Lipschitz&#24120;&#25968;&#21051;&#30011;&#21040;&#19968;&#20010;&#32477;&#23545;&#25968;&#20540;&#24120;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#25193;&#23637;&#21040;&#36275;&#22815;&#23485;&#24230;&#30340;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Lipschitz&#24120;&#25968;&#30340;&#19978;&#19979;&#30028;&#12290;&#36825;&#20123;&#30028;&#21305;&#37197;&#21040;&#19968;&#20010;&#20381;&#36182;&#20110;&#28145;&#24230;&#30340;&#23545;&#25968;&#22240;&#23376;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical studies have widely demonstrated that neural networks are highly sensitive to small, adversarial perturbations of the input. The worst-case robustness against these so-called adversarial examples can be quantified by the Lipschitz constant of the neural network. However, only few theoretical results regarding this quantity exist in the literature. In this paper, we initiate the study of the Lipschitz constant of random ReLU neural networks, i.e., neural networks whose weights are chosen at random and which employ the ReLU activation function. For shallow neural networks, we characterize the Lipschitz constant up to an absolute numerical constant. Moreover, we extend our analysis to deep neural networks of sufficiently large width where we prove upper and lower bounds for the Lipschitz constant. These bounds match up to a logarithmic factor that depends on the depth.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#20934;&#30830;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;GCM&#20013;&#37325;&#21472;&#30340;&#36879;&#26126;&#24230;&#29289;&#31181;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#32467;&#21512;&#21508;&#20010;&#30456;&#20851;-k&#36879;&#26126;&#24230;&#34920;&#65288;k-tables&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#28909;&#26408;&#26143;HD~209458 b&#30340;&#27169;&#25311;&#20013;&#34920;&#29616;&#20986;&#20102;&#31934;&#30830;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00775</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#20934;&#30830;&#22788;&#29702;GCM&#20013;&#37325;&#21472;&#36879;&#26126;&#24230;&#29289;&#31181;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Harnessing machine learning for accurate treatment of overlapping opacity species in GCMs. (arXiv:2311.00775v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#20934;&#30830;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;GCM&#20013;&#37325;&#21472;&#30340;&#36879;&#26126;&#24230;&#29289;&#31181;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#32467;&#21512;&#21508;&#20010;&#30456;&#20851;-k&#36879;&#26126;&#24230;&#34920;&#65288;k-tables&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#28909;&#26408;&#26143;HD~209458 b&#30340;&#27169;&#25311;&#20013;&#34920;&#29616;&#20986;&#20102;&#31934;&#30830;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#29702;&#35299;&#22806;&#34892;&#26143;&#21644;&#26837;&#30702;&#26143;&#30340;&#39640;&#31934;&#24230;&#35266;&#27979;&#32467;&#26524;&#65292;&#25105;&#20204;&#38656;&#35201;&#35814;&#32454;&#21644;&#22797;&#26434;&#30340;&#28085;&#30422;&#20102;&#27969;&#20307;&#21147;&#23398;&#12289;&#21270;&#23398;&#21644;&#36752;&#23556;&#30340;&#36890;&#29992;&#29615;&#27969;&#27169;&#22411;&#65288;GCMs&#65289;&#12290;&#26412;&#30740;&#31350;&#20855;&#20307;&#32771;&#23519;&#20102;GCMs&#20013;&#21270;&#23398;&#21644;&#36752;&#23556;&#20043;&#38388;&#30340;&#32806;&#21512;&#20851;&#31995;&#65292;&#24182;&#27604;&#36739;&#20102;&#22312;&#30456;&#20851;-k&#20551;&#35774;&#20013;&#28151;&#21512;&#19981;&#21516;&#21270;&#23398;&#29289;&#31181;&#36879;&#26126;&#24230;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#22312;&#19981;&#33021;&#20551;&#35774;&#24179;&#34913;&#21270;&#23398;&#21453;&#24212;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DeepSets&#65288;DS&#65289;&#30340;&#24555;&#36895;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#21508;&#20010;&#30456;&#20851;-k&#36879;&#26126;&#24230;&#34920;&#65288;k-tables&#65289;&#12290;&#25105;&#20204;&#23558;DS&#26041;&#27861;&#19982;&#20854;&#20182;&#24050;&#21457;&#34920;&#30340;&#26041;&#27861;&#22914;&#33258;&#36866;&#24212;&#31561;&#20215;&#28040;&#20809;&#65288;AEE&#65289;&#21644;&#24102;&#26377;&#37325;&#26032;&#20998;&#32452;&#21644;&#25490;&#24207;&#30340;&#38543;&#26426;&#37325;&#21472;&#65288;RORR&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#28151;&#21512;&#26041;&#27861;&#25972;&#21512;&#21040;&#20102;&#25105;&#20204;&#30340;GCM (expeRT/MITgcm)&#20013;&#65292;&#24182;&#23545;&#28909;&#26408;&#26143;HD~209458 b&#36827;&#34892;&#20102;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;DS&#26041;&#27861;&#22312;GCM&#20351;&#29992;&#26102;&#26082;&#20934;&#30830;&#21448;&#39640;&#25928;&#65292;&#32780;RORR&#26041;&#27861;&#21017;&#19981;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;
To understand high precision observations of exoplanets and brown dwarfs, we need detailed and complex general circulation models (GCMs) that incorporate hydrodynamics, chemistry, and radiation. In this study, we specifically examine the coupling between chemistry and radiation in GCMs and compare different methods for mixing opacities of different chemical species in the correlated-k assumption, when equilibrium chemistry cannot be assumed. We propose a fast machine learning method based on DeepSets (DS), which effectively combines individual correlated-k opacities (k-tables). We evaluate the DS method alongside other published methods like adaptive equivalent extinction (AEE) and random overlap with rebinning and resorting (RORR). We integrate these mixing methods into our GCM (expeRT/MITgcm) and assess their accuracy and performance for the example of the hot Jupiter HD~209458 b. Our findings indicate that the DS method is both accurate and efficient for GCM usage, whereas RORR is t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#37325;&#26032;&#22312;PyTorch&#19978;&#23454;&#29616;&#20102;VMAF&#65292;&#19982;&#26631;&#20934;&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;VMAF&#21333;&#20301;&#19978;&#30340;&#24046;&#24322;&#23567;&#20110;$10^{-2}$&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20102;&#22312;&#20351;&#29992;VMAF&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#26102;&#30340;&#26799;&#24230;&#35745;&#31639;&#65292;&#24182;&#35777;&#26126;&#20351;&#29992;&#35813;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#19981;&#20250;&#23548;&#33268;&#26799;&#24230;&#19981;&#33391;&#12290;</title><link>http://arxiv.org/abs/2310.15578</link><description>&lt;p&gt;
&#22312;PyTorch&#19978;&#37325;&#26032;&#23454;&#29616;&#30340;VMAF&#65306;&#19968;&#20123;&#23454;&#39564;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
VMAF Re-implementation on PyTorch: Some Experimental Results. (arXiv:2310.15578v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15578
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#37325;&#26032;&#22312;PyTorch&#19978;&#23454;&#29616;&#20102;VMAF&#65292;&#19982;&#26631;&#20934;&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;VMAF&#21333;&#20301;&#19978;&#30340;&#24046;&#24322;&#23567;&#20110;$10^{-2}$&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20102;&#22312;&#20351;&#29992;VMAF&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#26102;&#30340;&#26799;&#24230;&#35745;&#31639;&#65292;&#24182;&#35777;&#26126;&#20351;&#29992;&#35813;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#19981;&#20250;&#23548;&#33268;&#26799;&#24230;&#19981;&#33391;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26631;&#20934;&#30340;VMAF&#23454;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;PyTorch&#26694;&#26550;&#23454;&#29616;VMAF&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#36825;&#20010;&#23454;&#29616;&#65292;&#19982;&#26631;&#20934;&#30340;(libvmaf)&#36827;&#34892;&#27604;&#36739;&#65292;VMAF&#21333;&#20301;&#19978;&#30340;&#24046;&#24322;&#23567;&#20110;$10^{-2}$&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20351;&#29992;VMAF&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#26102;&#30340;&#26799;&#24230;&#35745;&#31639;&#65292;&#24182;&#35777;&#26126;&#20351;&#29992;&#35813;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#19981;&#20250;&#23548;&#33268;&#26799;&#24230;&#19981;&#33391;&#12290;
&lt;/p&gt;
&lt;p&gt;
Based on the standard VMAF implementation we propose an implementation of VMAF using PyTorch framework. For this implementation comparisons with the standard (libvmaf) show the discrepancy $\lesssim 10^{-2}$ in VMAF units. We investigate gradients computation when using VMAF as an objective function and demonstrate that training using this function does not result in ill-behaving gradients.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32477;&#32536;&#20307;&#25913;&#21892;&#28909;&#21147;&#25193;&#25955;&#36827;&#34892;&#26080;&#30896;&#25758;&#36816;&#21160;&#35268;&#21010;&#30340;&#21435;&#22122;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21333;&#19968;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#21487;&#36798;&#30446;&#26631;&#24182;&#35268;&#21010;&#36991;&#24320;&#38556;&#30861;&#29289;&#30340;&#36816;&#21160;&#36335;&#24452;&#65292;&#20855;&#26377;&#31283;&#20581;&#24615;&#21644;&#22810;&#27169;&#24577;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12609</link><description>&lt;p&gt;
&#20351;&#29992;&#32477;&#32536;&#20307;&#25913;&#21892;&#28909;&#21147;&#25193;&#25955;&#36827;&#34892;&#26080;&#30896;&#25758;&#36816;&#21160;&#35268;&#21010;&#30340;&#21435;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Denoising Heat-inspired Diffusion with Insulators for Collision Free Motion Planning. (arXiv:2310.12609v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32477;&#32536;&#20307;&#25913;&#21892;&#28909;&#21147;&#25193;&#25955;&#36827;&#34892;&#26080;&#30896;&#25758;&#36816;&#21160;&#35268;&#21010;&#30340;&#21435;&#22122;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21333;&#19968;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#21487;&#36798;&#30446;&#26631;&#24182;&#35268;&#21010;&#36991;&#24320;&#38556;&#30861;&#29289;&#30340;&#36816;&#21160;&#36335;&#24452;&#65292;&#20855;&#26377;&#31283;&#20581;&#24615;&#21644;&#22810;&#27169;&#24577;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#22810;&#27169;&#24577;&#24615;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;&#23613;&#31649;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22797;&#26434;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20005;&#37325;&#20381;&#36182;&#20110;&#25512;&#29702;&#26102;&#30340;&#38556;&#30861;&#29289;&#26816;&#27979;&#24182;&#38656;&#35201;&#39069;&#22806;&#30340;&#35774;&#22791;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#20174;&#21333;&#19968;&#30340;&#35270;&#35273;&#36755;&#20837;&#20013;&#21516;&#26102;&#29983;&#25104;&#21487;&#36798;&#30446;&#26631;&#24182;&#35268;&#21010;&#36991;&#24320;&#38556;&#30861;&#29289;&#30340;&#36816;&#21160;&#36335;&#24452;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26032;&#39062;&#30340;&#30896;&#25758;&#36991;&#20813;&#25193;&#25955;&#26680;&#36827;&#34892;&#20351;&#29992;&#12290;&#36890;&#36807;&#19982;&#34892;&#20026;&#20811;&#38534;&#21644;&#32463;&#20856;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#35777;&#26126;&#20102;&#20854;&#31283;&#20581;&#24615;&#12290;&#29305;&#21035;&#26159;&#22312;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#65292;&#23427;&#33021;&#22815;&#23548;&#33322;&#21040;&#30446;&#26631;&#24182;&#36991;&#24320;&#34987;&#38556;&#30861;&#29289;&#38459;&#25377;&#30340;&#19981;&#21487;&#36798;&#30446;&#26631;&#65292;&#21516;&#26102;&#30830;&#20445;&#36991;&#20813;&#30896;&#25758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have risen as a powerful tool in robotics due to their flexibility and multi-modality. While some of these methods effectively address complex problems, they often depend heavily on inference-time obstacle detection and require additional equipment. Addressing these challenges, we present a method that, during inference time, simultaneously generates only reachable goals and plans motions that avoid obstacles, all from a single visual input. Central to our approach is the novel use of a collision-avoiding diffusion kernel for training. Through evaluations against behavior-cloning and classical diffusion models, our framework has proven its robustness. It is particularly effective in multi-modal environments, navigating toward goals and avoiding unreachable ones blocked by obstacles, while ensuring collision avoidance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#36328;&#25991;&#26723;&#30340;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.10638</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#65306;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
In-Context Pretraining: Language Modeling Beyond Document Boundaries. (arXiv:2310.10638v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#36328;&#25991;&#26723;&#30340;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36890;&#36807;&#39044;&#27979;&#32473;&#23450;&#25991;&#26723;&#21069;&#32512;&#30340;&#26631;&#35760;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#33021;&#22815;&#30452;&#25509;&#36827;&#34892;&#38271;&#31687;&#29983;&#25104;&#21644;&#25552;&#31034;&#24335;&#20219;&#21153;&#65292;&#36825;&#21487;&#20197;&#31616;&#21270;&#20026;&#25991;&#26723;&#23436;&#25104;&#12290;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#36890;&#36807;&#36830;&#25509;&#38543;&#26426;&#32452;&#21512;&#30340;&#30701;&#25991;&#26723;&#26469;&#35757;&#32451;LMs&#65292;&#20197;&#21019;&#24314;&#36755;&#20837;&#19978;&#19979;&#25991;&#65292;&#20294;&#21069;&#19968;&#20010;&#25991;&#26723;&#23545;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#25991;&#26723;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#20449;&#21495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#65292;&#21363;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26126;&#30830;&#40723;&#21169;&#23427;&#20204;&#36328;&#36234;&#25991;&#26723;&#36793;&#30028;&#36827;&#34892;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#65292;&#20351;&#27599;&#20010;&#19978;&#19979;&#25991;&#21253;&#21547;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25991;&#26723;&#25490;&#24207;&#38382;&#39064;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26377;&#25968;&#21313;&#20159;&#20010;&#25991;&#26723;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#27599;&#20010;&#25991;&#26723;&#20013;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;&#30456;&#20284;&#24615;&#32780;&#19981;&#37325;&#22797;&#20219;&#20309;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) are currently trained to predict tokens given document prefixes, enabling them to directly perform long-form generation and prompting-style tasks which can be reduced to document completion. Existing pretraining pipelines train LMs by concatenating random sets of short documents to create input contexts but the prior documents provide no signal for predicting the next document. We instead present In-Context Pretraining, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. We can do In-Context Pretraining by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#26041;&#27861;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#30340;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#20915;&#31574;&#31574;&#30053;&#25286;&#20998;&#20026;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23454;&#29616;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.07918</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#65306;&#36890;&#36807;&#33258;&#36866;&#24212;&#27169;&#20223;&#23398;&#20064;&#23545;&#21307;&#30103;&#20915;&#31574;&#36827;&#34892;&#24314;&#27169;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning. (arXiv:2310.07918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#26041;&#27861;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#30340;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#20915;&#31574;&#31574;&#30053;&#25286;&#20998;&#20026;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23454;&#29616;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#23398;&#20064;&#26088;&#22312;&#20174;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#20013;&#20272;&#35745;&#21487;&#29702;&#35299;&#30340;&#20915;&#31574;&#31574;&#30053;&#65307;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#36825;&#31181;&#26435;&#34913;&#38480;&#21046;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23545;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#30340;&#35299;&#37322;&#65292;&#20363;&#22914;&#65292;&#23457;&#35745;&#21307;&#30103;&#20915;&#31574;&#30340;&#20559;&#35265;&#21644;&#27425;&#20248;&#23454;&#36341;&#65292;&#25105;&#20204;&#38656;&#35201;&#20915;&#31574;&#36807;&#31243;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;&#29616;&#26377;&#26041;&#27861;&#22522;&#26412;&#19978;&#30001;&#20110;&#23558;&#28508;&#22312;&#20915;&#31574;&#36807;&#31243;&#34920;&#31034;&#20026;&#36890;&#29992;&#31574;&#30053;&#32780;&#36127;&#25285;&#20102;&#36825;&#31181;&#26435;&#34913;&#65292;&#32780;&#23454;&#38469;&#19978;&#20154;&#31867;&#20915;&#31574;&#26159;&#21160;&#24577;&#30340;&#65292;&#21487;&#20197;&#38543;&#19978;&#19979;&#25991;&#20449;&#24687;&#32780;&#22823;&#24133;&#25913;&#21464;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#65288;CPR&#65289;&#65292;&#23558;&#24314;&#27169;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#30340;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#22797;&#26434;&#20915;&#31574;&#31574;&#30053;&#30001;&#29305;&#23450;&#19978;&#19979;&#25991;&#30340;&#31574;&#30053;&#32452;&#25104;&#12290;CPR&#23558;&#27599;&#20010;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#24314;&#27169;&#20026;&#32447;&#24615;&#30340;&#35266;&#23519;-&#21160;&#20316;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Interpretable policy learning seeks to estimate intelligible decision policies from observed actions; however, existing models fall short by forcing a tradeoff between accuracy and interpretability. This tradeoff limits data-driven interpretations of human decision-making process. e.g. to audit medical decisions for biases and suboptimal practices, we require models of decision processes which provide concise descriptions of complex behaviors. Fundamentally, existing approaches are burdened by this tradeoff because they represent the underlying decision process as a universal policy, when in fact human decisions are dynamic and can change drastically with contextual information. Thus, we propose Contextualized Policy Recovery (CPR), which re-frames the problem of modeling complex decision processes as a multi-task learning problem in which complex decision policies are comprised of context-specific policies. CPR models each context-specific policy as a linear observation-to-action mapp
&lt;/p&gt;</description></item><item><title>WiGenAI&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#65292;&#20026;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#33539;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24320;&#21457;&#38887;&#24615;&#30340;AI&#26412;&#22320;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07312</link><description>&lt;p&gt;
WiGenAI: &#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#26080;&#32447;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#32455;
&lt;/p&gt;
&lt;p&gt;
WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models. (arXiv:2310.07312v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07312
&lt;/p&gt;
&lt;p&gt;
WiGenAI&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#65292;&#20026;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#33539;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24320;&#21457;&#38887;&#24615;&#30340;AI&#26412;&#22320;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;GPT-3&#21644;&#31283;&#23450;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#23454;&#29616;&#20102;&#33539;&#24335;&#36716;&#21464;&#65292;&#21521;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21457;&#23637;&#12290;&#20174;&#25968;&#25454;&#36890;&#20449;&#21644;&#32593;&#32476;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#35745;&#23558;&#24191;&#27867;&#24212;&#29992;&#20110;&#26410;&#26469;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#30340;&#26032;&#19968;&#20195;&#20013;&#65292;&#24378;&#35843;&#20102;&#22312;&#26032;&#20852;&#36890;&#20449;&#22330;&#26223;&#20013;&#38656;&#35201;&#26032;&#39062;&#30340;AI&#26412;&#22320;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#20171;&#32461;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#20171;&#32461;&#20102;&#25193;&#25955;&#22411;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#33539;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#36824;&#25552;&#20379;&#20102;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#24320;&#21457;&#20855;&#26377;&#38887;&#24615;&#30340;AI&#26412;&#22320;&#36890;&#20449;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#23637;&#31034;&#20854;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Innovative foundation models, such as GPT-3 and stable diffusion models, have made a paradigm shift in the realm of artificial intelligence (AI) towards generative AI-based systems. In unison, from data communication and networking perspective, AI and machine learning (AI/ML) algorithms are envisioned to be pervasively incorporated into the future generations of wireless communications systems, highlighting the need for novel AI-native solutions for the emergent communication scenarios. In this article, we outline the applications of generative AI in wireless communication systems to lay the foundations for research in this field. Diffusion-based generative models, as the new state-of-the-art paradigm of generative models, are introduced, and their applications in wireless communication systems are discussed. Two case studies are also presented to showcase how diffusion models can be exploited for the development of resilient AI-native communication systems. Specifically, we propose de
&lt;/p&gt;</description></item><item><title>MuseChat&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#23545;&#35805;&#24335;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;&#21644;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#35805;&#20132;&#20114;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#20048;&#26631;&#31614;&#21644;&#33402;&#26415;&#23478;&#20449;&#24687;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#23450;&#21046;&#30340;&#38899;&#20048;&#25512;&#33616;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20010;&#24615;&#21270;&#36873;&#25321;&#20182;&#20204;&#21916;&#27426;&#30340;&#38899;&#20048;&#12290;</title><link>http://arxiv.org/abs/2310.06282</link><description>&lt;p&gt;
MuseChat:&#19968;&#31181;&#35270;&#39057;&#23545;&#35805;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MuseChat: A Conversational Music Recommendation System for Videos. (arXiv:2310.06282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06282
&lt;/p&gt;
&lt;p&gt;
MuseChat&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#23545;&#35805;&#24335;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;&#21644;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#35805;&#20132;&#20114;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#20048;&#26631;&#31614;&#21644;&#33402;&#26415;&#23478;&#20449;&#24687;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#23450;&#21046;&#30340;&#38899;&#20048;&#25512;&#33616;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20010;&#24615;&#21270;&#36873;&#25321;&#20182;&#20204;&#21916;&#27426;&#30340;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;MuseChat&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#23545;&#35805;&#30340;&#38899;&#20048;&#25512;&#33616;&#31995;&#32479;&#12290;&#36825;&#20010;&#29420;&#29305;&#30340;&#24179;&#21488;&#19981;&#20165;&#25552;&#20379;&#20114;&#21160;&#29992;&#25143;&#21442;&#19982;&#65292;&#36824;&#20026;&#36755;&#20837;&#30340;&#35270;&#39057;&#25552;&#20379;&#20102;&#23450;&#21046;&#30340;&#38899;&#20048;&#25512;&#33616;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#25913;&#36827;&#21644;&#20010;&#24615;&#21270;&#20182;&#20204;&#30340;&#38899;&#20048;&#36873;&#25321;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#20197;&#21069;&#30340;&#31995;&#32479;&#20027;&#35201;&#24378;&#35843;&#20869;&#23481;&#30340;&#20860;&#23481;&#24615;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#29992;&#25143;&#20010;&#20307;&#20559;&#22909;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#20363;&#22914;&#65292;&#25152;&#26377;&#30340;&#25968;&#25454;&#38598;&#37117;&#21482;&#25552;&#20379;&#22522;&#26412;&#30340;&#38899;&#20048;-&#35270;&#39057;&#37197;&#23545;&#65292;&#25110;&#32773;&#24102;&#26377;&#38899;&#20048;&#25551;&#36848;&#30340;&#37197;&#23545;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23545;&#35805;&#21512;&#25104;&#26041;&#27861;&#65292;&#27169;&#25311;&#20102;&#29992;&#25143;&#21644;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#20004;&#36718;&#20132;&#20114;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#20048;&#26631;&#31614;&#21644;&#33402;&#26415;&#23478;&#20449;&#24687;&#12290;&#22312;&#36825;&#20010;&#20132;&#20114;&#20013;&#65292;&#29992;&#25143;&#25552;&#20132;&#19968;&#20010;&#35270;&#39057;&#32473;&#31995;&#32479;&#65292;&#31995;&#32479;&#20250;&#25552;&#20379;&#19968;&#20010;&#21512;&#36866;&#30340;&#38899;&#20048;&#29255;&#27573;&#65292;&#24182;&#38468;&#24102;&#35299;&#37322;&#12290;&#20043;&#21518;&#65292;&#29992;&#25143;&#20250;&#34920;&#36798;&#20182;&#20204;&#23545;&#38899;&#20048;&#30340;&#20559;&#22909;&#65292;&#31995;&#32479;&#20250;&#21576;&#29616;&#19968;&#20010;&#25913;&#36827;&#21518;&#30340;&#38899;&#20048;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
We introduce MuseChat, an innovative dialog-based music recommendation system. This unique platform not only offers interactive user engagement but also suggests music tailored for input videos, so that users can refine and personalize their music selections. In contrast, previous systems predominantly emphasized content compatibility, often overlooking the nuances of users' individual preferences. For example, all the datasets only provide basic music-video pairings or such pairings with textual music descriptions. To address this gap, our research offers three contributions. First, we devise a conversation-synthesis method that simulates a two-turn interaction between a user and a recommendation system, which leverages pre-trained music tags and artist information. In this interaction, users submit a video to the system, which then suggests a suitable music piece with a rationale. Afterwards, users communicate their musical preferences, and the system presents a refined music recomme
&lt;/p&gt;</description></item><item><title>&#36817;&#20284;&#27880;&#24847;&#21147;&#26426;&#21046;HyperAttention&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#38271;&#19978;&#19979;&#25991;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#21442;&#25968;&#26469;&#34913;&#37327;&#38382;&#39064;&#30340;&#38590;&#24230;&#12290;HyperAttention&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#36731;&#26494;&#38598;&#25104;&#20854;&#20182;&#24555;&#36895;&#20302;&#32423;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.05869</link><description>&lt;p&gt;
&#36229;&#32423;&#20851;&#27880;&#21147;&#65306;&#36817;&#20284;&#32447;&#24615;&#26102;&#38388;&#19979;&#30340;&#38271;&#19978;&#19979;&#25991;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
HyperAttention: Long-context Attention in Near-Linear Time. (arXiv:2310.05869v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05869
&lt;/p&gt;
&lt;p&gt;
&#36817;&#20284;&#27880;&#24847;&#21147;&#26426;&#21046;HyperAttention&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#38271;&#19978;&#19979;&#25991;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#21442;&#25968;&#26469;&#34913;&#37327;&#38382;&#39064;&#30340;&#38590;&#24230;&#12290;HyperAttention&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#21487;&#36731;&#26494;&#38598;&#25104;&#20854;&#20182;&#24555;&#36895;&#20302;&#32423;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyperAttention&#30340;&#36817;&#20284;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#24212;&#23545;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#20351;&#29992;&#30340;&#38271;&#19978;&#19979;&#25991;&#30340;&#26085;&#30410;&#22797;&#26434;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#65292;&#38500;&#38750;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#26465;&#30446;&#34987;&#38480;&#21046;&#25110;&#30697;&#38453;&#20855;&#26377;&#20302;&#31283;&#23450;&#31209;&#65292;&#21542;&#21017;&#20108;&#27425;&#26102;&#38388;&#26159;&#24517;&#35201;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#21442;&#25968;&#65292;&#29992;&#20110;&#34913;&#37327;&#65306;&#65288;1&#65289;&#26631;&#20934;&#21270;&#27880;&#24847;&#21147;&#30697;&#38453;&#20013;&#30340;&#26368;&#22823;&#21015;&#33539;&#25968;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22312;&#26816;&#27979;&#21644;&#21024;&#38500;&#22823;&#26465;&#30446;&#21518;&#65292;&#38750;&#26631;&#20934;&#21270;&#27880;&#24847;&#21147;&#30697;&#38453;&#20013;&#34892;&#33539;&#25968;&#30340;&#27604;&#29575;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#32454;&#31890;&#24230;&#30340;&#21442;&#25968;&#26469;&#25429;&#25417;&#38382;&#39064;&#30340;&#38590;&#24230;&#12290;&#23613;&#31649;&#20808;&#21069;&#23384;&#22312;&#19979;&#30028;&#65292;&#20294;&#25105;&#20204;&#33021;&#22815;&#23454;&#29616;&#19968;&#20010;&#32447;&#24615;&#26102;&#38388;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#21363;&#20351;&#30697;&#38453;&#20855;&#26377;&#26080;&#30028;&#30340;&#26465;&#30446;&#25110;&#36739;&#22823;&#30340;&#31283;&#23450;&#31209;&#65292;&#21482;&#35201;&#19978;&#36848;&#21442;&#25968;&#36739;&#23567;&#12290;HyperAttention&#20855;&#26377;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#36731;&#26494;&#23481;&#32435;&#20854;&#20182;&#24555;&#36895;&#20302;&#32423;&#23454;&#29616;&#65292;&#29305;&#21035;&#26159;FlashAttention&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an approximate attention mechanism named HyperAttention to address the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs). Recent work suggests that in the worst-case scenario, quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank. We introduce two parameters which measure: (1) the max column norm in the normalized attention matrix, and (2) the ratio of row norms in the unnormalized attention matrix after detecting and removing large entries. We use these fine-grained parameters to capture the hardness of the problem. Despite previous lower bounds, we are able to achieve a linear time sampling algorithm even when the matrix has unbounded entries or a large stable rank, provided the above parameters are small. HyperAttention features a modular design that easily accommodates integration of other fast low-level implementations, particularly FlashAttention.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;Nisan&#30340;&#33258;&#28982;&#35777;&#26126;&#20013;&#30340;&#20998;&#24067;&#24335;PAC&#23398;&#20064;&#65292;&#24182;&#24471;&#21040;&#20102;&#27491;&#38754;&#21644;&#36127;&#38754;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.03641</link><description>&lt;p&gt;
&#20174;Nisan&#30340;&#33258;&#28982;&#35777;&#26126;&#20013;&#30340;&#20998;&#24067;&#24335;PAC&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributional PAC-Learning from Nisan's Natural Proofs. (arXiv:2310.03641v1 [cs.CC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;Nisan&#30340;&#33258;&#28982;&#35777;&#26126;&#20013;&#30340;&#20998;&#24067;&#24335;PAC&#23398;&#20064;&#65292;&#24182;&#24471;&#21040;&#20102;&#27491;&#38754;&#21644;&#36127;&#38754;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Carmosino&#31561;&#20154;&#35777;&#26126;&#20102;Lambda&#30340;&#33258;&#28982;&#35777;&#26126;&#24847;&#21619;&#30528;&#21487;&#20197;&#36890;&#36807;&#22343;&#21248;&#20998;&#24067;&#12289;&#25104;&#21592;&#26597;&#35810;&#26469;&#39640;&#25928;&#23398;&#20064;Lambda&#30005;&#36335;&#65292;&#25105;&#20204;&#32771;&#34385;&#23558;&#36825;&#20010;&#25512;&#35770;&#25512;&#24191;&#21040;&#19981;&#21253;&#21547;AC^0[p]&#30340;Lambda&#21644;&#22312;Valiant&#30340;PAC&#27169;&#22411;&#20013;&#20351;&#29992;&#38543;&#26426;&#31034;&#20363;&#21644;&#20219;&#24847;&#31034;&#20363;&#20998;&#24067;&#23398;&#20064;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#24471;&#21040;&#20102;&#31215;&#26497;&#21644;&#28040;&#26497;&#30340;&#32467;&#26524;&#12290;&#28040;&#26497;&#26041;&#38754;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22914;&#26524;&#23545;&#20110;&#27599;&#20010;&#30005;&#36335;&#31867;Lambda&#65292;&#20174;Lambda&#30340;&#33258;&#28982;&#35777;&#26126;&#21040;&#22312;Valiant&#30340;PAC&#27169;&#22411;&#20013;&#23398;&#20064;Lambda&#30005;&#36335;&#30340;&#25512;&#35770;&#25104;&#31435;&#65292;&#21017;&#23545;O(n^{1.5})-uSVP&#65288;&#21807;&#19968;&#30340;&#26368;&#30701;&#21521;&#37327;&#38382;&#39064;&#65289;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;O(n^{1.5})-SVP&#65288;&#26368;&#30701;&#21521;&#37327;&#38382;&#39064;&#65289;&#21644;O(n^{1.5})-SIVP&#65288;&#26368;&#30701;&#29420;&#31435;&#21521;&#37327;&#38382;&#39064;&#65289;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#37327;&#23376;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
(Abridged) Carmosino et al. (2016) demonstrated that natural proofs of circuit lower bounds for \Lambda imply efficient algorithms for learning \Lambda-circuits, but only over the uniform distribution, with membership queries, and provided \AC^0[p] \subseteq \Lambda. We consider whether this implication can be generalized to \Lambda \not\supseteq \AC^0[p], and to learning algorithms in Valiant's PAC model, which use only random examples and learn over arbitrary example distributions. We give results of both positive and negative flavor.  On the negative side, we observe that if, for every circuit class \Lambda, the implication from natural proofs for \Lambda to learning \Lambda-circuits in Valiant's PAC model holds, then there is a polynomial time solution to O(n^{1.5})-uSVP (unique Shortest Vector Problem), and polynomial time quantum solutions to O(n^{1.5})-SVP (Shortest Vector Problem) and O(n^{1.5})-SIVP (Shortest Independent Vector Problem). This indicates that whether natural pro
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Persona&#32534;&#30721;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#26041;&#27861;&#65292;&#21033;&#29992;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#26469;&#25552;&#39640;&#23545;&#35805;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.16770</link><description>&lt;p&gt;
Persona&#32534;&#30721;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#65306;Persona&#24341;&#23548;&#30340;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Persona-Coded Poly-Encoder: Persona-Guided Multi-Stream Conversational Sentence Scoring. (arXiv:2309.16770v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Persona&#32534;&#30721;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#26041;&#27861;&#65292;&#21033;&#29992;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#26469;&#25552;&#39640;&#23545;&#35805;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#24191;&#27867;&#24212;&#29992;&#20110;&#23545;&#35805;AI&#12290;&#28982;&#32780;&#65292;&#35201;&#21033;&#29992;&#21487;&#20197;&#25552;&#20379;&#23545;&#35805;&#32972;&#26223;&#25110;&#20010;&#24615;&#21270;&#35843;&#25972;&#30340;&#36741;&#21161;&#20449;&#24687;&#20197;&#25552;&#39640;&#23545;&#35805;&#36136;&#37327;&#20173;&#28982;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#20363;&#22914;&#65292;&#20851;&#20110;&#20351;&#29992;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#26469;&#25552;&#39640;&#23545;&#35805;&#36136;&#37327;&#30340;&#30740;&#31350;&#20165;&#26377;&#38480;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#23545;&#35805;AI&#25216;&#26415;&#20063;&#26080;&#27861;&#26377;&#25928;&#22320;&#21033;&#29992;&#26469;&#33258;&#22810;&#31181;&#26469;&#28304;&#30340;&#36741;&#21161;&#25968;&#25454;&#20449;&#21495;&#65292;&#20363;&#22914;&#22810;&#27169;&#24335;&#20132;&#20114;&#25968;&#25454;&#12289;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#31038;&#20250;&#30830;&#23450;&#22240;&#32032;&#25968;&#25454;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Persona&#32534;&#30721;&#22810;&#27969;&#31243;&#23545;&#35805;&#21477;&#23376;&#35780;&#20998;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22810;&#27969;&#31243;&#32534;&#30721;&#26041;&#26696;&#20013;&#30340;&#20010;&#20154;&#35282;&#33394;&#20449;&#24687;&#26469;&#25552;&#39640;&#23545;&#35805;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#23637;&#31034;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#22522;&#20110;&#20010;&#20154;&#35282;&#33394;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#21442;&#32771;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning and deep learning have led to the widespread use of Conversational AI in many practical applications. However, it is still very challenging to leverage auxiliary information that can provide conversational context or personalized tuning to improve the quality of conversations. For example, there has only been limited research on using an individuals persona information to improve conversation quality, and even state-of-the-art conversational AI techniques are unable to effectively leverage signals from heterogeneous sources of auxiliary data, such as multi-modal interaction data, demographics, SDOH data, etc. In this paper, we present a novel Persona-Coded Poly-Encoder method that leverages persona information in a multi-stream encoding scheme to improve the quality of response generation for conversations. To show the efficacy of the proposed method, we evaluate our method on two different persona-based conversational datasets, and compared against 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#26080;&#20154;&#26426;&#30340;&#19978;&#34892;&#35821;&#20041;&#36890;&#20449;&#26041;&#26696;&#65292;&#36890;&#36807;&#28151;&#21512;&#21160;&#20316;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23454;&#29616;&#20102;&#22312;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#21644;&#35745;&#31639;&#33021;&#37327;&#25104;&#26412;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16713</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#36741;&#21161;&#35821;&#20041;&#36890;&#20449;&#19982;&#28151;&#21512;&#21160;&#20316;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
UAV-assisted Semantic Communication with Hybrid Action Reinforcement Learning. (arXiv:2309.16713v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#26080;&#20154;&#26426;&#30340;&#19978;&#34892;&#35821;&#20041;&#36890;&#20449;&#26041;&#26696;&#65292;&#36890;&#36807;&#28151;&#21512;&#21160;&#20316;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#23454;&#29616;&#20102;&#22312;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#21644;&#35745;&#31639;&#33021;&#37327;&#25104;&#26412;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#21033;&#29992;&#26080;&#20154;&#26426;&#36827;&#34892;&#19978;&#34892;&#35821;&#20041;&#36890;&#20449;&#65292;&#20197;&#25552;&#39640;&#20559;&#36828;&#22320;&#21306;&#20803;&#23431;&#23449;&#29992;&#25143;&#30340;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#12290;&#20026;&#20102;&#22312;&#24179;&#34913;&#37325;&#24314;&#36136;&#37327;&#21644;&#35745;&#31639;&#33021;&#37327;&#25104;&#26412;&#20043;&#38388;&#20943;&#23569;&#19978;&#34892;&#25968;&#25454;&#25910;&#38598;&#26102;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#21160;&#20316;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#35821;&#20041;&#27169;&#22411;&#35268;&#27169;&#12289;&#20449;&#36947;&#20998;&#37197;&#12289;&#20256;&#36755;&#21151;&#29575;&#21644;&#26080;&#20154;&#26426;&#36712;&#36857;&#19978;&#20570;&#20986;&#20915;&#31574;&#12290;&#21464;&#37327;&#34987;&#21010;&#20998;&#20026;&#31163;&#25955;&#31867;&#22411;&#21644;&#36830;&#32493;&#31867;&#22411;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36827;&#34892;&#20248;&#21270;&#20197;&#29983;&#25104;&#32452;&#21512;&#21160;&#20316;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#21160;&#20316;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#33021;&#22815;&#22312;&#19981;&#21516;&#21442;&#25968;&#35774;&#32622;&#19979;&#26377;&#25928;&#25552;&#39640;&#19978;&#34892;&#35821;&#20041;&#25968;&#25454;&#25910;&#38598;&#30340;&#25928;&#29575;&#65292;&#24182;&#20248;&#20110;&#22522;&#20934;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we aim to explore the use of uplink semantic communications with the assistance of UAV in order to improve data collection effiicency for metaverse users in remote areas. To reduce the time for uplink data collection while balancing the trade-off between reconstruction quality and computational energy cost, we propose a hybrid action reinforcement learning (RL) framework to make decisions on semantic model scale, channel allocation, transmission power, and UAV trajectory. The variables are classified into discrete type and continuous type, which are optimized by two different RL agents to generate the combined action. Simulation results indicate that the proposed hybrid action reinforcement learning framework can effectively improve the efficiency of uplink semantic data collection under different parameter settings and outperforms the benchmark scenarios.
&lt;/p&gt;</description></item><item><title>beta&#25193;&#25955;&#26159;&#19968;&#31181;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21435;&#25513;&#30422;&#21644;&#21435;&#22122;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#32553;&#25918;&#21644;&#20559;&#31227;&#30340;beta&#20998;&#24067;&#36827;&#34892;&#20056;&#27861;&#36716;&#25442;&#65292;&#23454;&#29616;&#22312;&#26377;&#30028;&#33539;&#22260;&#20869;&#29983;&#25104;&#25968;&#25454;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;KL&#25955;&#24230;&#19978;&#30028;&#36827;&#34892;&#20248;&#21270;&#65292;&#35777;&#26126;&#20102;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.07867</link><description>&lt;p&gt;
Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07867
&lt;/p&gt;
&lt;p&gt;
beta&#25193;&#25955;&#26159;&#19968;&#31181;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21435;&#25513;&#30422;&#21644;&#21435;&#22122;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#32553;&#25918;&#21644;&#20559;&#31227;&#30340;beta&#20998;&#24067;&#36827;&#34892;&#20056;&#27861;&#36716;&#25442;&#65292;&#23454;&#29616;&#22312;&#26377;&#30028;&#33539;&#22260;&#20869;&#29983;&#25104;&#25968;&#25454;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;KL&#25955;&#24230;&#19978;&#30028;&#36827;&#34892;&#20248;&#21270;&#65292;&#35777;&#26126;&#20102;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;beta&#25193;&#25955;&#65292;&#19968;&#31181;&#23558;&#21435;&#25513;&#30422;&#21644;&#21435;&#22122;&#38598;&#25104;&#21040;&#19968;&#36215;&#30340;&#26032;&#22411;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26377;&#30028;&#33539;&#22260;&#20869;&#29983;&#25104;&#25968;&#25454;&#12290;&#20351;&#29992;&#20102;&#32553;&#25918;&#21644;&#20559;&#31227;&#30340;beta&#20998;&#24067;&#65292;beta&#25193;&#25955;&#21033;&#29992;&#20102;&#38543;&#26102;&#38388;&#30340;&#20056;&#27861;&#36716;&#25442;&#26469;&#21019;&#24314;&#27491;&#21521;&#21644;&#21453;&#21521;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#21516;&#26102;&#32500;&#25345;&#30528;&#27491;&#21521;&#36793;&#32536;&#20998;&#24067;&#21644;&#21453;&#21521;&#26465;&#20214;&#20998;&#24067;&#65292;&#32473;&#23450;&#20219;&#24847;&#26102;&#38388;&#28857;&#30340;&#25968;&#25454;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#19981;&#21516;&#65292;&#20256;&#32479;&#27169;&#22411;&#20381;&#36182;&#20110;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#21644;&#37325;&#26032;&#21152;&#26435;&#30340;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#65292;beta&#25193;&#25955;&#26159;&#20056;&#27861;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#20174;KL&#25955;&#24230;&#30340;&#20984;&#24615;&#25512;&#23548;&#20986;&#26469;&#30340;KL&#25955;&#24230;&#19978;&#30028;&#65288;KLUB&#65289;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;KLUB&#30456;&#23545;&#20110;&#36127;ELBO&#26469;&#35828;&#23545;&#20110;&#20248;&#21270;beta&#25193;&#25955;&#26356;&#21152;&#26377;&#25928;&#65292;&#36127;ELBO&#20063;&#21487;&#20197;&#20316;&#20026;&#30456;&#21516;KL&#25955;&#24230;&#30340;KLUB&#65292;&#21482;&#26159;&#20854;&#20004;&#20010;&#21442;&#25968;&#20132;&#25442;&#20102;&#20301;&#32622;&#12290;beta&#25193;&#25955;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;Bregman&#25955;&#24230;&#20026;&#25351;&#26631;&#26469;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce beta diffusion, a novel generative modeling method that integrates demasking and denoising to generate data within bounded ranges. Using scaled and shifted beta distributions, beta diffusion utilizes multiplicative transitions over time to create both forward and reverse diffusion processes, maintaining beta distributions in both the forward marginals and the reverse conditionals, given the data at any point in time. Unlike traditional diffusion-based generative models relying on additive Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived from the convexity of the KL divergence. We demonstrate that the proposed KLUBs are more effective for optimizing beta diffusion compared to negative ELBOs, which can also be derived as the KLUBs of the same KL divergence with its two arguments swapped. The loss function of beta diffusion, expressed in terms of Bregman divergence, furt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SAH-GNN&#65292;&#19968;&#31181;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24212;&#29992;&#32467;&#26500;&#24863;&#30693;&#30340;&#36763;&#31995;&#32479;&#21704;&#23494;&#39039;&#23884;&#20837;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;SAH-GNN&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#23398;&#20064;&#36763;&#32467;&#26500;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#39044;&#23450;&#20041;&#26631;&#20934;&#36763;&#32467;&#26500;&#24418;&#24335;&#30340;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#22270;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#25345;&#29289;&#29702;&#24847;&#20041;&#19978;&#30340;&#33021;&#37327;&#23432;&#24658;&#12290;</title><link>http://arxiv.org/abs/2309.04885</link><description>&lt;p&gt;
&#32467;&#26500;&#24863;&#30693;&#30340;&#36763;&#31995;&#32479;&#21704;&#23494;&#39039;&#65288;&#22270;&#65289;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Symplectic Structure-Aware Hamiltonian (Graph) Embeddings. (arXiv:2309.04885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SAH-GNN&#65292;&#19968;&#31181;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24212;&#29992;&#32467;&#26500;&#24863;&#30693;&#30340;&#36763;&#31995;&#32479;&#21704;&#23494;&#39039;&#23884;&#20837;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;SAH-GNN&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#23398;&#20064;&#36763;&#32467;&#26500;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#39044;&#23450;&#20041;&#26631;&#20934;&#36763;&#32467;&#26500;&#24418;&#24335;&#30340;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#22270;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#25345;&#29289;&#29702;&#24847;&#20041;&#19978;&#30340;&#33021;&#37327;&#23432;&#24658;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#65292;&#22266;&#23450;&#23884;&#20837;&#27969;&#24418;&#30340;&#20551;&#35774;&#24120;&#24120;&#38480;&#21046;&#20102;&#20854;&#23545;&#19981;&#21516;&#22270;&#20960;&#20309;&#32467;&#26500;&#30340;&#36866;&#24212;&#24615;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;GNNs&#65292;&#36890;&#36807;&#23558;&#29289;&#29702;&#23450;&#24459;&#32435;&#20837;&#33410;&#28857;&#29305;&#24449;&#26356;&#26032;&#20013;&#65292;&#26469;&#35299;&#20915;&#36825;&#31867;&#23884;&#20837;&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SAH-GNN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#25512;&#24191;&#21040;&#26356;&#28789;&#27963;&#30340;&#33410;&#28857;&#29305;&#24449;&#26356;&#26032;&#20013;&#12290;&#19982;&#29616;&#26377;&#30340;&#21463;&#21704;&#23494;&#39039;&#21551;&#21457;&#30340;GNNs&#19981;&#21516;&#65292;SAH-GNN&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#29992;&#36763;&#26031;&#33922;&#36153;&#23572;&#27969;&#24418;&#19978;&#30340;&#40654;&#26364;&#20248;&#21270;&#65292;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#28508;&#22312;&#30340;&#36763;&#32467;&#26500;&#65292;&#20174;&#32780;&#35268;&#36991;&#20102;&#29616;&#26377;&#20381;&#36182;&#39044;&#23450;&#20041;&#26631;&#20934;&#36763;&#32467;&#26500;&#24418;&#24335;&#30340;&#21704;&#23494;&#39039;GNNs&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#19968;&#21019;&#26032;&#20351;&#24471;SAH-GNN&#33021;&#22815;&#22312;&#27809;&#26377;&#22823;&#37327;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#36866;&#24212;&#21508;&#31181;&#22270;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#33021;&#37327;&#23432;&#24658;&#65292;&#20351;&#24471;&#38544;&#24335;&#21704;&#23494;&#39039;&#31995;&#32479;&#20855;&#26377;&#29289;&#29702;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
In traditional Graph Neural Networks (GNNs), the assumption of a fixed embedding manifold often limits their adaptability to diverse graph geometries. Recently, Hamiltonian system-inspired GNNs are proposed to address the dynamic nature of such embeddings by incorporating physical laws into node feature updates. In this work, we present SAH-GNN, a novel approach that generalizes Hamiltonian dynamics for more flexible node feature updates. Unlike existing Hamiltonian-inspired GNNs, SAH-GNN employs Riemannian optimization on the symplectic Stiefel manifold to adaptively learn the underlying symplectic structure during training, circumventing the limitations of existing Hamiltonian GNNs that rely on a pre-defined form of standard symplectic structure. This innovation allows SAH-GNN to automatically adapt to various graph datasets without extensive hyperparameter tuning. Moreover, it conserves energy during training such that the implicit Hamiltonian system is physically meaningful. To thi
&lt;/p&gt;</description></item><item><title>QuantEase&#26159;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#37327;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#36880;&#23618;&#37327;&#21270;&#21644;&#22522;&#20110;&#22352;&#26631;&#19979;&#38477;&#30340;&#31639;&#27861;&#65292;&#39640;&#36136;&#37327;&#22320;&#35299;&#20915;&#20102;&#22797;&#26434;&#30340;&#38750;&#20984;&#37327;&#21270;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#23545;&#24322;&#24120;&#20540;&#25935;&#24863;&#30340;&#21464;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.01885</link><description>&lt;p&gt;
QuantEase: &#22522;&#20110;&#20248;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#37327;&#21270;--&#19968;&#31181;&#39640;&#25928;&#32780;&#30452;&#35266;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
QuantEase: Optimization-based Quantization for Language Models -- An Efficient and Intuitive Algorithm. (arXiv:2309.01885v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01885
&lt;/p&gt;
&lt;p&gt;
QuantEase&#26159;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#37327;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#36880;&#23618;&#37327;&#21270;&#21644;&#22522;&#20110;&#22352;&#26631;&#19979;&#38477;&#30340;&#31639;&#27861;&#65292;&#39640;&#36136;&#37327;&#22320;&#35299;&#20915;&#20102;&#22797;&#26434;&#30340;&#38750;&#20984;&#37327;&#21270;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#23545;&#24322;&#24120;&#20540;&#25935;&#24863;&#30340;&#21464;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26222;&#21450;&#65292;&#23545;&#20110;&#33021;&#22815;&#23454;&#29616;&#20854;&#39640;&#25928;&#37096;&#32626;&#30340;&#21387;&#32553;&#25216;&#26415;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#21152;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;LLM&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#12290;&#20511;&#37492;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;QuantEase&#65292;&#19968;&#20010;&#36880;&#23618;&#37327;&#21270;&#26694;&#26550;&#65292;&#20854;&#20013;&#21508;&#20010;&#23618;&#38754;&#32463;&#36807;&#21333;&#29420;&#30340;&#37327;&#21270;&#12290;&#35813;&#38382;&#39064;&#34987;&#35270;&#20026;&#31163;&#25955;&#32467;&#26500;&#21270;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#20419;&#20351;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#22352;&#26631;&#19979;&#38477;&#65288;CD&#65289;&#25216;&#26415;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;&#22522;&#20110;CD&#30340;&#26041;&#27861;&#20026;&#22797;&#26434;&#30340;&#38750;&#20984;&#36880;&#23618;&#37327;&#21270;&#38382;&#39064;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;CD&#26041;&#27861;&#20855;&#26377;&#31616;&#21333;&#30340;&#26356;&#26032;&#27493;&#39588;&#65292;&#20165;&#20381;&#36182;&#20110;&#30697;&#38453;&#21644;&#21521;&#37327;&#36816;&#31639;&#65292;&#36991;&#20813;&#20102;&#30697;&#38453;&#27714;&#36870;&#25110;&#20998;&#35299;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#19968;&#31181;&#23545;&#24322;&#24120;&#20540;&#25935;&#24863;&#30340;&#21464;&#31181;&#26041;&#27861;&#65292;&#20801;&#35768;&#20445;&#30041;&#20855;&#26377;&#23436;&#20840;&#31934;&#24230;&#30340;&#37325;&#35201;&#26435;&#37325;&#65288;&#24322;&#24120;&#20540;&#65289;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rising popularity of Large Language Models (LLMs), there has been an increasing interest in compression techniques that enable their efficient deployment. This study focuses on the Post-Training Quantization (PTQ) of LLMs. Drawing from recent advances, our work introduces QuantEase, a layer-wise quantization framework where individual layers undergo separate quantization. The problem is framed as a discrete-structured non-convex optimization, prompting the development of algorithms rooted in Coordinate Descent (CD) techniques. These CD-based methods provide high-quality solutions to the complex non-convex layer-wise quantization problems. Notably, our CD-based approach features straightforward updates, relying solely on matrix and vector operations, circumventing the need for matrix inversion or decomposition. We also explore an outlier-aware variant of our approach, allowing for retaining significant weights (outliers) with complete precision. Our proposal attains state-of-th
&lt;/p&gt;</description></item><item><title>RLAIF&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;AI&#21453;&#39304;&#20195;&#26367;&#20154;&#31867;&#26631;&#27880;&#20559;&#22909;&#65292;&#30456;&#27604;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#35748;&#21487;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#28508;&#21147;&#35299;&#20915;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.00267</link><description>&lt;p&gt;
RLAIF: &#20351;&#29992;AI&#21453;&#39304;&#26469;&#25193;&#23637;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback. (arXiv:2309.00267v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00267
&lt;/p&gt;
&lt;p&gt;
RLAIF&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;AI&#21453;&#39304;&#20195;&#26367;&#20154;&#31867;&#26631;&#27880;&#20559;&#22909;&#65292;&#30456;&#27604;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#35748;&#21487;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#28508;&#21147;&#35299;&#20915;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#23545;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#26159;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#20559;&#22909;&#26631;&#31614;&#26159;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;RLHF&#21644;&#21033;&#29992;&#29616;&#25104;&#30340;LLM&#36827;&#34892;&#26631;&#35760;&#30340;RL from AI Feedback (RLAIF)&#25216;&#26415;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#33021;&#33719;&#24471;&#31867;&#20284;&#30340;&#25913;&#21892;&#25928;&#26524;&#12290;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#65292;&#20154;&#31867;&#35780;&#20272;&#32773;&#22312;&#32422;70%&#30340;&#26696;&#20363;&#20013;&#37117;&#26356;&#21916;&#27426;RLAIF&#21644;RLHF&#20135;&#29983;&#30340;&#25991;&#26412;&#65292;&#32780;&#19981;&#26159;&#22522;&#20934;&#30340;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#34987;&#35201;&#27714;&#35780;&#20272;RLAIF&#21644;RLHF&#30340;&#25688;&#35201;&#26102;&#65292;&#20154;&#31867;&#20197;&#30456;&#21516;&#30340;&#27604;&#29575;&#26356;&#21916;&#27426;&#20004;&#32773;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;RLAIF&#21487;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#20026;&#20811;&#26381;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is effective at aligning large language models (LLMs) to human preferences, but gathering high quality human preference labels is a key bottleneck. We conduct a head-to-head comparison of RLHF vs. RL from AI Feedback (RLAIF) - a technique where preferences are labeled by an off-the-shelf LLM in lieu of humans, and we find that they result in similar improvements. On the task of summarization, human evaluators prefer generations from both RLAIF and RLHF over a baseline supervised fine-tuned model in ~70% of cases. Furthermore, when asked to rate RLAIF vs. RLHF summaries, humans prefer both at equal rates. These results suggest that RLAIF can yield human-level performance, offering a potential solution to the scalability limitations of RLHF.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#8212;&#8212;&#32447;&#24615;&#25391;&#21160;&#65288;LoC&#65289;&#28608;&#27963;&#20989;&#25968;&#65292;&#23427;&#36890;&#36807;&#23558;&#32447;&#24615;&#36712;&#36857;&#21644;&#25391;&#33633;&#20559;&#24046;&#26080;&#32541;&#34701;&#21512;&#65292;&#25429;&#25417;&#21040;&#20102;&#8220;&#22256;&#24785;&#30340;&#37325;&#35201;&#24615;&#8221;&#30340;&#26412;&#36136;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38656;&#35201;&#21306;&#20998;&#24494;&#22937;&#27169;&#24335;&#30340;&#24773;&#22659;&#20013;&#65292;&#20351;&#29992;LoC&#28608;&#27963;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13670</link><description>&lt;p&gt;
&#32447;&#24615;&#25391;&#21160;&#65306;&#35270;&#35273;&#36716;&#25442;&#22120;&#20013;&#30340;&#22256;&#24785;&#32654;&#23398;
&lt;/p&gt;
&lt;p&gt;
Linear Oscillation: The Aesthetics of Confusion for Vision Transformer. (arXiv:2308.13670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13670
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#8212;&#8212;&#32447;&#24615;&#25391;&#21160;&#65288;LoC&#65289;&#28608;&#27963;&#20989;&#25968;&#65292;&#23427;&#36890;&#36807;&#23558;&#32447;&#24615;&#36712;&#36857;&#21644;&#25391;&#33633;&#20559;&#24046;&#26080;&#32541;&#34701;&#21512;&#65292;&#25429;&#25417;&#21040;&#20102;&#8220;&#22256;&#24785;&#30340;&#37325;&#35201;&#24615;&#8221;&#30340;&#26412;&#36136;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38656;&#35201;&#21306;&#20998;&#24494;&#22937;&#27169;&#24335;&#30340;&#24773;&#22659;&#20013;&#65292;&#20351;&#29992;LoC&#28608;&#27963;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#26159;&#28145;&#24230;&#23398;&#20064;&#30340;&#20851;&#38190;&#65292;&#28145;&#21051;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#21644;&#35757;&#32451;&#21160;&#21147;&#23398;&#12290;&#23427;&#20204;&#19981;&#20165;&#22609;&#36896;&#20102;&#34920;&#31034;&#30340;&#24615;&#36136;&#65292;&#36824;&#20248;&#21270;&#20102;&#25910;&#25947;&#36895;&#24230;&#24182;&#22686;&#24378;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#37492;&#20110;&#36825;&#19968;&#20851;&#38190;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32447;&#24615;&#25391;&#21160;&#65288;LoC&#65289;&#28608;&#27963;&#20989;&#25968;&#65292;&#23450;&#20041;&#20026;$f(x) = x \times \sin(\alpha x + \beta)$&#12290;&#19982;&#20256;&#32479;&#30340;&#28608;&#27963;&#20989;&#25968;&#19981;&#21516;&#65292;LoC&#23558;&#32447;&#24615;&#36712;&#36857;&#19982;&#25391;&#33633;&#20559;&#24046;&#26080;&#32541;&#34701;&#21512;&#12290;&#21629;&#21517;&#20026;&#8220;&#32447;&#24615;&#25391;&#21160;&#8221;&#26159;&#23545;&#20854;&#29420;&#29305;&#23646;&#24615;&#30340;&#33268;&#25964;&#65292;&#21363;&#36890;&#36807;&#21644;&#35856;&#30340;&#25391;&#21160;&#34701;&#20837;&#32447;&#24615;&#28608;&#27963;&#65292;&#25429;&#25417;&#8220;&#22256;&#24785;&#30340;&#37325;&#35201;&#24615;&#8221;&#30340;&#26412;&#36136;&#12290;&#32593;&#32476;&#28608;&#27963;&#20013;&#30340;&#8220;&#25511;&#21046;&#24615;&#22256;&#24785;&#8221;&#27010;&#24565;&#34987;&#35748;&#20026;&#33021;&#22815;&#20419;&#36827;&#26356;&#31283;&#20581;&#30340;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#21306;&#20998;&#24494;&#22937;&#27169;&#24335;&#30340;&#24773;&#22659;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;LoC&#28608;&#27963;&#20989;&#25968;&#26102;&#65292;&#32593;&#32476;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#38544;&#34255;&#30340;&#27169;&#24335;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Activation functions are the linchpins of deep learning, profoundly influencing both the representational capacity and training dynamics of neural networks. They shape not only the nature of representations but also optimize convergence rates and enhance generalization potential. Appreciating this critical role, we present the Linear Oscillation (LoC) activation function, defined as $f(x) = x \times \sin(\alpha x + \beta)$. Distinct from conventional activation functions which primarily introduce non-linearity, LoC seamlessly blends linear trajectories with oscillatory deviations. The nomenclature ``Linear Oscillation'' is a nod to its unique attribute of infusing linear activations with harmonious oscillations, capturing the essence of the 'Importance of Confusion'. This concept of ``controlled confusion'' within network activations is posited to foster more robust learning, particularly in contexts that necessitate discerning subtle patterns. Our empirical studies reveal that, when i
&lt;/p&gt;</description></item><item><title>TpuGraphs&#26159;&#19968;&#31181;&#20851;&#20110;&#22823;&#22411;&#24352;&#37327;&#35745;&#31639;&#22270;&#30340;&#24615;&#33021;&#39044;&#27979;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#20248;&#21270;&#32534;&#35793;&#22120;&#25110;&#33258;&#21160;&#35843;&#20248;&#24037;&#20855;&#30340;&#20915;&#31574;&#65292;&#24182;&#25552;&#20379;&#20102;&#22270;&#30340;&#25191;&#34892;&#26102;&#38388;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.13490</link><description>&lt;p&gt;
TpuGraphs:&#19968;&#31181;&#20851;&#20110;&#22823;&#22411;&#24352;&#37327;&#35745;&#31639;&#22270;&#30340;&#24615;&#33021;&#39044;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs. (arXiv:2308.13490v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13490
&lt;/p&gt;
&lt;p&gt;
TpuGraphs&#26159;&#19968;&#31181;&#20851;&#20110;&#22823;&#22411;&#24352;&#37327;&#35745;&#31639;&#22270;&#30340;&#24615;&#33021;&#39044;&#27979;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#20248;&#21270;&#32534;&#35793;&#22120;&#25110;&#33258;&#21160;&#35843;&#20248;&#24037;&#20855;&#30340;&#20915;&#31574;&#65292;&#24182;&#25552;&#20379;&#20102;&#22270;&#30340;&#25191;&#34892;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#30828;&#20214;&#24615;&#33021;&#27169;&#22411;&#22312;&#20195;&#30721;&#20248;&#21270;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23427;&#20204;&#21487;&#20197;&#24110;&#21161;&#32534;&#35793;&#22120;&#20570;&#20986;&#21551;&#21457;&#24615;&#20915;&#31574;&#65292;&#25110;&#32773;&#24110;&#21161;&#33258;&#21160;&#35843;&#20248;&#24037;&#20855;&#25214;&#21040;&#32473;&#23450;&#31243;&#24207;&#30340;&#26368;&#20339;&#37197;&#32622;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TpuGraphs&#65292;&#19968;&#31181;&#22312;Tensor Processing Units&#65288;TPUs&#65289;&#19978;&#36816;&#34892;&#30340;&#20840;&#24352;&#37327;&#31243;&#24207;&#30340;&#24615;&#33021;&#39044;&#27979;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#31243;&#24207;&#20197;&#35745;&#31639;&#22270;&#30340;&#24418;&#24335;&#34920;&#31034;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#22270;&#34920;&#31034;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#30340;&#20027;&#35201;&#35745;&#31639;&#65292;&#20363;&#22914;&#35757;&#32451;&#21608;&#26399;&#25110;&#25512;&#26029;&#27493;&#39588;&#12290;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#21253;&#21547;&#19968;&#20010;&#35745;&#31639;&#22270;&#12289;&#19968;&#20010;&#32534;&#35793;&#37197;&#32622;&#65292;&#20197;&#21450;&#20351;&#29992;&#35813;&#37197;&#32622;&#32534;&#35793;&#26102;&#22270;&#30340;&#25191;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise hardware performance models play a crucial role in code optimizations. They can assist compilers in making heuristic decisions or aid autotuners in identifying the optimal configuration for a given program. For example, the autotuner for XLA, a machine learning compiler, discovered 10-20% speedup on state-of-the-art models serving substantial production traffic at Google. Although there exist a few datasets for program performance prediction, they target small sub-programs such as basic blocks or kernels. This paper introduces TpuGraphs, a performance prediction dataset on full tensor programs, represented as computational graphs, running on Tensor Processing Units (TPUs). Each graph in the dataset represents the main computation of a machine learning workload, e.g., a training epoch or an inference step. Each data sample contains a computational graph, a compilation configuration, and the execution time of the graph when compiled with the configuration. The graphs in the datas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30005;&#22270;&#25968;&#25454;&#30340;&#21160;&#24577;&#29983;&#23384;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#24515;&#25615;&#39588;&#20572;&#21518;&#26127;&#36855;&#24739;&#32773;&#30340;&#31070;&#32463;&#23398;&#39044;&#21518;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#26102;&#28857;&#24739;&#32773;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#25552;&#20379;&#33487;&#37266;&#25110;&#27515;&#20129;&#30340;&#27010;&#29575;&#12290;&#36825;&#26159;&#30446;&#21069;&#24050;&#30693;&#30340;&#31532;&#19968;&#20010;&#20851;&#20110;&#36825;&#19968;&#39046;&#22495;&#30340;&#21160;&#24577;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2308.11645</link><description>&lt;p&gt;
&#20351;&#29992;&#33041;&#30005;&#22270;&#25968;&#25454;&#30340;&#24515;&#25615;&#39588;&#20572;&#21518;&#26127;&#36855;&#24739;&#32773;&#30340;&#31070;&#32463;&#23398;&#39044;&#21518;:&#19968;&#31181;&#20855;&#26377;&#31454;&#20105;&#39118;&#38505;&#30340;&#21160;&#24577;&#29983;&#23384;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Neurological Prognostication of Post-Cardiac-Arrest Coma Patients Using EEG Data: A Dynamic Survival Analysis Framework with Competing Risks. (arXiv:2308.11645v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#30005;&#22270;&#25968;&#25454;&#30340;&#21160;&#24577;&#29983;&#23384;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#24515;&#25615;&#39588;&#20572;&#21518;&#26127;&#36855;&#24739;&#32773;&#30340;&#31070;&#32463;&#23398;&#39044;&#21518;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#26102;&#28857;&#24739;&#32773;&#30340;&#33041;&#30005;&#22270;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#25552;&#20379;&#33487;&#37266;&#25110;&#27515;&#20129;&#30340;&#27010;&#29575;&#12290;&#36825;&#26159;&#30446;&#21069;&#24050;&#30693;&#30340;&#31532;&#19968;&#20010;&#20851;&#20110;&#36825;&#19968;&#39046;&#22495;&#30340;&#21160;&#24577;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#24515;&#25615;&#39588;&#20572;&#22797;&#33487;&#36827;&#20837;&#26127;&#36855;&#29366;&#24577;&#30340;&#24739;&#32773;&#38754;&#20020;&#30528;&#36739;&#39640;&#30340;&#27515;&#20129;&#39118;&#38505;&#12290;&#39044;&#27979;&#36825;&#20123;&#24739;&#32773;&#30340;&#31070;&#32463;&#23398;&#32467;&#23616;&#65288;&#31070;&#32463;&#23398;&#39044;&#21518;&#20219;&#21153;&#65289;&#21487;&#20197;&#24110;&#21161;&#20915;&#31574;&#27835;&#30103;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39318;&#20010;&#21160;&#24577;&#26694;&#26550;&#65292;&#20351;&#29992;&#33041;&#30005;&#22270;&#25968;&#25454;&#23545;&#24515;&#25615;&#39588;&#20572;&#21518;&#26127;&#36855;&#24739;&#32773;&#36827;&#34892;&#31070;&#32463;&#23398;&#39044;&#21518;&#65306;&#25105;&#20204;&#30340;&#26694;&#26550;&#26681;&#25454;&#38543;&#30528;&#26356;&#22810;&#33041;&#30005;&#22270;&#25968;&#25454;&#30340;&#33719;&#24471;&#65292;&#38543;&#26102;&#38388;&#20026;&#24739;&#32773;&#20570;&#20986;&#39044;&#27979;&#65292;&#24182;&#19988;&#19981;&#21516;&#30340;&#35757;&#32451;&#24739;&#32773;&#21487;&#29992;&#30340;&#33041;&#30005;&#22270;&#26102;&#38388;&#24207;&#21015;&#30340;&#38271;&#24230;&#21487;&#33021;&#23384;&#22312;&#24046;&#24322;&#12290;&#39044;&#27979;&#21487;&#36890;&#36807;&#26102;&#38388;-&#20107;&#20214;&#32467;&#26524;&#65288;&#33487;&#37266;&#26102;&#38388;&#25110;&#27515;&#20129;&#26102;&#38388;&#65289;&#25110;&#24739;&#32773;&#22312;&#22810;&#20010;&#26102;&#38388;&#27573;&#33487;&#37266;&#25110;&#27515;&#20129;&#30340;&#27010;&#29575;&#26469;&#34920;&#36848;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#20219;&#20309;&#25903;&#25345;&#31454;&#20105;&#39118;&#38505;&#30340;&#21160;&#24577;&#29983;&#23384;&#20998;&#26512;&#27169;&#22411;&#65292;&#20197;&#20272;&#35745;&#24739;&#32773;&#27700;&#24179;&#30340;&#32047;&#31215;&#21457;&#29983;&#20989;&#25968;&#12290;&#25105;&#20204;&#32771;&#34385;&#24739;&#32773;&#39318;&#20808;&#20986;&#29616;&#30340;&#19977;&#31181;&#31454;&#20105;&#39118;&#38505;&#65306;&#33487;&#37266;&#12289;&#27515;&#20129;&#25110;&#21457;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patients resuscitated from cardiac arrest who enter a coma are at high risk of death. Forecasting neurological outcomes of these patients (the task of neurological prognostication) could help with treatment decisions. In this paper, we propose, to the best of our knowledge, the first dynamic framework for neurological prognostication of post-cardiac-arrest comatose patients using EEG data: our framework makes predictions for a patient over time as more EEG data become available, and different training patients' available EEG time series could vary in length. Predictions are phrased in terms of either time-to-event outcomes (time-to-awakening or time-to-death) or as the patient's probability of awakening or of dying across multiple time horizons. Our framework uses any dynamic survival analysis model that supports competing risks in the form of estimating patient-level cumulative incidence functions. We consider three competing risks as to what happens first to a patient: awakening, bei
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#24191;&#20041;Onsager&#21407;&#29702;&#30340;&#24179;&#21488;&#65292;&#21487;&#20197;&#20174;&#24494;&#35266;&#36712;&#36857;&#30340;&#35266;&#23519;&#20013;&#23398;&#20064;&#20219;&#24847;&#38543;&#26426;&#32791;&#25955;&#31995;&#32479;&#30340;&#23439;&#35266;&#21160;&#21147;&#23398;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.04119</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26500;&#24314;&#23450;&#21046;&#28909;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Constructing Custom Thermodynamics Using Deep Learning. (arXiv:2308.04119v1 [cond-mat.soft])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#24191;&#20041;Onsager&#21407;&#29702;&#30340;&#24179;&#21488;&#65292;&#21487;&#20197;&#20174;&#24494;&#35266;&#36712;&#36857;&#30340;&#35266;&#23519;&#20013;&#23398;&#20064;&#20219;&#24847;&#38543;&#26426;&#32791;&#25955;&#31995;&#32479;&#30340;&#23439;&#35266;&#21160;&#21147;&#23398;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#24212;&#29992;&#26159;&#22522;&#20110;&#20808;&#21069;&#31215;&#32047;&#30340;&#25968;&#25454;&#20197;&#21450;&#24050;&#30693;&#30340;&#29289;&#29702;&#21407;&#29702;&#65288;&#21253;&#25324;&#23545;&#31216;&#24615;&#21644;&#23432;&#24658;&#23450;&#24459;&#65289;&#25552;&#20379;&#30340;&#38480;&#21046;&#65292;&#36827;&#34892;&#33258;&#21160;&#31185;&#23398;&#21457;&#29616;&#12290;&#36825;&#26679;&#30340;&#33258;&#21160;&#20551;&#35774;&#21019;&#24314;&#21644;&#39564;&#35777;&#21487;&#20197;&#24110;&#21161;&#31185;&#23398;&#23478;&#30740;&#31350;&#22797;&#26434;&#30340;&#29616;&#35937;&#65292;&#20256;&#32479;&#30340;&#29289;&#29702;&#30452;&#35273;&#21487;&#33021;&#26080;&#27861;&#24212;&#23545;&#12290;&#23588;&#20854;&#37325;&#35201;&#30340;&#26159;&#22797;&#26434;&#30340;&#21160;&#24577;&#31995;&#32479;&#65292;&#20854;&#26102;&#38388;&#28436;&#21464;&#21463;&#21040;&#21464;&#21270;&#30340;&#22806;&#37096;&#21442;&#25968;&#30340;&#24378;&#28872;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#24191;&#20041;Onsager&#21407;&#29702;&#30340;&#24179;&#21488;&#65292;&#20174;&#24494;&#35266;&#36712;&#36857;&#30340;&#35266;&#23519;&#20013;&#30452;&#25509;&#23398;&#20064;&#20219;&#24847;&#38543;&#26426;&#32791;&#25955;&#31995;&#32479;&#30340;&#23439;&#35266;&#21160;&#21147;&#23398;&#25551;&#36848;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#37027;&#20123;&#24494;&#35266;&#25551;&#36848;&#23436;&#25972;&#19981;&#20999;&#23454;&#38469;&#12289;&#26500;&#24314;&#29702;&#35770;&#23439;&#35266;&#27169;&#22411;&#38656;&#35201;&#24191;&#27867;&#39046;&#22495;&#30693;&#35782;&#25110;&#35797;&#38169;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#27169;&#25311;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most exciting applications of AI is automated scientific discovery based on previously amassed data, coupled with restrictions provided by the known physical principles, including symmetries and conservation laws. Such automated hypothesis creation and verification can assist scientists in studying complex phenomena, where traditional physical intuition may fail. Of particular importance are complex dynamic systems where their time evolution is strongly influenced by varying external parameters. In this paper we develop a platform based on a generalised Onsager principle to learn macroscopic dynamical descriptions of arbitrary stochastic dissipative systems directly from observations of their microscopic trajectories. We focus on systems whose complexity and sheer sizes render complete microscopic description impractical, and constructing theoretical macroscopic models requires extensive domain knowledge or trial-and-error. Our machine learning approach addresses this by sim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#20915;&#31574;&#31995;&#32479;&#20248;&#21270;&#20013;&#26799;&#24230;&#21453;&#39304;&#31232;&#32570;&#25110;&#26080;&#25928;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#21644;&#35282;&#33394;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#23545;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#25928;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.00629</link><description>&lt;p&gt;
Hessian-Aware Bayesian Optimization for Decision Making Systems - &#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20915;&#31574;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hessian-Aware Bayesian Optimization for Decision Making Systems. (arXiv:2308.00629v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#20915;&#31574;&#31995;&#32479;&#20248;&#21270;&#20013;&#26799;&#24230;&#21453;&#39304;&#31232;&#32570;&#25110;&#26080;&#25928;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#21644;&#35282;&#33394;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#23545;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#25928;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20248;&#21270;&#20915;&#31574;&#31995;&#32479;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#26799;&#24230;&#26041;&#27861;&#65292;&#38656;&#35201;&#20174;&#29615;&#22659;&#20013;&#33719;&#21462;&#26377;&#20449;&#24687;&#37327;&#30340;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#24403;&#21453;&#39304;&#31232;&#32570;&#25110;&#32773;&#26080;&#20449;&#24687;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#31561;&#26080;&#23548;&#25968;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#23545;&#26799;&#24230;&#21453;&#39304;&#36136;&#37327;&#30340;&#20381;&#36182;&#65292;&#20294;&#22312;&#22797;&#26434;&#20915;&#31574;&#31995;&#32479;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#24448;&#24448;&#38590;&#20197;&#25193;&#23637;&#12290;&#22914;&#26524;&#31995;&#32479;&#38656;&#35201;&#22810;&#20010;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20114;&#21160;&#26469;&#23454;&#29616;&#20849;&#21516;&#30446;&#26631;&#65292;&#36825;&#20010;&#38382;&#39064;&#23601;&#21152;&#21095;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#32500;&#24230;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#22810;&#23618;&#26550;&#26500;&#65292;&#36890;&#36807;&#35282;&#33394;&#30340;&#27010;&#24565;&#26469;&#24314;&#27169;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#21160;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#24863;&#30693;&#28023;&#26862;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#39640;&#25928;&#22320;&#20248;&#21270;&#30001;&#22823;&#37327;&#21442;&#25968;&#21442;&#25968;&#21270;&#30340;&#22810;&#23618;&#26550;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;(HA-GP-UCB)&#22312;&#25928;&#26524;&#19978;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many approaches for optimizing decision making systems rely on gradient based methods requiring informative feedback from the environment. However, in the case where such feedback is sparse or uninformative, such approaches may result in poor performance. Derivative-free approaches such as Bayesian Optimization mitigate the dependency on the quality of gradient feedback, but are known to scale poorly in the high-dimension setting of complex decision making systems. This problem is exacerbated if the system requires interactions between several actors cooperating to accomplish a shared goal. To address the dimensionality challenge, we propose a compact multi-layered architecture modeling the dynamics of actor interactions through the concept of role. Additionally, we introduce Hessian-aware Bayesian Optimization to efficiently optimize the multi-layered architecture parameterized by a large number of parameters. Experimental results demonstrate that our method (HA-GP-UCB) works effectiv
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;16&#20301;&#35745;&#31639;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Adam&#20248;&#21270;&#22120;&#30340;&#26032;&#26041;&#27861;&#26469;&#25552;&#39640;16&#20301;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.16189</link><description>&lt;p&gt;
&#29992;&#20110;16&#20301;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training. (arXiv:2307.16189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;16&#20301;&#35745;&#31639;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Adam&#20248;&#21270;&#22120;&#30340;&#26032;&#26041;&#27861;&#26469;&#25552;&#39640;16&#20301;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;16&#20301;&#35745;&#31639;&#20013;&#20351;&#29992;&#27969;&#34892;&#30340;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;RMSProp&#21644;Adam&#65289;&#26102;&#35266;&#23519;&#21040;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#30340;&#22797;&#26434;&#24615;&#12290;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#36890;&#24120;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#38454;&#27573;&#20013;&#20986;&#29616;&#65292;&#23548;&#33268;&#23398;&#20064;&#36807;&#31243;&#21463;&#21040;&#24178;&#25200;&#65292;&#20174;&#32780;&#22952;&#30861;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26377;&#25928;&#37096;&#32626;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21333;&#19968;&#36229;&#21442;&#25968;epsilon&#26159;&#36825;&#31181;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#23545;16&#20301;&#35745;&#31639;&#20013;&#36825;&#20123;&#20248;&#21270;&#22120;&#20013;epsilon&#30340;&#20316;&#29992;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#32034;&#65292;&#21457;&#29616;&#24494;&#35843;&#20854;&#20540;&#21487;&#20197;&#24674;&#22797;RMSProp&#21644;Adam&#30340;&#21151;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#21033;&#29992;16&#20301;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#34987;&#21457;&#29616;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;Adam&#20248;&#21270;&#22120;&#30340;&#26356;&#26032;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;16&#20301;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research, we delve into the intricacies of the numerical instability observed in 16-bit computations of machine learning models, particularly when employing popular optimization algorithms such as RMSProp and Adam. This instability is commonly experienced during the training phase of deep neural networks, leading to disrupted learning processes and hindering the effective deployment of such models. We identify the single hyperparameter, epsilon, as the main culprit behind this numerical instability. An in-depth exploration of the role of epsilon in these optimizers within 16-bit computations reveals that a minor adjustment of its value can restore the functionality of RMSProp and Adam, consequently enabling the effective utilization of 16-bit neural networks. We propose a novel method to mitigate the identified numerical instability issues. This method capitalizes on the updates from the Adam optimizer and significantly improves the robustness of the learning process in 16-bit 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;CNN&#21069;&#21521;&#22788;&#29702;&#20013;&#30340;&#36873;&#25321;&#24615;&#26059;&#36716;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#21270;&#25968;&#23398;&#24037;&#20855;&#23545;&#25968;&#25454;&#36827;&#34892;&#32479;&#35745;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#20154;&#33041;&#22312;&#25968;&#25454;&#22788;&#29702;&#27169;&#24335;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.15090</link><description>&lt;p&gt;
&#29702;&#35299;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21069;&#21521;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Understanding Forward Process of Convolutional Neural Network. (arXiv:2307.15090v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;CNN&#21069;&#21521;&#22788;&#29702;&#20013;&#30340;&#36873;&#25321;&#24615;&#26059;&#36716;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#21270;&#25968;&#23398;&#24037;&#20855;&#23545;&#25968;&#25454;&#36827;&#34892;&#32479;&#35745;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#20154;&#33041;&#22312;&#25968;&#25454;&#22788;&#29702;&#27169;&#24335;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;CNN&#21069;&#21521;&#22788;&#29702;&#20013;&#30340;&#36873;&#25321;&#24615;&#26059;&#36716;&#12290;&#23427;&#38416;&#26126;&#20102;&#28608;&#27963;&#20989;&#25968;&#20316;&#20026;&#19968;&#20010;&#20998;&#26512;&#26426;&#21046;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#30340;&#26059;&#36716;&#26041;&#38754;&#32479;&#19968;&#37327;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#23450;&#20041;&#30340;&#26041;&#27861;&#35770;&#21453;&#26144;&#20102;&#36827;&#31243;&#32593;&#32476;&#26681;&#25454;&#32479;&#35745;&#25351;&#26631;&#21306;&#20998;&#36755;&#20837;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#36890;&#36807;&#24212;&#29992;&#32467;&#26500;&#21270;&#30340;&#25968;&#23398;&#24037;&#20855;&#26469;&#29702;&#35299;&#25110;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#36824;&#25581;&#31034;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#20154;&#33041;&#22312;&#25968;&#25454;&#22788;&#29702;&#27169;&#24335;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reveal the selective rotation in the CNNs' forward processing. It elucidates the activation function as a discerning mechanism that unifies and quantizes the rotational aspects of the input data. Experiments show how this defined methodology reflects the progress network distinguish inputs based on statistical indicators, which can be comprehended or analyzed by applying structured mathematical tools. Our findings also unveil the consistency between artificial neural networks and the human brain in their data processing pattern.
&lt;/p&gt;</description></item><item><title>Duet&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#20013;&#39640;&#25104;&#26412;&#21644;&#38590;&#20197;&#21306;&#20998;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#39044;&#27979;&#36807;&#31243;&#25913;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13494</link><description>&lt;p&gt;
Duet: &#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Duet: efficient and scalable hybriD neUral rElation undersTanding. (arXiv:2307.13494v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13494
&lt;/p&gt;
&lt;p&gt;
Duet&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31070;&#32463;&#20851;&#31995;&#29702;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#20013;&#39640;&#25104;&#26412;&#21644;&#38590;&#20197;&#21306;&#20998;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#39044;&#27979;&#36807;&#31243;&#25913;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#30340;&#22522;&#25968;&#20272;&#35745;&#26041;&#27861;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#21462;&#24471;&#20102;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30001;&#20110;&#22312;&#22788;&#29702;&#33539;&#22260;&#26597;&#35810;&#26102;&#20351;&#29992;&#30340;&#37319;&#26679;&#26041;&#27861;&#32780;&#23548;&#33268;&#20272;&#35745;&#25104;&#26412;&#36739;&#39640;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#37319;&#26679;&#26041;&#27861;&#20063;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#21306;&#20998;&#65292;&#22240;&#27492;&#26469;&#33258;&#26597;&#35810;&#24037;&#20316;&#36127;&#36733;&#30340;&#30417;&#30563;&#20449;&#21495;&#24456;&#38590;&#35757;&#32451;&#27169;&#22411;&#20197;&#25552;&#39640;&#22522;&#25968;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#30830;&#23450;&#24615;&#24314;&#27169;&#26041;&#27861;&#65288;Duet&#65289;&#29992;&#20110;&#22522;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;Duet&#21487;&#20197;&#20197;&#26356;&#20302;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#25104;&#26412;&#30452;&#25509;&#20272;&#35745;&#33539;&#22260;&#26597;&#35810;&#30340;&#22522;&#25968;&#65292;&#24182;&#19988;&#20197;&#21487;&#21306;&#20998;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#30001;&#20110;&#27492;&#26041;&#27861;&#30340;&#39044;&#27979;&#36807;&#31243;&#26159;&#21487;&#24494;&#20998;&#30340;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#20272;&#35745;&#35823;&#24046;&#36739;&#22823;&#30340;&#26597;&#35810;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#20197;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardinality estimation methods based on probability distribution estimation have achieved high-precision estimation results compared to traditional methods. However, the most advanced methods suffer from high estimation costs due to the sampling method they use when dealing with range queries. Also, such a sampling method makes them difficult to differentiate, so the supervision signal from the query workload is difficult to train the model to improve the accuracy of cardinality estimation. In this paper, we propose a new hybrid and deterministic modeling approach (Duet) for the cardinality estimation problem which has better efficiency and scalability compared to previous approaches. Duet allows for direct cardinality estimation of range queries with significantly lower time and memory costs, as well as in a differentiable form. As the prediction process of this approach is differentiable, we can incorporate queries with larger model estimation errors into the training process to addr
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#20004;&#20010;&#29702;&#35770;&#65292;&#21457;&#23637;&#20102;&#19968;&#31181;&#20869;&#22312;&#39537;&#21160;&#30340;&#22270;&#25506;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#23558;&#25299;&#25169;&#29305;&#24449;&#20316;&#20026;&#22870;&#21169;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#25506;&#32034;&#12290;&#22312;&#22810;&#31867;&#21512;&#25104;&#29983;&#25104;&#22270;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#29615;&#22659;&#65292;&#36824;&#21487;&#20197;&#36827;&#34892;&#26356;&#38271;&#30340;&#25506;&#32034;&#27493;&#34892;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#36138;&#23146;&#35780;&#20272;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.04962</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#32593;&#32476;&#29702;&#35770;&#36827;&#34892;&#20869;&#22312;&#39537;&#21160;&#30340;&#22270;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Intrinsically motivated graph exploration using network theories of human curiosity. (arXiv:2307.04962v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04962
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#20004;&#20010;&#29702;&#35770;&#65292;&#21457;&#23637;&#20102;&#19968;&#31181;&#20869;&#22312;&#39537;&#21160;&#30340;&#22270;&#25506;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#23558;&#25299;&#25169;&#29305;&#24449;&#20316;&#20026;&#22870;&#21169;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#25506;&#32034;&#12290;&#22312;&#22810;&#31867;&#21512;&#25104;&#29983;&#25104;&#22270;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#29615;&#22659;&#65292;&#36824;&#21487;&#20197;&#36827;&#34892;&#26356;&#38271;&#30340;&#25506;&#32034;&#27493;&#34892;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#36138;&#23146;&#35780;&#20272;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#22312;&#39537;&#21160;&#30340;&#25506;&#32034;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#29992;&#36884;&#65292;&#21363;&#20351;&#27809;&#26377;&#39069;&#22806;&#30340;&#22806;&#22312;&#22870;&#21169;&#12290;&#24403;&#29615;&#22659;&#33258;&#28982;&#34920;&#31034;&#20026;&#22270;&#26102;&#65292;&#22914;&#20309;&#26368;&#22909;&#22320;&#24341;&#23548;&#25506;&#32034;&#20173;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#31867;&#22909;&#22855;&#24515;&#30340;&#20004;&#20010;&#29702;&#35770;&#65306;&#20449;&#24687;&#24046;&#29702;&#35770;&#21644;&#21387;&#32553;&#36827;&#23637;&#29702;&#35770;&#65292;&#26469;&#28608;&#21169;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#36827;&#34892;&#25506;&#32034;&#12290;&#36825;&#20123;&#29702;&#35770;&#23558;&#22909;&#22855;&#24515;&#35270;&#20026;&#23545;&#29615;&#22659;&#20013;&#35775;&#38382;&#33410;&#28857;&#25152;&#24341;&#21457;&#30340;&#23376;&#22270;&#30340;&#25299;&#25169;&#29305;&#24449;&#36827;&#34892;&#20248;&#21270;&#30340;&#20869;&#22312;&#21160;&#26426;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25552;&#20986;&#30340;&#29305;&#24449;&#20316;&#20026;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#22870;&#21169;&#12290;&#22312;&#22810;&#20010;&#31867;&#21035;&#30340;&#21512;&#25104;&#29983;&#25104;&#22270;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#20195;&#29702;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#29615;&#22659;&#21644;&#27604;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#38271;&#30340;&#25506;&#32034;&#24615;&#27493;&#34892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#39640;&#20110;&#30456;&#20851;&#25299;&#25169;&#23646;&#24615;&#30340;&#36138;&#23146;&#35780;&#20272;&#12290;&#25152;&#25552;&#20986;&#30340;&#20869;&#22312;&#21160;&#26426;&#20135;&#29983;&#30340;&#22870;&#21169;&#22312;&#22810;&#31867;&#21512;&#25104;&#29983;&#25104;&#22270;&#29983;&#25104;&#19978;&#25512;&#24191;&#33391;&#22909;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#26399;&#38388;&#33021;&#22815;&#22312;&#26356;&#22823;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#26356;&#38271;&#30340;&#25506;&#32034;&#27493;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intrinsically motivated exploration has proven useful for reinforcement learning, even without additional extrinsic rewards. When the environment is naturally represented as a graph, how to guide exploration best remains an open question. In this work, we propose a novel approach for exploring graph-structured data motivated by two theories of human curiosity: the information gap theory and the compression progress theory. The theories view curiosity as an intrinsic motivation to optimize for topological features of subgraphs induced by the visited nodes in the environment. We use these proposed features as rewards for graph neural-network-based reinforcement learning. On multiple classes of synthetically generated graphs, we find that trained agents generalize to larger environments and to longer exploratory walks than are seen during training. Our method computes more efficiently than the greedy evaluation of the relevant topological properties. The proposed intrinsic motivations bea
&lt;/p&gt;</description></item><item><title>GeoPhy&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#12289;&#23436;&#20840;&#21487;&#24494;&#30340;&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#20960;&#20309;&#31354;&#38388;&#20013;&#34920;&#31034;&#25299;&#25169;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#25512;&#26029;&#65292;&#20811;&#26381;&#20102;&#19981;&#38480;&#21046;&#25299;&#25169;&#32467;&#26500;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.03675</link><description>&lt;p&gt;
GeoPhy: &#21033;&#29992;&#20960;&#20309;&#26799;&#24230;&#23454;&#29616;&#21487;&#24494;&#20998;&#30340;&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
GeoPhy: Differentiable Phylogenetic Inference via Geometric Gradients of Tree Topologies. (arXiv:2307.03675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03675
&lt;/p&gt;
&lt;p&gt;
GeoPhy&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#12289;&#23436;&#20840;&#21487;&#24494;&#30340;&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36830;&#32493;&#20960;&#20309;&#31354;&#38388;&#20013;&#34920;&#31034;&#25299;&#25169;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#21464;&#20998;&#25512;&#26029;&#65292;&#20811;&#26381;&#20102;&#19981;&#38480;&#21046;&#25299;&#25169;&#32467;&#26500;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;&#26159;&#22312;&#20998;&#23376;&#36827;&#21270;&#27169;&#22411;&#22522;&#30784;&#19978;&#36827;&#34892;&#30340;&#65292;&#23427;&#23545;&#20110;&#29702;&#35299;&#29983;&#29289;&#25968;&#25454;&#20013;&#30340;&#36827;&#21270;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#12290;&#32771;&#34385;&#21040;&#36827;&#21270;&#26641;&#21464;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21253;&#25324;&#26641;&#25299;&#25169;&#32467;&#26500;&#21644;&#20998;&#25903;&#19978;&#30340;&#36827;&#21270;&#36317;&#31163;&#65292;&#23545;&#20110;&#20934;&#30830;&#22320;&#20174;&#20998;&#23376;&#25968;&#25454;&#20013;&#25512;&#26029;&#29289;&#31181;&#20851;&#31995;&#20197;&#21450;&#38656;&#35201;&#36827;&#34892;&#21464;&#37327;&#36793;&#32536;&#21270;&#30340;&#20219;&#21153;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#26159;&#24320;&#21457;&#21487;&#25193;&#23637;&#12289;&#23454;&#29992;&#27169;&#22411;&#30340;&#20851;&#38190;&#65292;&#28982;&#32780;&#65292;&#22312;&#19981;&#38480;&#21046;&#21487;&#33021;&#30340;&#26641;&#25299;&#25169;&#32467;&#26500;&#30340;&#32452;&#21512;&#25968;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#23436;&#20840;&#21487;&#24494;&#30340;&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;&#20844;&#24335;&#65292;&#21033;&#29992;&#36830;&#32493;&#20960;&#20309;&#31354;&#38388;&#20013;&#30340;&#25299;&#25169;&#20998;&#24067;&#26469;&#34920;&#31034;&#12290;&#36890;&#36807;&#23545;&#35774;&#35745;&#31354;&#38388;&#21644;&#28176;&#36817;&#30697;&#30340;&#23454;&#38469;&#32771;&#34385;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;GeoPhy&#21487;&#20197;&#23454;&#29616;&#21464;&#20998;&#25512;&#26029;&#32780;&#19981;&#38480;&#21046;&#25299;&#25169;&#32467;&#26500;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phylogenetic inference, grounded in molecular evolution models, is essential for understanding the evolutionary relationships in biological data. Accounting for the uncertainty of phylogenetic tree variables, which include tree topologies and evolutionary distances on branches, is crucial for accurately inferring species relationships from molecular data and tasks requiring variable marginalization. Variational Bayesian methods are key to developing scalable, practical models; however, it remains challenging to conduct phylogenetic inference without restricting the combinatorially vast number of possible tree topologies. In this work, we introduce a novel, fully differentiable formulation of phylogenetic inference that leverages a unique representation of topological distributions in continuous geometric spaces. Through practical considerations on design spaces and control variates for gradient estimations, our approach, GeoPhy, enables variational inference without limiting the topolo
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#24635;&#32467;&#20102;&#22312;&#28857;&#20113;&#20998;&#31867;&#20013;&#38024;&#23545;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#30340;&#24403;&#21069;&#36827;&#23637;&#65292;&#20171;&#32461;&#20102;&#23545;&#25239;&#25915;&#20987;&#21407;&#29702;&#21644;&#23545;&#25239;&#26679;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#21450;&#38450;&#24481;&#31574;&#30053;&#20998;&#31867;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.00309</link><description>&lt;p&gt;
&#23545;3D&#28857;&#20113;&#20998;&#31867;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks and Defenses on 3D Point Cloud Classification: A Survey. (arXiv:2307.00309v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00309
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#24635;&#32467;&#20102;&#22312;&#28857;&#20113;&#20998;&#31867;&#20013;&#38024;&#23545;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#30340;&#24403;&#21069;&#36827;&#23637;&#65292;&#20171;&#32461;&#20102;&#23545;&#25239;&#25915;&#20987;&#21407;&#29702;&#21644;&#23545;&#25239;&#26679;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#20197;&#21450;&#38450;&#24481;&#31574;&#30053;&#20998;&#31867;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;2D&#35270;&#35273;&#39046;&#22495;&#24050;&#32463;&#25104;&#21151;&#35299;&#20915;&#20102;&#21508;&#31181;&#20219;&#21153;&#65292;&#25104;&#20026;&#20027;&#27969;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#12290;&#26368;&#36817;&#65292;&#23545;3D&#28857;&#20113;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#35299;&#20915;&#35813;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#19978;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#65292;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#21364;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#25915;&#20987;&#22312;&#20154;&#30524;&#20013;&#26159;&#26080;&#27861;&#23519;&#35273;&#30340;&#65292;&#20294;&#21364;&#33021;&#36731;&#26131;&#27450;&#39575;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#27979;&#35797;&#21644;&#37096;&#32626;&#38454;&#27573;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#20851;&#20110;&#28857;&#20113;&#20998;&#31867;&#20013;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#30340;&#24403;&#21069;&#36827;&#23637;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#23545;&#25239;&#25915;&#20987;&#30340;&#21407;&#29702;&#21644;&#29305;&#28857;&#65292;&#24635;&#32467;&#24182;&#20998;&#26512;&#20102;&#36817;&#24180;&#26469;&#30340;&#23545;&#25239;&#26679;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#23427;&#23558;&#38450;&#24481;&#31574;&#30053;&#20998;&#20026;&#36755;&#20837;&#36716;&#25442;&#65292;&#25968;&#25454;&#20248;&#21270;&#21644;&#28145;&#24230;&#27169;&#22411;&#20462;&#25913;&#19977;&#31867;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#35813;&#39046;&#22495;&#20013;&#20960;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has successfully solved a wide range of tasks in 2D vision as a dominant AI technique. Recently, deep learning on 3D point clouds is becoming increasingly popular for addressing various tasks in this field. Despite remarkable achievements, deep learning algorithms are vulnerable to adversarial attacks. These attacks are imperceptible to the human eye but can easily fool deep neural networks in the testing and deployment stage. To encourage future research, this survey summarizes the current progress on adversarial attack and defense techniques on point cloud classification. This paper first introduces the principles and characteristics of adversarial attacks and summarizes and analyzes the adversarial example generation methods in recent years. Besides, it classifies defense strategies as input transformation, data optimization, and deep model modification. Finally, it presents several challenging issues and future research directions in this domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#26102;&#38388;&#31639;&#27861;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#23398;&#20064;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;(SNNs)&#20013;&#30340;&#24310;&#36831;&#65292;&#25552;&#39640;&#20102;SNNs&#22312;&#33410;&#33021;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.17670</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#23398;&#20064;&#38388;&#36317;&#30340;&#33192;&#32960;&#21367;&#31215;&#23398;&#20064;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24310;&#36831;
&lt;/p&gt;
&lt;p&gt;
Learning Delays in Spiking Neural Networks using Dilated Convolutions with Learnable Spacings. (arXiv:2306.17670v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#26102;&#38388;&#31639;&#27861;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#23398;&#20064;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;(SNNs)&#20013;&#30340;&#24310;&#36831;&#65292;&#25552;&#39640;&#20102;SNNs&#22312;&#33410;&#33021;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;(SNNs)&#26159;&#26500;&#24314;&#33410;&#33021;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22914;&#35821;&#38899;&#35782;&#21035;&#31561;&#26102;&#38388;&#20219;&#21153;&#12290;&#22312;SNNs&#20013;&#65292;&#24310;&#36831;&#25351;&#30340;&#26159;&#20174;&#19968;&#20010;&#31070;&#32463;&#20803;&#21040;&#21478;&#19968;&#20010;&#31070;&#32463;&#20803;&#20256;&#25773;&#38656;&#35201;&#30340;&#26102;&#38388;&#12290;&#36825;&#20123;&#24310;&#36831;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#24433;&#21709;&#33033;&#20914;&#21040;&#36798;&#26102;&#38388;&#65292;&#24050;&#30693;&#23574;&#23792;&#31070;&#32463;&#20803;&#23545;&#20110;&#37325;&#21472;&#30340;&#36755;&#20837;&#33033;&#20914;&#26377;&#26356;&#24378;&#30340;&#21709;&#24212;&#12290;&#26356;&#27491;&#24335;&#22320;&#35828;&#65292;&#29702;&#35770;&#19978;&#24050;&#32463;&#35777;&#26126;&#21487;&#22609;&#24615;&#24310;&#36831;&#26497;&#22823;&#22686;&#21152;&#20102;SNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#23398;&#20064;&#36825;&#20123;&#24310;&#36831;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#31163;&#25955;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#22312;&#28145;&#24230;&#21069;&#39304;SNNs&#20013;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#27169;&#25311;&#36830;&#32493;&#23618;&#20043;&#38388;&#30340;&#24310;&#36831;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#27839;&#26102;&#38388;&#36724;&#30340;&#19968;&#32500;&#21367;&#31215;&#12290;&#21367;&#31215;&#26680;&#20165;&#21253;&#21547;&#23569;&#25968;&#38750;&#38646;&#26435;&#37325; - &#27599;&#20010;&#31361;&#35302;&#19968;&#20010; - &#23427;&#20204;&#30340;&#20301;&#32622;&#23545;&#24212;&#20110;&#24310;&#36831;&#12290;&#36825;&#20123;&#20301;&#32622;&#19982;&#26435;&#37325;&#19968;&#36215;&#34987;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) are a promising research direction for building power-efficient information processing systems, especially for temporal tasks such as speech recognition. In SNNs, delays refer to the time needed for one spike to travel from one neuron to another. These delays matter because they influence the spike arrival times, and it is well-known that spiking neurons respond more strongly to coincident input spikes. More formally, it has been shown theoretically that plastic delays greatly increase the expressivity in SNNs. Yet, efficient algorithms to learn these delays have been lacking. Here, we propose a new discrete-time algorithm that addresses this issue in deep feedforward SNNs using backpropagation, in an offline manner. To simulate delays between consecutive layers, we use 1D convolutions across time. The kernels contain only a few non-zero weights - one per synapse - whose positions correspond to the delays. These positions are learned together with the wei
&lt;/p&gt;</description></item><item><title>G-NM&#26159;&#19968;&#32452;&#38598;&#21512;&#20102;&#20256;&#32479;&#21644;&#29616;&#20195;&#27169;&#22411;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#20013;&#30340;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.11667</link><description>&lt;p&gt;
G-NM&#65306;&#19968;&#32452;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
G-NM: A Group of Numerical Time Series Prediction Models. (arXiv:2306.11667v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11667
&lt;/p&gt;
&lt;p&gt;
G-NM&#26159;&#19968;&#32452;&#38598;&#21512;&#20102;&#20256;&#32479;&#21644;&#29616;&#20195;&#27169;&#22411;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#20013;&#30340;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#24320;&#21457;&#21644;&#23454;&#26045;&#19968;&#20010;&#32508;&#21512;&#30340;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#38598;&#21512;&#65292;&#32479;&#31216;&#20026;&#25968;&#23383;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#32452;&#65288;G-NM&#65289;&#12290;&#35813;&#38598;&#21512;&#21253;&#25324;&#20256;&#32479;&#27169;&#22411;&#22914;&#33258;&#22238;&#24402;&#32508;&#21512;&#31227;&#21160;&#24179;&#22343;&#65288;ARIMA&#65289;&#12289;Holt-Winters&#26041;&#27861;&#21644;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;SVR&#65289;&#65292;&#20197;&#21450;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#12290;G-NM&#26126;&#30830;&#26500;&#24314;&#20197;&#22686;&#24378;&#25105;&#20204;&#23545;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#20013;&#22266;&#26377;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#19982;&#36825;&#20123;&#20107;&#20214;&#30456;&#20851;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;G-NM&#20415;&#20110;&#23545;&#27492;&#31867;&#29616;&#35937;&#22312;&#24310;&#38271;&#26102;&#38388;&#27573;&#20869;&#36827;&#34892;&#39044;&#27979;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25512;&#36827;&#25105;&#20204;&#23545;&#27492;&#31867;&#20107;&#20214;&#30340;&#29702;&#35299;&#65292;&#24182;&#22823;&#24133;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;G-NM&#21253;&#25324;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#21450;&#23395;&#33410;&#24615;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we focus on the development and implementation of a comprehensive ensemble of numerical time series forecasting models, collectively referred to as the Group of Numerical Time Series Prediction Model (G-NM). This inclusive set comprises traditional models such as Autoregressive Integrated Moving Average (ARIMA), Holt-Winters' method, and Support Vector Regression (SVR), in addition to modern neural network models including Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM). G-NM is explicitly constructed to augment our predictive capabilities related to patterns and trends inherent in complex natural phenomena. By utilizing time series data relevant to these events, G-NM facilitates the prediction of such phenomena over extended periods. The primary objective of this research is to both advance our understanding of such occurrences and to significantly enhance the accuracy of our forecasts. G-NM encapsulates both linear and non-linear dependencies, seasonal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#25490;&#21517;&#31995;&#32479;&#20844;&#24179;&#24615;&#30340;&#27979;&#35797;&#26041;&#27861;&#8212;&#8212;&#21305;&#37197;&#23545;&#26657;&#20934;&#65292;&#36890;&#36807;&#26500;&#24314;&#28151;&#28102;&#24046;&#24322;&#26368;&#23567;&#30340;&#21305;&#37197;&#29289;&#21697;&#23545;&#26469;&#35745;&#31639;&#36866;&#24403;&#30340;&#25490;&#21517;&#35823;&#24046;&#27979;&#37327;&#32467;&#26524;&#65292;&#21487;&#20197;&#30452;&#25509;&#35828;&#26126;&#23376;&#32452;&#27700;&#24179;&#26333;&#20809;&#30340;&#19981;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#25490;&#21517;&#20559;&#24046;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03775</link><description>&lt;p&gt;
&#21305;&#37197;&#23545;&#26657;&#20934;&#29992;&#20110;&#25490;&#21517;&#20844;&#24179;&#24615;&#30340;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Matched Pair Calibration for Ranking Fairness. (arXiv:2306.03775v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#25490;&#21517;&#31995;&#32479;&#20844;&#24179;&#24615;&#30340;&#27979;&#35797;&#26041;&#27861;&#8212;&#8212;&#21305;&#37197;&#23545;&#26657;&#20934;&#65292;&#36890;&#36807;&#26500;&#24314;&#28151;&#28102;&#24046;&#24322;&#26368;&#23567;&#30340;&#21305;&#37197;&#29289;&#21697;&#23545;&#26469;&#35745;&#31639;&#36866;&#24403;&#30340;&#25490;&#21517;&#35823;&#24046;&#27979;&#37327;&#32467;&#26524;&#65292;&#21487;&#20197;&#30452;&#25509;&#35828;&#26126;&#23376;&#32452;&#27700;&#24179;&#26333;&#20809;&#30340;&#19981;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#25490;&#21517;&#20559;&#24046;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;&#20998;&#25968;&#30340;&#25490;&#21517;&#31995;&#32479;&#20013;&#20844;&#24179;&#24615;&#30340;&#27979;&#35797;&#26041;&#27861;&#8212;&#8212;&#21305;&#37197;&#23545;&#26657;&#20934;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#32452;&#21305;&#37197;&#30340;&#29289;&#21697;&#23545;&#65292;&#36825;&#20123;&#29289;&#21697;&#23545;&#23376;&#32452;&#20043;&#38388;&#30340;&#28151;&#28102;&#24046;&#24322;&#26368;&#23567;&#65292;&#28982;&#21518;&#22312;&#36825;&#32452;&#29289;&#21697;&#19978;&#35745;&#31639;&#36866;&#24403;&#30340;&#25490;&#21517;&#35823;&#24046;&#27979;&#37327;&#32467;&#26524;&#12290;&#21305;&#37197;&#27493;&#39588;&#30830;&#20445;&#20102;&#25105;&#20204;&#22312;&#30456;&#21516;&#20998;&#25968;&#30340;&#29289;&#21697;&#20043;&#38388;&#27604;&#36739;&#23376;&#32452;&#32467;&#26524;&#65292;&#20174;&#32780;&#30452;&#25509;&#35828;&#26126;&#23376;&#32452;&#27700;&#24179;&#26333;&#20809;&#30340;&#19981;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#23558;&#26657;&#20934;&#30340;&#20844;&#24179;&#24615;&#30452;&#35273;&#20174;&#20108;&#20803;&#20998;&#31867;&#35774;&#32622;&#25512;&#24191;&#21040;&#25490;&#21517;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#25552;&#35758;&#30340;&#25490;&#21517;&#20844;&#24179;&#24615;&#25514;&#26045;&#32852;&#31995;&#36215;&#26469;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#23637;&#31034;&#20102;&#36793;&#38469;&#32467;&#26524;&#27979;&#35797;&#36923;&#36753;&#22914;&#20309;&#25193;&#23637;&#21040;&#20998;&#26512;&#20154;&#21592;&#21487;&#20197;&#35775;&#38382;&#27169;&#22411;&#24471;&#20998;&#30340;&#24773;&#20917;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#23558;&#21305;&#37197;&#23545;&#26657;&#20934;&#24212;&#29992;&#20110;&#30495;&#23454;&#25490;&#21517;&#25968;&#25454;&#38598;&#20197;&#35777;&#26126;&#20854;&#26816;&#27979;&#25490;&#21517;&#20559;&#24046;&#30340;&#25928;&#33021;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a test of fairness in score-based ranking systems called matched pair calibration. Our approach constructs a set of matched item pairs with minimal confounding differences between subgroups before computing an appropriate measure of ranking error over the set. The matching step ensures that we compare subgroup outcomes between identically scored items so that measured performance differences directly imply unfairness in subgroup-level exposures. We show how our approach generalizes the fairness intuitions of calibration from a binary classification setting to ranking and connect our approach to other proposals for ranking fairness measures. Moreover, our strategy shows how the logic of marginal outcome tests extends to cases where the analyst has access to model scores. Lastly, we provide an example of applying matched pair calibration to a real-word ranking data set to demonstrate its efficacy in detecting ranking bias.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23384;&#22312;&#28431;&#27934;&#30340;&#20195;&#30721;&#34917;&#20840;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#24182;&#21457;&#29616;&#36825;&#20123;&#28431;&#27934;&#26174;&#33879;&#38477;&#20302;&#20102;Code-LLMs&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03438</link><description>&lt;p&gt;
&#20195;&#30721;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22635;&#20889;&#21487;&#33021;&#23384;&#22312;&#28431;&#27934;&#30340;&#20195;&#30721;&#26102;&#23384;&#22312;&#22833;&#36133;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Large Language Models of Code Fail at Completing Code with Potential Bugs. (arXiv:2306.03438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23384;&#22312;&#28431;&#27934;&#30340;&#20195;&#30721;&#34917;&#20840;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#24182;&#21457;&#29616;&#36825;&#20123;&#28431;&#27934;&#26174;&#33879;&#38477;&#20302;&#20102;Code-LLMs&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20195;&#30721;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;Code-LLMs&#65289;&#22312;&#20195;&#30721;&#34917;&#20840;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#36825;&#26159;&#32534;&#31243;&#36741;&#21161;&#21644;&#20195;&#30721;&#26234;&#33021;&#30340;&#22522;&#26412;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#24573;&#30053;&#20102;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20195;&#30721;&#19978;&#19979;&#25991;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#28431;&#27934;&#38382;&#39064;&#65292;&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#36825;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#30740;&#31350;&#20102;&#23384;&#22312;&#28431;&#27934;&#30340;&#20195;&#30721;&#34917;&#20840;&#38382;&#39064;&#65292;&#21463;&#23454;&#26102;&#20195;&#30721;&#24314;&#35758;&#30340;&#29616;&#23454;&#22330;&#26223;&#21551;&#21457;&#65292;&#20195;&#30721;&#19978;&#19979;&#25991;&#20013;&#21253;&#21547;&#21487;&#33021;&#30340;&#28431;&#27934;-&#21453;&#27169;&#24335;&#65292;&#36825;&#20123;&#21453;&#27169;&#24335;&#21487;&#20197;&#25104;&#20026;&#23436;&#25104;&#31243;&#24207;&#20013;&#30340;&#28431;&#27934;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#30740;&#31350;&#20219;&#21153;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#26159;&#20174;&#35821;&#20041;&#25913;&#21464;&#25805;&#20316;&#20013;&#27966;&#29983;&#30340;&#21512;&#25104;&#28431;&#27934;&#25968;&#25454;&#38598;&#65288;buggy-HumanEval&#65289;&#65292;&#21478;&#19968;&#20010;&#26159;&#20174;&#29992;&#25143;&#25552;&#20132;&#30340;&#32534;&#31243;&#38382;&#39064;&#20013;&#27966;&#29983;&#30340;&#29616;&#23454;&#28431;&#27934;&#25968;&#25454;&#38598;&#65288;buggy-FixEval&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21487;&#33021;&#23384;&#22312;&#28431;&#27934;&#30340;&#24773;&#20917;&#26174;&#33879;&#38477;&#20302;&#20102;&#39640;&#24615;&#33021;Code-LLMs&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;CodeGen-2B-mono&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30340;&#36890;&#36807;&#29575;
&lt;/p&gt;
&lt;p&gt;
Large language models of code (Code-LLMs) have recently brought tremendous advances to code completion, a fundamental feature of programming assistance and code intelligence. However, most existing works ignore the possible presence of bugs in the code context for generation, which are inevitable in software development. Therefore, we introduce and study the buggy-code completion problem, inspired by the realistic scenario of real-time code suggestion where the code context contains potential bugs -- anti-patterns that can become bugs in the completed program. To systematically study the task, we introduce two datasets: one with synthetic bugs derived from semantics-altering operator changes (buggy-HumanEval) and one with realistic bugs derived from user submissions to coding problems (buggy-FixEval). We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs. For instance, the passing rates of CodeGen-2B-mono on test 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;ICM-VAE&#26694;&#26550;&#65292;&#20351;&#24471;&#23398;&#20064;&#22240;&#26524;&#35299;&#32544;&#32469;&#34920;&#31034;&#26356;&#20934;&#30830;</title><link>http://arxiv.org/abs/2306.01213</link><description>&lt;p&gt;
&#22522;&#20110;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#21407;&#21017;&#23398;&#20064;&#22240;&#26524;&#35299;&#32544;&#32469;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Causally Disentangled Representations via the Principle of Independent Causal Mechanisms. (arXiv:2306.01213v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;ICM-VAE&#26694;&#26550;&#65292;&#20351;&#24471;&#23398;&#20064;&#22240;&#26524;&#35299;&#32544;&#32469;&#34920;&#31034;&#26356;&#20934;&#30830;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#35299;&#32544;&#32469;&#30340;&#22240;&#26524;&#34920;&#31034;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#36817;&#24180;&#26469;&#22240;&#20854;&#23545;&#25552;&#21462;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#24847;&#20041;&#20449;&#24687;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#20174;&#29420;&#31435;&#22240;&#26524;&#26426;&#21046;&#30340;&#35282;&#24230;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#35299;&#32544;&#32469;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ICM-VAE&#26694;&#26550;&#65292;&#36890;&#36807;&#22240;&#22240;&#26524;&#20851;&#31995;&#35266;&#23519;&#26631;&#31614;&#26469;&#30417;&#30563;&#23398;&#20064;&#22240;&#26524;&#35299;&#32544;&#32469;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#22522;&#20110;&#27969;&#30340;&#24494;&#20998;&#21516;&#32986;&#20989;&#25968;&#23558;&#22122;&#22768;&#21464;&#37327;&#26144;&#23556;&#21040;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#20013;&#26469;&#24314;&#27169;&#22240;&#26524;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20419;&#36827;&#22240;&#26524;&#35201;&#32032;&#30340;&#35299;&#32544;&#32469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#35299;&#32544;&#32469;&#20808;&#39564;&#65292;&#21033;&#29992;&#24050;&#30693;&#30340;&#22240;&#26524;&#32467;&#26500;&#26469;&#40723;&#21169;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#23398;&#20064;&#22240;&#26524;&#20998;&#35299;&#20998;&#24067;&#12290;&#22312;&#30456;&#23545;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#32467;&#26524;&#65292;&#26174;&#31034;&#20102;&#22240;&#26524;&#35201;&#32032;&#21644;&#26426;&#21046;&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#30452;&#21040;&#25490;&#21015;&#21644;&#36880;&#20803;&#37325;&#21442;&#25968;&#21270;&#30340;&#38480;&#24230;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;...
&lt;/p&gt;
&lt;p&gt;
Learning disentangled causal representations is a challenging problem that has gained significant attention recently due to its implications for extracting meaningful information for downstream tasks. In this work, we define a new notion of causal disentanglement from the perspective of independent causal mechanisms. We propose ICM-VAE, a framework for learning causally disentangled representations supervised by causally related observed labels. We model causal mechanisms using learnable flow-based diffeomorphic functions to map noise variables to latent causal variables. Further, to promote the disentanglement of causal factors, we propose a causal disentanglement prior that utilizes the known causal structure to encourage learning a causally factorized distribution in the latent space. Under relatively mild conditions, we provide theoretical results showing the identifiability of causal factors and mechanisms up to permutation and elementwise reparameterization. We empirically demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31895;&#31961;&#38598;&#29702;&#35770;&#30340;&#35268;&#21017;&#36890;&#29992;&#36870;&#25512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36716;&#21270;&#30446;&#26631;&#27010;&#24565;&#21644;&#23376;&#27010;&#24565;&#20026;&#20449;&#24687;&#34920;&#65292;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#35299;&#20915;&#39046;&#22495;&#30693;&#35782;&#33719;&#21462;&#21644;&#35268;&#21017;&#30340;&#20462;&#27491;&#12289;&#20943;&#23569;&#12289;&#29983;&#25104;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.19718</link><description>&lt;p&gt;
&#31895;&#31961;&#38598;&#19979;&#19968;&#31181;&#35268;&#21017;&#36890;&#29992;&#36870;&#25512;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A rule-general abductive learning by rough sets. (arXiv:2305.19718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31895;&#31961;&#38598;&#29702;&#35770;&#30340;&#35268;&#21017;&#36890;&#29992;&#36870;&#25512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36716;&#21270;&#30446;&#26631;&#27010;&#24565;&#21644;&#23376;&#27010;&#24565;&#20026;&#20449;&#24687;&#34920;&#65292;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#35299;&#20915;&#39046;&#22495;&#30693;&#35782;&#33719;&#21462;&#21644;&#35268;&#21017;&#30340;&#20462;&#27491;&#12289;&#20943;&#23569;&#12289;&#29983;&#25104;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#20219;&#21153;&#20013;&#65292;&#36890;&#24120;&#23384;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#26631;&#35760;&#25968;&#25454;&#12290;&#23558;&#20004;&#32773;&#32452;&#21512;&#36215;&#26469;&#36827;&#34892;&#23398;&#20064;&#30340;&#20219;&#21153;&#34987;&#31216;&#20026;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#19987;&#23478;&#21487;&#20197;&#20351;&#29992;&#36923;&#36753;&#35268;&#21017;&#26469;&#26631;&#35760;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#20294;&#36825;&#20010;&#25805;&#20316;&#24456;&#26114;&#36149;&#12290;&#24863;&#30693;&#21644;&#25512;&#29702;&#30340;&#32467;&#21512;&#22312;&#22788;&#29702;&#20855;&#26377;&#39046;&#22495;&#30693;&#35782;&#30340;&#21322;&#30417;&#30563;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#39046;&#22495;&#30693;&#35782;&#20197;&#21450;&#35268;&#21017;&#30340;&#20462;&#27491;&#12289;&#20943;&#23569;&#21644;&#29983;&#25104;&#20173;&#28982;&#26159;&#38656;&#35201;&#35299;&#20915;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#31895;&#31961;&#38598;&#29702;&#35770;&#26159;&#35299;&#20915;&#20449;&#24687;&#31995;&#32479;&#20013;&#30693;&#35782;&#22788;&#29702;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31895;&#31961;&#38598;&#19979;&#30340;&#35268;&#21017;&#36890;&#29992;&#36870;&#25512;&#23398;&#20064;&#26041;&#27861;&#65288;RS-ABL&#65289;&#12290;&#36890;&#36807;&#23558;&#35268;&#21017;&#30340;&#30446;&#26631;&#27010;&#24565;&#21644;&#23376;&#27010;&#24565;&#36716;&#21270;&#20026;&#20449;&#24687;&#34920;&#65292;&#21033;&#29992;&#31895;&#31961;&#38598;&#29702;&#35770;&#26469;&#35299;&#20915;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#33719;&#21462;&#39046;&#22495;&#30693;&#35782;&#21644;&#20462;&#27491;&#12289;&#20943;&#23569;&#12289;&#29983;&#25104;&#35268;&#21017;&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36824;&#21487;&#20197;&#29983;&#25104;&#26356;&#24191;&#27867;&#30340;&#36127;&#35268;&#21017;&#65292;&#20197;&#22686;&#24378;&#35268;&#21017;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world tasks, there is usually a large amount of unlabeled data and labeled data. The task of combining the two to learn is known as semi-supervised learning. Experts can use logical rules to label unlabeled data, but this operation is costly. The combination of perception and reasoning has a good effect in processing such semi-supervised tasks with domain knowledge. However, acquiring domain knowledge and the correction, reduction and generation of rules remain complex problems to be solved. Rough set theory is an important method for solving knowledge processing in information systems. In this paper, we propose a rule general abductive learning by rough set (RS-ABL). By transforming the target concept and sub-concepts of rules into information tables, rough set theory is used to solve the acquisition of domain knowledge and the correction, reduction and generation of rules at a lower cost. This framework can also generate more extensive negative rules to enhance the breadth of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28151;&#28102;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26032;&#22411;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#31163;&#32447;&#35774;&#32622;&#19979;&#21487;&#21516;&#26102;&#22788;&#29702;&#36830;&#32493;&#29366;&#24577;&#21644;&#35266;&#23519;&#31354;&#38388;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17083</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#28151;&#28102;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Policy Gradient Method for Confounded POMDPs. (arXiv:2305.17083v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28151;&#28102;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26032;&#22411;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#31163;&#32447;&#35774;&#32622;&#19979;&#21487;&#21516;&#26102;&#22788;&#29702;&#36830;&#32493;&#29366;&#24577;&#21644;&#35266;&#23519;&#31354;&#38388;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#35266;&#23519;&#31354;&#38388;&#30340;&#28151;&#28102;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#31163;&#32447;&#35774;&#32622;&#19979;&#20351;&#29992;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35782;&#21035;&#32467;&#26524;&#65292;&#20197;&#22312;&#31163;&#32447;&#25968;&#25454;&#19979;&#38750;&#21442;&#25968;&#22320;&#20272;&#35745;POMDP&#20013;&#30340;&#20219;&#20309;&#21382;&#21490;&#20381;&#36182;&#31574;&#30053;&#26799;&#24230;&#12290;&#35782;&#21035;&#32467;&#26524;&#20351;&#25105;&#20204;&#33021;&#22815;&#35299;&#20915;&#19968;&#31995;&#21015;&#26465;&#20214;&#30697;&#38480;&#21046;&#65292;&#24182;&#37319;&#29992;&#20855;&#26377;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#30340;&#26368;&#23567;&#26368;&#22823;&#23398;&#20064;&#36807;&#31243;&#26469;&#20272;&#35745;&#31574;&#30053;&#26799;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#38024;&#23545;&#39044;&#20808;&#25351;&#23450;&#30340;&#31574;&#30053;&#31867;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#38480;&#26679;&#26412;&#30340;&#38750;&#28176;&#36817;&#20272;&#35745;&#30028;&#38480;&#65292;&#20197;&#20102;&#35299;&#26679;&#26412;&#22823;&#23567;&#12289;&#26102;&#38388;&#38271;&#24230;&#12289;&#38598;&#20013;&#24230;&#31995;&#25968;&#21644;&#27714;&#35299;&#26465;&#20214;&#30697;&#38480;&#21046;&#30340;&#20266;&#27491;&#21017;&#24230;&#37327;&#23545;&#20110;&#22343;&#21248;&#20272;&#35745;&#26799;&#24230;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#22312;&#26799;&#24230;&#19978;&#21319;&#31639;&#27861;&#20013;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#26799;&#24230;&#20272;&#35745;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#25214;&#21040;&#21382;&#21490;&#20381;&#36182;&#24615;&#31574;&#30053;&#26799;&#24230;&#26041;&#38754;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a policy gradient method for confounded partially observable Markov decision processes (POMDPs) with continuous state and observation spaces in the offline setting. We first establish a novel identification result to non-parametrically estimate any history-dependent policy gradient under POMDPs using the offline data. The identification enables us to solve a sequence of conditional moment restrictions and adopt the min-max learning procedure with general function approximation for estimating the policy gradient. We then provide a finite-sample non-asymptotic bound for estimating the gradient uniformly over a pre-specified policy class in terms of the sample size, length of horizon, concentratability coefficient and the measure of ill-posedness in solving the conditional moment restrictions. Lastly, by deploying the proposed gradient estimation in the gradient ascent algorithm, we show the global convergence of the proposed algorithm in finding the history-depe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#31185;&#22827;&#36716;&#25442;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#39640;&#26031;&#21442;&#25968;&#21270;&#36801;&#31227;&#20998;&#24067;&#23454;&#29616;&#31532;&#19968;&#38454;&#27573;&#39532;&#23572;&#31185;&#22827;&#20381;&#36182;&#32467;&#26500;&#20013;&#30340;&#21487;&#36776;&#35782;&#24615;&#26465;&#20214;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20381;&#36182;&#20110;&#25919;&#26435;&#30340;&#22240;&#26524;&#21457;&#29616;&#21644;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2305.15925</link><description>&lt;p&gt;
&#20851;&#20110;&#39532;&#23572;&#31185;&#22827;&#36716;&#25442;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Identifiability of Markov Switching Models. (arXiv:2305.15925v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#31185;&#22827;&#36716;&#25442;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#39640;&#26031;&#21442;&#25968;&#21270;&#36801;&#31227;&#20998;&#24067;&#23454;&#29616;&#31532;&#19968;&#38454;&#27573;&#39532;&#23572;&#31185;&#22827;&#20381;&#36182;&#32467;&#26500;&#20013;&#30340;&#21487;&#36776;&#35782;&#24615;&#26465;&#20214;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20381;&#36182;&#20110;&#25919;&#26435;&#30340;&#22240;&#26524;&#21457;&#29616;&#21644;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#22240;&#20854;&#22312;&#21487;&#35299;&#37322;&#24615;&#25110;&#20998;&#24067;&#27867;&#21270;&#26041;&#38754;&#30340;&#24212;&#29992;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20316;&#20026;&#23558;&#26368;&#36817;&#30340;&#32467;&#26524;&#25193;&#23637;&#21040;&#24207;&#21015;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#31532;&#19968;&#27493;&#30340;&#39532;&#23572;&#31185;&#22827;&#36716;&#25442;&#27169;&#22411;&#30340;&#21487;&#36776;&#35782;&#24615;&#12290;&#25105;&#20204;&#22312;&#31532;&#19968;&#38454;&#27573;&#39532;&#23572;&#31185;&#22827;&#20381;&#36182;&#32467;&#26500;&#20013;&#25552;&#20986;&#20102;&#21487;&#36776;&#35782;&#24615;&#26465;&#20214;&#65292;&#24182;&#36890;&#36807;&#38750;&#32447;&#24615;&#39640;&#26031;&#21442;&#25968;&#21270;&#36801;&#31227;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#20381;&#36182;&#20110;&#25919;&#26435;&#30340;&#22240;&#26524;&#21457;&#29616;&#21644;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#20998;&#21106;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifiability of latent variable models has recently gained interest in terms of its applications to interpretability or out of distribution generalisation. In this work, we study identifiability of Markov Switching Models as a first step towards extending recent results to sequential latent variable models. We present identifiability conditions within first-order Markov dependency structures, and parametrise the transition distribution via non-linear Gaussians. Our experiments showcase the applicability of our approach for regime-dependent causal discovery and high-dimensional time series segmentation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30417;&#30563;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#31867;&#20869;&#22810;&#26679;&#24615;&#21644;&#31867;&#38388;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#23545;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#24182;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#19979;&#28216;&#24615;&#33021;&#21333;&#35843;&#22320;&#21462;&#20915;&#20110;&#36825;&#20004;&#31181;&#22810;&#26679;&#24615;&#12290;&#26368;&#20339;&#30340;&#31867;&#21035;&#26679;&#26412;&#27604;&#65288;#&#31867;&#21035; / #&#27599;&#31867;&#26679;&#26412;&#25968;&#65289;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#26080;&#20851;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#39044;&#27979;&#26368;&#20339;&#30340;&#39044;&#35757;&#32451;&#31867;&#21035;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.12224</link><description>&lt;p&gt;
&#30417;&#30563;&#39044;&#35757;&#32451;&#20013;&#31867;&#20869;/&#31867;&#38388;&#22810;&#26679;&#24615;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
On the Trade-off of Intra-/Inter-class Diversity for Supervised Pre-training. (arXiv:2305.12224v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30417;&#30563;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#31867;&#20869;&#22810;&#26679;&#24615;&#21644;&#31867;&#38388;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#23545;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#24182;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#19979;&#28216;&#24615;&#33021;&#21333;&#35843;&#22320;&#21462;&#20915;&#20110;&#36825;&#20004;&#31181;&#22810;&#26679;&#24615;&#12290;&#26368;&#20339;&#30340;&#31867;&#21035;&#26679;&#26412;&#27604;&#65288;#&#31867;&#21035; / #&#27599;&#31867;&#26679;&#26412;&#25968;&#65289;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#26080;&#20851;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#39044;&#27979;&#26368;&#20339;&#30340;&#39044;&#35757;&#32451;&#31867;&#21035;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;&#20110;&#26500;&#24314;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#38656;&#35201;&#23545;&#23427;&#20204;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#36827;&#34892;&#20005;&#26684;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#30417;&#30563;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#31867;&#20869;&#22810;&#26679;&#24615;&#65288;&#27599;&#20010;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#65289;&#21644;&#31867;&#38388;&#22810;&#26679;&#24615;&#65288;&#31867;&#21035;&#25968;&#65289;&#20043;&#38388;&#30340;&#26435;&#34913;&#23545;&#19979;&#28216;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#23454;&#35777;&#34920;&#26126;&#65292;&#24403;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#22266;&#23450;&#26102;&#65292;&#26368;&#20339;&#30340;&#19979;&#28216;&#34920;&#29616;&#21462;&#20915;&#20110;&#31867;&#20869;/&#31867;&#38388;&#22810;&#26679;&#24615;&#30340;&#24179;&#34913;&#12290;&#20026;&#20102;&#20102;&#35299;&#20854;&#22522;&#26412;&#26426;&#21046;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#19979;&#28216;&#34920;&#29616;&#21333;&#35843;&#22320;&#21462;&#20915;&#20110;&#20004;&#31181;&#22810;&#26679;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#25581;&#31034;&#20102;&#26368;&#20339;&#30340;&#31867;&#21035;&#26679;&#26412;&#27604;&#65288;#&#31867;&#21035; / #&#27599;&#31867;&#26679;&#26412;&#25968;&#65289;&#19981;&#21463;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#24433;&#21709;&#65292;&#36825;&#21551;&#21457;&#25105;&#20204;&#24212;&#29992;&#39044;&#27979;&#26368;&#20339;&#30340;&#39044;&#35757;&#32451;&#31867;&#21035;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#24212;&#29992;&#30340;&#26377;&#25928;&#24615;&#65292;&#24615;&#33021;&#25552;&#21319;&#32422;&#20026;2&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training datasets are critical for building state-of-the-art machine learning models, motivating rigorous study on their impact on downstream tasks. In this work, we study the impact of the trade-off between the intra-class diversity (the number of samples per class) and the inter-class diversity (the number of classes) of a supervised pre-training dataset. Empirically, we found that with the size of the pre-training dataset fixed, the best downstream performance comes with a balance on the intra-/inter-class diversity. To understand the underlying mechanism, we show theoretically that the downstream performance depends monotonically on both types of diversity. Notably, our theory reveals that the optimal class-to-sample ratio (#classes / #samples per class) is invariant to the size of the pre-training dataset, which motivates an application of predicting the optimal number of pre-training classes. We demonstrate the effectiveness of this application by an improvement of around 2 p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;Lipschitz&#24230;&#37327;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#37325;&#24314;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#30830;&#23450;&#29305;&#23450;&#28145;&#24230;&#23398;&#20064;&#37325;&#24314;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#35782;&#21035;&#20998;&#24067;&#22806;&#30340;&#27979;&#35797;&#26679;&#26412;&#24182;&#25351;&#23548;&#36866;&#24403;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2305.07618</link><description>&lt;p&gt;
&#22522;&#20110;&#23616;&#37096;Lipschitz&#24230;&#37327;&#30340;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#37325;&#24314;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Estimation for Deep Learning Image Reconstruction using a Local Lipschitz Metric. (arXiv:2305.07618v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;Lipschitz&#24230;&#37327;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#37325;&#24314;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#30830;&#23450;&#29305;&#23450;&#28145;&#24230;&#23398;&#20064;&#37325;&#24314;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#35782;&#21035;&#20998;&#24067;&#22806;&#30340;&#27979;&#35797;&#26679;&#26412;&#24182;&#25351;&#23548;&#36866;&#24403;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#19982;&#25104;&#20687;&#30456;&#20851;&#30340;&#21453;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22270;&#20687;&#37325;&#24314;&#12290;&#22312;&#27169;&#22411;&#37096;&#32626;&#26102;&#65292;&#21487;&#33021;&#20250;&#36935;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#24046;&#24322;&#36739;&#22823;&#30340;&#36755;&#20837;&#20998;&#24067;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#25968;&#25454;&#20559;&#24046;&#25110;&#28418;&#31227;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20174;&#21333;&#20010;&#35757;&#32451;&#27169;&#22411;&#20013;&#30830;&#23450;&#30340;&#23616;&#37096;Lipschitz&#24230;&#37327;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#22270;&#20687;&#37325;&#24314;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23616;&#37096;Lipschitz&#20540;&#19982;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20043;&#38388;&#30340;&#21333;&#35843;&#20851;&#31995;&#65292;&#24182;&#34920;&#26126;&#21487;&#20197;&#20351;&#29992;&#27492;&#26041;&#27861;&#25552;&#20379;&#30830;&#23450;&#26159;&#21542;&#36866;&#21512;&#29305;&#23450;&#28145;&#24230;&#23398;&#20064;&#37325;&#24314;&#26041;&#27861;&#30340;&#38408;&#20540;&#12290;&#25105;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#21487;&#29992;&#20110;&#35782;&#21035;&#20998;&#24067;&#22806;&#30340;&#27979;&#35797;&#26679;&#26412;&#65292;&#20851;&#32852;&#20851;&#20110;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#20449;&#24687;&#65292;&#24182;&#25351;&#23548;&#36866;&#24403;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;&#22312;&#21307;&#23398;&#25104;&#20687;&#24212;&#29992;&#20013;&#29305;&#21035;&#37325;&#35201;&#30340;&#26159;&#65292;&#37327;&#21270;&#23398;&#20064;&#37325;&#26500;&#26041;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22240;&#20026;&#35786;&#26029;&#21644;&#27835;&#30103;&#21487;&#33021;&#20250;&#21463;&#21040;&#37325;&#24314;&#22270;&#20687;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of deep learning approaches for image reconstruction is of contemporary interest in radiology, especially for approaches that solve inverse problems associated with imaging. In deployment, these models may be exposed to input distributions that are widely shifted from training data, due in part to data biases or drifts. We propose a metric based on local Lipschitz determined from a single trained model that can be used to estimate the model uncertainty for image reconstructions. We demonstrate a monotonic relationship between the local Lipschitz value and Mean Absolute Error and show that this method can be used to provide a threshold that determines whether a given DL reconstruction approach was well suited to the task. Our uncertainty estimation method can be used to identify out-of-distribution test samples, relate information regarding epistemic uncertainties, and guide proper data augmentation. Quantifying uncertainty of learned reconstruction approaches is especially pert
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#25968;&#25454;&#37319;&#38598;&#21644;&#21453;&#23556;&#26657;&#27491;&#26041;&#27861;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;3DMPCs&#65292;&#29992;&#20110;&#39640;&#25928;&#26893;&#29289;&#34920;&#22411;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.06777</link><description>&lt;p&gt;
&#37319;&#29992;&#33258;&#36866;&#24212;&#25968;&#25454;&#37319;&#38598;&#21644;&#22522;&#20110;NeREF&#30340;&#21453;&#23556;&#26657;&#27491;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;3DMPCs&#65292;&#20419;&#36827;&#39640;&#25928;&#26893;&#29289;&#34920;&#22411;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Generating high-quality 3DMPCs by adaptive data acquisition and NeREF-based reflectance correction to facilitate efficient plant phenotyping. (arXiv:2305.06777v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#25968;&#25454;&#37319;&#38598;&#21644;&#21453;&#23556;&#26657;&#27491;&#26041;&#27861;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;3DMPCs&#65292;&#29992;&#20110;&#39640;&#25928;&#26893;&#29289;&#34920;&#22411;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#19977;&#32500;&#65288;3D&#65289;&#21644;&#22810;&#20809;&#35889;&#25968;&#25454;&#23545;&#26893;&#29289;&#34920;&#22411;&#29305;&#24449;&#36827;&#34892;&#38750;&#30772;&#22351;&#24615;&#35780;&#20272;&#21487;&#20197;&#21152;&#28145;&#32946;&#31181;&#32773;&#23545;&#26893;&#29289;&#29983;&#38271;&#30340;&#29702;&#35299;&#65292;&#24182;&#20351;&#20182;&#20204;&#33021;&#22815;&#20570;&#20986;&#30693;&#24773;&#31649;&#29702;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#28982;&#20809;&#29031;&#26465;&#20214;&#19979;&#65292;&#20027;&#35266;&#35270;&#35282;&#36873;&#25321;&#21644;&#22797;&#26434;&#30340;&#20809;&#29031;&#25928;&#24212;&#20250;&#38477;&#20302;&#25968;&#25454;&#36136;&#37327;&#65292;&#22686;&#21152;&#35299;&#20915;&#34920;&#22411;&#21442;&#25968;&#30340;&#38590;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#25968;&#25454;&#37319;&#38598;&#21644;&#21453;&#23556;&#26657;&#27491;&#30340;&#26041;&#27861;&#65292;&#20197;&#20998;&#21035;&#29983;&#25104;&#26893;&#29289;&#30340;&#39640;&#36136;&#37327;3D&#22810;&#20809;&#35889;&#28857;&#20113;&#65288;3DMPCs&#65289;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#22411;UGV&#24179;&#21488;&#21644;&#22810;&#20256;&#24863;&#22120;&#35013;&#22791;&#30340;&#26426;&#22120;&#20154;&#33218;&#30340;&#39640;&#25928;&#30340;&#19979;&#19968;&#20010;&#26368;&#20339;&#35270;&#35282;&#65288;NBV&#65289;&#35268;&#21010;&#26041;&#27861;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#21442;&#32771;&#22330;&#65288;NeREF&#65289;&#26469;&#39044;&#27979;&#21442;&#32771;&#30340;&#25968;&#23383;&#65288;DN&#65289;&#26469;&#28040;&#38500;&#20809;&#29031;&#25928;&#24212;&#12290;&#25105;&#20204;&#22312;6&#20010;&#32043;&#33487;&#21644;6&#20010;&#30058;&#33540;&#26893;&#26666;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#36873;&#25321;&#20102;2&#29255;&#21487;&#35265;&#21494;&#21644;4&#20010;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-destructive assessments of plant phenotypic traits using high-quality three-dimensional (3D) and multispectral data can deepen breeders' understanding of plant growth and allow them to make informed managerial decisions. However, subjective viewpoint selection and complex illumination effects under natural light conditions decrease the data quality and increase the difficulty of resolving phenotypic parameters. We proposed methods for adaptive data acquisition and reflectance correction respectively, to generate high-quality 3D multispectral point clouds (3DMPCs) of plants. In the first stage, we proposed an efficient next-best-view (NBV) planning method based on a novel UGV platform with a multi-sensor-equipped robotic arm. In the second stage, we eliminated the illumination effects by using the neural reference field (NeREF) to predict the digital number (DN) of the reference. We tested them on 6 perilla and 6 tomato plants, and selected 2 visible leaves and 4 regions of interest
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#29992;&#20110;&#25552;&#21462;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#25152;&#38544;&#21547;&#30340;&#31232;&#30095;&#28508;&#22312;&#20989;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20026;&#34507;&#30333;&#36136;&#24037;&#31243;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#20989;&#25968;&#25512;&#26029;&#25552;&#20379;&#26377;&#29992;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.03136</link><description>&lt;p&gt;
&#23545;&#27604;&#25439;&#22833;&#20316;&#20026;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#30340;&#24191;&#20041;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Contrastive losses as generalized models of global epistasis. (arXiv:2305.03136v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03136
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#29992;&#20110;&#25552;&#21462;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#25152;&#38544;&#21547;&#30340;&#31232;&#30095;&#28508;&#22312;&#20989;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20026;&#34507;&#30333;&#36136;&#24037;&#31243;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#20989;&#25968;&#25512;&#26029;&#25552;&#20379;&#26377;&#29992;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#24615;&#20989;&#25968;&#23558;&#29983;&#29289;&#24207;&#21015;&#30340;&#22823;&#32452;&#21512;&#31354;&#38388;&#26144;&#23556;&#21040;&#25152;&#20851;&#27880;&#30340;&#29305;&#24615;&#19978;&#12290;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#25512;&#26029;&#36825;&#20123;&#22810;&#27169;&#24577;&#20989;&#25968;&#26159;&#29616;&#20195;&#34507;&#30333;&#36136;&#24037;&#31243;&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#12290;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#26159;&#19968;&#31867;&#26377;&#25928;&#19988;&#26377;&#29289;&#29702;&#22522;&#30784;&#30340;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#20272;&#35745;&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#25512;&#26029;&#36866;&#24212;&#24615;&#20989;&#25968;&#12290;&#36825;&#20123;&#27169;&#22411;&#20551;&#35774;&#31232;&#30095;&#30340;&#28508;&#22312;&#20989;&#25968;&#36890;&#36807;&#21333;&#35843;&#38750;&#32447;&#24615;&#21464;&#25442;&#20197;&#21457;&#23556;&#21487;&#27979;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#23567;&#21270;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65288;&#22914; Bradley-Terry &#25439;&#22833;&#65289;&#26159;&#25552;&#21462;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#25152;&#38544;&#31034;&#30340;&#31232;&#30095;&#28508;&#22312;&#20989;&#25968;&#30340;&#19968;&#31181;&#31616;&#21333;&#28789;&#27963;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#36890;&#36807;&#36866;&#24212;&#24615;-&#19978;&#20301;&#32852;&#31995;&#19981;&#30830;&#23450;&#24615;&#21407;&#29702;&#20105;&#36777;&#65292;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#20013;&#30340;&#38750;&#32447;&#24615;&#21487;&#20197;&#20135;&#29983;&#19981;&#20855;&#22791;&#31232;&#30095;&#34920;&#31034;&#30340;&#35266;&#23519;&#36866;&#24212;&#24615;&#20989;&#25968;&#65292;&#22240;&#27492;&#21487;&#33021;&#19981;&#36866;&#21512;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#25439;&#22833;&#65288;&#19968;&#31181;&#24120;&#35265;&#30340;&#20570;&#27861;&#65289;&#20174;&#35266;&#23519;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23545;&#27604;&#25439;&#22833;&#21487;&#29992;&#20110;&#25512;&#26029;&#19981;&#36866;&#21512; MSE &#25439;&#22833;&#30340;&#36866;&#24212;&#24615;&#20989;&#25968;&#65292;&#24182;&#19988;&#20840;&#23616;&#19978;&#20301;&#32852;&#31995;&#27169;&#22411;&#21487;&#20197;&#35299;&#37322;&#20026;&#19968;&#31181;&#35268;&#21017;&#21270;&#30340;&#23545;&#27604;&#25439;&#22833;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20026;&#34507;&#30333;&#36136;&#24037;&#31243;&#21644;&#30456;&#20851;&#39046;&#22495;&#30340;&#36866;&#24212;&#24615;&#20989;&#25968;&#25512;&#26029;&#25552;&#20379;&#26377;&#29992;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fitness functions map large combinatorial spaces of biological sequences to properties of interest. Inferring these multimodal functions from experimental data is a central task in modern protein engineering. Global epistasis models are an effective and physically-grounded class of models for estimating fitness functions from observed data. These models assume that a sparse latent function is transformed by a monotonic nonlinearity to emit measurable fitness. Here we demonstrate that minimizing contrastive loss functions, such as the Bradley-Terry loss, is a simple and flexible technique for extracting the sparse latent function implied by global epistasis. We argue by way of a fitness-epistasis uncertainty principle that the nonlinearities in global epistasis models can produce observed fitness functions that do not admit sparse representations, and thus may be inefficient to learn from observations when using a Mean Squared Error (MSE) loss (a common practice). We show that contrasti
&lt;/p&gt;</description></item><item><title>SmartChoices &#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#29616;&#26377;&#36719;&#20214;&#31995;&#32479;&#36731;&#26494;&#12289;&#23433;&#20840;&#12289;&#26377;&#25928;&#22320;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.13033</link><description>&lt;p&gt;
SmartChoices: &#23398;&#20064;&#23454;&#29616;&#22686;&#24378;&#36719;&#20214;
&lt;/p&gt;
&lt;p&gt;
SmartChoices: Augmenting Software with Learned Implementations. (arXiv:2304.13033v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13033
&lt;/p&gt;
&lt;p&gt;
SmartChoices &#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#29616;&#26377;&#36719;&#20214;&#31995;&#32479;&#36731;&#26494;&#12289;&#23433;&#20840;&#12289;&#26377;&#25928;&#22320;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27491;&#22788;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#40644;&#37329;&#26102;&#20195;&#12290;&#24378;&#22823;&#30340;&#27169;&#22411;&#27491;&#22312;&#35757;&#32451;&#20013;&#65292;&#36828;&#27604;&#20165;&#20351;&#29992;&#20256;&#32479;&#36719;&#20214;&#24037;&#31243;&#26041;&#27861;&#26356;&#22909;&#22320;&#25191;&#34892;&#35768;&#22810;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#24320;&#21457;&#24182;&#37096;&#32626;&#21040;&#29616;&#26377;&#36719;&#20214;&#31995;&#32479;&#20013;&#20173;&#28982;&#24456;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SmartChoices&#65292;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#36731;&#26494;&#12289;&#23433;&#20840;&#12289;&#26377;&#25928;&#22320;&#32467;&#21512;&#21040;&#25104;&#29087;&#36719;&#20214;&#22534;&#26632;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#24635;&#20307;&#35774;&#35745;&#29702;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992; SmartChoices &#22312;&#22823;&#22411;&#24037;&#19994;&#31995;&#32479;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are living in a golden age of machine learning. Powerful models are being trained to perform many tasks far better than is possible using traditional software engineering approaches alone. However, developing and deploying those models in existing software systems remains difficult. In this paper we present SmartChoices, a novel approach to incorporating machine learning into mature software stacks easily, safely, and effectively. We explain the overall design philosophy and present case studies using SmartChoices within large scale industrial systems.
&lt;/p&gt;</description></item><item><title>Phylo2Vec&#26159;&#19968;&#31181;&#26032;&#30340;&#20108;&#21449;&#26641;&#31616;&#26126;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36731;&#26494;&#37319;&#26679;&#20108;&#21449;&#26641;&#65292;&#24182;&#20197;&#31995;&#32479;&#24615;&#30340;&#26041;&#27861;&#36941;&#21382;&#26641;&#31354;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#29992;&#20110;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#34507;&#30333;&#36136;&#31867;&#21035;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12693</link><description>&lt;p&gt;
Phylo2Vec: &#19968;&#31181;&#20108;&#21449;&#26641;&#30340;&#21521;&#37327;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Phylo2Vec: a vector representation for binary trees. (arXiv:2304.12693v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12693
&lt;/p&gt;
&lt;p&gt;
Phylo2Vec&#26159;&#19968;&#31181;&#26032;&#30340;&#20108;&#21449;&#26641;&#31616;&#26126;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36731;&#26494;&#37319;&#26679;&#20108;&#21449;&#26641;&#65292;&#24182;&#20197;&#31995;&#32479;&#24615;&#30340;&#26041;&#27861;&#36941;&#21382;&#26641;&#31354;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#29992;&#20110;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#34507;&#30333;&#36136;&#31867;&#21035;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#29983;&#29289;&#25968;&#25454;&#25512;&#26029;&#24471;&#21040;&#30340;&#20108;&#21449;&#36827;&#21270;&#26641;&#23545;&#20110;&#29702;&#35299;&#29983;&#29289;&#20043;&#38388;&#20849;&#20139;&#30340;&#36827;&#21270;&#21382;&#21490;&#33267;&#20851;&#37325;&#35201;&#12290;&#26681;&#25454;&#26368;&#22823;&#20284;&#28982;&#31561;&#26576;&#20010;&#26368;&#20248;&#24615;&#20934;&#21017;&#25512;&#26029;&#20986;&#26641;&#20013;&#28508;&#22312;&#33410;&#28857;&#30340;&#20301;&#32622;&#26159;NP-hard&#38382;&#39064;&#65292;&#36825;&#25512;&#21160;&#20102;&#22823;&#37327;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21551;&#21457;&#24335;&#26041;&#27861;&#36890;&#24120;&#32570;&#20047;&#19968;&#31181;&#31995;&#32479;&#24615;&#30340;&#26041;&#27861;&#26469;&#22343;&#21248;&#37319;&#26679;&#38543;&#26426;&#26641;&#25110;&#26377;&#25928;&#22320;&#25506;&#32034;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#26641;&#31354;&#38388;&#65292;&#36825;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#31561;&#20248;&#21270;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Phylo2Vec&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#31616;&#26126;&#34920;&#31034;&#26041;&#27861;&#26469;&#34920;&#31034;&#36827;&#21270;&#26641;&#12290;Phylo2Vec&#23558;&#20219;&#20309;&#20855;&#26377;n&#20010;&#21494;&#23376;&#30340;&#20108;&#21449;&#26641;&#26144;&#23556;&#21040;&#38271;&#24230;&#20026;n&#30340;&#25972;&#25968;&#21521;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Phylo2Vec&#22312;&#31354;&#38388;&#20013;&#26082;&#26159;&#33391;&#23450;&#30340;&#21448;&#26159;&#21452;&#23556;&#30340;&#12290;Phylo2Vec&#30340;&#20248;&#28857;&#26159;&#65306;i&#65289;&#36731;&#26494;&#22343;&#21248;&#37319;&#26679;&#20108;&#21449;&#26641;&#65307;ii&#65289;&#20197;&#38750;&#24120;&#22823;&#25110;&#23567;&#30340;&#27493;&#38271;&#31995;&#32479;&#22320;&#36941;&#21382;&#26641;&#31354;&#38388;&#12290;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#20351;&#29992;Phylo2Vec&#26500;&#24314;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20174;&#27688;&#22522;&#37240;&#24207;&#21015;&#39044;&#27979;&#34507;&#30333;&#36136;&#31867;&#21035;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Phylo2Vec&#26174;&#33879;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Binary phylogenetic trees inferred from biological data are central to understanding the shared evolutionary history of organisms. Inferring the placement of latent nodes in a tree by any optimality criterion (e.g., maximum likelihood) is an NP-hard problem, propelling the development of myriad heuristic approaches. Yet, these heuristics often lack a systematic means of uniformly sampling random trees or effectively exploring a tree space that grows factorially, which are crucial to optimisation problems such as machine learning. Accordingly, we present Phylo2Vec, a new parsimonious representation of a phylogenetic tree. Phylo2Vec maps any binary tree with $n$ leaves to an integer vector of length $n$. We prove that Phylo2Vec is both well-defined and bijective to the space of phylogenetic trees. The advantages of Phylo2Vec are twofold: i) easy uniform sampling of binary trees and ii) systematic ability to traverse tree space in very large or small jumps. As a proof of concept, we use P
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#23548;&#38142;&#27491;&#30830;&#24615;&#21644;&#20449;&#24687;&#37327;&#30340;&#25512;&#29702;&#38142;&#35780;&#20272;&#26694;&#26550;ReCEval&#65292;&#29992;&#20197;&#35780;&#20272;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#23458;&#35266;&#12289;&#31995;&#32479;&#21644;&#20934;&#30830;&#22320;&#35780;&#20272;&#25512;&#29702;&#38142;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.10703</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#30830;&#24615;&#21644;&#20449;&#24687;&#37327;&#35780;&#20272;&#25512;&#29702;&#38142;&#30340;ReCEval
&lt;/p&gt;
&lt;p&gt;
ReCEval: Evaluating Reasoning Chains via Correctness and Informativeness. (arXiv:2304.10703v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#23548;&#38142;&#27491;&#30830;&#24615;&#21644;&#20449;&#24687;&#37327;&#30340;&#25512;&#29702;&#38142;&#35780;&#20272;&#26694;&#26550;ReCEval&#65292;&#29992;&#20197;&#35780;&#20272;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#23458;&#35266;&#12289;&#31995;&#32479;&#21644;&#20934;&#30830;&#22320;&#35780;&#20272;&#25512;&#29702;&#38142;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#37117;&#26159;&#22522;&#30784;&#65292;&#20294;&#20160;&#20040;&#26500;&#25104;&#22909;&#30340;&#25512;&#29702;&#38142;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#23427;&#20204;&#23578;&#19981;&#28165;&#26970;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20165;&#20851;&#27880;&#25512;&#29702;&#38142;&#26159;&#21542;&#23548;&#33268;&#27491;&#30830;&#30340;&#32467;&#35770;&#65292;&#20294;&#36825;&#31181;&#20197;&#31572;&#26696;&#20026;&#23548;&#21521;&#30340;&#35266;&#28857;&#21487;&#33021;&#20250;&#23558;&#22909;&#30340;&#25512;&#29702;&#36136;&#37327;&#19982;&#20854;&#20182;&#29992;&#20110;&#39044;&#27979;&#31572;&#26696;&#30340;&#20551;&#25463;&#24452;&#28151;&#28102;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23558;&#25512;&#29702;&#38142;&#35270;&#20026;&#25512;&#23548;&#26368;&#32456;&#31572;&#26696;&#30340;&#38750;&#27491;&#24335;&#35777;&#26126;&#65292;&#36890;&#36807;&#35780;&#20272;&#25512;&#29702;&#38142;&#30340;&#20004;&#20010;&#20851;&#38190;&#29305;&#24615;&#8212;&#8212;&#65288;1&#65289;&#27491;&#30830;&#24615;&#65292;&#21363;&#27599;&#20010;&#27493;&#39588;&#22522;&#20110;&#27493;&#39588;&#65292;&#21069;&#32622;&#27493;&#39588;&#21644;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#36827;&#34892;&#26377;&#25928;&#25512;&#29702;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20449;&#24687;&#37327;&#65292;&#21363;&#27599;&#20010;&#27493;&#39588;&#25552;&#20379;&#26032;&#20449;&#24687;&#26377;&#21161;&#20110;&#25512;&#23548;&#29983;&#25104;&#30340;&#31572;&#26696;&#8212;&#8212;&#25105;&#20204;&#25552;&#20986;&#20102;ReCEval&#65288;&#25512;&#29702;&#38142;&#35780;&#20272;&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#21644;&#20449;&#24687;&#29702;&#35770;&#27979;&#37327;&#23454;&#29616;&#20102;ReCEval&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#35780;&#20272;&#25512;&#29702;&#38142;&#26041;&#38754;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#21152;&#23458;&#35266;&#12289;&#31995;&#32479;&#21644;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-step reasoning ability is fundamental to many natural language tasks, yet it is unclear what constitutes a good reasoning chain and how to evaluate them. Most existing methods focus solely on whether the reasoning chain leads to the correct conclusion, but this answer-oriented view may confound the quality of reasoning with other spurious shortcuts to predict the answer. To bridge this gap, we evaluate reasoning chains by viewing them as informal proofs that derive the final answer. Specifically, we propose ReCEval (Reasoning Chain Evaluation), a framework that evaluates reasoning chains through two key properties: (1) correctness, i.e., each step makes a valid inference based on the information contained within the step, preceding steps, and input context, and (2) informativeness, i.e., each step provides new information that is helpful towards deriving the generated answer. We implement ReCEval using natural language inference models and information-theoretic measures. On multi
&lt;/p&gt;</description></item><item><title>RAFT&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#40784;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#30340;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06767</link><description>&lt;p&gt;
RAFT: &#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#29992;&#20110;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06767
&lt;/p&gt;
&lt;p&gt;
RAFT&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#40784;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#30340;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#24191;&#27867;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#24102;&#26469;&#30340;&#38544;&#24335;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#23376;&#20248;&#26679;&#26412;&#12289;&#25197;&#26354;&#30340;&#32467;&#26524;&#21644;&#19981;&#20844;&#24179;&#65292;&#21487;&#33021;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#30340;&#20262;&#29702;&#21644;&#20559;&#22909;&#23545;&#40784;&#26159;&#30830;&#20445;&#23427;&#20204;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#36127;&#36131;&#20219;&#21644;&#26377;&#25928;&#30340;&#37096;&#32626;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#37319;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288; RLHF&#65289;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#25163;&#27573;&#12290;&#22312; RL &#31639;&#27861;&#30340;&#25351;&#23548;&#19979;&#65292;&#29992;&#20154;&#31867;&#21453;&#39304;&#25351;&#23548;&#30340;&#22870;&#21169;&#27169;&#22411;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292; RL &#31639;&#27861;&#30340;&#20302;&#25928;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#24120;&#24120;&#20250;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#25104;&#21151;&#23545;&#40784;&#20135;&#29983;&#37325;&#22823;&#38556;&#30861;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#26356;&#20026;&#24378;&#22823;&#21644;&#31616;&#21270;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#65288; RAFT &#65289;&#65292;&#26088;&#22312;&#23545;&#40784;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means of addressing this problem, wherein generative models are fine-tuned using RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment of generative models, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generat
&lt;/p&gt;</description></item><item><title>HarsanyiNet &#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23427;&#21487;&#20197;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#25773;&#20013;&#35745;&#31639;&#36755;&#20837;&#21464;&#37327;&#30340;&#31934;&#30830; Shapley &#20540;&#12290;</title><link>http://arxiv.org/abs/2304.01811</link><description>&lt;p&gt;
HarsanyiNet: &#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#25773;&#20013;&#35745;&#31639;&#20934;&#30830;&#30340; Shapley &#20540;
&lt;/p&gt;
&lt;p&gt;
HarsanyiNet: Computing Accurate Shapley Values in a Single Forward Propagation. (arXiv:2304.01811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01811
&lt;/p&gt;
&lt;p&gt;
HarsanyiNet &#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23427;&#21487;&#20197;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#25773;&#20013;&#35745;&#31639;&#36755;&#20837;&#21464;&#37327;&#30340;&#31934;&#30830; Shapley &#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shapley &#20540;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#19968;&#31181;&#21487;&#20449;&#30340;&#23646;&#24615;&#24230;&#37327;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#20154;&#20204;&#20351;&#29992; Shapley &#20540;&#26469;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#36755;&#20837;&#21464;&#37327;&#30340;&#23646;&#24615;&#26102;&#65292;&#36890;&#24120;&#38656;&#35201;&#38750;&#24120;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#25165;&#33021;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36817;&#20284;&#35745;&#31639;&#20986;&#27604;&#36739;&#31934;&#30830;&#30340; Shapley &#20540;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#32593;&#32476;&#26550;&#26500; HarsanyiNet&#65292;&#22312;&#36755;&#20837;&#26679;&#26412;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#21516;&#26102;&#35745;&#31639;&#36755;&#20837;&#21464;&#37327;&#30340;&#31934;&#30830; Shapley &#20540;&#65292;&#21482;&#38656;&#35201;&#19968;&#27425;&#21069;&#21521;&#20256;&#25773;&#21363;&#21487;&#12290;HarsanyiNet &#26159;&#26500;&#24314;&#22312; Shapley &#20540;&#21487;&#20197;&#34987;&#37325;&#26032;&#26500;&#24314;&#20026;&#32593;&#32476;&#32534;&#30721;&#30340; Harsanyi &#20132;&#20114;&#37325;&#26032;&#20998;&#37197;&#30340;&#29702;&#35770;&#22522;&#30784;&#20043;&#19978;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Shapley value is widely regarded as a trustworthy attribution metric. However, when people use Shapley values to explain the attribution of input variables of a deep neural network (DNN), it usually requires a very high computational cost to approximate relatively accurate Shapley values in real-world applications. Therefore, we propose a novel network architecture, the HarsanyiNet, which makes inferences on the input sample and simultaneously computes the exact Shapley values of the input variables in a single forward propagation. The HarsanyiNet is designed on the theoretical foundation that the Shapley value can be reformulated as the redistribution of Harsanyi interactions encoded by the network.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;SALAMA&#29992;&#20110;&#39044;&#27979;&#38647;&#26292;&#21457;&#29983;&#24773;&#20917;&#65292;&#21487;&#20197;&#22312;&#38271;&#36798;11&#23567;&#26102;&#30340;&#26102;&#38388;&#20869;&#36827;&#34892;&#39044;&#27979;&#65292;&#39044;&#27979;&#25216;&#33021;&#20248;&#20110;&#20256;&#32479;&#26041;&#26696;&#65292;&#19988;&#39044;&#27979;&#26102;&#38388;&#23610;&#24230;&#38543;&#30528;&#39044;&#25253;&#30340;&#31354;&#38388;&#23610;&#24230;&#21576;&#32447;&#24615;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2303.08736</link><description>&lt;p&gt;
&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#36890;&#36807;&#21518;&#22788;&#29702;&#27169;&#25311;&#25968;&#25454;&#39044;&#27979;&#38647;&#26292;&#22825;&#27668;(arXiv:2303.08736v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
A machine-learning approach to thunderstorm forecasting through post-processing of simulation data. (arXiv:2303.08736v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08736
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;SALAMA&#29992;&#20110;&#39044;&#27979;&#38647;&#26292;&#21457;&#29983;&#24773;&#20917;&#65292;&#21487;&#20197;&#22312;&#38271;&#36798;11&#23567;&#26102;&#30340;&#26102;&#38388;&#20869;&#36827;&#34892;&#39044;&#27979;&#65292;&#39044;&#27979;&#25216;&#33021;&#20248;&#20110;&#20256;&#32479;&#26041;&#26696;&#65292;&#19988;&#39044;&#27979;&#26102;&#38388;&#23610;&#24230;&#38543;&#30528;&#39044;&#25253;&#30340;&#31354;&#38388;&#23610;&#24230;&#21576;&#32447;&#24615;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38647;&#26292;&#23545;&#31038;&#20250;&#21644;&#32463;&#27982;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#65292;&#38656;&#35201;&#21487;&#38752;&#30340;&#38647;&#26292;&#39044;&#27979;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SALAMA&#65292;&#19968;&#31181;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#25968;&#25454;&#20013;&#30340;&#38647;&#26292;&#21457;&#29983;&#24773;&#20917;&#12290;&#35813;&#27169;&#22411;&#22312;&#20013;&#27431;&#23545;&#27969;&#23618;&#35299;&#26512;&#38598;&#21512;&#39044;&#25253;&#21644;&#38647;&#30005;&#35266;&#27979;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#20165;&#32473;&#20986;&#20174;NWP&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#19982;&#38647;&#26292;&#21457;&#23637;&#30456;&#20851;&#30340;&#20687;&#32032;&#36755;&#20837;&#21442;&#25968;&#38598;&#65292;SALAMA&#20197;&#21487;&#38752;&#30340;&#26657;&#20934;&#26041;&#24335;&#25512;&#26029;&#38647;&#26292;&#21457;&#29983;&#30340;&#27010;&#29575;&#12290;&#23545;&#20110;&#38271;&#36798;11&#23567;&#26102;&#30340;&#21069;&#32622;&#26102;&#38388;&#65292;&#25105;&#20204;&#21457;&#29616;&#20854;&#39044;&#27979;&#25216;&#33021;&#20248;&#20110;&#20165;&#22522;&#20110;&#23545;&#27969;&#26377;&#25928;&#20301;&#33021;&#30340;&#20998;&#31867;&#12290;&#36890;&#36807;&#25913;&#21464;&#23558;&#38378;&#30005;&#35266;&#27979;&#19982;NWP&#25968;&#25454;&#30456;&#20851;&#32852;&#30340;&#26102;&#31354;&#26631;&#20934;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29087;&#32451;&#30340;&#38647;&#26292;&#39044;&#27979;&#26102;&#38388;&#23610;&#24230;&#38543;&#30528;&#39044;&#25253;&#30340;&#31354;&#38388;&#23610;&#24230;&#30340;&#32447;&#24615;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thunderstorms pose a major hazard to society and economy, which calls for reliable thunderstorm forecasts. In this work, we introduce SALAMA, a feedforward neural network model for identifying thunderstorm occurrence in numerical weather prediction (NWP) data. The model is trained on convection-resolving ensemble forecasts over Central Europe and lightning observations. Given only a set of pixel-wise input parameters that are extracted from NWP data and related to thunderstorm development, SALAMA infers the probability of thunderstorm occurrence in a reliably calibrated manner. For lead times up to eleven hours, we find a forecast skill superior to classification based only on convective available potential energy. Varying the spatiotemporal criteria by which we associate lightning observations with NWP data, we show that the time scale for skillful thunderstorm predictions increases linearly with the spatial scale of the forecast.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#20174;&#35299;&#37322;&#65288;MLX&#65289;&#37325;&#26032;&#26500;&#24314;&#20026;&#23545;&#25239;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#20154;&#31867;&#25552;&#20379;&#30340;&#35299;&#37322;&#26469;&#25351;&#23450;&#19968;&#20010;&#20302;&#32500;&#27969;&#24418;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#23545;&#24378;&#21442;&#25968;&#27491;&#21017;&#21270;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06419</link><description>&lt;p&gt;
&#20174;&#35299;&#37322;&#20013;&#36827;&#34892;&#40065;&#26834;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Learning from Explanations. (arXiv:2303.06419v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#20174;&#35299;&#37322;&#65288;MLX&#65289;&#37325;&#26032;&#26500;&#24314;&#20026;&#23545;&#25239;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#20154;&#31867;&#25552;&#20379;&#30340;&#35299;&#37322;&#26469;&#25351;&#23450;&#19968;&#20010;&#20302;&#32500;&#27969;&#24418;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#23545;&#24378;&#21442;&#25968;&#27491;&#21017;&#21270;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new machine learning approach, recasting machine learning from explanations (MLX) as an adversarial robustness problem, which specifies a lower dimensional manifold from which perturbations can be drawn based on human-provided annotations, and shows improved performance over prior MLX methods on both synthetic and real-world benchmarks.
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20174;&#35299;&#37322;&#65288;MLX&#65289;&#26159;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20154;&#31867;&#25552;&#20379;&#30340;&#26377;&#20851;&#27599;&#20010;&#36755;&#20837;&#30340;&#30456;&#20851;&#29305;&#24449;&#30340;&#27880;&#37322;&#65292;&#20197;&#30830;&#20445;&#27169;&#22411;&#39044;&#27979;&#30340;&#21407;&#22240;&#27491;&#30830;&#12290;&#29616;&#26377;&#30340;MLX&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#38656;&#35201;&#24378;&#22823;&#30340;&#21442;&#25968;&#27491;&#21017;&#21270;&#26469;&#23545;&#40784;&#27169;&#22411;&#21644;&#20154;&#31867;&#35299;&#37322;&#65292;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;MLX&#37325;&#26032;&#26500;&#24314;&#20026;&#23545;&#25239;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#20854;&#20013;&#20154;&#31867;&#35299;&#37322;&#25351;&#23450;&#20102;&#19968;&#20010;&#20302;&#32500;&#27969;&#24418;&#65292;&#21487;&#20197;&#20174;&#20013;&#32472;&#21046;&#25200;&#21160;&#65292;&#24182;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#20943;&#36731;&#23545;&#24378;&#21442;&#25968;&#27491;&#21017;&#21270;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#23454;&#29616;&#40065;&#26834;&#24615;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20808;&#21069;MLX&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#40065;&#26834;&#24615;&#19982;&#26089;&#26399;&#30340;MLX&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20135;&#29983;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning from explanations (MLX) is an approach to learning that uses human-provided annotations of relevant features for each input to ensure that model predictions are right for the right reasons. Existing MLX approaches rely heavily on a specific model interpretation approach and require strong parameter regularization to align model and human explanations, leading to sub-optimal performance. We recast MLX as an adversarial robustness problem, where human explanations specify a lower dimensional manifold from which perturbations can be drawn, and show both theoretically and empirically how this approach alleviates the need for strong parameter regularization. We consider various approaches to achieving robustness, leading to improved performance over prior MLX methods. Finally, we combine robustness with an earlier MLX method, yielding state-of-the-art results on both synthetic and real-world benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#30340;&#24341;&#23548;&#26694;&#33258;&#30001;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#29305;&#24449;&#34701;&#21512;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22312;&#24694;&#21155;&#22825;&#27668;&#19979;&#29289;&#20307;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.06108</link><description>&lt;p&gt;
RaLiBEV: &#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#24341;&#23548;&#26694;&#33258;&#30001;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#30340;&#34701;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object Detection System. (arXiv:2211.06108v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#30340;&#24341;&#23548;&#26694;&#33258;&#30001;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#29305;&#24449;&#34701;&#21512;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22312;&#24694;&#21155;&#22825;&#27668;&#19979;&#29289;&#20307;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#65292;&#28608;&#20809;&#38647;&#36798;&#21644;&#38647;&#36798;&#22312;&#24863;&#30693;&#21608;&#22260;&#29615;&#22659;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28608;&#20809;&#38647;&#36798;&#25552;&#20379;&#31934;&#30830;&#30340;&#19977;&#32500;&#31354;&#38388;&#24863;&#30693;&#20449;&#24687;&#65292;&#20294;&#22312;&#38654;&#31561;&#24694;&#21155;&#22825;&#27668;&#19979;&#26080;&#27861;&#24037;&#20316;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#38647;&#36798;&#20449;&#21495;&#30001;&#20110;&#27874;&#38271;&#30340;&#29305;&#24615;&#22312;&#36935;&#21040;&#38632;&#28404;&#25110;&#38654;&#31890;&#26102;&#20250;&#21457;&#29983;&#34893;&#23556;&#65292;&#20294;&#23427;&#21463;&#21040;&#22823;&#22122;&#22768;&#30340;&#24178;&#25200;&#12290;&#26368;&#36817;&#30340;&#26368;&#26032;&#30740;&#31350;&#34920;&#26126;&#65292;&#38647;&#36798;&#21644;&#28608;&#20809;&#38647;&#36798;&#30340;&#34701;&#21512;&#21487;&#20197;&#22312;&#24694;&#21155;&#22825;&#27668;&#19979;&#23454;&#29616;&#24378;&#20581;&#30340;&#26816;&#27979;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#37319;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20174;&#27599;&#20010;&#20256;&#24863;&#22120;&#25968;&#25454;&#27969;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#28982;&#21518;&#23545;&#40784;&#21644;&#27719;&#32858;&#20004;&#20010;&#20998;&#25903;&#30340;&#29305;&#24449;&#20197;&#39044;&#27979;&#29289;&#20307;&#26816;&#27979;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26631;&#31614;&#20998;&#37197;&#21644;&#34701;&#21512;&#31574;&#30053;&#30340;&#31616;&#21333;&#35774;&#35745;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#36793;&#30028;&#26694;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#40479;&#30640;&#35270;&#35282;&#34701;&#21512;&#23398;&#20064;&#30340;&#24341;&#23548;&#26694;&#33258;&#30001;&#29289;&#20307;&#26816;&#27979;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#23558;&#26469;&#33258;&#38647;&#36798;&#30340;&#36317;&#31163;-&#26041;&#20301;&#29305;&#24449;&#34701;&#21512;&#36215;&#26469;
&lt;/p&gt;
&lt;p&gt;
In autonomous driving systems, LiDAR and radar play important roles in the perception of the surrounding environment. LiDAR provides accurate 3D spatial sensing information but cannot work in adverse weather like fog. On the other hand, the radar signal can be diffracted when encountering raindrops or mist particles thanks to its wavelength, but it suffers from large noise. Recent state-of-the-art works reveal that fusion of radar and LiDAR can lead to robust detection in adverse weather. The existing works adopt convolutional neural network architecture to extract features from each sensor data stream, then align and aggregate the two branch features to predict object detection results. However, these methods have low accuracy of bounding box estimations due to a simple design of label assignment and fusion strategies. In this paper, we propose a bird's-eye view fusion learning-based anchor box-free object detection system, which fuses the feature derived from the radar range-azimuth 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20844;&#24179;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;(RFIB)&#65292;&#20860;&#39038;&#20102;&#34920;&#31034;&#25928;&#29992;&#12289;&#20844;&#24179;&#24615;&#21644;&#32039;&#20945;&#24615;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22270;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#20998;&#31867;&#20013;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#21040;&#20102;&#20154;&#21475;&#24179;&#31561;&#21644;&#31561;&#21270;&#36180;&#29575;&#31561;&#19981;&#21516;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#36890;&#36807;&#28041;&#21450;&#32463;&#20856;&#20449;&#24687;&#29942;&#39048;(IB)&#24230;&#37327;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#23454;&#29616;&#20844;&#24179;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.10043</link><description>&lt;p&gt;
&#21487;&#35843;&#20449;&#24687;&#29942;&#39048;&#21644;R&#233;nyi&#24230;&#37327;&#36890;&#36807;&#20998;&#31867;&#25928;&#29992;&#12289;&#20844;&#24179;&#24615;&#21644;&#32039;&#20945;&#24615;
&lt;/p&gt;
&lt;p&gt;
Classification Utility, Fairness, and Compactness via Tunable Information Bottleneck and R\'enyi Measures. (arXiv:2206.10043v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20844;&#24179;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;(RFIB)&#65292;&#20860;&#39038;&#20102;&#34920;&#31034;&#25928;&#29992;&#12289;&#20844;&#24179;&#24615;&#21644;&#32039;&#20945;&#24615;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22270;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#20998;&#31867;&#20013;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#21040;&#20102;&#20154;&#21475;&#24179;&#31561;&#21644;&#31561;&#21270;&#36180;&#29575;&#31561;&#19981;&#21516;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#36890;&#36807;&#28041;&#21450;&#32463;&#20856;&#20449;&#24687;&#29942;&#39048;(IB)&#24230;&#37327;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#23454;&#29616;&#20844;&#24179;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#35774;&#35745;&#21487;&#20197;&#20934;&#30830;&#33719;&#21462;&#20449;&#24687;&#32780;&#19981;&#27495;&#35270;&#20219;&#20309;&#25935;&#24863;&#23646;&#24615;&#30340;&#31639;&#27861;&#23545;&#20110;&#31038;&#20250;&#25509;&#21463;AI&#29992;&#20110;&#20851;&#38190;&#24212;&#29992;&#22330;&#26223;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20844;&#24179;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;R&#233;nyi&#20844;&#24179;&#20449;&#24687;&#29942;&#39048;&#26041;&#27861;(RFIB)&#65292;&#23427;&#20860;&#39038;&#20102;&#34920;&#31034;&#30340;&#25928;&#29992;&#12289;&#20844;&#24179;&#24615;&#21644;&#32039;&#20945;&#24615;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22270;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#20998;&#31867;&#20013;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32771;&#34385;&#21040;&#20102;&#20154;&#21475;&#24179;&#31561;&#21644;&#31561;&#21270;&#36180;&#29575;&#31561;&#19981;&#21516;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#20351;&#24471;&#28385;&#36275;&#36825;&#20004;&#20010;&#20934;&#21017;&#26356;&#21152;&#31934;&#32454;&#12290;&#21033;&#29992;&#21464;&#20998;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30446;&#26631;&#21487;&#20197;&#20135;&#29983;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#65292;&#20854;&#20013;&#28041;&#21450;&#20102;&#32463;&#20856;&#20449;&#24687;&#29942;&#39048;(IB)&#24230;&#37327;&#65292;&#24182;&#22312;&#20004;&#20010;R&#233;nyi$(\alpha)$&#24230;&#37327;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#19978;&#30028;&#65292;&#29992;&#20110;&#34913;&#37327;&#36755;&#20837;&#21644;&#34920;&#31034;&#20043;&#38388;&#30340;&#32039;&#20945;&#24230;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#29616;&#20844;&#24179;&#24615;&#30340;&#21516;&#26102;&#65292;&#20063;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing machine learning algorithms that are accurate yet fair, not discriminating based on any sensitive attribute, is of paramount importance for society to accept AI for critical applications. In this article, we propose a novel fair representation learning method termed the R\'enyi Fair Information Bottleneck Method (RFIB) which incorporates constraints for utility, fairness, and compactness (compression) of representation, and apply it to image and tabular data classification. A key attribute of our approach is that we consider - in contrast to most prior work - both demographic parity and equalized odds as fairness constraints, allowing for a more nuanced satisfaction of both criteria. Leveraging a variational approach, we show that our objectives yield a loss function involving classical Information Bottleneck (IB) measures and establish an upper bound in terms of two R\'enyi measures of order $\alpha$ on the mutual information IB term measuring compactness between the input a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38597;&#21487;&#27604;&#25511;&#21046;&#30340;&#24102;&#23485;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#38381;&#24335;&#12289;&#35745;&#31639;&#38750;&#24120;&#36731;&#30340;&#29305;&#28857;&#65292;&#24182;&#19988;&#22312;&#20851;&#27880;&#24102;&#23485;&#30340;&#21516;&#26102;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.11956</link><description>&lt;p&gt;
&#36890;&#36807;&#38597;&#21487;&#27604;&#25511;&#21046;&#36873;&#25321;&#39640;&#26031;&#26680;&#23725;&#22238;&#24402;&#24102;&#23485;
&lt;/p&gt;
&lt;p&gt;
Bandwidth Selection for Gaussian Kernel Ridge Regression via Jacobian Control. (arXiv:2205.11956v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38597;&#21487;&#27604;&#25511;&#21046;&#30340;&#24102;&#23485;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#38381;&#24335;&#12289;&#35745;&#31639;&#38750;&#24120;&#36731;&#30340;&#29305;&#28857;&#65292;&#24182;&#19988;&#22312;&#20851;&#27880;&#24102;&#23485;&#30340;&#21516;&#26102;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#35843;&#25972;&#36229;&#21442;&#25968;&#12290;&#23545;&#20110;&#39640;&#26031;&#26680;&#23725;&#22238;&#24402;&#65292;&#36229;&#21442;&#25968;&#26159;&#24102;&#23485;&#12290;&#24102;&#23485;&#25351;&#23450;&#26680;&#20989;&#25968;&#30340;&#38271;&#24230;&#23610;&#24230;&#65292;&#24517;&#39035;&#23567;&#24515;&#36873;&#25321;&#25165;&#33021;&#33719;&#24471;&#20855;&#26377;&#33391;&#22909;&#27867;&#21270;&#24615;&#33021;&#27169;&#22411;&#12290;&#24102;&#23485;&#36873;&#25321;&#30340;&#40664;&#35748;&#26041;&#27861;&#26159;&#20132;&#21449;&#39564;&#35777;&#21644;&#36793;&#32536;&#20284;&#28982;&#26368;&#22823;&#21270;&#65292;&#36825;&#36890;&#24120;&#20250;&#20135;&#29983;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#23613;&#31649;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#25552;&#20379;&#30340;&#20272;&#35745;&#24448;&#24448;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#26041;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#26102;&#12290;&#21463;&#38597;&#21487;&#27604;&#27491;&#21017;&#21270;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#36817;&#20284;&#34920;&#36798;&#24335;&#65292;&#29992;&#20110;&#25551;&#36848;&#39640;&#26031;&#26680;&#23725;&#22238;&#24402;&#25512;&#26029;&#20989;&#25968;&#30340;&#23548;&#25968;&#22914;&#20309;&#21462;&#20915;&#20110;&#26680;&#24102;&#23485;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#34920;&#36798;&#24335;&#26469;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#38597;&#21487;&#27604;&#25511;&#21046;&#30340;&#38381;&#24335;&#12289;&#35745;&#31639;&#38750;&#24120;&#36731;&#30340;&#24102;&#23485;&#36873;&#25321;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#38597;&#21487;&#27604;&#34920;&#36798;&#24335;&#34920;&#26126;&#20102;&#22312;&#26816;&#26597;&#24102;&#23485;&#36873;&#25321;&#30340;&#36136;&#37327;&#26102;&#24212;&#20851;&#27880;&#20160;&#20040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most machine learning methods require tuning of hyper-parameters. For kernel ridge regression with the Gaussian kernel, the hyper-parameter is the bandwidth. The bandwidth specifies the length-scale of the kernel and has to be carefully selected in order to obtain a model with good generalization. The default methods for bandwidth selection is cross-validation and marginal likelihood maximization, which often yields good results, albeit at high computational costs. Furthermore, the estimates provided by these methods tend to have very high variance, especially when training data are scarce. Inspired by Jacobian regularization, we formulate an approximate expression for how the derivatives of the functions inferred by kernel ridge regression with the Gaussian kernel depend on the kernel bandwidth. We then use this expression to propose a closed-form, computationally feather-light, bandwidth selection heuristic based on controlling the Jacobian. In addition, the Jacobian expression illum
&lt;/p&gt;</description></item></channel></rss>