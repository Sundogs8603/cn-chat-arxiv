<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#20013;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#23545;&#20154;&#31867;&#22996;&#25176;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#22996;&#25176;&#34892;&#20026;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20449;&#24687;&#19979;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;&#36825;&#39033;&#30740;&#31350;&#25512;&#36827;&#20102;&#23545;&#20154;&#24037;&#26234;&#33021;&#22996;&#25176;&#20013;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#29702;&#35299;&#65292;&#24182;&#20026;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#21327;&#20316;&#31995;&#32479;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.04729</link><description>&lt;p&gt;
&#20851;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#23545;&#20154;&#31867;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#20013;&#30340;&#22996;&#25176;&#34892;&#20026;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Effect of Contextual Information on Human Delegation Behavior in Human-AI collaboration. (arXiv:2401.04729v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#20013;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#23545;&#20154;&#31867;&#22996;&#25176;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#22996;&#25176;&#34892;&#20026;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20449;&#24687;&#19979;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;&#36825;&#39033;&#30740;&#31350;&#25512;&#36827;&#20102;&#23545;&#20154;&#24037;&#26234;&#33021;&#22996;&#25176;&#20013;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#29702;&#35299;&#65292;&#24182;&#20026;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#21327;&#20316;&#31995;&#32479;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#19981;&#26029;&#22686;&#24378;&#33021;&#21147;&#20026;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21327;&#20316;&#24102;&#26469;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#21033;&#29992;&#29616;&#26377;&#30340;&#20114;&#34917;&#33021;&#21147;&#65292;&#35753;&#20154;&#20204;&#23558;&#20010;&#21035;&#23454;&#20363;&#22996;&#25176;&#32473;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20351;&#20154;&#20204;&#26377;&#25928;&#22320;&#22996;&#25176;&#23454;&#20363;&#38656;&#35201;&#20182;&#20204;&#35780;&#20272;&#33258;&#24049;&#21644;&#20154;&#24037;&#26234;&#33021;&#22312;&#32473;&#23450;&#20219;&#21153;&#30340;&#32972;&#26223;&#19979;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#20154;&#31867;&#20915;&#23450;&#23558;&#23454;&#20363;&#22996;&#25176;&#32473;&#20154;&#24037;&#26234;&#33021;&#26102;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#24403;&#21442;&#19982;&#32773;&#25509;&#25910;&#21040;&#19981;&#21516;&#31867;&#22411;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26102;&#65292;&#22996;&#25176;&#34892;&#20026;&#20250;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#39033;&#30740;&#31350;&#25512;&#36827;&#20102;&#20154;&#24037;&#26234;&#33021;&#22996;&#25176;&#20013;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#29702;&#35299;&#65292;&#24182;&#20026;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#21327;&#20316;&#31995;&#32479;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The constantly increasing capabilities of artificial intelligence (AI) open new possibilities for human-AI collaboration. One promising approach to leverage existing complementary capabilities is allowing humans to delegate individual instances to the AI. However, enabling humans to delegate instances effectively requires them to assess both their own and the AI's capabilities in the context of the given task. In this work, we explore the effects of providing contextual information on human decisions to delegate instances to an AI. We find that providing participants with contextual information significantly improves the human-AI team performance. Additionally, we show that the delegation behavior changes significantly when participants receive varying types of contextual information. Overall, this research advances the understanding of human-AI interaction in human delegation and provides actionable insights for designing more effective collaborative systems.
&lt;/p&gt;</description></item><item><title>U-Mamba&#26159;&#19968;&#20010;&#36890;&#29992;&#32593;&#32476;&#65292;&#36890;&#36807;&#35774;&#35745;&#28151;&#21512;&#30340;CNN-SSM&#22359;&#65292;&#23558;&#21367;&#31215;&#23618;&#30340;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#19982;SSM&#30340;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#32467;&#21512;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#38271;&#31243;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04722</link><description>&lt;p&gt;
U-Mamba:&#22686;&#24378;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#38271;&#31243;&#20381;&#36182;&#24615;
&lt;/p&gt;
&lt;p&gt;
U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation. (arXiv:2401.04722v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04722
&lt;/p&gt;
&lt;p&gt;
U-Mamba&#26159;&#19968;&#20010;&#36890;&#29992;&#32593;&#32476;&#65292;&#36890;&#36807;&#35774;&#35745;&#28151;&#21512;&#30340;CNN-SSM&#22359;&#65292;&#23558;&#21367;&#31215;&#23618;&#30340;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#19982;SSM&#30340;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#32467;&#21512;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#38271;&#31243;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;Transformer&#26159;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#26368;&#27969;&#34892;&#30340;&#26550;&#26500;&#65292;&#20294;&#30001;&#20110;&#22266;&#26377;&#30340;&#23616;&#37096;&#24615;&#25110;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#30340;&#22788;&#29702;&#38271;&#31243;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;U-Mamba&#65292;&#19968;&#20010;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#36890;&#29992;&#32593;&#32476;&#12290;&#21463;&#21040;&#29366;&#24577;&#31354;&#38388;&#24207;&#21015;&#27169;&#22411;&#65288;SSM&#65289;&#30340;&#21551;&#21457;&#65292;SSM&#26159;&#19968;&#31867;&#20197;&#22788;&#29702;&#38271;&#24207;&#21015;&#33021;&#21147;&#24378;&#22823;&#32780;&#38395;&#21517;&#30340;&#28145;&#24230;&#24207;&#21015;&#27169;&#22411;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#28151;&#21512;&#30340;CNN-SSM&#22359;&#65292;&#23558;&#21367;&#31215;&#23618;&#30340;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#19982;SSM&#30340;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#24615;&#30340;&#33021;&#21147;&#32467;&#21512;&#36215;&#26469;&#12290;&#27492;&#22806;&#65292;U-Mamba&#20855;&#26377;&#33258;&#37197;&#32622;&#26426;&#21046;&#65292;&#21487;&#20197;&#33258;&#21160;&#36866;&#24212;&#21508;&#31181;&#25968;&#25454;&#38598;&#65292;&#26080;&#38656;&#25163;&#21160;&#24178;&#39044;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;CT&#21644;MR&#22270;&#20687;&#20013;&#30340;3D&#33145;&#37096;&#22120;&#23448;&#20998;&#21106;&#12289;&#20869;&#31397;&#38236;&#22270;&#20687;&#20013;&#30340;&#22120;&#26800;&#20998;&#21106;&#31561;&#22235;&#20010;&#19981;&#21516;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks (CNNs) and Transformers have been the most popular architectures for biomedical image segmentation, but both of them have limited ability to handle long-range dependencies because of inherent locality or computational complexity. To address this challenge, we introduce U-Mamba, a general-purpose network for biomedical image segmentation. Inspired by the State Space Sequence Models (SSMs), a new family of deep sequence models known for their strong capability in handling long sequences, we design a hybrid CNN-SSM block that integrates the local feature extraction power of convolutional layers with the abilities of SSMs for capturing the long-range dependency. Moreover, U-Mamba enjoys a self-configuring mechanism, allowing it to automatically adapt to various datasets without manual intervention. We conduct extensive experiments on four diverse tasks, including the 3D abdominal organ segmentation in CT and MR images, instrument segmentation in endoscopy imag
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#29289;&#31181;&#20998;&#24067;&#27169;&#22411;&#65292;&#23545;&#20840;&#29699;&#33539;&#22260;&#20869;&#20848;&#31185;&#32676;&#33853;&#30340;&#20445;&#25252;&#29366;&#20917;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#26144;&#23556;&#65292;&#25552;&#20986;&#20102;&#34913;&#37327;&#20445;&#25252;&#29366;&#20917;&#30340;&#20004;&#20010;&#25351;&#26631;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#25351;&#26631;&#22312;&#19990;&#30028;&#33539;&#22260;&#20869;&#30340;&#21464;&#21270;&#21644;&#19982;&#21463;&#20445;&#25252;&#22320;&#21306;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.04691</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#29699;&#33539;&#22260;&#20869;&#20848;&#31185;&#32676;&#33853;&#20445;&#25252;&#29366;&#20917;&#30340;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
AI-based Mapping of the Conservation Status of Orchid Assemblages at Global Scale. (arXiv:2401.04691v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#29289;&#31181;&#20998;&#24067;&#27169;&#22411;&#65292;&#23545;&#20840;&#29699;&#33539;&#22260;&#20869;&#20848;&#31185;&#32676;&#33853;&#30340;&#20445;&#25252;&#29366;&#20917;&#36827;&#34892;&#20102;&#35780;&#20272;&#21644;&#26144;&#23556;&#65292;&#25552;&#20986;&#20102;&#34913;&#37327;&#20445;&#25252;&#29366;&#20917;&#30340;&#20004;&#20010;&#25351;&#26631;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#25351;&#26631;&#22312;&#19990;&#30028;&#33539;&#22260;&#20869;&#30340;&#21464;&#21270;&#21644;&#19982;&#21463;&#20445;&#25252;&#22320;&#21306;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29983;&#29289;&#22810;&#26679;&#24615;&#38754;&#20020;&#36234;&#26469;&#36234;&#22810;&#30340;&#23041;&#32961;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#20934;&#30830;&#30340;&#20840;&#29699;&#22320;&#22270;&#26174;&#31034;&#29289;&#31181;&#32676;&#33853;&#26159;&#21542;&#22788;&#20110;&#21361;&#38505;&#20043;&#20013;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29289;&#31181;&#20998;&#24067;&#27169;&#22411;&#65292;&#23545;&#26631;&#24535;&#24615;&#30340;&#20848;&#31185;&#26893;&#29289;&#23478;&#26063;&#36827;&#34892;&#20102;&#20840;&#29699;&#33539;&#22260;&#20869;&#21644;&#20197;&#20844;&#37324;&#20026;&#21333;&#20301;&#30340;&#20445;&#25252;&#29366;&#20917;&#35780;&#20272;&#21644;&#26144;&#23556;&#65292;&#24182;&#23545;&#19981;&#21516;&#23610;&#24230;&#30340;&#27934;&#23519;&#21147;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;1&#30334;&#19975;&#20010;&#20848;&#31185;&#29289;&#31181;14&#21315;&#20010;&#35266;&#23519;&#35760;&#24405;&#30340;&#28145;&#24230;&#29289;&#31181;&#20998;&#24067;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20840;&#29699;&#33539;&#22260;&#20869;&#21644;&#20197;&#20844;&#37324;&#20026;&#21333;&#20301;&#30340;&#20848;&#31185;&#32676;&#33853;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#30340;&#32676;&#33853;&#20445;&#25252;&#29366;&#20917;&#25351;&#26631;&#65306;&#65288;&#19968;&#65289;&#21463;&#23041;&#32961;&#29289;&#31181;&#30340;&#27604;&#20363;&#65292;&#21644;&#65288;&#20108;&#65289;&#32676;&#33853;&#20013;&#26368;&#21463;&#23041;&#32961;&#29289;&#31181;&#30340;&#29366;&#24577;&#12290;&#25105;&#20204;&#23637;&#31034;&#24182;&#20998;&#26512;&#20102;&#36825;&#20123;&#25351;&#26631;&#22312;&#19990;&#30028;&#33539;&#22260;&#20869;&#30340;&#21464;&#21270;&#65292;&#20197;&#21450;&#23427;&#20204;&#19982;&#33487;&#38376;&#31572;&#33098;&#23707;&#19978;&#24403;&#21069;&#21463;&#20445;&#25252;&#22320;&#21306;&#30340;&#20851;&#31995;&#12290;&#32593;&#19978;&#25552;&#20379;&#30340;&#20840;&#29699;&#21644;&#20114;&#21160;&#22320;&#22270;&#26174;&#31034;&#20102;&#20848;&#31185;&#32676;&#33853;&#30340;&#20445;&#25252;&#29366;&#20917;&#25351;&#26631;&#65292;&#22312;&#25152;&#26377;&#23610;&#24230;&#19978;&#23384;&#22312;&#26126;&#26174;&#30340;&#31354;&#38388;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although increasing threats on biodiversity are now widely recognised, there are no accurate global maps showing whether and where species assemblages are at risk. We hereby assess and map at kilometre resolution the conservation status of the iconic orchid family, and discuss the insights conveyed at multiple scales. We introduce a new Deep Species Distribution Model trained on 1M occurrences of 14K orchid species to predict their assemblages at global scale and at kilometre resolution. We propose two main indicators of the conservation status of the assemblages: (i) the proportion of threatened species, and (ii) the status of the most threatened species in the assemblage. We show and analyze the variation of these indicators at World scale and in relation to currently protected areas in Sumatra island. Global and interactive maps available online show the indicators of conservation status of orchid assemblages, with sharp spatial variations at all scales. The highest level of threat 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#22810;&#23618;&#27425;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32858;&#21512;&#26469;&#33258;&#19981;&#21516;&#20449;&#24687;&#28304;&#30340;&#22810;&#20010;&#32858;&#31867;&#65292;&#24182;&#23558;&#35266;&#27979;&#20998;&#21306;&#21040;&#19981;&#21516;&#30340;&#32858;&#31867;&#20013;&#65292;&#32771;&#34385;&#21040;&#23427;&#20204;&#22312;&#32452;&#20214;&#20869;&#30340;&#29305;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#20840;&#29699;&#39135;&#21697;&#36152;&#26131;&#32593;&#32476;&#20013;&#24471;&#21040;&#20102;&#26377;&#36259;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.04682</link><description>&lt;p&gt;
&#28151;&#21512;&#22810;&#23618;&#27425;&#38543;&#26426;&#22359;&#27169;&#22411;&#29992;&#20110;&#22810;&#35270;&#35282;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Mixture of multilayer stochastic block models for multiview clustering. (arXiv:2401.04682v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#22810;&#23618;&#27425;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32858;&#21512;&#26469;&#33258;&#19981;&#21516;&#20449;&#24687;&#28304;&#30340;&#22810;&#20010;&#32858;&#31867;&#65292;&#24182;&#23558;&#35266;&#27979;&#20998;&#21306;&#21040;&#19981;&#21516;&#30340;&#32858;&#31867;&#20013;&#65292;&#32771;&#34385;&#21040;&#23427;&#20204;&#22312;&#32452;&#20214;&#20869;&#30340;&#29305;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#20840;&#29699;&#39135;&#21697;&#36152;&#26131;&#32593;&#32476;&#20013;&#24471;&#21040;&#20102;&#26377;&#36259;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32858;&#21512;&#26469;&#33258;&#19981;&#21516;&#20449;&#24687;&#28304;&#30340;&#22810;&#20010;&#32858;&#31867;&#30340;&#21407;&#22987;&#26041;&#27861;&#12290;&#27599;&#20010;&#20998;&#21306;&#30001;&#35266;&#27979;&#20043;&#38388;&#30340;&#20849;&#23646;&#24615;&#30697;&#38453;&#36827;&#34892;&#32534;&#30721;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#28151;&#21512;&#22810;&#23618;&#27425;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;SBM&#65289;&#23558;&#20855;&#26377;&#30456;&#20284;&#20449;&#24687;&#30340;&#20849;&#23646;&#24615;&#30697;&#38453;&#20998;&#32452;&#20026;&#32452;&#20214;&#65292;&#24182;&#23558;&#35266;&#27979;&#20998;&#21306;&#21040;&#19981;&#21516;&#30340;&#32858;&#31867;&#20013;&#65292;&#32771;&#34385;&#21040;&#23427;&#20204;&#22312;&#32452;&#20214;&#20869;&#30340;&#29305;&#23450;&#24615;&#12290;&#27169;&#22411;&#21442;&#25968;&#30340;&#21487;&#35782;&#21035;&#24615;&#34987;&#24314;&#31435;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21464;&#20998;&#36125;&#21494;&#26031;EM&#31639;&#27861;&#26469;&#20272;&#35745;&#36825;&#20123;&#21442;&#25968;&#12290;&#36125;&#21494;&#26031;&#26694;&#26550;&#20801;&#35768;&#36873;&#25321;&#26368;&#20248;&#30340;&#32858;&#31867;&#21644;&#32452;&#20214;&#25968;&#37327;&#12290;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#23558;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20849;&#35782;&#32858;&#31867;&#21644;&#22522;&#20110;&#24352;&#37327;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#22797;&#26434;&#32593;&#32476;&#20013;&#30340;&#31038;&#21306;&#26816;&#27979;&#12290;&#26368;&#21518;&#65292;&#35813;&#26041;&#27861;&#34987;&#29992;&#20110;&#20998;&#26512;&#20840;&#29699;&#39135;&#21697;&#36152;&#26131;&#32593;&#32476;&#65292;&#24471;&#21040;&#20102;&#26377;&#36259;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose an original method for aggregating multiple clustering coming from different sources of information. Each partition is encoded by a co-membership matrix between observations. Our approach uses a mixture of multilayer Stochastic Block Models (SBM) to group co-membership matrices with similar information into components and to partition observations into different clusters, taking into account their specificities within the components. The identifiability of the model parameters is established and a variational Bayesian EM algorithm is proposed for the estimation of these parameters. The Bayesian framework allows for selecting an optimal number of clusters and components. The proposed approach is compared using synthetic data with consensus clustering and tensor-based algorithms for community detection in large-scale complex networks. Finally, the method is utilized to analyze global food trading networks, leading to structures of interest.
&lt;/p&gt;</description></item><item><title>RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04679</link><description>&lt;p&gt;
RoSA: &#36890;&#36807;&#40065;&#26834;&#36866;&#24212;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04679
&lt;/p&gt;
&lt;p&gt;
RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#32972;&#26223;&#19979;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#39044;&#31639;&#19979;&#25552;&#20379;&#33391;&#22909;&#20934;&#30830;&#24615;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843; (PEFT) &#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#31216;&#20026;RoSA&#65292;&#21463;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512; (PCA) &#30340;&#21551;&#21457;&#65292;&#23427;&#22312;&#19968;&#32452;&#22266;&#23450;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#20849;&#21516;&#35757;&#32451;$\textit{&#20302;&#31209;}$&#21644;$\textit{&#39640;&#24230;&#31232;&#30095;}$&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#65288;FFT&#65289;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RoSA&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#23567;&#23398;&#25968;&#23398;&#21644;SQL&#26597;&#35810;&#29983;&#25104;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#20197;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#30456;&#21516;&#30340;&#21442;&#25968;&#39044;&#31639;&#19979;&#65292;RoSA&#20248;&#20110;LoRA&#21644;&#32431;&#31929;&#30340;&#31232;&#30095;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#31232;&#30095;GPU&#20869;&#26680;&#20026;RoSA&#25552;&#20379;&#31995;&#32479;&#25903;&#25345;&#65292;&#20197;&#34917;&#20805;&#35757;&#32451;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;https://github.com/IST-DASLab&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memoryand computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;Copula&#30340;&#36801;&#31227;&#23398;&#20064;&#33258;&#21160;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#35843;&#20248;&#25968;&#25454;&#20013;&#25628;&#32034;&#31354;&#38388;&#30340;&#39640;&#24615;&#33021;&#21306;&#22495;&#24314;&#27169;&#65292;&#20026;&#26032;&#20219;&#21153;&#29983;&#25104;&#39640;&#24615;&#33021;&#37197;&#32622;&#12290;</title><link>http://arxiv.org/abs/2401.04669</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;Copula&#30340;&#36801;&#31227;&#23398;&#20064;&#33258;&#21160;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Transfer-Learning-Based Autotuning Using Gaussian Copula. (arXiv:2401.04669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04669
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;Copula&#30340;&#36801;&#31227;&#23398;&#20064;&#33258;&#21160;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#35843;&#20248;&#25968;&#25454;&#20013;&#25628;&#32034;&#31354;&#38388;&#30340;&#39640;&#24615;&#33021;&#21306;&#22495;&#24314;&#27169;&#65292;&#20026;&#26032;&#20219;&#21153;&#29983;&#25104;&#39640;&#24615;&#33021;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#26679;&#21270;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#24212;&#29992;&#31243;&#24207;&#26377;&#26426;&#20250;&#35299;&#20915;&#27604;&#20197;&#24448;&#26356;&#22823;&#30340;&#38382;&#39064;&#12290;&#37492;&#20110;&#36825;&#20123;HPC&#31995;&#32479;&#21644;&#24212;&#29992;&#31243;&#24207;&#35843;&#20248;&#30340;&#26174;&#33879;&#22686;&#21152;&#30340;&#22797;&#26434;&#24615;&#65292;&#32463;&#39564;&#24615;&#33021;&#35843;&#20248;&#65288;&#22914;&#33258;&#21160;&#35843;&#20248;&#65289;&#36817;&#24180;&#26469;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#20854;&#26377;&#25928;&#24615;&#65292;&#33258;&#21160;&#35843;&#20248;&#24448;&#24448;&#26159;&#19968;&#31181;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#20256;&#36755;&#23398;&#20064;&#65288;TL&#65289;&#30340;&#33258;&#21160;&#35843;&#20248;&#35797;&#22270;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#35843;&#20248;&#30340;&#25968;&#25454;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#30446;&#21069;TL&#26041;&#27861;&#20026;&#33258;&#21160;&#35843;&#20248;&#22312;&#24314;&#27169;&#21442;&#25968;&#37197;&#32622;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#19978;&#33457;&#36153;&#20102;&#22823;&#37327;&#26102;&#38388;&#65292;&#23545;&#20110;&#26032;&#20219;&#21153;&#30340;&#23569;&#26679;&#26412;&#65288;&#21363;&#65292;&#23569;&#25968;&#32463;&#39564;&#35780;&#20272;&#65289;&#35843;&#20248;&#26159;&#26080;&#25928;&#30340;&#12290;&#25105;&#20204;&#39318;&#27425;&#20171;&#32461;&#20102;&#22522;&#20110;&#39640;&#26031;Copula&#65288;GC&#65289;&#30340;&#29983;&#25104;&#24335;TL&#33258;&#21160;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20808;&#21069;&#25968;&#25454;&#20013;&#25628;&#32034;&#31354;&#38388;&#30340;&#39640;&#24615;&#33021;&#21306;&#22495;&#24314;&#27169;&#65292;&#20026;&#26032;&#20219;&#21153;&#29983;&#25104;&#39640;&#24615;&#33021;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
As diverse high-performance computing (HPC) systems are built, many opportunities arise for applications to solve larger problems than ever before. Given the significantly increased complexity of these HPC systems and application tuning, empirical performance tuning, such as autotuning, has emerged as a promising approach in recent years. Despite its effectiveness, autotuning is often a computationally expensive approach. Transfer learning (TL)-based autotuning seeks to address this issue by leveraging the data from prior tuning. Current TL methods for autotuning spend significant time modeling the relationship between parameter configurations and performance, which is ineffective for few-shot (that is, few empirical evaluations) tuning on new tasks. We introduce the first generative TL-based autotuning approach based on the Gaussian copula (GC) to model the high-performing regions of the search space from prior data and then generate high-performing configurations for new tasks. This 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;ASSIRA&#29483;&#29399;&#25968;&#25454;&#38598;&#65292;&#27604;&#36739;&#20102;&#20351;&#29992;&#19981;&#21516;&#20248;&#21270;&#22120;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04666</link><description>&lt;p&gt;
ASSIRA&#29483;&#29399;&#25968;&#25454;&#38598;&#19978;&#21508;&#31181;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22522;&#20934;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Benchmark Analysis of Various Pre-trained Deep Learning Models on ASSIRA Cats and Dogs Dataset. (arXiv:2401.04666v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;ASSIRA&#29483;&#29399;&#25968;&#25454;&#38598;&#65292;&#27604;&#36739;&#20102;&#20351;&#29992;&#19981;&#21516;&#20248;&#21270;&#22120;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#22522;&#26412;&#24212;&#29992;&#21644;&#23454;&#29616;&#65292;&#22270;&#20687;&#20998;&#31867;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#30693;&#21517;&#25968;&#25454;&#31185;&#23398;&#31038;&#21306;&#25552;&#20379;&#20102;&#21508;&#31181;&#25968;&#25454;&#38598;&#26469;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;ASSIRA&#29483;&#29399;&#25968;&#25454;&#38598;&#26159;&#20854;&#20013;&#20043;&#19968;&#65292;&#24182;&#19988;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#34987;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#30340;&#25972;&#20307;&#25509;&#21463;&#24230;&#21644;&#22522;&#20934;&#26631;&#20934;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#20248;&#21270;&#22120;&#21644;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25913;&#21464;&#36229;&#21442;&#25968;&#20197;&#33719;&#24471;&#27169;&#22411;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;&#36890;&#36807;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#19981;&#23545;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#37325;&#22823;&#26356;&#25913;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#36816;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#25968;&#25454;&#38598;&#20197;&#20934;&#30830;&#29575;&#36229;&#36807;&#20808;&#21069;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the most basic application and implementation of deep learning, image classification has grown in popularity. Various datasets are provided by renowned data science communities for benchmarking machine learning algorithms and pre-trained models. The ASSIRA Cats &amp; Dogs dataset is one of them and is being used in this research for its overall acceptance and benchmark standards. A comparison of various pre-trained models is demonstrated by using different types of optimizers and loss functions. Hyper-parameters are changed to gain the best result from a model. By applying this approach, we have got higher accuracy without major changes in the training model. To run the experiment, we used three different computer architectures: a laptop equipped with NVIDIA GeForce GTX 1070, a laptop equipped with NVIDIA GeForce RTX 3080Ti, and a desktop equipped with NVIDIA GeForce RTX 3090. The acquired results demonstrate supremacy in terms of accuracy over the previously done experiments on this da
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#38544;&#21547;&#29289;&#29702;&#27169;&#22411;&#26694;&#26550;&#65292;&#21487;&#20197;&#27867;&#21270;&#36866;&#24212;&#31995;&#32479;&#36755;&#20837;&#12289;&#21442;&#25968;&#21644;&#22495;&#30340;&#21464;&#21270;&#65292;&#24182;&#22312;&#31995;&#32479;&#21457;&#29616;&#20013;&#23637;&#29616;&#20102;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04648</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#38544;&#21547;&#29289;&#29702;&#27169;&#22411;&#27867;&#21270;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A novel framework for generalization of deep hidden physics models. (arXiv:2401.04648v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04648
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#38544;&#21547;&#29289;&#29702;&#27169;&#22411;&#26694;&#26550;&#65292;&#21487;&#20197;&#27867;&#21270;&#36866;&#24212;&#31995;&#32479;&#36755;&#20837;&#12289;&#21442;&#25968;&#21644;&#22495;&#30340;&#21464;&#21270;&#65292;&#24182;&#22312;&#31995;&#32479;&#21457;&#29616;&#20013;&#23637;&#29616;&#20102;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#24037;&#31243;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#65292;&#23545;&#20110;&#31995;&#32479;&#30340;&#24314;&#27169;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#23436;&#25972;&#30340;&#31995;&#32479;&#20449;&#24687;&#26159;&#26410;&#30693;&#30340;&#65292;&#35201;&#20040;&#26159;&#22240;&#20026;&#32771;&#34385;&#21040;&#25152;&#26377;&#28041;&#21450;&#30340;&#22797;&#26434;&#29289;&#29702;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#35201;&#20040;&#26159;&#20026;&#20102;&#22312;&#21487;&#29992;&#36164;&#28304;&#30340;&#38480;&#21046;&#20869;&#32771;&#34385;&#26356;&#31616;&#21333;&#30340;&#27169;&#22411;&#12290;&#26368;&#36817;&#22312;&#28784;&#30418;&#24314;&#27169;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;&#22914;&#28145;&#24230;&#38544;&#21547;&#29289;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#25968;&#25454;&#21644;&#29289;&#29702;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#23454;&#38469;&#24212;&#29992;&#65292;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#22240;&#20026;&#20026;&#27599;&#20010;&#31995;&#32479;&#36755;&#20837;&#21644;&#21442;&#25968;&#30340;&#24494;&#23567;&#21464;&#21270;&#25110;&#22495;&#37197;&#32622;&#30340;&#20462;&#25913;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#32463;&#27982;&#19978;&#19981;&#21487;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#38544;&#21547;&#29289;&#29702;&#27169;&#22411;&#24605;&#24819;&#30340;&#26032;&#25913;&#36827;&#65292;&#21487;&#20197;&#36866;&#24212;&#31995;&#32479;&#36755;&#20837;&#12289;&#21442;&#25968;&#21644;&#22495;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#31995;&#32479;&#21457;&#29616;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#24110;&#21161;&#23398;&#20064;&#21040;&#20102;&#21464;&#21270;&#21518;&#31995;&#32479;&#36755;&#20837;&#12289;&#21442;&#25968;&#21644;&#22495;&#30340;&#38544;&#34255;&#29289;&#29702;&#35268;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modelling of systems where the full system information is unknown is an oft encountered problem for various engineering and industrial applications, as it's either impossible to consider all the complex physics involved or simpler models are considered to keep within the limits of the available resources. Recent advances in greybox modelling like the deep hidden physics models address this space by combining data and physics. However, for most real-life applications, model generalizability is a key issue, as retraining a model for every small change in system inputs and parameters or modification in domain configuration can render the model economically unviable. In this work we present a novel enhancement to the idea of hidden physics models which can generalize for changes in system inputs, parameters and domains. We also show that this approach holds promise in system discovery as well and helps learn the hidden physics for the changed system inputs, parameters and domain configurat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#39564;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#20027;&#20998;&#31867;&#22120;&#32593;&#32476;&#20013;&#28155;&#21152;&#26080;&#30417;&#30563;&#30340;&#35299;&#37322;&#29983;&#25104;&#22120;&#21644;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#35299;&#37322;&#27169;&#22359;&#25552;&#21462;&#35270;&#35273;&#27010;&#24565;&#65292;&#21516;&#26102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22359;&#26469;&#21306;&#20998;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#30495;&#23454;&#22270;&#20687;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23398;&#21040;&#30340;&#27010;&#24565;&#19982;&#23545;&#35937;&#37096;&#20998;&#21644;&#35270;&#35273;&#23646;&#24615;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04647</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25512;&#36827;&#20808;&#39564;&#21487;&#35299;&#37322;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Advancing Ante-Hoc Explainable Models through Generative Adversarial Networks. (arXiv:2401.04647v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#39564;&#21487;&#35299;&#37322;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#20027;&#20998;&#31867;&#22120;&#32593;&#32476;&#20013;&#28155;&#21152;&#26080;&#30417;&#30563;&#30340;&#35299;&#37322;&#29983;&#25104;&#22120;&#21644;&#23545;&#25239;&#35757;&#32451;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#35299;&#37322;&#27169;&#22359;&#25552;&#21462;&#35270;&#35273;&#27010;&#24565;&#65292;&#21516;&#26102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22359;&#26469;&#21306;&#20998;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#30495;&#23454;&#22270;&#20687;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23398;&#21040;&#30340;&#27010;&#24565;&#19982;&#23545;&#35937;&#37096;&#20998;&#21644;&#35270;&#35273;&#23646;&#24615;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#20013;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#35299;&#37322;&#29983;&#25104;&#22120;&#28155;&#21152;&#21040;&#20027;&#20998;&#31867;&#22120;&#32593;&#32476;&#20013;&#65292;&#24182;&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#35299;&#37322;&#27169;&#22359;&#34987;&#20248;&#21270;&#20197;&#20174;&#20998;&#31867;&#22120;&#30340;&#28508;&#22312;&#34920;&#31034;&#20013;&#25552;&#21462;&#35270;&#35273;&#27010;&#24565;&#65292;&#32780;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#27169;&#22359;&#21017;&#26088;&#22312;&#21306;&#20998;&#20174;&#27010;&#24565;&#20013;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#30495;&#23454;&#22270;&#20687;&#12290;&#36825;&#31181;&#32852;&#21512;&#35757;&#32451;&#26041;&#26696;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23558;&#20854;&#20869;&#37096;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#19982;&#20154;&#21487;&#35299;&#37322;&#30340;&#35270;&#35273;&#23646;&#24615;&#38544;&#24335;&#22320;&#23545;&#40784;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20135;&#29983;&#20102;&#36830;&#36143;&#30340;&#27010;&#24565;&#28608;&#27963;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#23398;&#21040;&#30340;&#27010;&#24565;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#23545;&#35937;&#37096;&#20998;&#21644;&#35270;&#35273;&#23646;&#24615;&#20043;&#38388;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#23545;&#25239;&#35757;&#32451;&#21327;&#35758;&#20013;&#30340;&#25200;&#21160;&#23545;&#20998;&#31867;&#21644;&#27010;&#24565;&#33719;&#21462;&#30340;&#24433;&#21709;&#12290;&#24635;&#20043;&#65292;&#26412;&#25991;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25512;&#36827;&#20102;&#20808;&#39564;&#21487;&#35299;&#37322;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel concept learning framework for enhancing model interpretability and performance in visual classification tasks. Our approach appends an unsupervised explanation generator to the primary classifier network and makes use of adversarial training. During training, the explanation module is optimized to extract visual concepts from the classifier's latent representations, while the GAN-based module aims to discriminate images generated from concepts, from true images. This joint training scheme enables the model to implicitly align its internally learned concepts with human-interpretable visual properties. Comprehensive experiments demonstrate the robustness of our approach, while producing coherent concept activations. We analyse the learned concepts, showing their semantic concordance with object parts and visual attributes. We also study how perturbations in the adversarial training protocol impact both classification and concept acquisition. In summary, this 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#65292;&#25552;&#20986;&#19968;&#31181;&#21487;&#38752;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#25253;&#21578;&#30340;&#20248;&#20808;&#32423;&#25490;&#24207;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#36739;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#19979;&#20173;&#33021;&#20445;&#25345;&#21487;&#38752;&#24615;&#65292;&#20943;&#23569;&#20102;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04637</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;API&#24212;&#29992;&#20110;&#38382;&#39064;&#20998;&#31867;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Applying Large Language Models API to Issue Classification Problem. (arXiv:2401.04637v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#65292;&#25552;&#20986;&#19968;&#31181;&#21487;&#38752;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#25253;&#21578;&#30340;&#20248;&#20808;&#32423;&#25490;&#24207;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#36739;&#23567;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#19979;&#20173;&#33021;&#20445;&#25345;&#21487;&#38752;&#24615;&#65292;&#20943;&#23569;&#20102;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#65292;&#38382;&#39064;&#25253;&#21578;&#30340;&#26377;&#25928;&#25490;&#24207;&#23545;&#20110;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#21644;&#21450;&#26102;&#35299;&#20915;&#20851;&#38190;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#23545;&#38382;&#39064;&#25253;&#21578;&#36827;&#34892;&#20998;&#31867;&#20197;&#36827;&#34892;&#25490;&#24207;&#26159;&#36153;&#21147;&#19988;&#32570;&#20047;&#21487;&#20280;&#32553;&#24615;&#30340;&#12290;&#30456;&#21453;&#65292;&#35768;&#22810;&#24320;&#28304;&#36719;&#20214;&#39033;&#30446;&#20351;&#29992;&#33258;&#21160;&#21270;&#27969;&#31243;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#23613;&#31649;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20805;&#20998;&#30340;&#35757;&#32451;&#12290;&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#35774;&#35745;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#22312;&#20351;&#29992;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#26102;&#20173;&#33021;&#30830;&#20445;&#38382;&#39064;&#25490;&#24207;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#30340;&#33021;&#21147;&#65292;&#35748;&#35782;&#21040;&#23427;&#20204;&#22312;&#22788;&#29702;&#36825;&#20010;&#20219;&#21153;&#26102;&#30340;&#39640;&#25928;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;&#20934;&#30830;&#20248;&#20808;&#32423;&#38382;&#39064;&#25253;&#21578;&#30340;&#21487;&#38752;&#31995;&#32479;&#65292;&#38477;&#20302;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#38752;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24050;&#32463;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#22522;&#20110;GPT&#30340;&#26041;&#27861;&#26469;&#20934;&#30830;&#26631;&#35760;&#38382;&#39064;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective prioritization of issue reports is crucial in software engineering to optimize resource allocation and address critical problems promptly. However, the manual classification of issue reports for prioritization is laborious and lacks scalability. Alternatively, many open source software (OSS) projects employ automated processes for this task, albeit relying on substantial datasets for adequate training. This research seeks to devise an automated approach that ensures reliability in issue prioritization, even when trained on smaller datasets. Our proposed methodology harnesses the power of Generative Pre-trained Transformers (GPT), recognizing their potential to efficiently handle this task. By leveraging the capabilities of such models, we aim to develop a robust system for prioritizing issue reports accurately, mitigating the necessity for extensive training data while maintaining reliability. In our research, we have developed a reliable GPT-based approach to accurately labe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27979;&#35797;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23558;&#20854;&#29992;&#20110;&#32929;&#31080;&#25968;&#25454;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20855;&#26377;&#36229;&#22797;&#25968;&#23494;&#38598;&#23618;&#30340;&#26550;&#26500;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#31867;&#20284;&#20110;&#20854;&#20182;&#26550;&#26500;&#65292;&#20294;&#21487;&#35757;&#32451;&#21442;&#25968;&#26356;&#23569;&#12290;&#27492;&#22806;&#65292;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#30340;&#39034;&#24207;&#23545;&#26377;&#25928;&#24615;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.04632</link><description>&lt;p&gt;
&#32929;&#31080;&#25968;&#25454;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#22235;&#20803;&#25968;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Hypercomplex neural network in time series forecasting of stock data. (arXiv:2401.04632v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27979;&#35797;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23558;&#20854;&#29992;&#20110;&#32929;&#31080;&#25968;&#25454;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20855;&#26377;&#36229;&#22797;&#25968;&#23494;&#38598;&#23618;&#30340;&#26550;&#26500;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#31867;&#20284;&#20110;&#20854;&#20182;&#26550;&#26500;&#65292;&#20294;&#21487;&#35757;&#32451;&#21442;&#25968;&#26356;&#23569;&#12290;&#27492;&#22806;&#65292;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#30340;&#39034;&#24207;&#23545;&#26377;&#25928;&#24615;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27979;&#35797;&#20102;&#19977;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26550;&#26500;&#12290;&#23427;&#20204;&#30340;&#21306;&#21035;&#22312;&#20110;&#36755;&#20837;&#23618;&#21253;&#21547;&#21367;&#31215;&#23618;&#12289;LSTM&#23618;&#25110;&#22235;&#20803;&#25968;4D&#20195;&#25968;&#30340;&#36229;&#22797;&#25968;&#23618;&#12290;&#36755;&#20837;&#26159;&#22235;&#20010;&#30456;&#20851;&#30340;&#32929;&#31080;&#24066;&#22330;&#26102;&#38388;&#24207;&#21015;&#65292;&#39044;&#27979;&#20854;&#20013;&#19968;&#20010;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#20248;&#21270;&#19982;&#26550;&#26500;&#31867;&#21035;&#30456;&#20851;&#30340;&#36229;&#21442;&#25968;&#65292;&#27604;&#36739;&#20102;&#26368;&#20339;&#31070;&#32463;&#32593;&#32476;&#22312;&#31867;&#21035;&#20869;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#36229;&#22797;&#25968;&#23494;&#38598;&#23618;&#30340;&#26550;&#26500;&#25552;&#20379;&#20102;&#19982;&#20854;&#20182;&#26550;&#26500;&#30456;&#20284;&#30340;MAE&#20934;&#30830;&#24615;&#65292;&#20294;&#21487;&#35757;&#32451;&#21442;&#25968;&#35201;&#23569;&#24471;&#22810;&#12290;&#30001;&#20110;&#36825;&#19968;&#28857;&#65292;&#36229;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#27604;&#20854;&#20182;&#27979;&#35797;&#30340;&#26550;&#26500;&#26356;&#24555;&#22320;&#23398;&#20064;&#21644;&#22788;&#29702;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#30340;&#39034;&#24207;&#23545;&#26377;&#25928;&#24615;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The three classes of architectures for time series prediction were tested. They differ by input layers which contain either convolutional, LSTM, or dense hypercomplex layers for 4D algebras. The input was four related Stock Market time series, and the prediction of one of them is expected. The optimization of hyperparameters related to the classes of architectures was performed in order to compare the best neural networks within the class. The results show that in most cases, the architecture with a hypercomplex dense layer provides similar MAE accuracy to other architectures, however, with considerably less trainable parameters. Thanks to it, hypercomplex neural networks can be learned and process data faster than the other tested architectures. Moreover, the order of the input time series has an impact on effectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#23616;&#37096;&#39640;&#26031;&#36807;&#31243;&#65292;&#29992;&#20110;&#27700;&#27745;&#26579;&#30417;&#27979;&#12290;&#20351;&#29992;&#23616;&#37096;&#39640;&#26031;&#36807;&#31243;&#20934;&#30830;&#24314;&#27169;&#19981;&#21516;&#31354;&#38388;&#30456;&#20851;&#24615;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#37319;&#29992;&#28145;&#24230;&#21367;&#31215;&#31574;&#30053;&#20915;&#31574;&#26041;&#27861;&#26469;&#33719;&#24471;&#26377;&#25928;&#30340;&#30417;&#27979;&#31574;&#30053;&#12290;&#36890;&#36807;&#21452;&#37325;&#28145;&#24230; Q &#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#26234;&#33021;&#20307;&#20197;&#20943;&#23567;&#20272;&#35745;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.04631</link><description>&lt;p&gt;
&#29992;&#20110;&#27700;&#27745;&#26579;&#30417;&#27979;&#30340;&#28145;&#24230;&#24378;&#21270;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#26694;&#26550;&#21644;&#23616;&#37096;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Multi-agent Learning framework for Information Gathering with Local Gaussian Processes for Water Monitoring. (arXiv:2401.04631v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#23616;&#37096;&#39640;&#26031;&#36807;&#31243;&#65292;&#29992;&#20110;&#27700;&#27745;&#26579;&#30417;&#27979;&#12290;&#20351;&#29992;&#23616;&#37096;&#39640;&#26031;&#36807;&#31243;&#20934;&#30830;&#24314;&#27169;&#19981;&#21516;&#31354;&#38388;&#30456;&#20851;&#24615;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#37319;&#29992;&#28145;&#24230;&#21367;&#31215;&#31574;&#30053;&#20915;&#31574;&#26041;&#27861;&#26469;&#33719;&#24471;&#26377;&#25928;&#30340;&#30417;&#27979;&#31574;&#30053;&#12290;&#36890;&#36807;&#21452;&#37325;&#28145;&#24230; Q &#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#26234;&#33021;&#20307;&#20197;&#20943;&#23567;&#20272;&#35745;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26377;&#25928;&#30417;&#27979;&#27700;&#36136;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#33258;&#20027;&#27700;&#38754;&#36710;&#32452;&#25104;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#33337;&#38431;&#30340;&#23433;&#20840;&#25511;&#21046;&#65292;&#33337;&#38431;&#31574;&#30053;&#24212;&#35813;&#33021;&#22815;&#22522;&#20110;&#27979;&#37327;&#32467;&#26524;&#21644;&#33337;&#38431;&#29366;&#24577;&#36827;&#34892;&#34892;&#21160;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23616;&#37096;&#39640;&#26031;&#36807;&#31243;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#32852;&#21512;&#33719;&#24471;&#26377;&#25928;&#30340;&#30417;&#27979;&#31574;&#30053;&#12290;&#23616;&#37096;&#39640;&#26031;&#36807;&#31243;&#21487;&#20197;&#20934;&#30830;&#22320;&#24314;&#27169;&#19981;&#21516;&#31354;&#38388;&#30456;&#20851;&#24615;&#20013;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#26356;&#20934;&#30830;&#22320;&#25429;&#25417;&#27700;&#36136;&#20449;&#24687;&#12290;&#25991;&#20013;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31574;&#30053;&#30340;&#20915;&#31574;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#36825;&#20010;&#27169;&#22411;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#65292;&#20351;&#29992;&#20449;&#24687;&#22686;&#30410;&#22870;&#21169;&#36827;&#34892;&#20915;&#31574;&#12290;&#36890;&#36807;&#21452;&#37325;&#28145;&#24230; Q &#23398;&#20064;&#31639;&#27861;&#65292;&#26234;&#33021;&#20307;&#34987;&#35757;&#32451;&#20197;&#22312;&#23433;&#20840;&#30340;&#24773;&#20917;&#19979;&#23613;&#37327;&#20943;&#23567;&#20272;&#35745;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conservation of hydrological resources involves continuously monitoring their contamination. A multi-agent system composed of autonomous surface vehicles is proposed in this paper to efficiently monitor the water quality. To achieve a safe control of the fleet, the fleet policy should be able to act based on measurements and to the the fleet state. It is proposed to use Local Gaussian Processes and Deep Reinforcement Learning to jointly obtain effective monitoring policies. Local Gaussian processes, unlike classical global Gaussian processes, can accurately model the information in a dissimilar spatial correlation which captures more accurately the water quality information. A Deep convolutional policy is proposed, that bases the decisions on the observation on the mean and variance of this model, by means of an information gain reward. Using a Double Deep Q-Learning algorithm, agents are trained to minimize the estimation error in a safe manner thanks to a Consensus-based heuristi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#31070;&#32463;&#26631;&#35760;&#26102;&#24207;&#28857;&#36807;&#31243;&#27169;&#22411;&#24320;&#21457;&#20102;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#30456;&#23481;&#39044;&#27979;&#26694;&#26550;&#29983;&#25104;&#26080;&#20998;&#24067;&#20551;&#35774;&#30340;&#32852;&#21512;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#25552;&#20379;&#36739;&#22909;&#30340;&#36793;&#38469;&#35206;&#30422;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.04612</link><description>&lt;p&gt;
&#26080;&#20998;&#24067;&#20551;&#35774;&#30340;&#31070;&#32463;&#26631;&#35760;&#26102;&#24207;&#28857;&#36807;&#31243;&#30340;&#26080;&#20559;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Distribution-Free Conformal Joint Prediction Regions for Neural Marked Temporal Point Processes. (arXiv:2401.04612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#31070;&#32463;&#26631;&#35760;&#26102;&#24207;&#28857;&#36807;&#31243;&#27169;&#22411;&#24320;&#21457;&#20102;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#30456;&#23481;&#39044;&#27979;&#26694;&#26550;&#29983;&#25104;&#26080;&#20998;&#24067;&#20551;&#35774;&#30340;&#32852;&#21512;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#25552;&#20379;&#36739;&#22909;&#30340;&#36793;&#38469;&#35206;&#30422;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#26102;&#38388;&#30340;&#19981;&#35268;&#21017;&#38388;&#38548;&#35266;&#27979;&#21040;&#30340;&#26631;&#35760;&#20107;&#20214;&#24207;&#21015;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#26102;&#24207;&#28857;&#36807;&#31243;&#65288;Temporal Point Processes&#65292;TPPs&#65289;&#25552;&#20379;&#20102;&#24314;&#27169;&#36825;&#20123;&#24207;&#21015;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#21487;&#20197;&#36827;&#34892;&#35832;&#22914;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#21040;&#36798;&#26102;&#38388;&#21644;&#30456;&#20851;&#26631;&#35760;&#31561;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#35268;&#33539;&#38169;&#35823;&#25110;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#20123;&#27010;&#29575;&#27169;&#22411;&#21487;&#33021;&#23545;&#26410;&#30693;&#30340;&#30495;&#23454;&#22522;&#30784;&#36807;&#31243;&#25552;&#20379;&#36739;&#24046;&#30340;&#36817;&#20284;&#65292;&#20174;&#20013;&#25552;&#21462;&#30340;&#39044;&#27979;&#21306;&#22495;&#21487;&#33021;&#26159;&#23545;&#22522;&#30784;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#21487;&#38752;&#20272;&#35745;&#12290;&#26412;&#25991;&#22522;&#20110;&#30456;&#23481;&#39044;&#27979;&#26694;&#26550;&#65292;&#20026;&#31070;&#32463;TPP&#27169;&#22411;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#12290;&#20854;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#26159;&#29983;&#25104;&#21040;&#36798;&#26102;&#38388;&#21644;&#26631;&#35760;&#30340;&#26080;&#20998;&#24067;&#20551;&#35774;&#30340;&#32852;&#21512;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#25552;&#20379;&#26377;&#38480;&#26679;&#26412;&#36793;&#38469;&#35206;&#30422;&#20445;&#35777;&#12290;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#21516;&#26102;&#22788;&#29702;&#20005;&#26684;&#27491;&#30340;&#36830;&#32493;&#21709;&#24212;&#21644;&#20998;&#31867;&#21709;&#24212;&#65292;&#32780;&#19981;&#38656;&#35201;&#20998;&#24067;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequences of labeled events observed at irregular intervals in continuous time are ubiquitous across various fields. Temporal Point Processes (TPPs) provide a mathematical framework for modeling these sequences, enabling inferences such as predicting the arrival time of future events and their associated label, called mark. However, due to model misspecification or lack of training data, these probabilistic models may provide a poor approximation of the true, unknown underlying process, with prediction regions extracted from them being unreliable estimates of the underlying uncertainty. This paper develops more reliable methods for uncertainty quantification in neural TPP models via the framework of conformal prediction. A primary objective is to generate a distribution-free joint prediction region for the arrival time and mark, with a finite-sample marginal coverage guarantee. A key challenge is to handle both a strictly positive, continuous response and a categorical response, withou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#20998;&#24067;&#23545;&#40784;&#26041;&#27861;&#20197;&#35299;&#20915;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#20110;&#24357;&#25955;&#27169;&#22411;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#24310;&#36831;&#24212;&#29992;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#28508;&#21147;&#65292;&#24182;&#19988;&#33021;&#26377;&#25928;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04585</link><description>&lt;p&gt;
&#25193;&#23637;&#20998;&#24067;&#23545;&#40784;&#26469;&#23454;&#29616;&#24357;&#25955;&#27169;&#22411;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models. (arXiv:2401.04585v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#20998;&#24067;&#23545;&#40784;&#26041;&#27861;&#20197;&#35299;&#20915;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#20110;&#24357;&#25955;&#27169;&#22411;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#24310;&#36831;&#24212;&#29992;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#28508;&#21147;&#65292;&#24182;&#19988;&#33021;&#26377;&#25928;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#22122;&#22768;&#20272;&#35745;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#32321;&#37325;&#30340;&#21435;&#22122;&#36807;&#31243;&#21644;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#20302;&#24310;&#36831;&#24212;&#29992;&#12290;&#37327;&#21270;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24230;&#65292;&#32780;&#21518;&#35757;&#32451;&#37327;&#21270;(PTQ)&#22312;&#21152;&#36895;&#21435;&#22122;&#36807;&#31243;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#28508;&#21147;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#24494;&#35843;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#30001;&#20110;&#19981;&#21516;&#21435;&#22122;&#27493;&#39588;&#20013;&#28608;&#27963;&#30340;&#39640;&#24230;&#21160;&#24577;&#20998;&#24067;&#65292;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;PTQ&#26041;&#27861;&#22312;&#26657;&#20934;&#26679;&#26412;&#21644;&#37325;&#26500;&#36755;&#20986;&#20004;&#20010;&#23618;&#38754;&#19978;&#37117;&#23384;&#22312;&#20998;&#24067;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#24615;&#33021;&#36828;&#20302;&#20110;&#20196;&#20154;&#28385;&#24847;&#30340;&#27700;&#24179;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#20301;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#30340;&#20998;&#24067;&#23545;&#40784;&#29992;&#20110;&#24357;&#25955;&#27169;&#22411;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;(EDA-DM)&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#26657;&#20934;&#26679;&#26412;&#23618;&#38754;&#65292;&#25105;&#20204;&#22522;&#20110;...[&#32570;&#30465;]
&lt;/p&gt;
&lt;p&gt;
Diffusion models have achieved great success in image generation tasks through iterative noise estimation. However, the heavy denoising process and complex neural networks hinder their low-latency applications in real-world scenarios. Quantization can effectively reduce model complexity, and post-training quantization (PTQ), which does not require fine-tuning, is highly promising in accelerating the denoising process. Unfortunately, we find that due to the highly dynamic distribution of activations in different denoising steps, existing PTQ methods for diffusion models suffer from distribution mismatch issues at both calibration sample level and reconstruction output level, which makes the performance far from satisfactory, especially in low-bit cases. In this paper, we propose Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models (EDA-DM) to address the above issues. Specifically, at the calibration sample level, we select calibration samples based on the 
&lt;/p&gt;</description></item><item><title>MAGNeT&#26159;&#19968;&#31181;&#36974;&#34109;&#29983;&#25104;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#19968;&#38750;&#33258;&#22238;&#24402;Transformer&#29983;&#25104;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37325;&#26032;&#35780;&#20998;&#26041;&#27861;&#26469;&#25552;&#39640;&#29983;&#25104;&#38899;&#39057;&#30340;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;MAGNeT&#36824;&#25506;&#32034;&#20102;&#28151;&#21512;&#29256;&#26412;&#65292;&#21487;&#22312;&#33258;&#22238;&#24402;&#27169;&#24335;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#24335;&#19979;&#29983;&#25104;&#24207;&#21015;&#12290;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;MAGNeT&#22312;&#25991;&#26412;&#21040;&#38899;&#20048;&#21644;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#20219;&#21153;&#20013;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04577</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#19968;&#38750;&#33258;&#22238;&#24402;Transformer&#29983;&#25104;&#36974;&#34109;&#38899;&#39057;
&lt;/p&gt;
&lt;p&gt;
Masked Audio Generation using a Single Non-Autoregressive Transformer. (arXiv:2401.04577v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04577
&lt;/p&gt;
&lt;p&gt;
MAGNeT&#26159;&#19968;&#31181;&#36974;&#34109;&#29983;&#25104;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#19968;&#38750;&#33258;&#22238;&#24402;Transformer&#29983;&#25104;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37325;&#26032;&#35780;&#20998;&#26041;&#27861;&#26469;&#25552;&#39640;&#29983;&#25104;&#38899;&#39057;&#30340;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;MAGNeT&#36824;&#25506;&#32034;&#20102;&#28151;&#21512;&#29256;&#26412;&#65292;&#21487;&#22312;&#33258;&#22238;&#24402;&#27169;&#24335;&#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#24335;&#19979;&#29983;&#25104;&#24207;&#21015;&#12290;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;MAGNeT&#22312;&#25991;&#26412;&#21040;&#38899;&#20048;&#21644;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#20219;&#21153;&#20013;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MAGNeT&#30340;&#36974;&#34109;&#29983;&#25104;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#30452;&#25509;&#25805;&#20316;&#22810;&#20010;&#38899;&#39057;&#20196;&#29260;&#27969;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;MAGNeT&#30001;&#21333;&#38454;&#27573;&#38750;&#33258;&#22238;&#24402;Transformer&#32452;&#25104;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#36974;&#34109;&#35745;&#21010;&#22120;&#39044;&#27979;&#36974;&#34109;&#20196;&#29260;&#30340;&#33539;&#22260;&#65292;&#32780;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36880;&#27493;&#26500;&#24314;&#36755;&#20986;&#24207;&#21015;&#20351;&#29992;&#22810;&#20010;&#35299;&#30721;&#27493;&#39588;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#29983;&#25104;&#38899;&#39057;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37325;&#26032;&#35780;&#20998;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#21033;&#29992;&#22806;&#37096;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#37325;&#26032;&#35780;&#20998;&#21644;&#25490;&#21517;MAGNeT&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#36825;&#20123;&#32467;&#26524;&#23558;&#34987;&#29992;&#20110;&#21518;&#32493;&#30340;&#35299;&#30721;&#27493;&#39588;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;MAGNeT&#30340;&#28151;&#21512;&#29256;&#26412;&#65292;&#20854;&#20013;&#25105;&#20204;&#22312;&#33258;&#22238;&#24402;&#27169;&#24335;&#19979;&#29983;&#25104;&#21069;&#20960;&#31186;&#38047;&#65292;&#32780;&#20854;&#20313;&#30340;&#24207;&#21015;&#21017;&#20197;&#24182;&#34892;&#26041;&#24335;&#36827;&#34892;&#35299;&#30721;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MAGNeT&#22312;&#25991;&#26412;&#21040;&#38899;&#20048;&#21644;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MAGNeT, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, MAGNeT is comprised of a single-stage, non-autoregressive transformer. During training, we predict spans of masked tokens obtained from a masking scheduler, while during inference we gradually construct the output sequence using several decoding steps. To further enhance the quality of the generated audio, we introduce a novel rescoring method in which, we leverage an external pre-trained model to rescore and rank predictions from MAGNeT, which will be then used for later decoding steps. Lastly, we explore a hybrid version of MAGNeT, in which we fuse between autoregressive and non-autoregressive models to generate the first few seconds in an autoregressive manner while the rest of the sequence is being decoded in parallel. We demonstrate the efficiency of MAGNeT for the task of text-to-music and text-to-audio generation and conduct an extensi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#26550;&#26500;EVOLUTE&#65292;&#32467;&#21512;&#20102;&#34892;&#20026;&#20811;&#38534;&#21644;&#33021;&#37327;&#27169;&#22411;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#28216;&#25103;&#27979;&#35797;&#12290;&#35813;&#26550;&#26500;&#23558;&#33258;&#20027;&#20195;&#29702;&#30340;&#21160;&#20316;&#31354;&#38388;&#25286;&#20998;&#20026;&#36830;&#32493;&#21644;&#31163;&#25955;&#20219;&#21153;&#65292;&#20197;&#20248;&#21270;&#25511;&#21046;&#21644;&#35757;&#32451;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#23556;&#20987;&#21644;&#39550;&#39542;&#28216;&#25103;&#20013;&#65292;EVOLUTE&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04572</link><description>&lt;p&gt;
&#33258;&#21160;&#28216;&#25103;&#27979;&#35797;&#30340;&#31283;&#20581;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Imitation Learning for Automated Game Testing. (arXiv:2401.04572v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04572
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#26550;&#26500;EVOLUTE&#65292;&#32467;&#21512;&#20102;&#34892;&#20026;&#20811;&#38534;&#21644;&#33021;&#37327;&#27169;&#22411;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#28216;&#25103;&#27979;&#35797;&#12290;&#35813;&#26550;&#26500;&#23558;&#33258;&#20027;&#20195;&#29702;&#30340;&#21160;&#20316;&#31354;&#38388;&#25286;&#20998;&#20026;&#36830;&#32493;&#21644;&#31163;&#25955;&#20219;&#21153;&#65292;&#20197;&#20248;&#21270;&#25511;&#21046;&#21644;&#35757;&#32451;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#23556;&#20987;&#21644;&#39550;&#39542;&#28216;&#25103;&#20013;&#65292;EVOLUTE&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28216;&#25103;&#24320;&#21457;&#26159;&#19968;&#20010;&#28459;&#38271;&#30340;&#36807;&#31243;&#65292;&#22312;&#20135;&#21697;&#19978;&#24066;&#20043;&#21069;&#38656;&#35201;&#32463;&#36807;&#35768;&#22810;&#38454;&#27573;&#12290;&#20154;&#24037;&#28216;&#25103;&#27979;&#35797;&#26159;&#20854;&#20013;&#26368;&#32791;&#26102;&#30340;&#65292;&#22240;&#20026;&#27979;&#35797;&#20154;&#21592;&#38656;&#35201;&#21453;&#22797;&#25191;&#34892;&#20219;&#21153;&#20197;&#23547;&#25214;&#20195;&#30721;&#20013;&#30340;&#38169;&#35823;&#12290;&#22240;&#27492;&#65292;&#33258;&#21160;&#21270;&#27979;&#35797;&#34987;&#35270;&#20026;&#28216;&#25103;&#34892;&#19994;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#23427;&#23558;&#26174;&#33879;&#25552;&#39640;&#24320;&#21457;&#25104;&#26412;&#21644;&#25928;&#29575;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EVOLUTE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#26550;&#26500;&#65292;&#23427;&#23558;&#34892;&#20026;&#20811;&#38534;&#65288;BC&#65289;&#19982;&#33021;&#37327;&#27169;&#22411;&#65288;EBM&#65289;&#30456;&#32467;&#21512;&#12290;EVOLUTE&#26159;&#19968;&#20010;&#20004;&#27969;&#27169;&#22411;&#65292;&#23558;&#33258;&#20027;&#20195;&#29702;&#30340;&#21160;&#20316;&#31354;&#38388;&#20998;&#20026;&#36830;&#32493;&#21644;&#31163;&#25955;&#20219;&#21153;&#12290;EBM&#27969;&#22788;&#29702;&#36830;&#32493;&#20219;&#21153;&#65292;&#20197;&#33719;&#24471;&#26356;&#31934;&#32454;&#21644;&#33258;&#36866;&#24212;&#30340;&#25511;&#21046;&#65292;&#32780;BC&#27969;&#22788;&#29702;&#31163;&#25955;&#21160;&#20316;&#65292;&#20197;&#26041;&#20415;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#23556;&#20987;&#21644;&#39550;&#39542;&#28216;&#25103;&#20013;&#35780;&#20272;&#20102;EVOLUTE&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#20195;&#29702;&#38656;&#35201;&#23548;&#33322;&#24182;&#25345;&#32493;&#35782;&#21035;&#30446;&#26631;&#36827;&#34892;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Game development is a long process that involves many stages before a product is ready for the market. Human play testing is among the most time consuming, as testers are required to repeatedly perform tasks in the search for errors in the code. Therefore, automated testing is seen as a key technology for the gaming industry, as it would dramatically improve development costs and efficiency. Toward this end, we propose EVOLUTE, a novel imitation learning-based architecture that combines behavioural cloning (BC) with energy based models (EBMs). EVOLUTE is a two-stream ensemble model that splits the action space of autonomous agents into continuous and discrete tasks. The EBM stream handles the continuous tasks, to have a more refined and adaptive control, while the BC stream handles discrete actions, to ease training. We evaluate the performance of EVOLUTE in a shooting-and-driving game, where the agent is required to navigate and continuously identify targets to attack. The proposed mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;HyperGANStrument&#26041;&#27861;&#65292;&#20351;&#29992;&#26080;&#20851;&#38899;&#39640;&#30340;&#36229;&#32593;&#32476;&#26469;&#35843;&#21046;&#39044;&#35757;&#32451;&#30340;GANStrument&#29983;&#25104;&#22120;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#37325;&#26500;&#33021;&#21147;&#21644;&#38899;&#39640;&#20934;&#30830;&#24615;&#65292;&#22686;&#24378;&#20102;&#21512;&#25104;&#22768;&#38899;&#30340;&#21487;&#32534;&#36753;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04558</link><description>&lt;p&gt;
HyperGANStrument: &#20351;&#29992;&#26080;&#20851;&#38899;&#39640;&#36229;&#32593;&#32476;&#30340;&#20048;&#22120;&#38899;&#22768;&#21512;&#25104;&#21644;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
HyperGANStrument: Instrument Sound Synthesis and Editing with Pitch-Invariant Hypernetworks. (arXiv:2401.04558v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;HyperGANStrument&#26041;&#27861;&#65292;&#20351;&#29992;&#26080;&#20851;&#38899;&#39640;&#30340;&#36229;&#32593;&#32476;&#26469;&#35843;&#21046;&#39044;&#35757;&#32451;&#30340;GANStrument&#29983;&#25104;&#22120;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#37325;&#26500;&#33021;&#21147;&#21644;&#38899;&#39640;&#20934;&#30830;&#24615;&#65292;&#22686;&#24378;&#20102;&#21512;&#25104;&#22768;&#38899;&#30340;&#21487;&#32534;&#36753;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GANStrument&#21033;&#29992;&#20855;&#26377;&#26080;&#20851;&#38899;&#39640;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#23454;&#20363;&#26465;&#20214;&#25216;&#26415;&#30340;GAN&#65292;&#22312;&#21512;&#25104;&#36924;&#30495;&#30340;&#20048;&#22120;&#22768;&#38899;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26174;&#30528;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#37325;&#26500;&#33021;&#21147;&#21644;&#38899;&#39640;&#20934;&#30830;&#24615;&#20197;&#22686;&#24378;&#29992;&#25143;&#25552;&#20379;&#22768;&#38899;&#30340;&#21487;&#32534;&#36753;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HyperGANStrument&#65292;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#26080;&#20851;&#38899;&#39640;&#30340;&#36229;&#32593;&#32476;&#26469;&#35843;&#21046;&#39044;&#35757;&#32451;&#30340;GANStrument&#29983;&#25104;&#22120;&#30340;&#26435;&#37325;&#65292;&#32473;&#23450;&#19968;&#27425;&#24615;&#22768;&#38899;&#20316;&#20026;&#36755;&#20837;&#12290;&#36229;&#32593;&#32476;&#35843;&#21046;&#20026;&#29983;&#25104;&#22120;&#22312;&#37325;&#26500;&#36755;&#20837;&#22768;&#38899;&#26041;&#38754;&#25552;&#20379;&#20102;&#21453;&#39304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#23545;&#25239;&#24494;&#35843;&#26041;&#26696;&#23545;&#36229;&#32593;&#32476;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#22120;&#30340;&#37325;&#26500;&#20445;&#30495;&#24230;&#21644;&#29983;&#25104;&#22810;&#26679;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#19981;&#20165;&#22686;&#24378;&#20102;GANStrument&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#36824;&#26174;&#33879;&#25552;&#39640;&#20102;&#21512;&#25104;&#22768;&#38899;&#30340;&#21487;&#32534;&#36753;&#24615;&#12290;&#22312;&#32447;&#28436;&#31034;&#39029;&#38754;&#25552;&#20379;&#38899;&#39057;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
GANStrument, exploiting GANs with a pitch-invariant feature extractor and instance conditioning technique, has shown remarkable capabilities in synthesizing realistic instrument sounds. To further improve the reconstruction ability and pitch accuracy to enhance the editability of user-provided sound, we propose HyperGANStrument, which introduces a pitch-invariant hypernetwork to modulate the weights of a pre-trained GANStrument generator, given a one-shot sound as input. The hypernetwork modulation provides feedback for the generator in the reconstruction of the input sound. In addition, we take advantage of an adversarial fine-tuning scheme for the hypernetwork to improve the reconstruction fidelity and generation diversity of the generator. Experimental results show that the proposed model not only enhances the generation capability of GANStrument but also significantly improves the editability of synthesized sounds. Audio examples are available at the online demo page.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35848;&#21028;&#28216;&#25103;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20849;&#21516;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#24615;&#33021;&#21644;&#23545;&#40784;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#26465;&#20214;&#65292;&#24182;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#36890;&#36807;&#35780;&#20272;&#22810;&#36718;&#27425;&#21644;&#36328;&#27169;&#22411;&#20132;&#20114;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;LM&#30340;&#33258;&#25105;&#23545;&#24328;&#21644;&#20132;&#21449;&#23545;&#24328;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04536</link><description>&lt;p&gt;
&#36890;&#36807;&#35848;&#21028;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Language Model Agency through Negotiations. (arXiv:2401.04536v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35848;&#21028;&#28216;&#25103;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20849;&#21516;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#24615;&#33021;&#21644;&#23545;&#40784;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#26465;&#20214;&#65292;&#24182;&#36991;&#20813;&#25968;&#25454;&#27844;&#28431;&#12290;&#36890;&#36807;&#35780;&#20272;&#22810;&#36718;&#27425;&#21644;&#36328;&#27169;&#22411;&#20132;&#20114;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;LM&#30340;&#33258;&#25105;&#23545;&#24328;&#21644;&#20132;&#21449;&#23545;&#24328;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#21496;&#12289;&#32452;&#32455;&#21644;&#25919;&#24220;&#36234;&#26469;&#36234;&#22810;&#22320;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#23637;&#31034;&#31867;&#20284;&#20195;&#29702;&#34892;&#20026;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#38543;&#30528;LM&#34987;&#37319;&#29992;&#26469;&#25191;&#34892;&#36234;&#26469;&#36234;&#20855;&#26377;&#33258;&#20027;&#24615;&#30340;&#20219;&#21153;&#65292;&#36843;&#20999;&#38656;&#35201;&#21487;&#38752;&#19988;&#21487;&#25193;&#23637;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;&#24403;&#21069;&#20027;&#35201;&#26159;&#38745;&#24577;&#30340;LM&#22522;&#20934;&#26080;&#27861;&#24456;&#22909;&#22320;&#35780;&#20272;&#27492;&#31867;&#21160;&#24577;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#35758;&#36890;&#36807;&#35848;&#21028;&#28216;&#25103;&#30340;&#35270;&#35282;&#26469;&#20849;&#21516;&#35780;&#20272;LM&#30340;&#24615;&#33021;&#21644;&#23545;&#40784;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20010;&#20849;&#21516;&#20219;&#21153;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#37096;&#32626;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;LM&#20915;&#31574;&#36807;&#31243;&#30340;&#35265;&#35299;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#35848;&#21028;&#28216;&#25103;&#20351;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#22810;&#36718;&#27425;&#21644;&#36328;&#27169;&#22411;&#20132;&#20114;&#65292;&#35843;&#25972;&#22797;&#26434;&#24615;&#65292;&#24182;&#36991;&#20813;&#35780;&#20272;&#20013;&#30340;&#24847;&#22806;&#25968;&#25454;&#27844;&#28431;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#26469;&#33258;&#20960;&#20010;&#20027;&#35201;&#20379;&#24212;&#21830;&#30340;&#20845;&#20010;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;LM&#22312;&#21508;&#31181;&#35848;&#21028;&#28216;&#25103;&#19978;&#30340;&#32467;&#26524;&#65292;&#35780;&#20272;&#20102;&#33258;&#25105;&#23545;&#24328;&#21644;&#20132;&#21449;&#23545;&#24328;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#21457;&#29616;&#21253;&#25324;&#65306;&#65288;i&#65289;&#24320;&#28304;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Companies, organizations, and governments increasingly exploit Language Models' (LM) remarkable capability to display agent-like behavior. As LMs are adopted to perform tasks with growing autonomy, there exists an urgent need for reliable and scalable evaluation benchmarks. Current, predominantly static LM benchmarks are ill-suited to evaluate such dynamic applications. Thus, we propose jointly evaluating LM performance and alignment through the lenses of negotiation games. We argue that this common task better reflects real-world deployment conditions while offering insights into LMs' decision-making processes. Crucially, negotiation games allow us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental data leakage in evaluation. We report results for six publicly accessible LMs from several major providers on a variety of negotiation games, evaluating both self-play and cross-play performance. Noteworthy findings include: (i) open-source mode
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#28145;&#24230;Sobolev&#22238;&#24402;&#22120;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26799;&#24230;&#33539;&#25968;&#27491;&#21017;&#21270;&#65292;&#21487;&#20197;&#21516;&#26102;&#20272;&#35745;&#22238;&#24402;&#20989;&#25968;&#21644;&#20854;&#26799;&#24230;&#65292;&#21363;&#20351;&#23384;&#22312;&#26174;&#33879;&#39046;&#22495;&#21464;&#21270;&#12290;&#36825;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#21487;&#35777;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.04535</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#28145;&#24230;Sobolev&#22238;&#24402;: &#20272;&#35745;&#12289;&#21464;&#37327;&#36873;&#25321;&#21450;&#20854;&#20182;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Deep Sobolev Regression: Estimation, Variable Selection and Beyond. (arXiv:2401.04535v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04535
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#28145;&#24230;Sobolev&#22238;&#24402;&#22120;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26799;&#24230;&#33539;&#25968;&#27491;&#21017;&#21270;&#65292;&#21487;&#20197;&#21516;&#26102;&#20272;&#35745;&#22238;&#24402;&#20989;&#25968;&#21644;&#20854;&#26799;&#24230;&#65292;&#21363;&#20351;&#23384;&#22312;&#26174;&#33879;&#39046;&#22495;&#21464;&#21270;&#12290;&#36825;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#21487;&#35777;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SDORE&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#28145;&#24230;Sobolev&#22238;&#24402;&#22120;&#65292;&#29992;&#20110;&#38750;&#21442;&#25968;&#20272;&#35745;&#28508;&#22312;&#30340;&#22238;&#24402;&#20989;&#25968;&#21450;&#20854;&#26799;&#24230;&#12290;SDORE&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#65292;&#24182;&#37319;&#29992;&#26799;&#24230;&#33539;&#25968;&#27491;&#21017;&#21270;&#65292;&#20801;&#35768;&#23545;&#26080;&#26631;&#31614;&#25968;&#25454;&#35745;&#31639;&#26799;&#24230;&#33539;&#25968;&#12290;&#25105;&#20204;&#23545;SDORE&#30340;&#25910;&#25947;&#36895;&#24230;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#24314;&#31435;&#20102;&#22238;&#24402;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#26368;&#20248;&#36895;&#29575;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#23384;&#22312;&#26174;&#33879;&#39046;&#22495;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#20102;&#20851;&#32852;&#30340;&#25554;&#20540;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#36825;&#20123;&#29702;&#35770;&#32467;&#26524;&#20026;&#36873;&#25321;&#27491;&#21017;&#21270;&#21442;&#25968;&#21644;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#23567;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#20808;&#39564;&#25351;&#23548;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#21487;&#35777;&#20248;&#21183;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SDORE&#26159;&#31532;&#19968;&#20010;&#21516;&#26102;&#20272;&#35745;&#22238;&#24402;&#20989;&#25968;&#21450;&#20854;&#26799;&#24230;&#30340;&#21487;&#35777;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose SDORE, a semi-supervised deep Sobolev regressor, for the nonparametric estimation of the underlying regression function and its gradient. SDORE employs deep neural networks to minimize empirical risk with gradient norm regularization, allowing computation of the gradient norm on unlabeled data. We conduct a comprehensive analysis of the convergence rates of SDORE and establish a minimax optimal rate for the regression function. Crucially, we also derive a convergence rate for the associated plug-in gradient estimator, even in the presence of significant domain shift. These theoretical findings offer valuable prior guidance for selecting regularization parameters and determining the size of the neural network, while showcasing the provable advantage of leveraging unlabeled data in semi-supervised learning. To the best of our knowledge, SDORE is the first provable neural network-based approach that simultaneously estimates the regression function and its gradient, with diverse
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#25193;&#23637;&#30340;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;&#65288;GAR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20195;&#30721;&#36827;&#34892;&#37325;&#20889;&#26469;&#35299;&#20915;&#20195;&#30721;&#25628;&#32034;&#20013;&#23384;&#22312;&#30340;&#39118;&#26684;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04514</link><description>&lt;p&gt;
&#37325;&#20889;&#20195;&#30721;&#65306;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#20195;&#30721;&#25628;&#32034;&#30340;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search. (arXiv:2401.04514v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#25193;&#23637;&#30340;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;&#65288;GAR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20195;&#30721;&#36827;&#34892;&#37325;&#20889;&#26469;&#35299;&#20915;&#20195;&#30721;&#25628;&#32034;&#20013;&#23384;&#22312;&#30340;&#39118;&#26684;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20195;&#30721;&#25628;&#32034;&#20013;&#65292;&#29983;&#25104;&#22686;&#24378;&#26816;&#32034;&#65288;GAR&#65289;&#26694;&#26550;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#29983;&#25104;&#31034;&#20363;&#20195;&#30721;&#29255;&#27573;&#26469;&#22686;&#24378;&#26597;&#35810;&#65292;&#20197;&#35299;&#20915;&#20195;&#30721;&#29255;&#27573;&#21644;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#20043;&#38388;&#30340;&#20027;&#35201;&#27169;&#24577;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#35843;&#26597;&#21457;&#29616;&#65292;LLM&#22686;&#24378;&#26694;&#26550;&#25152;&#25552;&#20379;&#30340;&#25913;&#36827;&#26377;&#19968;&#23450;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#38480;&#21046;&#21487;&#33021;&#26159;&#22240;&#20026;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#23613;&#31649;&#22312;&#21151;&#33021;&#19978;&#20934;&#30830;&#65292;&#20294;&#22312;&#20195;&#30721;&#24211;&#20013;&#19982;&#22522;&#20934;&#20195;&#30721;&#20043;&#38388;&#32463;&#24120;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#39118;&#26684;&#20559;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#22522;&#30784;GAR&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20195;&#30721;&#24211;&#20013;&#30340;&#20195;&#30721;&#36827;&#34892;&#37325;&#20889;&#65288;ReCo&#65289;&#26469;&#36827;&#34892;&#39118;&#26684;&#35268;&#33539;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ReCo&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally Rewrites the Code (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZEST&#30340;&#38646;&#26679;&#26412;&#24773;&#24863;&#39118;&#26684;&#36716;&#31227;&#26041;&#27861;&#65292;&#20801;&#35768;&#23558;&#32473;&#23450;&#28304;&#38899;&#39057;&#20013;&#30340;&#24773;&#24863;&#20869;&#23481;&#19982;&#30446;&#26631;&#38899;&#39057;&#20013;&#30340;&#24773;&#24863;&#20869;&#23481;&#36827;&#34892;&#36716;&#31227;&#65292;&#21516;&#26102;&#20445;&#30041;&#28304;&#38899;&#39057;&#30340;&#35828;&#35805;&#20154;&#21644;&#35821;&#38899;&#20869;&#23481;&#12290;&#36890;&#36807;&#20998;&#35299;&#35821;&#38899;&#20026;&#35821;&#20041;&#26631;&#35760;&#12289;&#35828;&#35805;&#20154;&#34920;&#31034;&#21644;&#24773;&#24863;&#23884;&#20837;&#65292;&#24182;&#21033;&#29992;&#37325;&#26500;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36716;&#25442;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2401.04511</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#38899;&#39057;&#21040;&#38899;&#39057;&#24773;&#24863;&#36716;&#31227;&#19982;&#35828;&#35805;&#20154;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Zero Shot Audio to Audio Emotion Transfer With Speaker Disentanglement. (arXiv:2401.04511v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZEST&#30340;&#38646;&#26679;&#26412;&#24773;&#24863;&#39118;&#26684;&#36716;&#31227;&#26041;&#27861;&#65292;&#20801;&#35768;&#23558;&#32473;&#23450;&#28304;&#38899;&#39057;&#20013;&#30340;&#24773;&#24863;&#20869;&#23481;&#19982;&#30446;&#26631;&#38899;&#39057;&#20013;&#30340;&#24773;&#24863;&#20869;&#23481;&#36827;&#34892;&#36716;&#31227;&#65292;&#21516;&#26102;&#20445;&#30041;&#28304;&#38899;&#39057;&#30340;&#35828;&#35805;&#20154;&#21644;&#35821;&#38899;&#20869;&#23481;&#12290;&#36890;&#36807;&#20998;&#35299;&#35821;&#38899;&#20026;&#35821;&#20041;&#26631;&#35760;&#12289;&#35828;&#35805;&#20154;&#34920;&#31034;&#21644;&#24773;&#24863;&#23884;&#20837;&#65292;&#24182;&#21033;&#29992;&#37325;&#26500;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36716;&#25442;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#21040;&#38899;&#39057;&#30340;&#39118;&#26684;&#36716;&#31227;&#38382;&#39064;&#28041;&#21450;&#23558;&#28304;&#38899;&#39057;&#30340;&#39118;&#26684;&#29305;&#24449;&#26367;&#25442;&#20026;&#30446;&#26631;&#38899;&#39057;&#30340;&#39118;&#26684;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;&#28304;&#38899;&#39057;&#30340;&#19982;&#20869;&#23481;&#30456;&#20851;&#30340;&#23646;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#38646;&#26679;&#26412;&#24773;&#24863;&#39118;&#26684;&#36716;&#31227;&#65288;ZEST&#65289;&#65292;&#23427;&#20801;&#35768;&#23558;&#32473;&#23450;&#28304;&#38899;&#39057;&#20013;&#30340;&#24773;&#24863;&#20869;&#23481;&#19982;&#23884;&#20837;&#22312;&#30446;&#26631;&#38899;&#39057;&#20013;&#30340;&#24773;&#24863;&#20869;&#23481;&#36827;&#34892;&#36716;&#31227;&#65292;&#21516;&#26102;&#20445;&#30041;&#28304;&#38899;&#39057;&#30340;&#35828;&#35805;&#20154;&#21644;&#35821;&#38899;&#20869;&#23481;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#22522;&#20110;&#23558;&#35821;&#38899;&#20998;&#35299;&#20026;&#35821;&#20041;&#26631;&#35760;&#12289;&#35828;&#35805;&#20154;&#34920;&#31034;&#21644;&#24773;&#24863;&#23884;&#20837;&#12290;&#21033;&#29992;&#36825;&#20123;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#37325;&#24314;&#32473;&#23450;&#35821;&#38899;&#20449;&#21495;&#30340;&#38899;&#39640;&#36718;&#24275;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#35299;&#30721;&#22120;&#26469;&#37325;&#24314;&#35821;&#38899;&#20449;&#21495;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#22522;&#20110;&#33258;&#30417;&#30563;&#30340;&#37325;&#26500;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#36716;&#25442;&#36807;&#31243;&#20013;&#65292;&#24773;&#24863;&#23884;&#20837;&#20165;&#20174;&#30446;&#26631;&#38899;&#39057;&#20013;&#24471;&#20986;&#65292;&#32780;&#20854;&#20182;&#22240;&#32032;&#22343;&#26469;&#33258;&#28304;&#38899;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of audio-to-audio (A2A) style transfer involves replacing the style features of the source audio with those from the target audio while preserving the content related attributes of the source audio. In this paper, we propose an efficient approach, termed as Zero-shot Emotion Style Transfer (ZEST), that allows the transfer of emotional content present in the given source audio with the one embedded in the target audio while retaining the speaker and speech content from the source. The proposed system builds upon decomposing speech into semantic tokens, speaker representations and emotion embeddings. Using these factors, we propose a framework to reconstruct the pitch contour of the given speech signal and train a decoder that reconstructs the speech signal. The model is trained using a self-supervision based reconstruction loss. During conversion, the emotion embedding is alone derived from the target audio, while rest of the factors are derived from the source audio. In our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;Koopman&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#38750;&#32447;&#24615;&#27169;&#22411;&#31616;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#24310;&#36831;&#22352;&#26631;&#32534;&#30721;&#21644;&#20840;&#29366;&#24577;&#35299;&#30721;&#30340;&#27169;&#22411;&#32467;&#26500;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#39640;&#32431;&#24230;&#20302;&#28201;&#31934;&#39311;&#22612;&#30340;&#23454;&#26102;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.04508</link><description>&lt;p&gt;
&#22522;&#20110;Koopman&#29702;&#35770;&#30340;&#25968;&#25454;&#39537;&#21160;&#38750;&#32447;&#24615;&#27169;&#22411;&#31616;&#21270;&#65306;&#32508;&#21512;&#25511;&#21046;&#24418;&#24335;&#21644;NMPC&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data-driven Nonlinear Model Reduction using Koopman Theory: Integrated Control Form and NMPC Case Study. (arXiv:2401.04508v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;Koopman&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#38750;&#32447;&#24615;&#27169;&#22411;&#31616;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#24310;&#36831;&#22352;&#26631;&#32534;&#30721;&#21644;&#20840;&#29366;&#24577;&#35299;&#30721;&#30340;&#27169;&#22411;&#32467;&#26500;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#39640;&#32431;&#24230;&#20302;&#28201;&#31934;&#39311;&#22612;&#30340;&#23454;&#26102;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;Koopman&#29702;&#35770;&#23545;&#24102;&#26377;&#25511;&#21046;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#31616;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#24310;&#36831;&#22352;&#26631;&#32534;&#30721;&#27979;&#37327;&#21644;&#20840;&#29366;&#24577;&#35299;&#30721;&#30456;&#32467;&#21512;&#30340;&#36890;&#29992;&#27169;&#22411;&#32467;&#26500;&#65292;&#20197;&#38598;&#25104;&#31616;&#21270;&#30340;Koopman&#24314;&#27169;&#21644;&#29366;&#24577;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#35757;&#32451;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#12290;&#26696;&#20363;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#31934;&#30830;&#30340;&#25511;&#21046;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#32431;&#24230;&#20302;&#28201;&#31934;&#39311;&#22612;&#30340;&#23454;&#26102;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use Koopman theory for data-driven model reduction of nonlinear dynamical systems with controls. We propose generic model structures combining delay-coordinate encoding of measurements and full-state decoding to integrate reduced Koopman modeling and state estimation. We present a deep-learning approach to train the proposed models. A case study demonstrates that our approach provides accurate control models and enables real-time capable nonlinear model predictive control of a high-purity cryogenic distillation column.
&lt;/p&gt;</description></item><item><title>SpiNNaker2&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#22522;&#20110;&#20107;&#20214;&#21644;&#24322;&#27493;&#26426;&#22120;&#23398;&#20064;&#30340;&#31070;&#32463;&#24418;&#24577;&#31995;&#32479;&#33455;&#29255;&#65292;&#33021;&#22815;&#36890;&#36807;&#25972;&#21512;&#35745;&#31639;&#21407;&#29702;&#26469;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#33021;&#28304;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2401.04491</link><description>&lt;p&gt;
SpiNNaker2: &#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#20107;&#20214;&#21644;&#24322;&#27493;&#26426;&#22120;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#31070;&#32463;&#24418;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SpiNNaker2: A Large-Scale Neuromorphic System for Event-Based and Asynchronous Machine Learning. (arXiv:2401.04491v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04491
&lt;/p&gt;
&lt;p&gt;
SpiNNaker2&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#22522;&#20110;&#20107;&#20214;&#21644;&#24322;&#27493;&#26426;&#22120;&#23398;&#20064;&#30340;&#31070;&#32463;&#24418;&#24577;&#31995;&#32479;&#33455;&#29255;&#65292;&#33021;&#22815;&#36890;&#36807;&#25972;&#21512;&#35745;&#31639;&#21407;&#29702;&#26469;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#33021;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#29305;&#23450;&#39046;&#22495;&#30828;&#20214;&#21152;&#36895;&#22120;&#65288;&#22914;GPU&#21644;TPU&#65289;&#30340;&#20849;&#21516;&#36827;&#27493;&#24050;&#32463;&#22312;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#26356;&#22823;&#27169;&#22411;&#21644;&#26356;&#22810;&#25968;&#25454;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#65292;&#36825;&#31181;&#21457;&#23637;&#20276;&#38543;&#30528;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#24555;&#36895;&#22686;&#38271;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22522;&#30784;&#27169;&#22411;&#30340;&#26032;&#29305;&#24615;&#65288;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#20026;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24212;&#29992;&#30340;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#20102;&#25968;&#25454;&#20013;&#24515;&#20197;&#21450;&#31227;&#21160;&#35774;&#22791;&#21644;&#36793;&#32536;&#31995;&#32479;&#30340;&#25216;&#26415;&#24212;&#29992;&#12290;&#20026;&#20102;&#20943;&#23569;&#33021;&#28304;&#28040;&#32791;&#21644;&#26174;&#33879;&#30340;&#24310;&#36831;&#65292;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#31995;&#32479;&#36890;&#36807;&#21033;&#29992;&#20302;&#21151;&#32791;&#27169;&#25311;&#21644;&#25968;&#23383;&#25216;&#26415;&#28145;&#24230;&#25972;&#21512;&#31070;&#32463;&#29983;&#29289;&#23398;&#31995;&#32479;&#30340;&#35745;&#31639;&#21407;&#29702;&#12290;SpiNNaker2&#26159;&#19968;&#31181;&#29992;&#20110;&#21487;&#25193;&#23637;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#23383;&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;&#12290;SpiNNaker2&#30340;&#22522;&#20110;&#20107;&#20214;&#21644;&#24322;&#27493;&#30340;&#35774;&#35745;&#20801;&#35768;&#28789;&#27963;&#30340;&#35745;&#31639;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The joint progress of artificial neural networks (ANNs) and domain specific hardware accelerators such as GPUs and TPUs took over many domains of machine learning research. This development is accompanied by a rapid growth of the required computational demands for larger models and more data. Concurrently, emerging properties of foundation models such as in-context learning drive new opportunities for machine learning applications. However, the computational cost of such applications is a limiting factor of the technology in data centers, and more importantly in mobile devices and edge systems. To mediate the energy footprint and non-trivial latency of contemporary systems, neuromorphic computing systems deeply integrate computational principles of neurobiological systems by leveraging low-power analog and digital technologies. SpiNNaker2 is a digital neuromorphic chip developed for scalable machine learning. The event-based and asynchronous design of SpiNNaker2 allows the composition 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#24577;&#35268;&#21010;&#30340;&#29983;&#23384;&#26641;&#26041;&#27861;&#65292;&#36890;&#36807;&#36882;&#24402;&#20998;&#21106;&#20154;&#21475;&#21644;&#39044;&#27979;&#19981;&#21516;&#30340;&#29983;&#23384;&#20998;&#24067;&#26469;&#21457;&#29616;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#21270;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#29305;&#27530;&#31639;&#27861;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#36816;&#34892;&#26102;&#38388;&#20248;&#20110;&#26576;&#20123;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04489</link><description>&lt;p&gt;
&#26368;&#20248;&#29983;&#23384;&#26641;: &#19968;&#31181;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Survival Trees: A Dynamic Programming Approach. (arXiv:2401.04489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#24577;&#35268;&#21010;&#30340;&#29983;&#23384;&#26641;&#26041;&#27861;&#65292;&#36890;&#36807;&#36882;&#24402;&#20998;&#21106;&#20154;&#21475;&#21644;&#39044;&#27979;&#19981;&#21516;&#30340;&#29983;&#23384;&#20998;&#24067;&#26469;&#21457;&#29616;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#21270;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#29305;&#27530;&#31639;&#27861;&#25552;&#39640;&#20102;&#21487;&#25193;&#23637;&#24615;&#65292;&#36816;&#34892;&#26102;&#38388;&#20248;&#20110;&#26576;&#20123;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#26469;&#30740;&#31350;&#21644;&#39044;&#27979;&#27515;&#20129;&#26102;&#38388;&#25110;&#20854;&#20182;&#19981;&#21487;&#37325;&#22797;&#20107;&#20214;&#30340;&#26102;&#38388;&#65292;&#32780;&#26576;&#20123;&#23454;&#20363;&#30340;&#30495;&#23454;&#27515;&#20129;&#26102;&#38388;&#26159;&#26410;&#30693;&#30340;&#12290;&#29983;&#23384;&#26641;&#36890;&#36807;&#36882;&#24402;&#22320;&#20998;&#21106;&#20154;&#21475;&#24182;&#22312;&#27599;&#20010;&#21494;&#33410;&#28857;&#39044;&#27979;&#19981;&#21516;&#30340;&#29983;&#23384;&#20998;&#24067;&#65292;&#33021;&#22815;&#21457;&#29616;&#32039;&#20945;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#27169;&#22411;&#20013;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#20248;&#21270;&#20445;&#35777;&#30340;&#29983;&#23384;&#26641;&#26041;&#27861;&#65292;&#33021;&#22815;&#35780;&#20272;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#20248;&#21270;&#38388;&#38553;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#29305;&#27530;&#30340;&#31639;&#27861;&#26469;&#35745;&#31639;&#28145;&#24230;&#20026;2&#30340;&#26641;&#65292;&#25552;&#39640;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#33719;&#21462;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#26679;&#26412;&#22806;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#26041;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#29978;&#33267;&#20248;&#20110;&#26576;&#20123;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival analysis studies and predicts the time of death, or other singular unrepeated events, based on historical data, while the true time of death for some instances is unknown. Survival trees enable the discovery of complex nonlinear relations in a compact human comprehensible model, by recursively splitting the population and predicting a distinct survival distribution in each leaf node. We use dynamic programming to provide the first survival tree method with optimality guarantees, enabling the assessment of the optimality gap of heuristics. We improve the scalability of our method through a special algorithm for computing trees up to depth two. The experiments show that our method's run time even outperforms some heuristics for realistic cases while obtaining similar out-of-sample performance with the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#35782;&#21035;&#26032;&#35789;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#35762;&#24231;&#24405;&#38899;&#36827;&#34892;&#25512;&#29702;&#21644;&#25910;&#38598;&#21253;&#21547;&#26032;&#35789;&#30340;&#35805;&#35821;&#65292;&#28982;&#21518;&#22312;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#26032;&#35789;&#20986;&#29616;&#39057;&#29575;&#36739;&#39640;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04482</link><description>&lt;p&gt;
&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#25345;&#32493;&#23398;&#20064;&#26032;&#35789;
&lt;/p&gt;
&lt;p&gt;
Continuously Learning New Words in Automatic Speech Recognition. (arXiv:2401.04482v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04482
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#35782;&#21035;&#26032;&#35789;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#35762;&#24231;&#24405;&#38899;&#36827;&#34892;&#25512;&#29702;&#21644;&#25910;&#38598;&#21253;&#21547;&#26032;&#35789;&#30340;&#35805;&#35821;&#65292;&#28982;&#21518;&#22312;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#26032;&#35789;&#20986;&#29616;&#39057;&#29575;&#36739;&#39640;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#20173;&#28982;&#36828;&#26410;&#23436;&#32654;&#12290;&#20856;&#22411;&#30340;&#38169;&#35823;&#21253;&#25324;&#32553;&#20889;&#35789;&#12289;&#21629;&#21517;&#23454;&#20307;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#19987;&#29992;&#35789;&#65292;&#36825;&#20123;&#35789;&#20960;&#20046;&#27809;&#26377;&#25110;&#27809;&#26377;&#25968;&#25454;&#21487;&#29992;&#26469;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#35782;&#21035;&#36825;&#20123;&#35789;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#12290;&#32473;&#23450;&#24102;&#26377;&#23545;&#24212;&#24187;&#28783;&#29255;&#30340;&#35762;&#24231;&#24405;&#38899;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#35760;&#24518;&#22686;&#24378;&#22411;ASR&#27169;&#22411;&#26469;&#23558;&#27169;&#22411;&#20559;&#21521;&#20110;&#20174;&#24187;&#28783;&#29255;&#20013;&#35299;&#30721;&#26032;&#35789;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#35762;&#24231;&#36827;&#34892;&#25512;&#29702;&#65292;&#23558;&#21253;&#21547;&#26816;&#27979;&#21040;&#30340;&#26032;&#35789;&#30340;&#35805;&#35821;&#25910;&#38598;&#21040;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#20013;&#12290;&#25509;&#30528;&#65292;&#23545;&#36825;&#20010;&#38598;&#21512;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#36890;&#36807;&#35843;&#25972;&#28155;&#21152;&#21040;&#27169;&#22411;&#30340;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#30340;&#20302;&#31209;&#30697;&#38453;&#26435;&#37325;&#12290;&#25972;&#20010;&#36807;&#31243;&#23545;&#22810;&#20010;&#35762;&#24231;&#36827;&#34892;&#36845;&#20195;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#26032;&#35789;&#20986;&#29616;&#39057;&#29575;&#36739;&#39640;&#26102;&#33719;&#24471;&#20102;&#24615;&#33021;&#30340;&#25552;&#21319;&#65288;&#36229;&#36807;80%&#30340;&#21484;&#22238;&#29575;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances, Automatic Speech Recognition (ASR) systems are still far from perfect. Typical errors include acronyms, named entities and domain-specific special words for which little or no data is available. To address the problem of recognizing these words, we propose an self-supervised continual learning approach. Given the audio of a lecture talk with corresponding slides, we bias the model towards decoding new words from the slides by using a memory-enhanced ASR model from previous work. Then, we perform inference on the talk, collecting utterances that contain detected new words into an adaptation dataset. Continual learning is then performed on this set by adapting low-rank matrix weights added to each weight matrix of the model. The whole procedure is iterated for many talks. We show that with this approach, we obtain increasing performance on the new words when they occur more frequently (more than 80% recall) while preserving the general performance of the model.
&lt;/p&gt;</description></item><item><title>TwinBooster&#32467;&#21512;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;Barlow Twins&#21644;&#26799;&#24230;&#25552;&#21319;&#65292;&#36890;&#36807;&#25972;&#21512;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#25351;&#32441;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#23646;&#24615;&#30340;&#31934;&#30830;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04478</link><description>&lt;p&gt;
TwinBooster: &#32467;&#21512;Barlow Twins&#21644;&#26799;&#24230;&#25552;&#21319;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#22686;&#24378;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TwinBooster: Synergising Large Language Models with Barlow Twins and Gradient Boosting for Enhanced Molecular Property Prediction. (arXiv:2401.04478v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04478
&lt;/p&gt;
&lt;p&gt;
TwinBooster&#32467;&#21512;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;Barlow Twins&#21644;&#26799;&#24230;&#25552;&#21319;&#65292;&#36890;&#36807;&#25972;&#21512;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#25351;&#32441;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#23646;&#24615;&#30340;&#31934;&#30830;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#21457;&#29616;&#21644;&#24320;&#21457;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#23545;&#20998;&#23376;&#27963;&#24615;&#21644;&#23646;&#24615;&#30340;&#31934;&#30830;&#39044;&#27979;&#12290;&#34429;&#28982;&#22522;&#20110;&#35745;&#31639;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#20351;&#29992;&#36804;&#20170;&#20026;&#27490;&#20165;&#38480;&#20110;&#22823;&#37327;&#25968;&#25454;&#21487;&#29992;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#25991;&#26412;&#20449;&#24687;&#30340;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;Siamese&#31070;&#32463;&#32593;&#32476;Barlow Twins&#12290;&#35813;&#26550;&#26500;&#21033;&#29992;&#26816;&#27979;&#26041;&#27861;&#20449;&#24687;&#21644;&#20998;&#23376;&#25351;&#32441;&#25552;&#21462;&#30495;&#23454;&#30340;&#20998;&#23376;&#20449;&#24687;&#12290;TwinBooster&#36890;&#36807;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#30340;&#23646;&#24615;&#39044;&#27979;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20154;&#24037;&#26234;&#33021;&#27969;&#27700;&#32447;&#22312;FS-Mol&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#36825;&#19968;&#31361;&#30772;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#36890;&#24120;&#25968;&#25454;&#31232;&#32570;&#30340;&#20851;&#38190;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of drug discovery and development relies on the precise prediction of molecular activities and properties. While in silico molecular property prediction has shown remarkable potential, its use has been limited so far to assays for which large amounts of data are available. In this study, we use a fine-tuned large language model to integrate biological assays based on their textual information, coupled with Barlow Twins, a Siamese neural network using a novel self-supervised learning approach. This architecture uses both assay information and molecular fingerprints to extract the true molecular information. TwinBooster enables the prediction of properties of unseen bioassays and molecules by providing state-of-the-art zero-shot learning tasks. Remarkably, our artificial intelligence pipeline shows excellent performance on the FS-Mol benchmark. This breakthrough demonstrates the application of deep learning to critical property prediction tasks where data is typically scarce.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#20197;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;&#35813;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;FL&#26694;&#26550;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04472</link><description>&lt;p&gt;
&#20851;&#20110;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Efficient Federated Learning Methods for Foundation Model Training. (arXiv:2401.04472v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04472
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#22312;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#20197;&#20248;&#21270;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;&#35813;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;FL&#26694;&#26550;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#20419;&#36827;&#38544;&#31169;&#20445;&#25252;&#21327;&#20316;&#35757;&#32451;&#30340;&#25104;&#29087;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#26032;&#30340;FL&#26041;&#27861;&#36890;&#24120;&#21482;&#28041;&#21450;&#23567;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36129;&#29486;&#12290;&#38543;&#30528;Transformer&#27169;&#22411;&#30340;&#24040;&#22823;&#25104;&#21151;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#22914;&#20309;&#20351;&#22522;&#30784;&#27169;&#22411;&#22312;FL&#24212;&#29992;&#20013;&#23454;&#26045;&#36215;&#26469;&#65311;&#37492;&#20110;&#22312;FL&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#30340;&#26102;&#38388;&#28040;&#32791;&#36890;&#24120;&#30456;&#20284;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20851;&#20110;&#22312;FL&#24212;&#29992;&#20013;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#26041;&#27861;&#30340;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#20248;&#21270;&#35757;&#32451;&#26102;&#38388;&#24182;&#20943;&#23569;&#23458;&#25143;&#31471;&#19982;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#30446;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;FL&#26694;&#26550;&#65292;&#24182;&#26681;&#25454;FL&#30740;&#31350;&#21450;&#20854;&#24310;&#20280;&#30340;&#29616;&#26377;&#26041;&#27861;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has become an established technique to facilitate privacy-preserving collaborative training. However, new approaches to FL often discuss their contributions involving small deep-learning models only. With the tremendous success of transformer models, the following question arises: What is necessary to operationalize foundation models in an FL application? Knowing that computation and communication often take up similar amounts of time in FL, we introduce a novel taxonomy focused on computational and communication efficiency methods in FL applications. This said, these methods aim to optimize the training time and reduce communication between clients and the server. We also look at the current state of widely used FL frameworks and discuss future research potentials based on existing approaches in FL research and beyond.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;PhilEO Bench&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;EO&#22522;&#30784;&#27169;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;EO&#39046;&#22495;&#20013;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#26694;&#26550;&#21253;&#25324;&#27979;&#35797;&#24179;&#21488;&#21644;&#19968;&#20010;400 GB Sentinel-2&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#24314;&#31569;&#23494;&#24230;&#20272;&#35745;&#12289;&#36947;&#36335;&#20998;&#21106;&#21644;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#30340;&#26631;&#31614;&#65292;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.04464</link><description>&lt;p&gt;
PhilEO Bench: &#35780;&#20272;&#22320;&#29702;&#31354;&#38388;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PhilEO Bench: Evaluating Geo-Spatial Foundation Models. (arXiv:2401.04464v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;PhilEO Bench&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;EO&#22522;&#30784;&#27169;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;EO&#39046;&#22495;&#20013;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#26694;&#26550;&#21253;&#25324;&#27979;&#35797;&#24179;&#21488;&#21644;&#19968;&#20010;400 GB Sentinel-2&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#24314;&#31569;&#23494;&#24230;&#20272;&#35745;&#12289;&#36947;&#36335;&#20998;&#21106;&#21644;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#30340;&#26631;&#31614;&#65292;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#35266;&#27979;&#65288;EO&#65289;&#21355;&#26143;&#25429;&#25417;&#21040;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#20854;&#20013;Sentinel-2&#26143;&#24231;&#27599;&#22825;&#20135;&#29983;1.6 TB&#30340;&#25968;&#25454;&#12290;&#36825;&#20351;&#24471;&#36965;&#24863;&#25104;&#20026;&#19968;&#20010;&#25968;&#25454;&#20016;&#23500;&#30340;&#39046;&#22495;&#65292;&#38750;&#24120;&#36866;&#21512;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24212;&#29992;ML&#27169;&#22411;&#21040;EO&#39046;&#22495;&#30340;&#29942;&#39048;&#22312;&#20110;&#32570;&#20047;&#32463;&#36807;&#27880;&#37322;&#30340;&#25968;&#25454;&#65292;&#22240;&#20026;&#27880;&#37322;&#26159;&#19968;&#39033;&#36153;&#26102;&#36153;&#21147;&#30340;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#21644;&#22522;&#30784;&#27169;&#22411;&#26041;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;PhilEO Bench&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;EO&#22522;&#30784;&#27169;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#19981;&#21516;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#21644;&#19968;&#33268;&#24615;&#22522;&#20934;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#21644;&#19968;&#20010;&#26032;&#39062;&#30340;400 GB Sentinel-2&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19977;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#26631;&#31614;&#65292;&#21363;&#24314;&#31569;&#23494;&#24230;&#20272;&#35745;&#12289;&#36947;&#36335;&#20998;&#21106;&#21644;&#22303;&#22320;&#35206;&#30422;&#20998;&#31867;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21253;&#25324;Prithvi&#21644;SatMAE&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive amounts of unlabelled data are captured by Earth Observation (EO) satellites, with the Sentinel-2 constellation generating 1.6 TB of data daily. This makes Remote Sensing a data-rich domain well suited to Machine Learning (ML) solutions. However, a bottleneck in applying ML models to EO is the lack of annotated data as annotation is a labour-intensive and costly process. As a result, research in this domain has focused on Self-Supervised Learning and Foundation Model approaches. This paper addresses the need to evaluate different Foundation Models on a fair and uniform benchmark by introducing the PhilEO Bench, a novel evaluation framework for EO Foundation Models. The framework comprises of a testbed and a novel 400 GB Sentinel-2 dataset containing labels for three downstream tasks, building density estimation, road segmentation, and land cover classification. We present experiments using our framework evaluating different Foundation Models, including Prithvi and SatMAE, at mu
&lt;/p&gt;</description></item><item><title>&#26412;&#31456;&#25552;&#20379;&#20102;&#32452;&#32455;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#25152;&#38656;&#32771;&#34385;&#30340;&#23454;&#36341;&#38382;&#39064;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#28608;&#21169;&#21442;&#19982;&#30340;&#31574;&#30053;&#12289;&#31038;&#21306;&#21442;&#19982;&#30340;&#26680;&#24515;&#20869;&#23481;&#20197;&#21450;&#21518;&#21220;&#38382;&#39064;&#30340;&#31649;&#29702;&#21644;&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2401.04452</link><description>&lt;p&gt;
AI&#31454;&#36187;&#21644;&#22522;&#20934;&#65292;&#23454;&#36341;&#38382;&#39064;&#65306;&#25552;&#26696;&#12289;&#25320;&#27454;&#12289;&#36190;&#21161;&#12289;&#22870;&#21697;&#12289;&#20256;&#25773;&#12289;&#23459;&#20256;
&lt;/p&gt;
&lt;p&gt;
AI Competitions and Benchmarks, Practical issues: Proposals, grant money, sponsors, prizes, dissemination, publicity. (arXiv:2401.04452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#25552;&#20379;&#20102;&#32452;&#32455;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#25152;&#38656;&#32771;&#34385;&#30340;&#23454;&#36341;&#38382;&#39064;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#28608;&#21169;&#21442;&#19982;&#30340;&#31574;&#30053;&#12289;&#31038;&#21306;&#21442;&#19982;&#30340;&#26680;&#24515;&#20869;&#23481;&#20197;&#21450;&#21518;&#21220;&#38382;&#39064;&#30340;&#31649;&#29702;&#21644;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#32508;&#36848;&#20102;&#32452;&#32455;&#20154;&#24037;&#26234;&#33021;&#31454;&#36187;&#25152;&#28041;&#21450;&#30340;&#23454;&#36341;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#28608;&#21169;&#21442;&#19982;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#26377;&#25928;&#30340;&#27807;&#36890;&#25216;&#24039;&#12289;&#19982;&#35813;&#39046;&#22495;&#30340;&#28909;&#38376;&#35805;&#39064;&#23545;&#40784;&#12289;&#22870;&#39033;&#32467;&#26500;&#12289;&#28508;&#22312;&#30340;&#25307;&#32856;&#26426;&#20250;&#31561;&#31561;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36716;&#21521;&#31038;&#21306;&#21442;&#19982;&#30340;&#26680;&#24515;&#65292;&#24182;&#25506;&#35752;&#32452;&#32455;&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#26377;&#25928;&#30340;&#36755;&#20986;&#20256;&#25773;&#26041;&#24335;&#12290;&#26368;&#21518;&#65292;&#26412;&#31456;&#20171;&#32461;&#20102;&#32452;&#32455;&#20154;&#21592;&#21644;&#36164;&#28304;&#20998;&#37197;&#31561;&#21518;&#21220;&#38382;&#39064;&#65292;&#20197;&#26377;&#25928;&#31649;&#29702;&#21644;&#25191;&#34892;&#31454;&#36187;&#12290;&#36890;&#36807;&#30740;&#31350;&#36825;&#20123;&#23454;&#38469;&#38382;&#39064;&#65292;&#35835;&#32773;&#23558;&#33719;&#24471;&#26377;&#20851;&#20174;&#21019;&#31435;&#21040;&#23436;&#25104;&#30340;AI&#31454;&#36187;&#32452;&#32455;&#30340;&#21487;&#34892;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This chapter provides a comprehensive overview of the pragmatic aspects involved in organizing AI competitions. We begin by discussing strategies to incentivize participation, touching upon effective communication techniques, aligning with trending topics in the field, structuring awards, potential recruitment opportunities, and more. We then shift to the essence of community engagement, and into organizational best practices and effective means of disseminating challenge outputs. Lastly, the chapter addresses the logistics, exposing on costs, required manpower, and resource allocation for effectively managing and executing a challenge. By examining these practical problems, readers will gain actionable insights to navigate the multifaceted landscape of AI competition organization, from inception to completion.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#24494;&#22320;&#38663;&#27979;&#37327;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#37325;&#24314;&#28023;&#28010;&#25968;&#25454;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#27979;&#37327;&#28023;&#28010;&#20135;&#29983;&#30340;&#24494;&#22320;&#38663;&#20449;&#21495;&#24182;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#23545;&#32570;&#22833;&#28014;&#26631;&#25968;&#25454;&#30340;&#20934;&#30830;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2401.04431</link><description>&lt;p&gt;
&#20351;&#29992;&#24494;&#22320;&#38663;&#27979;&#37327;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#37325;&#24314;&#28023;&#28010;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Sea wave data reconstruction using micro-seismic measurements and machine learning methods. (arXiv:2401.04431v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#24494;&#22320;&#38663;&#27979;&#37327;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#37325;&#24314;&#28023;&#28010;&#25968;&#25454;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#27979;&#37327;&#28023;&#28010;&#20135;&#29983;&#30340;&#24494;&#22320;&#38663;&#20449;&#21495;&#24182;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#23545;&#32570;&#22833;&#28014;&#26631;&#25968;&#25454;&#30340;&#20934;&#30830;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#28010;&#30417;&#27979;&#22312;&#28023;&#27915;&#23398;&#30340;&#35768;&#22810;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#22914;&#22825;&#27668;&#21644;&#27874;&#28010;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;&#20256;&#32479;&#30340;&#29616;&#22330;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#38170;&#23450;&#28014;&#26631;&#65292;&#20854;&#27979;&#37327;&#36890;&#24120;&#34987;&#35270;&#20026;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26292;&#38706;&#22312;&#24694;&#21155;&#29615;&#22659;&#20013;&#65292;&#23427;&#20204;&#19981;&#21487;&#38752;&#65292;&#38656;&#35201;&#39057;&#32321;&#32500;&#25252;&#65292;&#24182;&#19988;&#25968;&#25454;&#38598;&#23384;&#22312;&#35768;&#22810;&#38388;&#26029;&#12290;&#20026;&#20102;&#20811;&#26381;&#20197;&#21069;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#28014;&#26631;&#65292;&#19968;&#20010;&#24494;&#22320;&#38663;&#27979;&#37327;&#31449;&#21644;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#24037;&#20316;&#21407;&#29702;&#22522;&#20110;&#27979;&#37327;&#28023;&#28010;&#20135;&#29983;&#30340;&#24494;&#22320;&#38663;&#20449;&#21495;&#12290;&#22240;&#27492;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23558;&#34987;&#35757;&#32451;&#20197;&#20174;&#24494;&#22320;&#38663;&#25968;&#25454;&#37325;&#24314;&#32570;&#22833;&#30340;&#28014;&#26631;&#25968;&#25454;&#12290;&#30001;&#20110;&#24494;&#22320;&#38663;&#31449;&#21487;&#20197;&#23433;&#35013;&#22312;&#23460;&#20869;&#65292;&#23427;&#20445;&#35777;&#20102;&#39640;&#21487;&#38752;&#24615;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#20102;&#20934;&#30830;&#37325;&#24314;&#32570;&#22833;&#30340;&#28014;&#26631;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22788;&#29702;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#21644;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sea wave monitoring is key in many applications in oceanography such as the validation of weather and wave models. Conventional in situ solutions are based on moored buoys whose measurements are often recognized as a standard. However, being exposed to a harsh environment, they are not reliable, need frequent maintenance, and the datasets feature many gaps. To overcome the previous limitations, we propose a system including a buoy, a micro-seismic measuring station, and a machine learning algorithm. The working principle is based on measuring the micro-seismic signals generated by the sea waves. Thus, the machine learning algorithm will be trained to reconstruct the missing buoy data from the micro-seismic data. As the micro-seismic station can be installed indoor, it assures high reliability while the machine learning algorithm provides accurate reconstruction of the missing buoy data. In this work, we present the methods to process the data, develop and train the machine learning alg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#27867;&#21270;&#31639;&#27861;"&#20803;&#26862;&#26519;"&#65292;&#36890;&#36807;&#25972;&#21512;&#20803;&#23398;&#20064;&#31574;&#30053;&#21644;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#24230;&#37327;&#65292;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#31639;&#27861;&#22312;&#27599;&#20010;&#20803;&#20219;&#21153;&#20013;&#36827;&#34892;&#20803;&#23398;&#20064;&#20248;&#21270;&#65292;&#24182;&#21033;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#65292;&#26469;&#24809;&#32602;&#27867;&#21270;&#24615;&#33021;&#24046;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.04425</link><description>&lt;p&gt;
&#20803;&#26862;&#26519;&#65306;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#38543;&#26426;&#26862;&#26519;&#30340;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Meta-forests: Domain generalization on random forests with meta-learning. (arXiv:2401.04425v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#27867;&#21270;&#31639;&#27861;"&#20803;&#26862;&#26519;"&#65292;&#36890;&#36807;&#25972;&#21512;&#20803;&#23398;&#20064;&#31574;&#30053;&#21644;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#24230;&#37327;&#65292;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#31639;&#27861;&#22312;&#27599;&#20010;&#20803;&#20219;&#21153;&#20013;&#36827;&#34892;&#20803;&#23398;&#20064;&#20248;&#21270;&#65292;&#24182;&#21033;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#65292;&#26469;&#24809;&#32602;&#27867;&#21270;&#24615;&#33021;&#24046;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#28304;&#39046;&#22495;&#23398;&#20064;&#65292;&#20351;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#39046;&#22495;&#27867;&#21270;&#22312;&#25968;&#25454;&#26377;&#38480;&#12289;&#22256;&#38590;&#25110;&#26114;&#36149;&#25910;&#38598;&#30340;&#24773;&#20917;&#19979;&#38750;&#24120;&#26377;&#29992;&#65292;&#27604;&#22914;&#30446;&#26631;&#35782;&#21035;&#21644;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#27867;&#21270;&#31639;&#27861;&#65292;&#21517;&#20026;"&#20803;&#26862;&#26519;"&#65292;&#23427;&#22312;&#22522;&#26412;&#30340;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#22522;&#30784;&#19978;&#65292;&#32467;&#21512;&#20102;&#20803;&#23398;&#20064;&#31574;&#30053;&#21644;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#24230;&#37327;&#12290;&#20803;&#26862;&#26519;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20943;&#23569;&#26641;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#24182;&#22686;&#24378;&#20854;&#24378;&#24230;&#65292;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20803;&#26862;&#26519;&#22312;&#27599;&#20010;&#20803;&#20219;&#21153;&#20013;&#36827;&#34892;&#20803;&#23398;&#20064;&#20248;&#21270;&#65292;&#21516;&#26102;&#21033;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#65292;&#22312;&#20803;&#27979;&#35797;&#36807;&#31243;&#20013;&#24809;&#32602;&#27867;&#21270;&#24615;&#33021;&#24046;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#20004;&#20010;&#20844;&#24320;&#30340;&#39046;&#22495;&#27867;&#21270;&#25968;&#25454;&#38598;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization is a popular machine learning technique that enables models to perform well on the unseen target domain, by learning from multiple source domains. Domain generalization is useful in cases where data is limited, difficult, or expensive to collect, such as in object recognition and biomedicine. In this paper, we propose a novel domain generalization algorithm called "meta-forests", which builds upon the basic random forests model by incorporating the meta-learning strategy and maximum mean discrepancy measure. The aim of meta-forests is to enhance the generalization ability of classifiers by reducing the correlation among trees and increasing their strength. More specifically, meta-forests conducts meta-learning optimization during each meta-task, while also utilizing the maximum mean discrepancy as a regularization term to penalize poor generalization performance in the meta-test process. To evaluate the effectiveness of our algorithm, we test it on two publicly ob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#23884;&#20837;&#32500;&#24230;&#20248;&#21270;&#26041;&#27861;&#65288;FIITED&#65289;&#65292;&#33021;&#22815;&#22312;&#25512;&#33616;&#31995;&#32479;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#26681;&#25454;&#23884;&#20837;&#21521;&#37327;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#35843;&#25972;&#20854;&#32500;&#24230;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#34394;&#25311;&#21704;&#24076;&#32034;&#24341;&#21704;&#24076;&#34920;&#30340;&#23884;&#20837;&#23384;&#20648;&#31995;&#32479;&#20197;&#26377;&#25928;&#33410;&#30465;&#20869;&#23384;&#12290;</title><link>http://arxiv.org/abs/2401.04408</link><description>&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#20248;&#21270;&#32454;&#31890;&#24230;&#23884;&#20837;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Fine-Grained Embedding Dimension Optimization During Training for Recommender Systems. (arXiv:2401.04408v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#23884;&#20837;&#32500;&#24230;&#20248;&#21270;&#26041;&#27861;&#65288;FIITED&#65289;&#65292;&#33021;&#22815;&#22312;&#25512;&#33616;&#31995;&#32479;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#26681;&#25454;&#23884;&#20837;&#21521;&#37327;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#35843;&#25972;&#20854;&#32500;&#24230;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#34394;&#25311;&#21704;&#24076;&#32034;&#24341;&#21704;&#24076;&#34920;&#30340;&#23884;&#20837;&#23384;&#20648;&#31995;&#32479;&#20197;&#26377;&#25928;&#33410;&#30465;&#20869;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;&#20013;&#30340;&#22823;&#22411;&#23884;&#20837;&#34920;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#38656;&#35201;&#36807;&#22823;&#30340;&#20869;&#23384;&#12290;&#20026;&#20102;&#20943;&#23567;&#35757;&#32451;&#26102;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#23884;&#20837;&#32500;&#24230;&#20248;&#21270;&#26041;&#27861; (FIITED)&#12290;&#26681;&#25454;&#23884;&#20837;&#21521;&#37327;&#30340;&#37325;&#35201;&#24615;&#19981;&#21516;&#65292;FIITED&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36830;&#32493;&#35843;&#25972;&#27599;&#20010;&#23884;&#20837;&#21521;&#37327;&#30340;&#32500;&#24230;&#65292;&#23558;&#26356;&#37325;&#35201;&#30340;&#23884;&#20837;&#21521;&#37327;&#20998;&#37197;&#26356;&#38271;&#30340;&#32500;&#24230;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#25968;&#25454;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#34394;&#25311;&#21704;&#24076;&#30340;&#29289;&#29702;&#32034;&#24341;&#21704;&#24076;&#34920;&#30340;&#23884;&#20837;&#23384;&#20648;&#31995;&#32479;&#65292;&#20197;&#23454;&#29616;&#23884;&#20837;&#32500;&#24230;&#30340;&#35843;&#25972;&#24182;&#26377;&#25928;&#22320;&#33410;&#30465;&#20869;&#23384;&#12290;&#23545;&#20004;&#20010;&#34892;&#19994;&#27169;&#22411;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FIITED&#33021;&#22815;&#23558;&#23884;&#20837;&#30340;&#22823;&#23567;&#20943;&#23567;&#36229;&#36807;65%&#65292;&#21516;&#26102;&#20445;&#25345;&#35757;&#32451;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#27604;&#29616;&#26377;&#30340;&#19968;&#31181;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#23884;&#20837;&#20462;&#21098;&#30340;&#26041;&#27861;&#33410;&#30465;&#26356;&#22810;&#20869;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;
Huge embedding tables in modern Deep Learning Recommender Models (DLRM) require prohibitively large memory during training and inference. Aiming to reduce the memory footprint of training, this paper proposes FIne-grained In-Training Embedding Dimension optimization (FIITED). Given the observation that embedding vectors are not equally important, FIITED adjusts the dimension of each individual embedding vector continuously during training, assigning longer dimensions to more important embeddings while adapting to dynamic changes in data. A novel embedding storage system based on virtually-hashed physically-indexed hash tables is designed to efficiently implement the embedding dimension adjustment and effectively enable memory saving. Experiments on two industry models show that FIITED is able to reduce the size of embeddings by more than 65% while maintaining the trained model's quality, saving significantly more memory than a state-of-the-art in-training embedding pruning method. On p
&lt;/p&gt;</description></item><item><title>&#20010;&#20307;&#21270;&#26102;&#38388;&#24207;&#21015;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#29983;&#25104;&#27169;&#22411;IGNITE&#36890;&#36807;&#23398;&#20064;&#20010;&#20307;&#30340;&#21160;&#24577;&#29305;&#24449;&#65292;&#32467;&#21512;&#20154;&#21475;&#29305;&#24449;&#21644;&#27835;&#30103;&#20449;&#24687;&#65292;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#30495;&#23454;&#20540;&#65292;&#20026;&#20010;&#20307;&#21270;&#21307;&#30103;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.04402</link><description>&lt;p&gt;
IGNITE: &#20010;&#20307;&#21270;&#26102;&#38388;&#24207;&#21015;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IGNITE: Individualized GeNeration of Imputations in Time-series Electronic health records. (arXiv:2401.04402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04402
&lt;/p&gt;
&lt;p&gt;
&#20010;&#20307;&#21270;&#26102;&#38388;&#24207;&#21015;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#29983;&#25104;&#27169;&#22411;IGNITE&#36890;&#36807;&#23398;&#20064;&#20010;&#20307;&#30340;&#21160;&#24577;&#29305;&#24449;&#65292;&#32467;&#21512;&#20154;&#21475;&#29305;&#24449;&#21644;&#27835;&#30103;&#20449;&#24687;&#65292;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#30495;&#23454;&#20540;&#65292;&#20026;&#20010;&#20307;&#21270;&#21307;&#30103;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20026;&#25512;&#21160;&#20010;&#20307;&#21270;&#21307;&#30103;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#26681;&#25454;&#20010;&#20307;&#24046;&#24322;&#37327;&#36523;&#23450;&#21046;&#27835;&#30103;&#26041;&#26696;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#35768;&#22810;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#27169;&#22411;&#20511;&#21161;&#20016;&#23500;&#30340;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#26469;&#30740;&#31350;&#24739;&#32773;&#30340;&#29983;&#29702;&#21644;&#27835;&#30103;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#24448;&#24448;&#31232;&#30095;&#19988;&#23384;&#22312;&#22823;&#37327;&#32570;&#22833;&#65292;&#20854;&#20013;&#32570;&#22833;&#30340;&#20449;&#24687;&#20063;&#21487;&#33021;&#21453;&#26144;&#24739;&#32773;&#30340;&#20581;&#24247;&#29366;&#20917;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#22312;&#20010;&#20307;&#21270;&#21307;&#30103;&#20013;&#30340;&#25104;&#21151;&#20005;&#37325;&#20381;&#36182;&#20110;&#22914;&#20309;&#20174;&#29983;&#29702;&#25968;&#25454;&#12289;&#27835;&#30103;&#20197;&#21450;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#26469;&#34920;&#31034;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#20010;&#20307;&#30340;&#20154;&#21475;&#29305;&#24449;&#21644;&#27835;&#30103;&#30340;&#26465;&#20214;&#19979;&#65292;&#23398;&#20064;&#22810;&#21464;&#37327;&#25968;&#25454;&#30340;&#24739;&#32773;&#21160;&#24577;&#65292;&#24182;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#30495;&#23454;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Records present a valuable modality for driving personalized medicine, where treatment is tailored to fit individual-level differences. For this purpose, many data-driven machine learning and statistical models rely on the wealth of longitudinal EHRs to study patients' physiological and treatment effects. However, longitudinal EHRs tend to be sparse and highly missing, where missingness could also be informative and reflect the underlying patient's health status. Therefore, the success of data-driven models for personalized medicine highly depends on how the EHR data is represented from physiological data, treatments, and the missing values in the data. To this end, we propose a novel deep-learning model that learns the underlying patient dynamics over time across multivariate data to generate personalized realistic values conditioning on an individual's demographic characteristics and treatments. Our proposed model, IGNITE (Individualized GeNeration of Imputations in
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#26500;&#24314;&#33021;&#19982;&#20154;&#31867;&#39640;&#25928;&#21327;&#20316;&#30340;&#26426;&#22120;&#30340;&#30446;&#26631;&#65292;&#20171;&#32461;&#20102;&#20351;&#29992;&#39640;&#38454;&#35748;&#30693;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;&#30340;&#23454;&#38469;&#20363;&#23376;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#65292;&#21033;&#29992;&#20154;&#31867;&#20316;&#20026;&#20027;&#21160;&#25968;&#25454;&#28304;&#65292;&#24182;&#32771;&#34385;&#20182;&#20204;&#26356;&#39640;&#32423;&#21035;&#30340;&#20195;&#29702;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.04397</link><description>&lt;p&gt;
&#39640;&#38454;&#35748;&#30693;&#27169;&#22411;&#22312;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Role of Higher-Order Cognitive Models in Active Learning. (arXiv:2401.04397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#26500;&#24314;&#33021;&#19982;&#20154;&#31867;&#39640;&#25928;&#21327;&#20316;&#30340;&#26426;&#22120;&#30340;&#30446;&#26631;&#65292;&#20171;&#32461;&#20102;&#20351;&#29992;&#39640;&#38454;&#35748;&#30693;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;&#30340;&#23454;&#38469;&#20363;&#23376;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#65292;&#21033;&#29992;&#20154;&#31867;&#20316;&#20026;&#20027;&#21160;&#25968;&#25454;&#28304;&#65292;&#24182;&#32771;&#34385;&#20182;&#20204;&#26356;&#39640;&#32423;&#21035;&#30340;&#20195;&#29702;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#33021;&#22815;&#19982;&#20154;&#31867;&#39640;&#25928;&#21327;&#20316;&#30340;&#26426;&#22120;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#38271;&#26399;&#20197;&#26469;&#30340;&#30446;&#26631;&#12290;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#30340;&#21512;&#20316;&#36890;&#24120;&#38656;&#35201;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20043;&#38388;&#30456;&#20114;&#24314;&#27169;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#25512;&#26029;&#28508;&#22312;&#30340;&#30446;&#26631;&#12289;&#20449;&#24565;&#25110;&#24847;&#22270;&#65292;&#21487;&#33021;&#28041;&#21450;&#22810;&#32423;&#36882;&#24402;&#12290;&#22312;&#20197;&#21069;&#30340;&#35748;&#30693;&#31185;&#23398;&#12289;&#35821;&#35328;&#23398;&#21644;&#26426;&#22120;&#20154;&#23398;&#30340;&#30740;&#31350;&#20013;&#65292;&#20063;&#25552;&#20379;&#20102;&#36825;&#31181;&#39640;&#38454;&#35748;&#30693;&#22312;&#20154;&#31867;&#34892;&#20026;&#20013;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290;&#25105;&#20204;&#25552;&#20513;&#19968;&#31181;&#26032;&#30340;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#65292;&#21033;&#29992;&#20154;&#31867;&#20316;&#20026;&#20027;&#21160;&#25968;&#25454;&#28304;&#65292;&#24182;&#32771;&#34385;&#20182;&#20204;&#26356;&#39640;&#32423;&#21035;&#30340;&#20195;&#29702;&#20316;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25552;&#21319;&#20195;&#29702;&#32423;&#21035;&#22914;&#20309;&#23548;&#33268;&#20027;&#21160;&#23398;&#20064;&#31995;&#32479;&#21644;&#25945;&#24072;&#20043;&#38388;&#20135;&#29983;&#19981;&#21516;&#24418;&#24335;&#30340;&#21512;&#29702;&#27807;&#36890;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#39640;&#38454;&#35748;&#30693;&#27169;&#22411;&#30340;&#20027;&#21160;&#23398;&#20064;&#30340;&#23454;&#38469;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building machines capable of efficiently collaborating with humans has been a longstanding goal in artificial intelligence. Especially in the presence of uncertainties, optimal cooperation often requires that humans and artificial agents model each other's behavior and use these models to infer underlying goals, beliefs or intentions, potentially involving multiple levels of recursion. Empirical evidence for such higher-order cognition in human behavior is also provided by previous works in cognitive science, linguistics, and robotics. We advocate for a new paradigm for active learning for human feedback that utilises humans as active data sources while accounting for their higher levels of agency. In particular, we discuss how increasing level of agency results in qualitatively different forms of rational communication between an active learning system and a teacher. Additionally, we provide a practical example of active learning using a higher-order cognitive model. This is accompani
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#27169;&#22411;&#21442;&#25968;&#30340;&#25200;&#21160;&#26469;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#25511;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#37319;&#29992;&#36951;&#24536;&#29575;&#21644;&#35760;&#24518;&#20445;&#30041;&#29575;&#31561;&#26032;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#21435;&#23398;&#20064;&#25928;&#26524;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04385</link><description>&lt;p&gt;
&#36890;&#36807;&#32454;&#31890;&#24230;&#27169;&#22411;&#21442;&#25968;&#25200;&#21160;&#23454;&#29616;&#26426;&#22120;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning through fine-grained model parameters perturbation. (arXiv:2401.04385v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#27169;&#22411;&#21442;&#25968;&#30340;&#25200;&#21160;&#26469;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#25511;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#37319;&#29992;&#36951;&#24536;&#29575;&#21644;&#35760;&#24518;&#20445;&#30041;&#29575;&#31561;&#26032;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#21435;&#23398;&#20064;&#25928;&#26524;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#21435;&#23398;&#20064;&#25216;&#26415;&#28041;&#21450;&#21040;&#25764;&#38144;&#25968;&#25454;&#35760;&#24405;&#21644;&#20943;&#23567;&#35813;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#24110;&#21161;&#23454;&#29616;&#29992;&#25143;&#38544;&#31169;&#20445;&#25252;&#30446;&#26631;&#65292;&#20294;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22522;&#20110;&#21442;&#25968;&#25200;&#21160;&#30340;&#26435;&#37325;&#21435;&#23398;&#20064;&#26159;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20294;&#36890;&#24120;&#28041;&#21450;&#21040;&#20840;&#23616;&#20462;&#25913;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31934;&#32454;&#30340;Top-K&#21644;Random-k&#21442;&#25968;&#25200;&#21160;&#19981;&#31934;&#30830;&#26426;&#22120;&#21435;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#28385;&#36275;&#38544;&#31169;&#38656;&#27714;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#21487;&#25511;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#35780;&#20272;&#26426;&#22120;&#21435;&#23398;&#20064;&#25928;&#26524;&#30340;&#25361;&#25112;&#65292;&#32771;&#34385;&#20102;&#27169;&#22411;&#22312;&#21435;&#23398;&#20064;&#21644;&#21097;&#20313;&#25968;&#25454;&#19978;&#30340;&#24191;&#20041;&#24615;&#33021;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35780;&#20272;&#21435;&#23398;&#20064;&#25928;&#26524;&#21644;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#25351;&#26631;&#65292;&#21363;&#36951;&#24536;&#29575;&#21644;&#35760;&#24518;&#20445;&#30041;&#29575;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19981;&#31934;&#30830;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#65292;&#29616;&#26377;&#30340;&#25351;&#26631;&#26080;&#27861;&#23545;&#21435;&#23398;&#20064;&#31243;&#24230;&#36827;&#34892;&#20934;&#30830;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning techniques, which involve retracting data records and reducing influence of said data on trained models, help with the user privacy protection objective but incur significant computational costs. Weight perturbation-based unlearning is a general approach, but it typically involves globally modifying the parameters. We propose fine-grained Top-K and Random-k parameters perturbed inexact machine unlearning strategies that address the privacy needs while keeping the computational costs tractable.  In order to demonstrate the efficacy of our strategies we also tackle the challenge of evaluating the effectiveness of machine unlearning by considering the model's generalization performance across both unlearning and remaining data. To better assess the unlearning effect and model generalization, we propose novel metrics, namely, the forgetting rate and memory retention rate. However, for inexact machine unlearning, current metrics are inadequate in quantifying the degree of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#25968;&#25454;&#20026;&#20013;&#24515;&#8221;&#30340;&#35270;&#35282;&#65292;&#25506;&#35752;&#20102;&#25968;&#25454;&#25910;&#38598;&#12289;&#22788;&#29702;&#21644;&#20998;&#26512;&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#23558;&#29616;&#26377;&#24037;&#20316;&#20998;&#20026;&#19977;&#20010;&#31867;&#21035;&#65306;&#28145;&#24230;&#27169;&#22411;&#30340;&#35299;&#37322;&#12289;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#21644;&#39046;&#22495;&#30693;&#35782;&#30340;&#35265;&#35299;&#12290;&#36890;&#36807;&#25968;&#25454;&#25366;&#25496;&#25805;&#20316;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#20123;XAI&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04374</link><description>&lt;p&gt;
&#36808;&#21521;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#65306;&#19968;&#20010;&#25968;&#25454;&#25366;&#25496;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable Artificial Intelligence (XAI): A Data Mining Perspective. (arXiv:2401.04374v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#25968;&#25454;&#20026;&#20013;&#24515;&#8221;&#30340;&#35270;&#35282;&#65292;&#25506;&#35752;&#20102;&#25968;&#25454;&#25910;&#38598;&#12289;&#22788;&#29702;&#21644;&#20998;&#26512;&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#23558;&#29616;&#26377;&#24037;&#20316;&#20998;&#20026;&#19977;&#20010;&#31867;&#21035;&#65306;&#28145;&#24230;&#27169;&#22411;&#30340;&#35299;&#37322;&#12289;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#21644;&#39046;&#22495;&#30693;&#35782;&#30340;&#35265;&#35299;&#12290;&#36890;&#36807;&#25968;&#25454;&#25366;&#25496;&#25805;&#20316;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#20123;XAI&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#22797;&#26434;&#24615;&#21644;&#36879;&#26126;&#24230;&#19981;&#36275;&#65292;&#20154;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20197;&#20351;&#36825;&#20123;&#31995;&#32479;&#26356;&#20855;&#35299;&#37322;&#24615;&#25110;&#22312;&#21487;&#35775;&#38382;&#30340;&#26415;&#35821;&#20013;&#35299;&#37322;&#20854;&#34892;&#20026;&#12290;&#19982;&#22823;&#22810;&#25968;&#35780;&#35770;&#19981;&#21516;&#65292;&#35813;&#24037;&#20316;&#37319;&#29992;&#8220;&#25968;&#25454;&#20026;&#20013;&#24515;&#8221;&#30340;&#35266;&#28857;&#65292;&#30740;&#31350;&#25968;&#25454;&#25910;&#38598;&#65292;&#22788;&#29702;&#21644;&#20998;&#26512;&#22914;&#20309;&#20419;&#25104;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#12290;&#25105;&#20204;&#23558;&#29616;&#26377;&#24037;&#20316;&#20998;&#20026;&#19977;&#31867;&#65292;&#26681;&#25454;&#20854;&#30446;&#30340;&#36827;&#34892;&#20998;&#31867;&#65306;&#28145;&#24230;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#28041;&#21450;&#23558;&#25968;&#25454;&#28857;&#19982;&#27169;&#22411;&#36755;&#20986;&#30456;&#20851;&#32852;&#30340;&#29305;&#24449;&#24402;&#22240;&#21644;&#25512;&#29702;&#36807;&#31243;&#65307;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#35757;&#32451;&#25968;&#25454;&#32454;&#24494;&#24046;&#24322;&#65288;&#22914;&#25968;&#25454;&#35780;&#20272;&#21644;&#26679;&#26412;&#24322;&#24120;&#65289;&#23545;&#20915;&#31574;&#36807;&#31243;&#30340;&#24433;&#21709;&#65307;&#20197;&#21450;&#39046;&#22495;&#30693;&#35782;&#30340;&#35265;&#35299;&#65292;&#20174;&#25968;&#25454;&#21644;&#27169;&#22411;&#20013;&#21457;&#29616;&#28508;&#22312;&#27169;&#24335;&#65292;&#24182;&#20419;&#36827;&#31038;&#20250;&#20215;&#20540;&#21644;&#31185;&#23398;&#21457;&#29616;&#30340;&#26032;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;XAI&#26041;&#27861;&#35770;&#25552;&#28860;&#20026;&#25968;&#25454;&#25366;&#25496;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the complexity and lack of transparency in deep neural networks (DNNs), extensive efforts have been made to make these systems more interpretable or explain their behaviors in accessible terms. Unlike most reviews, which focus on algorithmic and model-centric perspectives, this work takes a "data-centric" view, examining how data collection, processing, and analysis contribute to explainable AI (XAI). We categorize existing work into three categories subject to their purposes: interpretations of deep models, referring to feature attributions and reasoning processes that correlate data points with model outputs; influences of training data, examining the impact of training data nuances, such as data valuation and sample anomalies, on decision-making processes; and insights of domain knowledge, discovering latent patterns and fostering new knowledge from data and models to advance social values and scientific discovery. Specifically, we distill XAI methodologies into data mining op
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#26144;&#23556;&#19982;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#65292;&#22312;&#20165;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#26102;&#38388;&#27493;&#38271;&#20725;&#30828;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.04372</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#26144;&#23556;&#36827;&#34892;&#31283;&#23450;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Stable generative modeling using diffusion maps. (arXiv:2401.04372v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25193;&#25955;&#26144;&#23556;&#19982;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#65292;&#22312;&#20165;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#26102;&#38388;&#27493;&#38271;&#20725;&#30828;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#20165;&#26377;&#36275;&#22815;&#25968;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#21487;&#24471;&#21040;&#30340;&#26410;&#30693;&#20998;&#24067;&#20013;&#25277;&#26679;&#30340;&#38382;&#39064;&#12290;&#22312;&#29983;&#25104;&#24314;&#27169;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#26679;&#30340;&#35774;&#32622;&#26368;&#36817;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#26144;&#23556;&#21644;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#25193;&#25955;&#26144;&#23556;&#29992;&#20110;&#20174;&#21487;&#29992;&#30340;&#35757;&#32451;&#26679;&#26412;&#20013;&#36817;&#20284;&#24471;&#21040;&#28418;&#31227;&#39033;&#65292;&#28982;&#21518;&#22312;&#31163;&#25955;&#26102;&#38388;&#30340;&#26391;&#20043;&#19975;&#37319;&#26679;&#22120;&#20013;&#23454;&#29616;&#29983;&#25104;&#26032;&#26679;&#26412;&#12290;&#36890;&#36807;&#23558;&#26680;&#24102;&#23485;&#35774;&#32622;&#20026;&#19982;&#26410;&#35843;&#25972;&#30340;&#26391;&#20043;&#19975;&#31639;&#27861;&#20013;&#20351;&#29992;&#30340;&#26102;&#38388;&#27493;&#38271;&#21305;&#37197;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#36991;&#20813;&#36890;&#24120;&#19982;&#26102;&#38388;&#27493;&#38271;&#20725;&#30828;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30456;&#20851;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#26356;&#20934;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35010;&#27493;&#39588;&#26041;&#26696;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#26679;&#26412;&#20445;&#25345;&#22312;&#35757;&#32451;&#26679;&#26412;&#30340;&#20984;&#21253;&#20869;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#33258;&#28982;&#22320;&#25193;&#23637;&#20026;&#29983;&#25104;&#26465;&#20214;&#26679;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of sampling from an unknown distribution for which only a sufficiently large number of training samples are available. Such settings have recently drawn considerable interest in the context of generative modelling. In this paper, we propose a generative model combining diffusion maps and Langevin dynamics. Diffusion maps are used to approximate the drift term from the available training samples, which is then implemented in a discrete-time Langevin sampler to generate new samples. By setting the kernel bandwidth to match the time step size used in the unadjusted Langevin algorithm, our method effectively circumvents any stability issues typically associated with time-stepping stiff stochastic differential equations. More precisely, we introduce a novel split-step scheme, ensuring that the generated samples remain within the convex hull of the training samples. Our framework can be naturally extended to generate conditional samples. We demonstrate the performance
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#23569;&#37327;&#30340;&#31354;&#27668;&#36136;&#37327;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#31354;&#27668;&#36136;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#22825;&#27668;&#23384;&#20648;&#24211;&#30340;&#25968;&#25454;&#65292;&#32771;&#34385;&#20102;&#26469;&#33258;197&#20010;&#39318;&#37117;&#22478;&#24066;&#30340;&#27668;&#35937;&#12289;&#31354;&#27668;&#27745;&#26579;&#29289;&#21644;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#22312;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04369</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#31354;&#27668;&#36136;&#37327;&#39044;&#25253;&#65306;&#20855;&#26377;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#30340;&#20840;&#29699;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Air Quality Forecasting Using Machine Learning: A Global perspective with Relevance to Low-Resource Settings. (arXiv:2401.04369v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#23569;&#37327;&#30340;&#31354;&#27668;&#36136;&#37327;&#25968;&#25454;&#26469;&#20934;&#30830;&#39044;&#27979;&#31354;&#27668;&#36136;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#22825;&#27668;&#23384;&#20648;&#24211;&#30340;&#25968;&#25454;&#65292;&#32771;&#34385;&#20102;&#26469;&#33258;197&#20010;&#39318;&#37117;&#22478;&#24066;&#30340;&#27668;&#35937;&#12289;&#31354;&#27668;&#27745;&#26579;&#29289;&#21644;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#22312;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#31354;&#27668;&#36136;&#37327;&#39044;&#27979;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#27745;&#26579;&#26159;&#20840;&#29699;&#31532;&#22235;&#22823;&#27515;&#22240;&#12290;&#34429;&#28982;&#22312;&#36825;&#20010;&#39046;&#22495;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#22312;&#39044;&#27979;&#26102;&#20381;&#36182;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#32780;&#36825;&#26679;&#30340;&#29615;&#22659;&#26356;&#21152;&#33030;&#24369;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#20934;&#30830;&#39044;&#27979;&#31354;&#27668;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#19990;&#30028;&#22825;&#27668;&#23384;&#20648;&#24211;&#65292;&#32771;&#34385;&#20102;&#26469;&#33258;197&#20010;&#39318;&#37117;&#22478;&#24066;&#30340;&#27668;&#35937;&#12289;&#31354;&#27668;&#27745;&#26579;&#29289;&#21644;&#31354;&#27668;&#36136;&#37327;&#25351;&#25968;&#25968;&#25454;&#65292;&#39044;&#27979;&#20102;&#26410;&#26469;&#19968;&#22825;&#30340;&#31354;&#27668;&#36136;&#37327;&#12290;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35780;&#20272;&#35777;&#26126;&#20102;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#22312;&#29983;&#25104;&#21487;&#38752;&#39044;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#24212;&#29992;&#20110;&#20998;&#31867;&#32780;&#19981;&#26159;&#22238;&#24402;&#26102;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;42%&#65292;&#23454;&#29616;&#20102;&#22238;&#24402;0.38&#21644;&#20998;&#31867;0.89&#30340;&#20132;&#21449;&#39564;&#35777;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Air pollution stands as the fourth leading cause of death globally. While extensive research has been conducted in this domain, most approaches rely on large datasets when it comes to prediction. This limits their applicability in low-resource settings though more vulnerable. This study addresses this gap by proposing a novel machine learning approach for accurate air quality prediction using two months of air quality data. By leveraging the World Weather Repository, the meteorological, air pollutant, and Air Quality Index features from 197 capital cities were considered to predict air quality for the next day. The evaluation of several machine learning models demonstrates the effectiveness of the Random Forest algorithm in generating reliable predictions, particularly when applied to classification rather than regression, approach which enhances the model's generalizability by 42%, achieving a cross-validation score of 0.38 for regression and 0.89 for classification. To instill confid
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#33647;&#29289;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#20013;&#24613;&#24615;&#32958;&#25439;&#20260;&#39044;&#27979;&#27169;&#22411;&#12290;&#36890;&#36807;&#21033;&#29992;&#24739;&#32773;&#22788;&#26041;&#25968;&#25454;&#21644;&#25193;&#23637;&#36830;&#25509;&#25351;&#32441;&#65288;ECFP&#65289;&#36827;&#34892;&#27169;&#24577;&#36716;&#25442;&#65292;&#22635;&#34917;&#20102;&#37325;&#30151;&#30417;&#25252;&#35774;&#32622;&#20013;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2401.04368</link><description>&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#33647;&#29289;&#29305;&#24449;&#22686;&#24378;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#20013;&#24613;&#24615;&#32958;&#25439;&#20260;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Acute Kidney Injury Prediction through Integration of Drug Features in Intensive Care Units. (arXiv:2401.04368v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#33647;&#29289;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#22686;&#24378;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#20013;&#24613;&#24615;&#32958;&#25439;&#20260;&#39044;&#27979;&#27169;&#22411;&#12290;&#36890;&#36807;&#21033;&#29992;&#24739;&#32773;&#22788;&#26041;&#25968;&#25454;&#21644;&#25193;&#23637;&#36830;&#25509;&#25351;&#32441;&#65288;ECFP&#65289;&#36827;&#34892;&#27169;&#24577;&#36716;&#25442;&#65292;&#22635;&#34917;&#20102;&#37325;&#30151;&#30417;&#25252;&#35774;&#32622;&#20013;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24613;&#24615;&#32958;&#25439;&#20260;&#65288;AKI&#65289;&#39044;&#27979;&#19982;&#23545;&#32958;&#21151;&#33021;&#20135;&#29983;&#19981;&#33391;&#24433;&#21709;&#30340;&#32958;&#27602;&#24615;&#33647;&#29289;&#20043;&#38388;&#30340;&#20851;&#31995;&#23578;&#26410;&#22312;&#37325;&#30151;&#30417;&#25252;&#35774;&#32622;&#20013;&#24471;&#21040;&#25506;&#32034;&#12290;&#23548;&#33268;&#35813;&#30740;&#31350;&#31354;&#30333;&#30340;&#19968;&#20010;&#22240;&#32032;&#26159;&#22312;&#37325;&#30151;&#30417;&#25252;&#23460;&#65288;ICU&#65289;&#32972;&#26223;&#19979;&#23545;&#33647;&#29289;&#27169;&#24577;&#30340;&#26377;&#38480;&#30740;&#31350;&#65292;&#36825;&#26159;&#30001;&#20110;&#23558;&#22788;&#26041;&#25968;&#25454;&#22788;&#29702;&#20026;&#30456;&#24212;&#30340;&#33647;&#29289;&#34920;&#31034;&#24182;&#23545;&#36825;&#20123;&#33647;&#29289;&#34920;&#31034;&#30340;&#20840;&#38754;&#29702;&#35299;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#24739;&#32773;&#22788;&#26041;&#25968;&#25454;&#20316;&#20026;&#19968;&#31181;&#27169;&#24577;&#26469;&#25913;&#36827;&#29616;&#26377;&#30340;AKI&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#65292;&#25552;&#21462;&#30456;&#20851;&#30340;&#24739;&#32773;&#22788;&#26041;&#20449;&#24687;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#36873;&#25321;&#30340;&#33647;&#29289;&#34920;&#31034;&#65292;&#21363;&#25193;&#23637;&#36830;&#25509;&#25351;&#32441;&#65288;ECFP&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#29420;&#29305;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
The relationship between acute kidney injury (AKI) prediction and nephrotoxic drugs, or drugs that adversely affect kidney function, is one that has yet to be explored in the critical care setting. One contributing factor to this gap in research is the limited investigation of drug modalities in the intensive care unit (ICU) context, due to the challenges of processing prescription data into the corresponding drug representations and a lack in the comprehensive understanding of these drug representations. This study addresses this gap by proposing a novel approach that leverages patient prescription data as a modality to improve existing models for AKI prediction. We base our research on Electronic Health Record (EHR) data, extracting the relevant patient prescription information and converting it into the selected drug representation for our research, the extended-connectivity fingerprint (ECFP). Furthermore, we adopt a unique multimodal approach, developing machine learning models an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#26368;&#26032;&#30340;&#38754;&#37096;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#26377;&#25928;&#24615;&#24433;&#21709;&#22240;&#32032;&#30340;&#28145;&#20837;&#35265;&#35299;&#65292;&#24182;&#22312;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.04364</link><description>&lt;p&gt;
SoK&#65306;&#38754;&#37096;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
SoK: Facial Deepfake Detectors. (arXiv:2401.04364v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#26368;&#26032;&#30340;&#38754;&#37096;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#26377;&#25928;&#24615;&#24433;&#21709;&#22240;&#32032;&#30340;&#28145;&#20837;&#35265;&#35299;&#65292;&#24182;&#22312;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#36805;&#36895;&#25104;&#20026;&#23545;&#31038;&#20250;&#26500;&#25104;&#28145;&#36828;&#21644;&#20005;&#37325;&#23041;&#32961;&#30340;&#21407;&#22240;&#20043;&#19968;&#65292;&#20027;&#35201;&#30001;&#20110;&#20854;&#26131;&#20110;&#21046;&#20316;&#21644;&#20256;&#25773;&#12290;&#36825;&#31181;&#24773;&#20917;&#21152;&#36895;&#20102;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#22312;&#39564;&#35777;&#26102; heavily &#20381;&#36182;&#23454;&#39564;&#23460;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#22320;&#35753;&#23427;&#20204;&#24212;&#23545;&#26032;&#39062;&#12289;&#26032;&#20852;&#21644;&#23454;&#38469;&#30340;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#12290;&#26412;&#25991;&#23545;&#26368;&#26032;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#36827;&#34892;&#24191;&#27867;&#20840;&#38754;&#30340;&#22238;&#39038;&#21644;&#20998;&#26512;&#65292;&#26681;&#25454;&#20960;&#20010;&#20851;&#38190;&#26631;&#20934;&#23545;&#23427;&#20204;&#36827;&#34892;&#35780;&#20272;&#12290;&#36825;&#20123;&#26631;&#20934;&#23558;&#36825;&#20123;&#26816;&#27979;&#22120;&#20998;&#20026; 4 &#20010;&#39640;&#32423;&#32452;&#21035;&#21644; 13 &#20010;&#32454;&#31890;&#24230;&#23376;&#32452;&#21035;&#65292;&#37117;&#36981;&#24490;&#19968;&#20010;&#32479;&#19968;&#30340;&#26631;&#20934;&#27010;&#24565;&#26694;&#26550;&#12290;&#36825;&#31181;&#20998;&#31867;&#21644;&#26694;&#26550;&#25552;&#20379;&#20102;&#23545;&#24433;&#21709;&#26816;&#27979;&#22120;&#21151;&#25928;&#30340;&#22240;&#32032;&#30340;&#28145;&#20837;&#21644;&#23454;&#29992;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#23545; 16 &#20010;&#20027;&#35201;&#30340;&#26816;&#27979;&#22120;&#22312;&#21508;&#31181;&#26631;&#20934;&#30340;&#25915;&#20987;&#22330;&#26223;&#20013;&#30340;&#26222;&#36866;&#24615;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#40657;&#30418;&#25915;&#20987;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deepfakes have rapidly emerged as a profound and serious threat to society, primarily due to their ease of creation and dissemination. This situation has triggered an accelerated development of deepfake detection technologies. However, many existing detectors rely heavily on lab-generated datasets for validation, which may not effectively prepare them for novel, emerging, and real-world deepfake techniques. In this paper, we conduct an extensive and comprehensive review and analysis of the latest state-of-the-art deepfake detectors, evaluating them against several critical criteria. These criteria facilitate the categorization of these detectors into 4 high-level groups and 13 fine-grained sub-groups, all aligned with a unified standard conceptual framework. This classification and framework offer deep and practical insights into the factors that affect detector efficacy. We assess the generalizability of 16 leading detectors across various standard attack scenarios, including black-bo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#21160;&#24577;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#21487;&#21464;&#24037;&#20917;&#19979;&#26816;&#27979;&#35774;&#22791;&#30340;&#21464;&#28857;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#21464;&#28857;&#26469;&#25552;&#39640;&#21097;&#20313;&#23551;&#21629;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04351</link><description>&lt;p&gt;
&#22312;&#21487;&#21464;&#24037;&#20917;&#19979;&#30340;&#21464;&#28857;&#26816;&#27979;&#32508;&#21512;&#21097;&#20313;&#23551;&#21629;&#20272;&#35745;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Change Point Detection Integrated Remaining Useful Life Estimation Model under Variable Operating Conditions. (arXiv:2401.04351v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#21160;&#24577;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#21487;&#21464;&#24037;&#20917;&#19979;&#26816;&#27979;&#35774;&#22791;&#30340;&#21464;&#28857;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#21464;&#28857;&#26469;&#25552;&#39640;&#21097;&#20313;&#23551;&#21629;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20379;&#36864;&#21270;&#36807;&#31243;&#30340;&#24320;&#22987;&#20449;&#24687;&#65292;&#20581;&#24247;&#29366;&#24577;&#35780;&#20272;&#25104;&#20026;&#21487;&#38752;&#30340;&#22797;&#26434;&#35774;&#22791;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#20272;&#35745;&#30340;&#37325;&#35201;&#21069;&#25552;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26102;&#38388;&#21160;&#24577;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#20010;&#20307;&#35774;&#22791;&#30340;&#21464;&#28857;&#65292;&#21363;&#20351;&#22312;&#21487;&#21464;&#30340;&#24037;&#20917;&#19979;&#65292;&#24182;&#21033;&#29992;&#25152;&#23398;&#21040;&#30340;&#21464;&#28857;&#26469;&#25552;&#39640;RUL&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#31163;&#32447;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#20013;&#65292;&#22810;&#21464;&#37327;&#20256;&#24863;&#22120;&#25968;&#25454;&#34987;&#20998;&#35299;&#65292;&#20197;&#23398;&#20064;&#21487;&#25512;&#24191;&#21644;&#20195;&#34920;&#22810;&#20010;&#24037;&#20917;&#19979;&#27491;&#24120;&#36816;&#34892;&#21160;&#24577;&#30340;&#34701;&#21512;&#26102;&#38388;&#30456;&#20851;&#29305;&#24449;&#12290;&#22522;&#20110;&#36825;&#20123;&#23398;&#21040;&#30340;&#26102;&#38388;&#29305;&#24449;&#65292;&#26500;&#24314;&#30417;&#25511;&#32479;&#35745;&#20540;&#21644;&#25511;&#21046;&#38480;&#21046;&#38408;&#20540;&#20197;&#21160;&#24577;&#22320;&#26816;&#27979;&#35774;&#22791;&#32423;&#21035;&#30340;&#21464;&#28857;&#12290;&#28982;&#21518;&#65292;&#26816;&#27979;&#21040;&#30340;&#21464;&#28857;&#20026;&#35757;&#32451;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#30340;RUL&#20272;&#35745;&#27169;&#22411;&#25552;&#20379;&#36864;&#21270;&#25968;&#25454;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
By informing the onset of the degradation process, health status evaluation serves as a significant preliminary step for reliable remaining useful life (RUL) estimation of complex equipment. This paper proposes a novel temporal dynamics learning-based model for detecting change points of individual devices, even under variable operating conditions, and utilises the learnt change points to improve the RUL estimation accuracy. During offline model development, the multivariate sensor data are decomposed to learn fused temporal correlation features that are generalisable and representative of normal operation dynamics across multiple operating conditions. Monitoring statistics and control limit thresholds for normal behaviour are dynamically constructed from these learnt temporal features for the unsupervised detection of device-level change points. The detected change points then inform the degradation data labelling for training a long short-term memory (LSTM)-based RUL estimation model
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;DP-ZO&#65292;&#19968;&#31181;&#36890;&#36807;&#31169;&#26377;&#21270;&#38646;&#38454;&#20248;&#21270;&#26469;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04343</link><description>&lt;p&gt;
&#31169;&#26377;&#38646;&#38454;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31169;&#26377;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Private Fine-tuning of Large Language Models with Zeroth-order Optimization. (arXiv:2401.04343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04343
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;DP-ZO&#65292;&#19968;&#31181;&#36890;&#36807;&#31169;&#26377;&#21270;&#38646;&#38454;&#20248;&#21270;&#26469;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#33021;&#20250;&#23384;&#22312;&#36829;&#21453;&#38544;&#31169;&#30340;&#39118;&#38505;&#12290;&#24046;&#20998;&#38544;&#31169;&#26159;&#19968;&#31181;&#36890;&#36807;&#24378;&#21046;&#31639;&#27861;&#31283;&#23450;&#24615;&#26469;&#20943;&#36731;&#38544;&#31169;&#39118;&#38505;&#30340;&#26694;&#26550;&#12290;DP-SGD&#21487;&#20197;&#20197;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#24335;&#35757;&#32451;&#20855;&#26377;&#31169;&#26377;&#25968;&#25454;&#30340;&#27169;&#22411;&#65292;&#20294;&#20250;&#24102;&#26469;&#24615;&#33021;&#25439;&#22833;&#21644;&#37325;&#22823;&#24037;&#31243;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DP-ZO&#65292;&#19968;&#31181;&#36890;&#36807;&#31169;&#26377;&#21270;&#38646;&#38454;&#20248;&#21270;&#26469;&#20445;&#25252;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35774;&#35745;&#30340;&#19968;&#20010;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#25105;&#20204;&#20351;&#29992;&#30340;&#38646;&#38454;&#31639;&#27861;SPSA&#20013;&#30340;&#26799;&#24230;&#26041;&#21521;&#22987;&#32456;&#26159;&#38543;&#26426;&#30340;&#65292;&#32780;&#20165;&#20381;&#36182;&#20110;&#31169;&#26377;&#25968;&#25454;&#30340;&#20449;&#24687;&#26159;&#27493;&#38271;&#65292;&#21363;&#19968;&#20010;&#26631;&#37327;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21482;&#38656;&#35201;&#23545;&#26631;&#37327;&#27493;&#38271;&#36827;&#34892;&#38544;&#31169;&#22788;&#29702;&#65292;&#36825;&#26159;&#23384;&#20648;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#12290;DP-ZO&#21487;&#20197;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#22122;&#22768;&#25110;&#39640;&#26031;&#22122;&#22768;&#26469;&#23454;&#29616;&#65292;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#25552;&#20379;&#20102;&#38544;&#31169;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#24378;&#22823;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning large pretrained models on private datasets may run the risk of violating privacy. Differential privacy is a framework for mitigating privacy risks by enforcing algorithmic stability. DP-SGD enables training models with private data in a privacy-preserving manner, but raises new obstacles in the form of performance loss and significant engineering challenges. We introduce DP-ZO, a new method for fine-tuning large language models that preserves the privacy of training data by privatizing zeroth-order optimization. A key insight into the design of our method is that the direction of the gradient in SPSA, the zeroth-order algorithm we use, is always random and the only information that depends on private data is the step size, i.e., a scalar. Therefore, we only need to privatize the scalar step size, which is memory-efficient. DP-ZO, which can be instantiated with either Laplace or Gaussian noise, provides a strong privacy-utility trade-off across different tasks, and model si
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;GPU&#38598;&#32676;&#20998;&#24067;&#24335;&#20803;&#23398;&#20064;&#30340;&#39640;&#24615;&#33021;&#26694;&#26550;G-Meta&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#24182;&#34892;&#24615;&#21644;&#27169;&#22411;&#24182;&#34892;&#24615;&#20197;&#21450;&#35774;&#35745;&#39640;&#25928;&#30340;&#20803;-IO&#27969;&#27700;&#32447;&#65292;&#23454;&#29616;&#20102;&#39640;&#36895;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2401.04338</link><description>&lt;p&gt;
G-Meta: &#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;GPU&#38598;&#32676;&#20998;&#24067;&#24335;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
G-Meta: Distributed Meta Learning in GPU Clusters for Large-Scale Recommender Systems. (arXiv:2401.04338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;GPU&#38598;&#32676;&#20998;&#24067;&#24335;&#20803;&#23398;&#20064;&#30340;&#39640;&#24615;&#33021;&#26694;&#26550;G-Meta&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#24182;&#34892;&#24615;&#21644;&#27169;&#22411;&#24182;&#34892;&#24615;&#20197;&#21450;&#35774;&#35745;&#39640;&#25928;&#30340;&#20803;-IO&#27969;&#27700;&#32447;&#65292;&#23454;&#29616;&#20102;&#39640;&#36895;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#19968;&#20010;&#21517;&#20026;&#20803;&#23398;&#20064;&#30340;&#26032;&#33539;&#24335;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#25512;&#33616;&#27169;&#22411;(DLRM)&#65292;&#24182;&#22312;&#32479;&#35745;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#22312;&#20919;&#21551;&#21160;&#22330;&#26223;&#20013;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31995;&#32479;&#24182;&#27809;&#26377;&#20026;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;DLRM&#27169;&#22411;&#37327;&#36523;&#23450;&#21046;&#65292;&#24182;&#19988;&#22312;GPU&#38598;&#32676;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;&#23384;&#22312;&#20851;&#20110;&#25928;&#29575;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#36825;&#26159;&#22240;&#20026;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#23545;&#20110;&#20803;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#26356;&#26032;&#24490;&#29615;&#24182;&#27809;&#26377;&#36827;&#34892;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24615;&#33021;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;GPU&#38598;&#32676;&#19978;&#36827;&#34892;&#22522;&#20110;&#20248;&#21270;&#30340;&#20803;DLRM&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#65292;&#21363;G-Meta&#12290;&#39318;&#20808;&#65292;G-Meta&#21033;&#29992;&#25968;&#25454;&#24182;&#34892;&#24615;&#21644;&#27169;&#22411;&#24182;&#34892;&#24615;&#65292;&#24182;&#23545;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#36827;&#34892;&#31934;&#24515;&#21327;&#35843;&#65292;&#23454;&#29616;&#39640;&#36895;&#20998;&#24067;&#24335;&#35757;&#32451;&#12290;&#20854;&#27425;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#39640;&#25928;&#25968;&#25454;&#25668;&#20837;&#30340;&#20803;-IO&#27969;&#27700;&#32447;&#65292;&#20197;&#32531;&#35299;&#36755;&#20837;/&#36755;&#20986;&#29942;&#39048;&#12290;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Recently, a new paradigm, meta learning, has been widely applied to Deep Learning Recommendation Models (DLRM) and significantly improves statistical performance, especially in cold-start scenarios. However, the existing systems are not tailored for meta learning based DLRM models and have critical problems regarding efficiency in distributed training in the GPU cluster. It is because the conventional deep learning pipeline is not optimized for two task-specific datasets and two update loops in meta learning. This paper provides a high-performance framework for large-scale training for Optimization-based Meta DLRM models over the \textbf{G}PU cluster, namely \textbf{G}-Meta. Firstly, G-Meta utilizes both data parallelism and model parallelism with careful orchestration regarding computation and communication efficiency, to enable high-speed distributed training. Secondly, it proposes a Meta-IO pipeline for efficient data ingestion to alleviate the I/O bottleneck. Various experimental r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedDEP&#65292;&#29992;&#20110;&#35299;&#20915;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#20256;&#25773;&#19981;&#23436;&#25972;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25216;&#26415;&#35774;&#35745;&#65292;&#21253;&#25324;&#28145;&#24230;&#37051;&#23621;&#29983;&#25104;&#21644;&#39640;&#25928;&#30340;&#31169;&#23494;&#39046;&#22495;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2401.04336</link><description>&lt;p&gt;
&#28145;&#24230;&#39640;&#25928;&#30340;&#31169;&#23494;&#39046;&#22495;&#29983;&#25104;&#29992;&#20110;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Efficient Private Neighbor Generation for Subgraph Federated Learning. (arXiv:2401.04336v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedDEP&#65292;&#29992;&#20110;&#35299;&#20915;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#20256;&#25773;&#19981;&#23436;&#25972;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25216;&#26415;&#35774;&#35745;&#65292;&#21253;&#25324;&#28145;&#24230;&#37051;&#23621;&#29983;&#25104;&#21644;&#39640;&#25928;&#30340;&#31169;&#23494;&#39046;&#22495;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#24040;&#22823;&#22270;&#36890;&#24120;&#20197;&#38750;&#20013;&#24515;&#21270;&#23376;&#22270;&#30340;&#24418;&#24335;&#30001;&#22810;&#20010;&#25968;&#25454;&#25152;&#26377;&#32773;&#20998;&#25955;&#23384;&#20648;&#12290;&#20026;&#20102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#22312;&#19981;&#25439;&#23475;&#25968;&#25454;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#65292;&#32771;&#34385;&#21040;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#65288;subgraph FL&#65289;&#22330;&#26223;&#26159;&#24456;&#33258;&#28982;&#30340;&#65292;&#20854;&#20013;&#27599;&#20010;&#26412;&#22320;&#23458;&#25143;&#31471;&#25345;&#26377;&#25972;&#20010;&#20840;&#23616;&#22270;&#30340;&#23376;&#22270;&#65292;&#20197;&#33719;&#21462;&#20840;&#23616;&#19968;&#33324;&#21270;&#30340;&#22270;&#25366;&#25496;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#30001;&#20110;&#32570;&#23569;&#36328;&#23376;&#22270;&#37051;&#23621;&#32780;&#23548;&#33268;&#30340;&#23616;&#37096;&#23376;&#22270;&#19978;&#30340;&#20449;&#24687;&#20256;&#25773;&#19981;&#23436;&#25972;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20197;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#32570;&#22833;&#37051;&#23621;&#29983;&#25104;&#22120;&#21644;GNN&#30340;&#32852;&#21512;FL&#26469;&#22686;&#21152;&#26412;&#22320;&#37051;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;FL&#30340;&#25928;&#29992;&#24615;&#12289;&#25928;&#29575;&#24615;&#21644;&#38544;&#31169;&#30446;&#26631;&#26041;&#38754;&#23384;&#22312;&#28145;&#23618;&#27425;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedDEP&#26469;&#20840;&#38754;&#35299;&#20915;&#23376;&#22270;FL&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#12290;FedDEP&#21253;&#25324;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25216;&#26415;&#35774;&#35745;&#65306;(1) &#21033;&#29992;&#28508;&#22312;&#32570;&#22833;&#37051;&#23621;&#30340;GNN&#23884;&#20837;&#36827;&#34892;&#28145;&#24230;&#37051;&#23621;&#29983;&#25104;&#65307;(2) Effic...
&lt;/p&gt;
&lt;p&gt;
Behemoth graphs are often fragmented and separately stored by multiple data owners as distributed subgraphs in many realistic applications. Without harming data privacy, it is natural to consider the subgraph federated learning (subgraph FL) scenario, where each local client holds a subgraph of the entire global graph, to obtain globally generalized graph mining models. To overcome the unique challenge of incomplete information propagation on local subgraphs due to missing cross-subgraph neighbors, previous works resort to the augmentation of local neighborhoods through the joint FL of missing neighbor generators and GNNs. Yet their technical designs have profound limitations regarding the utility, efficiency, and privacy goals of FL. In this work, we propose FedDEP to comprehensively tackle these challenges in subgraph FL. FedDEP consists of a series of novel technical designs: (1) Deep neighbor generation through leveraging the GNN embeddings of potential missing neighbors; (2) Effic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23454;&#26045;&#20998;&#25968;&#38454;&#24494;&#31215;&#20998;&#65292;&#27169;&#22411;&#22312;&#29305;&#24449;&#26356;&#26032;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#38271;&#26399;&#35760;&#24518;&#65292;&#23545;&#25239;&#24615;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#20173;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.04331</link><description>&lt;p&gt;
&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#20998;&#25968;&#38454;&#36830;&#32493;&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#65306;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Coupling Graph Neural Networks with Fractional Order Continuous Dynamics: A Robustness Study. (arXiv:2401.04331v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23454;&#26045;&#20998;&#25968;&#38454;&#24494;&#31215;&#20998;&#65292;&#27169;&#22411;&#22312;&#29305;&#24449;&#26356;&#26032;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#38271;&#26399;&#35760;&#24518;&#65292;&#23545;&#25239;&#24615;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#20173;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20005;&#26684;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;(FDE)&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23454;&#26045;&#20998;&#25968;&#38454;Caputo&#23548;&#25968;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#25972;&#25968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#27169;&#22411;&#12290;&#21033;&#29992;&#20998;&#25968;&#38454;&#24494;&#31215;&#20998;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#29305;&#24449;&#26356;&#26032;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#38271;&#26399;&#35760;&#24518;&#65292;&#19982;&#20256;&#32479;&#22270;&#31070;&#32463;ODE&#27169;&#22411;&#20013;&#30340;&#26080;&#35760;&#24518;&#39532;&#23572;&#21487;&#22827;&#26356;&#26032;&#19981;&#21516;&#12290;&#22270;&#31070;&#32463;FDE&#27169;&#22411;&#30456;&#23545;&#20110;&#22270;&#31070;&#32463;ODE&#27169;&#22411;&#22312;&#27809;&#26377;&#25915;&#20987;&#25110;&#25200;&#21160;&#30340;&#29615;&#22659;&#20013;&#24050;&#32463;&#34987;&#35777;&#26126;&#20855;&#26377;&#20248;&#21183;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;ODE&#27169;&#22411;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#24050;&#32463;&#34987;&#39564;&#35777;&#22312;&#23384;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#26102;&#20855;&#26377;&#19968;&#23450;&#30340;&#31283;&#23450;&#24615;&#21644;&#24377;&#24615;&#65292;&#20294;&#22270;&#31070;&#32463;FDE&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#25239;&#24615;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#65292;&#20173;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#31350;&#12290;&#26412;&#25991;&#23545;&#22270;&#31070;&#32463;FDE&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#35814;&#32454;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we rigorously investigate the robustness of graph neural fractional-order differential equation (FDE) models. This framework extends beyond traditional graph neural (integer-order) ordinary differential equation (ODE) models by implementing the time-fractional Caputo derivative. Utilizing fractional calculus allows our model to consider long-term memory during the feature updating process, diverging from the memoryless Markovian updates seen in traditional graph neural ODE models. The superiority of graph neural FDE models over graph neural ODE models has been established in environments free from attacks or perturbations. While traditional graph neural ODE models have been verified to possess a degree of stability and resilience in the presence of adversarial attacks in existing literature, the robustness of graph neural FDE models, especially under adversarial conditions, remains largely unexplored. This paper undertakes a detailed assessment of the robustness of graph 
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#20171;&#32461;&#20102;&#31169;&#20154;&#30495;&#27491;&#27704;&#24658;&#30340;&#24378;&#22823;&#39044;&#27979;&#65288;PEP&#65289;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#25552;&#20379;&#23545;&#39044;&#27979;oracle&#30340;&#40657;&#30418;&#35775;&#38382;&#26469;&#23454;&#29616;&#19981;&#21516;ially private learning&#65292;&#20445;&#25252;&#20102;&#21021;&#22987;&#35757;&#32451;&#38598;&#21644;&#26080;&#23613;&#20998;&#31867;&#26597;&#35810;&#30340;&#38544;&#31169;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#23545;PEP&#23450;&#20041;&#30340;&#20004;&#20010;&#27010;&#24565;&#24615;&#25913;&#36827;&#65292;&#24182;&#23637;&#31034;&#20102;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#26356;&#22909;&#30340;&#26032;&#26500;&#24314;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#21152;&#20837;&#40065;&#26834;&#24615;&#21644;&#31169;&#20154;&#38597;&#20811;&#27604;&#26426;&#21046;&#65292;&#25552;&#20379;&#20102;&#21487;&#38752;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.04311</link><description>&lt;p&gt;
&#31169;&#20154;&#30495;&#27491;&#27704;&#24658;&#30340;&#24378;&#22823;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Private Truly-Everlasting Robust-Prediction. (arXiv:2401.04311v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04311
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#20171;&#32461;&#20102;&#31169;&#20154;&#30495;&#27491;&#27704;&#24658;&#30340;&#24378;&#22823;&#39044;&#27979;&#65288;PEP&#65289;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#25552;&#20379;&#23545;&#39044;&#27979;oracle&#30340;&#40657;&#30418;&#35775;&#38382;&#26469;&#23454;&#29616;&#19981;&#21516;ially private learning&#65292;&#20445;&#25252;&#20102;&#21021;&#22987;&#35757;&#32451;&#38598;&#21644;&#26080;&#23613;&#20998;&#31867;&#26597;&#35810;&#30340;&#38544;&#31169;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#23545;PEP&#23450;&#20041;&#30340;&#20004;&#20010;&#27010;&#24565;&#24615;&#25913;&#36827;&#65292;&#24182;&#23637;&#31034;&#20102;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#26356;&#22909;&#30340;&#26032;&#26500;&#24314;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#21152;&#20837;&#40065;&#26834;&#24615;&#21644;&#31169;&#20154;&#38597;&#20811;&#27604;&#26426;&#21046;&#65292;&#25552;&#20379;&#20102;&#21487;&#38752;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31169;&#20154;&#27704;&#24658;&#39044;&#27979;&#65288;PEP&#65289;&#26159;&#26368;&#36817;&#30001;Naor&#31561;&#20154;[2023]&#25552;&#20986;&#30340;&#19981;&#21516;ially private learning&#27169;&#22411;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#20174;&#19981;&#20844;&#24320;&#21457;&#24067;&#20551;&#35774;&#12290;&#30456;&#21453;&#65292;&#23427;&#25552;&#20379;&#23545;&#8220;&#39044;&#27979;oracle&#8221;&#30340;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#65292;&#35813;oracle&#21487;&#20197;&#39044;&#27979;&#20174;&#22522;&#30784;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#26080;&#23613;&#26410;&#26631;&#35760;&#31034;&#20363;&#30340;&#26631;&#31614;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;PEP&#21487;&#21516;&#26102;&#20445;&#25252;&#21021;&#22987;&#35757;&#32451;&#38598;&#21644;&#26080;&#23613;&#20998;&#31867;&#26597;&#35810;&#30340;&#38544;&#31169;&#12290;&#25105;&#20204;&#23545;PEP&#30340;&#23450;&#20041;&#36827;&#34892;&#20102;&#20004;&#20010;&#27010;&#24565;&#24615;&#25913;&#36827;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#30340;&#26174;&#30528;&#25913;&#36827;&#30340;&#26032;&#26500;&#24314;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#65288;1&#65289;&#40065;&#26834;&#24615;&#65306;PEP&#21482;&#22312;&#25152;&#26377;&#20998;&#31867;&#26597;&#35810;&#37117;&#26159;&#20174;&#27491;&#30830;&#30340;&#22522;&#30784;&#20998;&#24067;&#20013;&#25277;&#21462;&#24471;&#21040;&#26102;&#25165;&#20445;&#35777;&#20934;&#30830;&#24615;&#12290;&#19968;&#20123;&#36229;&#20986;&#20998;&#24067;&#30340;&#26597;&#35810;&#21487;&#33021;&#20250;&#30772;&#22351;&#26410;&#26469;&#26597;&#35810;&#30340;&#39044;&#27979;oracle&#30340;&#26377;&#25928;&#24615;&#65292; &#21363;&#20351;&#36825;&#20123;&#26597;&#35810;&#26159;&#20174;&#27491;&#30830;&#30340;&#20998;&#24067;&#20013;&#25277;&#26679;&#24471;&#21040;&#30340;&#12290;&#25105;&#20204;&#23558;&#40065;&#26834;&#24615;&#19982;&#31169;&#20154;&#38597;&#20811;&#27604;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#26377;&#20445;&#38556;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Private Everlasting Prediction (PEP), recently introduced by Naor et al. [2023], is a model for differentially private learning in which the learner never publicly releases a hypothesis. Instead, it provides black-box access to a "prediction oracle" that can predict the labels of an endless stream of unlabeled examples drawn from the underlying distribution. Importantly, PEP provides privacy both for the initial training set and for the endless stream of classification queries. We present two conceptual modifications to the definition of PEP, as well as new constructions exhibiting significant improvements over prior work. Specifically,  (1) Robustness: PEP only guarantees accuracy provided that all the classification queries are drawn from the correct underlying distribution. A few out-of-distribution queries might break the validity of the prediction oracle for future queries, even for future queries which are sampled from the correct distribution. We incorporate robustness against s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#20449;&#24687;&#35770;&#21407;&#21017;&#30340;&#20027;&#21160;&#23398;&#20064;&#21644;&#20027;&#21160;&#37319;&#26679;&#26041;&#38754;&#30340;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26631;&#31614;&#21644;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.04305</link><description>&lt;p&gt;
&#25512;&#36827;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#21644;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#65306;&#29992;&#20449;&#24687;&#35770;&#30452;&#35273;&#32479;&#19968;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
Advancing Deep Active Learning &amp; Data Subset Selection: Unifying Principles with Information-Theory Intuitions. (arXiv:2401.04305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#20449;&#24687;&#35770;&#21407;&#21017;&#30340;&#20027;&#21160;&#23398;&#20064;&#21644;&#20027;&#21160;&#37319;&#26679;&#26041;&#38754;&#30340;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26631;&#31614;&#21644;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30340;&#26680;&#24515;&#30446;&#26631;&#26159;&#36890;&#36807;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26631;&#31614;&#21644;&#35757;&#32451;&#25928;&#29575;&#26469;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;&#20449;&#24687;&#35770;&#21407;&#21017;&#30340;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#20027;&#21160;&#23398;&#20064;&#21644;&#20027;&#21160;&#37319;&#26679;&#12290;&#20027;&#21160;&#23398;&#20064;&#25552;&#39640;&#20102;&#26631;&#31614;&#25928;&#29575;&#65292;&#32780;&#20027;&#21160;&#37319;&#26679;&#21017;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;&#30417;&#30563;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#26631;&#31614;&#33719;&#21462;&#21487;&#33021;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#65292;&#24182;&#19988;&#35757;&#32451;&#22823;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#22312;&#23398;&#26415;&#30740;&#31350;&#21644;&#8220;&#22823;&#22411;&#31185;&#25216;&#20844;&#21496;&#8221;&#20197;&#22806;&#30340;&#24212;&#29992;&#12290;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#25110;&#32570;&#20047;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#21407;&#21017;&#22522;&#30784;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#35770;&#25991;&#23545;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#30446;&#26631;&#21450;&#20854;&#24212;&#29992;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21147;&#27714;&#36890;&#36807;&#20449;&#24687;&#35770;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#26356;&#20855;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
At its core, this thesis aims to enhance the practicality of deep learning by improving the label and training efficiency of deep learning models. To this end, we investigate data subset selection techniques, specifically active learning and active sampling, grounded in information-theoretic principles. Active learning improves label efficiency, while active sampling enhances training efficiency. Supervised deep learning models often require extensive training with labeled data. Label acquisition can be expensive and time-consuming, and training large models is resource-intensive, hindering the adoption outside academic research and ``big tech.'' Existing methods for data subset selection in deep learning often rely on heuristics or lack a principled information-theoretic foundation. In contrast, this thesis examines several objectives for data subset selection and their applications within deep learning, striving for a more principled approach inspired by information theory. We begin 
&lt;/p&gt;</description></item><item><title>Transformers are not inherently low-pass filters as previously thought and whether they oversmooth or not depends on the eigenspectrum of their update equations. Based on the analysis, a simple way to parameterize the weights of the Transformer update equations is derived, allowing for control over its spectrum and preventing oversmoothing.</title><link>http://arxiv.org/abs/2401.04301</link><description>&lt;p&gt;
&#12298;&#20851;&#20110;Transformer&#36807;&#24230;&#24179;&#28369;&#30340;&#30495;&#30456;&#12299;
&lt;/p&gt;
&lt;p&gt;
Setting the Record Straight on Transformer Oversmoothing. (arXiv:2401.04301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04301
&lt;/p&gt;
&lt;p&gt;
Transformers are not inherently low-pass filters as previously thought and whether they oversmooth or not depends on the eigenspectrum of their update equations. Based on the analysis, a simple way to parameterize the weights of the Transformer update equations is derived, allowing for control over its spectrum and preventing oversmoothing.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#26032;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;Transformer&#26412;&#36136;&#19978;&#26159;&#19968;&#31181;&#20302;&#36890;&#28388;&#27874;&#22120;&#65292;&#20250;&#36880;&#28176;&#36807;&#24230;&#24179;&#28369;&#36755;&#20837;&#25968;&#25454;&#65292;&#38477;&#20302;&#20854;&#34920;&#31034;&#33021;&#21147;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65306;&#22312;&#23384;&#22312;&#36825;&#20010;&#32570;&#38519;&#30340;&#24773;&#20917;&#19979;&#65292;Transformer&#26159;&#22914;&#20309;&#21462;&#24471;&#36825;&#20123;&#25104;&#21151;&#30340;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20107;&#23454;&#19978;Transformer&#24182;&#19981;&#26412;&#36136;&#19978;&#26159;&#19968;&#31181;&#20302;&#36890;&#28388;&#27874;&#22120;&#12290;&#30456;&#21453;&#65292;Transformer&#26159;&#21542;&#36807;&#24230;&#24179;&#28369;&#21462;&#20915;&#20110;&#20854;&#26356;&#26032;&#26041;&#31243;&#30340;&#29305;&#24449;&#35889;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25193;&#23637;&#20102;&#20043;&#21069;&#20851;&#20110;&#36807;&#24230;&#24179;&#28369;&#21644;&#30456;&#20851;&#29616;&#35937;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#35768;&#22810;&#25104;&#21151;&#30340;Transformer&#27169;&#22411;&#20855;&#26377;&#28385;&#36275;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#26465;&#20214;&#30340;&#27880;&#24847;&#21147;&#21644;&#26435;&#37325;&#12290;&#22522;&#20110;&#36825;&#20010;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#23545;Transformer&#26356;&#26032;&#26041;&#31243;&#30340;&#26435;&#37325;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#20351;&#20854;&#21487;&#20197;&#25511;&#21046;&#20854;&#35889;&#29305;&#24615;&#65292;&#30830;&#20445;&#19981;&#20250;&#21457;&#29983;&#36807;&#24230;&#24179;&#28369;&#12290;&#19982;&#20256;&#32479;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#25511;&#21046;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have recently become wildly successful across a diverse set of domains. At the same time, recent work has shown that Transformers are inherently low-pass filters that gradually oversmooth the inputs, reducing the expressivity of their representations. A natural question is: How can Transformers achieve these successes given this shortcoming? In this work we show that in fact Transformers are not inherently low-pass filters. Instead, whether Transformers oversmooth or not depends on the eigenspectrum of their update equations. Our analysis extends prior work in oversmoothing and in the closely-related phenomenon of rank collapse. We show that many successful Transformer models have attention and weights which satisfy conditions that avoid oversmoothing. Based on this analysis, we derive a simple way to parameterize the weights of the Transformer update equations that allows for control over its spectrum, ensuring that oversmoothing does not occur. Compared to a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#23485;&#32780;&#28145;&#30340;ReLU&#31070;&#32463;&#32593;&#32476;&#21644;&#36923;&#36753;&#25439;&#22833;&#35757;&#32451;&#30340;&#20998;&#31867;&#35268;&#21017;&#20855;&#26377;&#26222;&#36866;&#19968;&#33268;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#31867;&#27010;&#29575;&#27979;&#24230;&#26465;&#20214;&#19979;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#22120;&#23454;&#29616;&#26497;&#23567;&#26497;&#38480;&#25910;&#25947;&#36895;&#29575;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2401.04286</link><description>&lt;p&gt;
&#23485;&#32780;&#28145;&#30340;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#36866;&#19968;&#33268;&#24615;&#20197;&#21450;Kolmogorov-Donoho&#26368;&#20248;&#20989;&#25968;&#31867;&#30340;&#26497;&#23567;&#26497;&#38480;&#25910;&#25947;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Universal Consistency of Wide and Deep ReLU Neural Networks and Minimax Optimal Convergence Rates for Kolmogorov-Donoho Optimal Function Classes. (arXiv:2401.04286v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#23485;&#32780;&#28145;&#30340;ReLU&#31070;&#32463;&#32593;&#32476;&#21644;&#36923;&#36753;&#25439;&#22833;&#35757;&#32451;&#30340;&#20998;&#31867;&#35268;&#21017;&#20855;&#26377;&#26222;&#36866;&#19968;&#33268;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#31867;&#27010;&#29575;&#27979;&#24230;&#26465;&#20214;&#19979;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#22120;&#23454;&#29616;&#26497;&#23567;&#26497;&#38480;&#25910;&#25947;&#36895;&#29575;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#20808;&#25193;&#23637;&#20102;FL93&#30340;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;&#20102;&#22522;&#20110;&#23485;&#32780;&#28145;&#30340;ReLU&#31070;&#32463;&#32593;&#32476;&#21644;&#36923;&#36753;&#25439;&#22833;&#35757;&#32451;&#30340;&#20998;&#31867;&#35268;&#21017;&#30340;&#26222;&#36866;&#19968;&#33268;&#24615;&#12290;&#19982;FL93&#20013;&#20998;&#35299;&#20272;&#35745;&#21644;&#32463;&#39564;&#35823;&#24046;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#26681;&#25454;&#19968;&#20010;&#24191;&#27867;&#30340;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#25554;&#20540;&#20219;&#24847;&#25968;&#37327;&#30340;&#28857;&#30340;&#35266;&#23519;&#65292;&#30452;&#25509;&#20998;&#26512;&#20998;&#31867;&#39118;&#38505;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#31867;&#27010;&#29575;&#27979;&#24230;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#22120;&#23454;&#29616;&#20102;&#26497;&#23567;&#26497;&#38480;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#28304;&#20110;&#23454;&#36341;&#32773;&#35266;&#23519;&#21040;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#34987;&#35757;&#32451;&#25104;&#36798;&#21040;0&#35757;&#32451;&#35823;&#24046;&#30340;&#20107;&#23454;&#65292;&#36825;&#20063;&#26159;&#25105;&#20204;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#20381;&#36182;&#20110;&#26368;&#36817;&#22312;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#21644;&#28145;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#36895;&#29575;&#26041;&#38754;&#30340;&#21457;&#23637;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#24863;&#20852;&#36259;&#20989;&#25968;&#31867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we first extend the result of FL93 and prove universal consistency for a classification rule based on wide and deep ReLU neural networks trained on the logistic loss. Unlike the approach in FL93 that decomposes the estimation and empirical error, we directly analyze the classification risk based on the observation that a realization of a neural network that is wide enough is capable of interpolating an arbitrary number of points. Secondly, we give sufficient conditions for a class of probability measures under which classifiers based on neural networks achieve minimax optimal rates of convergence. Our result is motivated from the practitioner's observation that neural networks are often trained to achieve 0 training error, which is the case for our proposed neural network classifiers. Our proofs hinge on recent developments in empirical risk minimization and on approximation rates of deep ReLU neural networks for various function classes of interest. Applications to clas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20108;&#20998;&#31867;&#38382;&#39064;&#30340;&#24555;&#36895;&#22270;&#25628;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#20248;&#21270;&#21644;&#20943;&#23569;&#30452;&#26041;&#22270;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#21306;&#20998;&#32467;&#26524;&#12290;&#35813;&#31639;&#27861;&#22312;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#24212;&#29992;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#30495;&#27491;&#20363;&#24182;&#20943;&#23569;&#20102;&#20551;&#27491;&#20363;&#12290;</title><link>http://arxiv.org/abs/2401.04282</link><description>&lt;p&gt;
&#24555;&#36895;&#22270;&#25628;&#32034;&#31639;&#27861;&#19982;&#21160;&#24577;&#20248;&#21270;&#21644;&#20943;&#23569;&#30452;&#26041;&#22270;&#29992;&#20110;&#20108;&#20998;&#31867;&#38382;&#39064;&#30340;&#21306;&#20998; (arXiv:2401.04282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
A Fast Graph Search Algorithm with Dynamic Optimization and Reduced Histogram for Discrimination of Binary Classification Problem. (arXiv:2401.04282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20108;&#20998;&#31867;&#38382;&#39064;&#30340;&#24555;&#36895;&#22270;&#25628;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#20248;&#21270;&#21644;&#20943;&#23569;&#30452;&#26041;&#22270;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#21306;&#20998;&#32467;&#26524;&#12290;&#35813;&#31639;&#27861;&#22312;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#24212;&#29992;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#30495;&#27491;&#20363;&#24182;&#20943;&#23569;&#20102;&#20551;&#27491;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22270;&#25628;&#32034;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#20108;&#20998;&#31867;&#38382;&#39064;&#30340;&#26368;&#20248;&#21306;&#20998;&#36335;&#24452;&#12290;&#30446;&#26631;&#20989;&#25968;&#34987;&#23450;&#20041;&#20026;&#30495;&#27491;&#20363;&#65288;TP&#65289;&#21644;&#20551;&#27491;&#20363;&#65288;FP&#65289;&#20043;&#38388;&#21464;&#24322;&#24615;&#30340;&#24046;&#24322;&#12290;&#23427;&#20351;&#29992;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#65288;DFS&#65289;&#31639;&#27861;&#26469;&#23547;&#25214;&#33258;&#39030;&#21521;&#19979;&#30340;&#21306;&#20998;&#36335;&#24452;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#20248;&#21270;&#36807;&#31243;&#65292;&#20197;&#22312;&#19978;&#23618;&#20248;&#21270;TP&#65292;&#28982;&#21518;&#22312;&#19979;&#23618;&#20943;&#23569;FP&#12290;&#20026;&#20102;&#21152;&#36895;&#35745;&#31639;&#36895;&#24230;&#24182;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21487;&#21464;&#31665;&#22823;&#23567;&#30340;&#20943;&#23567;&#30452;&#26041;&#22270;&#31639;&#27861;&#65292;&#32780;&#19981;&#26159;&#24490;&#29615;&#36941;&#21382;&#25152;&#26377;&#25968;&#25454;&#28857;&#65292;&#20197;&#25214;&#21040;&#21306;&#20998;&#30340;&#29305;&#24449;&#38408;&#20540;&#12290;&#35813;&#31639;&#27861;&#24212;&#29992;&#20110;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#27169;&#22411;&#19978;&#65292;&#29992;&#20110;&#39044;&#27979;&#19968;&#20010;&#20154;&#26159;&#21542;&#20581;&#24247;&#12290;&#23427;&#26174;&#33879;&#25552;&#39640;&#20102;SVM&#32467;&#26524;&#30340;TP&#24182;&#20943;&#23569;&#20102;FP &#65288;&#20363;&#22914;&#65292;FP&#20943;&#23569;&#20102;90%&#65292;&#32780;TP&#20165;&#25439;&#22833;&#20102;5%&#65289;&#12290;&#22270;&#25628;&#32034;&#33258;&#21160;&#29983;&#25104;&#20102;39&#20010;&#25490;&#24207;&#30340;&#21306;&#20998;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study develops a graph search algorithm to find the optimal discrimination path for the binary classification problem. The objective function is defined as the difference of variations between the true positive (TP) and false positive (FP). It uses the depth first search (DFS) algorithm to find the top-down paths for discrimination. It proposes a dynamic optimization procedure to optimize TP at the upper levels and then reduce FP at the lower levels. To accelerate computing speed with improving accuracy, it proposes a reduced histogram algorithm with variable bin size instead of looping over all data points, to find the feature threshold of discrimination. The algorithm is applied on top of a Support Vector Machine (SVM) model for a binary classification problem on whether a person is fit or unfit. It significantly improves TP and reduces FP of the SVM results (e.g., reduced FP by 90% with a loss of only\ 5% TP). The graph search auto-generates 39 ranked discrimination paths withi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#21160;&#24577;&#22270;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#26041;&#27861;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#28857;&#30340;&#33410;&#28857;&#24230;&#65292;&#24182;&#32467;&#21512;&#36890;&#37327;&#24179;&#34913;&#20998;&#26512;&#26041;&#27861;&#33719;&#24471;&#26410;&#26469;&#22270;&#30340;&#32467;&#26500;&#65292;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#29992;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04280</link><description>&lt;p&gt;
&#39044;&#27979;&#21160;&#24577;&#22270;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Predicting the structure of dynamic graphs. (arXiv:2401.04280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#21160;&#24577;&#22270;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#26041;&#27861;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#28857;&#30340;&#33410;&#28857;&#24230;&#65292;&#24182;&#32467;&#21512;&#36890;&#37327;&#24179;&#34913;&#20998;&#26512;&#26041;&#27861;&#33719;&#24471;&#26410;&#26469;&#22270;&#30340;&#32467;&#26500;&#65292;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#29992;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#23884;&#20837;&#12289;&#24402;&#32435;&#21644;&#22686;&#37327;&#23398;&#20064;&#26377;&#21161;&#20110;&#39044;&#27979;&#20219;&#21153;&#65292;&#22914;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#20174;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#20013;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27493;&#30340;&#22270;&#32467;&#26500;&#65292;&#20801;&#35768;&#26377;&#26032;&#33410;&#28857;&#65292;&#24182;&#27809;&#26377;&#21463;&#21040;&#22826;&#22810;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36825;&#26679;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#26041;&#27861;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#28857;&#30340;&#33410;&#28857;&#24230;&#65292;&#24182;&#23558;&#20854;&#19982;&#36890;&#37327;&#24179;&#34913;&#20998;&#26512;&#65288;&#19968;&#31181;&#22312;&#29983;&#29289;&#21270;&#23398;&#20013;&#20351;&#29992;&#30340;&#32447;&#24615;&#35268;&#21010;&#26041;&#27861;&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#33719;&#24471;&#26410;&#26469;&#22270;&#30340;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#21442;&#25968;&#20540;&#30340;&#39044;&#27979;&#22270;&#20998;&#24067;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic graph embeddings, inductive and incremental learning facilitate predictive tasks such as node classification and link prediction. However, predicting the structure of a graph at a future time step from a time series of graphs, allowing for new nodes has not gained much attention. In this paper, we present such an approach. We use time series methods to predict the node degree at future time points and combine it with flux balance analysis -- a linear programming method used in biochemistry -- to obtain the structure of future graphs. Furthermore, we explore the predictive graph distribution for different parameter values. We evaluate this method using synthetic and real datasets and demonstrate its utility and applicability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24191;&#27867;&#35780;&#20272;28&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;&#23545;&#27604;&#20102;&#27880;&#24847;&#21147;&#21644;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#19982;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#38656;&#35201;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#36827;&#34892;&#35780;&#27979;&#65292;&#20197;&#35299;&#20915;&#24615;&#33021;&#24046;&#36317;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.04266</link><description>&lt;p&gt;
Attention versus Contrastive Learning of Tabular Data -- A Data-centric Benchmarking. (arXiv:2401.04266v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Attention versus Contrastive Learning of Tabular Data -- A Data-centric Benchmarking. (arXiv:2401.04266v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24191;&#27867;&#35780;&#20272;28&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;&#23545;&#27604;&#20102;&#27880;&#24847;&#21147;&#21644;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#19982;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#38656;&#35201;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#36827;&#34892;&#35780;&#27979;&#65292;&#20197;&#35299;&#20915;&#24615;&#33021;&#24046;&#36317;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#25104;&#21151;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#30456;&#27604;&#24182;&#26410;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;&#36825;&#31181;&#24615;&#33021;&#24046;&#36317;&#20984;&#26174;&#20102;&#23545;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#22788;&#29702;&#21644;&#35780;&#27979;&#30340;&#38656;&#27714;&#12290;&#26368;&#36817;&#65292;&#27880;&#24847;&#21147;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#31361;&#30772;&#24050;&#32463;&#25913;&#21464;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20808;&#36827;&#30340;&#28145;&#24230;&#27169;&#22411;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;&#38750;&#24120;&#26377;&#38480;&#65292;&#20165;&#20351;&#29992;&#20102;&#23569;&#25968;&#20960;&#20010;&#20855;&#26377;&#38750;&#24120;&#22823;&#30340;&#26679;&#26412;&#25968;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#19982;&#26377;&#38480;&#25968;&#37327;&#30340;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#35780;&#27979;&#21518;&#25253;&#21578;&#20102;&#28151;&#21512;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#25991;&#29486;&#20013;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#24322;&#36136;&#24615;&#21644;&#22522;&#32447;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#21487;&#33021;&#20250;&#23545;&#35780;&#27979;&#32467;&#26524;&#20135;&#29983;&#20559;&#24046;&#12290;&#26412;&#25991;&#22312;28&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#65288;14&#20010;&#26131;&#20110;&#20998;&#31867;&#21644;14&#20010;&#38590;&#20197;&#20998;&#31867;&#65289;&#19978;&#24191;&#27867;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#27880;&#24847;&#21147;&#21644;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#19982;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#23545;&#27604;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite groundbreaking success in image and text learning, deep learning has not achieved significant improvements against traditional machine learning (ML) when it comes to tabular data. This performance gap underscores the need for data-centric treatment and benchmarking of learning algorithms. Recently, attention and contrastive learning breakthroughs have shifted computer vision and natural language processing paradigms. However, the effectiveness of these advanced deep models on tabular data is sparsely studied using a few data sets with very large sample sizes, reporting mixed findings after benchmarking against a limited number of baselines. We argue that the heterogeneity of tabular data sets and selective baselines in the literature can bias the benchmarking outcomes. This article extensively evaluates state-of-the-art attention and contrastive learning methods on a wide selection of 28 tabular data sets (14 easy and 14 hard-to-classify) against traditional deep and machine le
&lt;/p&gt;</description></item><item><title>&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#34987;&#36190;&#36175;&#20854;&#25429;&#25417;&#25968;&#25454;&#20013;&#22797;&#26434;&#24418;&#29366;&#21644;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#20854;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#24471;&#21040;&#39564;&#35777;&#65292;&#20294;&#22312;&#29305;&#23450;&#23454;&#39564;&#20013;&#26410;&#33021;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#26041;&#27861;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04250</link><description>&lt;p&gt;
&#35299;&#37322;&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Explaining the Power of Topological Data Analysis in Graph Machine Learning. (arXiv:2401.04250v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04250
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#34987;&#36190;&#36175;&#20854;&#25429;&#25417;&#25968;&#25454;&#20013;&#22797;&#26434;&#24418;&#29366;&#21644;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#20854;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#24471;&#21040;&#39564;&#35777;&#65292;&#20294;&#22312;&#29305;&#23450;&#23454;&#39564;&#20013;&#26410;&#33021;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#26041;&#27861;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#22240;&#20854;&#33021;&#22815;&#25429;&#25417;&#25968;&#25454;&#20013;&#22797;&#26434;&#30340;&#24418;&#29366;&#21644;&#32467;&#26500;&#32780;&#21463;&#21040;&#30740;&#31350;&#20154;&#21592;&#30340;&#36190;&#35465;&#12290;TDA&#34987;&#35748;&#20026;&#21487;&#20197;&#22788;&#29702;&#22122;&#22768;&#21644;&#39640;&#32500;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#20854;&#21487;&#35299;&#37322;&#24615;&#34987;&#35748;&#20026;&#26377;&#21161;&#20110;&#30452;&#35266;&#29702;&#35299;&#27169;&#22411;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#26377;&#20851;TDA&#30340;&#33021;&#21147;&#21644;&#20351;&#29992;&#20215;&#20540;&#30340;&#22768;&#26126;&#20165;&#22312;&#23558;&#22522;&#20110;TDA&#30340;&#27169;&#22411;&#19982;&#20854;&#20182;&#22270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#36827;&#34892;&#27604;&#36739;&#30340;&#24212;&#29992;&#39046;&#22495;&#20013;&#37096;&#20998;&#22320;&#24471;&#21040;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#32454;&#33268;&#30340;&#23454;&#39564;&#23545;TDA&#30340;&#22768;&#26126;&#36827;&#34892;&#20102;&#20840;&#38754;&#26816;&#39564;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#20248;&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#23454;&#20102;TDA&#23545;&#31163;&#32676;&#28857;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#19982;&#25903;&#25345;&#32773;&#30340;&#35266;&#28857;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#25105;&#20204;&#30340;&#29305;&#23450;&#23454;&#39564;&#20013;&#65292;TDA&#24182;&#27809;&#26377;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#26041;&#27861;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#21516;&#26102;&#36824;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;&#22270;&#29305;&#24615;&#30456;&#20851;&#30340;&#29616;&#35937;&#65292;&#20363;&#22914;&#23567;&#23610;&#24230;&#19990;&#30028;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topological Data Analysis (TDA) has been praised by researchers for its ability to capture intricate shapes and structures within data. TDA is considered robust in handling noisy and high-dimensional datasets, and its interpretability is believed to promote an intuitive understanding of model behavior. However, claims regarding the power and usefulness of TDA have only been partially tested in application domains where TDA-based models are compared to other graph machine learning approaches, such as graph neural networks. We meticulously test claims on TDA through a comprehensive set of experiments and validate their merits. Our results affirm TDA's robustness against outliers and its interpretability, aligning with proponents' arguments. However, we find that TDA does not significantly enhance the predictive power of existing methods in our specific experiments, while incurring significant computational costs. We investigate phenomena related to graph characteristics, such as small di
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#20307;&#26550;&#26500;&#26469;&#39640;&#25928;&#22320;&#23398;&#20064;&#34507;&#30333;&#36136;&#30340;&#26500;&#35937;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;2-Wasserstein&#25439;&#22833;&#24179;&#28369;&#22320;&#23454;&#29616;&#20102;&#22823;&#20998;&#23376;&#30340;&#29627;&#23572;&#20857;&#26364;&#21457;&#29983;&#22120;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2401.04246</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#24402;&#19968;&#21270;&#27969;&#20351;&#24471;&#29627;&#23572;&#20857;&#26364;&#21457;&#29983;&#22120;&#33021;&#22815;&#27169;&#25311;&#22823;&#20998;&#23376;&#29289;&#36136;
&lt;/p&gt;
&lt;p&gt;
Scalable Normalizing Flows Enable Boltzmann Generators for Macromolecules. (arXiv:2401.04246v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04246
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#20307;&#26550;&#26500;&#26469;&#39640;&#25928;&#22320;&#23398;&#20064;&#34507;&#30333;&#36136;&#30340;&#26500;&#35937;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;2-Wasserstein&#25439;&#22833;&#24179;&#28369;&#22320;&#23454;&#29616;&#20102;&#22823;&#20998;&#23376;&#30340;&#29627;&#23572;&#20857;&#26364;&#21457;&#29983;&#22120;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#30340;&#29627;&#23572;&#20857;&#26364;&#20998;&#24067;&#20026;&#20854;&#25152;&#26377;&#21151;&#33021;&#29366;&#24577;&#25552;&#20379;&#20102;&#19968;&#20010;&#36335;&#32447;&#22270;&#12290;&#24402;&#19968;&#21270;&#27969;&#26159;&#27169;&#25311;&#36825;&#31181;&#20998;&#24067;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#65292;&#20294;&#26159;&#24403;&#21069;&#30340;&#26041;&#27861;&#23545;&#20110;&#20856;&#22411;&#30340;&#33647;&#29289;&#38774;&#26631;&#26469;&#35828;&#26159;&#19981;&#21487;&#35299;&#30340;&#65307;&#30001;&#20110;&#31995;&#32479;&#30340;&#22823;&#23567;&#12289;&#20998;&#23376;&#20869;&#37096;&#21183;&#33021;&#30340;&#24322;&#36136;&#24615;&#21644;&#36828;&#31243;&#30456;&#20114;&#20316;&#29992;&#30340;&#23384;&#22312;&#65292;&#23427;&#20204;&#22312;&#35745;&#31639;&#19978;&#26159;&#19981;&#21487;&#35299;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27969;&#20307;&#26550;&#26500;&#65292;&#21033;&#29992;&#20998;&#35010;&#36890;&#36947;&#21644;&#38376;&#25511;&#27880;&#24847;&#21147;&#26469;&#39640;&#25928;&#22320;&#23398;&#20064;&#30001;&#20869;&#37096;&#22352;&#26631;&#23450;&#20041;&#30340;&#34507;&#30333;&#36136;&#30340;&#26500;&#35937;&#20998;&#24067;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#21033;&#29992;2-Wasserstein&#25439;&#22833;&#65292;&#21487;&#20197;&#24179;&#28369;&#22320;&#20174;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#36807;&#28193;&#21040;&#22522;&#20110;&#33021;&#37327;&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#23454;&#29616;&#22823;&#20998;&#23376;&#30340;&#29627;&#23572;&#20857;&#26364;&#21457;&#29983;&#22120;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;HP35(nle-nle)&#32500;&#26519;&#22836;&#37096;&#29255;&#27573;&#21644;&#34507;&#30333;&#36136;G&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21644;&#35757;&#32451;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26631;&#20934;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#26080;&#27861;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Boltzmann distribution of a protein provides a roadmap to all of its functional states. Normalizing flows are a promising tool for modeling this distribution, but current methods are intractable for typical pharmacological targets; they become computationally intractable due to the size of the system, heterogeneity of intra-molecular potential energy, and long-range interactions. To remedy these issues, we present a novel flow architecture that utilizes split channels and gated attention to efficiently learn the conformational distribution of proteins defined by internal coordinates. We show that by utilizing a 2-Wasserstein loss, one can smooth the transition from maximum likelihood training to energy-based training, enabling the training of Boltzmann Generators for macromolecules. We evaluate our model and training strategy on villin headpiece HP35(nle-nle), a 35-residue subdomain, and protein G, a 56-residue protein. We demonstrate that standard architectures and training strate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#37197;&#32622;&#20248;&#21270;&#27714;&#35299;&#22120;&#12290;&#36890;&#36807;&#23398;&#20064;&#24050;&#35299;&#20915;&#30340;&#23454;&#20363;&#21644;&#37197;&#32622;&#65292;&#26500;&#24314;&#20102;&#24615;&#33021;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;&#25968;&#23398;&#35268;&#21010;&#26469;&#23547;&#25214;&#32473;&#23450;&#23454;&#20363;&#30340;&#26368;&#20339;&#27714;&#35299;&#22120;&#37197;&#32622;&#12290;</title><link>http://arxiv.org/abs/2401.04237</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#25968;&#23398;&#35268;&#21010;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#37197;&#32622;&#20248;&#21270;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
A learning-based mathematical programming formulation for the automatic configuration of optimization solvers. (arXiv:2401.04237v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04237
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#37197;&#32622;&#20248;&#21270;&#27714;&#35299;&#22120;&#12290;&#36890;&#36807;&#23398;&#20064;&#24050;&#35299;&#20915;&#30340;&#23454;&#20363;&#21644;&#37197;&#32622;&#65292;&#26500;&#24314;&#20102;&#24615;&#33021;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;&#25968;&#23398;&#35268;&#21010;&#26469;&#23547;&#25214;&#32473;&#23450;&#23454;&#20363;&#30340;&#26368;&#20339;&#27714;&#35299;&#22120;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#32473;&#23450;&#23454;&#20363;&#30340;&#27714;&#35299;&#22120;&#37197;&#32622;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#32452;&#24050;&#35299;&#20915;&#30340;&#23454;&#20363;&#21644;&#37197;&#32622;&#26469;&#23398;&#20064;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#20989;&#25968;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#35268;&#21010;&#65292;&#20854;&#20013;&#30446;&#26631;/&#32422;&#26463;&#26126;&#30830;&#22320;&#32534;&#30721;&#20102;&#23398;&#20064;&#21040;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#22312;&#26410;&#30693;&#23454;&#20363;&#21040;&#26469;&#26102;&#36827;&#34892;&#27714;&#35299;&#65292;&#20197;&#25214;&#21040;&#22522;&#20110;&#24615;&#33021;&#20989;&#25968;&#30340;&#26368;&#20339;&#27714;&#35299;&#22120;&#37197;&#32622;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#23558;&#37197;&#32622;&#38598;&#26597;&#25214;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#25968;&#23398;&#31243;&#24207;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;a)&#23545;&#37197;&#32622;&#24378;&#21046;&#25191;&#34892;&#20381;&#36182;&#21644;&#20860;&#23481;&#24615;&#32422;&#26463;&#65292;&#24182;b)&#21033;&#29992;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21270;&#24037;&#20855;&#39640;&#25928;&#27714;&#35299;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a methodology, based on machine learning and optimization, for selecting a solver configuration for a given instance. First, we employ a set of solved instances and configurations in order to learn a performance function of the solver. Secondly, we formulate a mixed-integer nonlinear program where the objective/constraints explicitly encode the learnt information, and which we solve, upon the arrival of an unknown instance, to find the best solver configuration for that instance, based on the performance function. The main novelty of our approach lies in the fact that the configuration set search problem is formulated as a mathematical program, which allows us to a) enforce hard dependence and compatibility constraints on the configurations, and b) solve it efficiently with off-the-shelf optimization tools.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#31354;&#38388;&#29289;&#20307;&#30340;&#23494;&#24230;&#20998;&#24067;&#65292;&#20197;&#24212;&#23545;&#20302;&#22320;&#29699;&#36712;&#36947;&#30340;&#25317;&#22622;&#24182;&#25552;&#21319;&#23545;&#31354;&#38388;&#29615;&#22659;&#30340;&#21487;&#25345;&#32493;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04212</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#27979;&#31354;&#38388;&#29289;&#20307;&#23494;&#24230;&#20998;&#24067;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards a Machine Learning-Based Approach to Predict Space Object Density Distributions. (arXiv:2401.04212v1 [physics.space-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04212
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#31354;&#38388;&#29289;&#20307;&#30340;&#23494;&#24230;&#20998;&#24067;&#65292;&#20197;&#24212;&#23545;&#20302;&#22320;&#29699;&#36712;&#36947;&#30340;&#25317;&#22622;&#24182;&#25552;&#21319;&#23545;&#31354;&#38388;&#29615;&#22659;&#30340;&#21487;&#25345;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#36896;&#31354;&#38388;&#29289;&#20307;&#25968;&#37327;&#30340;&#24555;&#36895;&#22686;&#21152;&#65292;&#20302;&#22320;&#29699;&#36712;&#36947;&#65288;LEO&#65289;&#38754;&#20020;&#30528;&#20005;&#37325;&#30340;&#25317;&#22622;&#38382;&#39064;&#65292;&#36825;&#32473;&#31354;&#38388;&#36816;&#33829;&#21830;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#24182;&#23041;&#32961;&#30528;&#31354;&#38388;&#29615;&#22659;&#30340;&#21487;&#25345;&#32493;&#24615;&#12290;&#30446;&#21069;&#29992;&#20110;&#30740;&#31350;&#36825;&#19968;&#28436;&#21464;&#30340;&#27169;&#22411;&#34429;&#28982;&#35814;&#32454;&#65292;&#20294;&#35745;&#31639;&#37327;&#22823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#20316;&#20026;MIT&#36712;&#36947;&#33021;&#21147;&#24037;&#20855;&#65288;MOCAT&#65289;&#30340;&#24310;&#20280;&#12290;&#36825;&#20010;&#20808;&#36827;&#27169;&#22411;&#26088;&#22312;&#21152;&#36895;ASO&#23494;&#24230;&#20998;&#24067;&#30340;&#20256;&#25773;&#65292;&#24182;&#19988;&#23427;&#30340;&#35757;&#32451;&#22522;&#20110;&#30001;&#19968;&#20010;&#24314;&#31435;&#22312;&#24050;&#26377;&#31934;&#30830;&#27169;&#22411;&#30340;&#31354;&#38388;&#29615;&#22659;&#28436;&#21270;&#27169;&#25311;&#29983;&#25104;&#30340;&#25968;&#30334;&#20010;&#23454;&#20363;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#20316;&#20026;ASO&#20256;&#25773;&#30340;&#33391;&#22909;&#20505;&#36873;&#65292;&#24182;&#31649;&#29702;&#25968;&#25454;&#30340;&#39640;&#32500;&#24230;&#29305;&#28857;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#22312;&#38271;&#26399;&#39044;&#27979;&#22330;&#26223;&#65288;&#32422;100&#24180;&#65289;&#36827;&#34892;&#23454;&#39564;&#65292;&#20998;&#26512;&#24615;&#33021;&#19979;&#38477;&#30340;&#21407;&#22240;&#21644;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid increase in the number of Anthropogenic Space Objects (ASOs), Low Earth Orbit (LEO) is facing significant congestion, thereby posing challenges to space operators and risking the viability of the space environment for varied uses. Current models for examining this evolution, while detailed, are computationally demanding. To address these issues, we propose a novel machine learning-based model, as an extension of the MIT Orbital Capacity Tool (MOCAT). This advanced model is designed to accelerate the propagation of ASO density distributions, and it is trained on hundreds of simulations generated by an established and accurate model of the space environment evolution. We study how different deep learning-based solutions can potentially be good candidates for ASO propagation and manage the high-dimensionality of the data. To assess the model's capabilities, we conduct experiments in long term forecasting scenarios (around 100 years), analyze how and why the performance degr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25351;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#29615;&#22659;&#19979;&#36890;&#36807;&#22909;&#22855;&#24515;&#21644;&#29109;&#39537;&#21160;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.04198</link><description>&lt;p&gt;
&#26080;&#25351;&#23548;&#19979;&#22810;&#29615;&#22659;&#20013;&#30340;&#22909;&#22855;&#24515;&#19982;&#29109;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Curiosity &amp; Entropy Driven Unsupervised RL in Multiple Environments. (arXiv:2401.04198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25351;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#29615;&#22659;&#19979;&#36890;&#36807;&#22909;&#22855;&#24515;&#21644;&#29109;&#39537;&#21160;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30340;&#20316;&#32773;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;alpha-MEPOL&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#29615;&#22659;&#19979;&#30340;&#26080;&#25351;&#23548;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#20182;&#20204;&#36890;&#36807;&#20351;&#29992;&#25972;&#20010;&#29615;&#22659;&#31867;&#21035;&#30340;&#20132;&#20114;&#26469;&#39044;&#35757;&#32451;&#19968;&#20010;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#28982;&#21518;&#21033;&#29992;&#30417;&#30563;&#26469;&#23545;&#21508;&#31181;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#21407;&#22987;&#24037;&#20316;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#26088;&#22312;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#20027;&#35201;&#25552;&#20986;&#24182;&#23454;&#39564;&#20102;&#20116;&#20010;&#26032;&#30340;&#20462;&#25913;&#26041;&#27861;&#65306;&#20351;&#29992;&#22522;&#20110;&#29109;&#30340;&#27010;&#29575;&#20998;&#24067;&#26469;&#37319;&#26679;&#36712;&#36857;&#65292;&#21160;&#24577;alpha&#65292;&#26356;&#39640;&#30340;KL&#25955;&#24230;&#38408;&#20540;&#65292;&#20197;&#22909;&#22855;&#24515;&#39537;&#21160;&#30340;&#25506;&#32034;&#65292;&#20197;&#21450;&#22522;&#20110;&#22909;&#22855;&#24515;&#30340;alpha&#20998;&#20301;&#25968;&#37319;&#26679;&#12290;&#21160;&#24577;alpha&#21644;&#26356;&#39640;&#30340;KL&#25955;&#24230;&#38408;&#20540;&#37117;&#30456;&#23545;&#20110;&#26089;&#26399;&#24037;&#20316;&#30340;&#22522;&#32447;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#24403;&#26679;&#26412;&#31354;&#38388;&#36739;&#23567;&#26102;&#65292;PDF&#37319;&#26679;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#25913;&#36827;&#65292;&#22240;&#20026;&#23427;&#19982;&#22522;&#32447;&#26041;&#27861;&#36817;&#20284;&#31561;&#20215;&#12290;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;&#20462;&#25913;&#26041;&#27861;&#30340;&#28155;&#21152;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The authors of 'Unsupervised Reinforcement Learning in Multiple environments' propose a method, alpha-MEPOL, to tackle unsupervised RL across multiple environments. They pre-train a task-agnostic exploration policy using interactions from an entire environment class and then fine-tune this policy for various tasks using supervision. We expanded upon this work, with the goal of improving performance. We primarily propose and experiment with five new modifications to the original work: sampling trajectories using an entropy-based probability distribution, dynamic alpha, higher KL Divergence threshold, curiosity-driven exploration, and alpha-percentile sampling on curiosity. Dynamic alpha and higher KL-Divergence threshold both provided a significant improvement over the baseline from the earlier work. PDF-sampling failed to provide any improvement due to it being approximately equivalent to the baseline method when the sample space is small. In high-dimensional environments, the addition
&lt;/p&gt;</description></item><item><title>&#23494;&#38598;&#21270;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#22312;&#24072;&#29983;&#27169;&#24335;&#19979;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#38081;&#30913;&#30456;&#23398;&#20064;&#21644;&#21407;&#22411;&#23398;&#20064;&#30340;&#29305;&#28857;&#65292;&#21516;&#26102;&#21457;&#29616;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#20851;&#38190;&#35757;&#32451;&#38598;&#22823;&#23567;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#34920;&#26126;&#23398;&#29983;&#27604;&#25945;&#24072;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#23481;&#24525;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.04191</link><description>&lt;p&gt;
&#23494;&#38598;&#21270;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#22312;&#24072;&#29983;&#27169;&#24335;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Dense Hopfield Networks in the Teacher-Student Setting. (arXiv:2401.04191v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04191
&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#21270;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#22312;&#24072;&#29983;&#27169;&#24335;&#19979;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#38081;&#30913;&#30456;&#23398;&#20064;&#21644;&#21407;&#22411;&#23398;&#20064;&#30340;&#29305;&#28857;&#65292;&#21516;&#26102;&#21457;&#29616;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#20851;&#38190;&#35757;&#32451;&#38598;&#22823;&#23567;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#34920;&#26126;&#23398;&#29983;&#27604;&#25945;&#24072;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#23481;&#24525;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#21270;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#20197;&#20854;&#20174;&#29305;&#24449;&#21040;&#21407;&#22411;&#30340;&#36716;&#21464;&#21644;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#32780;&#38395;&#21517;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#29702;&#35770;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20854;&#23384;&#20648;&#23481;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#24072;&#29983;&#27169;&#24335;&#19979;&#30340;p-&#20307;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#30340;&#30456;&#22270;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#25581;&#31034;&#20102;&#31867;&#20284;&#20110;&#21407;&#22411;&#21644;&#29305;&#24449;&#23398;&#20064;&#33539;&#22260;&#30340;&#38081;&#30913;&#30456;&#12290;&#22312;Nishimori&#32447;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#39640;&#25928;&#27169;&#24335;&#26816;&#32034;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#30340;&#20020;&#30028;&#22823;&#23567;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24072;&#29983;&#27169;&#24335;&#30340;&#39034;&#30913;&#21040;&#38081;&#30913;&#36716;&#21464;&#19982;&#30452;&#25509;&#27169;&#22411;&#65288;&#21363;&#38543;&#26426;&#27169;&#24335;&#65289;&#30340;&#39034;&#30913;&#21040;&#33258;&#26059;&#29627;&#29827;&#36716;&#21464;&#19968;&#33268;&#12290;&#22312;Nishimori&#32447;&#20043;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#24615;&#33021;&#19982;&#25512;&#26029;&#28201;&#24230;&#21644;&#25968;&#25454;&#38598;&#22122;&#22768;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#23398;&#29983;&#27604;&#25945;&#24072;&#20351;&#29992;&#36739;&#22823;&#30340;p&#20540;&#26102;&#65292;&#23398;&#29983;&#20855;&#26377;&#24191;&#27867;&#30340;&#23481;&#24525;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense Hopfield networks are known for their feature to prototype transition and adversarial robustness. However, previous theoretical studies have been mostly concerned with their storage capacity. We bridge this gap by studying the phase diagram of p-body Hopfield networks in the teacher-student setting of an unsupervised learning problem, uncovering ferromagnetic phases reminiscent of the prototype and feature learning regimes. On the Nishimori line, we find the critical size of the training set necessary for efficient pattern retrieval. Interestingly, we find that that the paramagnetic to ferromagnetic transition of the teacher-student setting coincides with the paramagnetic to spin-glass transition of the direct model, i.e. with random patterns. Outside of the Nishimori line, we investigate the learning performance in relation to the inference temperature and dataset noise. Moreover, we show that using a larger p for the student than the teacher gives the student an extensive toler
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#20102;&#33258;&#27965;&#30340;&#22806;&#34892;&#26143;&#22823;&#27668;&#21484;&#22238;&#65292;&#21152;&#24555;&#20102;&#21484;&#22238;&#36895;&#24230;&#24182;&#20801;&#35768;&#20351;&#29992;&#26356;&#22797;&#26434;&#30340;&#22823;&#27668;&#27169;&#22411;&#36827;&#34892;&#21484;&#22238;&#12290;</title><link>http://arxiv.org/abs/2401.04168</link><description>&lt;p&gt;
FlopPITy: &#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#33258;&#27965;&#30340;&#22806;&#34892;&#26143;&#22823;&#27668;&#21484;&#22238;
&lt;/p&gt;
&lt;p&gt;
FlopPITy: Enabling self-consistent exoplanet atmospheric retrievals with machine learning. (arXiv:2401.04168v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#20102;&#33258;&#27965;&#30340;&#22806;&#34892;&#26143;&#22823;&#27668;&#21484;&#22238;&#65292;&#21152;&#24555;&#20102;&#21484;&#22238;&#36895;&#24230;&#24182;&#20801;&#35768;&#20351;&#29992;&#26356;&#22797;&#26434;&#30340;&#22823;&#27668;&#27169;&#22411;&#36827;&#34892;&#21484;&#22238;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33324;&#20351;&#29992;&#36125;&#21494;&#26031;&#21484;&#22238;&#25216;&#26415;&#26469;&#35299;&#37322;&#22806;&#34892;&#26143;&#22823;&#27668;&#35266;&#27979;&#32467;&#26524;&#20197;&#32422;&#26463;&#29289;&#29702;&#21644;&#21270;&#23398;&#24615;&#36136;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#27169;&#22411;&#35745;&#31639;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#36816;&#34892;&#26102;&#38388;&#20043;&#38388;&#20570;&#20986;&#22949;&#21327;&#12290;&#20026;&#20102;&#21152;&#24555;&#21484;&#22238;&#36895;&#24230;&#65292;&#26412;&#30740;&#31350;&#23454;&#29616;&#24182;&#27979;&#35797;&#20102;&#39034;&#24207;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#65288;SNPE&#65289;&#65292;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#31639;&#27861;&#65292;&#29992;&#20110;&#22806;&#34892;&#26143;&#22823;&#27668;&#21484;&#22238;&#12290;&#36890;&#36807;&#29983;&#25104;100&#20010;&#21512;&#25104;&#35266;&#27979;&#32467;&#26524;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#36827;&#34892;&#21484;&#22238;&#27979;&#35797;&#65292;&#26088;&#22312;&#20351;&#29992;&#35745;&#31639;&#28201;&#24230;&#32467;&#26500;&#30340;&#36752;&#23556;&#20256;&#36755;&#27169;&#22411;&#31561;&#26356;&#22797;&#26434;&#30340;&#22823;&#27668;&#27169;&#22411;&#26469;&#25552;&#21319;&#21484;&#22238;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting the observations of exoplanet atmospheres to constrain physical and chemical properties is typically done using Bayesian retrieval techniques. Because these methods require many model computations, a compromise is made between model complexity and run time. Reaching this compromise leads to the simplification of many physical and chemical processes (e.g. parameterised temperature structure). Here we implement and test sequential neural posterior estimation (SNPE), a machine learning inference algorithm, for exoplanet atmospheric retrievals. The goal is to speed up retrievals so they can be run with more computationally expensive atmospheric models, such as those computing the temperature structure using radiative transfer. We generate 100 synthetic observations using ARCiS (ARtful Modeling Code for exoplanet Science, an atmospheric modelling code with the flexibility to compute models in varying degrees of complexity) and perform retrievals on them to test the faithfulness
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36873;&#25321;&#24615;&#38899;&#39057;&#25513;&#34109;&#22810;&#27169;&#24577;&#29942;&#39048;Transformer&#29992;&#20110;&#38899;&#35270;&#39057;&#20998;&#31867;&#65292;&#36890;&#36807;&#38899;&#35270;&#39057;Transformer&#25552;&#21462;&#26102;&#31354;&#34920;&#31034;&#24182;&#32467;&#21512;&#33258;&#30417;&#30563;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#21644;&#35821;&#20041;&#38899;&#39057;&#27963;&#21160;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.04154</link><description>&lt;p&gt;
&#39640;&#25928;&#36873;&#25321;&#24615;&#38899;&#39057;&#25513;&#34109;&#22810;&#27169;&#24577;&#29942;&#39048;Transformer&#29992;&#20110;&#38899;&#35270;&#39057;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Efficient Selective Audio Masked Multimodal Bottleneck Transformer for Audio-Video Classification. (arXiv:2401.04154v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04154
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36873;&#25321;&#24615;&#38899;&#39057;&#25513;&#34109;&#22810;&#27169;&#24577;&#29942;&#39048;Transformer&#29992;&#20110;&#38899;&#35270;&#39057;&#20998;&#31867;&#65292;&#36890;&#36807;&#38899;&#35270;&#39057;Transformer&#25552;&#21462;&#26102;&#31354;&#34920;&#31034;&#24182;&#32467;&#21512;&#33258;&#30417;&#30563;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#21644;&#35821;&#20041;&#38899;&#39057;&#27963;&#21160;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#21644;&#35270;&#39057;&#26159;&#20027;&#27969;&#23186;&#20307;&#24179;&#21488;&#65288;&#22914;YouTube&#65289;&#20013;&#26368;&#24120;&#35265;&#30340;&#20004;&#31181;&#24418;&#24335;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#20174;&#22810;&#27169;&#24577;&#35270;&#39057;&#20013;&#23398;&#20064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38899;&#35270;&#39057;&#35782;&#21035;&#26041;&#27861;&#65292;&#31216;&#20026;&#38899;&#35270;&#39057;Transformer&#65288;AVT&#65289;&#65292;&#21033;&#29992;&#35270;&#39057;Transformer&#30340;&#26377;&#25928;&#26102;&#31354;&#34920;&#31034;&#26469;&#25552;&#39640;&#21160;&#20316;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#36827;&#34892;&#22810;&#27169;&#24577;&#34701;&#21512;&#65292;&#31616;&#21333;&#22320;&#36890;&#36807;&#36328;&#27169;&#24577;Transformer&#36830;&#25509;&#22810;&#27169;&#24577;&#20196;&#29260;&#20250;&#21344;&#29992;&#22823;&#37327;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#65292;&#22240;&#27492;&#25105;&#20204;&#36890;&#36807;&#38899;&#35270;&#39057;&#29942;&#39048;Transformer&#38477;&#20302;&#20102;&#36328;&#27169;&#24577;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#22810;&#27169;&#24577;Transformer&#30340;&#23398;&#20064;&#25928;&#29575;&#65292;&#25105;&#20204;&#23558;&#33258;&#30417;&#30563;&#30446;&#26631;&#65288;&#21363;&#38899;&#35270;&#39057;&#23545;&#27604;&#23398;&#20064;&#12289;&#38899;&#35270;&#39057;&#21305;&#37197;&#21644;&#38899;&#35270;&#39057;&#36974;&#34109;&#23398;&#20064;&#65289;&#25972;&#21512;&#21040;AVT&#35757;&#32451;&#20013;&#65292;&#23558;&#22810;&#26679;&#30340;&#38899;&#39057;&#21644;&#35270;&#39057;&#34920;&#31034;&#26144;&#23556;&#21040;&#19968;&#20010;&#36890;&#29992;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#38899;&#39057;&#29255;&#27573;&#36974;&#34109;&#25439;&#22833;&#26469;&#23398;&#20064;&#35821;&#20041;&#38899;&#39057;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio and video are two most common modalities in the mainstream media platforms, e.g., YouTube. To learn from multimodal videos effectively, in this work, we propose a novel audio-video recognition approach termed audio video Transformer, AVT, leveraging the effective spatio-temporal representation by the video Transformer to improve action recognition accuracy. For multimodal fusion, simply concatenating multimodal tokens in a cross-modal Transformer requires large computational and memory resources, instead we reduce the cross-modality complexity through an audio-video bottleneck Transformer. To improve the learning efficiency of multimodal Transformer, we integrate self-supervised objectives, i.e., audio-video contrastive learning, audio-video matching, and masked audio and video learning, into AVT training, which maps diverse audio and video representations into a common multimodal representation space. We further propose a masked audio segment loss to learn semantic audio activit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoRA&#38142;&#65288;COLA&#65289;&#30340;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#27531;&#24046;&#23398;&#20064;&#36807;&#31243;&#23558;LoRA&#27169;&#22359;&#19982;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#21512;&#24182;&#65292;&#24182;&#37325;&#26032;&#21021;&#22987;&#21270;&#26032;&#30340;LoRA&#27169;&#22359;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;LoRA&#21644;&#20840;&#21442;&#25968;&#24494;&#35843;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.04151</link><description>&lt;p&gt;
LoRA&#38142;&#65306;&#36890;&#36807;&#27531;&#24046;&#23398;&#20064;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning. (arXiv:2401.04151v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoRA&#38142;&#65288;COLA&#65289;&#30340;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#27531;&#24046;&#23398;&#20064;&#36807;&#31243;&#23558;LoRA&#27169;&#22359;&#19982;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#21512;&#24182;&#65292;&#24182;&#37325;&#26032;&#21021;&#22987;&#21270;&#26032;&#30340;LoRA&#27169;&#22359;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;LoRA&#21644;&#20840;&#21442;&#25968;&#24494;&#35843;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#26159;&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23450;&#21046;&#20026;&#29305;&#23450;&#20219;&#21153;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#21644;&#20219;&#21153;&#22810;&#26679;&#24615;&#30340;&#25193;&#22823;&#65292;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#20043;&#19968;&#26159;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#21450;&#20854;&#21464;&#20307;&#12290;LoRA&#23558;&#26435;&#37325;&#26356;&#26032;&#32534;&#30721;&#20026;&#20004;&#20010;&#20302;&#31209;&#30697;&#38453;&#30340;&#20056;&#31215;&#12290;&#23613;&#31649;&#20855;&#26377;&#19968;&#23450;&#20248;&#21183;&#65292;&#20294;&#22312;&#26576;&#20123;&#20219;&#21153;&#30340;&#27867;&#21270;&#38169;&#35823;&#26041;&#38754;&#65292;LoRA&#26080;&#27861;&#23436;&#20840;&#21442;&#25968;&#21270;&#24494;&#35843;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;LoRA&#38142;&#65288;COLA&#65289;&#30340;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#65292;&#21463;Frank-Wolfe&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#20197;&#24357;&#21512;LoRA&#21644;&#20840;&#21442;&#25968;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#32780;&#19981;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#25110;&#20869;&#23384;&#24320;&#38144;&#12290;COLA&#37319;&#29992;&#27531;&#24046;&#23398;&#20064;&#36807;&#31243;&#65292;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#20013;&#21512;&#24182;&#23398;&#21040;&#30340;LoRA&#27169;&#22359;&#65292;&#24182;&#37325;&#26032;&#21021;&#22987;&#21270;&#26032;&#29983;&#30340;LoRA&#27169;&#22359;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#25910;&#25947;&#20445;&#35777;&#20197;&#21450;...
&lt;/p&gt;
&lt;p&gt;
Fine-tuning is the primary methodology for tailoring pre-trained large language models to specific tasks. As the model's scale and the diversity of tasks expand, parameter-efficient fine-tuning methods are of paramount importance. One of the most widely used family of methods is low-rank adaptation (LoRA) and its variants. LoRA encodes weight update as the product of two low-rank matrices. Despite its advantages, LoRA falls short of full-parameter fine-tuning in terms of generalization error for certain tasks.  We introduce Chain of LoRA (COLA), an iterative optimization framework inspired by the Frank-Wolfe algorithm, to bridge the gap between LoRA and full parameter fine-tuning, without incurring additional computational costs or memory overheads. COLA employs a residual learning procedure where it merges learned LoRA modules into the pre-trained language model parameters and re-initilize optimization for new born LoRA modules. We provide theoretical convergence guarantees as well as
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#25216;&#26415;&#22312;&#26102;&#31354;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#21452;&#37325;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#20998;&#35299;&#21644;&#26657;&#27491;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04148</link><description>&lt;p&gt;
&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#30340;&#26102;&#31354;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Online Test-Time Adaptation of Spatial-Temporal Traffic Flow Forecasting. (arXiv:2401.04148v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#25216;&#26415;&#22312;&#26102;&#31354;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#21452;&#37325;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#20998;&#35299;&#21644;&#26657;&#27491;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#26102;&#31354;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#23545;&#20110;&#20132;&#36890;&#31649;&#29702;&#20154;&#21592;&#23454;&#26045;&#25511;&#21046;&#25514;&#26045;&#21644;&#39550;&#39542;&#21592;&#36873;&#25321;&#26368;&#20339;&#34892;&#39542;&#36335;&#32447;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#21382;&#21490;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#29992;&#20110;&#23545;&#26410;&#26469;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21382;&#21490;&#25968;&#25454;&#21644;&#26410;&#26469;&#25968;&#25454;&#20043;&#38388;&#30340;&#26102;&#38388;&#28418;&#31227;&#65292;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#36890;&#24120;&#20250;&#19979;&#38477;&#12290;&#20026;&#20102;&#20351;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;&#23436;&#20840;&#22312;&#32447;&#30340;&#26041;&#24335;&#19979;&#26356;&#22909;&#22320;&#36866;&#24212;&#26410;&#26469;&#25968;&#25454;&#65292;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#26102;&#31354;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#38382;&#39064;&#30340;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#25216;&#26415;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#21452;&#37325;&#26657;&#27491;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#23395;&#33410;&#24615;&#21644;&#36235;&#21183;&#21608;&#26399;&#37096;&#20998;&#30340;&#20998;&#35299;&#65292;&#24182;&#22312;&#27979;&#35797;&#38454;&#27573;&#20351;&#29992;&#20004;&#20010;&#29420;&#31435;&#30340;&#27169;&#22359;&#23545;&#20854;&#36827;&#34892;&#26657;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate spatial-temporal traffic flow forecasting is crucial in aiding traffic managers in implementing control measures and assisting drivers in selecting optimal travel routes. Traditional deep-learning based methods for traffic flow forecasting typically rely on historical data to train their models, which are then used to make predictions on future data. However, the performance of the trained model usually degrades due to the temporal drift between the historical and future data. To make the model trained on historical data better adapt to future data in a fully online manner, this paper conducts the first study of the online test-time adaptation techniques for spatial-temporal traffic flow forecasting problems. To this end, we propose an Adaptive Double Correction by Series Decomposition (ADCSD) method, which first decomposes the output of the trained model into seasonal and trend-cyclical parts and then corrects them by two separate modules during the testing phase using the la
&lt;/p&gt;</description></item><item><title>LOPA&#26159;&#19968;&#31181;&#22686;&#24378;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20840;&#23616;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#25910;&#25947;&#24615;&#21644;&#27867;&#21270;&#24615;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.04145</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20840;&#23616;&#36335;&#24452;&#35268;&#21010;&#30340;&#22686;&#24378;&#27880;&#24847;&#21147;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65306;Learn Once Plan Arbitrarily (LOPA)
&lt;/p&gt;
&lt;p&gt;
Learn Once Plan Arbitrarily (LOPA): Attention-Enhanced Deep Reinforcement Learning Method for Global Path Planning. (arXiv:2401.04145v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04145
&lt;/p&gt;
&lt;p&gt;
LOPA&#26159;&#19968;&#31181;&#22686;&#24378;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20840;&#23616;&#36335;&#24452;&#35268;&#21010;&#20013;&#30340;&#25910;&#25947;&#24615;&#21644;&#27867;&#21270;&#24615;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#26041;&#27861;&#22312;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#20840;&#23616;&#35268;&#21010;&#20219;&#21153;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30528;&#25910;&#25947;&#24615;&#21644;&#27867;&#21270;&#24615;&#31561;&#20005;&#37325;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LOPA&#65288;Learn Once Plan Arbitrarily&#65289;&#30340;&#22686;&#24378;&#27880;&#24847;&#21147;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;DRL&#30340;&#35266;&#23519;&#35282;&#24230;&#20998;&#26512;&#20102;&#36825;&#20123;&#38382;&#39064;&#30340;&#21407;&#22240;&#65292;&#25581;&#31034;&#20102;&#20256;&#32479;&#35774;&#35745;&#23548;&#33268;DRL&#21463;&#21040;&#26080;&#20851;&#22320;&#22270;&#20449;&#24687;&#30340;&#24178;&#25200;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;LOPA&#65292;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22686;&#24378;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#25913;&#21892;&#23545;&#35266;&#23519;&#20013;&#20851;&#38190;&#20449;&#24687;&#30340;&#27880;&#24847;&#21147;&#33021;&#21147;&#12290;&#36825;&#31181;&#26426;&#21046;&#36890;&#36807;&#20004;&#20010;&#27493;&#39588;&#23454;&#29616;&#65306;(1)&#26500;&#24314;&#19968;&#20010;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23558;DRL&#30340;&#35266;&#23519;&#36716;&#25442;&#20026;&#20004;&#20010;&#21160;&#24577;&#35270;&#22270;&#65306;&#23616;&#37096;&#21644;&#20840;&#23616;&#65292;&#26174;&#33879;&#25351;&#23548;LOPA&#20851;&#27880;&#32473;&#23450;&#22320;&#22270;&#19978;&#30340;&#20851;&#38190;&#20449;&#24687;&#65307;(2)&#26500;&#24314;&#19968;&#20010;&#21452;&#36890;&#36947;&#32593;&#32476;&#26469;&#22788;&#29702;&#36825;&#20004;&#20010;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) methods have recently shown promise in path planning tasks. However, when dealing with global planning tasks, these methods face serious challenges such as poor convergence and generalization. To this end, we propose an attention-enhanced DRL method called LOPA (Learn Once Plan Arbitrarily) in this paper. Firstly, we analyze the reasons of these problems from the perspective of DRL's observation, revealing that the traditional design causes DRL to be interfered by irrelevant map information. Secondly, we develop the LOPA which utilizes a novel attention-enhanced mechanism to attain an improved attention capability towards the key information of the observation. Such a mechanism is realized by two steps: (1) an attention model is built to transform the DRL's observation into two dynamic views: local and global, significantly guiding the LOPA to focus on the key information on the given maps; (2) a dual-channel network is constructed to process these two
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#40065;&#26834;&#21518;&#26657;&#20934;&#26041;&#27861;&#65292;&#30456;&#27604;&#25552;&#21319;&#26641;&#27169;&#22411;&#65292;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22825;&#27668;&#39044;&#27979;&#20013;&#23454;&#29616;&#26356;&#20934;&#30830;&#21644;&#26356;&#22909;&#26657;&#20934;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.04144</link><description>&lt;p&gt;
&#25913;&#36827;&#22825;&#27668;&#39044;&#27979;&#30340;&#40065;&#26834;&#24615;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Robust Calibration For Improved Weather Prediction Under Distributional Shift. (arXiv:2401.04144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#40065;&#26834;&#21518;&#26657;&#20934;&#26041;&#27861;&#65292;&#30456;&#27604;&#25552;&#21319;&#26641;&#27169;&#22411;&#65292;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22825;&#27668;&#39044;&#27979;&#20013;&#23454;&#29616;&#26356;&#20934;&#30830;&#21644;&#26356;&#22909;&#26657;&#20934;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#8220;&#40065;&#26834;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#22312;&#29616;&#23454;&#19990;&#30028;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#25361;&#25112;&#8212;&#8212;&#31227;&#20301;&#25361;&#25112;&#8221;&#20013;&#25913;&#21892;&#36328;&#22495;&#22825;&#27668;&#39044;&#27979;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#21033;&#29992;&#20174;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20511;&#37492;&#30340;&#39640;&#32423;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#20197;&#21450;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#23545;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#40065;&#26834;&#21518;&#26657;&#20934;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#27604;&#25552;&#21319;&#26641;&#27169;&#22411;&#26356;&#20934;&#30830;&#21644;&#26356;&#22909;&#26657;&#20934;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#31181;&#25351;&#26631;&#23545;&#25105;&#20204;&#30340;&#39044;&#27979;&#36827;&#34892;&#37327;&#21270;&#65292;&#25552;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#30340;&#25506;&#31350;&#21644;&#23454;&#39564;&#26041;&#21521;&#26469;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present results on improving out-of-domain weather prediction and uncertainty estimation as part of the \texttt{Shifts Challenge on Robustness and Uncertainty under Real-World Distributional Shift} challenge. We find that by leveraging a mixture of experts in conjunction with an advanced data augmentation technique borrowed from the computer vision domain, in conjunction with robust \textit{post-hoc} calibration of predictive uncertainties, we can potentially achieve more accurate and better-calibrated results with deep neural networks than with boosted tree models for tabular data. We quantify our predictions using several metrics and propose several future lines of inquiry and experimentation to boost performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20998;&#24418;&#20960;&#20309;&#30340;&#28508;&#21147;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#23545;&#20854;&#32534;&#30721;&#33021;&#21147;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#30456;&#20851;&#24615;&#20998;&#26512;&#23454;&#39564;&#21644;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#28145;&#24230;&#32593;&#32476;&#19981;&#33021;&#25552;&#21462;&#22797;&#26434;&#19988;&#39640;&#23618;&#27425;&#30340;&#20998;&#24418;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#23545;&#35937;&#32467;&#26500;&#23545;&#20998;&#31867;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#30340;&#24212;&#29992;&#20013;&#65292;&#20998;&#24418;&#29305;&#24449;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04141</link><description>&lt;p&gt;
&#20851;&#20110;&#20998;&#24418;&#20960;&#20309;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23545;&#20854;&#32534;&#30721;&#33021;&#21147;&#30340;&#28508;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On The Potential of The Fractal Geometry and The CNNs Ability to Encode it. (arXiv:2401.04141v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20998;&#24418;&#20960;&#20309;&#30340;&#28508;&#21147;&#21644;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#23545;&#20854;&#32534;&#30721;&#33021;&#21147;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#30456;&#20851;&#24615;&#20998;&#26512;&#23454;&#39564;&#21644;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#28145;&#24230;&#32593;&#32476;&#19981;&#33021;&#25552;&#21462;&#22797;&#26434;&#19988;&#39640;&#23618;&#27425;&#30340;&#20998;&#24418;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#23545;&#35937;&#32467;&#26500;&#23545;&#20998;&#31867;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#30340;&#24212;&#29992;&#20013;&#65292;&#20998;&#24418;&#29305;&#24449;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24418;&#32500;&#25968;&#36890;&#36807;&#30740;&#31350;&#27169;&#24335;&#22312;&#27979;&#37327;&#23610;&#24230;&#19979;&#30340;&#21464;&#21270;&#26469;&#25552;&#20379;&#23545;&#35937;&#22797;&#26434;&#24615;&#30340;&#32479;&#35745;&#25351;&#26631;&#12290;&#34429;&#28982;&#22312;&#20960;&#31181;&#20998;&#31867;&#20219;&#21153;&#20013;&#26377;&#29992;&#65292;&#20294;&#22312;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#23545;&#20998;&#24418;&#32500;&#25968;&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#28145;&#20837;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#20123;&#28145;&#24230;&#32593;&#32476;&#26159;&#21542;&#33021;&#22815;&#20687;&#20998;&#24418;&#32500;&#25968;&#19968;&#26679;&#32534;&#30721;&#22797;&#26434;&#19988;&#39640;&#23618;&#27425;&#30340;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#30456;&#20851;&#24615;&#20998;&#26512;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#28145;&#24230;&#32593;&#32476;&#22312;&#20219;&#20309;&#23618;&#27425;&#37117;&#19981;&#33021;&#25552;&#21462;&#20986;&#36825;&#26679;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#23558;&#20998;&#26512;&#30740;&#31350;&#19982;&#20154;&#31867;&#35780;&#20272;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#21644;&#20165;&#25805;&#20316;&#20998;&#24418;&#29305;&#24449;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20998;&#24418;&#29305;&#24449;&#22312;&#38656;&#35201;&#23545;&#35937;&#32467;&#26500;&#23545;&#20998;&#31867;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#30340;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;&#22312;&#20998;&#24418;&#29305;&#24449;&#19978;&#35757;&#32451;&#27973;&#23618;&#32593;&#32476;&#33021;&#22815;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fractal dimension provides a statistical index of object complexity by studying how the pattern changes with the measuring scale. Although useful in several classification tasks, the fractal dimension is under-explored in deep learning applications. In this work, we investigate the features that are learned by deep models and we study whether these deep networks are able to encode features as complex and high-level as the fractal dimensions. Specifically, we conduct a correlation analysis experiment to show that deep networks are not able to extract such a feature in none of their layers. We combine our analytical study with a human evaluation to investigate the differences between deep learning networks and models that operate on the fractal feature solely. Moreover, we show the effectiveness of fractal features in applications where the object structure is crucial for the classification task. We empirically show that training a shallow network on fractal features achieves perform
&lt;/p&gt;</description></item><item><title>CCNETS&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33041;&#21551;&#21457;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#65292;&#36890;&#36807;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#35782;&#21035;&#65292;&#29305;&#21035;&#20851;&#27880;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.04139</link><description>&lt;p&gt;
CCNETS:&#19968;&#31181;&#26032;&#39062;&#30340;&#33041;&#21551;&#21457;&#26041;&#27861;&#29992;&#20110;&#22686;&#24378;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
CCNETS: A Novel Brain-Inspired Approach for Enhanced Pattern Recognition in Imbalanced Datasets. (arXiv:2401.04139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04139
&lt;/p&gt;
&lt;p&gt;
CCNETS&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33041;&#21551;&#21457;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#65292;&#36890;&#36807;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#35782;&#21035;&#65292;&#29305;&#21035;&#20851;&#27880;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;CCNETS&#65288;&#20855;&#26377;&#22240;&#26524;&#21512;&#20316;&#32593;&#32476;&#30340;&#22240;&#26524;&#23398;&#20064;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#20998;&#31867;&#22120;&#65292;&#26088;&#22312;&#35299;&#20915;&#27169;&#24335;&#35782;&#21035;&#20013;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#25361;&#25112;&#12290;CCNETS&#29420;&#29305;&#22320;&#35774;&#35745;&#25104;&#27169;&#25311;&#31867;&#20284;&#20110;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#65292;&#24182;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;&#35299;&#37322;&#22120;&#12289;&#29983;&#25104;&#22120;&#21644;&#25512;&#29702;&#22120;&#12290;&#27599;&#20010;&#32452;&#20214;&#37117;&#34987;&#35774;&#35745;&#25104;&#27169;&#20223;&#29305;&#23450;&#30340;&#22823;&#33041;&#21151;&#33021;&#65292;&#26377;&#21161;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#24182;&#22686;&#24378;&#20998;&#31867;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#29305;&#21035;&#20851;&#27880;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#24120;&#35265;&#21644;&#37325;&#35201;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;CCNETS&#24212;&#29992;&#20110;&#19968;&#20010;&#8220;&#27450;&#35784;&#25968;&#25454;&#38598;&#8221;&#65292;&#20854;&#20013;&#27491;&#24120;&#20132;&#26131;&#26126;&#26174;&#22810;&#20110;&#27450;&#35784;&#20132;&#26131;&#65288;99.83&#65285; vs. 0.17&#65285;&#65289;&#65292;&#35777;&#26126;&#20102;CCNETS&#30340;&#26377;&#25928;&#24615;&#12290;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#22312;&#22788;&#29702;&#36825;&#31181;&#19981;&#24179;&#34913;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#23548;&#33268;&#24615;&#33021;&#25351;&#26631;&#19981;&#22343;&#34913;&#12290;&#28982;&#32780;&#65292;CCNETS&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#20998;&#31867;&#33021;&#21147;&#65292;&#36890;&#36807;&#20854;&#24615;&#33021;&#25351;&#26631;&#30340;&#25913;&#21892;&#26469;&#20307;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces CCNETS (Causal Learning with Causal Cooperative Nets), a novel generative model-based classifier designed to tackle the challenge of generating data for imbalanced datasets in pattern recognition. CCNETS is uniquely crafted to emulate brain-like information processing and comprises three main components: Explainer, Producer, and Reasoner. Each component is designed to mimic specific brain functions, which aids in generating high-quality datasets and enhancing classification performance.  The model is particularly focused on addressing the common and significant challenge of handling imbalanced datasets in machine learning. CCNETS's effectiveness is demonstrated through its application to a "fraud dataset," where normal transactions significantly outnumber fraudulent ones (99.83% vs. 0.17%). Traditional methods often struggle with such imbalances, leading to skewed performance metrics. However, CCNETS exhibits superior classification ability, as evidenced by its pe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GA-STGRN&#30340;&#26032;&#30340;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#26102;&#31354;&#24314;&#27169;&#12290;&#35813;&#26694;&#26550;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#21019;&#26032;&#26159;&#24341;&#20837;&#20102;&#20840;&#23616;&#24863;&#30693;&#23618;&#26469;&#24110;&#21161;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#24314;&#27169;&#38750;&#22266;&#23450;&#30340;&#22270;&#32467;&#26500;&#21644;&#25429;&#25417;&#23616;&#37096;&#29305;&#24449;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#24207;&#21015;&#24863;&#30693;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2401.04135</link><description>&lt;p&gt;
&#20855;&#26377;&#20840;&#23616;&#24863;&#30693;&#22686;&#24378;&#30340;&#26102;&#31354;&#22270;&#36882;&#24402;&#32593;&#32476;&#65306;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Global-Aware Enhanced Spatial-Temporal Graph Recurrent Networks: A New Framework For Traffic Flow Prediction. (arXiv:2401.04135v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04135
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GA-STGRN&#30340;&#26032;&#30340;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#26102;&#31354;&#24314;&#27169;&#12290;&#35813;&#26694;&#26550;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#21019;&#26032;&#26159;&#24341;&#20837;&#20102;&#20840;&#23616;&#24863;&#30693;&#23618;&#26469;&#24110;&#21161;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#24314;&#27169;&#38750;&#22266;&#23450;&#30340;&#22270;&#32467;&#26500;&#21644;&#25429;&#25417;&#23616;&#37096;&#29305;&#24449;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#24207;&#21015;&#24863;&#30693;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#22312;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#21644;&#25552;&#39640;&#36816;&#36755;&#25928;&#29575;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#34429;&#28982;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#19982;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#36827;&#34892;&#26102;&#31354;&#24314;&#27169;&#22312;&#36825;&#20010;&#39046;&#22495;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#31574;&#30053;&#65292;&#20294;&#26159;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#21463;&#38480;&#32467;&#26500;&#38480;&#21046;&#20102;&#23427;&#20204;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#31354;&#38388;&#24314;&#27169;&#65292;&#35768;&#22810;&#20043;&#21069;&#30340;&#30740;&#31350;&#23398;&#20064;&#19968;&#20010;&#34987;&#20551;&#35774;&#20026;&#22266;&#23450;&#19988;&#22312;&#25152;&#26377;&#26102;&#38388;&#27493;&#39588;&#19978;&#22343;&#21248;&#30340;&#22270;&#32467;&#26500;&#65292;&#36825;&#21487;&#33021;&#24182;&#19981;&#27491;&#30830;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#36890;&#39044;&#27979;&#26694;&#26550;&#65292;&#31216;&#20026;&#20840;&#23616;&#24863;&#30693;&#22686;&#24378;&#26102;&#31354;&#22270;&#36882;&#24402;&#32593;&#32476;&#65288;GA-STGRN&#65289;&#65292;&#21253;&#25324;&#20004;&#20010;&#26680;&#24515;&#32452;&#20214;&#65306;&#26102;&#31354;&#22270;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21644;&#20840;&#23616;&#24863;&#30693;&#23618;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#21019;&#26032;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#24207;&#21015;&#24863;&#30693;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#20013;&#65292;&#20197;&#23398;&#20064;&#19981;&#21516;&#26102;&#38388;&#27493;&#39588;&#19979;&#38750;&#22266;&#23450;&#30340;&#22270;&#24418;&#24182;&#25429;&#25417;&#23616;&#37096;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic flow prediction plays a crucial role in alleviating traffic congestion and enhancing transport efficiency. While combining graph convolution networks with recurrent neural networks for spatial-temporal modeling is a common strategy in this realm, the restricted structure of recurrent neural networks limits their ability to capture global information. For spatial modeling, many prior studies learn a graph structure that is assumed to be fixed and uniform at all time steps, which may not be true. This paper introduces a novel traffic prediction framework, Global-Aware Enhanced Spatial-Temporal Graph Recurrent Network (GA-STGRN), comprising two core components: a spatial-temporal graph recurrent neural network and a global awareness layer. Within this framework, three innovative prediction models are formulated. A sequence-aware graph neural network is proposed and integrated into the Gated Recurrent Unit (GRU) to learn non-fixed graphs at different time steps and capture local te
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21512;&#25104;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35782;&#21035;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#65292;&#26500;&#24314;&#21512;&#25104;&#32593;&#32476;&#65292;&#24182;&#30830;&#20445;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#25509;&#36817;&#12290;&#36825;&#25552;&#20379;&#20102;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#21512;&#25104;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.04133</link><description>&lt;p&gt;
SynHIN: &#29983;&#25104;&#29992;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#21512;&#25104;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SynHIN: Generating Synthetic Heterogeneous Information Network for Explainable AI. (arXiv:2401.04133v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04133
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#21512;&#25104;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35782;&#21035;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#65292;&#26500;&#24314;&#21512;&#25104;&#32593;&#32476;&#65292;&#24182;&#30830;&#20445;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#25509;&#36817;&#12290;&#36825;&#25552;&#20379;&#20102;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#21512;&#25104;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#26377;&#30528;&#20248;&#31168;&#30340;&#34920;&#29616;&#65292;&#20174;&#26816;&#27979;&#30005;&#23376;&#21830;&#21153;&#22403;&#22334;&#37038;&#20214;&#21040;&#31038;&#20132;&#32593;&#32476;&#20998;&#31867;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#20844;&#20849;&#22270;&#25968;&#25454;&#38598;&#38459;&#30861;&#20102;&#30740;&#31350;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#22312;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#65288;HIN&#65289;&#26041;&#38754;&#12290;&#30001;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#27169;&#22411;&#30340;&#36827;&#23637;&#65292;&#23545;&#20110;&#20844;&#24179;&#30340;HIN&#27604;&#36739;&#32780;&#35328;&#65292;&#38656;&#35201;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#36234;&#26469;&#36234;&#22823;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SynHIN&#65292;&#19968;&#31181;&#29983;&#25104;&#21512;&#25104;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#30340;&#29420;&#29305;&#26041;&#27861;&#12290;SynHIN&#35782;&#21035;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#65292;&#24635;&#32467;&#22270;&#32479;&#35745;&#25968;&#25454;&#65292;&#24182;&#26500;&#24314;&#21512;&#25104;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;In-Cluster&#21644;Out-Cluster Merge&#27169;&#22359;&#20174;&#20027;&#35201;&#30340;&#27169;&#24335;&#38598;&#32676;&#26500;&#24314;&#21512;&#25104;HIN&#12290;&#22312;In/Out-Cluster&#21512;&#24182;&#21644;&#31526;&#21512;&#30495;&#23454;&#25968;&#25454;&#38598;&#32422;&#26463;&#30340;&#21518;&#20462;&#21098;&#36807;&#31243;&#21518;&#65292;&#25105;&#20204;&#30830;&#20445;&#21512;&#25104;&#30340;&#22270;&#32479;&#35745;&#25968;&#25454;&#19982;&#21442;&#32771;&#25968;&#25454;&#25509;&#36817;&#12290;SynHIN&#29983;&#25104;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#21512;&#25104;&#24322;&#26500;&#22270;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#20027;&#35201;&#30340;&#27169;&#24335;&#20316;&#20026;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) excel in various domains, from detecting e-commerce spam to social network classification problems. However, the lack of public graph datasets hampers research progress, particularly in heterogeneous information networks (HIN). The demand for datasets for fair HIN comparisons is growing due to advancements in GNN interpretation models. In response, we propose SynHIN, a unique method for generating synthetic heterogeneous information networks. SynHIN identifies motifs in real-world datasets, summarizes graph statistics, and constructs a synthetic network. Our approach utilizes In-Cluster and Out-Cluster Merge modules to build the synthetic HIN from primary motif clusters. After In/Our-Cluster mergers and a post-pruning process fitting the real dataset constraints, we ensure the synthetic graph statistics align closely with the reference one. SynHIN generates a synthetic heterogeneous graph dataset for node classification tasks, using the primary motif as the
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;PLUTO:&#19968;&#31181;&#25554;&#25300;&#24335;&#27169;&#22359;&#21270;&#30340;&#27979;&#35797;&#26102;&#39046;&#22495;&#36866;&#24212;&#31574;&#30053;&#65292;&#36890;&#36807;&#39044;&#20808;&#35757;&#32451;&#19968;&#31995;&#21015;&#38024;&#23545;&#19981;&#21516;&#28304;&#39046;&#22495;&#30340;&#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#21019;&#24314;&#20102;&#19968;&#20010;"&#27169;&#22359;&#23384;&#20648;&#24211;"&#12290;&#37319;&#29992;&#26080;&#30417;&#30563;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20174;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#31232;&#30095;&#30340;&#30456;&#20851;&#27169;&#22359;&#30340;&#23376;&#38598;&#65292;&#24182;&#21019;&#24314;&#36873;&#20013;&#27169;&#22359;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#39046;&#22495;&#30340;&#33258;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2401.04130</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#65306;&#36890;&#36807;&#25554;&#20837;&#21644;&#25773;&#25918;&#21464;&#25442;&#22120;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Test-Time Adaptation via Plug-and-Play Transformer Modules. (arXiv:2401.04130v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04130
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;PLUTO:&#19968;&#31181;&#25554;&#25300;&#24335;&#27169;&#22359;&#21270;&#30340;&#27979;&#35797;&#26102;&#39046;&#22495;&#36866;&#24212;&#31574;&#30053;&#65292;&#36890;&#36807;&#39044;&#20808;&#35757;&#32451;&#19968;&#31995;&#21015;&#38024;&#23545;&#19981;&#21516;&#28304;&#39046;&#22495;&#30340;&#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#21019;&#24314;&#20102;&#19968;&#20010;"&#27169;&#22359;&#23384;&#20648;&#24211;"&#12290;&#37319;&#29992;&#26080;&#30417;&#30563;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20174;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#31232;&#30095;&#30340;&#30456;&#20851;&#27169;&#22359;&#30340;&#23376;&#38598;&#65292;&#24182;&#21019;&#24314;&#36873;&#20013;&#27169;&#22359;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#39046;&#22495;&#30340;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;(PET)&#26041;&#27861;&#65292;&#22914;LoRA&#12289;Adapter&#21644;Visual Prompt Tuning(VPT)&#65292;&#36890;&#36807;&#35843;&#25972;&#21464;&#25442;&#22120;&#27169;&#22411;&#20013;&#30340;&#23567;&#27169;&#22359;&#65292;&#22312;&#20351;&#36866;&#24212;&#26032;&#39046;&#22495;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#39046;&#22495;&#25968;&#37327;&#21487;&#33021;&#38750;&#24120;&#22823;&#65292;&#25968;&#25454;&#36890;&#24120;&#26159;&#26080;&#26631;&#31614;&#30340;&#12290;&#22240;&#27492;&#65292;&#36866;&#24212;&#26032;&#39046;&#22495;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#20063;&#19981;&#29616;&#23454;&#20026;&#27599;&#20010;&#36825;&#26679;&#30340;&#39046;&#22495;&#29983;&#25104;&#23450;&#21046;&#30340;&#35843;&#25972;&#27169;&#22359;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;PLUTO&#65306;&#19968;&#31181;&#25554;&#25300;&#27169;&#22359;&#21270;&#30340;&#27979;&#35797;&#26102;&#39046;&#22495;&#36866;&#24212;&#31574;&#30053;&#12290;&#25105;&#20204;&#39044;&#35757;&#32451;&#20102;&#19968;&#31995;&#21015;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#19987;&#20026;&#19981;&#21516;&#30340;&#28304;&#39046;&#22495;&#36827;&#34892;&#20102;&#19987;&#38376;&#35774;&#35745;&#65292;&#26377;&#25928;&#22320;&#21019;&#24314;&#20102;&#19968;&#20010;"&#27169;&#22359;&#23384;&#20648;&#24211;"&#12290;&#32473;&#23450;&#19968;&#20010;&#24102;&#26377;&#23569;&#26679;&#26412;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#30446;&#26631;&#22495;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;(TTA)&#26041;&#27861;&#65292;&#26469;(1)&#20174;&#24211;&#20013;&#36873;&#25321;&#20986;&#31232;&#30095;&#30340;&#30456;&#20851;&#27169;&#22359;&#30340;&#23376;&#38598;&#65292;&#24182;&#19988;(2)&#22312;&#19981;&#35843;&#25972;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#21019;&#24314;&#36873;&#20013;&#27169;&#22359;&#30340;&#21152;&#26435;&#32452;&#21512;&#12290;&#36825;&#31181;&#25554;&#25300;&#24335;&#30340;&#29305;&#24615;&#20351;&#24471;&#23427;&#21487;===
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient tuning (PET) methods such as LoRA, Adapter, and Visual Prompt Tuning (VPT) have found success in enabling adaptation to new domains by tuning small modules within a transformer model. However, the number of domains encountered during test time can be very large, and the data is usually unlabeled. Thus, adaptation to new domains is challenging; it is also impractical to generate customized tuned modules for each such domain. Toward addressing these challenges, this work introduces PLUTO: a Plug-and-pLay modUlar Test-time domain adaptatiOn strategy. We pre-train a large set of modules, each specialized for different source domains, effectively creating a ``module store''. Given a target domain with few-shot unlabeled data, we introduce an unsupervised test-time adaptation (TTA) method to (1) select a sparse subset of relevant modules from this store and (2) create a weighted combination of selected modules without tuning their weights. This plug-and-play nature enable
&lt;/p&gt;</description></item><item><title>DeepPhysiNet&#26694;&#26550;&#23558;&#29289;&#29702;&#23450;&#24459;&#32435;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#21644;&#36830;&#32493;&#30340;&#22825;&#27668;&#31995;&#32479;&#27169;&#25311;&#12290;</title><link>http://arxiv.org/abs/2401.04125</link><description>&lt;p&gt;
DeepPhysiNet: &#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#27668;&#29289;&#29702;&#23398;&#36827;&#34892;&#20934;&#30830;&#21644;&#36830;&#32493;&#30340;&#22825;&#27668;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
DeepPhysiNet: Bridging Deep Learning and Atmospheric Physics for Accurate and Continuous Weather Modeling. (arXiv:2401.04125v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04125
&lt;/p&gt;
&lt;p&gt;
DeepPhysiNet&#26694;&#26550;&#23558;&#29289;&#29702;&#23450;&#24459;&#32435;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#21644;&#36830;&#32493;&#30340;&#22825;&#27668;&#31995;&#32479;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#22825;&#27668;&#39044;&#25253;&#23545;&#20154;&#31867;&#27963;&#21160;&#38750;&#24120;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#22825;&#27668;&#39044;&#25253;&#26377;&#20004;&#31181;&#33539;&#24335;&#65306;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39044;&#27979;&#65288;DLP&#65289;&#12290;NWP&#21033;&#29992;&#22823;&#27668;&#29289;&#29702;&#23398;&#36827;&#34892;&#22825;&#27668;&#27169;&#25311;&#65292;&#20294;&#25968;&#25454;&#21033;&#29992;&#19981;&#36275;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#20351;&#20854;&#21463;&#21040;&#38480;&#21046;&#65292;&#32780;DLP&#21487;&#20197;&#30452;&#25509;&#20174;&#22823;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#22825;&#27668;&#27169;&#24335;&#65292;&#20294;&#38590;&#20197;&#34701;&#20837;&#29289;&#29702;&#23450;&#24459;&#12290;&#36825;&#20004;&#31181;&#33539;&#24335;&#21508;&#26377;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#24182;&#19988;&#19981;&#20114;&#30456;&#20860;&#23481;&#65292;&#22240;&#20026;NWP&#20013;&#37319;&#29992;&#30340;&#29289;&#29702;&#23450;&#24459;&#25551;&#36848;&#20102;&#22352;&#26631;&#21644;&#27668;&#35937;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#32780;DLP&#30452;&#25509;&#23398;&#20064;&#27668;&#35937;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#32780;&#19981;&#32771;&#34385;&#22352;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DeepPhysiNet&#26694;&#26550;&#65292;&#23558;&#29289;&#29702;&#23450;&#24459;&#32435;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#21644;&#36830;&#32493;&#30340;&#22825;&#27668;&#31995;&#32479;&#27169;&#25311;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#22810;&#23618;&#32593;&#32476;&#26500;&#24314;&#29289;&#29702;&#32593;&#32476;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate weather forecasting holds significant importance to human activities. Currently, there are two paradigms for weather forecasting: Numerical Weather Prediction (NWP) and Deep Learning-based Prediction (DLP). NWP utilizes atmospheric physics for weather modeling but suffers from poor data utilization and high computational costs, while DLP can learn weather patterns from vast amounts of data directly but struggles to incorporate physical laws. Both paradigms possess their respective strengths and weaknesses, and are incompatible, because physical laws adopted in NWP describe the relationship between coordinates and meteorological variables, while DLP directly learns the relationships between meteorological variables without consideration of coordinates. To address these problems, we introduce the DeepPhysiNet framework, incorporating physical laws into deep learning models for accurate and continuous weather system modeling. First, we construct physics networks based on multilay
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;BERT&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#26816;&#27979;&#21644;LIME&#12289;SHAP&#31561;&#35299;&#37322;&#25216;&#26415;&#36827;&#34892;&#35299;&#37322;&#65292;&#25581;&#31034;&#20102;&#40657;&#26263;&#27169;&#24335;&#20013;&#24433;&#21709;&#39044;&#27979;&#30340;&#20851;&#38190;&#26415;&#35821;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#38450;&#33539;&#40657;&#26263;&#27169;&#24335;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.04119</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#29992;&#25143;&#30028;&#38754;&#26159;&#19968;&#31181;&#40657;&#26263;&#27169;&#24335;&#65311;&#65306;&#21487;&#35299;&#37322;&#30340;&#33258;&#21160;&#26816;&#27979;&#21450;&#20854;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why is the User Interface a Dark Pattern? : Explainable Auto-Detection and its Analysis. (arXiv:2401.04119v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;BERT&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#26816;&#27979;&#21644;LIME&#12289;SHAP&#31561;&#35299;&#37322;&#25216;&#26415;&#36827;&#34892;&#35299;&#37322;&#65292;&#25581;&#31034;&#20102;&#40657;&#26263;&#27169;&#24335;&#20013;&#24433;&#21709;&#39044;&#27979;&#30340;&#20851;&#38190;&#26415;&#35821;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#38450;&#33539;&#40657;&#26263;&#27169;&#24335;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#26263;&#27169;&#24335;&#26159;&#22312;&#32447;&#26381;&#21153;&#20013;&#35823;&#23548;&#29992;&#25143;&#34892;&#20026;&#30340;&#27450;&#39575;&#24615;&#29992;&#25143;&#30028;&#38754;&#35774;&#35745;&#12290;&#38544;&#31169;&#20405;&#29359;&#12289;&#36130;&#21153;&#25439;&#22833;&#21644;&#24773;&#32490;&#22256;&#25200;&#31561;&#40657;&#26263;&#27169;&#24335;&#21487;&#33021;&#20250;&#23545;&#29992;&#25143;&#36896;&#25104;&#20260;&#23475;&#12290;&#36825;&#20123;&#38382;&#39064;&#36817;&#24180;&#26469;&#19968;&#30452;&#26159;&#24191;&#27867;&#35752;&#35770;&#30340;&#35805;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21487;&#35299;&#37322;&#30340;&#40657;&#26263;&#27169;&#24335;&#33258;&#21160;&#26816;&#27979;&#65292;&#21363;&#20026;&#20160;&#20040;&#20250;&#23558;&#29305;&#23450;&#30340;&#29992;&#25143;&#30028;&#38754;&#26816;&#27979;&#20026;&#20855;&#26377;&#40657;&#26263;&#27169;&#24335;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#23545;&#30005;&#23376;&#21830;&#21153;&#20013;&#30340;&#40657;&#26263;&#27169;&#24335;&#36827;&#34892;&#20102;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#33258;&#21160;&#26816;&#27979;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;LIME&#21644;SHAP&#31561;&#21518;&#32622;&#35299;&#37322;&#25216;&#26415;&#23545;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20102;&#35299;&#37322;&#65292;&#25581;&#31034;&#20102;&#24433;&#21709;&#27599;&#20010;&#39044;&#27979;&#20316;&#20026;&#40657;&#26263;&#27169;&#24335;&#30340;&#26415;&#35821;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#21462;&#21644;&#20998;&#26512;&#20102;&#24433;&#21709;&#40657;&#26263;&#27169;&#24335;&#30340;&#26415;&#35821;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#20813;&#21463;&#40657;&#26263;&#27169;&#24335;&#30340;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dark patterns are deceptive user interface designs for online services that make users behave in unintended ways. Dark patterns, such as privacy invasion, financial loss, and emotional distress, can harm users. These issues have been the subject of considerable debate in recent years. In this paper, we study interpretable dark pattern auto-detection, that is, why a particular user interface is detected as having dark patterns. First, we trained a model using transformer-based pre-trained language models, BERT, on a text-based dataset for the automatic detection of dark patterns in e-commerce. Then, we applied post-hoc explanation techniques, including local interpretable model agnostic explanation (LIME) and Shapley additive explanations (SHAP), to the trained model, which revealed which terms influence each prediction as a dark pattern. In addition, we extracted and analyzed terms that affected the dark patterns. Our findings may prevent users from being manipulated by dark patterns, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#32447;&#30340;&#27969;&#31243;&#21457;&#29616;&#26041;&#27861;&#65292;&#33021;&#22815;&#26126;&#30830;&#34920;&#31034;&#26102;&#38388;&#36724;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;BPIC&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#19987;&#26377;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20013;&#23637;&#29616;&#20986;&#19982;&#26631;&#20934;&#24067;&#23616;&#25216;&#26415;&#30456;&#27604;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.04114</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#32447;&#30340;&#27969;&#31243;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Timeline-based Process Discovery. (arXiv:2401.04114v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#32447;&#30340;&#27969;&#31243;&#21457;&#29616;&#26041;&#27861;&#65292;&#33021;&#22815;&#26126;&#30830;&#34920;&#31034;&#26102;&#38388;&#36724;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;BPIC&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#19987;&#26377;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20013;&#23637;&#29616;&#20986;&#19982;&#26631;&#20934;&#24067;&#23616;&#25216;&#26415;&#30456;&#27604;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#27969;&#31243;&#21457;&#29616;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#25552;&#20379;&#23545;&#19994;&#21153;&#27969;&#31243;&#24615;&#33021;&#26041;&#38754;&#30340;&#27934;&#23519;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#31561;&#24453;&#26102;&#38388;&#23588;&#20026;&#37325;&#35201;&#12290;&#20986;&#20046;&#24847;&#26009;&#30340;&#26159;&#65292;&#30446;&#21069;&#30340;&#33258;&#21160;&#27969;&#31243;&#21457;&#29616;&#25216;&#26415;&#29983;&#25104;&#30452;&#25509;&#21518;&#32493;&#22270;&#21644;&#21487;&#27604;&#36739;&#30340;&#27969;&#31243;&#27169;&#22411;&#65292;&#20294;&#24448;&#24448;&#38169;&#22833;&#20102;&#26126;&#30830;&#34920;&#31034;&#26102;&#38388;&#36724;&#30340;&#26426;&#20250;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26500;&#24314;&#19982;&#26102;&#38388;&#36724;&#26126;&#30830;&#23545;&#40784;&#30340;&#27969;&#31243;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20197;&#30452;&#25509;&#21518;&#32493;&#22270;&#20026;&#20363;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;BPIC&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#19987;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#31361;&#26174;&#20102;&#19982;&#26631;&#20934;&#24067;&#23616;&#25216;&#26415;&#30456;&#27604;&#65292;&#36825;&#31181;&#34920;&#31034;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key concern of automatic process discovery is to provide insights into performance aspects of business processes. Waiting times are of particular importance in this context. For that reason, it is surprising that current techniques for automatic process discovery generate directly-follows graphs and comparable process models, but often miss the opportunity to explicitly represent the time axis. In this paper, we present an approach for automatically constructing process models that explicitly align with a time axis. We exemplify our approach for directly-follows graphs. Our evaluation using two BPIC datasets and a proprietary dataset highlight the benefits of this representation in comparison to standard layout techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26102;&#38388;&#22270;&#23398;&#20064;&#30340;&#22522;&#26412;&#30693;&#35782;&#65292;&#21253;&#25324;TGL&#26694;&#26550;&#20013;&#30340;&#37325;&#35201;&#27010;&#24565;&#12289;&#30456;&#20851;&#30340;&#23398;&#20064;&#26550;&#26500;&#20197;&#21450;&#32463;&#20856;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#20026;TGL&#30340;&#21487;&#35299;&#37322;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#28789;&#24863;&#12290;</title><link>http://arxiv.org/abs/2401.03988</link><description>&lt;p&gt;
&#26102;&#38388;&#22270;&#23398;&#20064;&#30340;&#22522;&#26412;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
A Primer on Temporal Graph Learning. (arXiv:2401.03988v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26102;&#38388;&#22270;&#23398;&#20064;&#30340;&#22522;&#26412;&#30693;&#35782;&#65292;&#21253;&#25324;TGL&#26694;&#26550;&#20013;&#30340;&#37325;&#35201;&#27010;&#24565;&#12289;&#30456;&#20851;&#30340;&#23398;&#20064;&#26550;&#26500;&#20197;&#21450;&#32463;&#20856;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#20026;TGL&#30340;&#21487;&#35299;&#37322;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#27010;&#24565;&#20808;&#34892;&#30340;&#26041;&#27861;&#65292;&#20351;&#35835;&#32773;&#29087;&#24713;&#26102;&#38388;&#22270;&#23398;&#20064;&#65288;TGL&#65289;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#20171;&#32461;&#20102;&#29702;&#35299;TGL&#26694;&#26550;&#25152;&#24517;&#19981;&#21487;&#23569;&#30340;&#37325;&#35201;&#27010;&#24565;&#12290;&#38500;&#20102;&#23450;&#24615;&#35299;&#37322;&#65292;&#25105;&#20204;&#36824;&#22312;&#36866;&#29992;&#30340;&#24773;&#20917;&#19979;&#24341;&#20837;&#20102;&#25968;&#23398;&#20844;&#24335;&#65292;&#25552;&#21319;&#20102;&#25991;&#26412;&#30340;&#28165;&#26224;&#24230;&#12290;&#30001;&#20110;TGL&#28041;&#21450;&#26102;&#38388;&#21644;&#31354;&#38388;&#23398;&#20064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#30456;&#20851;&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#20174;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21040;&#36716;&#25442;&#22120;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#32463;&#20856;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#20197;&#28608;&#21457;&#23545;TGL&#30340;&#21487;&#35299;&#37322;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#30340;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
This document aims to familiarize readers with temporal graph learning (TGL) through a concept-first approach. We have systematically presented vital concepts essential for understanding the workings of a TGL framework. In addition to qualitative explanations, we have incorporated mathematical formulations where applicable, enhancing the clarity of the text. Since TGL involves temporal and spatial learning, we introduce relevant learning architectures ranging from recurrent and convolutional neural networks to transformers and graph neural networks. We also discuss classical time series forecasting methods to inspire interpretable learning solutions for TGL.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20174;&#37325;&#29616;&#20135;&#21697;&#39044;&#27979;&#28909;&#24102;&#27668;&#26059;&#29983;&#25104;&#30340;&#32463;&#39564;&#20013;&#25552;&#28860;&#20986;&#30340;&#25945;&#35757;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20108;&#32500;&#26694;&#26550;&#26469;&#25351;&#23548;&#22806;&#37096;&#30740;&#31350;&#30340;&#20877;&#29616;&#21644;&#22797;&#21046;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#25968;&#25454;&#38598;&#12289;&#24230;&#37327;&#25351;&#26631;&#21644;&#27169;&#22411;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#22312;&#33258;&#24049;&#30340;&#30740;&#31350;&#20013;&#24212;&#29992;&#21069;&#26399;&#24037;&#20316;&#21644;&#25351;&#23548;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2401.03736</link><description>&lt;p&gt;
&#35838;&#31243;&#23398;&#20064;&#65306;&#21487;&#37325;&#22797;&#24615;&#12289;&#21487;&#22797;&#21046;&#24615;&#21644;&#20572;&#27490;&#26102;&#26426;
&lt;/p&gt;
&lt;p&gt;
Lessons Learned: Reproducibility, Replicability, and When to Stop. (arXiv:2401.03736v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20174;&#37325;&#29616;&#20135;&#21697;&#39044;&#27979;&#28909;&#24102;&#27668;&#26059;&#29983;&#25104;&#30340;&#32463;&#39564;&#20013;&#25552;&#28860;&#20986;&#30340;&#25945;&#35757;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20108;&#32500;&#26694;&#26550;&#26469;&#25351;&#23548;&#22806;&#37096;&#30740;&#31350;&#30340;&#20877;&#29616;&#21644;&#22797;&#21046;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#25968;&#25454;&#38598;&#12289;&#24230;&#37327;&#25351;&#26631;&#21644;&#27169;&#22411;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#22312;&#33258;&#24049;&#30340;&#30740;&#31350;&#20013;&#24212;&#29992;&#21069;&#26399;&#24037;&#20316;&#21644;&#25351;&#23548;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30830;&#20445;&#33258;&#24049;&#30740;&#31350;&#30340;&#21487;&#37325;&#22797;&#24615;&#26041;&#38754;&#24050;&#32463;&#26377;&#20102;&#24191;&#27867;&#30340;&#25351;&#23548;&#65292;&#20294;&#20851;&#20110;&#22914;&#20309;&#22312;&#33258;&#24049;&#30340;&#30740;&#31350;&#20013;&#23545;&#22806;&#37096;&#30740;&#31350;&#36827;&#34892;&#20877;&#29616;&#21644;&#22797;&#21046;&#30340;&#35752;&#35770;&#24456;&#23569;&#12290;&#20026;&#20102;&#24341;&#21457;&#36825;&#20010;&#35752;&#35770;&#65292;&#25105;&#20204;&#20174;&#25105;&#20204;&#37325;&#29616;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#28909;&#24102;&#27668;&#26059;&#29983;&#25104;&#30340;&#36816;&#33829;&#20135;&#21697;&#30340;&#32463;&#39564;&#20013;&#25552;&#21462;&#25945;&#35757;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20108;&#32500;&#26694;&#26550;&#65292;&#20197;&#25552;&#20379;&#20851;&#20110;&#20877;&#29616;&#21644;&#22797;&#21046;&#30340;&#25351;&#23548;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#19968;&#20010;&#36724;&#19978;&#34920;&#31034;&#27169;&#22411;&#25311;&#21512;&#65292;&#22312;&#21478;&#19968;&#20010;&#36724;&#19978;&#34920;&#31034;&#25512;&#29702;&#20013;&#30340;&#20351;&#29992;&#65292;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#12289;&#24230;&#37327;&#25351;&#26631;&#21644;&#27169;&#22411;&#36825;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#36890;&#36807;&#22312;&#36825;&#20010;&#20108;&#32500;&#24179;&#38754;&#19978;&#35780;&#20272;&#25105;&#20204;&#30740;&#31350;&#30340;&#36712;&#36857;&#65292;&#25105;&#20204;&#21487;&#20197;&#26356;&#22909;&#22320;&#35828;&#26126;&#20351;&#29992;&#25105;&#20204;&#30740;&#31350;&#24471;&#20986;&#30340;&#32467;&#35770;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#26469;&#35780;&#20272;&#22312;&#22823;&#27668;&#31185;&#23398;&#20013;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#20108;&#32500;&#26694;&#26550;&#20026;&#30740;&#31350;&#20154;&#21592;&#65292;&#29305;&#21035;&#26159;&#21021;&#32423;&#30740;&#31350;&#20154;&#21592;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24037;&#20855;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#23558;&#21069;&#26399;&#24037;&#20316;&#32435;&#20837;&#21040;&#33258;&#24049;&#30340;&#30740;&#31350;&#20013;&#65292;&#24182;&#25351;&#23548;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
While extensive guidance exists for ensuring the reproducibility of one's own study, there is little discussion regarding the reproduction and replication of external studies within one's own research. To initiate this discussion, drawing lessons from our experience reproducing an operational product for predicting tropical cyclogenesis, we present a two-dimensional framework to offer guidance on reproduction and replication. Our framework, representing model fitting on one axis and its use in inference on the other, builds upon three key aspects: the dataset, the metrics, and the model itself. By assessing the trajectories of our studies on this 2D plane, we can better inform the claims made using our research. Additionally, we use this framework to contextualize the utility of benchmark datasets in the atmospheric sciences. Our two-dimensional framework provides a tool for researchers, especially early career researchers, to incorporate prior work in their own research and to inform 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#25289;&#26684;&#26391;&#26085;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#24320;&#21019;&#24615;&#25193;&#23637;&#30340;&#24191;&#20041;&#25289;&#26684;&#26391;&#26085;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#24191;&#20041;&#25289;&#26684;&#26391;&#26085;&#26041;&#31243;&#36816;&#29992;&#20110;&#27169;&#22411;&#26500;&#24314;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#24182;&#20445;&#35777;&#20102;&#25289;&#26684;&#26391;&#26085;&#26041;&#31243;&#30340;&#25104;&#31435;&#12290;</title><link>http://arxiv.org/abs/2401.03728</link><description>&lt;p&gt;
&#24191;&#20041;&#25289;&#26684;&#26391;&#26085;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generalized Lagrangian Neural Networks. (arXiv:2401.03728v2 [math.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#25289;&#26684;&#26391;&#26085;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#24320;&#21019;&#24615;&#25193;&#23637;&#30340;&#24191;&#20041;&#25289;&#26684;&#26391;&#26085;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#24191;&#20041;&#25289;&#26684;&#26391;&#26085;&#26041;&#31243;&#36816;&#29992;&#20110;&#27169;&#22411;&#26500;&#24314;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#24182;&#20445;&#35777;&#20102;&#25289;&#26684;&#26391;&#26085;&#26041;&#31243;&#30340;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#27714;&#35299;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#25968;&#23398;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#22312;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20013;&#65292;&#23558;ODEs&#30340;&#20869;&#22312;&#32467;&#26500;&#25972;&#21512;&#36827;&#21435;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#33021;&#21147;&#65292;&#20943;&#23569;&#25968;&#25454;&#20351;&#29992;&#37327;&#12290;&#22312;&#36825;&#20123;&#32467;&#26500;&#21270;ODE&#24418;&#24335;&#20013;&#65292;&#25289;&#26684;&#26391;&#26085;&#34920;&#31034;&#27861;&#30001;&#20110;&#20854;&#37325;&#35201;&#30340;&#29289;&#29702;&#22522;&#30784;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19978;&#65292;Bhattoo&#24341;&#20837;&#20102;&#25289;&#26684;&#26391;&#26085;&#31070;&#32463;&#32593;&#32476;&#65288;LNNs&#65289;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#23545;&#25289;&#26684;&#26391;&#26085;&#31070;&#32463;&#32593;&#32476;&#65288;LNNs&#65289;&#36827;&#34892;&#20102;&#24320;&#21019;&#24615;&#25193;&#23637;&#30340;&#24191;&#20041;&#25289;&#26684;&#26391;&#26085;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#21033;&#29992;&#25289;&#26684;&#26391;&#26085;&#26041;&#31243;&#20013;&#25289;&#26684;&#26391;&#26085;&#37327;&#30340;&#22522;&#26412;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#26681;&#25454;&#24191;&#20041;&#25289;&#26684;&#26391;&#26085;&#26041;&#31243;&#26469;&#26500;&#24314;&#27169;&#22411;&#12290;&#36825;&#31181;&#20462;&#25913;&#19981;&#20165;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#20445;&#35777;&#20102;&#25289;&#26684;&#26391;&#26085;&#26041;&#31243;&#30340;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating neural networks for the solution of Ordinary Differential Equations (ODEs) represents a pivotal research direction within computational mathematics. Within neural network architectures, the integration of the intrinsic structure of ODEs offers advantages such as enhanced predictive capabilities and reduced data utilization. Among these structural ODE forms, the Lagrangian representation stands out due to its significant physical underpinnings. Building upon this framework, Bhattoo introduced the concept of Lagrangian Neural Networks (LNNs). Then in this article, we introduce a groundbreaking extension (Genralized Lagrangian Neural Networks) to Lagrangian Neural Networks (LNNs), innovatively tailoring them for non-conservative systems. By leveraging the foundational importance of the Lagrangian within Lagrange's equations, we formulate the model based on the generalized Lagrange's equation. This modification not only enhances prediction accuracy but also guarantees Lagrang
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20998;&#35789;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#29983;&#25104;&#20013;&#22269;&#21476;&#20856;&#35799;&#35789;&#65292;&#24182;&#35299;&#20915;&#20102;&#26684;&#24335;&#19981;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;&#39564;&#35777;&#20102;&#29616;&#26377;&#22522;&#20110;&#20998;&#35789;&#30340;&#27169;&#22411;&#22312;&#23383;&#31526;&#21644;&#20998;&#35789;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#30340;&#30693;&#35782;&#26377;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23450;&#21046;&#27169;&#22411;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.03512</link><description>&lt;p&gt;
&#26080;&#38656;&#20998;&#35789;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20197;&#26356;&#20934;&#30830;&#30340;&#26684;&#24335;&#29983;&#25104;&#20013;&#22269;&#21476;&#20856;&#35799;&#35789;
&lt;/p&gt;
&lt;p&gt;
Token-free LLMs Can Generate Chinese Classical Poetry with More Accurate Format. (arXiv:2401.03512v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20998;&#35789;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#29983;&#25104;&#20013;&#22269;&#21476;&#20856;&#35799;&#35789;&#65292;&#24182;&#35299;&#20915;&#20102;&#26684;&#24335;&#19981;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;&#39564;&#35777;&#20102;&#29616;&#26377;&#22522;&#20110;&#20998;&#35789;&#30340;&#27169;&#22411;&#22312;&#23383;&#31526;&#21644;&#20998;&#35789;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#30340;&#30693;&#35782;&#26377;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23450;&#21046;&#27169;&#22411;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#21644;Qwen-chat&#65289;&#33021;&#22815;&#26681;&#25454;&#20154;&#31867;&#30340;&#25351;&#20196;&#29983;&#25104;&#20013;&#22269;&#21476;&#20856;&#35799;&#35789;&#12290;&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#22312;&#20869;&#23481;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36890;&#24120;&#22312;&#26684;&#24335;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#27599;&#34892;&#23383;&#31526;&#30340;&#25968;&#37327;&#26377;&#26102;&#36807;&#22810;&#25110;&#19981;&#36275;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#26368;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#22522;&#20110;&#20998;&#35789;&#30340;&#65292;&#25105;&#20204;&#35748;&#20026;&#26684;&#24335;&#19981;&#20934;&#30830;&#26159;&#30001;&#20110;"&#20998;&#35789;&#35268;&#21010;"&#20219;&#21153;&#30340;&#38590;&#24230;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#20934;&#30830;&#30693;&#36947;&#27599;&#20010;&#20998;&#35789;&#20013;&#21253;&#21547;&#22810;&#23569;&#20010;&#23383;&#31526;&#65292;&#24182;&#22522;&#20110;&#36825;&#20010;&#30693;&#35782;&#36827;&#34892;&#38271;&#24230;&#25511;&#21046;&#35268;&#21010;&#12290;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#23637;&#31034;&#29616;&#26377;&#30340;&#22522;&#20110;&#20998;&#35789;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#35789;&#21644;&#23383;&#31526;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#30693;&#35782;&#26377;&#38480;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#25340;&#20889;&#27604;&#36187;&#25506;&#27979;&#31243;&#24207;&#65292;&#24182;&#21457;&#29616;Qwen-chat&#22312;&#36817;15%&#30340;&#20013;&#25991;&#25340;&#20889;&#27979;&#35797;&#20013;&#22833;&#36133;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#35789;&#30340;&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#23450;&#21046;&#25104;&#26080;&#38656;&#20998;&#35789;&#30340;&#27169;&#22411;&#65288;&#23545;&#20110;&#20013;&#25991;&#26469;&#35828;&#65289;&#65292;&#20174;&#32780;&#33021;&#22815;&#24456;&#22823;&#31243;&#24230;&#19978;&#35299;&#20915;&#26684;&#24335;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuned large language models (such as ChatGPT and Qwen-chat) can generate Chinese classical poetry following human's instructions. LLMs perform well in content, but are usually lacking in format, with occasionally excess or insufficient number of characters in each line. Since most SOTA LLMs are token-based, we assume that the format inaccuracy is due to the difficulty of the "token planning" task, which means that the LLM need to know exactly how much characters are contained in each token and do length-control planning based on that knowledge. In this paper, we first confirm our assumption by showing that existing token-based large language models has limited knowledge on token-character relationship. We use a spelling bee probing procedure, and find that Qwen-chat failed in nearly 15% Chinese spelling test. We then show that a token-based model can be easily tailored into a token-free model (in terms of Chinese), which can largely solve the format accuracy problem. Our tailoring 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;SGGRL&#65292;&#23558;&#24207;&#21015;&#12289;&#22270;&#24418;&#21644;&#20960;&#20309;&#29305;&#24449;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.03369</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#22312;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#24207;&#21015;&#12289;&#22270;&#24418;&#12289;&#20960;&#20309;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Representation Learning for Molecular Property Prediction: Sequence, Graph, Geometry. (arXiv:2401.03369v2 [q-bio.MN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03369
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;SGGRL&#65292;&#23558;&#24207;&#21015;&#12289;&#22270;&#24418;&#21644;&#20960;&#20309;&#29305;&#24449;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#26159;&#23558;&#20998;&#23376;&#19982;&#26576;&#20123;&#29983;&#21270;&#24615;&#36136;&#36827;&#34892;&#26631;&#35760;&#30340;&#20219;&#21153;&#65292;&#23545;&#33647;&#29289;&#21457;&#29616;&#21644;&#35774;&#35745;&#36807;&#31243;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#24050;&#25104;&#20026;&#20256;&#32479;&#26041;&#27861;&#36164;&#28304;&#23494;&#38598;&#22411;&#24615;&#36136;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#26159;&#20915;&#23450;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#24207;&#21015;&#12289;&#22270;&#24418;&#21644;&#20960;&#20309;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#21482;&#20851;&#27880;&#19968;&#31181;&#27169;&#24577;&#29992;&#20110;&#23398;&#20064;&#20998;&#23376;&#34920;&#31034;&#65292;&#26410;&#33021;&#20840;&#38754;&#25429;&#25417;&#20998;&#23376;&#30340;&#29305;&#24449;&#21644;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;SGGRL&#65292;&#23558;&#24207;&#21015;&#12289;&#22270;&#24418;&#21644;&#20960;&#20309;&#29305;&#24449;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular property prediction refers to the task of labeling molecules with some biochemical properties, playing a pivotal role in the drug discovery and design process. Recently, with the advancement of machine learning, deep learning-based molecular property prediction has emerged as a solution to the resource-intensive nature of traditional methods, garnering significant attention. Among them, molecular representation learning is the key factor for molecular property prediction performance. And there are lots of sequence-based, graph-based, and geometry-based methods that have been proposed. However, the majority of existing studies focus solely on one modality for learning molecular representations, failing to comprehensively capture molecular characteristics and information. In this paper, a novel multi-modal representation learning model, which integrates the sequence, graph, and geometry characteristics, is proposed for molecular property prediction, called SGGRL. Specifically, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#25968;&#25454;&#22810;&#26679;&#24615;&#27010;&#24565;&#26469;&#32479;&#19968;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#22522;&#20110;&#29256;&#26412;&#31354;&#38388;&#12289;&#27491;&#21017;&#21270;&#20248;&#21270;&#21644;&#21518;&#39564;&#37319;&#26679;&#30340;&#31639;&#27861;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#36798;&#21040;&#20102;&#21487;&#27604;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.03301</link><description>&lt;p&gt;
&#22312;&#26679;&#26412;&#39640;&#25928;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65306;&#25968;&#25454;&#22810;&#26679;&#24615;&#12289;&#21518;&#39564;&#37319;&#26679;&#65292;&#20197;&#21450;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling, and Beyond. (arXiv:2401.03301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#25968;&#25454;&#22810;&#26679;&#24615;&#27010;&#24565;&#26469;&#32479;&#19968;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#22522;&#20110;&#29256;&#26412;&#31354;&#38388;&#12289;&#27491;&#21017;&#21270;&#20248;&#21270;&#21644;&#21518;&#39564;&#37319;&#26679;&#30340;&#31639;&#27861;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#36798;&#21040;&#20102;&#21487;&#27604;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35797;&#22270;&#29702;&#35299;&#20160;&#20040;&#20419;&#36827;&#20102;&#23545;&#20110;&#24207;&#36125;&#21494;&#26031;&#20915;&#31574;&#30340;&#21382;&#21490;&#25968;&#25454;&#38598;&#36827;&#34892;&#26679;&#26412;&#39640;&#25928;&#23398;&#20064;&#65292;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#34987;&#31216;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#20110;&#22312;&#21033;&#29992;&#65288;&#20540;&#65289;&#20989;&#25968;&#36924;&#36817;&#30340;&#21516;&#26102;&#20139;&#21463;&#26679;&#26412;&#25928;&#29575;&#30340;&#31639;&#27861;&#24863;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#21253;&#25324;&#31163;&#32447;RL&#20013;&#35206;&#30422;&#24230;&#37327;&#30340;&#20808;&#21069;&#27010;&#24565;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#27010;&#24565;&#26469;&#35299;&#20915;&#36825;&#20123;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#19988;&#21033;&#29992;&#36825;&#20010;&#27010;&#24565;&#23558;&#22522;&#20110;&#29256;&#26412;&#31354;&#38388;&#65288;VS&#65289;&#12289;&#27491;&#21017;&#21270;&#20248;&#21270;&#65288;RO&#65289;&#21644;&#21518;&#39564;&#37319;&#26679;&#65288;PS&#65289;&#30340;&#19977;&#20010;&#19981;&#21516;&#31867;&#21035;&#30340;&#31163;&#32447;RL&#31639;&#27861;&#36827;&#34892;&#32479;&#19968;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#35777;&#26126;&#65292;&#22522;&#20110;VS&#12289;&#22522;&#20110;RO&#21644;&#22522;&#20110;PS&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;\emph{&#21487;&#27604;}&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#36825;&#24674;&#22797;&#20102;&#22312;&#26377;&#38480;&#21644;&#32447;&#24615;&#27169;&#22411;&#31867;&#21035;&#19979;&#30340;&#26368;&#20248;&#24615;&#30340;&#26631;&#20934;&#20551;&#35774;&#30340;&#36793;&#30028;&#12290;&#36825;&#20010;&#32467;&#26524;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#19981;&#20855;&#26377;&#26377;&#21033;&#24615;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We seek to understand what facilitates sample-efficient learning from historical datasets for sequential decision-making, a problem that is popularly known as offline reinforcement learning (RL). Further, we are interested in algorithms that enjoy sample efficiency while leveraging (value) function approximation. In this paper, we address these fundamental questions by (i) proposing a notion of data diversity that subsumes the previous notions of coverage measures in offline RL and (ii) using this notion to {unify} three distinct classes of offline RL algorithms based on version spaces (VS), regularized optimization (RO), and posterior sampling (PS). We establish that VS-based, RO-based, and PS-based algorithms, under standard assumptions, achieve \emph{comparable} sample efficiency, which recovers the state-of-the-art sub-optimality bounds for finite and linear model classes with the standard assumptions. This result is surprising, given that the prior work suggested an unfavorable sa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#23646;&#24615;&#20999;&#25442;&#8221;&#30340;&#20844;&#24179;&#25277;&#26679;&#26426;&#21046;&#65292;&#29992;&#20110;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#28151;&#28102;&#25935;&#24863;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#29983;&#25104;&#20844;&#24179;&#25968;&#25454;&#21644;&#20445;&#25345;&#25968;&#25454;&#25928;&#29992;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.03140</link><description>&lt;p&gt;
&#36890;&#36807;&#20999;&#25442;&#26426;&#21046;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#23454;&#29616;&#20844;&#24179;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Fair Sampling in Diffusion Models through Switching Mechanism. (arXiv:2401.03140v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#23646;&#24615;&#20999;&#25442;&#8221;&#30340;&#20844;&#24179;&#25277;&#26679;&#26426;&#21046;&#65292;&#29992;&#20110;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#28151;&#28102;&#25935;&#24863;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#29983;&#25104;&#20844;&#24179;&#25968;&#25454;&#21644;&#20445;&#25345;&#25968;&#25454;&#25928;&#29992;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#33391;&#22909;&#36924;&#36817;&#28508;&#22312;&#27010;&#29575;&#20998;&#24067;&#65292;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#20844;&#24179;&#24615;&#26041;&#38754;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#30340;&#20869;&#22312;&#20559;&#24046;&#30340;&#25918;&#22823;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#30340;&#25277;&#26679;&#36807;&#31243;&#21487;&#20197;&#36890;&#36807;&#26465;&#20214;&#24341;&#23548;&#26469;&#25511;&#21046;&#65292;&#20294;&#20043;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#25214;&#21040;&#23454;&#35777;&#24341;&#23548;&#26469;&#23454;&#29616;&#23450;&#37327;&#20844;&#24179;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#23646;&#24615;&#20999;&#25442;&#8221;&#26426;&#21046;&#30340;&#20855;&#26377;&#20844;&#24179;&#24847;&#35782;&#30340;&#25277;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#12290;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#25277;&#26679;&#26041;&#27861;&#21487;&#20197;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#28151;&#28102;&#25935;&#24863;&#23646;&#24615;&#65292;&#32780;&#19981;&#20381;&#36182;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#20174;&#25968;&#23398;&#19978;&#35777;&#26126;&#20102;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;(i)&#29983;&#25104;&#20844;&#24179;&#25968;&#25454;&#21644;(ii)&#20445;&#25345;&#29983;&#25104;&#25968;&#25454;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have shown their effectiveness in generation tasks by well-approximating the underlying probability distribution. However, diffusion models are known to suffer from an amplified inherent bias from the training data in terms of fairness. While the sampling process of diffusion models can be controlled by conditional guidance, previous works have attempted to find empirical guidance to achieve quantitative fairness. To address this limitation, we propose a fairness-aware sampling method called \textit{attribute switching} mechanism for diffusion models. Without additional training, the proposed sampling can obfuscate sensitive attributes in generated data without relying on classifiers. We mathematically prove and experimentally demonstrate the effectiveness of the proposed method on two key aspects: (i) the generation of fair data and (ii) the preservation of the utility of the generated data.
&lt;/p&gt;</description></item><item><title>The Tactician's Web&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#24418;&#24335;&#21270;&#25968;&#23398;&#30693;&#35782;&#32593;&#32476;&#65292;&#36890;&#36807;Coq&#35777;&#26126;&#21161;&#25163;&#21644;&#20016;&#23500;&#30340;&#25968;&#25454;&#34920;&#31034;&#19982;&#35777;&#26126;&#24037;&#31243;&#24072;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#25552;&#20379;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#20998;&#26512;&#21644;&#35777;&#26126;&#24037;&#31243;&#30340;&#23454;&#29992;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2401.02950</link><description>&lt;p&gt;
The Tactician's Web&#30340;&#22823;&#35268;&#27169;&#24418;&#24335;&#30693;&#35782;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
The Tactician's Web of Large-Scale Formal Knowledge. (arXiv:2401.02950v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02950
&lt;/p&gt;
&lt;p&gt;
The Tactician's Web&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#24418;&#24335;&#21270;&#25968;&#23398;&#30693;&#35782;&#32593;&#32476;&#65292;&#36890;&#36807;Coq&#35777;&#26126;&#21161;&#25163;&#21644;&#20016;&#23500;&#30340;&#25968;&#25454;&#34920;&#31034;&#19982;&#35777;&#26126;&#24037;&#31243;&#24072;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#25552;&#20379;&#20102;&#26426;&#22120;&#23398;&#20064;&#12289;&#20998;&#26512;&#21644;&#35777;&#26126;&#24037;&#31243;&#30340;&#23454;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
The Tactician's Web&#26159;&#19968;&#20010;&#24179;&#21488;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24378;&#20851;&#32852;&#30340;&#12289;&#26426;&#22120;&#39564;&#35777;&#30340;&#24418;&#24335;&#21270;&#25968;&#23398;&#30693;&#35782;&#32593;&#32476;&#65292;&#26041;&#20415;&#26426;&#22120;&#23398;&#20064;&#12289;&#20998;&#26512;&#21644;&#35777;&#26126;&#24037;&#31243;&#12290;&#22522;&#20110;Coq&#35777;&#26126;&#21161;&#25163;&#26500;&#24314;&#65292;&#35813;&#24179;&#21488;&#23548;&#20986;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#24418;&#24335;&#29702;&#35770;&#30340;&#25968;&#25454;&#38598;&#65292;&#21576;&#29616;&#20026;&#23450;&#20041;&#12289;&#23450;&#29702;&#12289;&#35777;&#26126;&#39033;&#12289;&#31574;&#30053;&#21644;&#35777;&#26126;&#29366;&#24577;&#30340;&#32593;&#32476;&#12290;&#29702;&#35770;&#26082;&#21487;&#20197;&#32534;&#30721;&#20026;&#35821;&#20041;&#22270;&#65288;&#22914;&#19979;&#25152;&#31034;&#65289;&#65292;&#20063;&#21487;&#20197;&#20316;&#20026;&#21487;&#35835;&#30340;&#25991;&#26412;&#65292;&#21508;&#26377;&#20248;&#32570;&#28857;&#12290;&#35777;&#26126;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#30456;&#21516;&#30340;&#20016;&#23500;&#25968;&#25454;&#34920;&#31034;&#19982;Coq&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#21487;&#20197;&#22312;&#19968;&#32452;&#23450;&#29702;&#19978;&#33258;&#21160;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#19982;Coq&#30340;&#32039;&#23494;&#38598;&#25104;&#25552;&#20379;&#20102;&#23558;&#20195;&#29702;&#20316;&#20026;&#23454;&#29992;&#24037;&#20855;&#25552;&#20379;&#32473;&#35777;&#26126;&#24037;&#31243;&#24072;&#30340;&#29420;&#29305;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Tactician's Web is a platform offering a large web of strongly interconnected, machine-checked, formal mathematical knowledge conveniently packaged for machine learning, analytics, and proof engineering. Built on top of the Coq proof assistant, the platform exports a dataset containing a wide variety of formal theories, presented as a web of definitions, theorems, proof terms, tactics, and proof states. Theories are encoded both as a semantic graph (rendered below) and as human-readable text, each with a unique set of advantages and disadvantages. Proving agents may interact with Coq through the same rich data representation and can be automatically benchmarked on a set of theorems. Tight integration with Coq provides the unique possibility to make agents available to proof engineers as practical tools.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Graph2Tac&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#23450;&#29702;&#35777;&#26126;&#20013;&#23398;&#20064;&#25968;&#23398;&#27010;&#24565;&#30340;&#20998;&#23618;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#21160;&#24577;&#22320;&#23558;&#26032;&#30340;&#25968;&#23398;&#27010;&#24565;&#32435;&#20837;&#21040;&#30693;&#35782;&#24211;&#20013;&#65292;&#24182;&#22312;Coq&#35777;&#26126;&#21161;&#25163;&#20013;&#36827;&#34892;&#35757;&#32451;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.02949</link><description>&lt;p&gt;
Graph2Tac: &#22312;&#23450;&#29702;&#35777;&#26126;&#20013;&#23398;&#20064;&#25968;&#23398;&#27010;&#24565;&#30340;&#20998;&#23618;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Graph2Tac: Learning Hierarchical Representations of Math Concepts in Theorem proving. (arXiv:2401.02949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Graph2Tac&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#23450;&#29702;&#35777;&#26126;&#20013;&#23398;&#20064;&#25968;&#23398;&#27010;&#24565;&#30340;&#20998;&#23618;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#21160;&#24577;&#22320;&#23558;&#26032;&#30340;&#25968;&#23398;&#27010;&#24565;&#32435;&#20837;&#21040;&#30693;&#35782;&#24211;&#20013;&#65292;&#24182;&#22312;Coq&#35777;&#26126;&#21161;&#25163;&#20013;&#36827;&#34892;&#35757;&#32451;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#21450;&#20854;&#24212;&#29992;&#20013;&#23384;&#22312;&#22823;&#37327;&#30340;&#27010;&#24565;&#12290;&#23427;&#20204;&#22312;&#19981;&#21516;&#30340;&#23398;&#31185;&#39046;&#22495;&#20013;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#65292;&#24182;&#19988;&#27599;&#31687;&#25968;&#23398;&#35770;&#25991;&#25110;&#24212;&#29992;&#20013;&#37117;&#20250;&#24341;&#20837;&#26032;&#30340;&#27010;&#24565;&#12290;&#24418;&#24335;&#21270;&#29702;&#35770;&#24314;&#31435;&#20102;&#19968;&#20010;&#23618;&#27425;&#32467;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#23450;&#20041;&#12289;&#23450;&#29702;&#21644;&#30456;&#20114;&#24341;&#29992;&#30340;&#35777;&#26126;&#12290;&#24403;&#19968;&#20010;AI&#20195;&#29702;&#20154;&#35777;&#26126;&#19968;&#20010;&#26032;&#30340;&#23450;&#29702;&#26102;&#65292;&#22823;&#22810;&#25968;&#19982;&#35813;&#23450;&#29702;&#30456;&#20851;&#30340;&#25968;&#23398;&#27010;&#24565;&#21644;&#24341;&#29702;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#20174;&#26410;&#34987;&#35265;&#36807;&#12290;&#36825;&#22312;Coq&#35777;&#26126;&#21161;&#25163;&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;&#35813;&#21161;&#25163;&#25317;&#26377;&#21508;&#31181;&#21508;&#26679;&#30340;Coq&#39033;&#30446;&#65292;&#27599;&#20010;&#39033;&#30446;&#37117;&#26377;&#33258;&#24049;&#30340;&#23450;&#20041;&#12289;&#24341;&#29702;&#65292;&#29978;&#33267;&#29992;&#20110;&#35777;&#26126;&#36825;&#20123;&#24341;&#29702;&#30340;&#33258;&#23450;&#20041;&#31574;&#30053;&#36807;&#31243;&#12290;&#23558;&#36825;&#26679;&#30340;&#26032;&#20449;&#24687;&#21363;&#26102;&#22320;&#34701;&#20837;&#21040;&#20195;&#29702;&#20154;&#30340;&#30693;&#35782;&#24211;&#20013;&#23545;&#20110;&#20195;&#29702;&#20154;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#26032;&#30340;&#12289;&#22823;&#35268;&#27169;&#30340;&#12289;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;Coq&#20013;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#21033;&#29992;Coq&#26415;&#35821;&#30340;&#24544;&#23454;&#22270;&#34920;&#31034;&#65292;&#21019;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;Graph2Tac&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#23450;&#20041;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#21019;&#24314;&#20102;&#19968;&#20010;&#26377;&#21521;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concepts abound in mathematics and its applications. They vary greatly between subject areas, and new ones are introduced in each mathematical paper or application. A formal theory builds a hierarchy of definitions, theorems and proofs that reference each other. When an AI agent is proving a new theorem, most of the mathematical concepts and lemmas relevant to that theorem may have never been seen during training. This is especially true in the Coq proof assistant, which has a diverse library of Coq projects, each with its own definitions, lemmas, and even custom tactic procedures used to prove those lemmas. It is essential for agents to incorporate such new information into their knowledge base on the fly. We work towards this goal by utilizing a new, large-scale, graph-based dataset for machine learning in Coq. We leverage a faithful graph-representation of Coq terms that induces a directed graph of dependencies between definitions to create a novel graph neural network, Graph2Tac (G
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24494;&#21019;&#25163;&#26415;&#35270;&#39057;&#20013;&#24369;&#21322;&#30417;&#30563;&#19979;&#26816;&#27979;&#25163;&#26415;&#24037;&#20855;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20849;&#29616;&#25439;&#22833;&#26469;&#21033;&#29992;&#22270;&#20687;&#32423;&#26631;&#31614;&#20013;&#24037;&#20855;&#23545;&#20043;&#38388;&#30340;&#20849;&#29616;&#20851;&#31995;&#65292;&#24179;&#34913;&#20102;&#27880;&#37322;&#36127;&#25285;&#21644;&#26816;&#27979;&#24615;&#33021;&#65292;&#20811;&#26381;&#20102;&#20998;&#31867;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.02791</link><description>&lt;p&gt;
&#24369;&#21322;&#30417;&#30563;&#19979;&#30340;&#24494;&#21019;&#25163;&#26415;&#35270;&#39057;&#24037;&#20855;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Weakly Semi-supervised Tool Detection in Minimally Invasive Surgery Videos. (arXiv:2401.02791v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24494;&#21019;&#25163;&#26415;&#35270;&#39057;&#20013;&#24369;&#21322;&#30417;&#30563;&#19979;&#26816;&#27979;&#25163;&#26415;&#24037;&#20855;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20849;&#29616;&#25439;&#22833;&#26469;&#21033;&#29992;&#22270;&#20687;&#32423;&#26631;&#31614;&#20013;&#24037;&#20855;&#23545;&#20043;&#38388;&#30340;&#20849;&#29616;&#20851;&#31995;&#65292;&#24179;&#34913;&#20102;&#27880;&#37322;&#36127;&#25285;&#21644;&#26816;&#27979;&#24615;&#33021;&#65292;&#20811;&#26381;&#20102;&#20998;&#31867;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#26415;&#24037;&#20855;&#26816;&#27979;&#23545;&#20110;&#20998;&#26512;&#21644;&#35780;&#20272;&#24494;&#21019;&#25163;&#26415;&#35270;&#39057;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#38656;&#35201;&#22823;&#37327;&#23436;&#25972;&#30340;&#23454;&#20363;&#32423;&#26631;&#31614;&#65288;&#21363;&#36793;&#30028;&#26694;&#65289;&#30340;&#30417;&#30563;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27880;&#37322;&#30340;&#36127;&#25285;&#65292;&#20855;&#26377;&#23454;&#20363;&#32423;&#26631;&#31614;&#30340;&#22823;&#22411;&#22270;&#20687;&#25968;&#25454;&#38598;&#36890;&#24120;&#24456;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#22312;&#25552;&#20379;&#22270;&#20687;&#32423;&#26631;&#31614;&#32780;&#19981;&#26159;&#23454;&#20363;&#32423;&#26631;&#31614;&#26102;&#65292;&#25163;&#26415;&#24037;&#20855;&#26816;&#27979;&#21464;&#24471;&#37325;&#35201;&#65292;&#22240;&#20026;&#22270;&#20687;&#32423;&#27880;&#37322;&#27604;&#23454;&#20363;&#32423;&#27880;&#37322;&#26356;&#20855;&#26102;&#38388;&#25928;&#29575;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#26497;&#39640;&#30340;&#27880;&#37322;&#36127;&#25285;&#21644;&#26816;&#27979;&#24615;&#33021;&#20043;&#38388;&#23547;&#27714;&#24179;&#34913;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20849;&#29616;&#25439;&#22833;&#65292;&#35813;&#25439;&#22833;&#32771;&#34385;&#20102;&#26576;&#20123;&#24037;&#20855;&#23545;&#22312;&#22270;&#20687;&#20013;&#32463;&#24120;&#20849;&#21516;&#20986;&#29616;&#30340;&#29305;&#24615;&#65292;&#20197;&#21033;&#29992;&#22270;&#20687;&#32423;&#26631;&#31614;&#12290;&#29992;&#20849;&#29616;&#25439;&#22833;&#23545;&#20849;&#29616;&#20851;&#31995;&#30340;&#30693;&#35782;&#36827;&#34892;&#23553;&#35013;&#26377;&#21161;&#20110;&#20811;&#26381;&#20998;&#31867;&#22256;&#38590;&#65292;&#22240;&#20026;&#19968;&#20123;&#25163;&#26415;&#24037;&#20855;&#30340;&#20998;&#31867;&#22256;&#38590;&#28304;&#20110;&#36825;&#26679;&#19968;&#20010;&#20107;&#23454;&#65306;&#23427;&#20204;&#32463;&#24120;&#20197;&#25104;&#23545;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surgical tool detection is essential for analyzing and evaluating minimally invasive surgery videos. Current approaches are mostly based on supervised methods that require large, fully instance-level labels (i.e., bounding boxes). However, large image datasets with instance-level labels are often limited because of the burden of annotation. Thus, surgical tool detection is important when providing image-level labels instead of instance-level labels since image-level annotations are considerably more time-efficient than instance-level annotations. In this work, we propose to strike a balance between the extremely costly annotation burden and detection performance. We further propose a co-occurrence loss, which considers a characteristic that some tool pairs often co-occur together in an image to leverage image-level labels. Encapsulating the knowledge of co-occurrence using the co-occurrence loss helps to overcome the difficulty in classification that originates from the fact that some 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#22270;&#30340;&#35299;&#37322;&#26041;&#27861;&#26469;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#34892;&#20026;&#65292;&#36890;&#36807;&#29983;&#25104;&#35299;&#37322;&#35270;&#22270;&#21644;&#22270;&#27169;&#24335;&#26469;&#35299;&#37322;&#29305;&#23450;&#31867;&#21035;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.02086</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
View-based Explanations for Graph Neural Networks. (arXiv:2401.02086v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02086
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#22270;&#30340;&#35299;&#37322;&#26041;&#27861;&#26469;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#34892;&#20026;&#65292;&#36890;&#36807;&#29983;&#25104;&#35299;&#37322;&#35270;&#22270;&#21644;&#22270;&#27169;&#24335;&#26469;&#35299;&#37322;&#29305;&#23450;&#31867;&#21035;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#29983;&#25104;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#35299;&#37322;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22312;&#22270;&#20998;&#31867;&#31561;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#26088;&#22312;&#29702;&#35299;GNNs&#30340;&#25972;&#20307;&#32467;&#26524;&#65292;&#32780;&#19981;&#26159;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#33021;&#36820;&#22238;&#38590;&#20197;&#35775;&#38382;&#25110;&#30452;&#25509;&#26597;&#35810;&#30340;&#35299;&#37322;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33539;&#24335;GVEX&#65292;&#29992;&#20110;&#29983;&#25104;&#22270;&#35299;&#37322;&#30340;&#22270;&#35270;&#22270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20004;&#23618;&#30340;&#35299;&#37322;&#32467;&#26500;&#65292;&#31216;&#20026;&#35299;&#37322;&#35270;&#22270;&#12290;&#35299;&#37322;&#35270;&#22270;&#21253;&#25324;&#19968;&#32452;&#22270;&#27169;&#24335;&#21644;&#19968;&#32452;&#35825;&#21457;&#30340;&#35299;&#37322;&#23376;&#22270;&#12290;&#32473;&#23450;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;&#22270;&#21644;&#30001;&#22522;&#20110;GNN&#30340;&#20998;&#31867;&#22120;M&#20998;&#37197;&#30340;&#29305;&#23450;&#31867;&#21035;&#26631;&#31614;l&#30340;&#25968;&#25454;&#24211;G&#65292;&#23427;&#31616;&#27905;&#22320;&#25551;&#36848;&#20102;&#26368;&#22909;&#35299;&#37322;&#20026;&#20160;&#20040;l&#30001;M&#20998;&#37197;&#30340;G&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#21046;&#23450;&#20102;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#26469;&#35745;&#31639;GNN&#35299;&#37322;&#30340;&#26368;&#20339;&#35299;&#37322;&#35270;&#22270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#38382;&#39064;&#26159;&#931;^2_P&#38590;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating explanations for graph neural networks (GNNs) has been studied to understand their behavior in analytical tasks such as graph classification. Existing approaches aim to understand the overall results of GNNs rather than providing explanations for specific class labels of interest, and may return explanation structures that are hard to access, nor directly queryable.  We propose GVEX, a novel paradigm that generates Graph Views for EXplanation. (1) We design a two-tier explanation structure called explanation views. An explanation view consists of a set of graph patterns and a set of induced explanation subgraphs. Given a database G of multiple graphs and a specific class label l assigned by a GNN-based classifier M, it concisely describes the fraction of G that best explains why l is assigned by M. (2) We propose quality measures and formulate an optimization problem to compute optimal explanation views for GNN explanation. We show that the problem is $\Sigma^2_P$-hard. (3) 
&lt;/p&gt;</description></item><item><title>GPS-SSL&#26159;&#19968;&#31181;&#23558;&#20808;&#39564;&#30693;&#35782;&#27880;&#20837;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#24230;&#37327;&#31354;&#38388;&#24182;&#21033;&#29992;&#26368;&#36817;&#37051;&#37319;&#26679;&#29983;&#25104;&#27491;&#26679;&#26412;&#12290;&#23427;&#21487;&#20197;&#20943;&#23569;&#23545;&#24378;&#25968;&#25454;&#22686;&#24378;&#30340;&#20381;&#36182;&#65292;&#22240;&#27492;&#22312;Cifar10&#19978;&#36798;&#21040;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.01990</link><description>&lt;p&gt;
GPS-SSL: &#24341;&#23548;&#27491;&#26679;&#26412;&#37319;&#26679;&#23558;&#20808;&#39564;&#30693;&#35782;&#27880;&#20837;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;
&lt;/p&gt;
&lt;p&gt;
GPS-SSL: Guided Positive Sampling to Inject Prior Into Self-Supervised Learning. (arXiv:2401.01990v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01990
&lt;/p&gt;
&lt;p&gt;
GPS-SSL&#26159;&#19968;&#31181;&#23558;&#20808;&#39564;&#30693;&#35782;&#27880;&#20837;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#24230;&#37327;&#31354;&#38388;&#24182;&#21033;&#29992;&#26368;&#36817;&#37051;&#37319;&#26679;&#29983;&#25104;&#27491;&#26679;&#26412;&#12290;&#23427;&#21487;&#20197;&#20943;&#23569;&#23545;&#24378;&#25968;&#25454;&#22686;&#24378;&#30340;&#20381;&#36182;&#65292;&#22240;&#27492;&#22312;Cifar10&#19978;&#36798;&#21040;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#24341;&#23548;&#27491;&#26679;&#26412;&#37319;&#26679;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;GPS-SSL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#20808;&#39564;&#30693;&#35782;&#27880;&#20837;&#21040;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#27491;&#26679;&#26412;&#36873;&#25321;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#24403;&#21069;&#30340;SSL&#26041;&#27861;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#29983;&#25104;&#27491;&#26679;&#26412;&#65292;&#24182;&#23558;&#20808;&#39564;&#30693;&#35782;&#32467;&#21512;&#36827;&#21435;&#65292;&#20294;&#26159;&#38169;&#35823;&#25110;&#32773;&#36807;&#24369;&#30340;DA&#20250;&#20005;&#37325;&#38477;&#20302;&#25152;&#23398;&#21040;&#30340;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;GPS-SSL&#21017;&#25552;&#20986;&#35774;&#35745;&#19968;&#20010;&#24230;&#37327;&#31354;&#38388;&#65292;&#20351;&#24471;&#27431;&#27663;&#36317;&#31163;&#25104;&#20026;&#35821;&#20041;&#20851;&#31995;&#30340;&#26377;&#24847;&#20041;&#30340;&#26367;&#20195;&#12290;&#22312;&#36825;&#20010;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#26368;&#36817;&#37051;&#37319;&#26679;&#29983;&#25104;&#27491;&#26679;&#26412;&#12290;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#37117;&#21487;&#20197;&#29420;&#31435;&#22320;&#23884;&#20837;&#21040;&#36825;&#20010;&#24230;&#37327;&#31354;&#38388;&#20013;&#65292;&#32780;&#19981;&#21463;&#25152;&#20351;&#29992;&#30340;DA&#24433;&#21709;&#12290;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#65292;GPS-SSL&#36866;&#29992;&#20110;&#20219;&#20309;SSL&#26041;&#27861;&#65292;&#22914;SimCLR&#25110;BYOL&#12290;GPS-SSL&#30340;&#19968;&#20010;&#20851;&#38190;&#22909;&#22788;&#26159;&#20943;&#23569;&#20102;&#23450;&#21046;&#24378;DA&#30340;&#21387;&#21147;&#12290;&#20363;&#22914;&#65292;GPS-SSL&#22312;Cifar10&#19978;&#20351;&#29992;&#24369;DA&#36798;&#21040;&#20102;85.58&#65285;&#65292;&#32780;&#22522;&#20934;&#20540;&#21482;&#36798;&#21040;&#20102;37.51&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Guided Positive Sampling Self-Supervised Learning (GPS-SSL), a general method to inject a priori knowledge into Self-Supervised Learning (SSL) positive samples selection. Current SSL methods leverage Data-Augmentations (DA) for generating positive samples and incorporate prior knowledge - an incorrect, or too weak DA will drastically reduce the quality of the learned representation. GPS-SSL proposes instead to design a metric space where Euclidean distances become a meaningful proxy for semantic relationship. In that space, it is now possible to generate positive samples from nearest neighbor sampling. Any prior knowledge can now be embedded into that metric space independently from the employed DA. From its simplicity, GPS-SSL is applicable to any SSL method, e.g. SimCLR or BYOL. A key benefit of GPS-SSL is in reducing the pressure in tailoring strong DAs. For example GPS-SSL reaches 85.58% on Cifar10 with weak DA while the baseline only reaches 37.51%. We therefore move a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;</title><link>http://arxiv.org/abs/2401.01854</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multilingual Instruction Tuning With Just a Pinch of Multilinguality. (arXiv:2401.01854v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20840;&#29699;&#37319;&#32435;&#65292;&#23427;&#20204;&#22312;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#36890;&#36807;&#22312;&#21478;&#19968;&#31181;&#35821;&#35328;&#19978;&#24494;&#35843;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#26576;&#31181;&#35821;&#35328;&#19978;&#33719;&#24471;&#29305;&#23450;&#30340;&#21151;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;LLM&#22312;&#25351;&#20196;&#35843;&#20248;&#36807;&#31243;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#39318;&#20808;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#33521;&#35821;&#35843;&#20248;&#38598;&#21512;&#20013;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#65292;&#22312;&#35843;&#20248;&#36807;&#31243;&#20013;&#19981;&#35770;&#26159;&#24050;&#35265;&#35821;&#35328;&#36824;&#26159;&#26410;&#35265;&#35821;&#35328;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial. One promising approach is cross-lingual transfer, where a model acquires specific functionality on some language by finetuning on another language. In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages. We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning. Furthermore, we find that only 40 multilingual examples in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning. In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in several languages compared to monolingually tuned models, despite training on 10x fewer examples in those language
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01286</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#19982;&#20154;&#31867;&#20132;&#27969;&#32039;&#23494;&#30456;&#20284;&#30340;&#25991;&#26412;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20854;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26174;&#33879;&#35745;&#31639;&#38656;&#27714;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#21442;&#25968;&#21270;&#36896;&#25104;&#30340;&#12290;&#36825;&#19968;&#25361;&#25112;&#22312;&#20110;&#19990;&#30028;&#30340;&#21160;&#24577;&#24615;&#65292;&#38656;&#35201;&#39057;&#32321;&#26356;&#26032;LLM&#20197;&#20462;&#27491;&#36807;&#26102;&#30340;&#20449;&#24687;&#25110;&#38598;&#25104;&#26032;&#30693;&#35782;&#65292;&#20174;&#32780;&#30830;&#20445;&#20854;&#25345;&#32493;&#30340;&#30456;&#20851;&#24615;&#12290;&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#22312;&#35757;&#32451;&#21518;&#36827;&#34892;&#25345;&#32493;&#30340;&#27169;&#22411;&#35843;&#25972;&#65292;&#20197;&#35299;&#20915;&#32570;&#38519;&#25110;&#19981;&#33391;&#34892;&#20026;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;LLM&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#39640;&#65292;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#26377;&#25928;&#22320;&#20462;&#25913;LLM&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#22312;&#21508;&#31181;&#36755;&#20837;&#20013;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#39318;&#20808;&#23450;&#20041;&#20102;&#30693;&#35782;&#32534;&#36753;&#30340;&#30446;&#26631;&#21644;&#25361;&#25112;&#65292;&#28982;&#21518;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#24212;&#29992;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the kno
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22686;&#37327;&#23398;&#20064;&#21644;&#33258;&#33976;&#39311;&#30340;&#32852;&#37030;&#21270;&#20998;&#31867;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20016;&#23500;&#21382;&#21490;&#27169;&#22411;&#30340;&#31867;&#21035;&#20998;&#25968;&#24182;&#21033;&#29992;&#32467;&#21512;&#30340;&#30693;&#35782;&#36827;&#34892;&#33258;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#26356;&#20805;&#20998;&#31934;&#30830;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;</title><link>http://arxiv.org/abs/2401.00622</link><description>&lt;p&gt;
&#20855;&#26377;&#22686;&#37327;&#23398;&#20064;&#21644;&#33258;&#33976;&#39311;&#30340;&#32852;&#37030;&#21270;&#20998;&#31867;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Class-Incremental Learning with New-Class Augmented Self-Distillation. (arXiv:2401.00622v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00622
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#22686;&#37327;&#23398;&#20064;&#21644;&#33258;&#33976;&#39311;&#30340;&#32852;&#37030;&#21270;&#20998;&#31867;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20016;&#23500;&#21382;&#21490;&#27169;&#22411;&#30340;&#31867;&#21035;&#20998;&#25968;&#24182;&#21033;&#29992;&#32467;&#21512;&#30340;&#30693;&#35782;&#36827;&#34892;&#33258;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#26356;&#20805;&#20998;&#31934;&#30830;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#21442;&#19982;&#32773;&#33021;&#22815;&#22312;&#20445;&#25252;&#21407;&#22987;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#36827;&#34892;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#12290;&#20027;&#27969;&#30340;FL&#26041;&#27861;&#24573;&#35270;&#20102;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#23588;&#20854;&#26159;&#25968;&#25454;&#22312;&#26102;&#38388;&#19978;&#30340;&#22686;&#38271;&#21644;&#31867;&#21035;&#19978;&#30340;&#22810;&#26679;&#21270;&#12290;&#36825;&#19968;&#24573;&#35270;&#23548;&#33268;FL&#26041;&#27861;&#22312;&#21560;&#25910;&#26032;&#25968;&#25454;&#26102;&#19981;&#21487;&#36991;&#20813;&#22320;&#36951;&#24536;&#20043;&#21069;&#23398;&#20064;&#21040;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#21270;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#32852;&#37030;&#21270;&#22686;&#37327;&#23398;&#20064;&#19982;&#22686;&#24378;&#33258;&#33976;&#39311;&#65288;FedCLASS&#65289;&#12290;FedCLASS&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#24403;&#21069;&#27169;&#22411;&#39044;&#27979;&#30340;&#26032;&#31867;&#21035;&#20998;&#25968;&#20016;&#23500;&#21382;&#21490;&#27169;&#22411;&#30340;&#31867;&#21035;&#20998;&#25968;&#65292;&#24182;&#21033;&#29992;&#32467;&#21512;&#30340;&#30693;&#35782;&#36827;&#34892;&#33258;&#33976;&#39311;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#20805;&#20998;&#31934;&#30830;&#30340;&#30693;&#35782;&#20256;&#36882;&#65292;&#20174;&#21382;&#21490;&#27169;&#22411;&#21040;&#24403;&#21069;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) enables collaborative model training among participants while guaranteeing the privacy of raw data. Mainstream FL methodologies overlook the dynamic nature of real-world data, particularly its tendency to grow in volume and diversify in classes over time. This oversight results in FL methods suffering from catastrophic forgetting, where the trained models inadvertently discard previously learned information upon assimilating new data. In response to this challenge, we propose a novel Federated Class-Incremental Learning (FCIL) method, named \underline{Fed}erated \underline{C}lass-Incremental \underline{L}earning with New-Class \underline{A}ugmented \underline{S}elf-Di\underline{S}tillation (FedCLASS). The core of FedCLASS is to enrich the class scores of historical models with new class scores predicted by current models and utilize the combined knowledge for self-distillation, enabling a more sufficient and precise knowledge transfer from historical models to c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#27169;&#25311;&#25512;&#26029;&#26041;&#27861;&#32467;&#21512;&#33033;&#20914;&#26143;&#31181;&#32676;&#21512;&#25104;&#65292;&#26469;&#38480;&#21046;&#23396;&#31435;&#38134;&#27827;&#23556;&#30005;&#33033;&#20914;&#26143;&#30340;&#30913;&#26059;&#36716;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.14848</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#27169;&#25311;&#25512;&#26029;&#30340;&#23396;&#31435;&#33033;&#20914;&#26143;&#31181;&#32676;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Isolated pulsar population synthesis with simulation-based inference. (arXiv:2312.14848v1 [astro-ph.HE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#27169;&#25311;&#25512;&#26029;&#26041;&#27861;&#32467;&#21512;&#33033;&#20914;&#26143;&#31181;&#32676;&#21512;&#25104;&#65292;&#26469;&#38480;&#21046;&#23396;&#31435;&#38134;&#27827;&#23556;&#30005;&#33033;&#20914;&#26143;&#30340;&#30913;&#26059;&#36716;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#33033;&#20914;&#26143;&#31181;&#32676;&#21512;&#25104;&#19982;&#22522;&#20110;&#27169;&#25311;&#25512;&#26029;&#30456;&#32467;&#21512;&#65292;&#20197;&#38480;&#21046;&#23396;&#31435;&#38134;&#27827;&#23556;&#30005;&#33033;&#20914;&#26143;&#30340;&#30913;&#26059;&#36716;&#29305;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#26469;&#27169;&#25311;&#20013;&#23376;&#26143;&#30340;&#35806;&#29983;&#29305;&#24615;&#21644;&#28436;&#21270;&#65292;&#37325;&#28857;&#26159;&#23427;&#20204;&#30340;&#21160;&#21147;&#23398;&#12289;&#26059;&#36716;&#21644;&#30913;&#24615;&#29305;&#24449;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20174;&#23545;&#25968;&#27491;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#21021;&#22987;&#30913;&#22330;&#24378;&#24230;B&#21644;&#33258;&#36716;&#21608;&#26399;P&#65292;&#24182;&#29992;&#24130;&#24459;&#26469;&#25429;&#25417;&#21518;&#26399;&#30913;&#22330;&#30340;&#34928;&#20943;&#12290;&#27599;&#20010;&#23545;&#25968;&#27491;&#24577;&#20998;&#24067;&#30001;&#22343;&#20540;&#956;logB&#65292;&#956;logP&#21644;&#26631;&#20934;&#24046;&#963;logB&#65292;&#963;logP&#25551;&#36848;&#65292;&#32780;&#24130;&#24459;&#30001;&#25351;&#25968;a_late&#25551;&#36848;&#65292;&#20849;&#35745;&#20116;&#20010;&#33258;&#30001;&#21442;&#25968;&#12290;&#28982;&#21518;&#25105;&#20204;&#27169;&#25311;&#20102;&#26143;&#20307;&#30340;&#23556;&#30005;&#21457;&#23556;&#21644;&#35266;&#27979;&#20559;&#24046;&#65292;&#20197;&#27169;&#25311;&#19977;&#20010;&#23556;&#30005;&#35843;&#26597;&#20013;&#30340;&#25506;&#27979;&#65292;&#24182;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#21442;&#25968;&#20135;&#29983;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#21512;&#25104;P-&#7766;&#22270;&#25968;&#25454;&#24211;&#12290;&#25509;&#30528;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#27169;&#25311;&#25512;&#26029;&#30340;&#26041;&#27861;&#36827;&#34892;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
We combine pulsar population synthesis with simulation-based inference to constrain the magneto-rotational properties of isolated Galactic radio pulsars. We first develop a flexible framework to model neutron-star birth properties and evolution, focusing on their dynamical, rotational and magnetic characteristics. In particular, we sample initial magnetic-field strengths, $B$, and spin periods, $P$, from log-normal distributions and capture the late-time magnetic-field decay with a power law. Each log-normal is described by a mean, $\mu_{\log B}, \mu_{\log P}$, and standard deviation, $\sigma_{\log B}, \sigma_{\log P}$, while the power law is characterized by the index, $a_{\rm late}$, resulting in five free parameters. We subsequently model the stars' radio emission and observational biases to mimic detections with three radio surveys, and produce a large database of synthetic $P$-$\dot{P}$ diagrams by varying our input parameters. We then follow a simulation-based inference approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#26102;&#38388;&#21464;&#25442;&#22120;&#21516;&#26102;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26356;&#22909;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.11714</link><description>&lt;p&gt;
&#26102;&#38388;&#21464;&#25442;&#22120;&#65306;&#34701;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Time-Transformer: Integrating Local and Global Features for Better Time Series Generation. (arXiv:2312.11714v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#26102;&#38388;&#21464;&#25442;&#22120;&#21516;&#26102;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26356;&#22909;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26159;&#35299;&#20915;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#26102;&#38388;&#29305;&#24615;&#65292;&#21253;&#25324;&#26412;&#22320;&#30456;&#20851;&#24615;&#21644;&#20840;&#23616;&#20381;&#36182;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#26410;&#33021;&#26377;&#25928;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;'&#26102;&#38388;&#21464;&#25442;&#22120;AAE'&#65292;&#23427;&#30001;&#19968;&#20010;&#23545;&#25239;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;AAE&#65289;&#21644;&#19968;&#20010;&#21517;&#20026;'&#26102;&#38388;&#21464;&#25442;&#22120;'&#30340;&#26032;&#35774;&#35745;&#26550;&#26500;&#32452;&#25104;&#12290;&#26102;&#38388;&#21464;&#25442;&#22120;&#39318;&#20808;&#36890;&#36807;&#23618;&#27425;&#24182;&#34892;&#35774;&#35745;&#21516;&#26102;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#32467;&#21512;&#20102;&#26102;&#38388;&#21367;&#31215;&#32593;&#32476;&#21644;Transformer&#30340;&#33021;&#21147;&#65292;&#20998;&#21035;&#25552;&#21462;&#26412;&#22320;&#29305;&#24449;&#21644;&#20840;&#23616;&#20381;&#36182;&#24615;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#21521;&#20132;&#21449;&#27880;&#24847;&#21147;&#26469;&#22312;&#20004;&#20010;&#20998;&#25903;&#20043;&#38388;&#25552;&#20379;&#20114;&#34917;&#30340;&#24341;&#23548;&#65292;&#24182;&#23454;&#29616;&#26412;&#22320;&#29305;&#24449;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#21512;&#36866;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating time series data is a promising approach to address data deficiency problems. However, it is also challenging due to the complex temporal properties of time series data, including local correlations as well as global dependencies. Most existing generative models have failed to effectively learn both the local and global properties of time series data. To address this open problem, we propose a novel time series generative model named 'Time-Transformer AAE', which consists of an adversarial autoencoder (AAE) and a newly designed architecture named 'Time-Transformer' within the decoder. The Time-Transformer first simultaneously learns local and global features in a layer-wise parallel design, combining the abilities of Temporal Convolutional Networks and Transformer in extracting local features and global dependencies respectively. Second, a bidirectional cross attention is proposed to provide complementary guidance across the two branches and achieve proper fusion between loc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;SOTIF&#26102;&#38388;&#35823;&#24046;&#19982;&#25925;&#38556;&#27169;&#22411;(STEAM)&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#26631;&#20934;&#20013;&#22312;&#35782;&#21035;&#21644;&#35780;&#20272;AI&#24341;&#36215;&#30340;&#21361;&#38505;&#38169;&#35823;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;AI&#39537;&#21160;&#30340;&#39550;&#39542;&#33258;&#21160;&#21270;&#31995;&#32479;&#20013;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2312.09559</link><description>&lt;p&gt;
STEAM &amp; MoSAFE: AI-Enabled Driving Automation&#30340;SOTIF&#38169;&#35823;&#19982;&#25925;&#38556;&#27169;&#22411;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
STEAM &amp; MoSAFE: SOTIF Error-and-Failure Model &amp; Analysis for AI-Enabled Driving Automation. (arXiv:2312.09559v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23450;&#20041;SOTIF&#26102;&#38388;&#35823;&#24046;&#19982;&#25925;&#38556;&#27169;&#22411;(STEAM)&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#26631;&#20934;&#20013;&#22312;&#35782;&#21035;&#21644;&#35780;&#20272;AI&#24341;&#36215;&#30340;&#21361;&#38505;&#38169;&#35823;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;AI&#39537;&#21160;&#30340;&#39550;&#39542;&#33258;&#21160;&#21270;&#31995;&#32479;&#20013;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39550;&#39542;&#33258;&#21160;&#21270;&#31995;&#32479;(DAS)&#21463;&#22797;&#26434;&#30340;&#36947;&#36335;&#29615;&#22659;&#21644;&#36710;&#36742;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#36234;&#26469;&#36234;&#20381;&#36182;&#20110;&#20808;&#36827;&#30340;&#20256;&#24863;&#22120;&#21644;&#20154;&#24037;&#26234;&#33021;(AI)&#12290;&#36825;&#20123;&#29305;&#24615;&#23548;&#33268;&#20102;&#28304;&#33258;&#35268;&#33539;&#19981;&#36275;&#21644;&#25216;&#26415;&#24615;&#33021;&#38480;&#21046;&#30340;&#29305;&#27530;&#23433;&#20840;&#25925;&#38556;&#65292;&#20854;&#20013;&#20256;&#24863;&#22120;&#21644;AI&#24341;&#20837;&#30340;&#38169;&#35823;&#22312;&#22823;&#23567;&#21644;&#26102;&#38388;&#27169;&#24335;&#19978;&#21464;&#21270;&#65292;&#21487;&#33021;&#36896;&#25104;&#23433;&#20840;&#39118;&#38505;&#12290;&#23433;&#20840;&#30340;&#39044;&#26399;&#21151;&#33021;&#24615;(SOTIF)&#26631;&#20934;&#26159;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26694;&#26550;&#65292;&#19987;&#27880;&#20110;&#22522;&#20110;&#22330;&#26223;&#30340;&#20998;&#26512;&#20197;&#35782;&#21035;&#21361;&#38505;&#34892;&#20026;&#21450;&#20854;&#21407;&#22240;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#26631;&#20934;&#25552;&#20379;&#20102;&#22522;&#26412;&#30340;&#22240;&#26524;&#27169;&#22411;&#21644;&#39640;&#32423;&#27969;&#31243;&#25351;&#23548;&#65292;&#20294;&#22312;AI&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#23427;&#32570;&#20047;&#29992;&#20110;&#35782;&#21035;&#21644;&#35780;&#20272;&#21361;&#38505;&#38169;&#35823;&#30340;&#27010;&#24565;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#36129;&#29486;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#39318;&#20808;&#65292;&#23427;&#23558;SOTIF&#26102;&#38388;&#35823;&#24046;&#19982;&#25925;&#38556;&#27169;&#22411;(STEAM)&#23450;&#20041;&#20026;&#19968;&#31181;&#32454;&#21270;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#29992;&#20110;&#25551;&#36848;AI&#24341;&#36215;&#30340;&#21508;&#31181;&#38169;&#35823;&#30340;&#35201;&#32032;&#12290;&#36890;&#36807;&#36825;&#19968;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;AI&#39537;&#21160;&#30340;&#39550;&#39542;&#33258;&#21160;&#21270;&#31995;&#32479;&#20013;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driving Automation Systems (DAS) are subject to complex road environments and vehicle behaviors and increasingly rely on sophisticated sensors and Artificial Intelligence (AI). These properties give rise to unique safety faults stemming from specification insufficiencies and technological performance limitations, where sensors and AI introduce errors that vary in magnitude and temporal patterns, posing potential safety risks. The Safety of the Intended Functionality (SOTIF) standard emerges as a promising framework for addressing these concerns, focusing on scenario-based analysis to identify hazardous behaviors and their causes. Although the current standard provides a basic cause-and-effect model and high-level process guidance, it lacks concepts required to identify and evaluate hazardous errors, especially within the context of AI.  This paper introduces two key contributions to bridge this gap. First, it defines the SOTIF Temporal Error and Failure Model (STEAM) as a refinement of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#26080;&#30028;&#26041;&#24046;&#30340;&#26377;&#20559;&#22122;&#22768;&#23545;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#20171;&#32461;&#20102;&#35813;&#31639;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.02828</link><description>&lt;p&gt;
&#38543;&#26426;&#36924;&#36817;&#30340;&#25910;&#25947;&#36895;&#24230;&#65306;&#24102;&#26377;&#26080;&#30028;&#26041;&#24046;&#30340;&#26377;&#20559;&#22122;&#22768;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Convergence Rates for Stochastic Approximation: Biased Noise with Unbounded Variance, and Applications. (arXiv:2312.02828v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#26080;&#30028;&#26041;&#24046;&#30340;&#26377;&#20559;&#22122;&#22768;&#23545;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#20171;&#32461;&#20102;&#35813;&#31639;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
1951&#24180;&#32599;&#23486;&#26031;&#21644;&#33707;&#27931;&#24341;&#20837;&#30340;&#38543;&#26426;&#36924;&#36817;&#65288;SA&#65289;&#31639;&#27861;&#24050;&#32463;&#25104;&#20026;&#35299;&#26041;&#31243;$\mathbf{f}({\boldsymbol{\theta}}) = \mathbf{0}$&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#24403;&#21482;&#26377;$\mathbf{f}(\cdot)$&#30340;&#24102;&#22122;&#22768;&#27979;&#37327;&#21487;&#29992;&#26102;&#12290;&#22914;&#26524;&#23545;&#20110;&#26576;&#20010;&#20989;&#25968;$J(\cdot)$&#65292;$\mathbf{f}({\boldsymbol{\theta}}) = \nabla J({\boldsymbol{\theta}})$&#65292;&#37027;&#20040;SA&#20063;&#21487;&#20197;&#29992;&#26469;&#23547;&#25214;$J(\cdot)$&#30340;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;$t$&#65292;&#24403;&#21069;&#30340;&#29468;&#27979;${\boldsymbol{\theta}}_t$&#36890;&#36807;&#24418;&#24335;&#20026;$\mathbf{f}({\boldsymbol{\theta}}_t) + {\boldsymbol{\xi}}_{t+1}$&#30340;&#24102;&#22122;&#22768;&#27979;&#37327;&#26356;&#26032;&#20026;${\boldsymbol{\theta}}_{t+1}$&#12290;&#22312;&#35768;&#22810;&#25991;&#29486;&#20013;&#65292;&#20551;&#35774;&#35823;&#24046;&#39033;${\boldsymbol{\xi}}_{t+1}$&#30340;&#26465;&#20214;&#22343;&#20540;&#20026;&#38646;&#65292;&#21644;/&#25110;&#32773;&#23427;&#30340;&#26465;&#20214;&#26041;&#24046;&#38543;$t$&#65288;&#32780;&#19981;&#26159;${\boldsymbol{\theta}}_t$&#65289;&#34987;&#38480;&#21046;&#12290;&#22810;&#24180;&#26469;&#65292;SA&#24050;&#32463;&#24212;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#65292;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20854;&#20013;&#19968;&#20010;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Stochastic Approximation (SA) algorithm introduced by Robbins and Monro in 1951 has been a standard method for solving equations of the form $\mathbf{f}({\boldsymbol {\theta}}) = \mathbf{0}$, when only noisy measurements of $\mathbf{f}(\cdot)$ are available. If $\mathbf{f}({\boldsymbol {\theta}}) = \nabla J({\boldsymbol {\theta}})$ for some function $J(\cdot)$, then SA can also be used to find a stationary point of $J(\cdot)$. At each time $t$, the current guess ${\boldsymbol {\theta}}_t$ is updated to ${\boldsymbol {\theta}}_{t+1}$ using a noisy measurement of the form $\mathbf{f}({\boldsymbol {\theta}}_t) + {\boldsymbol {\xi}}_{t+1}$. In much of the literature, it is assumed that the error term ${\boldsymbol {\xi}}_{t+1}$ has zero conditional mean, and/or that its conditional variance is bounded as a function of $t$ (though not necessarily with respect to ${\boldsymbol {\theta}}_t$). Over the years, SA has been applied to a variety of areas, out of which the focus in this paper i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#65288;ALMs&#65289;&#22312;Verilog&#32534;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#37319;&#29992;&#21019;&#26032;&#30340;&#26694;&#26550;&#21644;&#21452;&#38454;&#27573;&#32454;&#21270;&#21327;&#35758;&#65292;&#33021;&#22815;&#25552;&#39640;&#20195;&#30721;&#30340;&#31934;&#30830;&#24615;&#21644;&#19982;&#21151;&#32791;-&#24615;&#33021;-&#38754;&#31215;&#65288;PPA&#65289;&#22522;&#20934;&#30340;&#23545;&#40784;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.01022</link><description>&lt;p&gt;
&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#30340;Verilog&#24320;&#21457;&#65306;&#22312;&#20195;&#30721;&#21512;&#25104;&#20013;&#22686;&#24378;&#21151;&#32791;&#12289;&#24615;&#33021;&#21644;&#38754;&#31215;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Advanced Large Language Model (LLM)-Driven Verilog Development: Enhancing Power, Performance, and Area Optimization in Code Synthesis. (arXiv:2312.01022v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#65288;ALMs&#65289;&#22312;Verilog&#32534;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#37319;&#29992;&#21019;&#26032;&#30340;&#26694;&#26550;&#21644;&#21452;&#38454;&#27573;&#32454;&#21270;&#21327;&#35758;&#65292;&#33021;&#22815;&#25552;&#39640;&#20195;&#30721;&#30340;&#31934;&#30830;&#24615;&#21644;&#19982;&#21151;&#32791;-&#24615;&#33021;-&#38754;&#31215;&#65288;PPA&#65289;&#22522;&#20934;&#30340;&#23545;&#40784;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#65288;ALMs&#65289;&#22312;&#30005;&#23376;&#30828;&#20214;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;Verilog&#32534;&#31243;&#30340;&#21512;&#25104;&#21644;&#22686;&#24378;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#25552;&#39640;ALMs&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#29983;&#20135;&#21147;&#12290;&#35813;&#26041;&#27861;&#20174;&#36890;&#36807;ALMs&#36827;&#34892;Verilog&#32534;&#31243;&#30340;&#21021;&#22987;&#26500;&#24314;&#24320;&#22987;&#65292;&#28982;&#21518;&#37319;&#29992;&#29420;&#29305;&#30340;&#21452;&#38454;&#27573;&#32454;&#21270;&#21327;&#35758;&#12290;&#39318;&#20010;&#38454;&#27573;&#20248;&#20808;&#22686;&#24378;&#20195;&#30721;&#30340;&#25805;&#20316;&#21644;&#35821;&#35328;&#31934;&#30830;&#24615;&#65292;&#32780;&#21518;&#19968;&#20010;&#38454;&#27573;&#21017;&#33268;&#21147;&#20110;&#23558;&#20195;&#30721;&#19982;&#21151;&#32791;-&#24615;&#33021;-&#38754;&#31215;&#65288;PPA&#65289;&#22522;&#20934;&#23545;&#40784;&#65292;&#36825;&#26159;&#26377;&#25928;&#30828;&#20214;&#35774;&#35745;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#31181;&#23558;&#38169;&#35823;&#20462;&#22797;&#19982;PPA&#22686;&#24378;&#30456;&#32467;&#21512;&#30340;&#20998;&#27969;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;ALM&#21019;&#24314;&#30340;&#20195;&#30721;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing use of Advanced Language Models (ALMs) in diverse sectors, particularly due to their impressive capability to generate top-tier content following linguistic instructions, forms the core of this investigation. This study probes into ALMs' deployment in electronic hardware design, with a specific emphasis on the synthesis and enhancement of Verilog programming. We introduce an innovative framework, crafted to assess and amplify ALMs' productivity in this niche. The methodology commences with the initial crafting of Verilog programming via ALMs, succeeded by a distinct dual-stage refinement protocol. The premier stage prioritizes augmenting the code's operational and linguistic precision, while the latter stage is dedicated to aligning the code with Power-Performance-Area (PPA) benchmarks, a pivotal component in proficient hardware design. This bifurcated strategy, merging error remediation with PPA enhancement, has yielded substantial upgrades in the caliber of ALM-created
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#21307;&#23398;&#22270;&#20687;&#20132;&#20114;&#24335;&#20998;&#21106;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#25512;&#21160;&#20102;&#39046;&#22495;&#21457;&#23637;&#65292;&#20294;&#30446;&#21069;&#23384;&#22312;&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#32570;&#20047;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.13964</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#30340;&#28145;&#24230;&#20132;&#20114;&#24335;&#20998;&#21106;&#65306;&#31995;&#32479;&#32508;&#36848;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Interactive Segmentation of Medical Images: A Systematic Review and Taxonomy. (arXiv:2311.13964v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#21307;&#23398;&#22270;&#20687;&#20132;&#20114;&#24335;&#20998;&#21106;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#25512;&#21160;&#20102;&#39046;&#22495;&#21457;&#23637;&#65292;&#20294;&#30446;&#21069;&#23384;&#22312;&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#32570;&#20047;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#20998;&#21106;&#26159;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#36890;&#36807;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#26469;&#25552;&#39640;&#26114;&#36149;&#27880;&#37322;&#30340;&#25928;&#29575;&#12290;&#36825;&#20123;&#21453;&#39304;&#20197;&#28857;&#20987;&#12289;&#28034;&#40486;&#25110;&#25513;&#33180;&#30340;&#24418;&#24335;&#20986;&#29616;&#65292;&#20801;&#35768;&#23545;&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#36845;&#20195;&#20248;&#21270;&#65292;&#20197;&#20415;&#26377;&#25928;&#24341;&#23548;&#31995;&#32479;&#23454;&#29616;&#25152;&#38656;&#30340;&#34892;&#20026;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#25512;&#21160;&#20102;&#32467;&#26524;&#36798;&#21040;&#19968;&#20010;&#26032;&#27700;&#24179;&#65292;&#23548;&#33268;&#35813;&#39046;&#22495;&#24555;&#36895;&#22686;&#38271;&#65292;&#20165;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#23601;&#25552;&#20986;&#20102;121&#31181;&#26041;&#27861;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#32467;&#26500;&#21270;&#27010;&#36848;&#65292;&#21253;&#25324;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#12289;&#29616;&#26377;&#26041;&#27861;&#30340;&#31995;&#32479;&#32508;&#36848;&#20197;&#21450;&#23545;&#24403;&#21069;&#23454;&#36341;&#30340;&#28145;&#20837;&#20998;&#26512;&#12290;&#22522;&#20110;&#36825;&#20123;&#36129;&#29486;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#20005;&#37325;&#32570;&#20047;&#65292;&#38656;&#35201;&#36890;&#36807;&#26631;&#20934;&#21270;&#22522;&#20934;&#21644;&#22522;&#20934;&#25968;&#25454;&#36827;&#34892;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactive segmentation is a crucial research area in medical image analysis aiming to boost the efficiency of costly annotations by incorporating human feedback. This feedback takes the form of clicks, scribbles, or masks and allows for iterative refinement of the model output so as to efficiently guide the system towards the desired behavior. In recent years, deep learning-based approaches have propelled results to a new level causing a rapid growth in the field with 121 methods proposed in the medical imaging domain alone. In this review, we provide a structured overview of this emerging field featuring a comprehensive taxonomy, a systematic review of existing methods, and an in-depth analysis of current practices. Based on these contributions, we discuss the challenges and opportunities in the field. For instance, we find that there is a severe lack of comparison across methods which needs to be tackled by standardized baselines and benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#33258;&#25105;&#32416;&#27491;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#21457;&#29616;&#21644;&#36755;&#20986;&#32416;&#27491;&#20004;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#30740;&#31350;&#21457;&#29616;LLMs&#36890;&#24120;&#38590;&#20197;&#21457;&#29616;&#36923;&#36753;&#38169;&#35823;&#65292;&#20294;&#36890;&#36807;&#20351;&#29992;&#22238;&#28335;&#26041;&#27861;&#21487;&#20197;&#22312;&#25552;&#20379;&#38169;&#35823;&#20301;&#32622;&#20449;&#24687;&#26102;&#33719;&#24471;&#22823;&#24133;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2311.08516</link><description>&lt;p&gt;
LLMs&#26080;&#27861;&#25214;&#21040;&#25512;&#29702;&#38169;&#35823;&#65292;&#20294;&#21487;&#20197;&#32416;&#27491;&#23427;&#20204;&#65281;&#65288;arXiv&#65306;2311.08516v2 [cs.AI] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
LLMs cannot find reasoning errors, but can correct them!. (arXiv:2311.08516v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#33258;&#25105;&#32416;&#27491;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#21457;&#29616;&#21644;&#36755;&#20986;&#32416;&#27491;&#20004;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#30740;&#31350;&#21457;&#29616;LLMs&#36890;&#24120;&#38590;&#20197;&#21457;&#29616;&#36923;&#36753;&#38169;&#35823;&#65292;&#20294;&#36890;&#36807;&#20351;&#29992;&#22238;&#28335;&#26041;&#27861;&#21487;&#20197;&#22312;&#25552;&#20379;&#38169;&#35823;&#20301;&#32622;&#20449;&#24687;&#26102;&#33719;&#24471;&#22823;&#24133;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#25105;&#32416;&#27491;&#22312;&#25913;&#21892;LLM&#36755;&#20986;&#30340;&#39118;&#26684;&#21644;&#36136;&#37327;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65288;&#20363;&#22914;Chen&#31561;&#65292;2023&#65307;Madaan&#31561;&#65292;2023&#65289;&#65292;&#26368;&#36817;&#23545;&#36923;&#36753;&#25110;&#25512;&#29702;&#38169;&#35823;&#36827;&#34892;&#33258;&#25105;&#32416;&#27491;&#30340;&#23581;&#35797;&#36890;&#24120;&#20250;&#23548;&#33268;&#27491;&#30830;&#31572;&#26696;&#21464;&#20026;&#38169;&#35823;&#65292;&#20174;&#32780;&#24635;&#20307;&#34920;&#29616;&#21464;&#24046;&#65288;Huang&#31561;&#65292;2023&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#33258;&#25105;&#32416;&#27491;&#36807;&#31243;&#20998;&#35299;&#20026;&#20004;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65306;&#38169;&#35823;&#21457;&#29616;&#21644;&#36755;&#20986;&#32416;&#27491;&#12290;&#23545;&#20110;&#38169;&#35823;&#21457;&#29616;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;BIG-Bench Mistake&#65292;&#36825;&#26159;&#19968;&#20010;Chain-of-Thought&#25512;&#29702;&#36712;&#36857;&#20013;&#30340;&#36923;&#36753;&#38169;&#35823;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20026;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;LLM&#25552;&#20379;&#22522;&#20934;&#25968;&#65292;&#24182;&#35777;&#26126;LLM&#36890;&#24120;&#38590;&#20197;&#21457;&#29616;&#36923;&#36753;&#38169;&#35823;&#12290;&#23545;&#20110;&#36755;&#20986;&#32416;&#27491;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22238;&#28335;&#26041;&#27861;&#65292;&#22312;&#25552;&#20379;&#38169;&#35823;&#20301;&#32622;&#20449;&#24687;&#26102;&#21487;&#20197;&#22823;&#24133;&#25913;&#36827;&#12290;&#25105;&#20204;&#23558;&#22238;&#28335;&#35299;&#37322;&#20026;&#23545;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#36731;&#37327;&#32423;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;60-70&#65285;&#20934;&#30830;&#29575;&#19979;&#20445;&#25345;&#26377;&#25928;&#24615;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
While self-correction has shown promise in improving LLM outputs in terms of style and quality (e.g. Chen et al., 2023; Madaan et al., 2023), recent attempts to self-correct logical or reasoning errors often cause correct answers to become incorrect, resulting in worse performances overall (Huang et al., 2023). In this paper, we break down the self-correction process into two core components: mistake finding and output correction. For mistake finding, we release BIG-Bench Mistake, a dataset of logical mistakes in Chain-of-Thought reasoning traces. We provide benchmark numbers for several state-of-the-art LLMs, and demonstrate that LLMs generally struggle with finding logical mistakes. For output correction, we propose a backtracking method which provides large improvements when given information on mistake location. We construe backtracking as a lightweight alternative to reinforcement learning methods, and show that it remains effective with a reward model at 60-70% accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20013;&#38388;&#29305;&#24449;&#30340;&#32467;&#26500;&#65292;&#25581;&#31034;&#20102;&#28145;&#24230;&#32593;&#32476;&#22312;&#23618;&#32423;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#28436;&#21270;&#27169;&#24335;&#12290;&#30740;&#31350;&#21457;&#29616;&#32447;&#24615;&#23618;&#22312;&#29305;&#24449;&#23398;&#20064;&#20013;&#36215;&#21040;&#20102;&#19982;&#28145;&#23618;&#38750;&#32447;&#24615;&#32593;&#32476;&#31867;&#20284;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.02960</link><description>&lt;p&gt;
&#36890;&#36807;&#23618;&#38388;&#29305;&#24449;&#21387;&#32553;&#21644;&#24046;&#21035;&#24615;&#23398;&#20064;&#29702;&#35299;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination. (arXiv:2311.02960v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20013;&#38388;&#29305;&#24449;&#30340;&#32467;&#26500;&#65292;&#25581;&#31034;&#20102;&#28145;&#24230;&#32593;&#32476;&#22312;&#23618;&#32423;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#28436;&#21270;&#27169;&#24335;&#12290;&#30740;&#31350;&#21457;&#29616;&#32447;&#24615;&#23618;&#22312;&#29305;&#24449;&#23398;&#20064;&#20013;&#36215;&#21040;&#20102;&#19982;&#28145;&#23618;&#38750;&#32447;&#24615;&#32593;&#32476;&#31867;&#20284;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#35777;&#26126;&#26159;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#24847;&#20041;&#29305;&#24449;&#30340;&#19968;&#31181;&#39640;&#25928;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#32593;&#32476;&#22914;&#20309;&#22312;&#19981;&#21516;&#23618;&#32423;&#19978;&#36827;&#34892;&#31561;&#32423;&#29305;&#24449;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#30740;&#31350;&#20013;&#38388;&#29305;&#24449;&#30340;&#32467;&#26500;&#25581;&#31034;&#36825;&#20010;&#35868;&#22242;&#12290;&#21463;&#21040;&#25105;&#20204;&#23454;&#35777;&#21457;&#29616;&#30340;&#32447;&#24615;&#23618;&#22312;&#29305;&#24449;&#23398;&#20064;&#20013;&#27169;&#20223;&#38750;&#32447;&#24615;&#32593;&#32476;&#20013;&#28145;&#23618;&#30340;&#35282;&#33394;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#22914;&#20309;&#23558;&#36755;&#20837;&#25968;&#25454;&#36716;&#21270;&#20026;&#36755;&#20986;&#65292;&#36890;&#36807;&#30740;&#31350;&#35757;&#32451;&#21518;&#30340;&#27599;&#20010;&#23618;&#30340;&#36755;&#20986;&#65288;&#21363;&#29305;&#24449;&#65289;&#22312;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#34913;&#37327;&#20013;&#38388;&#29305;&#24449;&#30340;&#31867;&#20869;&#21387;&#32553;&#21644;&#31867;&#38388;&#24046;&#21035;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#36890;&#36807;&#23545;&#36825;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29305;&#24449;&#20174;&#27973;&#23618;&#21040;&#28145;&#23618;&#30340;&#28436;&#21464;&#36981;&#24490;&#30528;&#19968;&#31181;&#31616;&#21333;&#32780;&#37327;&#21270;&#30340;&#27169;&#24335;&#65292;&#21069;&#25552;&#26159;&#36755;&#20837;&#25968;&#25454;&#26159;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, deep learning has proven to be a highly effective tool for learning meaningful features from raw data. However, it remains an open question how deep networks perform hierarchical feature learning across layers. In this work, we attempt to unveil this mystery by investigating the structures of intermediate features. Motivated by our empirical findings that linear layers mimic the roles of deep layers in nonlinear networks for feature learning, we explore how deep linear networks transform input data into output by investigating the output (i.e., features) of each layer after training in the context of multi-class classification problems. Toward this goal, we first define metrics to measure within-class compression and between-class discrimination of intermediate features, respectively. Through theoretical analysis of these two metrics, we show that the evolution of features follows a simple and quantitative pattern from shallow to deep layers when the input data is
&lt;/p&gt;</description></item><item><title>PolyThrottle&#26159;&#19968;&#31181;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#33410;&#33021;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20248;&#21270;&#35774;&#22791;&#19978;&#30340;&#30828;&#20214;&#20803;&#32032;&#37197;&#32622;&#65292;&#21487;&#20197;&#36798;&#21040;&#39640;&#36798;36%&#30340;&#33021;&#37327;&#33410;&#30465;&#65292;&#24182;&#19988;&#33021;&#22815;&#24555;&#36895;&#25910;&#25947;&#21040;&#36817;&#20046;&#26368;&#20248;&#30340;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2310.19991</link><description>&lt;p&gt;
PolyThrottle: &#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#33410;&#33021;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
PolyThrottle: Energy-efficient Neural Network Inference on Edge Devices. (arXiv:2310.19991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19991
&lt;/p&gt;
&lt;p&gt;
PolyThrottle&#26159;&#19968;&#31181;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#33410;&#33021;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20248;&#21270;&#35774;&#22791;&#19978;&#30340;&#30828;&#20214;&#20803;&#32032;&#37197;&#32622;&#65292;&#21487;&#20197;&#36798;&#21040;&#39640;&#36798;36%&#30340;&#33021;&#37327;&#33410;&#30465;&#65292;&#24182;&#19988;&#33021;&#22815;&#24555;&#36895;&#25910;&#25947;&#21040;&#36817;&#20046;&#26368;&#20248;&#30340;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#37096;&#32626;&#65292;&#20854;&#33021;&#37327;&#38656;&#27714;&#20063;&#30456;&#24212;&#22686;&#38271;&#12290;&#23613;&#31649;&#19968;&#20123;&#20808;&#21069;&#30340;&#24037;&#20316;&#19987;&#27880;&#20110;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#65292;&#20294;&#26159;ML&#39537;&#21160;&#31995;&#32479;&#30340;&#36830;&#32493;&#36816;&#34892;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#35774;&#22791;&#19978;&#30340;&#30828;&#20214;&#20803;&#32032;&#37197;&#32622;&#65288;&#22914;GPU&#12289;&#20869;&#23384;&#21644;CPU&#39057;&#29575;&#65289;&#22312;&#24120;&#35268;&#24494;&#35843;&#30340;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#20013;&#22914;&#20309;&#24433;&#21709;&#33021;&#37327;&#28040;&#32791;&#65292;&#36825;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PolyThrottle&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21463;&#32422;&#26463;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#20197;&#33410;&#33021;&#30340;&#26041;&#24335;&#23545;&#21508;&#20010;&#30828;&#20214;&#32452;&#20214;&#30340;&#37197;&#32622;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#25581;&#31034;&#20102;&#33021;&#37327;&#24615;&#33021;&#24179;&#34913;&#30340;&#26032;&#39062;&#26041;&#38754;&#65292;&#34920;&#26126;&#25105;&#20204;&#21487;&#20197;&#20026;&#27969;&#34892;&#27169;&#22411;&#33410;&#30465;&#39640;&#36798;36%&#30340;&#33021;&#37327;&#12290;&#25105;&#20204;&#36824;&#39564;&#35777;&#20102;PolyThrottle&#21487;&#20197;&#22312;&#28385;&#36275;&#24212;&#29992;&#32422;&#26463;&#26465;&#20214;&#30340;&#21516;&#26102;&#24555;&#36895;&#25910;&#25947;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
As neural networks (NN) are deployed across diverse sectors, their energy demand correspondingly grows. While several prior works have focused on reducing energy consumption during training, the continuous operation of ML-powered systems leads to significant energy use during inference. This paper investigates how the configuration of on-device hardware-elements such as GPU, memory, and CPU frequency, often neglected in prior studies, affects energy consumption for NN inference with regular fine-tuning. We propose PolyThrottle, a solution that optimizes configurations across individual hardware components using Constrained Bayesian Optimization in an energy-conserving manner. Our empirical evaluation uncovers novel facets of the energy-performance equilibrium showing that we can save up to 36 percent of energy for popular models. We also validate that PolyThrottle can quickly converge towards near-optimal settings while satisfying application constraints.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#32467;&#26500;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#25928;&#26524;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26631;&#31614;&#19982;&#23646;&#24615;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12803</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#32467;&#26500;&#30340;&#25991;&#26412;&#31163;&#32676;&#20540;&#27867;&#21270;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Causal-structure Driven Augmentations for Text OOD Generalization. (arXiv:2310.12803v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#32467;&#26500;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#25928;&#26524;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26631;&#31614;&#19982;&#23646;&#24615;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#22120;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#20381;&#36182;&#21487;&#33021;&#23548;&#33268;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#25928;&#26524;&#19981;&#20339;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#20854;&#22312;&#22914;&#21307;&#30103;&#39046;&#22495;&#31561;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#20351;&#29992;&#30340;&#25285;&#24551;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22240;&#26524;&#32467;&#26500;&#30693;&#35782;&#25351;&#23548;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#27169;&#25311;&#23545;&#34394;&#20551;&#29305;&#24449;&#36827;&#34892;&#24178;&#39044;&#65292;&#20197;&#23398;&#20064;&#26356;&#21152;&#40065;&#26834;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26631;&#31614;&#19982;&#23646;&#24615;&#20043;&#38388;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#39044;&#27979;&#38382;&#39064;&#20013;&#65292;&#36825;&#31181;&#31574;&#30053;&#26159;&#21512;&#36866;&#30340;&#12290;&#22312;&#36825;&#31181;&#38382;&#39064;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#30456;&#23545;&#20110;&#37325;&#35201;&#24615;&#37325;&#21152;&#26435;&#30340;&#26377;&#21033;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#36741;&#21161;&#25968;&#25454;&#36890;&#36807;&#24046;&#20998;&#22312;&#24046;&#20998;&#30340;&#26041;&#27861;&#26469;&#21305;&#37197;&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#34920;&#31034;&#25991;&#26412;&#30340;&#26465;&#20214;&#27010;&#29575;&#12290;&#36890;&#36807;&#23545;&#20174;&#21307;&#23398;&#21465;&#36848;&#20013;&#23398;&#20064;&#19982;&#30475;&#25252;&#32773;&#26080;&#20851;&#30340;&#20020;&#24202;&#35786;&#26029;&#39044;&#27979;&#22120;&#20197;&#21450;&#21322;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reliance of text classifiers on spurious correlations can lead to poor generalization at deployment, raising concerns about their use in safety-critical domains such as healthcare. In this work, we propose to use counterfactual data augmentation, guided by knowledge of the causal structure of the data, to simulate interventions on spurious features and to learn more robust text classifiers. We show that this strategy is appropriate in prediction problems where the label is spuriously correlated with an attribute. Under the assumptions of such problems, we discuss the favorable sample complexity of counterfactual data augmentation, compared to importance re-weighting. Pragmatically, we match examples using auxiliary data, based on diff-in-diff methodology, and use a large language model (LLM) to represent a conditional probability of text. Through extensive experimentation on learning caregiver-invariant predictors of clinical diagnoses from medical narratives and on semi-synthetic 
&lt;/p&gt;</description></item><item><title>CORN&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#20840;&#21442;&#32771;&#21644;&#38750;&#21442;&#32771;&#38899;&#39057;&#24230;&#37327;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#23581;&#35797;&#22312;&#35757;&#32451;&#26102;&#21516;&#26102;&#35757;&#32451;&#36825;&#20004;&#31181;&#27169;&#22411;&#12290;CORN FR&#27169;&#24335;&#21516;&#26102;&#20855;&#22791;&#20840;&#21442;&#32771;&#21644;&#38750;&#21442;&#32771;&#24230;&#37327;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09388</link><description>&lt;p&gt;
CORN: &#20840;&#21442;&#32771;&#21644;&#38750;&#21442;&#32771;&#38899;&#39057;&#24230;&#37327;&#30340;&#20849;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CORN: Co-Trained Full-Reference And No-Reference Audio Metrics. (arXiv:2310.09388v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09388
&lt;/p&gt;
&lt;p&gt;
CORN&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#20840;&#21442;&#32771;&#21644;&#38750;&#21442;&#32771;&#38899;&#39057;&#24230;&#37327;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#23581;&#35797;&#22312;&#35757;&#32451;&#26102;&#21516;&#26102;&#35757;&#32451;&#36825;&#20004;&#31181;&#27169;&#22411;&#12290;CORN FR&#27169;&#24335;&#21516;&#26102;&#20855;&#22791;&#20840;&#21442;&#32771;&#21644;&#38750;&#21442;&#32771;&#24230;&#37327;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#35780;&#20272;&#26159;&#21508;&#31181;&#38899;&#39057;&#22788;&#29702;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#26041;&#38754;&#12290;&#20840;&#21442;&#32771;&#65288;FR&#65289;&#25110;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#24230;&#37327;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#21442;&#32771;&#24405;&#38899;&#65292;&#23558;&#20854;&#19982;&#24405;&#38899;&#30340;&#20302;&#36136;&#37327;&#25110;&#25439;&#22351;&#29256;&#26412;&#36827;&#34892;&#27604;&#36739;&#20197;&#36827;&#34892;&#35780;&#20272;&#12290;&#30456;&#21453;&#65292;&#38750;&#21442;&#32771;&#65288;NR&#65289;&#24230;&#37327;&#35780;&#20272;&#24405;&#38899;&#32780;&#19981;&#20381;&#36182;&#21442;&#32771;&#12290;FR&#21644;NR&#20004;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#24444;&#27492;&#37117;&#20855;&#26377;&#20248;&#21183;&#21644;&#32570;&#28857;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#31216;&#20026;CORN&#65292;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#21516;&#26102;&#35757;&#32451;FR&#21644;NR&#27169;&#22411;&#12290;&#35757;&#32451;&#23436;&#25104;&#21518;&#65292;&#21487;&#20197;&#29420;&#31435;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#39044;&#27979;&#20960;&#20010;&#24120;&#35265;&#30340;&#23458;&#35266;&#24230;&#37327;&#25351;&#26631;&#20197;&#21450;&#22312;&#20004;&#31181;&#19981;&#21516;&#26550;&#26500;&#19978;&#36827;&#34892;&#35780;&#20272;CORN&#12290;&#20351;&#29992;CORN&#35757;&#32451;&#30340;NR&#27169;&#22411;&#22312;&#35757;&#32451;&#26399;&#38388;&#21487;&#20197;&#35775;&#38382;&#21442;&#32771;&#24405;&#38899;&#65292;&#22240;&#27492;&#21487;&#20197;&#39044;&#26399;&#65292;&#23427;&#22987;&#32456;&#20248;&#20110;&#29420;&#31435;&#35757;&#32451;&#30340;&#22522;&#32447;NR&#27169;&#22411;&#12290;&#26356;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26159;CORN FR&#27169;&#24335;&#21487;&#20197;&#21516;&#26102;&#25552;&#20379;&#20840;&#21442;&#32771;&#21644;&#38750;&#21442;&#32771;&#24230;&#37327;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perceptual evaluation constitutes a crucial aspect of various audio-processing tasks. Full reference (FR) or similarity-based metrics rely on high-quality reference recordings, to which lower-quality or corrupted versions of the recording may be compared for evaluation. In contrast, no-reference (NR) metrics evaluate a recording without relying on a reference. Both the FR and NR approaches exhibit advantages and drawbacks relative to each other. In this paper, we present a novel framework called CORN that amalgamates these dual approaches, concurrently training both FR and NR models together. After training, the models can be applied independently. We evaluate CORN by predicting several common objective metrics and across two different architectures. The NR model trained using CORN has access to a reference recording during training, and thus, as one would expect, it consistently outperforms baseline NR models trained independently. Perhaps even more remarkable is that the CORN FR mode
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#39118;&#38505;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#30456;&#23545;&#27979;&#35797;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#19968;&#38454;&#21644;&#20108;&#38454;&#38543;&#26426;&#20248;&#21183;&#65292;&#24182;&#20511;&#37492;&#20102;&#35745;&#37327;&#32463;&#27982;&#23398;&#21644;&#25968;&#23398;&#37329;&#34701;&#20013;&#24120;&#29992;&#30340;&#24179;&#22343;&#39118;&#38505;&#27169;&#22411;&#12290;&#22312;&#32473;&#23450;&#25351;&#23450;&#24230;&#37327;&#37327;&#21270;&#30340;&#38450;&#25252;&#26639;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#38505;&#24847;&#35782;&#30340;&#22522;&#30784;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#12290;&#21463;&#25968;&#23398;&#37329;&#34701;&#20013;&#30340;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#21644;&#36873;&#25321;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#27169;&#22411;&#23450;&#20041;&#20102;&#19968;&#20010;"&#24230;&#37327;&#32452;&#21512;"&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#32452;&#21512;&#30340;&#38543;&#26426;&#20248;&#21183;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2310.07132</link><description>&lt;p&gt;
&#22312;&#22522;&#30784;&#27169;&#22411;&#26102;&#20195;&#30340;&#39118;&#38505;&#35780;&#20272;&#21644;&#32479;&#35745;&#26174;&#33879;&#24615;
&lt;/p&gt;
&lt;p&gt;
Risk Assessment and Statistical Significance in the Age of Foundation Models. (arXiv:2310.07132v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#39118;&#38505;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#30456;&#23545;&#27979;&#35797;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#19968;&#38454;&#21644;&#20108;&#38454;&#38543;&#26426;&#20248;&#21183;&#65292;&#24182;&#20511;&#37492;&#20102;&#35745;&#37327;&#32463;&#27982;&#23398;&#21644;&#25968;&#23398;&#37329;&#34701;&#20013;&#24120;&#29992;&#30340;&#24179;&#22343;&#39118;&#38505;&#27169;&#22411;&#12290;&#22312;&#32473;&#23450;&#25351;&#23450;&#24230;&#37327;&#37327;&#21270;&#30340;&#38450;&#25252;&#26639;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#38505;&#24847;&#35782;&#30340;&#22522;&#30784;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#12290;&#21463;&#25968;&#23398;&#37329;&#34701;&#20013;&#30340;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#21644;&#36873;&#25321;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#27169;&#22411;&#23450;&#20041;&#20102;&#19968;&#20010;"&#24230;&#37327;&#32452;&#21512;"&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#32452;&#21512;&#30340;&#38543;&#26426;&#20248;&#21183;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#31038;&#20250;&#25216;&#26415;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#31181;&#22522;&#20110;&#23454;&#38469;&#38543;&#26426;&#21464;&#37327;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#38543;&#26426;&#20248;&#21183;&#30340;&#26032;&#30340;&#32479;&#35745;&#30456;&#23545;&#27979;&#35797;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20010;&#27979;&#35797;&#20013;&#30340;&#20108;&#38454;&#32479;&#35745;&#19982;&#22312;&#35745;&#37327;&#32463;&#27982;&#23398;&#21644;&#25968;&#23398;&#37329;&#34701;&#20013;&#24120;&#29992;&#30340;&#24179;&#22343;&#39118;&#38505;&#27169;&#22411;&#30456;&#32852;&#31995;&#65292;&#29992;&#20110;&#22312;&#36873;&#25321;&#26041;&#26696;&#26102;&#24179;&#34913;&#39118;&#38505;&#21644;&#25928;&#29992;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#27491;&#24335;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#38505;&#24847;&#35782;&#30340;&#22522;&#30784;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#32473;&#23450;&#30001;&#25351;&#23450;&#24230;&#37327;&#37327;&#21270;&#30340;&#38450;&#25252;&#26639;&#12290;&#21463;&#25968;&#23398;&#37329;&#34701;&#20013;&#30340;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#21644;&#36873;&#25321;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#27169;&#22411;&#23450;&#20041;&#20102;&#19968;&#20010;"&#24230;&#37327;&#32452;&#21512;"&#65292;&#20316;&#20026;&#32858;&#21512;&#19968;&#31995;&#21015;&#24230;&#37327;&#30340;&#25163;&#27573;&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#32452;&#21512;&#30340;&#38543;&#26426;&#20248;&#21183;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#27979;&#35797;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;&#22312;&#29702;&#35770;&#19978;&#30001;&#36890;&#36807;&#20013;&#24515;&#26497;&#38480;&#30340;&#28176;&#36817;&#20998;&#26512;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a distributional framework for assessing socio-technical risks of foundation models with quantified statistical significance. Our approach hinges on a new statistical relative testing based on first and second order stochastic dominance of real random variables. We show that the second order statistics in this test are linked to mean-risk models commonly used in econometrics and mathematical finance to balance risk and utility when choosing between alternatives. Using this framework, we formally develop a risk-aware approach for foundation model selection given guardrails quantified by specified metrics. Inspired by portfolio optimization and selection theory in mathematical finance, we define a \emph{metrics portfolio} for each model as a means to aggregate a collection of metrics, and perform model selection based on the stochastic dominance of these portfolios. The statistical significance of our tests is backed theoretically by an asymptotic analysis via central limit th
&lt;/p&gt;</description></item><item><title>FABind&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21475;&#34955;&#39044;&#27979;&#21644;&#23545;&#25509;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.06763</link><description>&lt;p&gt;
FABind: &#24555;&#36895;&#20934;&#30830;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
FABind: Fast and Accurate Protein-Ligand Binding. (arXiv:2310.06763v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06763
&lt;/p&gt;
&lt;p&gt;
FABind&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21475;&#34955;&#39044;&#27979;&#21644;&#23545;&#25509;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#26088;&#22312;&#23454;&#29616;&#24555;&#36895;&#20934;&#30830;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#23545;&#34507;&#30333;&#36136;&#21644;&#37197;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#24314;&#27169;&#24182;&#20934;&#30830;&#39044;&#27979;&#20854;&#32467;&#21512;&#32467;&#26500;&#26159;&#19968;&#39033;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#65292;&#37319;&#26679;&#27861;&#21644;&#22238;&#24402;&#27861;&#25104;&#20026;&#20004;&#31181;&#31361;&#20986;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#23384;&#22312;&#26126;&#26174;&#30340;&#23616;&#38480;&#24615;&#12290;&#37319;&#26679;&#27861;&#36890;&#24120;&#30001;&#20110;&#38656;&#35201;&#29983;&#25104;&#22810;&#20010;&#20505;&#36873;&#32467;&#26500;&#26469;&#36827;&#34892;&#36873;&#25321;&#32780;&#25928;&#29575;&#36739;&#20302;&#12290;&#32780;&#22238;&#24402;&#27861;&#25552;&#20379;&#20102;&#24555;&#36895;&#30340;&#39044;&#27979;&#65292;&#20294;&#21487;&#33021;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#21478;&#22806;&#65292;&#34507;&#30333;&#36136;&#22823;&#23567;&#30340;&#21464;&#21270;&#36890;&#24120;&#38656;&#35201;&#22806;&#37096;&#27169;&#22359;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#32467;&#21512;&#21475;&#34955;&#65292;&#36827;&#19968;&#27493;&#24433;&#21709;&#25928;&#29575;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FABind&#65292;&#19968;&#20010;&#23558;&#21475;&#34955;&#39044;&#27979;&#21644;&#23545;&#25509;&#30456;&#32467;&#21512;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#21644;&#24555;&#36895;&#30340;&#34507;&#30333;-&#37197;&#20307;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling the interaction between proteins and ligands and accurately predicting their binding structures is a critical yet challenging task in drug discovery. Recent advancements in deep learning have shown promise in addressing this challenge, with sampling-based and regression-based methods emerging as two prominent approaches. However, these methods have notable limitations. Sampling-based methods often suffer from low efficiency due to the need for generating multiple candidate structures for selection. On the other hand, regression-based methods offer fast predictions but may experience decreased accuracy. Additionally, the variation in protein sizes often requires external modules for selecting suitable binding pockets, further impacting efficiency. In this work, we propose $\mathbf{FABind}$, an end-to-end model that combines pocket prediction and docking to achieve accurate and fast protein-ligand binding. $\mathbf{FABind}$ incorporates a unique ligand-informed pocket prediction
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;NAEPro&#27169;&#22411;&#65292;&#36890;&#36807;&#20989;&#25968;&#20960;&#20309;&#24341;&#23548;&#30340;&#26041;&#27861;&#20849;&#21516;&#35774;&#35745;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#27688;&#22522;&#37240;&#24674;&#22797;&#29575;&#12289;TM&#20998;&#25968;&#21644;RMSD&#31561;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.04343</link><description>&lt;p&gt;
&#20989;&#25968;&#20960;&#20309;&#24341;&#23548;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#39592;&#26550;&#32467;&#26500;&#20849;&#21516;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Functional Geometry Guided Protein Sequence and Backbone Structure Co-Design. (arXiv:2310.04343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;NAEPro&#27169;&#22411;&#65292;&#36890;&#36807;&#20989;&#25968;&#20960;&#20309;&#24341;&#23548;&#30340;&#26041;&#27861;&#20849;&#21516;&#35774;&#35745;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#27688;&#22522;&#37240;&#24674;&#22797;&#29575;&#12289;TM&#20998;&#25968;&#21644;RMSD&#31561;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#26159;&#20960;&#20046;&#25152;&#26377;&#29983;&#29289;&#20307;&#20013;&#36127;&#36131;&#22522;&#26412;&#21151;&#33021;&#30340;&#22823;&#20998;&#23376;&#12290;&#35774;&#35745;&#21512;&#29702;&#30340;&#20855;&#26377;&#26399;&#26395;&#21151;&#33021;&#30340;&#34507;&#30333;&#36136;&#33267;&#20851;&#37325;&#35201;&#12290;&#34507;&#30333;&#36136;&#30340;&#24207;&#21015;&#21644;&#32467;&#26500;&#23494;&#20999;&#30456;&#20851;&#65292;&#23427;&#20204;&#20849;&#21516;&#20915;&#23450;&#20102;&#20854;&#21151;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NAEPro&#65292;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#26816;&#27979;&#21040;&#30340;&#21151;&#33021;&#20301;&#28857;&#20849;&#21516;&#35774;&#35745;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#32467;&#26500;&#30340;&#27169;&#22411;&#12290;NAEPro&#37319;&#29992;&#20102;&#27880;&#24847;&#21147;&#21644;&#31561;&#21464;&#23618;&#30340;&#20132;&#38169;&#32593;&#32476;&#65292;&#21487;&#20197;&#25429;&#25417;&#24207;&#21015;&#20013;&#30340;&#20840;&#23616;&#30456;&#20851;&#24615;&#20197;&#21450;&#19977;&#32500;&#31354;&#38388;&#20013;&#26368;&#36817;&#27688;&#22522;&#37240;&#30340;&#23616;&#37096;&#24433;&#21709;&#12290;&#36825;&#31181;&#26550;&#26500;&#22312;&#20004;&#20010;&#23618;&#38754;&#19978;&#20419;&#36827;&#20102;&#26377;&#25928;&#32780;&#32463;&#27982;&#30340;&#20449;&#24687;&#20256;&#36882;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#34507;&#30333;&#36136;&#25968;&#25454;&#38598;&#65288;&#946;-&#20869;&#37232;&#33018;&#37238;&#21644;&#32908;&#32418;&#34507;&#30333;&#65289;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21644;&#20960;&#20010;&#24378;&#31454;&#20105;&#22522;&#32447;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#31454;&#20105;&#23545;&#25163;&#20013;&#22987;&#32456;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#27688;&#22522;&#37240;&#24674;&#22797;&#29575;&#12289;TM&#20998;&#25968;&#21644;&#26368;&#20302;&#30340;RMSD&#12290;&#36825;&#20123;&#21457;&#29616;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proteins are macromolecules responsible for essential functions in almost all living organisms. Designing reasonable proteins with desired functions is crucial. A protein's sequence and structure are strongly correlated and they together determine its function. In this paper, we propose NAEPro, a model to jointly design Protein sequence and structure based on automatically detected functional sites. NAEPro is powered by an interleaving network of attention and equivariant layers, which can capture global correlation in a whole sequence and local influence from nearest amino acids in three dimensional (3D) space. Such an architecture facilitates effective yet economic message passing at two levels. We evaluate our model and several strong baselines on two protein datasets, $\beta$-lactamase and myoglobin. Experimental results show that our model consistently achieves the highest amino acid recovery rate, TM-score, and the lowest RMSD among all competitors. These findings prove the capab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21516;&#20262;&#26494;&#24347;&#35757;&#32451;&#31639;&#27861;&#65288;HRTA&#65289;&#30340;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26500;&#24314;&#26080;&#32541;&#36830;&#25509;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#21644;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#21516;&#20262;&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#26494;&#24347;&#21516;&#20262;&#21442;&#25968;&#20197;&#22686;&#24378;&#35757;&#32451;&#31934;&#32454;&#21270;&#36807;&#31243;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#36807;&#31243;&#65292;&#22312;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;NTK&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.15244</link><description>&lt;p&gt;
&#26080;&#31351;&#23485;&#24230;&#20004;&#23618;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#21516;&#20262;&#26494;&#24347;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Homotopy Relaxation Training Algorithms for Infinite-Width Two-Layer ReLU Neural Networks. (arXiv:2309.15244v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21516;&#20262;&#26494;&#24347;&#35757;&#32451;&#31639;&#27861;&#65288;HRTA&#65289;&#30340;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26500;&#24314;&#26080;&#32541;&#36830;&#25509;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#21644;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#21516;&#20262;&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#26494;&#24347;&#21516;&#20262;&#21442;&#25968;&#20197;&#22686;&#24378;&#35757;&#32451;&#31934;&#32454;&#21270;&#36807;&#31243;&#65292;&#21152;&#36895;&#20102;&#35757;&#32451;&#36807;&#31243;&#65292;&#22312;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;NTK&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;&#21516;&#20262;&#26494;&#24347;&#35757;&#32451;&#31639;&#27861;&#65288;HRTA&#65289;&#65292;&#26088;&#22312;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#32467;&#21512;&#20102;&#20004;&#20010;&#20851;&#38190;&#26426;&#21046;&#65306;&#19968;&#20010;&#26159;&#26500;&#24314;&#26080;&#32541;&#36830;&#25509;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#21644;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#21516;&#20262;&#28608;&#27963;&#20989;&#25968;&#65307;&#21478;&#19968;&#20010;&#25216;&#26415;&#26159;&#26494;&#24347;&#21516;&#20262;&#21442;&#25968;&#20197;&#22686;&#24378;&#35757;&#32451;&#31934;&#32454;&#21270;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;NTK&#65289;&#30340;&#32972;&#26223;&#19979;&#23545;&#36825;&#31181;&#26032;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26174;&#33879;&#25913;&#36827;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#23588;&#20854;&#26159;&#22312;&#32771;&#34385;&#26356;&#22823;&#23485;&#24230;&#30340;&#32593;&#32476;&#26102;&#65292;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#35770;&#12290;&#36825;&#31181;&#25552;&#35758;&#30340;HRTA&#23637;&#31034;&#20102;&#23545;&#20854;&#20182;&#28608;&#27963;&#20989;&#25968;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel training approach called the Homotopy Relaxation Training Algorithm (HRTA), aimed at accelerating the training process in contrast to traditional methods. Our algorithm incorporates two key mechanisms: one involves building a homotopy activation function that seamlessly connects the linear activation function with the ReLU activation function; the other technique entails relaxing the homotopy parameter to enhance the training refinement process. We have conducted an in-depth analysis of this novel method within the context of the neural tangent kernel (NTK), revealing significantly improved convergence rates. Our experimental results, especially when considering networks with larger widths, validate the theoretical conclusions. This proposed HRTA exhibits the potential for other activation functions and deep neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;C&#32534;&#31243;&#20316;&#19994;&#36827;&#34892;&#33258;&#21160;&#35780;&#20998;&#30340;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#20195;&#30721;&#30340;&#36716;&#25442;&#22120;&#35789;&#23884;&#20837;&#27169;&#22411;CodeBERT&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#35813;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#32479;&#35745;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20043;&#38388;&#30340;&#23545;&#27604;&#12290;</title><link>http://arxiv.org/abs/2309.15216</link><description>&lt;p&gt;
&#20351;&#29992;CodeBERT&#21644;Random Forest Regressor&#36827;&#34892;&#33258;&#21160;&#35780;&#20998;&#30340;C&#32534;&#31243;&#20316;&#19994;
&lt;/p&gt;
&lt;p&gt;
Auto-grading C programming assignments with CodeBERT and Random Forest Regressor. (arXiv:2309.15216v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;C&#32534;&#31243;&#20316;&#19994;&#36827;&#34892;&#33258;&#21160;&#35780;&#20998;&#30340;&#20998;&#26512;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#20195;&#30721;&#30340;&#36716;&#25442;&#22120;&#35789;&#23884;&#20837;&#27169;&#22411;CodeBERT&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#35813;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#32479;&#35745;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20043;&#38388;&#30340;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#21160;&#35780;&#20998;&#32534;&#31243;&#20316;&#19994;&#22240;&#22797;&#26434;&#24615;&#21644;&#20027;&#35266;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#33258;&#21160;&#35780;&#20998;&#31616;&#21270;&#20102;&#20219;&#21153;&#12290;&#23427;&#23458;&#35266;&#22320;&#35780;&#20272;&#20195;&#30721;&#36136;&#37327;&#65292;&#26816;&#27979;&#38169;&#35823;&#65292;&#24182;&#20934;&#30830;&#22320;&#20998;&#37197;&#20998;&#25968;&#65292;&#20943;&#36731;&#20102;&#25945;&#24072;&#30340;&#36127;&#25285;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#39640;&#25928;&#21644;&#20844;&#24179;&#30340;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22238;&#24402;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#31561;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;C&#32534;&#31243;&#20316;&#19994;&#36827;&#34892;&#33258;&#21160;&#35780;&#20998;&#30340;&#20998;&#26512;&#12290;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#20195;&#30721;&#30340;&#36716;&#25442;&#22120;&#35789;&#23884;&#20837;&#27169;&#22411;CodeBERT&#65292;&#23558;&#25991;&#26412;&#20195;&#30721;&#36755;&#20837;&#36716;&#25442;&#20026;&#21521;&#37327;&#65292;&#28982;&#21518;&#23558;&#21521;&#37327;&#36755;&#20837;&#21040;&#20960;&#20010;&#27169;&#22411;&#20013;&#12290;&#27979;&#35797;&#32467;&#26524;&#35777;&#26126;&#20102;&#24314;&#35758;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#20026;1.89&#12290;&#26412;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#32479;&#35745;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20043;&#38388;&#30340;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grading coding assignments manually is challenging due to complexity and subjectivity. However, auto-grading with deep learning simplifies the task. It objectively assesses code quality, detects errors, and assigns marks accurately, reducing the burden on instructors while ensuring efficient and fair assessment. This study provides an analysis of auto-grading of the C programming assignments using machine learning and deep learning approaches like regression, convolutional neural networks (CNN) and long short-term memory (LSTM). Using a code-based transformer word embedding model called CodeBERT, the textual code inputs were transformed into vectors, and the vectors were then fed into several models. The testing findings demonstrated the efficacy of the suggested strategy with a root mean squared error (RMSE) of 1.89. The contrast between statistical methods and deep learning techniques is discussed in the study.
&lt;/p&gt;</description></item><item><title>BiSinger&#26159;&#19968;&#20010;&#21452;&#35821;&#21512;&#25104;&#27468;&#22768;&#31995;&#32479;&#65292;&#36890;&#36807;&#35774;&#35745;&#20849;&#20139;&#34920;&#31034;&#12289;&#34701;&#21512;&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#24320;&#28304;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#21487;&#20197;&#36827;&#34892;&#33521;&#35821;&#21644;&#27721;&#35821;&#26222;&#36890;&#35805;&#28151;&#21512;&#32534;&#30721;&#27468;&#22768;&#21512;&#25104;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#24182;&#20445;&#25345;&#20102;&#27721;&#35821;&#27468;&#26354;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.14089</link><description>&lt;p&gt;
BiSinger: &#21452;&#35821;&#21512;&#25104;&#27468;&#22768;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
BiSinger: Bilingual Singing Voice Synthesis. (arXiv:2309.14089v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14089
&lt;/p&gt;
&lt;p&gt;
BiSinger&#26159;&#19968;&#20010;&#21452;&#35821;&#21512;&#25104;&#27468;&#22768;&#31995;&#32479;&#65292;&#36890;&#36807;&#35774;&#35745;&#20849;&#20139;&#34920;&#31034;&#12289;&#34701;&#21512;&#25968;&#25454;&#38598;&#21644;&#20351;&#29992;&#24320;&#28304;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#21487;&#20197;&#36827;&#34892;&#33521;&#35821;&#21644;&#27721;&#35821;&#26222;&#36890;&#35805;&#28151;&#21512;&#32534;&#30721;&#27468;&#22768;&#21512;&#25104;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#24182;&#20445;&#25345;&#20102;&#27721;&#35821;&#27468;&#26354;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#25216;&#26415;&#22312;&#27468;&#22768;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#22810;&#35821;&#31181;&#21512;&#25104;&#27468;&#22768;&#27169;&#22411;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#25506;&#32034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BiSinger&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#33521;&#35821;&#21644;&#27721;&#35821;&#26222;&#36890;&#35805;&#30340;&#21452;&#35821;&#27969;&#34892;&#27468;&#22768;&#21512;&#25104;&#31995;&#32479;&#12290;&#24403;&#21069;&#30340;&#31995;&#32479;&#38656;&#35201;&#20998;&#21035;&#38024;&#23545;&#27599;&#31181;&#35821;&#35328;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#19988;&#26080;&#27861;&#20934;&#30830;&#22320;&#34920;&#31034;&#27721;&#35821;&#21644;&#33521;&#35821;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#28151;&#21512;&#32534;&#30721;&#27468;&#22768;&#21512;&#25104;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#27721;&#35821;&#21644;&#33521;&#35821;&#27468;&#22768;&#20043;&#38388;&#30340;&#20849;&#20139;&#34920;&#31034;&#65292;&#36890;&#36807;&#20351;&#29992;CMU&#23383;&#20856;&#21644;&#26144;&#23556;&#35268;&#21017;&#23454;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;&#24320;&#28304;&#27468;&#22768;&#36716;&#25442;&#25216;&#26415;&#34701;&#21512;&#20102;&#21333;&#35821;&#27468;&#22768;&#25968;&#25454;&#38598;&#65292;&#20197;&#29983;&#25104;&#21452;&#35821;&#27468;&#22768;&#65292;&#21516;&#26102;&#36824;&#25506;&#32034;&#20102;&#21452;&#35821;&#35328;&#38899;&#25968;&#25454;&#30340;&#28508;&#22312;&#29992;&#36884;&#12290;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35821;&#35328;&#26080;&#20851;&#34920;&#31034;&#21644;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#25972;&#21512;&#20351;&#24471;&#21333;&#19968;&#27169;&#22411;&#22312;&#33521;&#35821;&#21644;&#28151;&#21512;&#32534;&#30721;&#27468;&#22768;&#21512;&#25104;&#26041;&#38754;&#24615;&#33021;&#26356;&#22909;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27721;&#35821;&#27468;&#26354;&#30340;&#34920;&#29616;&#12290;&#38899;&#39057;&#26679;&#26412;&#21487;&#22312;&#26576;&#32593;&#22336;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Singing Voice Synthesis (SVS) has made great strides with Text-to-Speech (TTS) techniques, multilingual singing voice modeling remains relatively unexplored. This paper presents BiSinger, a bilingual pop SVS system for English and Chinese Mandarin. Current systems require separate models per language and cannot accurately represent both Chinese and English, hindering code-switch SVS. To address this gap, we design a shared representation between Chinese and English singing voices, achieved by using the CMU dictionary with mapping rules. We fuse monolingual singing datasets with open-source singing voice conversion techniques to generate bilingual singing voices while also exploring the potential use of bilingual speech data. Experiments affirm that our language-independent representation and incorporation of related datasets enable a single model with enhanced performance in English and code-switch SVS while maintaining Chinese song performance. Audio samples are available at 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;I&#178;F&#65292;&#21487;&#20197;&#26377;&#25928;&#36817;&#20284;&#28145;&#24230;&#26799;&#24230;&#27844;&#38706;&#25915;&#20987;&#65292;&#24182;&#24314;&#31435;&#20102;&#24674;&#22797;&#22270;&#20687;&#21644;&#31169;&#26377;&#26799;&#24230;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;&#36890;&#36807;&#36825;&#20010;&#26041;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#24212;&#23545;&#28145;&#24230;&#26799;&#24230;&#27844;&#38706;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2309.13016</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#28436;&#24433;&#21709;&#20989;&#25968;&#29702;&#35299;&#28145;&#24230;&#26799;&#24230;&#27844;&#38706;
&lt;/p&gt;
&lt;p&gt;
Understanding Deep Gradient Leakage via Inversion Influence Functions. (arXiv:2309.13016v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;I&#178;F&#65292;&#21487;&#20197;&#26377;&#25928;&#36817;&#20284;&#28145;&#24230;&#26799;&#24230;&#27844;&#38706;&#25915;&#20987;&#65292;&#24182;&#24314;&#31435;&#20102;&#24674;&#22797;&#22270;&#20687;&#21644;&#31169;&#26377;&#26799;&#24230;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;&#36890;&#36807;&#36825;&#20010;&#26041;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#24212;&#23545;&#28145;&#24230;&#26799;&#24230;&#27844;&#38706;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26799;&#24230;&#27844;&#38706;&#65288;DGL&#65289;&#26159;&#19968;&#31181;&#38750;&#24120;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#26799;&#24230;&#21521;&#37327;&#20013;&#24674;&#22797;&#31169;&#26377;&#35757;&#32451;&#22270;&#20687;&#12290;&#36825;&#31181;&#25915;&#20987;&#23545;&#20110;&#20855;&#26377;&#25935;&#24863;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#20998;&#24067;&#24335;&#23398;&#20064;&#25552;&#20986;&#20102;&#37325;&#35201;&#30340;&#38544;&#31169;&#25361;&#25112;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#38656;&#35201;&#20849;&#20139;&#26799;&#24230;&#12290;&#38450;&#24481;&#27492;&#31867;&#25915;&#20987;&#38656;&#35201;&#20294;&#32570;&#20047;&#23545;&#38544;&#31169;&#27844;&#38706;&#21457;&#29983;&#30340;&#26102;&#38388;&#21644;&#26041;&#24335;&#30340;&#29702;&#35299;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#28145;&#24230;&#32593;&#32476;&#30340;&#40657;&#30418;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#28436;&#24433;&#21709;&#20989;&#25968;&#65288;I&#178;F&#65289;&#65292;&#36890;&#36807;&#38544;&#24335;&#35299;&#20915;DGL&#38382;&#39064;&#65292;&#24314;&#31435;&#20102;&#24674;&#22797;&#22270;&#20687;&#21644;&#31169;&#26377;&#26799;&#24230;&#20043;&#38388;&#30340;&#38381;&#24335;&#36830;&#25509;&#12290;&#19982;&#30452;&#25509;&#35299;&#20915;DGL&#30456;&#27604;&#65292;I&#178;F&#22312;&#20998;&#26512;&#28145;&#24230;&#32593;&#32476;&#26102;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#65292;&#20165;&#38656;&#35201;&#26799;&#24230;&#21644;&#38597;&#21487;&#27604;&#21521;&#37327;&#20056;&#31215;&#30340;&#39044;&#35328;&#35775;&#38382;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;I&#178;F&#22312;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#12289;&#25968;&#25454;&#38598;&#12289;&#25915;&#20987;&#23454;&#29616;&#21644;&#22522;&#20110;&#22122;&#22768;&#30340;&#38450;&#24481;&#20013;&#37117;&#33021;&#26377;&#25928;&#36817;&#20284;DGL&#12290;&#25105;&#20204;&#36890;&#36807;&#36825;&#31181;&#26032;&#39062;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#20102;&#35299;&#28145;&#24230;&#26799;&#24230;&#27844;&#38706;&#30340;&#26426;&#29702;&#21644;&#24212;&#23545;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Gradient Leakage (DGL) is a highly effective attack that recovers private training images from gradient vectors. This attack casts significant privacy challenges on distributed learning from clients with sensitive data, where clients are required to share gradients. Defending against such attacks requires but lacks an understanding of when and how privacy leakage happens, mostly because of the black-box nature of deep networks. In this paper, we propose a novel Inversion Influence Function (I$^2$F) that establishes a closed-form connection between the recovered images and the private gradients by implicitly solving the DGL problem. Compared to directly solving DGL, I$^2$F is scalable for analyzing deep networks, requiring only oracle access to gradients and Jacobian-vector products. We empirically demonstrate that I$^2$F effectively approximated the DGL generally on different model architectures, datasets, attack implementations, and noise-based defenses. With this novel tool, we 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDCSR&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26469;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2309.08420</link><description>&lt;p&gt;
FedDCSR: &#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#23454;&#29616;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning. (arXiv:2309.08420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08420
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedDCSR&#30340;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#26469;&#22788;&#29702;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#24182;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#29992;&#25143;&#24207;&#21015;&#25968;&#25454;&#30340;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;(CSR)&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CSR&#26041;&#27861;&#38656;&#35201;&#22312;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#21407;&#22987;&#29992;&#25143;&#25968;&#25454;&#65292;&#36825;&#36829;&#21453;&#20102;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;(GDPR)&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#23558;&#32852;&#37030;&#23398;&#20064;(FL)&#21644;CSR&#30456;&#32467;&#21512;&#65292;&#20805;&#20998;&#21033;&#29992;&#19981;&#21516;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#23545;FL&#30340;&#25972;&#20307;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedDCSR&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#22411;&#32852;&#37030;&#36328;&#39046;&#22495;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24207;&#21015;&#29305;&#24449;&#24322;&#36136;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#39046;&#22495;&#20869;-&#39046;&#22495;&#38388;&#24207;&#21015;&#34920;&#31034;&#35299;&#32544;(SRD)&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#25143;&#24207;&#21015;&#29305;&#24449;&#35299;&#32544;&#25104;&#39046;&#22495;&#20849;&#20139;&#21644;&#39046;&#22495;&#19987;&#23646;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Sequential Recommendation (CSR) which leverages user sequence data from multiple domains has received extensive attention in recent years. However, the existing CSR methods require sharing origin user data across domains, which violates the General Data Protection Regulation (GDPR). Thus, it is necessary to combine federated learning (FL) and CSR to fully utilize knowledge from different domains while preserving data privacy. Nonetheless, the sequence feature heterogeneity across different domains significantly impacts the overall performance of FL. In this paper, we propose FedDCSR, a novel federated cross-domain sequential recommendation framework via disentangled representation learning. Specifically, to address the sequence feature heterogeneity across domains, we introduce an approach called inter-intra domain sequence representation disentanglement (SRD) to disentangle the user sequence features into domain-shared and domain-exclusive features. In addition, we design
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22320;&#29702;&#27668;&#35937;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38271;&#26399;&#24178;&#26097;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#26469;&#39044;&#27979;&#29305;&#23450;&#22320;&#21306;&#24178;&#26097;&#27010;&#29575;&#12290;&#37319;&#29992;&#21367;&#31215;LSTM&#21644;Transformer&#27169;&#22411;&#30456;&#27604;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06212</link><description>&lt;p&gt;
&#22522;&#20110;&#22320;&#29702;&#27668;&#35937;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38271;&#26399;&#24178;&#26097;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Long-term drought prediction using deep neural networks based on geospatial weather data. (arXiv:2309.06212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06212
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22320;&#29702;&#27668;&#35937;&#25968;&#25454;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38271;&#26399;&#24178;&#26097;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#26469;&#39044;&#27979;&#29305;&#23450;&#22320;&#21306;&#24178;&#26097;&#27010;&#29575;&#12290;&#37319;&#29992;&#21367;&#31215;LSTM&#21644;Transformer&#27169;&#22411;&#30456;&#27604;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20892;&#19994;&#23454;&#36341;&#20013;&#65292;&#20934;&#30830;&#39044;&#27979;&#29305;&#23450;&#22320;&#21306;&#24178;&#26097;&#27010;&#29575;&#23545;&#20110;&#20915;&#31574;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#23588;&#20854;&#23545;&#20110;&#38271;&#26399;&#20915;&#31574;&#65292;&#25552;&#21069;&#19968;&#24180;&#36827;&#34892;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24863;&#20852;&#36259;&#21306;&#22495;&#21450;&#20854;&#30456;&#37051;&#21306;&#22495;&#20869;&#21508;&#31181;&#22240;&#32032;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#39044;&#27979;&#36825;&#19968;&#27010;&#29575;&#23384;&#22312;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21508;&#31181;&#26102;&#31354;&#31070;&#32463;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25152;&#32771;&#34385;&#30340;&#27169;&#22411;&#20027;&#35201;&#26159;&#26681;&#25454;Palmer&#24178;&#26097;&#20005;&#37325;&#25351;&#25968;&#65288;PDSI&#65289;&#39044;&#27979;&#24863;&#20852;&#36259;&#20122;&#21306;&#30340;&#24178;&#26097;&#24378;&#24230;&#65292;&#21033;&#29992;&#27668;&#20505;&#27169;&#22411;&#30340;&#20869;&#22312;&#22240;&#32032;&#21644;&#35265;&#35299;&#26469;&#25552;&#39640;&#24178;&#26097;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#27604;&#36739;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20934;&#26799;&#24230;&#25552;&#21319;&#21644;&#36923;&#36753;&#22238;&#24402;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#21367;&#31215;LSTM&#65288;ConvLSTM&#65289;&#21644;Transformer&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;&#21069;&#20004;&#31181;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;ROC AUC&#20998;&#25968;&#65292;&#39640;&#36798;0.90
&lt;/p&gt;
&lt;p&gt;
The accurate prediction of drought probability in specific regions is crucial for informed decision-making in agricultural practices. It is important to make predictions one year in advance, particularly for long-term decisions. However, forecasting this probability presents challenges due to the complex interplay of various factors within the region of interest and neighboring areas. In this study, we propose an end-to-end solution to address this issue based on various spatiotemporal neural networks. The models considered focus on predicting the drought intensity based on the Palmer Drought Severity Index (PDSI) for subregions of interest, leveraging intrinsic factors and insights from climate models to enhance drought predictions.  Comparative evaluations demonstrate the superior accuracy of Convolutional LSTM (ConvLSTM) and transformer models compared to baseline gradient boosting and logistic regression solutions. The two former models achieved impressive ROC AUC scores from 0.90 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#23376;&#32676;&#20307;&#20013;&#36827;&#34892;&#22240;&#26524;&#25928;&#24212;&#35782;&#21035;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#24182;&#20513;&#23548;&#20102;s-ID&#38382;&#39064;&#12290;&#35770;&#25991;&#25552;&#20379;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#20197;&#20415;&#20174;&#23376;&#32676;&#20307;&#30340;&#35266;&#27979;&#20998;&#24067;&#20013;&#35782;&#21035;&#20986;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.02281</link><description>&lt;p&gt;
s-ID&#65306;&#22312;&#23376;&#32676;&#20307;&#20013;&#30340;&#22240;&#26524;&#25928;&#24212;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
s-ID: Causal Effect Identification in a Sub-Population. (arXiv:2309.02281v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02281
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#23376;&#32676;&#20307;&#20013;&#36827;&#34892;&#22240;&#26524;&#25928;&#24212;&#35782;&#21035;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#24182;&#20513;&#23548;&#20102;s-ID&#38382;&#39064;&#12290;&#35770;&#25991;&#25552;&#20379;&#20102;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#20197;&#20415;&#20174;&#23376;&#32676;&#20307;&#30340;&#35266;&#27979;&#20998;&#24067;&#20013;&#35782;&#21035;&#20986;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23376;&#32676;&#20307;&#20013;&#30340;&#22240;&#26524;&#25512;&#26029;&#28041;&#21450;&#21040;&#35782;&#21035;&#24178;&#39044;&#23545;&#29305;&#23450;&#23376;&#32452;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#36825;&#20123;&#23376;&#32452;&#36890;&#36807;&#25277;&#26679;&#36807;&#31243;&#20013;&#30340;&#31995;&#32479;&#20559;&#24046;&#19982;&#25972;&#20010;&#32676;&#20307;&#26377;&#25152;&#21306;&#21035;&#12290;&#28982;&#32780;&#65292;&#24573;&#30053;&#23376;&#32676;&#20307;&#24341;&#20837;&#30340;&#32454;&#24494;&#24046;&#21035;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#25512;&#26029;&#65292;&#25110;&#32773;&#38480;&#21046;&#29616;&#26377;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#24182;&#20513;&#23548;&#23376;&#32676;&#20307;&#20013;&#30340;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;&#65288;&#20197;&#19979;&#31616;&#31216;s-ID&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#20165;&#21487;&#20197;&#35775;&#38382;&#30446;&#26631;&#23376;&#32676;&#20307;&#30340;&#35266;&#27979;&#25968;&#25454;&#65288;&#32780;&#19981;&#26159;&#25972;&#20010;&#32676;&#20307;&#65289;&#12290;&#29616;&#26377;&#30340;&#23376;&#32676;&#20307;&#25512;&#26029;&#38382;&#39064;&#26159;&#22522;&#20110;&#32473;&#23450;&#30340;&#25968;&#25454;&#20998;&#24067;&#28304;&#20110;&#25972;&#20010;&#32676;&#20307;&#30340;&#21069;&#25552;&#65292;&#22240;&#27492;&#26080;&#27861;&#35299;&#20915;s-ID&#38382;&#39064;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#22240;&#26524;&#22270;&#20013;&#24517;&#39035;&#28385;&#36275;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#20197;&#20415;&#20174;&#35813;&#23376;&#32676;&#20307;&#30340;&#35266;&#27979;&#20998;&#24067;&#20013;&#35782;&#21035;&#20986;&#23376;&#32676;&#20307;&#20013;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference in a sub-population involves identifying the causal effect of an intervention on a specific subgroup, which is distinguished from the whole population through the influence of systematic biases in the sampling process. However, ignoring the subtleties introduced by sub-populations can either lead to erroneous inference or limit the applicability of existing methods. We introduce and advocate for a causal inference problem in sub-populations (henceforth called s-ID), in which we merely have access to observational data of the targeted sub-population (as opposed to the entire population). Existing inference problems in sub-populations operate on the premise that the given data distributions originate from the entire population, thus, cannot tackle the s-ID problem. To address this gap, we provide necessary and sufficient conditions that must hold in the causal graph for a causal effect in a sub-population to be identifiable from the observational distribution of that sub
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20215;&#26684;&#24046;&#24322;&#21270;&#28216;&#25103;&#65288;PDG&#65289;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#23458;&#25143;&#25552;&#20379;&#30340;&#26381;&#21153;&#36827;&#34892;&#23450;&#20215;&#24046;&#24322;&#21270;&#65292;&#25913;&#21892;&#20102;&#32852;&#21512;&#23398;&#20064;&#30340;&#24615;&#33021;&#24182;&#38477;&#20302;&#20102;&#28608;&#21169;&#23458;&#25143;&#21442;&#19982;&#32852;&#21512;&#23398;&#20064;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.13838</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#36164;&#28304;&#31649;&#29702;&#20013;&#30340;&#20215;&#26684;&#24046;&#24322;&#21270;&#28216;&#25103;&#23545;&#32852;&#21512;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Price-Discrimination Game for Distributed Resource Management in Federated Learning. (arXiv:2308.13838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20215;&#26684;&#24046;&#24322;&#21270;&#28216;&#25103;&#65288;PDG&#65289;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#23458;&#25143;&#25552;&#20379;&#30340;&#26381;&#21153;&#36827;&#34892;&#23450;&#20215;&#24046;&#24322;&#21270;&#65292;&#25913;&#21892;&#20102;&#32852;&#21512;&#23398;&#20064;&#30340;&#24615;&#33021;&#24182;&#38477;&#20302;&#20102;&#28608;&#21169;&#23458;&#25143;&#21442;&#19982;&#32852;&#21512;&#23398;&#20064;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#32852;&#21512;&#23398;&#20064;&#20013;&#65292;&#21442;&#25968;&#26381;&#21153;&#22120;&#21644;&#22810;&#20010;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#21487;&#20197;&#24418;&#25104;&#20856;&#22411;&#30340;&#20080;&#26041;&#24066;&#22330;&#65292;&#20854;&#20013;PS/&#20080;&#23478;&#25968;&#37327;&#36828;&#36828;&#23569;&#20110;&#23458;&#25143;&#31471;/&#21334;&#23478;&#25968;&#37327;&#12290;&#20026;&#20102;&#25913;&#21892;&#32852;&#21512;&#23398;&#20064;&#30340;&#24615;&#33021;&#24182;&#20943;&#23569;&#28608;&#21169;&#23458;&#25143;&#21442;&#19982;&#32852;&#21512;&#23398;&#20064;&#30340;&#25104;&#26412;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#19981;&#21516;&#23458;&#25143;&#25552;&#20379;&#30340;&#26381;&#21153;&#36827;&#34892;&#23450;&#20215;&#24046;&#24322;&#21270;&#65292;&#32780;&#19981;&#26159;&#31616;&#21333;&#22320;&#20026;&#19981;&#21516;&#23458;&#25143;&#25552;&#20379;&#30456;&#21516;&#30340;&#26381;&#21153;&#23450;&#20215;&#12290;&#20215;&#26684;&#24046;&#24322;&#21270;&#22522;&#20110;&#23545;&#32852;&#21512;&#23398;&#20064;&#24102;&#26469;&#30340;&#24615;&#33021;&#25913;&#36827;&#21644;&#35745;&#31639;&#36890;&#20449;&#33021;&#21147;&#30340;&#24322;&#36136;&#24615;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20215;&#26684;&#24046;&#24322;&#21270;&#28216;&#25103;&#65288;PDG&#65289;&#65292;&#20840;&#38754;&#35299;&#20915;&#20102;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#24335;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#22810;&#30446;&#26631;&#26435;&#34913;&#12289;&#23458;&#25143;&#31471;&#36873;&#25321;&#21644;&#28608;&#21169;&#26426;&#21046;&#12290;&#30001;&#20110;PDG&#26159;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#35268;&#21010;&#65288;MINLP&#65289;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#20998;&#24067;&#24335;&#21322;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In vanilla federated learning (FL) such as FedAvg, the parameter server (PS) and multiple distributed clients can form a typical buyer's market, where the number of PS/buyers of FL services is far less than the number of clients/sellers. In order to improve the performance of FL and reduce the cost of motivating clients to participate in FL, this paper proposes to differentiate the pricing for services provided by different clients rather than simply providing the same service pricing for different clients. The price is differentiated based on the performance improvements brought to FL and their heterogeneity in computing and communication capabilities. To this end, a price-discrimination game (PDG) is formulated to comprehensively address the distributed resource management problems in FL, including multi-objective trade-off, client selection, and incentive mechanism. As the PDG is a mixed-integer nonlinear programming (MINLP) problem, a distributed semi-heuristic algorithm with low c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#21160;&#24577;&#22270;&#20013;&#30340;k&#20013;&#24515;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#21253;&#25324;&#30830;&#23450;&#24615;&#36882;&#20943;&#30340;&#65288;2+&#949;&#65289;&#36817;&#20284;&#31639;&#27861;&#21644;&#38543;&#26426;&#22686;&#37327;&#30340;&#65288;4+&#949;&#65289;&#36817;&#20284;&#31639;&#27861;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#38024;&#23545;&#21152;&#26435;&#22270;&#30340;&#25674;&#38144;&#26356;&#26032;&#26102;&#38388;&#20026;kn^{o(1)}&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#31616;&#21270;&#26041;&#27861;&#24471;&#21040;&#20102;&#23545;&#20110;k&#20013;&#24515;&#38382;&#39064;&#30340;&#20840;&#21160;&#24577;&#65288;2+&#949;&#65289;&#36817;&#20284;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.15557</link><description>&lt;p&gt;
&#22270;&#19978;k&#20013;&#24515;&#38382;&#39064;&#30340;&#21160;&#24577;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dynamic algorithms for k-center on graphs. (arXiv:2307.15557v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#21160;&#24577;&#22270;&#20013;&#30340;k&#20013;&#24515;&#38382;&#39064;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#21253;&#25324;&#30830;&#23450;&#24615;&#36882;&#20943;&#30340;&#65288;2+&#949;&#65289;&#36817;&#20284;&#31639;&#27861;&#21644;&#38543;&#26426;&#22686;&#37327;&#30340;&#65288;4+&#949;&#65289;&#36817;&#20284;&#31639;&#27861;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#38024;&#23545;&#21152;&#26435;&#22270;&#30340;&#25674;&#38144;&#26356;&#26032;&#26102;&#38388;&#20026;kn^{o(1)}&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#31616;&#21270;&#26041;&#27861;&#24471;&#21040;&#20102;&#23545;&#20110;k&#20013;&#24515;&#38382;&#39064;&#30340;&#20840;&#21160;&#24577;&#65288;2+&#949;&#65289;&#36817;&#20284;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21160;&#24577;&#22270;&#20013;&#30340;k&#20013;&#24515;&#38382;&#39064;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#36873;&#25321;k&#20010;&#20013;&#24515;&#23558;&#36755;&#20837;&#20998;&#20026;k&#20010;&#38598;&#21512;&#65292;&#20351;&#24471;&#20219;&#24847;&#25968;&#25454;&#28857;&#21040;&#26368;&#36817;&#20013;&#24515;&#30340;&#26368;&#22823;&#36317;&#31163;&#26368;&#23567;&#21270;&#12290;&#24050;&#30693;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#35201;&#33719;&#24471;&#20248;&#20110;2&#30340;&#36817;&#20284;&#35299;&#26159;NP&#38590;&#30340;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#36755;&#20837;&#21487;&#20197;&#33258;&#28982;&#22320;&#24314;&#27169;&#20026;&#22270;&#65292;&#20294;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;k&#20013;&#24515;&#38382;&#39064;&#30340;&#20808;&#21069;&#30740;&#31350;&#37117;&#26159;&#22522;&#20110;&#24230;&#37327;&#31354;&#38388;&#30340;&#12290;&#26412;&#25991;&#32473;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#36882;&#20943;&#30340;&#65288;2+&#949;&#65289;&#36817;&#20284;&#31639;&#27861;&#21644;&#19968;&#31181;&#38543;&#26426;&#22686;&#37327;&#30340;&#65288;4+&#949;&#65289;&#36817;&#20284;&#31639;&#27861;&#65292;&#23545;&#20110;&#21152;&#26435;&#22270;&#65292;&#20004;&#31181;&#31639;&#27861;&#30340;&#25674;&#38144;&#26356;&#26032;&#26102;&#38388;&#20026;kn^{o(1)}&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#31181;&#32422;&#31616;&#26041;&#27861;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#23545;&#20110;k&#20013;&#24515;&#38382;&#39064;&#30340;&#20840;&#21160;&#24577;&#65288;2+&#949;&#65289;&#36817;&#20284;&#31639;&#27861;&#65292;&#20854;&#26368;&#22351;&#24773;&#20917;&#26356;&#26032;&#26102;&#38388;&#19982;&#32500;&#25252;&#65288;1+k&#36817;&#20284;&#35299;&#30340;&#26368;&#26032;&#19978;&#30028;&#30456;&#24046;&#19981;&#36229;&#36807;k&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we give the first efficient algorithms for the $k$-center problem on dynamic graphs undergoing edge updates. In this problem, the goal is to partition the input into $k$ sets by choosing $k$ centers such that the maximum distance from any data point to the closest center is minimized. It is known that it is NP-hard to get a better than $2$ approximation for this problem.  While in many applications the input may naturally be modeled as a graph, all prior works on $k$-center problem in dynamic settings are on metrics. In this paper, we give a deterministic decremental $(2+\epsilon)$-approximation algorithm and a randomized incremental $(4+\epsilon)$-approximation algorithm, both with amortized update time $kn^{o(1)}$ for weighted graphs. Moreover, we show a reduction that leads to a fully dynamic $(2+\epsilon)$-approximation algorithm for the $k$-center problem, with worst-case update time that is within a factor $k$ of the state-of-the-art upper bound for maintaining $(1+
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36890;&#20449;&#29702;&#35770;&#20013;&#32467;&#21512;&#27880;&#24847;&#21147;&#21644;&#30456;&#23545;&#29109;&#30340;&#27010;&#24565;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36890;&#20449;&#20013;&#20351;&#29992;&#27880;&#24847;&#21147;&#23548;&#21521;&#30340;&#21152;&#26435;&#30456;&#23545;&#29109;&#26159;&#19981;&#36866;&#24403;&#30340;&#65292;&#32780;&#36866;&#24403;&#30340;&#27880;&#24847;&#21147;&#36890;&#20449;&#21487;&#36890;&#36807;&#21457;&#36865;&#32773;&#20165;&#38656;&#35201;&#20102;&#35299;&#25509;&#25910;&#32773;&#30340;&#25928;&#29992;&#20989;&#25968;&#26469;&#23454;&#29616;&#26368;&#20339;&#36890;&#30693;&#12290;</title><link>http://arxiv.org/abs/2307.11423</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#23545;&#29109;&#36890;&#20449;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Attention to Entropic Communication. (arXiv:2307.11423v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11423
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36890;&#20449;&#29702;&#35770;&#20013;&#32467;&#21512;&#27880;&#24847;&#21147;&#21644;&#30456;&#23545;&#29109;&#30340;&#27010;&#24565;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36890;&#20449;&#20013;&#20351;&#29992;&#27880;&#24847;&#21147;&#23548;&#21521;&#30340;&#21152;&#26435;&#30456;&#23545;&#29109;&#26159;&#19981;&#36866;&#24403;&#30340;&#65292;&#32780;&#36866;&#24403;&#30340;&#27880;&#24847;&#21147;&#36890;&#20449;&#21487;&#36890;&#36807;&#21457;&#36865;&#32773;&#20165;&#38656;&#35201;&#20102;&#35299;&#25509;&#25910;&#32773;&#30340;&#25928;&#29992;&#20989;&#25968;&#26469;&#23454;&#29616;&#26368;&#20339;&#36890;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#30340;&#27010;&#24565;&#26159;&#25351;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#24378;&#35843;&#29305;&#23450;&#25968;&#25454;&#37325;&#35201;&#24615;&#30340;&#25968;&#20540;&#26435;&#37325;&#65292;&#22312;&#36890;&#20449;&#29702;&#35770;&#20013;&#30456;&#23545;&#29109;&#65288;RE&#65292;&#20063;&#31216;&#20026;&#24211;&#23572;&#24052;&#20811;-&#21202;&#24067;&#21202;&#25955;&#24230;&#65289;&#21457;&#25381;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#36825;&#20123;&#27010;&#24565;&#65292;&#21363;&#27880;&#24847;&#21147;&#21644;RE&#12290;RE&#24341;&#23548;&#24102;&#23485;&#26377;&#38480;&#36890;&#20449;&#20013;&#30340;&#26368;&#20339;&#32534;&#30721;&#20197;&#21450;&#36890;&#36807;&#26368;&#22823;&#29109;&#21407;&#29702;&#65288;MEP&#65289;&#36827;&#34892;&#26368;&#20339;&#28040;&#24687;&#35299;&#30721;&#12290;&#22312;&#32534;&#30721;&#22330;&#26223;&#20013;&#65292;RE&#21487;&#20197;&#20174;&#22235;&#20010;&#35201;&#27714;&#20013;&#25512;&#23548;&#20986;&#26469;&#65292;&#21363;&#20998;&#26512;&#24615;&#12289;&#23616;&#37096;&#24615;&#12289;&#36866;&#24403;&#24615;&#21644;&#26657;&#20934;&#24615;&#12290;&#32780;&#29992;&#20110;&#36890;&#20449;&#20013;&#27880;&#24847;&#21147;&#23548;&#21521;&#30340;&#21152;&#26435;RE&#23454;&#38469;&#19978;&#26159;&#19981;&#36866;&#24403;&#30340;&#12290;&#20026;&#20102;&#30475;&#21040;&#36866;&#24403;&#30340;&#27880;&#24847;&#21147;&#36890;&#20449;&#26159;&#22914;&#20309;&#20986;&#29616;&#30340;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#22330;&#26223;&#65292;&#21363;&#28040;&#24687;&#21457;&#36865;&#32773;&#24076;&#26395;&#30830;&#20445;&#25509;&#25910;&#32773;&#33021;&#22815;&#25191;&#34892;&#30693;&#24773;&#30340;&#25805;&#20316;&#12290;&#22914;&#26524;&#25509;&#25910;&#32773;&#20351;&#29992;MEP&#35299;&#30721;&#28040;&#24687;&#65292;&#21017;&#21457;&#36865;&#32773;&#21482;&#38656;&#35201;&#30693;&#36947;&#25509;&#25910;&#32773;&#30340;&#25928;&#29992;&#20989;&#25968;&#26469;&#36827;&#34892;&#26368;&#20339;&#36890;&#30693;&#65292;&#19981;&#38656;&#35201;&#30693;&#36947;&#25509;&#25910;&#32773;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The concept of attention, numerical weights that emphasize the importance of particular data, has proven to be very relevant in artificial intelligence. Relative entropy (RE, aka Kullback-Leibler divergence) plays a central role in communication theory. Here we combine these concepts, attention and RE. RE guides optimal encoding of messages in bandwidth-limited communication as well as optimal message decoding via the maximum entropy principle (MEP). In the coding scenario, RE can be derived from four requirements, namely being analytical, local, proper, and calibrated. Weighted RE, used for attention steering in communications, turns out to be improper. To see how proper attention communication can emerge, we analyze a scenario of a message sender who wants to ensure that the receiver of the message can perform well-informed actions. If the receiver decodes the message using the MEP, the sender only needs to know the receiver's utility function to inform optimally, but not the receive
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;fab-in-the-loop&#31639;&#27861;&#65292;&#29992;&#20110;&#20809;&#23376;&#22120;&#20214;&#35774;&#35745;&#65292;&#24182;&#25104;&#21151;&#23558;&#25554;&#20837;&#25439;&#32791;&#38477;&#20302;&#33267;3.24 dB&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;150&#32435;&#31859;&#24102;&#23485;&#19979;&#19981;&#21040;10.2 dB&#30340;&#25439;&#32791;&#12290;</title><link>http://arxiv.org/abs/2307.11075</link><description>&lt;p&gt;
&#20809;&#23376;&#22120;&#20214;&#35774;&#35745;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Photonic Component Design. (arXiv:2307.11075v2 [physics.optics] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;fab-in-the-loop&#31639;&#27861;&#65292;&#29992;&#20110;&#20809;&#23376;&#22120;&#20214;&#35774;&#35745;&#65292;&#24182;&#25104;&#21151;&#23558;&#25554;&#20837;&#25439;&#32791;&#38477;&#20302;&#33267;3.24 dB&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;150&#32435;&#31859;&#24102;&#23485;&#19979;&#19981;&#21040;10.2 dB&#30340;&#25439;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;fab-in-the-loop&#31639;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#32771;&#34385;&#21040;&#32435;&#31859;&#21046;&#36896;&#36807;&#31243;&#20013;&#30340;&#32570;&#38519;&#30340;&#32435;&#31859;&#20809;&#23376;&#22120;&#20214;&#12290;&#20316;&#20026;&#36825;&#31181;&#25216;&#26415;&#28508;&#21147;&#30340;&#23637;&#31034;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#22312;&#27668;&#20307;&#21253;&#35206;&#30340;220&#32435;&#31859;&#30789;&#23618;&#32477;&#32536;&#20307;&#21333;&#21051;&#34432;&#24179;&#21488;&#19978;&#21046;&#36896;&#30340;&#20809;&#23376;&#26230;&#20307;&#20809;&#26629;&#32806;&#21512;&#22120;&#30340;&#35774;&#35745;&#12290;&#36825;&#31181;fab-in-the-loop&#31639;&#27861;&#23558;&#25554;&#20837;&#25439;&#32791;&#20174;8.8 dB&#38477;&#20302;&#21040;3.24 dB&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;fab-in-the-loop&#31639;&#27861;&#65292;&#20135;&#29983;&#30340;&#26368;&#23485;&#24102;&#23485;&#35774;&#35745;&#21487;&#20197;&#22312;&#20854;&#26368;&#20302;&#28857;&#22788;&#20197;&#19981;&#21040;10.2 dB&#30340;&#25439;&#32791;&#35206;&#30422;150&#32435;&#31859;&#30340;&#24102;&#23485;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new fab-in-the-loop reinforcement learning algorithm for the design of nano-photonic components that accounts for the imperfections present in nanofabrication processes. As a demonstration of the potential of this technique, we apply it to the design of photonic crystal grating couplers fabricated on an air clad 220 nm silicon on insulator single etch platform. This fab-in-the-loop algorithm improves the insertion loss from 8.8 to 3.24 dB. The widest bandwidth designs produced using our fab-in-the-loop algorithm can cover a 150 nm bandwidth with less than 10.2 dB of loss at their lowest point.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#25512;&#23548;&#20102;&#22312;&#32447; Laplace &#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#30446;&#26631;&#23450;&#20301;&#20026;&#27169;&#24577;&#20462;&#27491;&#30340;&#21464;&#20998;&#19978;&#30028;&#65292;&#36991;&#20813;&#20102;&#23545;&#24179;&#31283;&#24615;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#20351;&#29992;&#20840;&#25209;&#37327;&#26799;&#24230;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22312;&#23454;&#36341;&#20013;&#23454;&#29616;&#20102;&#36825;&#20123;&#26368;&#20248;&#28857;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06093</link><description>&lt;p&gt;
&#22312;&#32447; Laplace &#27169;&#22411;&#36873;&#25321;&#30340;&#20877;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Online Laplace Model Selection Revisited. (arXiv:2307.06093v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#25512;&#23548;&#20102;&#22312;&#32447; Laplace &#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#30446;&#26631;&#23450;&#20301;&#20026;&#27169;&#24577;&#20462;&#27491;&#30340;&#21464;&#20998;&#19978;&#30028;&#65292;&#36991;&#20813;&#20102;&#23545;&#24179;&#31283;&#24615;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#20351;&#29992;&#20840;&#25209;&#37327;&#26799;&#24230;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22312;&#23454;&#36341;&#20013;&#23454;&#29616;&#20102;&#36825;&#20123;&#26368;&#20248;&#28857;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Laplace &#36817;&#20284;&#20026;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#23553;&#38381;&#24418;&#24335;&#30340;&#27169;&#22411;&#36873;&#25321;&#30446;&#26631;&#12290;&#22312;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#19982;&#36229;&#21442;&#25968;&#65288;&#22914;&#26435;&#37325;&#34928;&#20943;&#24378;&#24230;&#65289;&#19968;&#36215;&#36827;&#34892;&#20248;&#21270;&#30340;&#22312;&#32447;&#21464;&#20307;&#26041;&#27861;&#20877;&#27425;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36829;&#21453;&#20102; Laplace &#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#20551;&#35774;&#65292;&#21363;&#36817;&#20284;&#26159;&#22260;&#32469;&#25439;&#22833;&#30340;&#27169;&#24577;&#36827;&#34892;&#30340;&#65292;&#36825;&#23601;&#23545;&#23427;&#20204;&#30340;&#21512;&#29702;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#26412;&#30740;&#31350;&#37325;&#26032;&#25512;&#23548;&#20102;&#22312;&#32447; Laplace &#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#38024;&#23545; Laplace &#35777;&#25454;&#30340;&#19968;&#20010;&#20462;&#27491;&#27169;&#24577;&#30340;&#21464;&#20998;&#19978;&#30028;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#23545;&#24179;&#31283;&#24615;&#30340;&#20551;&#35774;&#12290;&#22312;&#32447; Laplace &#26041;&#27861;&#21450;&#20854;&#20462;&#27491;&#27169;&#24577;&#30340;&#23545;&#24212;&#28857;&#28385;&#36275;&#20004;&#20010;&#26465;&#20214;&#65306;1. &#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#26159;&#19968;&#20010;&#26368;&#22823;&#21518;&#39564;&#27010;&#29575;&#65292;&#28385;&#36275; Laplace &#26041;&#27861;&#30340;&#20551;&#35774;&#65307;2. &#36229;&#21442;&#25968;&#26368;&#22823;&#21270; Laplace &#35777;&#25454;&#65292;&#20174;&#32780;&#20419;&#20351;&#22312;&#32447;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20840;&#25209;&#37327;&#26799;&#24230;&#30340;&#22312;&#32447;&#31639;&#27861;&#28436;&#31034;&#20102;&#36825;&#20123;&#26368;&#20248;&#28857;&#22312;&#23454;&#36341;&#20013;&#30340;&#36817;&#20284;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Laplace approximation provides a closed-form model selection objective for neural networks (NN). Online variants, which optimise NN parameters jointly with hyperparameters, like weight decay strength, have seen renewed interest in the Bayesian deep learning community. However, these methods violate Laplace's method's critical assumption that the approximation is performed around a mode of the loss, calling into question their soundness. This work re-derives online Laplace methods, showing them to target a variational bound on a mode-corrected variant of the Laplace evidence which does not make stationarity assumptions. Online Laplace and its mode-corrected counterpart share stationary points where 1. the NN parameters are a maximum a posteriori, satisfying the Laplace method's assumption, and 2. the hyperparameters maximise the Laplace evidence, motivating online methods. We demonstrate that these optima are roughly attained in practise by online algorithms using full-batch gradien
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#22810;&#32593;&#26684;&#27714;&#35299;&#22120;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#31163;&#25955;&#24322;&#36136;Helmholtz&#26041;&#31243;&#30340;&#36845;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#21487;&#20280;&#32553;&#24615;&#21644;&#27714;&#35299;&#36895;&#24230;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#20854;&#20013;&#30340;&#19977;&#20010;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#24341;&#20837;&#38544;&#24335;&#23618;&#26469;&#35299;&#20915;CNN&#20013;&#30340;&#35270;&#37326;&#38382;&#39064;&#12289;&#25913;&#36827;CNN&#39044;&#26465;&#20214;&#25216;&#26415;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#35757;&#32451;&#26041;&#27861;&#20351;&#32593;&#32476;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#23610;&#23544;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17486</link><description>&lt;p&gt;
&#29992;&#20110;Helmholtz&#26041;&#31243;&#30340;&#22810;&#32593;&#26684;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65306;&#36890;&#36807;&#32039;&#33268;&#38544;&#24335;&#23618;&#25552;&#39640;&#21487;&#20280;&#32553;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multigrid-Augmented Deep Learning for the Helmholtz Equation: Better Scalability with Compact Implicit Layers. (arXiv:2306.17486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17486
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#22810;&#32593;&#26684;&#27714;&#35299;&#22120;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#31163;&#25955;&#24322;&#36136;Helmholtz&#26041;&#31243;&#30340;&#36845;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#21487;&#20280;&#32553;&#24615;&#21644;&#27714;&#35299;&#36895;&#24230;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#20854;&#20013;&#30340;&#19977;&#20010;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#24341;&#20837;&#38544;&#24335;&#23618;&#26469;&#35299;&#20915;CNN&#20013;&#30340;&#35270;&#37326;&#38382;&#39064;&#12289;&#25913;&#36827;CNN&#39044;&#26465;&#20214;&#25216;&#26415;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#35757;&#32451;&#26041;&#27861;&#20351;&#32593;&#32476;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#23610;&#23544;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36845;&#20195;&#26041;&#27861;&#26469;&#35299;&#20915;&#31163;&#25955;&#24322;&#36136;Helmholtz&#26041;&#31243;&#22312;&#39640;&#27874;&#25968;&#19979;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#32463;&#20856;&#30340;&#36845;&#20195;&#22810;&#32593;&#26684;&#27714;&#35299;&#22120;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#19982;&#39044;&#26465;&#20214;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#26356;&#24555;&#19988;&#21487;&#20280;&#32553;&#24615;&#26356;&#22909;&#30340;&#23398;&#20064;&#22411;&#31070;&#32463;&#27714;&#35299;&#22120;&#65292;&#30456;&#27604;&#26631;&#20934;&#30340;&#22810;&#32593;&#26684;&#27714;&#35299;&#22120;&#26356;&#20248;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20808;&#21069;&#36825;&#31867;&#31070;&#32463;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#23618;U-Net-like&#32534;&#30721;&#22120;-&#27714;&#35299;&#22120;CNN&#65292;&#20854;&#20013;&#22312;U-Net&#30340;&#26368;&#31895;&#31961;&#32593;&#26684;&#19978;&#21253;&#21547;&#19968;&#20010;&#38544;&#24335;&#23618;&#65292;&#21367;&#31215;&#26680;&#34987;&#21453;&#36716;&#12290;&#36825;&#31181;&#26041;&#27861;&#32531;&#35299;&#20102;CNN&#20013;&#30340;&#35270;&#37326;&#38382;&#39064;&#65292;&#24182;&#20801;&#35768;&#26356;&#22909;&#30340;&#21487;&#20280;&#32553;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22312;&#21442;&#25968;&#25968;&#37327;&#12289;&#35745;&#31639;&#26102;&#38388;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;CNN&#39044;&#26465;&#20214;&#22120;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#32593;&#32476;&#33021;&#22815;&#25193;&#23637;&#21040;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#23610;&#23544;&#38382;&#39064;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#21512;&#29702;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
We present a deep learning-based iterative approach to solve the discrete heterogeneous Helmholtz equation for high wavenumbers. Combining classical iterative multigrid solvers and convolutional neural networks (CNNs) via preconditioning, we obtain a learned neural solver that is faster and scales better than a standard multigrid solver. Our approach offers three main contributions over previous neural methods of this kind. First, we construct a multilevel U-Net-like encoder-solver CNN with an implicit layer on the coarsest grid of the U-Net, where convolution kernels are inverted. This alleviates the field of view problem in CNNs and allows better scalability. Second, we improve upon the previous CNN preconditioner in terms of the number of parameters, computation time, and convergence rates. Third, we propose a multiscale training approach that enables the network to scale to problems of previously unseen dimensions while still maintaining a reasonable training procedure. Our encoder
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#38543;&#26426;&#20877;&#29983;&#27169;&#22411;&#30340;&#39118;&#22122;&#22768;&#38477;&#22122;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#24471;&#39118;&#22122;&#22768;&#38477;&#22122;&#25928;&#26524;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#21644;&#32431;&#39044;&#27979;&#21644;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#20351;&#29992;&#27169;&#25311;&#21644;&#30495;&#23454;&#35760;&#24405;&#30340;&#39118;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#22312;&#30495;&#23454;&#35760;&#24405;&#30340;&#39118;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12867</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#38543;&#26426;&#20877;&#29983;&#27169;&#22411;&#30340;&#39118;&#22122;&#22768;&#38477;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Wind Noise Reduction with a Diffusion-based Stochastic Regeneration Model. (arXiv:2306.12867v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#38543;&#26426;&#20877;&#29983;&#27169;&#22411;&#30340;&#39118;&#22122;&#22768;&#38477;&#22122;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#24471;&#39118;&#22122;&#22768;&#38477;&#22122;&#25928;&#26524;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#21644;&#32431;&#39044;&#27979;&#21644;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#20351;&#29992;&#27169;&#25311;&#21644;&#30495;&#23454;&#35760;&#24405;&#30340;&#39118;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#22312;&#30495;&#23454;&#35760;&#24405;&#30340;&#39118;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21333;&#36890;&#36947;&#39118;&#22122;&#38899;&#38477;&#20302;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#25105;&#20204;&#20808;&#21069;&#25552;&#20986;&#30340;&#32467;&#21512;&#39044;&#27979;&#21644;&#29983;&#25104;&#24314;&#27169;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#38543;&#26426;&#20877;&#29983;&#27169;&#22411;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38750;&#21152;&#24615;&#30340;&#35821;&#38899;&#22122;&#22768;&#27169;&#22411;&#26469;&#35299;&#37322;&#33180;&#30340;&#38750;&#32447;&#24615;&#21464;&#24418;&#65292;&#36825;&#31181;&#21464;&#24418;&#26159;&#30001;&#39118;&#27969;&#21644;&#21487;&#33021;&#30340;&#21098;&#35009;&#24341;&#36215;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#38543;&#26426;&#20877;&#29983;&#27169;&#22411;&#22312;&#20351;&#29992;&#27169;&#25311;&#21644;&#30495;&#23454;&#35760;&#24405;&#30340;&#39118;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39118;&#22122;&#22768;&#38477;&#20302;&#26041;&#27861;&#20197;&#21450;&#32431;&#39044;&#27979;&#21644;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26410;&#35265;&#36807;&#30340;&#30495;&#23454;&#35760;&#24405;&#30340;&#39118;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#38899;&#39057;&#26679;&#26412;&#65292;&#25968;&#25454;&#29983;&#25104;&#33050;&#26412;&#21644;&#20195;&#30721;&#21487;&#20197;&#22312;&#32447;&#19978;&#25214;&#21040;(https://uhh.de/inf-sp-storm-wind)&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present a method for single-channel wind noise reduction using our previously proposed diffusion-based stochastic regeneration model combining predictive and generative modelling. We introduce a non-additive speech in noise model to account for the non-linear deformation of the membrane caused by the wind flow and possible clipping. We show that our stochastic regeneration model outperforms other neural-network-based wind noise reduction methods as well as purely predictive and generative models, on a dataset using simulated and real-recorded wind noise. We further show that the proposed method generalizes well by testing on an unseen dataset with real-recorded wind noise. Audio samples, data generation scripts and code for the proposed methods can be found online (https://uhh.de/inf-sp-storm-wind).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#37051;&#22495;&#28151;&#28102;&#24230;&#37327;&#26469;&#20998;&#31163;&#23398;&#20064;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#28151;&#28102;&#33410;&#28857;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#21487;&#38752;&#22320;&#21306;&#20998;&#24322;&#36136;&#33410;&#28857;&#21644;&#21516;&#36136;&#33410;&#28857;&#65292;&#24182;&#25913;&#21892;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02285</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#31163;&#23398;&#20064;&#35299;&#20915;&#28151;&#28102;&#33410;&#28857;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Clarify Confused Nodes Through Separated Learning. (arXiv:2306.02285v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#37051;&#22495;&#28151;&#28102;&#24230;&#37327;&#26469;&#20998;&#31163;&#23398;&#20064;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#28151;&#28102;&#33410;&#28857;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26356;&#21487;&#38752;&#22320;&#21306;&#20998;&#24322;&#36136;&#33410;&#28857;&#21644;&#21516;&#36136;&#33410;&#28857;&#65292;&#24182;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22270;&#23548;&#21521;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#20013;&#19981;&#21487;&#36991;&#20813;&#22320;&#21253;&#21547;&#19968;&#23450;&#27604;&#20363;&#30340;&#24322;&#36136;&#33410;&#28857;&#65292;&#36825;&#25361;&#25112;&#20102;&#32463;&#20856;GNN&#30340;&#21516;&#36136;&#24615;&#20551;&#35774;&#65292;&#24182;&#38459;&#30861;&#20102;&#20854;&#24615;&#33021;&#12290;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#25968;&#20173;&#35774;&#35745;&#20102;&#20855;&#26377;&#24322;&#36136;&#33410;&#28857;&#21644;&#21516;&#36136;&#33410;&#28857;&#38388;&#20849;&#20139;&#26435;&#37325;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;&#23613;&#31649;&#36825;&#20123;&#21162;&#21147;&#20013;&#21253;&#21547;&#20102;&#39640;&#38454;&#20449;&#24687;&#21644;&#22810;&#36890;&#36947;&#26550;&#26500;&#65292;&#20294;&#24448;&#24448;&#25928;&#26524;&#19981;&#20339;&#12290;&#23569;&#25968;&#30740;&#31350;&#23581;&#35797;&#35757;&#32451;&#19981;&#21516;&#33410;&#28857;&#32452;&#30340;&#20998;&#31163;&#23398;&#20064;&#65292;&#20294;&#21463;&#21040;&#20102;&#19981;&#21512;&#36866;&#30340;&#20998;&#31163;&#24230;&#37327;&#21644;&#20302;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#31216;&#20026;&#37051;&#22495;&#28151;&#28102;&#65288;NC&#65289;&#65292;&#20197;&#20415;&#26356;&#21487;&#38752;&#22320;&#20998;&#31163;&#33410;&#28857;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20855;&#26377;&#19981;&#21516;NC&#20540;&#30340;&#33410;&#28857;&#32452;&#22312;&#32452;&#20869;&#20934;&#30830;&#24230;&#21644;&#21487;&#35270;&#21270;&#23884;&#20837;&#19978;&#23384;&#22312;&#19968;&#23450;&#24046;&#24322;&#12290;&#36825;&#20026;&#22522;&#20110;&#37051;&#22495;&#28151;&#28102;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;NC-GCN&#65289;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have achieved remarkable advances in graph-oriented tasks. However, real-world graphs invariably contain a certain proportion of heterophilous nodes, challenging the homophily assumption of classical GNNs and hindering their performance. Most existing studies continue to design generic models with shared weights between heterophilous and homophilous nodes. Despite the incorporation of high-order messages or multi-channel architectures, these efforts often fall short. A minority of studies attempt to train different node groups separately but suffer from inappropriate separation metrics and low efficiency. In this paper, we first propose a new metric, termed Neighborhood Confusion (NC), to facilitate a more reliable separation of nodes. We observe that node groups with different levels of NC values exhibit certain differences in intra-group accuracy and visualized embeddings. These pave the way for Neighborhood Confusion-guided Graph Convolutional Network (N
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;UnionSNN&#65292;&#27880;&#20837;&#20102;&#37051;&#23621;&#36830;&#25509;&#20449;&#24687;&#65292;&#36890;&#36807;&#32852;&#21512;&#23376;&#22270;&#26469;&#32534;&#30721;&#39640;&#38454;&#36830;&#25509;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;1-WL&#21644;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;GNN&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.15747</link><description>&lt;p&gt;
Union Subgraph&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Union Subgraph Neural Networks. (arXiv:2305.15747v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;UnionSNN&#65292;&#27880;&#20837;&#20102;&#37051;&#23621;&#36830;&#25509;&#20449;&#24687;&#65292;&#36890;&#36807;&#32852;&#21512;&#23376;&#22270;&#26469;&#32534;&#30721;&#39640;&#38454;&#36830;&#25509;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;1-WL&#21644;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;GNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#34987;&#24191;&#27867;&#29992;&#20110;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#30001;&#20110;&#23427;&#20204;&#36890;&#36807;&#36845;&#20195;&#20256;&#36882;&#28040;&#24687;&#26469;&#22788;&#29702;&#26377;&#26681;&#23376;&#26641;&#65292;&#22240;&#27492;&#26222;&#36890;&#30340;GNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#19978;&#38480;&#20026;1&#32500;Weisfeiler-Leman(1-WL)&#27979;&#35797;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27880;&#20837;&#20174;&#26032;&#31867;&#22411;&#30340;&#23376;&#32467;&#26500;&#20013;&#25552;&#21462;&#30340;&#37051;&#23621;&#36830;&#25509;&#20449;&#24687;&#26469;&#22686;&#24378;GNNs&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#23616;&#37096;&#37051;&#22495;&#20013;&#23384;&#22312;&#30340;&#19981;&#21516;&#36830;&#25509;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20010;&#31216;&#20026;&#32852;&#21512;&#23376;&#22270;&#30340;&#23376;&#32467;&#26500;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#21040;&#19968;&#26465;&#36793;&#30340;1-&#36339;&#37051;&#23621;&#30340;&#23436;&#25972;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#30701;&#36335;&#24452;&#30340;&#23376;&#32467;&#26500;&#25551;&#36848;&#31526;&#65292;&#20855;&#26377;&#19977;&#20010;&#33391;&#22909;&#30340;&#24615;&#36136;&#65292;&#24182;&#19988;&#21487;&#20197;&#26377;&#25928;&#22320;&#32534;&#30721;&#32852;&#21512;&#23376;&#22270;&#20013;&#30340;&#39640;&#38454;&#36830;&#25509;&#24615;&#12290;&#36890;&#36807;&#27880;&#20837;&#32534;&#30721;&#37051;&#23621;&#36830;&#25509;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21363;Union Subgraph&#31070;&#32463;&#32593;&#32476;(UnionSNN)&#65292;&#23427;&#34987;&#35777;&#26126;&#22312;&#21306;&#20998;&#38750;&#21516;&#26500;&#22270;&#26041;&#38754;&#27604;1-WL&#20005;&#26684;&#26356;&#24378;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;UnionSNN&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GNN&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are widely used for graph representation learning in many application domains. The expressiveness of vanilla GNNs is upper-bounded by 1-dimensional Weisfeiler-Leman (1-WL) test as they operate on rooted subtrees through iterative message passing. In this paper, we empower GNNs by injecting neighbor-connectivity information extracted from a new type of substructure. We first investigate different kinds of connectivities existing in a local neighborhood and identify a substructure called union subgraph, which is able to capture the complete picture of the 1-hop neighborhood of an edge. We then design a shortest-path-based substructure descriptor that possesses three nice properties and can effectively encode the high-order connectivities in union subgraphs. By infusing the encoded neighbor connectivities, we propose a novel model, namely Union Subgraph Neural Network (UnionSNN), which is proven to be strictly more powerful than 1-WL in distinguishing non-isom
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25913;&#36827;&#32553;&#25918;&#23450;&#24459;&#26041;&#27861;&#25512;&#27979;&#20986;&#35745;&#31639;-&#20248;&#21270;&#27169;&#22411;&#24418;&#29366;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#24418;&#29366;&#20248;&#21270;&#35270;&#35273;&#21464;&#25442;&#22120;SoViT&#65292;&#35813;&#27169;&#22411;&#22312;&#30456;&#21516;&#35745;&#31639;&#37327;&#19979;&#65292;&#21462;&#24471;&#20102;&#19982;&#36229;&#36807;&#20854;&#20004;&#20493;&#22823;&#23567;&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13035</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#20351;ViT&#25104;&#24418;&#65306;&#35745;&#31639;-&#20248;&#21270;&#27169;&#22411;&#35774;&#35745;&#30340;&#32553;&#25918;&#23450;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;
Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design. (arXiv:2305.13035v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25913;&#36827;&#32553;&#25918;&#23450;&#24459;&#26041;&#27861;&#25512;&#27979;&#20986;&#35745;&#31639;-&#20248;&#21270;&#27169;&#22411;&#24418;&#29366;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#24418;&#29366;&#20248;&#21270;&#35270;&#35273;&#21464;&#25442;&#22120;SoViT&#65292;&#35813;&#27169;&#22411;&#22312;&#30456;&#21516;&#35745;&#31639;&#37327;&#19979;&#65292;&#21462;&#24471;&#20102;&#19982;&#36229;&#36807;&#20854;&#20004;&#20493;&#22823;&#23567;&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#32553;&#25918;&#23450;&#24459;&#34987;&#29992;&#26469;&#25512;&#23548;&#22312;&#32473;&#23450;&#35745;&#31639;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#35745;&#31639;-&#20248;&#21270;&#27169;&#22411;&#22823;&#23567;&#65288;&#21442;&#25968;&#25968;&#37327;&#65289;&#12290;&#25105;&#20204;&#21457;&#23637;&#24182;&#25913;&#36827;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#20197;&#25512;&#27979;&#22914;&#23485;&#24230;&#21644;&#28145;&#24230;&#31561;&#35745;&#31639;-&#20248;&#21270;&#27169;&#22411;&#24418;&#29366;&#65292;&#24182;&#22312;&#35270;&#35273;&#21464;&#25442;&#22120;&#20013;&#25104;&#21151;&#23454;&#29616;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#32463;&#36807;&#24418;&#29366;&#20248;&#21270;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;SoViT&#65292;&#22312;&#20165;&#20351;&#29992;&#30456;&#21516;&#25968;&#37327;&#30340;&#35745;&#31639;&#37327;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#21462;&#24471;&#20102;&#19982;&#36229;&#36807;&#20854;&#20004;&#20493;&#22823;&#23567;&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#32467;&#26524;&#12290;&#20363;&#22914;&#65292;SoViT-400m/14&#22312;ILSRCV2012&#19978;&#21462;&#24471;&#20102;90.3%&#30340;&#24494;&#35843;&#20934;&#30830;&#24230;&#65292;&#36229;&#36807;&#20102;&#26356;&#22823;&#30340;ViT-g/14&#65292;&#22312;&#30456;&#21516;&#35774;&#32622;&#19979;&#25509;&#36817;ViT-G/14&#65292;&#21516;&#26102;&#25512;&#26029;&#25104;&#26412;&#20063;&#19981;&#21040;&#19968;&#21322;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#20010;&#20219;&#21153;&#30340;&#24443;&#24213;&#35780;&#20272;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#23383;&#24149;&#12289;VQA&#21644;&#38646;-shot&#36716;&#31227;&#65292;&#22312;&#24191;&#27867;&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#24182;&#30830;&#23450;&#20102;&#20854;&#38480;&#21046;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#25361;&#25112;&#20102;&#30450;&#30446;&#25193;&#22823;&#35270;&#35273;&#27169;&#22411;&#30340;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling laws have been recently employed to derive compute-optimal model size (number of parameters) for a given compute duration. We advance and refine such methods to infer compute-optimal model shapes, such as width and depth, and successfully implement this in vision transformers. Our shape-optimized vision transformer, SoViT, achieves results competitive with models that exceed twice its size, despite being pre-trained with an equivalent amount of compute. For example, SoViT-400m/14 achieves 90.3% fine-tuning accuracy on ILSRCV2012, surpassing the much larger ViT-g/14 and approaching ViT-G/14 under identical settings, with also less than half the inference cost. We conduct a thorough evaluation across multiple tasks, such as image classification, captioning, VQA and zero-shot transfer, demonstrating the effectiveness of our model across a broad range of domains and identifying limitations. Overall, our findings challenge the prevailing approach of blindly scaling up vision models 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#26469;&#20248;&#21270;&#35757;&#32451;&#65292;&#20801;&#35768;&#28789;&#27963;&#30340;&#35774;&#35745;&#36873;&#25321;&#20197;&#25913;&#21892;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#20110;&#22495;&#22806;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#20445;&#25345;&#12290;</title><link>http://arxiv.org/abs/2305.10406</link><description>&lt;p&gt;
&#21464;&#20998;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Variational Classification. (arXiv:2305.10406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10406
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#26469;&#20248;&#21270;&#35757;&#32451;&#65292;&#20801;&#35768;&#28789;&#27963;&#30340;&#35774;&#35745;&#36873;&#25321;&#20197;&#25913;&#21892;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#20110;&#22495;&#22806;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#20445;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#26032;&#22411;&#25193;&#23637;&#65292;&#31216;&#20026;&#21464;&#20998;&#20998;&#31867; (VC)&#12290;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#65292;&#31867;&#20284;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#20256;&#32479;&#33258;&#32534;&#30721;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#22522;&#20110;&#35777;&#25454;&#19979;&#30028; (ELBO) &#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#37319;&#29992;&#23545;&#25239;&#24615;&#26041;&#27861;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;VC&#27169;&#22411;&#20801;&#35768;&#22312;&#35774;&#35745;&#36873;&#25321;&#26041;&#38754;&#26356;&#21152;&#28789;&#27963;&#65292;&#29305;&#21035;&#26159;&#31867;&#26465;&#20214;&#28508;&#20808;&#39564;&#65292;&#32780;&#19981;&#26159;&#22312;&#29616;&#25104;&#30340;softmax&#20998;&#31867;&#22120;&#20013;&#20570;&#20986;&#30340;&#38544;&#24335;&#20551;&#35774;&#12290;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#25913;&#21892;&#20102;&#20854;&#20182;&#33391;&#22909;&#29305;&#24615;&#65292;&#22914;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21363;&#20351;&#24212;&#29992;&#20110;&#22495;&#22806;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel extension of the traditional neural network approach to classification tasks, referred to as variational classification (VC). By incorporating latent variable modeling, akin to the relationship between variational autoencoders and traditional autoencoders, we derive a training objective based on the evidence lower bound (ELBO), optimized using an adversarial approach. Our VC model allows for more flexibility in design choices, in particular class-conditional latent priors, in place of the implicit assumptions made in off-the-shelf softmax classifiers. Empirical evaluation on image and text classification datasets demonstrates the effectiveness of our approach in terms of maintaining prediction accuracy while improving other desirable properties such as calibration and adversarial robustness, even when applied to out-of-domain data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;FedNC&#65292;&#19968;&#20010;&#32852;&#21512;&#23398;&#20064;&#30340;&#36890;&#20449;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#32593;&#32476;&#32534;&#30721;&#25216;&#26415;&#65292;&#33021;&#22815;&#25552;&#39640;&#31995;&#32479;&#30340;&#38544;&#31169;&#12289;&#21534;&#21520;&#37327;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03292</link><description>&lt;p&gt;
FedNC&#65306;&#22522;&#20110;&#32593;&#32476;&#32534;&#30721;&#21551;&#21457;&#30340;&#23433;&#20840;&#39640;&#25928;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedNC: A Secure and Efficient Federated Learning Method Inspired by Network Coding. (arXiv:2305.03292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;FedNC&#65292;&#19968;&#20010;&#32852;&#21512;&#23398;&#20064;&#30340;&#36890;&#20449;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#32593;&#32476;&#32534;&#30721;&#25216;&#26415;&#65292;&#33021;&#22815;&#25552;&#39640;&#31995;&#32479;&#30340;&#38544;&#31169;&#12289;&#21534;&#21520;&#37327;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26426;&#21046;&#65292;&#20294;&#20173;&#28982;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#21363;&#38544;&#31169;&#27844;&#28431;&#21644;&#31995;&#32479;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#32593;&#32476;&#20449;&#24687;&#29702;&#35770;&#30340;&#35282;&#24230;&#37325;&#26032;&#26500;&#24605;&#20102;&#32852;&#21512;&#23398;&#20064;&#31995;&#32479;&#65292;&#24182;&#21046;&#23450;&#20102;&#19968;&#20010;&#21407;&#21019;&#30340;&#32852;&#21512;&#23398;&#20064;&#36890;&#20449;&#26694;&#26550;FedNC&#65292;&#35813;&#26694;&#26550;&#21463;&#21040;&#32593;&#32476;&#32534;&#30721;&#30340;&#21551;&#21457;&#12290; FedNC&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#23545;&#21407;&#22987;&#25968;&#25454;&#21253;&#36827;&#34892;&#38543;&#26426;&#32447;&#24615;&#32452;&#21512;&#65292;&#23558;&#26412;&#22320;&#27169;&#22411;&#30340;&#20449;&#24687;&#28151;&#21512;&#22312;&#19968;&#36215;&#65292;&#28982;&#21518;&#20877;&#19978;&#20256;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#32858;&#21512;&#65292;&#20174;&#32780;&#20351;FL&#31995;&#32479;&#26356;&#21152;&#23433;&#20840;&#65292;&#21534;&#21520;&#37327;&#26356;&#39640;&#65292;&#40065;&#26834;&#24615;&#26356;&#22909;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;NC&#24341;&#20837;FL&#30340;&#26694;&#26550;&#12290;&#38543;&#30528;FL&#22312;&#23454;&#38469;&#32593;&#32476;&#26694;&#26550;&#20013;&#30340;&#19981;&#26029;&#28436;&#21464;&#65292;&#21487;&#20197;&#22522;&#20110;FedNC&#36827;&#19968;&#27493;&#35774;&#35745;&#26356;&#22810;&#30340;&#24212;&#29992;&#21644;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a promising distributed learning mechanism which still faces two major challenges, namely privacy breaches and system efficiency. In this work, we reconceptualize the FL system from the perspective of network information theory, and formulate an original FL communication framework, FedNC, which is inspired by Network Coding (NC). The main idea of FedNC is mixing the information of the local models by making random linear combinations of the original packets, before uploading for further aggregation. Due to the benefits of the coding scheme, both theoretical and experimental analysis indicate that FedNC improves the performance of traditional FL in several important ways, including security, throughput, and robustness. To the best of our knowledge, this is the first framework where NC is introduced in FL. As FL continues to evolve within practical network frameworks, more applications and variants can be further designed based on FedNC.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20174;&#26631;&#35760;&#25968;&#25454;&#20016;&#23500;&#30340;&#35774;&#22791;&#36716;&#31227;&#21040;&#26410;&#26631;&#35760;&#25968;&#25454;&#35774;&#22791;&#20197;&#25552;&#39640;&#25968;&#25454;&#21033;&#29992;&#29575;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#35774;&#22791;&#20998;&#31867;&#21644;&#28304;-&#30446;&#26631;&#38142;&#25509;&#24418;&#25104;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.12422</link><description>&lt;p&gt;
&#22810;&#28304;&#21040;&#22810;&#30446;&#26631;&#30340;&#20998;&#24067;&#24335;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Multi-Source to Multi-Target Decentralized Federated Domain Adaptation. (arXiv:2304.12422v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20174;&#26631;&#35760;&#25968;&#25454;&#20016;&#23500;&#30340;&#35774;&#22791;&#36716;&#31227;&#21040;&#26410;&#26631;&#35760;&#25968;&#25454;&#35774;&#22791;&#20197;&#25552;&#39640;&#25968;&#25454;&#21033;&#29992;&#29575;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#35774;&#22791;&#20998;&#31867;&#21644;&#28304;-&#30446;&#26631;&#38142;&#25509;&#24418;&#25104;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#35774;&#22791;&#38388;&#30340;&#24322;&#36136;&#24615;&#36890;&#24120;&#25351;&#32479;&#35745;&#65288;&#20363;&#22914;&#65292;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#20998;&#24067;&#65289;&#21644;&#36164;&#28304;&#65288;&#20363;&#22914;&#65292;&#36890;&#20449;&#24102;&#23485;&#65289;&#32500;&#24230;&#12290;&#26412;&#25991;&#32858;&#28966;&#21478;&#19968;&#20010;&#37325;&#35201;&#32500;&#24230;&#65306;&#21508;&#35774;&#22791;&#25152;&#25317;&#26377;&#30340;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#25968;&#37327;/&#20998;&#24067;&#12290;&#20026;&#20102;&#21033;&#29992;&#25152;&#26377;&#25968;&#25454;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#32852;&#37030;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20174;&#26631;&#35760;&#25968;&#25454;&#39640;&#36136;&#37327;&#35774;&#22791;&#65288;&#31216;&#20026;&#28304;&#65289;&#36716;&#31227;&#21040;&#20302;&#36136;&#37327;&#25110;&#26410;&#26631;&#35760;&#25968;&#25454;&#35774;&#22791;&#65288;&#31216;&#20026;&#30446;&#26631;&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#8220;&#28304;-&#30446;&#26631;&#30830;&#23450;&#21644;&#38142;&#25509;&#24418;&#25104;&#8221;&#65288;ST-LF&#65289;&#65292;&#22312;&#32771;&#34385;&#27169;&#22411;&#31934;&#24230;&#21644;&#36890;&#20449;&#33021;&#37327;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#21516;&#26102;&#65292;&#20248;&#21270;&#35774;&#22791;&#20998;&#31867;&#21644;&#28304;-&#30446;&#26631;&#38142;&#25509;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneity across devices in federated learning (FL) typically refers to statistical (e.g., non-i.i.d. data distributions) and resource (e.g., communication bandwidth) dimensions. In this paper, we focus on another important dimension that has received less attention: varying quantities/distributions of labeled and unlabeled data across devices. In order to leverage all data, we develop a decentralized federated domain adaptation methodology which considers the transfer of ML models from devices with high quality labeled data (called sources) to devices with low quality or unlabeled data (called targets). Our methodology, Source-Target Determination and Link Formation (ST-LF), optimizes both (i) classification of devices into sources and targets and (ii) source-target link formation, in a manner that considers the trade-off between ML model accuracy and communication energy efficiency. To obtain a concrete objective function, we derive a measurable generalization error bound that ac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23457;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#20840;&#38754;&#30340;&#26041;&#24335;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#21644;AI&#27169;&#22411;&#30340;&#20855;&#20307;&#25928;&#26524;&#65292;&#21253;&#25324;&#20559;&#35265;&#21644;&#27495;&#35270;&#39044;&#38450;&#12289;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#24544;&#23454;&#31243;&#24230;&#12289;&#25928;&#29992;&#12289;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;&#22810;&#20010;&#29992;&#20363;&#20013;&#65292;&#23457;&#35745;&#26694;&#26550;&#24179;&#34913;&#20102;&#20449;&#20219;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.10819</link><description>&lt;p&gt;
&#21487;&#25511;&#30340;&#20449;&#20219;&#26435;&#34913;&#19979;&#30340;&#21512;&#25104;&#25968;&#25454;&#23457;&#35745;&#19982;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Auditing and Generating Synthetic Data with Controllable Trust Trade-offs. (arXiv:2304.10819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23457;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#20840;&#38754;&#30340;&#26041;&#24335;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#21644;AI&#27169;&#22411;&#30340;&#20855;&#20307;&#25928;&#26524;&#65292;&#21253;&#25324;&#20559;&#35265;&#21644;&#27495;&#35270;&#39044;&#38450;&#12289;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#24544;&#23454;&#31243;&#24230;&#12289;&#25928;&#29992;&#12289;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;&#22810;&#20010;&#29992;&#20363;&#20013;&#65292;&#23457;&#35745;&#26694;&#26550;&#24179;&#34913;&#20102;&#20449;&#20219;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#24448;&#24448;&#23384;&#22312;&#20559;&#24046;&#12289;&#19981;&#24179;&#34913;&#65292;&#24182;&#19988;&#26377;&#27844;&#38706;&#25935;&#24863;&#21644;&#38544;&#31169;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#36825;&#19968;&#20107;&#23454;&#24341;&#21457;&#20102;&#21019;&#24314;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#24819;&#27861;&#65292;&#20197;&#20943;&#36731;&#30495;&#23454;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#39118;&#38505;&#12289;&#20559;&#35265;&#12289;&#20260;&#23475;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#36825;&#20010;&#27010;&#24565;&#20381;&#36182;&#20110;&#29983;&#25104;AI&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#19981;&#20559;&#25191;&#12289;&#20445;&#25252;&#38544;&#31169;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#21516;&#26102;&#24544;&#23454;&#20110;&#30495;&#23454;&#25968;&#25454;&#12290;&#22312;&#36825;&#31181;&#26032;&#33539;&#24335;&#20013;&#65292;&#25105;&#20204;&#22914;&#20309;&#30693;&#36947;&#36825;&#31181;&#26041;&#27861;&#26159;&#21542;&#20817;&#29616;&#20102;&#20854;&#25215;&#35834;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23457;&#35745;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#23427;&#20204;&#35757;&#32451;&#30340;AI&#27169;&#22411;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#22260;&#32469;&#20559;&#35265;&#21644;&#27495;&#35270;&#30340;&#39044;&#38450;&#12289;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#24544;&#23454;&#31243;&#24230;&#12289;&#25928;&#29992;&#12289;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#25105;&#20204;&#36890;&#36807;&#23457;&#35745;&#22810;&#20010;&#29983;&#25104;&#27169;&#22411;&#22312;&#19981;&#21516;&#29992;&#20363;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#25945;&#32946;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#38134;&#34892;&#12289;&#20154;&#21147;&#36164;&#28304;&#65292;&#20197;&#21450;&#20174;&#34920;&#26684;&#65292;&#26102;&#38388;&#24207;&#21015;&#21040;&#33258;&#28982;&#35821;&#35328;&#30340;&#19981;&#21516;&#27169;&#24577;&#12290;&#25105;&#20204;&#30340;&#29992;&#20363;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#20013;&#24179;&#34913;&#20449;&#20219;&#21644;&#25928;&#29992;&#30340;&#26435;&#34913;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data collected from the real world tends to be biased, unbalanced, and at risk of exposing sensitive and private information. This reality has given rise to the idea of creating synthetic datasets to alleviate risk, bias, harm, and privacy concerns inherent in the real data. This concept relies on Generative AI models to produce unbiased, privacy-preserving synthetic data while being true to the real data. In this new paradigm, how can we tell if this approach delivers on its promises? We present an auditing framework that offers a holistic assessment of synthetic datasets and AI models trained on them, centered around bias and discrimination prevention, fidelity to the real data, utility, robustness, and privacy preservation. We showcase our framework by auditing multiple generative models on diverse use cases, including education, healthcare, banking, human resources, and across different modalities, from tabular, to time-series, to natural language. Our use cases demonstrate the imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Shallow ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#23481;&#37327;&#65292;&#35777;&#26126;&#20102;&#20854;&#26368;&#20248;&#36924;&#36817;&#36895;&#29575;&#12290;&#24212;&#29992;&#21040;&#38750;&#21442;&#25968;&#22238;&#24402;&#19978;&#65292;&#35777;&#26126;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#23398;&#20064;H\"older&#20989;&#25968;&#30340;&#26368;&#20248;&#28176;&#36827;&#36895;&#29575;&#65292;&#34917;&#20805;&#20102;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#36817;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.01561</link><description>&lt;p&gt;
Shallow ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#36895;&#29575;&#21450;&#20854;&#22312;&#38750;&#21442;&#25968;&#22238;&#24402;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Optimal rates of approximation by shallow ReLU$^k$ neural networks and applications to nonparametric regression. (arXiv:2304.01561v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Shallow ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#23481;&#37327;&#65292;&#35777;&#26126;&#20102;&#20854;&#26368;&#20248;&#36924;&#36817;&#36895;&#29575;&#12290;&#24212;&#29992;&#21040;&#38750;&#21442;&#25968;&#22238;&#24402;&#19978;&#65292;&#35777;&#26126;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#23398;&#20064;H\"older&#20989;&#25968;&#30340;&#26368;&#20248;&#28176;&#36827;&#36895;&#29575;&#65292;&#34917;&#20805;&#20102;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#36817;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#19982;Shallow ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#30456;&#20851;&#30340;&#21464;&#24322;&#31354;&#38388;&#30340;&#36924;&#36817;&#23481;&#37327;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26377;&#38480;&#21464;&#24322;&#33539;&#25968;&#19979;&#65292;&#23481;&#32435;&#20102;&#36275;&#22815;&#24179;&#28369;&#30340;&#20989;&#25968;&#12290;&#23545;&#20110;&#36739;&#23569;&#24179;&#28369;&#30340;&#20989;&#25968;&#65292;&#26681;&#25454;&#21464;&#24322;&#33539;&#25968;&#24314;&#31435;&#20102;&#36924;&#36817;&#36895;&#29575;&#12290;&#36816;&#29992;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;Shallow ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#36924;&#36817;&#36895;&#29575;&#12290;&#21516;&#26102;&#38416;&#26126;&#20102;&#36825;&#20123;&#32467;&#26524;&#22914;&#20309;&#29992;&#20110;&#25512;&#23548;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#30340;&#36924;&#36817;&#30028;&#38480;&#12290;&#20026;&#24212;&#29992;&#30740;&#31350;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19977;&#31181;ReLU&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65306;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#36229;&#21442;&#25968;&#31070;&#32463;&#32593;&#32476;&#21644;CNN&#36827;&#34892;&#38750;&#21442;&#25968;&#22238;&#24402;&#25910;&#25947;&#36895;&#29575;&#30740;&#31350;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#23398;&#20064;H\"older&#20989;&#25968;&#30340;&#26368;&#20248;&#28176;&#36827;&#36895;&#29575;&#65292;&#36825;&#34917;&#20805;&#20102;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#36817;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms. For functions with less smoothness, the approximation rates in terms of the variation norm are established. Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks. It is also shown how these results can be used to derive approximation bounds for deep neural networks and convolutional neural networks (CNNs). As applications, we study convergence rates for nonparametric regression using three ReLU neural network models: shallow neural network, over-parameterized neural network, and CNN. In particular, we show that shallow neural networks can achieve the minimax optimal rates for learning H\"older functions, which complements recent results for deep neural networks. It is also proven th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39044;&#27979;&#35823;&#24046;&#21442;&#25968;&#21270;&#20026;&#22810;&#20010;&#26102;&#38388;&#27493;&#38271;&#65292;&#20197;&#25214;&#21040;&#19981;&#20445;&#23432;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#23454;&#29616;&#22312;&#20351;&#29992;&#23398;&#20064;&#21551;&#29992;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;&#36827;&#34892;&#38271;&#26399;&#35268;&#21010;&#21644;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.01075</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#24615;&#20114;&#34917;&#32534;&#31243;&#30340;&#26102;&#24207;&#31526;&#21512;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction Regions for Time Series using Linear Complementarity Programming. (arXiv:2304.01075v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39044;&#27979;&#35823;&#24046;&#21442;&#25968;&#21270;&#20026;&#22810;&#20010;&#26102;&#38388;&#27493;&#38271;&#65292;&#20197;&#25214;&#21040;&#19981;&#20445;&#23432;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#23454;&#29616;&#22312;&#20351;&#29992;&#23398;&#20064;&#21551;&#29992;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;&#36827;&#34892;&#38271;&#26399;&#35268;&#21010;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21512;&#39044;&#27979;&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#21306;&#38388;&#30340;&#32479;&#35745;&#24037;&#20855;&#65292;&#20854;&#20855;&#26377;&#39640;&#27010;&#29575;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23558;&#31526;&#21512;&#39044;&#27979;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20250;&#23548;&#33268;&#20445;&#23432;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36825;&#31181;&#20445;&#23432;&#24615;&#65292;&#20197;&#20415;&#22312;&#20351;&#29992;&#23398;&#20064;&#21551;&#29992;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;&#36827;&#34892;&#38271;&#26399;&#35268;&#21010;&#21644;&#39564;&#35777;&#12290;&#25105;&#20204;&#23558;&#39044;&#27979;&#35823;&#24046;&#21442;&#25968;&#21270;&#20026;&#22810;&#20010;&#26102;&#38388;&#27493;&#38271;&#65292;&#36890;&#36807;&#23545;&#39069;&#22806;&#25968;&#25454;&#38598;&#19978;&#30340;&#21442;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#25214;&#21040;&#20102;&#19981;&#20445;&#23432;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#35813;&#38382;&#39064;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#20114;&#34917;&#35268;&#21010;&#65288;MILCP&#65289;&#65292;&#25105;&#20204;&#23558;&#20854;&#25918;&#23485;&#20026;&#19968;&#20010;&#32447;&#24615;&#20114;&#34917;&#35268;&#21010;&#65288;LCP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction is a statistical tool for producing prediction regions of machine learning models that are valid with high probability. However, applying conformal prediction to time series data leads to conservative prediction regions. In fact, to obtain prediction regions over $T$ time steps with confidence $1-\delta$, {previous works require that each individual prediction region is valid} with confidence $1-\delta/T$. We propose an optimization-based method for reducing this conservatism to enable long horizon planning and verification when using learning-enabled time series predictors. Instead of considering prediction errors individually at each time step, we consider a parameterized prediction error over multiple time steps. By optimizing the parameters over an additional dataset, we find prediction regions that are not conservative. We show that this problem can be cast as a mixed integer linear complementarity program (MILCP), which we then relax into a linear complementa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32422;&#26463;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#21046;&#35299;&#20915;&#20102;&#38271;&#26399;&#21644;&#20016;&#23500;&#32422;&#26463;&#30340;&#20219;&#21153;&#65292;&#22312;&#26426;&#22120;&#20154;&#28165;&#27905;&#25151;&#23627;&#30340;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.10639</link><description>&lt;p&gt;
&#36890;&#36807;&#32422;&#26463;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#22788;&#29702;&#38271;&#26399;&#21644;&#20016;&#23500;&#32422;&#26463;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Handling Long and Richly Constrained Tasks through Constrained Hierarchical Reinforcement Learning. (arXiv:2302.10639v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32422;&#26463;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#21046;&#35299;&#20915;&#20102;&#38271;&#26399;&#21644;&#20016;&#23500;&#32422;&#26463;&#30340;&#20219;&#21153;&#65292;&#22312;&#26426;&#22120;&#20154;&#28165;&#27905;&#25151;&#23627;&#30340;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#22312;&#30446;&#26631;&#23548;&#21521;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35774;&#32622;&#20013;&#65292;&#36890;&#24120;&#36890;&#36807;&#23545;&#36712;&#36857;&#26045;&#21152;&#32422;&#26463;&#26469;&#22788;&#29702;&#23433;&#20840;&#38382;&#39064;&#65292;&#23545;&#20110;&#30701;&#26399;&#20219;&#21153;&#34920;&#29616;&#33391;&#22909;&#12290;&#26412;&#25991;&#29305;&#21035;&#20851;&#27880;&#35299;&#20915;&#26102;&#38388;&#19978;&#24310;&#32493;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#22312;&#28165;&#27905;&#25151;&#23627;&#30340;&#19981;&#21516;&#21306;&#22495;&#26102;&#65292;&#38656;&#35201;&#36991;&#24320;&#28287;&#28369;&#21644;&#19981;&#23433;&#20840;&#30340;&#21306;&#22495;&#65288;&#20363;&#22914;&#27004;&#26799;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#36275;&#22815;&#30340;&#30005;&#37327;&#31227;&#21160;&#21040;&#20805;&#30005;&#31449;&#65307;&#32780;&#19988;&#38754;&#20020;&#22797;&#26434;&#30340;&#23433;&#20840;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#23558;&#19978;&#23618;&#30340;&#32422;&#26463;&#25628;&#32034;&#20195;&#29702;&#65288;&#20174;&#32473;&#23450;&#30340;&#36215;&#22987;&#29366;&#24577;&#21040;&#36828;&#22788;&#30446;&#26631;&#29366;&#24577;&#35745;&#31639;&#26368;&#22823;&#21270;&#22238;&#25253;&#31574;&#30053;&#65292;&#21516;&#26102;&#28385;&#36275;&#25104;&#26412;&#32422;&#26463;&#65289;&#19982;&#24213;&#23618;&#30340;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65288;&#20272;&#35745;&#22312;&#38468;&#36817;&#29366;&#24577;&#20043;&#38388;&#31227;&#21160;&#30340;&#25104;&#26412;&#21644;&#22238;&#25253;&#20540;&#65289;&#32467;&#21512;&#20351;&#29992;&#30340;&#23433;&#20840;&#32422;&#26463;&#25628;&#32034;&#19982;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#65288;CoSHRL&#65289;&#26426;&#21046;&#12290;CoSHRL&#30340;&#19968;&#20010;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#23427;&#21487;&#20197;&#22788;&#29702;&#23545;&#36712;&#36857;&#19978;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety in goal directed Reinforcement Learning (RL) settings has typically been handled through constraints over trajectories and have demonstrated good performance in primarily short horizon tasks. In this paper, we are specifically interested in the problem of solving temporally extended decision making problems such as robots cleaning different areas in a house while avoiding slippery and unsafe areas (e.g., stairs) and retaining enough charge to move to a charging dock; in the presence of complex safety constraints. Our key contribution is a (safety) Constrained Search with Hierarchical Reinforcement Learning (CoSHRL) mechanism that combines an upper level constrained search agent (which computes a reward maximizing policy from a given start to a far away goal state while satisfying cost constraints) with a low-level goal conditioned RL agent (which estimates cost and reward values to move between nearby states). A major advantage of CoSHRL is that it can handle constraints on the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#35889;&#26041;&#27861;&#21644;&#35856;&#25391;&#23376;&#30340;&#38750;&#21487;&#20998;&#31163;&#21327;&#26041;&#24046;&#26680;&#30340;&#26102;&#31354;&#39640;&#26031;&#36807;&#31243;&#30740;&#31350;&#65292;&#36890;&#36807;&#29289;&#29702;&#35770;&#35777;&#25512;&#23548;&#20986;&#19968;&#31867;&#26032;&#22411;&#30340;&#38750;&#21487;&#20998;&#31163;&#21327;&#26041;&#24046;&#26680;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35266;&#27979;&#21040;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.09580</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#35889;&#26041;&#27861;&#21644;&#35856;&#25391;&#23376;&#30340;&#38750;&#21487;&#20998;&#31163;&#21327;&#26041;&#24046;&#26680;&#30340;&#26102;&#31354;&#39640;&#26031;&#36807;&#31243;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Non-separable Covariance Kernels for Spatiotemporal Gaussian Processes based on a Hybrid Spectral Method and the Harmonic Oscillator. (arXiv:2302.09580v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09580
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#35889;&#26041;&#27861;&#21644;&#35856;&#25391;&#23376;&#30340;&#38750;&#21487;&#20998;&#31163;&#21327;&#26041;&#24046;&#26680;&#30340;&#26102;&#31354;&#39640;&#26031;&#36807;&#31243;&#30740;&#31350;&#65292;&#36890;&#36807;&#29289;&#29702;&#35770;&#35777;&#25512;&#23548;&#20986;&#19968;&#31867;&#26032;&#22411;&#30340;&#38750;&#21487;&#20998;&#31163;&#21327;&#26041;&#24046;&#26680;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35266;&#27979;&#21040;&#30340;&#26102;&#31354;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#25552;&#20379;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#38750;&#21442;&#25968;&#26694;&#26550;&#65292;&#29992;&#20110;&#36817;&#20284;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#20989;&#25968;&#12290;&#21327;&#26041;&#24046;&#26680;&#26159;&#39640;&#26031;&#36807;&#31243;&#30340;&#20027;&#35201;&#24341;&#25806;&#65292;&#21253;&#21547;&#20102;&#39044;&#27979;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#12290;&#23545;&#20110;&#20855;&#26377;&#26102;&#31354;&#25968;&#25454;&#38598;&#30340;&#24212;&#29992;&#65292;&#21512;&#36866;&#30340;&#26680;&#24212;&#35813;&#24314;&#27169;&#32852;&#21512;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#12290;&#21487;&#20998;&#31163;&#30340;&#26102;&#31354;&#21327;&#26041;&#24046;&#26680;&#25552;&#20379;&#20102;&#31616;&#21333;&#21644;&#35745;&#31639;&#25928;&#29575;&#36739;&#39640;&#30340;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#38750;&#21487;&#20998;&#31163;&#26680;&#21253;&#21547;&#20102;&#26356;&#22909;&#22320;&#25429;&#25417;&#35266;&#27979;&#21040;&#30340;&#30456;&#20851;&#24615;&#30340;&#26102;&#31354;&#20132;&#20114;&#20316;&#29992;&#12290;&#22823;&#22810;&#25968;&#20855;&#26377;&#26174;&#24335;&#34920;&#36798;&#24335;&#30340;&#38750;&#21487;&#20998;&#31163;&#26680;&#26159;&#22522;&#20110;&#25968;&#23398;&#32771;&#34385;&#65288;&#21487;&#20801;&#35768;&#26465;&#20214;&#65289;&#32780;&#38750;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#23548;&#20986;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#35770;&#35777;&#30340;&#28151;&#21512;&#35889;&#26041;&#27861;&#26469;&#29983;&#25104;&#21327;&#26041;&#24046;&#26680;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#25512;&#23548;&#20102;&#19968;&#31867;&#26032;&#22411;&#30340;&#29289;&#29702;&#21160;&#26426;&#30340;&#38750;&#21487;&#20998;&#31163;&#21327;&#26041;&#24046;&#26680;&#65292;&#23427;&#20204;&#30340;&#26681;&#28304;&#26469;&#33258;&#38543;&#26426;&#32447;&#24615;...
&lt;/p&gt;
&lt;p&gt;
Gaussian processes provide a flexible, non-parametric framework for the approximation of functions in high-dimensional spaces. The covariance kernel is the main engine of Gaussian processes, incorporating correlations that underpin the predictive distribution. For applications with spatiotemporal datasets, suitable kernels should model joint spatial and temporal dependence. Separable space-time covariance kernels offer simplicity and computational efficiency. However, non-separable kernels include space-time interactions that better capture observed correlations. Most non-separable kernels that admit explicit expressions are based on mathematical considerations (admissibility conditions) rather than first-principles derivations. We present a hybrid spectral approach for generating covariance kernels which is based on physical arguments. We use this approach to derive a new class of physically motivated, non-separable covariance kernels which have their roots in the stochastic, linear, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26377;&#32422;&#26463;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30701;&#35270;&#39057;&#25512;&#33616;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#30701;&#35270;&#39057;&#25512;&#33616;&#38382;&#39064;&#24314;&#27169;&#20026;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#29992;&#25143;&#20132;&#20114;&#21644;&#22810;&#26679;&#30340;&#21709;&#24212;&#20013;&#20248;&#21270;&#32047;&#35745;&#35266;&#30475;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;&#36827;&#34892;&#20102;&#20004;&#38454;&#27573;&#30340;&#31574;&#30053;&#23398;&#20064;&#65292;&#24182;&#19988;&#33021;&#22815;&#21516;&#26102;&#28385;&#36275;&#20027;&#35201;&#30446;&#26631;&#21644;&#36741;&#21161;&#30446;&#26631;&#30340;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2302.01680</link><description>&lt;p&gt;
&#20004;&#38454;&#27573;&#26377;&#32422;&#26463;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#29992;&#20110;&#30701;&#35270;&#39057;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Two-Stage Constrained Actor-Critic for Short Video Recommendation. (arXiv:2302.01680v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26377;&#32422;&#26463;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30701;&#35270;&#39057;&#25512;&#33616;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#30701;&#35270;&#39057;&#25512;&#33616;&#38382;&#39064;&#24314;&#27169;&#20026;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#29992;&#25143;&#20132;&#20114;&#21644;&#22810;&#26679;&#30340;&#21709;&#24212;&#20013;&#20248;&#21270;&#32047;&#35745;&#35266;&#30475;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;&#36827;&#34892;&#20102;&#20004;&#38454;&#27573;&#30340;&#31574;&#30053;&#23398;&#20064;&#65292;&#24182;&#19988;&#33021;&#22815;&#21516;&#26102;&#28385;&#36275;&#20027;&#35201;&#30446;&#26631;&#21644;&#36741;&#21161;&#30446;&#26631;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#30701;&#35270;&#39057;&#30340;&#24191;&#27867;&#27969;&#34892;&#20026;&#35270;&#39057;&#20998;&#20139;&#24179;&#21488;&#19978;&#30340;&#25512;&#33616;&#31995;&#32479;&#20248;&#21270;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#29992;&#25143;&#19982;&#31995;&#32479;&#20381;&#27425;&#20132;&#20114;&#65292;&#24182;&#25552;&#20379;&#21253;&#25324;&#35266;&#30475;&#26102;&#38388;&#21644;&#23545;&#22810;&#20010;&#35270;&#39057;&#30340;&#21508;&#31181;&#31867;&#22411;&#20132;&#20114;&#22312;&#20869;&#30340;&#22797;&#26434;&#22810;&#38754; responses&#12290;&#19968;&#26041;&#38754;&#65292;&#24179;&#21488;&#26088;&#22312;&#38271;&#26399;&#20248;&#21270;&#29992;&#25143;&#30340;&#32047;&#35745;&#35266;&#30475;&#26102;&#38388;&#65288;&#20027;&#35201;&#30446;&#26631;&#65289;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26377;&#25928;&#20248;&#21270;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24179;&#21488;&#36824;&#38656;&#35201;&#28385;&#36275;&#36866;&#24212;&#22810;&#20010;&#29992;&#25143;&#20132;&#20114; responses&#65288;&#36741;&#21161;&#30446;&#26631;&#65289;&#30340;&#32422;&#26463;&#65292;&#22914; follow&#12289;share &#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#30701;&#35270;&#39057;&#25512;&#33616;&#38382;&#39064;&#20316;&#20026;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#36827;&#34892;&#20102;&#24314;&#27169;&#12290;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#30340;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25928;&#26524;&#19981;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#26377;&#32422;&#26463;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#65306;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#23398;&#20064;&#20010;&#20307;&#30340;&#31574;&#30053;&#65292;&#20197;&#20248;&#21270;&#20027;&#35201;&#30446;&#26631;&#12290;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23398;&#20064;&#20849;&#20139;&#30340;&#31574;&#30053;&#65292;&#20197;&#28385;&#36275;&#36741;&#21161;&#30446;&#26631;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
The wide popularity of short videos on social media poses new opportunities and challenges to optimize recommender systems on the video-sharing platforms. Users sequentially interact with the system and provide complex and multi-faceted responses, including watch time and various types of interactions with multiple videos. One the one hand, the platforms aims at optimizing the users' cumulative watch time (main goal) in long term, which can be effectively optimized by Reinforcement Learning. On the other hand, the platforms also needs to satisfy the constraint of accommodating the responses of multiple user interactions (auxiliary goals) such like, follow, share etc. In this paper, we formulate the problem of short video recommendation as a Constrained Markov Decision Process (CMDP). We find that traditional constrained reinforcement learning algorithms can not work well in this setting. We propose a novel two-stage constrained actor-critic method: At stage one, we learn individual pol
&lt;/p&gt;</description></item><item><title>&#27492;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#39640;&#26031;&#36807;&#31243;&#28388;&#27874;&#22120;&#65292;&#29992;&#20110;&#24515;&#30005;&#22270;&#21435;&#22122;&#12290;&#36890;&#36807;&#20351;&#29992;&#24515;&#30005;&#22270;&#30456;&#20301;&#22495;&#21644;&#39640;&#26031;&#20998;&#24067;&#20551;&#35774;&#65292;&#31616;&#21270;&#20102;&#35745;&#31639;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26080;&#20027;&#35266;&#36229;&#21442;&#25968;&#30340;&#39640;&#25928;&#28388;&#27874;&#22120;&#12290;</title><link>http://arxiv.org/abs/2301.02607</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#39640;&#26031;&#36807;&#31243;&#28388;&#27874;&#22120;&#29992;&#20110;&#24515;&#30005;&#22270;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
A Data-Driven Gaussian Process Filter for Electrocardiogram Denoising. (arXiv:2301.02607v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02607
&lt;/p&gt;
&lt;p&gt;
&#27492;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#39640;&#26031;&#36807;&#31243;&#28388;&#27874;&#22120;&#65292;&#29992;&#20110;&#24515;&#30005;&#22270;&#21435;&#22122;&#12290;&#36890;&#36807;&#20351;&#29992;&#24515;&#30005;&#22270;&#30456;&#20301;&#22495;&#21644;&#39640;&#26031;&#20998;&#24067;&#20551;&#35774;&#65292;&#31616;&#21270;&#20102;&#35745;&#31639;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#26080;&#20027;&#35266;&#36229;&#21442;&#25968;&#30340;&#39640;&#25928;&#28388;&#27874;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;: &#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#30340;&#28388;&#27874;&#22120;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#24050;&#32463;&#26377;&#25928;&#22320;&#29992;&#20110;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#28388;&#27874;&#65292;&#20294;&#26159;&#35745;&#31639;&#22797;&#26434;&#24230;&#36739;&#39640;&#65292;&#32780;&#19988;&#20854;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#36890;&#24120;&#26159;&#20027;&#35266;&#30340;&#12290;&#26041;&#27861;: &#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;GP&#28388;&#27874;&#22120;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#20351;&#29992;ECG&#30456;&#20301;&#22495;&#30340;&#27010;&#24565;&#65292;&#35813;&#22495;&#26159;&#23558;ECG&#33410;&#25293;&#21464;&#24418;&#21040;&#22266;&#23450;&#25968;&#37327;&#30340;&#26679;&#26412;&#21644;&#23545;&#40784;&#30340;R&#23792;&#19978;&#65292;&#20551;&#35774;&#20854;&#31526;&#21512;&#39640;&#26031;&#20998;&#24067;&#12290;&#22312;&#36825;&#20010;&#20551;&#35774;&#19979;&#65292;&#26679;&#26412;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#35745;&#31639;&#21464;&#24471;&#31616;&#21270;&#65292;&#21487;&#20197;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#19979;&#39640;&#25928;&#22320;&#23454;&#29616;GP&#28388;&#27874;&#22120;&#65292;&#32780;&#19981;&#38656;&#35201;&#20027;&#35266;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#22312;PhysioNet QT&#25968;&#25454;&#24211;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#28388;&#27874;&#22120;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#23567;&#27874;&#28388;&#27874;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36890;&#36807;&#27979;&#37327;&#28388;&#27874;&#22120;&#22312;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#20026;-5&#33267;30 dB&#33539;&#22260;&#20869;&#65288;&#27599;&#38548;5 dB&#65289;&#30340;&#25552;&#21319;&#26469;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Gaussian Processes (GP)-based filters, which have been effectively used for various applications including electrocardiogram (ECG) filtering can be computationally demanding and the choice of their hyperparameters is typically ad hoc. Methods: We develop a data-driven GP filter to address both issues, using the notion of the ECG phase domain -- a time-warped representation of the ECG beats onto a fixed number of samples and aligned R-peaks, which is assumed to follow a Gaussian distribution. Under this assumption, the computation of the sample mean and covariance matrix is simplified, enabling an efficient implementation of the GP filter in a data-driven manner, with no ad hoc hyperparameters. The proposed filter is evaluated and compared with a state-of-the-art wavelet-based filter, on the PhysioNet QT Database. The performance is evaluated by measuring the signal-to-noise ratio (SNR) improvement of the filter at SNR levels ranging from -5 to 30dB, in 5dB steps, using addit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;Transformer&#21644;&#20854;&#20182;&#40657;&#30418;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20803;&#23398;&#20064;&#35757;&#32451;&#25104;&#20026;&#36890;&#29992;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22120;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#36827;&#34892;&#27979;&#35797;&#38598;&#39044;&#27979;&#65292;&#26080;&#38656;&#23450;&#20041;&#25512;&#29702;&#27169;&#22411;&#12289;&#35757;&#32451;&#25439;&#22833;&#25110;&#20248;&#21270;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.04458</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;Transformer&#23454;&#29616;&#36890;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
General-Purpose In-Context Learning by Meta-Learning Transformers. (arXiv:2212.04458v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;Transformer&#21644;&#20854;&#20182;&#40657;&#30418;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20803;&#23398;&#20064;&#35757;&#32451;&#25104;&#20026;&#36890;&#29992;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22120;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#36827;&#34892;&#27979;&#35797;&#38598;&#39044;&#27979;&#65292;&#26080;&#38656;&#23450;&#20041;&#25512;&#29702;&#27169;&#22411;&#12289;&#35757;&#32451;&#25439;&#22833;&#25110;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#35201;&#27714;&#31995;&#32479;&#35774;&#35745;&#32773;&#25351;&#23450;&#23398;&#20064;&#27969;&#31243;&#30340;&#26041;&#26041;&#38754;&#38754;&#65292;&#20363;&#22914;&#25439;&#22833;&#20989;&#25968;&#12289;&#26550;&#26500;&#21644;&#20248;&#21270;&#22120;&#12290;&#32780;&#20803;&#23398;&#20064;&#65292;&#25110;&#32773;&#23398;&#20250;&#23398;&#20064;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#36825;&#20123;&#26041;&#38754;&#65292;&#24182;&#25215;&#35834;&#20197;&#26356;&#23569;&#30340;&#25163;&#21160;&#24037;&#20316;&#24320;&#21551;&#26356;&#22823;&#30340;&#33021;&#21147;&#12290;&#20803;&#23398;&#20064;&#30340;&#19968;&#20010;&#29305;&#21035;&#38596;&#24515;&#21187;&#21187;&#30340;&#30446;&#26631;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#36890;&#29992;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31639;&#27861;&#65292;&#20165;&#20351;&#29992;&#24102;&#26377;&#26368;&#23567;&#24402;&#32435;&#20559;&#35265;&#30340;&#40657;&#30418;&#27169;&#22411;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#25509;&#25910;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#20135;&#29983;&#27979;&#35797;&#38598;&#39044;&#27979;&#65292;&#32780;&#26080;&#38656;&#23450;&#20041;&#25512;&#29702;&#27169;&#22411;&#12289;&#35757;&#32451;&#25439;&#22833;&#25110;&#20248;&#21270;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Transformer&#21644;&#20854;&#20182;&#40657;&#30418;&#27169;&#22411;&#21487;&#20197;&#34987;&#20803;&#35757;&#32451;&#25104;&#20026;&#36890;&#29992;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#22823;&#23567;&#12289;&#20219;&#21153;&#25968;&#37327;&#21644;&#20803;&#20248;&#21270;&#24341;&#36215;&#30340;&#31639;&#27861;&#20043;&#38388;&#30340;&#36716;&#25442;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#27867;&#21270;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#35760;&#24518;&#65292;&#36824;&#26377;&#19968;&#20123;&#31639;&#27861;&#26681;&#26412;&#26080;&#27861;&#36827;&#34892;&#20803;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern machine learning requires system designers to specify aspects of the learning pipeline, such as losses, architectures, and optimizers. Meta-learning, or learning-to-learn, instead aims to learn those aspects, and promises to unlock greater capabilities with less manual effort. One particularly ambitious goal of meta-learning is to train general-purpose in-context learning algorithms from scratch, using only black-box models with minimal inductive bias. Such a model takes in training data, and produces test-set predictions across a wide range of problems, without any explicit definition of an inference model, training loss, or optimization algorithm. In this paper we show that Transformers and other black-box models can be meta-trained to act as general-purpose in-context learners. We characterize transitions between algorithms that generalize, algorithms that memorize, and algorithms that fail to meta-train at all, induced by changes in model size, number of tasks, and meta-opti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#36817;&#26399;&#22312;&#35774;&#23450;&#39044;&#27979;&#19979;&#30340;&#36827;&#23637;&#65292;&#26500;&#24314;&#20102;&#39044;&#27979;&#38598;&#20197;&#36866;&#24212;&#24402;&#32435;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#33410;&#28857;&#20998;&#31867;&#65292;&#35777;&#26126;&#20102;&#25552;&#20379;&#20102;&#27604;&#31616;&#21333;&#30340;&#31526;&#21512;&#39044;&#27979;&#24212;&#29992;&#26356;&#21152;&#32039;&#33268;&#21644;&#33391;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#38598;&#12290;</title><link>http://arxiv.org/abs/2211.14555</link><description>&lt;p&gt;
&#26080;&#20998;&#24067;&#20551;&#35774;&#30340;&#33410;&#28857;&#20998;&#31867;&#39044;&#27979;&#38598;
&lt;/p&gt;
&lt;p&gt;
Distribution Free Prediction Sets for Node Classification. (arXiv:2211.14555v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#36817;&#26399;&#22312;&#35774;&#23450;&#39044;&#27979;&#19979;&#30340;&#36827;&#23637;&#65292;&#26500;&#24314;&#20102;&#39044;&#27979;&#38598;&#20197;&#36866;&#24212;&#24402;&#32435;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#33410;&#28857;&#20998;&#31867;&#65292;&#35777;&#26126;&#20102;&#25552;&#20379;&#20102;&#27604;&#31616;&#21333;&#30340;&#31526;&#21512;&#39044;&#27979;&#24212;&#29992;&#26356;&#21152;&#32039;&#33268;&#21644;&#33391;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#21487;&#20197;&#22312;&#35768;&#22810;&#37325;&#35201;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#39640;&#31934;&#24230;&#20998;&#31867;&#30340;&#25928;&#26524;&#65292;&#20294;&#20854;&#26080;&#27861;&#25552;&#20379;&#20005;&#26684;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#23450;&#20041;&#12290;&#30001;&#20110;&#22270;&#32467;&#26500;&#24341;&#36215;&#30340;&#25968;&#25454;&#28857;&#20381;&#36182;&#24615;&#65292;&#37327;&#21270;GNN&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#21033;&#29992;&#36817;&#26399;&#22312;&#35774;&#23450;&#39044;&#27979;&#19979;&#30340;&#36827;&#23637;&#65292;&#26500;&#24314;&#20102;&#39044;&#27979;&#38598;&#20197;&#36866;&#24212;&#24402;&#32435;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;&#33410;&#28857;&#20998;&#31867;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#25442;&#20301;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#36890;&#36807;&#36866;&#24403;&#21152;&#26435;&#31526;&#21512;&#20998;&#25968;&#26469;&#21453;&#26144;&#32593;&#32476;&#32467;&#26500;&#12290;&#36890;&#36807;&#22312;&#24120;&#29992;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#27969;&#34892;&#30340;GNN&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#27604;&#31616;&#21333;&#30340;&#31526;&#21512;&#39044;&#27979;&#24212;&#29992;&#26356;&#21152;&#32039;&#33268;&#21644;&#33391;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are able to achieve high classification accuracy on many important real world datasets, but provide no rigorous notion of predictive uncertainty. Quantifying the confidence of GNN models is difficult due to the dependence between datapoints induced by the graph structure.  We leverage recent advances in conformal prediction to construct prediction sets for node classification in inductive learning scenarios. We do this by taking an existing approach for conformal classification that relies on \textit{exchangeable} data and modifying it by appropriately weighting the conformal scores to reflect the network structure. We show through experiments on standard benchmark datasets using popular GNN models that our approach provides tighter and better calibrated prediction sets than a naive application of conformal prediction.
&lt;/p&gt;</description></item><item><title>DyG2Vec&#26159;&#19968;&#20010;&#20855;&#26377;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#30340;&#21160;&#24577;&#22270;&#34920;&#24449;&#23398;&#20064;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;&#21644;&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#21462;&#20016;&#23500;&#30340;&#26102;&#38388;&#23884;&#20837;&#34920;&#31034;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#26410;&#26469;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2210.16906</link><description>&lt;p&gt;
DyG2Vec: &#24102;&#26377;&#33258;&#30417;&#30563;&#30340;&#21160;&#24577;&#22270;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DyG2Vec: Representation Learning for Dynamic Graphs with Self-Supervision. (arXiv:2210.16906v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16906
&lt;/p&gt;
&lt;p&gt;
DyG2Vec&#26159;&#19968;&#20010;&#20855;&#26377;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#30340;&#21160;&#24577;&#22270;&#34920;&#24449;&#23398;&#20064;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;&#21644;&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#21462;&#20016;&#23500;&#30340;&#26102;&#38388;&#23884;&#20837;&#34920;&#31034;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#26410;&#26469;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#36890;&#36807;&#33258;&#21160;&#25552;&#21462;&#26102;&#38388;&#27169;&#24335;&#26469;&#23398;&#20064;&#24402;&#32435;&#34920;&#31034;&#26041;&#38754;&#30340;&#26377;&#24076;&#26395;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#24037;&#20316;&#24120;&#24120;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#35760;&#24518;&#27169;&#22359;&#25110;&#20302;&#25928;&#30340;&#38543;&#26426;&#28216;&#36208;&#26041;&#27861;&#26469;&#26500;&#24314;&#26102;&#38388;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#21160;&#24577;&#22270;&#32534;&#30721;&#22120;&#19981;&#23481;&#26131;&#36866;&#24212;&#33258;&#30417;&#30563;&#33539;&#24335;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#26102;&#38388;&#36793;&#32534;&#30721;&#21644;&#22522;&#20110;&#31383;&#21475;&#30340;&#23376;&#22270;&#37319;&#26679;&#26469;&#29983;&#25104;&#20219;&#21153;&#26080;&#20851;&#30340;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#23545;&#27604;SSL&#30340;&#32852;&#21512;&#23884;&#20837;&#26550;&#26500;&#65292;&#20197;&#23398;&#20064;&#20016;&#23500;&#30340;&#26102;&#38388;&#23884;&#20837;&#32780;&#19981;&#38656;&#35201;&#26631;&#31614;&#12290;&#22312;7&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20256;&#23548;&#35774;&#32622;&#21644;&#24402;&#32435;&#35774;&#32622;&#30340;&#26410;&#26469;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#24179;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;SoTA&#22522;&#32447;4.23&#65285;&#21644;3.30&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal graph neural networks have shown promising results in learning inductive representations by automatically extracting temporal patterns. However, previous works often rely on complex memory modules or inefficient random walk methods to construct temporal representations. In addition, the existing dynamic graph encoders are non-trivial to adapt to self-supervised paradigms, which prevents them from utilizing unlabeled data. To address these limitations, we present an efficient yet effective attention-based encoder that leverages temporal edge encodings and window-based subgraph sampling to generate task-agnostic embeddings. Moreover, we propose a joint-embedding architecture using non-contrastive SSL to learn rich temporal embeddings without labels. Experimental results on 7 benchmark datasets indicate that on average, our model outperforms SoTA baselines on the future link prediction task by 4.23% for the transductive setting and 3.30% for the inductive setting while only requi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#20581;&#24247;&#32452;&#32455;&#36827;&#34892;&#36741;&#21161;&#20219;&#21153;&#35757;&#32451;&#65292;&#20351;&#34920;&#31034;&#36866;&#24212;&#32452;&#32455;&#20013;&#30340;&#30456;&#20851;&#32454;&#33410;&#65292;&#23454;&#29616;&#23545;&#32452;&#32455;&#23398;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2210.07675</link><description>&lt;p&gt;
&#23398;&#20064;&#22270;&#20687;&#34920;&#31034;&#20197;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65306;&#22312;&#33647;&#29289;&#24320;&#21457;&#20013;&#21457;&#29616;&#32452;&#32455;&#23398;&#25913;&#21464;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning image representations for anomaly detection: application to discovery of histological alterations in drug development. (arXiv:2210.07675v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07675
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;&#20581;&#24247;&#32452;&#32455;&#36827;&#34892;&#36741;&#21161;&#20219;&#21153;&#35757;&#32451;&#65292;&#20351;&#34920;&#31034;&#36866;&#24212;&#32452;&#32455;&#20013;&#30340;&#30456;&#20851;&#32454;&#33410;&#65292;&#23454;&#29616;&#23545;&#32452;&#32455;&#23398;&#22270;&#20687;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#30340;&#31995;&#32479;&#12290;&#22312;&#32452;&#32455;&#23398;&#20013;&#65292;&#27491;&#24120;&#26679;&#26412;&#36890;&#24120;&#26159;&#22823;&#37327;&#23384;&#22312;&#30340;&#65292;&#32780;&#24322;&#24120;&#65288;&#30149;&#29702;&#65289;&#24773;&#20917;&#36890;&#24120;&#24456;&#23569;&#25110;&#19981;&#21487;&#29992;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#22312;&#20581;&#24247;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#21333;&#31867;&#20998;&#31867;&#22120;&#21487;&#20197;&#26816;&#27979;&#21040;&#20998;&#24067;&#22806;&#30340;&#24322;&#24120;&#26679;&#26412;&#12290;&#36825;&#26679;&#30340;&#26041;&#27861;&#19982;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22270;&#20687;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#20197;&#21069;&#24050;&#32463;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#12290;&#20294;&#26159;&#65292;&#39044;&#35757;&#32451;&#30340;&#29616;&#25104;CNN&#34920;&#31034;&#21487;&#33021;&#23545;&#32452;&#32455;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#19981;&#25935;&#24863;&#65292;&#32780;&#20581;&#24247;&#32452;&#32455;&#30340;&#33258;&#28982;&#21464;&#24322;&#21487;&#33021;&#23548;&#33268;&#36828;&#31163;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#20351;&#34920;&#31034;&#36866;&#24212;&#20581;&#24247;&#32452;&#32455;&#20013;&#30340;&#30456;&#20851;&#32454;&#33410;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#36741;&#21161;&#20219;&#21153;&#19978;&#35757;&#32451;CNN&#65292;&#35813;&#20219;&#21153;&#21306;&#20998;&#19981;&#21516;&#29289;&#31181;&#12289;&#22120;&#23448;&#21644;&#26579;&#33394;&#35797;&#21058;&#30340;&#20581;&#24247;&#32452;&#32455;&#12290;&#20960;&#20046;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#26631;&#27880;&#24037;&#20316;&#37327;&#65292;&#22240;&#20026;&#20581;&#24247;&#26679;&#26412;&#21487;&#20197;&#33258;&#21160;&#33719;&#24471;&#19978;&#36848;&#26631;&#31614;&#12290;&#22312;&#35757;&#32451;&#20013;&#65292;&#25105;&#20204;&#24378;&#21046;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
We present a system for anomaly detection in histopathological images. In histology, normal samples are usually abundant, whereas anomalous (pathological) cases are scarce or not available. Under such settings, one-class classifiers trained on healthy data can detect out-of-distribution anomalous samples. Such approaches combined with pre-trained Convolutional Neural Network (CNN) representations of images were previously employed for anomaly detection (AD). However, pre-trained off-the-shelf CNN representations may not be sensitive to abnormal conditions in tissues, while natural variations of healthy tissue may result in distant representations. To adapt representations to relevant details in healthy tissue we propose training a CNN on an auxiliary task that discriminates healthy tissue of different species, organs, and staining reagents. Almost no additional labeling workload is required, since healthy samples come automatically with aforementioned labels. During training we enforce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;FPGA&#30340;&#20302;&#24310;&#36831;&#22270;&#31070;&#32463;&#32593;&#32476;(LL-GNN)&#26550;&#26500;&#65292;&#38024;&#23545;&#31890;&#23376;&#25506;&#27979;&#22120;&#39046;&#22495;&#30340;&#29305;&#27530;&#38656;&#27714;&#65292;&#36890;&#36807;&#22806;&#31215;&#30697;&#38453;&#20056;&#27861;&#26041;&#27861;&#12289;&#32467;&#26500;&#21270;&#37051;&#25509;&#30697;&#38453;&#21644;&#21015;&#20027;&#25968;&#25454;&#24067;&#23616;&#31561;&#20248;&#21270;&#25514;&#26045;&#65292;&#23454;&#29616;&#20102;&#20122;&#24494;&#31186;&#32423;&#21035;&#30340;&#32593;&#32476;&#37096;&#32626;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;GNN&#29305;&#23450;&#30340;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.14065</link><description>&lt;p&gt;
LL-GNN: &#22522;&#20110;FPGA&#30340;&#20302;&#24310;&#36831;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#39640;&#33021;&#29289;&#29702;&#39046;&#22495;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
LL-GNN: Low Latency Graph Neural Networks on FPGAs for High Energy Physics. (arXiv:2209.14065v4 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;FPGA&#30340;&#20302;&#24310;&#36831;&#22270;&#31070;&#32463;&#32593;&#32476;(LL-GNN)&#26550;&#26500;&#65292;&#38024;&#23545;&#31890;&#23376;&#25506;&#27979;&#22120;&#39046;&#22495;&#30340;&#29305;&#27530;&#38656;&#27714;&#65292;&#36890;&#36807;&#22806;&#31215;&#30697;&#38453;&#20056;&#27861;&#26041;&#27861;&#12289;&#32467;&#26500;&#21270;&#37051;&#25509;&#30697;&#38453;&#21644;&#21015;&#20027;&#25968;&#25454;&#24067;&#23616;&#31561;&#20248;&#21270;&#25514;&#26045;&#65292;&#23454;&#29616;&#20102;&#20122;&#24494;&#31186;&#32423;&#21035;&#30340;&#32593;&#32476;&#37096;&#32626;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;GNN&#29305;&#23450;&#30340;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21487;&#37325;&#26500;&#26550;&#26500;&#65292;&#29992;&#20110;&#23454;&#29616;&#22522;&#20110;FPGA&#30340;&#20302;&#24310;&#36831;&#22270;&#31070;&#32463;&#32593;&#32476;(LL-GNN)&#65292;&#20197;&#25903;&#25345;&#20855;&#26377;&#21069;&#25152;&#26410;&#26377;&#30340;&#20302;&#24310;&#36831;&#24615;&#33021;&#30340;&#31890;&#23376;&#25506;&#27979;&#22120;&#12290;&#30001;&#20110;&#22312;&#27431;&#27954;&#26680;&#23376;&#30740;&#31350;&#20013;&#24515;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#23454;&#39564;&#30340;&#19968;&#32423;&#35302;&#21457;&#22120;&#20013;&#38656;&#35201;&#20197;&#27599;&#31186;&#25968;&#30334;&#22826;&#23383;&#33410;&#30340;&#25968;&#25454;&#36895;&#29575;&#36827;&#34892;&#22312;&#32447;&#20107;&#20214;&#36873;&#25321;&#65292;&#22240;&#27492;&#23558;&#22522;&#20110;FPGA&#30340;GNNs&#24212;&#29992;&#20110;&#31890;&#23376;&#25506;&#27979;&#22120;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#22312;&#20122;&#24494;&#31186;&#32423;&#21035;&#20869;&#37096;&#32626;&#32593;&#32476;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22806;&#31215;&#30340;&#30697;&#38453;&#20056;&#27861;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#32467;&#26500;&#21270;&#37051;&#25509;&#30697;&#38453;&#21644;&#21015;&#20027;&#25968;&#25454;&#24067;&#23616;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#34701;&#21512;&#27493;&#39588;&#65292;&#36890;&#36807;&#28040;&#38500;&#19981;&#24517;&#35201;&#30340;&#36793;&#30028;&#36827;&#19968;&#27493;&#38477;&#20302;&#20102;&#31471;&#21040;&#31471;&#35774;&#35745;&#30340;&#24310;&#36831;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;GNN&#29305;&#23450;&#30340;&#31639;&#27861;-&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#26041;&#27861;&#65292;&#22312;&#32473;&#23450;&#24310;&#36831;&#38480;&#21046;&#19979;&#23547;&#25214;&#26356;&#22909;&#24310;&#36831;&#21644;&#26356;&#39640;&#31934;&#24230;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a novel reconfigurable architecture for Low Latency Graph Neural Network (LL-GNN) designs for particle detectors, delivering unprecedented low latency performance. Incorporating FPGA-based GNNs into particle detectors presents a unique challenge since it requires sub-microsecond latency to deploy the networks for online event selection with a data rate of hundreds of terabytes per second in the Level-1 triggers at the CERN Large Hadron Collider experiments. This paper proposes a novel outer-product based matrix multiplication approach, which is enhanced by exploiting the structured adjacency matrix and a column-major data layout. Moreover, a fusion step is introduced to further reduce the end-to-end design latency by eliminating unnecessary boundaries. Furthermore, a GNN-specific algorithm-hardware co-design approach is presented which not only finds a design with a much better latency but also finds a high accuracy design under given latency constraints. To facilita
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#25554;&#20837;&#24322;&#24418;&#23383;&#65292;&#27169;&#22411;&#20250;&#21453;&#26144;&#29983;&#25104;&#22270;&#29255;&#20013;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#21644;&#20559;&#35265;&#12290;&#36825;&#19968;&#29616;&#35937;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#27169;&#22411;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#12290;&#32780;&#24694;&#24847;&#29992;&#25143;&#25110;&#26381;&#21153;&#25552;&#20379;&#21830;&#36824;&#21487;&#33021;&#21033;&#29992;&#31867;&#20284;&#22806;&#24418;&#30340;&#38750;&#25289;&#19969;&#23383;&#31526;&#65292;&#25925;&#24847;&#24341;&#20837;&#20559;&#35265;&#65292;&#21019;&#36896;&#31181;&#26063;&#20027;&#20041;&#21051;&#26495;&#21360;&#35937;&#12290;</title><link>http://arxiv.org/abs/2209.08891</link><description>&lt;p&gt;
&#21033;&#29992;&#21516;&#24418;&#24322;&#20041;&#23383;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#25366;&#25496;&#25991;&#21270;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Exploiting Cultural Biases via Homoglyphs in Text-to-Image Synthesis. (arXiv:2209.08891v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08891
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#25554;&#20837;&#24322;&#24418;&#23383;&#65292;&#27169;&#22411;&#20250;&#21453;&#26144;&#29983;&#25104;&#22270;&#29255;&#20013;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#21644;&#20559;&#35265;&#12290;&#36825;&#19968;&#29616;&#35937;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#27169;&#22411;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#12290;&#32780;&#24694;&#24847;&#29992;&#25143;&#25110;&#26381;&#21153;&#25552;&#20379;&#21830;&#36824;&#21487;&#33021;&#21033;&#29992;&#31867;&#20284;&#22806;&#24418;&#30340;&#38750;&#25289;&#19969;&#23383;&#31526;&#65292;&#25925;&#24847;&#24341;&#20837;&#20559;&#35265;&#65292;&#21019;&#36896;&#31181;&#26063;&#20027;&#20041;&#21051;&#26495;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#65292;&#20363;&#22914;DALL-E 2&#21644;Stable Diffusion&#65292;&#36817;&#24180;&#26469;&#21560;&#24341;&#20102;&#23398;&#26415;&#30028;&#21644;&#24191;&#22823;&#20844;&#20247;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22312;&#25991;&#26412;&#25551;&#36848;&#30340;&#26465;&#20214;&#19979;&#29983;&#25104;&#25551;&#32472;&#21508;&#31181;&#27010;&#24565;&#21644;&#39118;&#26684;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20174;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#37319;&#29992;&#20102;&#19982;&#29305;&#23450;Unicode&#33050;&#26412;&#30456;&#20851;&#30340;&#25991;&#21270;&#29305;&#24449;&#65292;&#36825;&#21487;&#33021;&#19981;&#20250;&#31435;&#21363;&#26174;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#22312;&#25991;&#26412;&#25551;&#36848;&#20013;&#31616;&#21333;&#25554;&#20837;&#21333;&#20010;&#38750;&#25289;&#19969;&#23383;&#31526;&#65292;&#24120;&#35265;&#27169;&#22411;&#21576;&#29616;&#20986;&#29983;&#25104;&#22270;&#20687;&#20013;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#21644;&#20559;&#35265;&#12290;&#25105;&#20204;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#20102;&#36825;&#31181;&#34892;&#20026;&#65292;&#24182;&#30830;&#23450;&#20102;&#27169;&#22411;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#26159;&#36825;&#19968;&#29616;&#35937;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#24694;&#24847;&#29992;&#25143;&#25110;&#26381;&#21153;&#25552;&#20379;&#21830;&#21487;&#33021;&#35797;&#22270;&#24847;&#22270;&#24615;&#22320;&#36890;&#36807;&#23558;&#25289;&#19969;&#23383;&#31526;&#26367;&#25442;&#20026;&#38750;&#25289;&#19969;&#33050;&#26412;&#20013;&#22806;&#24418;&#30456;&#20284;&#30340;&#23383;&#31526;&#65292;&#26469;&#24341;&#20837;&#20559;&#35265;&#65292;&#21019;&#36896;&#31181;&#26063;&#20027;&#20041;&#21051;&#26495;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models for text-to-image synthesis, such as DALL-E~2 and Stable Diffusion, have recently drawn a lot of interest from academia and the general public. These models are capable of producing high-quality images that depict a variety of concepts and styles when conditioned on textual descriptions. However, these models adopt cultural characteristics associated with specific Unicode scripts from their vast amount of training data, which may not be immediately apparent. We show that by simply inserting single non-Latin characters in a textual description, common models reflect cultural stereotypes and biases in their generated images. We analyze this behavior both qualitatively and quantitatively, and identify a model's text encoder as the root cause of the phenomenon. Additionally, malicious users or service providers may try to intentionally bias the image generation to create racist stereotypes by replacing Latin characters with similarly-looking characters from non-Latin scripts, so-cal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#28436;&#21464;&#65292;&#36890;&#36807;&#34913;&#37327;&#30740;&#31350;&#32773;&#22312;&#35813;&#39046;&#22495;&#30340;&#24433;&#21709;&#12289;&#24433;&#21709;&#21147;&#21644;&#39046;&#23548;&#21147;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#22312;&#20027;&#35201;&#20154;&#24037;&#26234;&#33021;&#20250;&#35758;&#19978;&#21457;&#34920;&#30340;&#35770;&#25991;&#65292;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21457;&#23637;&#21644;&#28436;&#21464;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#23548;&#33268;&#20102;&#23398;&#26415;&#35770;&#25991;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#20840;&#38754;&#30340;&#24341;&#29992;&#21644;&#21512;&#20316;&#25968;&#25454;&#38598;&#65292;&#23545;&#30456;&#20851;&#20851;&#31995;&#36827;&#34892;&#20102;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2205.13131</link><description>&lt;p&gt;
&#35770;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#28436;&#21464;&#65306;&#26397;&#30528;&#22312;&#20027;&#35201;&#20154;&#24037;&#26234;&#33021;&#20250;&#35758;&#19978;&#34913;&#37327;&#21644;&#29702;&#35299;&#24433;&#21709;&#12289;&#24433;&#21709;&#21147;&#21644;&#39046;&#23548;&#21147;&#30340;&#20803;&#32423;&#30340;&#26041;&#21521;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
On the Evolution of A.I. and Machine Learning: Towards a Meta-level Measuring and Understanding Impact, Influence, and Leadership at Premier A.I. Conferences. (arXiv:2205.13131v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#28436;&#21464;&#65292;&#36890;&#36807;&#34913;&#37327;&#30740;&#31350;&#32773;&#22312;&#35813;&#39046;&#22495;&#30340;&#24433;&#21709;&#12289;&#24433;&#21709;&#21147;&#21644;&#39046;&#23548;&#21147;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#22312;&#20027;&#35201;&#20154;&#24037;&#26234;&#33021;&#20250;&#35758;&#19978;&#21457;&#34920;&#30340;&#35770;&#25991;&#65292;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21457;&#23637;&#21644;&#28436;&#21464;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#23548;&#33268;&#20102;&#23398;&#26415;&#35770;&#25991;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#20840;&#38754;&#30340;&#24341;&#29992;&#21644;&#21512;&#20316;&#25968;&#25454;&#38598;&#65292;&#23545;&#30456;&#20851;&#20851;&#31995;&#36827;&#34892;&#20102;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29616;&#22312;&#34987;&#35748;&#20026;&#26159;&#19968;&#39033;&#23545;&#20154;&#31867;&#29983;&#27963;&#20135;&#29983;&#24191;&#27867;&#24433;&#21709;&#30340;&#36890;&#29992;&#25216;&#26415;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#30740;&#31350;&#32773;&#23545;&#35813;&#39046;&#22495;&#30340;&#36129;&#29486;&#35282;&#24230;&#26469;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#30340;&#28436;&#21464;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#32773;&#24433;&#21709;&#12289;&#24433;&#21709;&#21147;&#21644;&#39046;&#23548;&#21147;&#30340;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#33258;1969&#24180;&#39318;&#27425;&#20030;&#21150;&#22269;&#38469;&#20154;&#24037;&#26234;&#33021;&#32852;&#21512;&#20250;&#35758; (IJCAI) &#20197;&#26469;&#22312;&#20027;&#35201;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#20250;&#35758;&#19978;&#21457;&#34920;&#30340;&#35770;&#25991;&#65292;&#23545;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21457;&#23637;&#21644;&#28436;&#21464;&#36827;&#34892;&#25506;&#32034;&#65292;&#20174;&#32780;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#21382;&#21490;&#21644;&#28436;&#21464;&#26377;&#20102;&#26032;&#30340;&#35748;&#35782;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#21644;&#28436;&#21464;&#23548;&#33268;&#20102;&#23398;&#26415;&#35770;&#25991;&#30340;&#22686;&#21152;&#65292;&#22312;&#36807;&#21435;&#30340;&#20845;&#21313;&#24180;&#26469;&#21457;&#34920;&#30340;&#25991;&#31456;&#25968;&#37327;&#20063;&#26377;&#25152;&#21453;&#26144;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#20840;&#38754;&#30340;&#24341;&#29992;&#21512;&#20316;&#19982;&#35770;&#25991;-&#20316;&#32773;&#25968;&#25454;&#38598;&#65292;&#24182;&#35745;&#31639;&#20102;&#21512;&#20316;&#24341;&#29992;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence is now recognized as a general-purpose technology with ample impact on human life. This work aims at understanding the evolution of AI and, in particular Machine learning, from the perspective of researchers' contributions to the field. In order to do so, we present several measures allowing the analyses of AI and machine learning researchers' impact, influence, and leadership over the last decades. This work also contributes, to a certain extent, to shed new light on the history and evolution of AI by exploring the dynamics involved in the field's evolution by looking at papers published at the flagship AI and machine learning conferences since the first International Joint Conference on Artificial Intelligence (IJCAI) held in 1969. AI development and evolution have led to increasing research output, reflected in the number of articles published over the last sixty years. We construct comprehensive citation collaboration and paper-author datasets and compute co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#20998;&#23618;VAE&#20316;&#20026;&#20808;&#39564;&#65292;&#36890;&#36807;&#32467;&#21512;&#20302;&#20998;&#36776;&#29575;&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#22788;&#29702;&#12290;&#22312;&#20154;&#33080;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#26679;&#26412;&#36136;&#37327;&#20043;&#38388;&#21462;&#24471;&#20102;&#26377;&#21033;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2205.10347</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#20998;&#23618;VAE&#23454;&#29616;&#22810;&#26679;&#30340;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Diverse super-resolution with pretrained deep hiererarchical VAEs. (arXiv:2205.10347v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#20998;&#23618;VAE&#20316;&#20026;&#20808;&#39564;&#65292;&#36890;&#36807;&#32467;&#21512;&#20302;&#20998;&#36776;&#29575;&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#22788;&#29702;&#12290;&#22312;&#20154;&#33080;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#26679;&#26412;&#36136;&#37327;&#20043;&#38388;&#21462;&#24471;&#20102;&#26377;&#21033;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#38382;&#39064;&#20013;&#29983;&#25104;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#30340;&#38382;&#39064;&#12290;&#20174;&#27010;&#29575;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#21487;&#20197;&#36890;&#36807;&#20174;&#36870;&#38382;&#39064;&#30340;&#21518;&#39564;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#36825;&#38656;&#35201;&#22312;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#19978;&#23450;&#20041;&#19968;&#20010;&#20808;&#39564;&#20998;&#24067;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;HVAE&#65289;&#20316;&#20026;&#20808;&#39564;&#12290;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#38543;&#26426;&#32534;&#30721;&#22120;&#65292;&#23558;&#20302;&#20998;&#36776;&#29575;&#22270;&#20687;&#32534;&#30721;&#21040;&#39044;&#35757;&#32451;HVAE&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23558;&#20302;&#20998;&#36776;&#29575;&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#23545;&#22270;&#20687;&#36827;&#34892;&#36229;&#20998;&#36776;&#29575;&#22788;&#29702;&#12290;&#25105;&#20204;&#22312;&#20154;&#33080;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26465;&#20214;&#27491;&#21017;&#21270;&#27969;&#25216;&#26415;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#25193;&#25955;&#26041;&#27861;&#30340;&#26679;&#26412;&#36136;&#37327;&#20043;&#38388;&#25552;&#20379;&#20102;&#26377;&#21033;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the problem of producing diverse solutions to an image super-resolution problem. From a probabilistic perspective, this can be done by sampling from the posterior distribution of an inverse problem, which requires the definition of a prior distribution on the high-resolution images. In this work, we propose to use a pretrained hierarchical variational autoencoder (HVAE) as a prior. We train a lightweight stochastic encoder to encode low-resolution images in the latent space of a pretrained HVAE. At inference, we combine the low-resolution encoder and the pretrained generative model to super-resolve an image. We demonstrate on the task of face super-resolution that our method provides an advantageous trade-off between the computational efficiency of conditional normalizing flows techniques and the sample quality of diffusion based methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#37325;&#34920;&#31034;&#30340;&#32456;&#36523;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24320;&#25918;&#24335;&#22330;&#26223;&#19979;&#30340;&#23569;&#26679;&#26412;&#30446;&#26631;&#35782;&#21035;&#38382;&#39064;&#65292;&#36866;&#29992;&#20110;&#19977;&#32500;&#29289;&#20307;&#31867;&#21035;&#25968;&#37327;&#19981;&#22266;&#23450;&#12289;&#38543;&#26102;&#38388;&#22686;&#38271;&#30340;&#22330;&#26223;&#12290;&#27169;&#22411;&#38024;&#23545;&#21508;&#31181;&#31867;&#22411;&#29289;&#20307;&#36827;&#34892;&#22788;&#29702;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2205.01982</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#37325;&#34920;&#31034;&#30340;&#32456;&#36523;&#38598;&#25104;&#23398;&#20064;&#22312;&#23569;&#26679;&#26412;&#30446;&#26631;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Lifelong Ensemble Learning based on Multiple Representations for Few-Shot Object Recognition. (arXiv:2205.01982v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.01982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#37325;&#34920;&#31034;&#30340;&#32456;&#36523;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24320;&#25918;&#24335;&#22330;&#26223;&#19979;&#30340;&#23569;&#26679;&#26412;&#30446;&#26631;&#35782;&#21035;&#38382;&#39064;&#65292;&#36866;&#29992;&#20110;&#19977;&#32500;&#29289;&#20307;&#31867;&#21035;&#25968;&#37327;&#19981;&#22266;&#23450;&#12289;&#38543;&#26102;&#38388;&#22686;&#38271;&#30340;&#22330;&#26223;&#12290;&#27169;&#22411;&#38024;&#23545;&#21508;&#31181;&#31867;&#22411;&#29289;&#20307;&#36827;&#34892;&#22788;&#29702;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26381;&#21153;&#26426;&#22120;&#20154;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#65292;&#24110;&#21161;&#25105;&#20204;&#22788;&#29702;&#21508;&#31181;&#20219;&#21153;&#12290;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#65292;&#26426;&#22120;&#20154;&#32463;&#24120;&#38754;&#23545;&#26032;&#30340;&#29289;&#20307;&#65292;&#24182;&#38656;&#35201;&#20197;&#24320;&#25918;&#24335;&#26041;&#24335;&#23398;&#20064;&#23427;&#20204;&#12290;&#27492;&#22806;&#65292;&#36825;&#26679;&#30340;&#26426;&#22120;&#20154;&#24517;&#39035;&#33021;&#22815;&#35782;&#21035;&#21508;&#31181;&#29289;&#20307;&#31867;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#37325;&#34920;&#31034;&#30340;&#32456;&#36523;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23569;&#26679;&#26412;&#30446;&#26631;&#35782;&#21035;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#22522;&#20110;&#28145;&#24230;&#34920;&#31034;&#21644;&#25163;&#24037;&#21046;&#20316;&#30340;3D&#24418;&#29366;&#25551;&#36848;&#31526;&#24418;&#25104;&#38598;&#25104;&#26041;&#27861;&#12290;&#20026;&#20102;&#20419;&#36827;&#32456;&#36523;&#23398;&#20064;&#65292;&#27599;&#31181;&#26041;&#27861;&#37117;&#37197;&#22791;&#20102;&#19968;&#20010;&#23384;&#20648;&#21644;&#30636;&#38388;&#26816;&#32034;&#29289;&#20307;&#20449;&#24687;&#30340;&#35760;&#24518;&#21333;&#20803;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36866;&#29992;&#20110;&#19977;&#32500;&#29289;&#20307;&#31867;&#21035;&#25968;&#37327;&#19981;&#22266;&#23450;&#12289;&#38543;&#26102;&#38388;&#22686;&#38271;&#30340;&#24320;&#25918;&#24335;&#23398;&#20064;&#22330;&#26223;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#31163;&#32447;&#21450;&#24320;&#25918;&#24335;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Service robots are integrating more and more into our daily lives to help us with various tasks. In such environments, robots frequently face new objects while working in the environment and need to learn them in an open-ended fashion. Furthermore, such robots must be able to recognize a wide range of object categories. In this paper, we present a lifelong ensemble learning approach based on multiple representations to address the few-shot object recognition problem. In particular, we form ensemble methods based on deep representations and handcrafted 3D shape descriptors. To facilitate lifelong learning, each approach is equipped with a memory unit for storing and retrieving object information instantly. The proposed model is suitable for open-ended learning scenarios where the number of 3D object categories is not fixed and can grow over time. We have performed extensive sets of experiments to assess the performance of the proposed approach in offline, and open-ended scenarios. For t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#30740;&#20102;&#33647;&#29289;&#35774;&#35745;&#39046;&#22495;&#20013;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#20998;&#20026;&#19968;&#27493;&#21040;&#20301;&#27861;&#12289;&#22522;&#20110;&#29255;&#27573;&#27861;&#21644;&#33410;&#28857;&#36880;&#20010;&#27861;&#19977;&#31867;&#65292;&#20171;&#32461;&#20102;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2202.09212</link><description>&lt;p&gt;
&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#20998;&#23376;&#29983;&#25104;&#65306;&#22270;&#23398;&#20064;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Molecule Generation for Drug Design: a Graph Learning Perspective. (arXiv:2202.09212v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.09212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#30740;&#20102;&#33647;&#29289;&#35774;&#35745;&#39046;&#22495;&#20013;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#20998;&#20026;&#19968;&#27493;&#21040;&#20301;&#27861;&#12289;&#22522;&#20110;&#29255;&#27573;&#27861;&#21644;&#33410;&#28857;&#36880;&#20010;&#27861;&#19977;&#31867;&#65292;&#20171;&#32461;&#20102;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#22270;&#23398;&#20064;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#33719;&#24471;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#35748;&#21487;&#65292;&#20855;&#26377;&#21464;&#38761;&#24615;&#30340;&#24433;&#21709;&#12290;&#20854;&#20013;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#24212;&#29992;&#39046;&#22495;&#26159;&#33647;&#29289;&#35774;&#35745;&#21644;&#21457;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#21046;&#33647;&#34892;&#19994;&#20013;&#12290;&#25105;&#20204;&#30340;&#35843;&#30740;&#25552;&#20379;&#20102;&#33647;&#29289;&#35774;&#35745;&#39046;&#22495;&#29616;&#26377;&#26041;&#27861;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#29305;&#21035;&#20851;&#27880;&#23558;&#65288;&#28145;&#24230;&#65289;&#22270;&#23398;&#20064;&#25216;&#26415;&#32435;&#20837;&#20854;&#20013;&#30340;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#20998;&#20026;&#19977;&#20010;&#19981;&#21516;&#30340;&#32452;&#65306;&#19968;&#65289;&#19968;&#27493;&#21040;&#20301;&#27861;&#65292;&#20108;&#65289;&#22522;&#20110;&#29255;&#27573;&#27861;&#65292;&#19977;&#65289;&#33410;&#28857;&#36880;&#20010;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20123;&#20851;&#38190;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#24182;&#27010;&#36848;&#20102;&#20998;&#23376;&#29983;&#25104;&#21644;&#20248;&#21270;&#24120;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#29616;&#26377;&#30340;&#25361;&#25112;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning, particularly graph learning, is gaining increasing recognition for its transformative impact across various fields. One such promising application is in the realm of molecule design and discovery, notably within the pharmaceutical industry. Our survey offers a comprehensive overview of state-of-the-art methods in molecule design, particularly focusing on \emph{de novo} drug design, which incorporates (deep) graph learning techniques. We categorize these methods into three distinct groups: \emph{i)} \emph{all-at-once}, \emph{ii)} \emph{fragment-based}, and \emph{iii)} \emph{node-by-node}. Additionally, we introduce some key public datasets and outline the commonly used evaluation metrics for both the generation and optimization of molecules. In the end, we discuss the existing challenges in this field and suggest potential directions for future research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21152;&#26435;&#23396;&#31435;&#26862;&#26519;&#65288;WIF&#65289;&#21644;&#21152;&#26435;&#38543;&#26426;&#20999;&#21106;&#26862;&#26519;&#65288;WRCF&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#24322;&#24120;&#26816;&#27979;&#65292;&#29305;&#21035;&#26159;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#36825;&#20123;&#31639;&#27861;&#21033;&#29992;&#32771;&#34385;&#25972;&#20307;&#25968;&#25454;&#32467;&#26500;&#30340;&#26435;&#37325;&#31574;&#30053;&#30830;&#23450;&#20998;&#21106;&#20540;&#65292;&#30456;&#27604;&#21407;&#26377;&#31639;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2202.01891</link><description>&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#21152;&#26435;&#23396;&#31435;&#26862;&#26519;&#21644;&#38543;&#26426;&#20999;&#21106;&#26862;&#26519;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Weighted Isolation and Random Cut Forest Algorithms for Anomaly Detection. (arXiv:2202.01891v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.01891
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21152;&#26435;&#23396;&#31435;&#26862;&#26519;&#65288;WIF&#65289;&#21644;&#21152;&#26435;&#38543;&#26426;&#20999;&#21106;&#26862;&#26519;&#65288;WRCF&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#24322;&#24120;&#26816;&#27979;&#65292;&#29305;&#21035;&#26159;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#36825;&#20123;&#31639;&#27861;&#21033;&#29992;&#32771;&#34385;&#25972;&#20307;&#25968;&#25454;&#32467;&#26500;&#30340;&#26435;&#37325;&#31574;&#30053;&#30830;&#23450;&#20998;&#21106;&#20540;&#65292;&#30456;&#27604;&#21407;&#26377;&#31639;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#24322;&#24120;&#26816;&#27979;&#65292;&#29305;&#21035;&#26159;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#38543;&#26426;&#20999;&#21106;&#26862;&#26519;&#65288;RCF&#65289;&#31639;&#27861;&#12290;RCF&#31639;&#27861;&#26159;&#23396;&#31435;&#26862;&#26519;&#65288;IF&#65289;&#31639;&#27861;&#30340;&#25913;&#36827;&#29256;&#26412;&#12290;&#19982;IF&#31639;&#27861;&#19981;&#21516;&#65292;RCF&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#23558;&#36755;&#20837;&#25554;&#20837;&#26500;&#24314;&#30340;&#26641;&#32593;&#32476;&#26469;&#30830;&#23450;&#23454;&#26102;&#36755;&#20837;&#26159;&#21542;&#21253;&#21547;&#24322;&#24120;&#12290;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#22810;&#31181;RCF&#31639;&#27861;&#65292;&#21253;&#25324;&#40065;&#26834;RCF&#65288;RRCF&#65289;&#65292;&#20854;&#20013;&#20999;&#21106;&#36807;&#31243;&#26159;&#33258;&#36866;&#24212;&#22320;&#27010;&#29575;&#36873;&#25321;&#30340;&#12290;RRCF&#31639;&#27861;&#34920;&#29616;&#20986;&#27604;IF&#31639;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#23427;&#22522;&#20110;&#25968;&#25454;&#30340;&#20960;&#20309;&#33539;&#22260;&#20915;&#23450;&#32500;&#24230;&#30340;&#20999;&#21106;&#65292;&#32780;IF&#31639;&#27861;&#38543;&#26426;&#36873;&#25321;&#32500;&#24230;&#30340;&#20999;&#21106;&#12290;&#28982;&#32780;&#65292;IF&#21644;RRCF&#37117;&#27809;&#26377;&#32771;&#34385;&#25972;&#20307;&#25968;&#25454;&#32467;&#26500;&#65292;&#22240;&#20026;&#20998;&#21106;&#20540;&#26159;&#38543;&#26426;&#36873;&#25321;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;IF&#21644;RCF&#31639;&#27861;&#65292;&#20998;&#21035;&#31216;&#20026;&#21152;&#26435;IF&#65288;WIF&#65289;&#21644;&#21152;&#26435;RCF&#65288;WRCF&#65289;&#31639;&#27861;&#12290;&#23427;&#20204;&#30340;&#20998;&#21106;&#20540;&#30001;&#19968;&#31181;&#32771;&#34385;&#25972;&#20307;&#25968;&#25454;&#32467;&#26500;&#30340;&#26435;&#37325;&#31574;&#30053;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Random cut forest (RCF) algorithms have been developed for anomaly detection, particularly in time series data. The RCF algorithm is an improved version of the isolation forest (IF) algorithm. Unlike the IF algorithm, the RCF algorithm can determine whether real-time input contains an anomaly by inserting the input into the constructed tree network. Various RCF algorithms, including Robust RCF (RRCF), have been developed, where the cutting procedure is adaptively chosen probabilistically. The RRCF algorithm demonstrates better performance than the IF algorithm, as dimension cuts are decided based on the geometric range of the data, whereas the IF algorithm randomly chooses dimension cuts. However, the overall data structure is not considered in both IF and RRCF, given that split values are chosen randomly. In this paper, we propose new IF and RCF algorithms, referred to as the weighted IF (WIF) and weighted RCF (WRCF) algorithms, respectively. Their split values are determined by consi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#31181;&#8212;&#8212;&#20108;&#27425;&#26799;&#24230;&#65292;&#29992;&#20110;&#22312;&#21516;&#24577;&#21152;&#23494;&#39046;&#22495;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#65292;&#24182;&#25104;&#21151;&#25552;&#21319;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#23454;&#29616;&#20102;&#21516;&#24577;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#20165;&#38656;&#36739;&#23569;&#30340;&#36845;&#20195;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2201.10838</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#21450;&#20854;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#31181;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Logistic Regression Training with A Faster Gradient Variant. (arXiv:2201.10838v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.10838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#31181;&#8212;&#8212;&#20108;&#27425;&#26799;&#24230;&#65292;&#29992;&#20110;&#22312;&#21516;&#24577;&#21152;&#23494;&#39046;&#22495;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#65292;&#24182;&#25104;&#21151;&#25552;&#21319;&#20102;&#25910;&#25947;&#36895;&#24230;&#65292;&#23454;&#29616;&#20102;&#21516;&#24577;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#20165;&#38656;&#36739;&#23569;&#30340;&#36845;&#20195;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#21152;&#23494;&#25968;&#25454;&#19978;&#30340;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#19968;&#30452;&#26159;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#31181;&#8212;&#8212;&#20108;&#27425;&#26799;&#24230;&#65292;&#29992;&#20110;&#22312;&#21516;&#24577;&#21152;&#23494;&#39046;&#22495;&#23454;&#29616;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#65292;&#20854;&#26680;&#24515;&#21487;&#20197;&#30475;&#20316;&#26159;&#31616;&#21270;&#30340;&#22266;&#23450;Hessian&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#20998;&#21035;&#21521;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#65288;NAG&#65289;&#21644;&#33258;&#36866;&#24212;&#26799;&#24230;&#31639;&#27861;&#65288;Adagrad&#65289;&#22686;&#24378;&#20102;&#35813;&#26799;&#24230;&#21464;&#31181;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#22686;&#24378;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#24378;&#26041;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#19978;&#27604;&#26420;&#32032;&#30340;&#19968;&#38454;&#26799;&#24230;&#26041;&#27861;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#22686;&#24378;&#30340;NAG&#26041;&#27861;&#26469;&#23454;&#29616;&#21516;&#24577;&#36923;&#36753;&#22238;&#24402;&#35757;&#32451;&#65292;&#24182;&#22312;&#20165;3&#27425;&#36845;&#20195;&#20013;&#33719;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logistic regression training over encrypted data has been an attractive idea to security concerns for years. In this paper, we propose a faster gradient variant called $\texttt{quadratic gradient}$ to implement logistic regression training in a homomorphic encryption domain, the core of which can be seen as an extension of the simplified fixed Hessian.  We enhance Nesterov's accelerated gradient (NAG) and Adaptive Gradient Algorithm (Adagrad) respectively with this gradient variant and evaluate the enhanced algorithms on several datasets. Experimental results show that the enhanced methods have a state-of-the-art performance in convergence speed compared to the naive first-order gradient methods. We then adopt the enhanced NAG method to implement homomorphic logistic regression training and obtain a comparable result by only $3$ iterations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Lojasiewicz-landscape&#19979;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#26696;&#30340;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;&#24403;SGD&#20445;&#25345;&#26377;&#30028;&#19988;&#20855;&#26377;&#21487;&#25968;&#20010;&#20020;&#30028;&#28857;&#65292;&#25110;&#30446;&#26631;&#20989;&#25968;&#28385;&#36275;Lojasiewicz&#19981;&#31561;&#24335;&#26102;&#65292;SGD&#23558;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#19968;&#20123;&#29305;&#23450;&#30340;&#35299;&#26512;&#28608;&#27963;&#20989;&#25968;&#26102;&#65292;&#22914;&#26524;&#35757;&#32451;&#20013;&#30340;&#20449;&#21495;&#21644;&#21709;&#24212;&#26159;&#32039;&#25903;&#25745;&#30340;&#65292;SGD&#20063;&#23558;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2102.09385</link><description>&lt;p&gt;
Lojasiewicz-landscape&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#26696;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Convergence of stochastic gradient descent schemes for Lojasiewicz-landscapes. (arXiv:2102.09385v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.09385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Lojasiewicz-landscape&#19979;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#26696;&#30340;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;&#24403;SGD&#20445;&#25345;&#26377;&#30028;&#19988;&#20855;&#26377;&#21487;&#25968;&#20010;&#20020;&#30028;&#28857;&#65292;&#25110;&#30446;&#26631;&#20989;&#25968;&#28385;&#36275;Lojasiewicz&#19981;&#31561;&#24335;&#26102;&#65292;SGD&#23558;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#19968;&#20123;&#29305;&#23450;&#30340;&#35299;&#26512;&#28608;&#27963;&#20989;&#25968;&#26102;&#65292;&#22914;&#26524;&#35757;&#32451;&#20013;&#30340;&#20449;&#21495;&#21644;&#21709;&#24212;&#26159;&#32039;&#25903;&#25745;&#30340;&#65292;SGD&#20063;&#23558;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23545;&#24369;&#20551;&#35774;&#19979;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#26696;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;&#21160;&#37327;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;MSGD&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;SGD&#20445;&#25345;&#26377;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#23384;&#22312;&#21487;&#25968;&#20010;&#20020;&#30028;&#28857;&#25110;&#32773;&#30446;&#26631;&#20989;&#25968;&#22312;&#25152;&#26377;&#20020;&#30028;&#27700;&#24179;&#38468;&#36817;&#28385;&#36275;Lojasiewicz&#19981;&#31561;&#24335;&#65292;&#22914;&#21516;&#25152;&#26377;&#35299;&#26512;&#20989;&#25968;&#19968;&#26679;&#65292;SGD&#23558;&#20250;&#25910;&#25947;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20855;&#26377;&#35299;&#26512;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;softplus&#12289;sigmoid&#21644;&#21452;&#26354;&#27491;&#20999;&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#38543;&#26426;&#21464;&#37327;&#23545;&#35757;&#32451;&#20013;&#30340;&#20449;&#21495;&#21644;&#21709;&#24212;&#36827;&#34892;&#32039;&#25903;&#25745;&#30340;&#24773;&#20917;&#19979;&#65292;SGD&#23558;&#22312;&#20445;&#25345;&#26377;&#30028;&#30340;&#24773;&#20917;&#19979;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we consider convergence of stochastic gradient descent schemes (SGD), including momentum stochastic gradient descent (MSGD), under weak assumptions on the underlying landscape. More explicitly, we show that on the event that the SGD stays bounded we have convergence of the SGD if there is only a countable number of critical points or if the objective function satisfies Lojasiewicz-inequalities around all critical levels as all analytic functions do. In particular, we show that for neural networks with analytic activation function such as softplus, sigmoid and the hyperbolic tangent, SGD converges on the event of staying bounded, if the random variables modelling the signal and response in the training are compactly supported.
&lt;/p&gt;</description></item></channel></rss>