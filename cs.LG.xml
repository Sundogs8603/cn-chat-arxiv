<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UAlign&#65292;&#19968;&#31181;&#26080;&#27169;&#26495;&#21270;&#30340;&#22270;&#21040;&#24207;&#21015;&#30340;&#36870;&#21512;&#25104;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#65292;&#21033;&#29992;&#20998;&#23376;&#30340;&#22266;&#26377;&#22270;&#32467;&#26500;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;SMILES&#23545;&#40784;&#25216;&#26415;&#26469;&#20419;&#36827;&#26410;&#25913;&#21464;&#32467;&#26500;&#30340;&#22797;&#29992;&#12290;</title><link>https://arxiv.org/abs/2404.00044</link><description>&lt;p&gt;
UAlign: &#26080;&#27169;&#26495;&#21270;&#30340;&#38750;&#30417;&#30563;&#24335;SMILES&#23545;&#40784;&#25512;&#21160;&#26080;&#27169;&#26495;&#21270;&#36870;&#21512;&#25104;&#39044;&#27979;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
UAlign: Pushing the Limit of Template-free Retrosynthesis Prediction with Unsupervised SMILES Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UAlign&#65292;&#19968;&#31181;&#26080;&#27169;&#26495;&#21270;&#30340;&#22270;&#21040;&#24207;&#21015;&#30340;&#36870;&#21512;&#25104;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#65292;&#21033;&#29992;&#20998;&#23376;&#30340;&#22266;&#26377;&#22270;&#32467;&#26500;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;SMILES&#23545;&#40784;&#25216;&#26415;&#26469;&#20419;&#36827;&#26410;&#25913;&#21464;&#32467;&#26500;&#30340;&#22797;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21512;&#25104;&#35268;&#21010;&#22312;&#26377;&#26426;&#21270;&#24037;&#34892;&#19994;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#21046;&#33647;&#39046;&#22495;&#65292;&#38754;&#20020;&#30528;&#24040;&#22823;&#25361;&#25112;&#12290;&#21333;&#27493;&#36870;&#21512;&#25104;&#39044;&#27979;&#26159;&#35268;&#21010;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#27493;&#65292;&#36817;&#24180;&#26469;&#30001;&#20110;&#31185;&#23398;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#65292;&#36825;&#19968;&#27493;&#39588;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#27987;&#21402;&#20852;&#36259;&#12290;&#36817;&#24180;&#26469;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#25324;&#19981;&#21516;&#31243;&#24230;&#30340;&#39069;&#22806;&#21270;&#23398;&#30693;&#35782;&#20381;&#36182;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;UAlign&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#21040;&#24207;&#21015;&#30340;&#26080;&#27169;&#26495;&#21270;&#36870;&#21512;&#25104;&#39044;&#27979;&#31649;&#32447;&#12290;&#36890;&#36807;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#20998;&#23376;&#30340;&#22266;&#26377;&#22270;&#32467;&#26500;&#12290;&#22522;&#20110;&#20998;&#23376;&#32467;&#26500;&#22312;&#21270;&#23398;&#21453;&#24212;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#30340;&#20107;&#23454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;SMILES&#23545;&#40784;&#25216;&#26415;&#65292;&#20197;&#20419;&#36827;&#26410;&#25913;&#21464;&#32467;&#26500;&#30340;&#22797;&#29992;&#20197;&#29983;&#25104;&#21453;&#24212;&#29289;&#12290;&#22823;&#37327;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00044v1 Announce Type: cross  Abstract: Retrosynthesis planning poses a formidable challenge in the organic chemical industry, particularly in pharmaceuticals. Single-step retrosynthesis prediction, a crucial step in the planning process, has witnessed a surge in interest in recent years due to advancements in AI for science. Various deep learning-based methods have been proposed for this task in recent years, incorporating diverse levels of additional chemical knowledge dependency. This paper introduces UAlign, a template-free graph-to-sequence pipeline for retrosynthesis prediction. By combining graph neural networks and Transformers, our method can more effectively leverage the inherent graph structure of molecules. Based on the fact that the majority of molecule structures remain unchanged during a chemical reaction, we propose a simple yet effective SMILES alignment technique to facilitate the reuse of unchanged structures for reactant generation. Extensive experiments 
&lt;/p&gt;</description></item><item><title>&#38750;&#36127;&#23545;&#27604;&#23398;&#20064;(NCL)&#26159;&#23545;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#30340;&#37325;&#26032;&#28436;&#32462;&#65292;&#36890;&#36807;&#23545;&#29305;&#24449;&#26045;&#21152;&#38750;&#36127;&#32422;&#26463;&#26469;&#33719;&#24471;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#65292;&#20445;&#30041;&#20102;NMF&#30340;&#21487;&#35299;&#37322;&#23646;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#27604;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;(CL)&#26356;&#31232;&#30095;&#21644;&#35299;&#32806;&#30340;&#34920;&#31034;</title><link>https://arxiv.org/abs/2403.12459</link><description>&lt;p&gt;
&#38750;&#36127;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Non-negative Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12459
&lt;/p&gt;
&lt;p&gt;
&#38750;&#36127;&#23545;&#27604;&#23398;&#20064;(NCL)&#26159;&#23545;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#30340;&#37325;&#26032;&#28436;&#32462;&#65292;&#36890;&#36807;&#23545;&#29305;&#24449;&#26045;&#21152;&#38750;&#36127;&#32422;&#26463;&#26469;&#33719;&#24471;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#65292;&#20445;&#30041;&#20102;NMF&#30340;&#21487;&#35299;&#37322;&#23646;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#27604;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;(CL)&#26356;&#31232;&#30095;&#21644;&#35299;&#32806;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#34920;&#31034;&#22312;&#20197;&#40657;&#30418;&#26041;&#24335;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22266;&#26377;&#30340;&#19981;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#23545;&#20154;&#31867;&#29702;&#35299;&#32780;&#35328;&#26159;&#19981;&#36879;&#26126;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38750;&#36127;&#23545;&#27604;&#23398;&#20064;&#65288;NCL&#65289;&#65292;&#36825;&#26159;&#23545;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#30340;&#22797;&#20852;&#65292;&#26088;&#22312;&#24471;&#20986;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#12290;NCL&#30340;&#21147;&#37327;&#22312;&#20110;&#24378;&#21046;&#23558;&#38750;&#36127;&#32422;&#26463;&#24212;&#29992;&#20110;&#29305;&#24449;&#65292;&#36825;&#35753;&#20154;&#24819;&#36215;NMF&#33021;&#22815;&#25552;&#21462;&#19982;&#26679;&#26412;&#38598;&#32676;&#32039;&#23494;&#23545;&#40784;&#30340;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;NCL&#19981;&#20165;&#22312;&#25968;&#23398;&#19978;&#19982;NMF&#30446;&#26631;&#24456;&#22909;&#22320;&#23545;&#40784;&#65292;&#32780;&#19988;&#20445;&#30041;&#20102;NMF&#30340;&#21487;&#35299;&#37322;&#23646;&#24615;&#65292;&#20351;&#24471;&#19982;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#30456;&#27604;&#65292;&#24471;&#21040;&#20102;&#26356;&#21152;&#31232;&#30095;&#21644;&#35299;&#32806;&#30340;&#34920;&#31034;&#12290;&#20174;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#20026;NCL&#30340;&#21487;&#35782;&#21035;&#24615;&#21644;&#19979;&#28216;&#27867;&#21270;&#24615;&#33021;&#25552;&#20379;&#20102;&#20445;&#35777;&#12290;&#20174;&#32463;&#39564;&#19978;&#30475;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12459v1 Announce Type: cross  Abstract: Deep representations have shown promising performance when transferred to downstream tasks in a black-box manner. Yet, their inherent lack of interpretability remains a significant challenge, as these features are often opaque to human understanding. In this paper, we propose Non-negative Contrastive Learning (NCL), a renaissance of Non-negative Matrix Factorization (NMF) aimed at deriving interpretable features. The power of NCL lies in its enforcement of non-negativity constraints on features, reminiscent of NMF's capability to extract features that align closely with sample clusters. NCL not only aligns mathematically well with an NMF objective but also preserves NMF's interpretability attributes, resulting in a more sparse and disentangled representation compared to standard contrastive learning (CL). Theoretically, we establish guarantees on the identifiability and downstream generalization of NCL. Empirically, we show that these 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#23450;&#20041;&#36890;&#36807;&#25968;&#23383;&#34892;&#20026;&#21464;&#21270;&#24178;&#39044;&#25903;&#25345;&#30284;&#30151;&#24739;&#32773;&#25552;&#39640;&#29983;&#27963;&#36136;&#37327;&#30340;&#26377;&#25928;&#21442;&#19982;&#26041;&#24335;&#65292;&#21457;&#29616;&#21307;&#29983;&#22788;&#26041;&#26174;&#33879;&#22686;&#21152;&#24739;&#32773;&#23545;&#31227;&#21160;&#25968;&#23383;&#34892;&#20026;&#21464;&#21270;&#24178;&#39044;&#30340;&#25345;&#32493;&#21442;&#19982;&#65292;&#21516;&#26102;&#25351;&#20986;&#27599;&#21608;&#21442;&#19982;&#19968;&#27425;&#24050;&#36275;&#20197;&#32500;&#25345;&#31119;&#31049;&#65292;&#20294;&#20869;&#22312;&#21160;&#26426;&#21487;&#33021;&#38656;&#35201;&#26356;&#39640;&#27700;&#24179;&#30340;&#21442;&#19982;&#12290;</title><link>https://arxiv.org/abs/2403.12007</link><description>&lt;p&gt;
&#30830;&#23450;&#36890;&#36807;&#31227;&#21160;&#25968;&#23383;&#34892;&#20026;&#21464;&#21270;&#24178;&#39044;&#25552;&#39640;&#30284;&#30151;&#24739;&#32773;&#31119;&#31049;&#30340;&#26377;&#25928;&#21442;&#19982;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Defining Effective Engagement For Enhancing Cancer Patients' Well-being with Mobile Digital Behavior Change Interventions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#23450;&#20041;&#36890;&#36807;&#25968;&#23383;&#34892;&#20026;&#21464;&#21270;&#24178;&#39044;&#25903;&#25345;&#30284;&#30151;&#24739;&#32773;&#25552;&#39640;&#29983;&#27963;&#36136;&#37327;&#30340;&#26377;&#25928;&#21442;&#19982;&#26041;&#24335;&#65292;&#21457;&#29616;&#21307;&#29983;&#22788;&#26041;&#26174;&#33879;&#22686;&#21152;&#24739;&#32773;&#23545;&#31227;&#21160;&#25968;&#23383;&#34892;&#20026;&#21464;&#21270;&#24178;&#39044;&#30340;&#25345;&#32493;&#21442;&#19982;&#65292;&#21516;&#26102;&#25351;&#20986;&#27599;&#21608;&#21442;&#19982;&#19968;&#27425;&#24050;&#36275;&#20197;&#32500;&#25345;&#31119;&#31049;&#65292;&#20294;&#20869;&#22312;&#21160;&#26426;&#21487;&#33021;&#38656;&#35201;&#26356;&#39640;&#27700;&#24179;&#30340;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#34892;&#20026;&#21464;&#21270;&#24178;&#39044;&#65288;DBCI&#65289;&#27491;&#22312;&#25903;&#25345;&#26032;&#20581;&#24247;&#34892;&#20026;&#30340;&#21457;&#23637;&#12290;&#35780;&#20272;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#23545;&#20110;&#25913;&#36827;&#23427;&#20204;&#21644;&#29702;&#35299;&#25104;&#21151;&#22240;&#32032;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29305;&#21035;&#26159;&#22312;&#21463;&#20262;&#29702;&#38480;&#21046;&#30340;&#23567;&#35268;&#27169;&#30740;&#31350;&#20013;&#65292;&#24320;&#21457;&#32773;&#30340;&#20840;&#38754;&#25351;&#23548;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;CAPABLE&#39033;&#30446;&#65292;&#26088;&#22312;&#23450;&#20041;&#36890;&#36807;DBCI&#25903;&#25345;&#30284;&#30151;&#24739;&#32773;&#25552;&#39640;&#29983;&#27963;&#36136;&#37327;&#30340;&#26377;&#25928;&#21442;&#19982;&#26041;&#24335;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#34913;&#37327;&#21442;&#19982;&#24230;&#30340;&#25351;&#26631;&#65292;&#25506;&#35752;&#20102;&#24739;&#32773;&#21644;&#20020;&#24202;&#21307;&#29983;&#23545;DBCI&#30340;&#20852;&#36259;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#35780;&#20272;DBCI&#24433;&#21709;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21307;&#29983;&#30340;&#22788;&#26041;&#26174;&#30528;&#22686;&#21152;&#20102;&#24739;&#32773;&#23545;&#31227;&#21160;DBCI&#30340;&#25345;&#32493;&#21442;&#19982;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#27599;&#21608;&#19968;&#27425;&#21442;&#19982;DBCI&#23601;&#36275;&#20197;&#32500;&#25345;&#31119;&#31049;&#65292;&#20294;&#20174;&#22806;&#22312;&#21160;&#26426;&#21521;&#20869;&#22312;&#21160;&#26426;&#30340;&#36716;&#21464;&#21487;&#33021;&#38656;&#35201;&#26356;&#39640;&#27700;&#24179;&#30340;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12007v1 Announce Type: cross  Abstract: Digital Behavior Change Interventions (DBCIs) are supporting development of new health behaviors. Evaluating their effectiveness is crucial for their improvement and understanding of success factors. However, comprehensive guidance for developers, particularly in small-scale studies with ethical constraints, is limited. Building on the CAPABLE project, this study aims to define effective engagement with DBCIs for supporting cancer patients in enhancing their quality of life. We identify metrics for measuring engagement, explore the interest of both patients and clinicians in DBCIs, and propose hypotheses for assessing the impact of DBCIs in such contexts. Our findings suggest that clinician prescriptions significantly increase sustained engagement with mobile DBCIs. In addition, while one weekly engagement with a DBCI is sufficient to maintain well-being, transitioning from extrinsic to intrinsic motivation may require a higher level o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35814;&#32454;&#30740;&#31350;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#30830;&#23450;&#20102;&#23545;&#20110;&#23454;&#29616;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26368;&#26032;&#28526;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#30340;&#20851;&#38190;&#35774;&#35745;&#32463;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.09611</link><description>&lt;p&gt;
MM1&#65306;&#22810;&#27169;&#24335;LLM&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12289;&#20998;&#26512;&#19982;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09611
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35814;&#32454;&#30740;&#31350;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#30830;&#23450;&#20102;&#23545;&#20110;&#23454;&#29616;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26368;&#26032;&#28526;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#30340;&#20851;&#38190;&#35774;&#35745;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26500;&#24314;&#39640;&#24615;&#33021;&#30340;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#26550;&#26500;&#32452;&#20214;&#21644;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23545;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#21644;&#21508;&#31181;&#39044;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#36827;&#34892;&#20180;&#32454;&#21644;&#20840;&#38754;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#20010;&#20851;&#38190;&#30340;&#35774;&#35745;&#32463;&#39564;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#22823;&#35268;&#27169;&#22810;&#27169;&#24335;&#39044;&#35757;&#32451;&#20351;&#29992;&#20180;&#32454;&#28151;&#21512;&#30340;&#22270;&#20687;&#26631;&#39064;&#12289;&#20132;&#26367;&#22270;&#20687;&#25991;&#26412;&#21644;&#20165;&#25991;&#26412;&#25968;&#25454;&#23545;&#20110;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#26368;&#26032;&#28526;&#65288;SOTA&#65289;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#19982;&#20854;&#20182;&#24050;&#21457;&#34920;&#30340;&#39044;&#35757;&#32451;&#32467;&#26524;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#22270;&#20687;&#32534;&#30721;&#22120;&#36830;&#21516;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#22270;&#20687;&#26631;&#35760;&#35745;&#25968;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#32780;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#35774;&#35745;&#30456;&#23545;&#37325;&#35201;&#24615;&#36739;&#23567;&#12290;&#36890;&#36807;&#25193;&#22823;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;MM1&#65292;&#19968;&#20010;&#22810;&#27169;&#24335;&#27169;&#22411;&#31995;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09611v1 Announce Type: cross  Abstract: In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;SUMO&#20013;&#38543;&#26426;&#21270;&#35268;&#21017;&#24494;&#35266;&#20132;&#36890;&#27969;&#30340;&#34892;&#20026;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#25552;&#39640;&#20854;&#22312;&#26356;&#30495;&#23454;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02882</link><description>&lt;p&gt;
&#36890;&#36807;&#20197;&#20132;&#36890;&#27969;&#38543;&#26426;&#21270;&#26041;&#24335;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20915;&#31574;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicle decision and control through reinforcement learning with traffic flow randomization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02882
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;SUMO&#20013;&#38543;&#26426;&#21270;&#35268;&#21017;&#24494;&#35266;&#20132;&#36890;&#27969;&#30340;&#34892;&#20026;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#25552;&#39640;&#20854;&#22312;&#26356;&#30495;&#23454;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22823;&#22810;&#25968;&#20851;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20915;&#31574;&#21644;&#25511;&#21046;&#20219;&#21153;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#26159;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#12290;&#36825;&#20123;&#30740;&#31350;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#26159;&#22312;&#22522;&#20110;&#35268;&#21017;&#30340;&#24494;&#35266;&#20132;&#36890;&#27969;&#19979;&#36827;&#34892;&#30340;&#65292;&#24456;&#23569;&#32771;&#34385;&#23558;&#23427;&#20204;&#36801;&#31227;&#21040;&#30495;&#23454;&#25110;&#25509;&#36817;&#30495;&#23454;&#30340;&#29615;&#22659;&#20013;&#20197;&#27979;&#35797;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;SUMO&#20013;&#22522;&#20110;&#35268;&#21017;&#30340;&#24494;&#35266;&#20132;&#36890;&#27969;&#30340;&#36710;&#36742;&#36319;&#39536;&#27169;&#22411;&#21644;&#25442;&#36947;&#27169;&#22411;&#30340;&#26576;&#20123;&#21442;&#25968;&#26469;&#38543;&#26426;&#21270;&#21608;&#22260;&#36710;&#36742;&#30340;&#39550;&#39542;&#39118;&#26684;&#21644;&#34892;&#20026;&#12290;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#20844;&#36335;&#21644;&#21512;&#24182;&#22330;&#26223;&#20013;&#30340;&#39046;&#22495;&#38543;&#26426;&#21270;&#35268;&#21017;&#24494;&#35266;&#20132;&#36890;&#27969;&#19979;&#35757;&#32451;&#31574;&#30053;&#65292;&#28982;&#21518;&#20998;&#21035;&#22312;&#22522;&#20110;&#35268;&#21017;&#30340;&#24494;&#35266;&#20132;&#36890;&#27969;&#21644;.
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02882v1 Announce Type: cross  Abstract: Most of the current studies on autonomous vehicle decision-making and control tasks based on reinforcement learning are conducted in simulated environments. The training and testing of these studies are carried out under rule-based microscopic traffic flow, with little consideration of migrating them to real or near-real environments to test their performance. It may lead to a degradation in performance when the trained model is tested in more realistic traffic scenes. In this study, we propose a method to randomize the driving style and behavior of surrounding vehicles by randomizing certain parameters of the car-following model and the lane-changing model of rule-based microscopic traffic flow in SUMO. We trained policies with deep reinforcement learning algorithms under the domain randomized rule-based microscopic traffic flow in freeway and merging scenes, and then tested them separately in rule-based microscopic traffic flow and h
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31616;&#21333;&#30340;&#36845;&#20195;&#36125;&#21494;&#26031;&#26356;&#26032;&#26041;&#26696;&#21644;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#27169;&#22411;&#20026;&#20154;&#32676;&#23548;&#33322;&#25552;&#20379;&#20102;&#23454;&#26102;&#19988;&#21487;&#25193;&#23637;&#30340;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01537</link><description>&lt;p&gt;
&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#29992;&#20110;&#20154;&#32676;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Mixed-Strategy Nash Equilibrium for Crowd Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01537
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#30340;&#36845;&#20195;&#36125;&#21494;&#26031;&#26356;&#26032;&#26041;&#26696;&#21644;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#27169;&#22411;&#20026;&#20154;&#32676;&#23548;&#33322;&#25552;&#20379;&#20102;&#23454;&#26102;&#19988;&#21487;&#25193;&#23637;&#30340;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#38024;&#23545;&#20154;&#32676;&#23548;&#33322;&#25214;&#21040;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#30340;&#38382;&#39064;&#12290;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#20026;&#26426;&#22120;&#20154;&#25552;&#20379;&#20102;&#19968;&#20010;&#20005;&#35880;&#30340;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#39044;&#27979;&#20154;&#32676;&#20013;&#19981;&#30830;&#23450;&#20294;&#21512;&#20316;&#30340;&#20154;&#31867;&#34892;&#20026;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36890;&#24120;&#22826;&#39640;&#65292;&#26080;&#27861;&#36827;&#34892;&#21487;&#25193;&#23637;&#21644;&#23454;&#26102;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#36845;&#20195;&#36125;&#21494;&#26031;&#26356;&#26032;&#26041;&#26696;&#25910;&#25947;&#20110;&#28151;&#21512;&#31574;&#30053;&#31038;&#20132;&#23548;&#33322;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20195;&#29702;&#31574;&#30053;&#21021;&#22987;&#21270;&#20026;&#20174;&#20154;&#31867;&#25968;&#25454;&#38598;&#23398;&#20064;&#30340;&#39640;&#26031;&#36807;&#31243;&#65292;&#26469;&#26500;&#24314;&#35813;&#28216;&#25103;&#12290;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#27169;&#22411;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#37319;&#26679;&#30340;&#20154;&#32676;&#23548;&#33322;&#26694;&#26550;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#29616;&#26377;&#23548;&#33322;&#26041;&#27861;&#20013;&#65292;&#24182;&#21487;&#22312;&#31508;&#35760;&#26412;&#30005;&#33041; CPU &#19978;&#23454;&#26102;&#36816;&#34892;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#20154;&#31867;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#26694;&#26550;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01537v1 Announce Type: cross  Abstract: We address the problem of finding mixed-strategy Nash equilibrium for crowd navigation. Mixed-strategy Nash equilibrium provides a rigorous model for the robot to anticipate uncertain yet cooperative human behavior in crowds, but the computation cost is often too high for scalable and real-time decision-making. Here we prove that a simple iterative Bayesian updating scheme converges to the Nash equilibrium of a mixed-strategy social navigation game. Furthermore, we propose a data-driven framework to construct the game by initializing agent strategies as Gaussian processes learned from human datasets. Based on the proposed mixed-strategy Nash equilibrium model, we develop a sampling-based crowd navigation framework that can be integrated into existing navigation methods and runs in real-time on a laptop CPU. We evaluate our framework in both simulated environments and real-world human datasets in unstructured environments. Our framework
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#22909;&#20248;&#21270;&#65292;&#25913;&#36827;&#20102;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#29983;&#25104;&#26041;&#27861;&#65292;&#20943;&#36731;&#25945;&#24072;&#32321;&#37325;&#30340;&#24037;&#20316;&#37327;&#65292;&#38450;&#27490;&#29983;&#25104;&#26080;&#25928;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00199</link><description>&lt;p&gt;
&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#22909;&#20248;&#21270;&#25913;&#36827;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving Socratic Question Generation using Data Augmentation and Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00199
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#22909;&#20248;&#21270;&#65292;&#25913;&#36827;&#20102;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#29983;&#25104;&#26041;&#27861;&#65292;&#20943;&#36731;&#25945;&#24072;&#32321;&#37325;&#30340;&#24037;&#20316;&#37327;&#65292;&#38450;&#27490;&#29983;&#25104;&#26080;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#26159;&#19968;&#31181;&#24341;&#23548;&#23398;&#29983;&#29420;&#31435;&#35299;&#20915;&#38382;&#39064;&#32780;&#19981;&#30452;&#25509;&#25581;&#31034;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#22909;&#20248;&#21270;&#25913;&#36827;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#24040;&#22823;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#33487;&#26684;&#25289;&#24213;&#38382;&#39064;&#65292;&#20197;&#20943;&#36731;&#25945;&#24072;&#30340;&#32321;&#37325;&#24037;&#20316;&#37327;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#28041;&#21450;&#25552;&#31034;&#36825;&#20123;&#24040;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26377;&#26102;&#20250;&#20135;&#29983;&#26080;&#25928;&#30340;&#36755;&#20986;&#65292;&#20363;&#22914;&#30452;&#25509;&#25581;&#31034;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#25110;&#25552;&#20379;&#26080;&#20851;&#25110;&#36807;&#26089;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#39318;&#20808;&#25552;&#20986;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#20016;&#23500;&#29616;&#26377;&#30340;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#25968;&#25454;&#38598;&#65307;&#20854;&#27425;&#65292;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#24320;&#28304;&#24040;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#20363;&#22914;LLama 2&#65292;&#20197;&#26356;&#20542;&#21521;&#20110;&#22320;&#38754;&#30495;&#20540;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00199v1 Announce Type: new  Abstract: The Socratic method is a way of guiding students toward solving a problem independently without directly revealing the solution to the problem. Although this method has been shown to significantly improve student learning outcomes, it remains a complex labor-intensive task for instructors. Large language models (LLMs) can be used to augment human effort by automatically generating Socratic questions for students. However, existing methods that involve prompting these LLMs sometimes produce invalid outputs, e.g., those that directly reveal the solution to the problem or provide irrelevant or premature questions. To alleviate this problem, inspired by reinforcement learning with AI feedback (RLAIF), we first propose a data augmentation method to enrich existing Socratic questioning datasets with questions that are invalid in specific ways. Next, we propose a method to optimize open-source LLMs such as LLama 2 to prefer ground-truth questio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#20010;&#20449;&#24687;&#28304;&#26469;&#26356;&#22909;&#22320;&#39044;&#27979;&#31038;&#21306;&#23646;&#24615;&#65292;&#29305;&#21035;&#26159;&#39044;&#27979;&#24403;&#22320;&#25991;&#21270;&#65292;&#32467;&#26524;&#26174;&#31034;&#20102;&#36825;&#19968;&#26041;&#27861;&#22312;&#32771;&#34385;&#32467;&#26500;&#30456;&#36830;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17905</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#24403;&#22320;&#25991;&#21270;
&lt;/p&gt;
&lt;p&gt;
Using Graph Neural Networks to Predict Local Culture
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#20010;&#20449;&#24687;&#28304;&#26469;&#26356;&#22909;&#22320;&#39044;&#27979;&#31038;&#21306;&#23646;&#24615;&#65292;&#29305;&#21035;&#26159;&#39044;&#27979;&#24403;&#22320;&#25991;&#21270;&#65292;&#32467;&#26524;&#26174;&#31034;&#20102;&#36825;&#19968;&#26041;&#27861;&#22312;&#32771;&#34385;&#32467;&#26500;&#30456;&#36830;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#30740;&#31350;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#35748;&#35782;&#21040;&#31038;&#21306;&#26159;&#21160;&#24577;&#21644;&#20851;&#32852;&#30340;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#25968;&#25454;&#12289;&#26041;&#27861;&#35770;&#21644;&#35745;&#31639;&#22788;&#29702;&#33021;&#21147;&#38459;&#30861;&#20102;&#23545;&#31038;&#21306;&#20851;&#31995;&#21160;&#24577;&#36827;&#34892;&#27491;&#24335;&#23450;&#37327;&#20998;&#26512;&#12290;&#20026;&#20102;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#21462;&#24471;&#36827;&#23637;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26041;&#27861;&#65292;&#20801;&#35768;&#32467;&#21512;&#21644;&#35780;&#20272;&#20851;&#20110;&#31038;&#21306;&#20869;&#37096;&#29305;&#24449;&#12289;&#23427;&#20204;&#30340;&#36807;&#21435;&#29305;&#24449;&#20197;&#21450;&#22312;&#23427;&#20204;&#20043;&#38388;&#27969;&#21160;&#30340;&#32676;&#20307;&#30340;&#22810;&#20010;&#20449;&#24687;&#28304;&#65292;&#28508;&#22312;&#22320;&#20026;&#39044;&#27979;&#27169;&#22411;&#25552;&#20379;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#36890;&#36807;&#25506;&#32034; Yelp &#30340;&#20844;&#24320;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32771;&#34385;&#32467;&#26500;&#30456;&#36830;&#24615;&#26041;&#38754;&#23545;&#39044;&#27979;&#31038;&#21306;&#23646;&#24615;&#65288;&#29305;&#21035;&#26159;&#39044;&#27979;&#24403;&#22320;&#25991;&#21270;&#65289;&#30340;&#28508;&#21147;&#12290;&#20174;&#23454;&#36136;&#21644;&#26041;&#27861;&#35770;&#35282;&#24230;&#26469;&#30475;&#65292;&#32467;&#26524;&#26159;&#20196;&#20154;&#40723;&#33310;&#30340;&#12290;&#20174;&#23454;&#36136;&#19978;&#35762;&#65292;&#25105;&#20204;&#21457;&#29616;&#26080;&#35770;&#26159;&#24403;&#22320;&#21306;&#22495;&#20449;&#24687;&#65288;&#20363;&#22914;&#21306;&#22495;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17905v1 Announce Type: new  Abstract: Urban research has long recognized that neighbourhoods are dynamic and relational. However, lack of data, methodologies, and computer processing power have hampered a formal quantitative examination of neighbourhood relational dynamics. To make progress on this issue, this study proposes a graph neural network (GNN) approach that permits combining and evaluating multiple sources of information about internal characteristics of neighbourhoods, their past characteristics, and flows of groups among them, potentially providing greater expressive power in predictive models. By exploring a public large-scale dataset from Yelp, we show the potential of our approach for considering structural connectedness in predicting neighbourhood attributes, specifically to predict local culture. Results are promising from a substantive and methodologically point of view. Substantively, we find that either local area information (e.g. area demographics) or g
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#22522;&#30784;&#25216;&#33021;&#21644;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#26032;&#27169;&#22411;&#23545;&#20110;&#20302;&#24180;&#32423;&#25216;&#33021;&#30340;&#26377;&#38480;&#25484;&#25569;&#12290;</title><link>https://arxiv.org/abs/2402.17205</link><description>&lt;p&gt;
&#27979;&#37327;&#31070;&#32463;&#27169;&#22411;&#30340;&#35270;&#35273;&#35821;&#35328;STEM&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Measuring Vision-Language STEM Skills of Neural Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17205
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#22522;&#30784;&#25216;&#33021;&#21644;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#26032;&#27169;&#22411;&#23545;&#20110;&#20302;&#24180;&#32423;&#25216;&#33021;&#30340;&#26377;&#38480;&#25484;&#25569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#39064;&#36890;&#24120;&#38656;&#35201;&#32467;&#21512;STEM&#65288;&#31185;&#23398;&#12289;&#25216;&#26415;&#12289;&#24037;&#31243;&#21644;&#25968;&#23398;&#65289;&#30693;&#35782;&#26469;&#35299;&#20915;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26159;&#25361;&#25112;&#24615;&#38382;&#39064;&#20013;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290;&#23427;&#21253;&#25324;448&#39033;&#25216;&#33021;&#21644;1,073,146&#20010;&#36328;&#36234;&#25152;&#26377;STEM&#31185;&#30446;&#30340;&#38382;&#39064;&#12290;&#19982;&#36890;&#24120;&#20391;&#37325;&#20110;&#26816;&#39564;&#19987;&#23478;&#27700;&#24179;&#33021;&#21147;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#22522;&#30784;&#25216;&#33021;&#21644;&#26681;&#25454;K-12&#35838;&#31243;&#35774;&#35745;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23558;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;CLIP&#21644;GPT-3.5-Turbo&#65292;&#28155;&#21152;&#21040;&#25105;&#20204;&#30340;&#22522;&#20934;&#20013;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#36817;&#30340;&#27169;&#22411;&#36827;&#23637;&#21482;&#26377;&#21161;&#20110;&#25484;&#25569;&#25968;&#25454;&#38598;&#20013;&#38750;&#24120;&#26377;&#38480;&#25968;&#37327;&#30340;&#20302;&#24180;&#32423;&#25216;&#33021;&#65288;&#19977;&#24180;&#32423;&#20013;&#30340;2.5%&#65289;&#12290;&#20107;&#23454;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#36828;&#27809;&#26377;&#23436;&#20840;&#25484;&#25569;&#23398;&#21069;&#25945;&#32946;&#38454;&#27573;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17205v1 Announce Type: cross  Abstract: We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding of multimodal vision-language information of STEM. Our dataset features one of the largest and most comprehensive datasets for the challenge. It includes 448 skills and 1,073,146 questions spanning all STEM subjects. Compared to existing datasets that often focus on examining expert-level ability, our dataset includes fundamental skills and questions designed based on the K-12 curriculum. We also add state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our benchmark. Results show that the recent model advances only help master a very limited number of lower grade-level skills (2.5% in the third grade) in our dataset. In fact, these models are still well bel
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BESA&#30340;&#26032;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20462;&#21098;&#25216;&#26415;&#65292;&#36890;&#36807;&#24212;&#29992;&#20998;&#22359;&#37325;&#26500;&#25439;&#22833;&#65292;&#19982;&#20256;&#32479;&#30340;&#36880;&#23618;&#20462;&#21098;&#25216;&#26415;&#19981;&#21516;&#65292;BESA&#20855;&#26377;&#20248;&#21183;</title><link>https://arxiv.org/abs/2402.16880</link><description>&lt;p&gt;
BESA: &#20351;&#29992;&#20998;&#22359;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#20998;&#37197;&#20462;&#21098;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BESA&#30340;&#26032;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20462;&#21098;&#25216;&#26415;&#65292;&#36890;&#36807;&#24212;&#29992;&#20998;&#22359;&#37325;&#26500;&#25439;&#22833;&#65292;&#19982;&#20256;&#32479;&#30340;&#36880;&#23618;&#20462;&#21098;&#25216;&#26415;&#19981;&#21516;&#65292;BESA&#20855;&#26377;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#25688;&#35201;&#12289;&#25991;&#26412;&#38382;&#31572;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#24615;&#33021;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#30001;&#20110;&#22823;&#37327;&#21442;&#25968;&#36896;&#25104;&#30340;&#35745;&#31639;&#21344;&#29992;&#21487;&#33021;&#26159;&#31105;&#38178;&#30340;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65288;&#22914;SparseGPT&#21644;Wanda&#65289;&#23581;&#35797;&#36890;&#36807;&#26435;&#37325;&#20462;&#21098;&#32531;&#35299;&#27492;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#36880;&#23618;&#26041;&#27861;&#20250;&#23548;&#33268;&#27169;&#22411;&#36755;&#20986;&#26174;&#33879;&#25200;&#21160;&#65292;&#24182;&#38656;&#35201;&#32454;&#33268;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#22914;&#20462;&#21098;&#36895;&#29575;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#20462;&#21098;&#25216;&#26415;&#65292;&#31216;&#20026;&#20998;&#22359;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#20998;&#37197;&#65288;BESA&#65289;&#65292;&#36890;&#36807;&#24212;&#29992;&#20998;&#22359;&#37325;&#26500;&#25439;&#22833;&#12290;&#19982;&#20856;&#22411;&#30340;&#36880;&#23618;&#20462;&#21098;&#25216;&#26415;&#30456;&#27604;&#65292;BESA&#20855;&#26377;&#20004;&#20010;&#29420;&#29305;&#30340;&#29305;&#28857;&#65306;i&#65289;&#23427;&#23450;&#20301;&#20110;&#25972;&#20307;&#20462;&#21098;&#35823;&#24046;&#30456;&#23545;&#20110;&#27599;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16880v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated outstanding performance in various tasks, such as text summarization, text question-answering, and etc. While their performance is impressive, the computational footprint due to their vast number of parameters can be prohibitive. Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance. To address this, this paper introduces a novel LLM pruning technique dubbed blockwise parameter-efficient sparsity allocation (BESA) by applying a blockwise reconstruction loss. In contrast to the typical layer-wise pruning techniques, BESA is characterized by two distinctive attributes: i) it targets the overall pruning error with respect to indi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20613;&#37324;&#21494;&#34920;&#36798;&#24335;&#23548;&#20986;&#23574;&#23792;&#21464;&#25442;&#65292;&#23454;&#29616;&#20102;&#23545;&#21508;&#31181;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#25551;&#36848;&#21644;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.15984</link><description>&lt;p&gt;
&#29992;&#32479;&#19968;&#30340;&#20613;&#37324;&#21494;&#20999;&#29255;&#26041;&#27861;&#23548;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#31181;&#28145;&#24230;-2&#31070;&#32463;&#32593;&#32476;&#30340;&#23574;&#23792;&#21464;&#25442;
&lt;/p&gt;
&lt;p&gt;
A unified Fourier slice method to derive ridgelet transform for a variety of depth-2 neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15984
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20613;&#37324;&#21494;&#34920;&#36798;&#24335;&#23548;&#20986;&#23574;&#23792;&#21464;&#25442;&#65292;&#23454;&#29616;&#20102;&#23545;&#21508;&#31181;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#25551;&#36848;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#26102;&#65292;&#30740;&#31350;&#21442;&#25968;&#20998;&#24067;&#27604;&#30740;&#31350;&#27599;&#20010;&#31070;&#32463;&#20803;&#30340;&#21442;&#25968;&#26356;&#23481;&#26131;&#12290;&#23574;&#23792;&#21464;&#25442;&#26159;&#19968;&#20010;&#20266;&#36870;&#31639;&#23376;&#65292;&#23558;&#32473;&#23450;&#20989;&#25968; $f$ &#26144;&#23556;&#21040;&#21442;&#25968;&#20998;&#24067; $\gamma$&#65292;&#20351;&#24471;&#32593;&#32476; $\mathtt{NN}[\gamma]$ &#33021;&#22815;&#37325;&#29616; $f$&#65292;&#21363; $\mathtt{NN}[\gamma]=f$&#12290;&#22312;&#27431;&#27663;&#31354;&#38388;&#19978;&#30340;&#28145;&#24230;-2&#20840;&#36830;&#25509;&#32593;&#32476;&#20013;&#65292;&#24050;&#21457;&#29616;&#20102;&#23574;&#23792;&#21464;&#25442;&#30340;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#25551;&#36848;&#21442;&#25968;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22810;&#31181;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23578;&#19981;&#30693;&#36947;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20613;&#37324;&#21494;&#34920;&#36798;&#24335;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#23548;&#21508;&#31181;&#29616;&#20195;&#32593;&#32476;&#30340;&#23574;&#23792;&#21464;&#25442;&#65292;&#20363;&#22914;&#26377;&#38480;&#22495; $\mathbb{F}_p$ &#19978;&#30340;&#32593;&#32476;&#12289;&#25277;&#35937;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388; $\mathcal{H}$ &#19978;&#30340;&#32676;&#21367;&#31215;&#32593;&#32476;&#65292;&#20197;&#21450;&#38750;&#32039;&#33268;&#23545;&#31216;&#30340;&#20840;&#36830;&#25509;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15984v1 Announce Type: new  Abstract: To investigate neural network parameters, it is easier to study the distribution of parameters than to study the parameters in each neuron. The ridgelet transform is a pseudo-inverse operator that maps a given function $f$ to the parameter distribution $\gamma$ so that a network $\mathtt{NN}[\gamma]$ reproduces $f$, i.e. $\mathtt{NN}[\gamma]=f$. For depth-2 fully-connected networks on a Euclidean space, the ridgelet transform has been discovered up to the closed-form expression, thus we could describe how the parameters are distributed. However, for a variety of modern neural network architectures, the closed-form expression has not been known. In this paper, we explain a systematic method using Fourier expressions to derive ridgelet transforms for a variety of modern networks such as networks on finite fields $\mathbb{F}_p$, group convolutional networks on abstract Hilbert space $\mathcal{H}$, fully-connected networks on noncompact symm
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#19987;&#27880;&#20110;&#20248;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#20307;&#21147;&#23398;&#20013;&#20027;&#21160;&#27969;&#25511;&#21046;&#20013;&#30340;&#24182;&#34892;&#35774;&#32622;&#65292;&#36890;&#36807;&#25286;&#35299;DRL&#26694;&#26550;&#12289;&#36827;&#34892;&#25193;&#23637;&#24615;&#22522;&#20934;&#27979;&#35797;&#12289;&#25552;&#20986;&#28151;&#21512;&#24182;&#34892;&#21270;&#37197;&#32622;&#24182;&#20248;&#21270;&#22810;&#29615;&#22659;DRL&#35757;&#32451;&#20013;&#30340;I/O&#25805;&#20316;&#65292;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24182;&#34892;&#21270;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.11515</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#20307;&#21147;&#23398;&#20013;&#20027;&#21160;&#27969;&#25511;&#21046;&#20013;&#30340;&#26368;&#20339;&#24182;&#34892;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Optimal Parallelization Strategies for Active Flow Control in Deep Reinforcement Learning-Based Computational Fluid Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11515
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#19987;&#27880;&#20110;&#20248;&#21270;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#27969;&#20307;&#21147;&#23398;&#20013;&#20027;&#21160;&#27969;&#25511;&#21046;&#20013;&#30340;&#24182;&#34892;&#35774;&#32622;&#65292;&#36890;&#36807;&#25286;&#35299;DRL&#26694;&#26550;&#12289;&#36827;&#34892;&#25193;&#23637;&#24615;&#22522;&#20934;&#27979;&#35797;&#12289;&#25552;&#20986;&#28151;&#21512;&#24182;&#34892;&#21270;&#37197;&#32622;&#24182;&#20248;&#21270;&#22810;&#29615;&#22659;DRL&#35757;&#32451;&#20013;&#30340;I/O&#25805;&#20316;&#65292;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24182;&#34892;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#22788;&#29702;&#39640;&#21160;&#24577;&#21644;&#38750;&#32447;&#24615;&#20027;&#21160;&#27969;&#25511;&#21046;&#65288;AFC&#65289;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19982;&#35757;&#32451;DRL&#27169;&#22411;&#30456;&#20851;&#30340;&#35745;&#31639;&#25104;&#26412;&#26500;&#25104;&#20102;&#37325;&#35201;&#30340;&#24615;&#33021;&#29942;&#39048;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#24182;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#26550;&#26500;&#19978;&#23454;&#29616;&#26377;&#25928;&#30340;&#25193;&#23637;&#65292;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#20248;&#21270;&#24182;&#34892;&#35774;&#32622;&#20013;&#30340;&#22522;&#20110;DRL&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#29992;&#20110;AFC&#38382;&#39064;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;DRL&#26694;&#26550;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#25928;&#29575;&#29942;&#39048;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#25286;&#35299;&#25972;&#20307;&#26694;&#26550;&#65292;&#24182;&#20026;&#21508;&#20010;&#32452;&#20214;&#36827;&#34892;&#24191;&#27867;&#30340;&#21487;&#25193;&#23637;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#28151;&#21512;&#24182;&#34892;&#21270;&#37197;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#24182;&#34892;&#21270;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#22810;&#29615;&#22659;DRL&#35757;&#32451;&#20013;&#30340;&#36755;&#20837;/&#36755;&#20986;&#65288;I/O&#65289;&#25805;&#20316;&#65292;&#20197;&#35299;&#20915;&#19982;&#25968;&#25454;&#31227;&#21160;&#30456;&#20851;&#30340;&#20851;&#38190;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11515v1 Announce Type: new  Abstract: Deep Reinforcement Learning (DRL) has emerged as a promising approach for handling highly dynamic and nonlinear Active Flow Control (AFC) problems. However, the computational cost associated with training DRL models presents a significant performance bottleneck. To address this challenge and enable efficient scaling on high-performance computing architectures, this study focuses on optimizing DRL-based algorithms in parallel settings. We validate an existing state-of-the-art DRL framework used for AFC problems and discuss its efficiency bottlenecks. Subsequently, by deconstructing the overall framework and conducting extensive scalability benchmarks for individual components, we investigate various hybrid parallelization configurations and propose efficient parallelization strategies. Moreover, we refine input/output (I/O) operations in multi-environment DRL training to tackle critical overhead associated with data movement. Finally, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#24179;&#22343;&#26657;&#20934;&#24615;&#30340;&#26041;&#27861;&#65292;&#23558;&#26657;&#20934;&#35823;&#24046;&#19982;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20043;&#38388;&#30340;&#24046;&#20540;&#21644;&#23558;&#24179;&#22343;&#24179;&#26041;z-&#20998;&#25968;&#19982;1&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21069;&#32773;&#23545;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#25935;&#24863;&#65292;&#32780;&#21518;&#32773;&#22312;&#35813;&#26041;&#38754;&#25552;&#20379;&#20102;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10043</link><description>&lt;p&gt;
&#22914;&#20309;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#30340;&#24179;&#22343;&#26657;&#20934;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to validate average calibration for machine learning regression tasks ?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#39564;&#35777;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#24179;&#22343;&#26657;&#20934;&#24615;&#30340;&#26041;&#27861;&#65292;&#23558;&#26657;&#20934;&#35823;&#24046;&#19982;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20043;&#38388;&#30340;&#24046;&#20540;&#21644;&#23558;&#24179;&#22343;&#24179;&#26041;z-&#20998;&#25968;&#19982;1&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21069;&#32773;&#23545;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#25935;&#24863;&#65292;&#32780;&#21518;&#32773;&#22312;&#35813;&#26041;&#38754;&#25552;&#20379;&#20102;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#20219;&#21153;&#30340;&#24179;&#22343;&#26657;&#20934;&#24615;&#21487;&#20197;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#36827;&#34892;&#27979;&#35797;&#12290;&#19968;&#31181;&#26041;&#24335;&#26159;&#23558;&#26657;&#20934;&#35823;&#24046;&#65288;CE&#65289;&#20272;&#35745;&#20026;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MSE&#65289;&#19982;&#24179;&#22343;&#26041;&#24046;&#65288;MV&#65289;&#25110;&#24179;&#22343;&#24179;&#26041;&#19981;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#24046;&#20540;&#12290;&#21478;&#19968;&#31181;&#26041;&#24335;&#26159;&#23558;&#24179;&#22343;&#24179;&#26041;z-&#20998;&#25968;&#25110;&#32553;&#25918;&#35823;&#24046;&#65288;ZMS&#65289;&#19982;1&#36827;&#34892;&#27604;&#36739;&#12290;&#20004;&#31181;&#26041;&#27861;&#21487;&#33021;&#24471;&#20986;&#19981;&#21516;&#30340;&#32467;&#35770;&#65292;&#27491;&#22914;&#26469;&#33258;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25991;&#29486;&#20013;&#30340;&#25968;&#25454;&#38598;&#38598;&#21512;&#25152;&#31034;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;CE&#23545;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#38750;&#24120;&#25935;&#24863;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#31163;&#32676;&#19981;&#30830;&#23450;&#24615;&#30340;&#23384;&#22312;&#65292;&#22240;&#27492;&#26080;&#27861;&#21487;&#38752;&#22320;&#29992;&#20110;&#26657;&#20934;&#27979;&#35797;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;ZMS&#32479;&#35745;&#37327;&#19981;&#20855;&#26377;&#36825;&#31181;&#25935;&#24863;&#24615;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#23545;&#26465;&#20214;&#26657;&#20934;&#39564;&#35777;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10043v1 Announce Type: cross  Abstract: Average calibration of the uncertainties of machine learning regression tasks can be tested in two ways. One way is to estimate the calibration error (CE) as the difference between the mean absolute error (MSE) and the mean variance (MV) or mean squared uncertainty. The alternative is to compare the mean squared z-scores or scaled errors (ZMS) to 1. Both approaches might lead to different conclusion, as illustrated on an ensemble of datasets from the recent machine learning uncertainty quantification literature. It is shown here that the CE is very sensitive to the distribution of uncertainties, and notably to the presence of outlying uncertainties, and that it cannot be used reliably for calibration testing. By contrast, the ZMS statistic does not present this sensitivity issue and offers the most reliable approach in this context. Implications for the validation of conditional calibration are discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22320;&#26631;&#21644;&#32858;&#31867;&#30340;&#23618;&#32423;&#20301;&#32622;&#23884;&#20837;&#26041;&#27861;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#39640;&#24230;&#20013;&#24515;&#24230;&#30340;&#33410;&#28857;&#20316;&#20026;&#22320;&#26631;&#21644;&#36827;&#34892;&#22270;&#32858;&#31867;&#65292;&#26412;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#20301;&#32622;&#20449;&#24687;&#23884;&#20837;&#21040;&#22270;&#20013;&#65292;&#25552;&#39640;&#20102;&#38142;&#25509;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08174</link><description>&lt;p&gt;
&#20351;&#29992;&#22320;&#26631;&#21644;&#32858;&#31867;&#30340;&#23618;&#32423;&#20301;&#32622;&#23884;&#20837;&#22270;&#24418;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Position Embedding of Graphs with Landmarks and Clustering for Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22320;&#26631;&#21644;&#32858;&#31867;&#30340;&#23618;&#32423;&#20301;&#32622;&#23884;&#20837;&#26041;&#27861;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#39640;&#24230;&#20013;&#24515;&#24230;&#30340;&#33410;&#28857;&#20316;&#20026;&#22320;&#26631;&#21644;&#36827;&#34892;&#22270;&#32858;&#31867;&#65292;&#26412;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#20301;&#32622;&#20449;&#24687;&#23884;&#20837;&#21040;&#22270;&#20013;&#65292;&#25552;&#39640;&#20102;&#38142;&#25509;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22270;&#20013;&#33410;&#28857;&#30340;&#20301;&#32622;&#20449;&#24687;&#23545;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#20195;&#34920;&#24615;&#33410;&#28857;&#65288;&#31216;&#20026;&#22320;&#26631;&#65289;&#26469;&#34920;&#31034;&#20301;&#32622;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36873;&#25321;&#23569;&#37327;&#20855;&#26377;&#39640;&#24230;&#20013;&#24515;&#24230;&#30340;&#33410;&#28857;&#20316;&#20026;&#22320;&#26631;&#65292;&#23427;&#20204;&#20316;&#20026;&#33410;&#28857;&#20301;&#32622;&#30340;&#21442;&#32771;&#28857;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#36873;&#25321;&#31574;&#30053;&#23545;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#38543;&#26426;&#22270;&#27169;&#22411;&#26159;&#21512;&#29702;&#30340;&#65292;&#24182;&#25512;&#23548;&#20986;&#28041;&#21450;&#22320;&#26631;&#30340;&#24179;&#22343;&#36335;&#24452;&#38271;&#24230;&#30340;&#38381;&#21512;&#24418;&#24335;&#19978;&#30028;&#12290;&#22312;&#24130;&#24459;&#22270;&#30340;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22320;&#26631;&#20026;&#33410;&#28857;&#20043;&#38388;&#36317;&#31163;&#25552;&#20379;&#20102;&#28176;&#36817;&#23436;&#20840;&#20934;&#30830;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#29702;&#35770;&#27934;&#23519;&#21147;&#24212;&#29992;&#20110;&#23454;&#38469;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#22320;&#26631;&#21644;&#32858;&#31867;&#30340;&#23618;&#32423;&#20301;&#32622;&#23884;&#20837;&#65288;HPLC&#65289;&#26041;&#27861;&#12290;HPLC&#23558;&#22320;&#26631;&#36873;&#25321;&#21644;&#22270;&#32858;&#31867;&#30456;&#32467;&#21512;&#65292;&#20854;&#20013;&#22270;&#34987;&#20998;&#21106;&#20026;&#36830;&#36890;&#23494;&#38598;&#30340;&#32858;&#31867;&#65292;&#36873;&#25321;&#20855;&#26377;&#26368;&#39640;&#24230;&#20013;&#24515;&#24230;&#30340;&#33410;&#28857;&#20316;&#20026;&#22320;&#26631;&#12290;HPLC&#21033;&#29992;&#20102;&#22522;&#20110;&#22320;&#26631;&#30340;&#33410;&#28857;&#20301;&#32622;&#20449;&#24687;&#30340;&#23618;&#32423;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning positional information of nodes in a graph is important for link prediction tasks. We propose a representation of positional information using representative nodes called landmarks. A small number of nodes with high degree centrality are selected as landmarks, which serve as reference points for the nodes' positions. We justify this selection strategy for well-known random graph models and derive closed-form bounds on the average path lengths involving landmarks. In a model for power-law graphs, we prove that landmarks provide asymptotically exact information on inter-node distances. We apply theoretical insights to practical networks and propose Hierarchical Position embedding with Landmarks and Clustering (HPLC). HPLC combines landmark selection and graph clustering, where the graph is partitioned into densely connected clusters in which nodes with the highest degree are selected as landmarks. HPLC leverages the positional information of nodes based on landmarks at various l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07876</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#26469;&#25913;&#36827;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Policy Improvement using Language Feedback Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#65292;&#29992;&#20110;&#22312;&#25351;&#20196;&#36981;&#24490;&#20013;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;-&#26377;&#21161;&#20110;&#23454;&#29616;&#25351;&#20196;&#20013;&#25351;&#23450;&#20219;&#21153;&#30340;&#34892;&#21160;-&#20197;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#12290;&#20026;&#20102;&#35757;&#32451;LFMs&#65292;&#25105;&#20204;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33719;&#21462;&#23545;&#35270;&#35273;&#36712;&#36857;&#36827;&#34892;&#35821;&#35328;&#25551;&#36848;&#30340;&#21453;&#39304;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#20351;&#29992;LFMs&#35782;&#21035;&#26399;&#26395;&#27169;&#20223;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#22522;&#30784;&#29615;&#22659;&#65288;Touchdown&#65292;ScienceWorld&#21644;ALFWorld&#65289;&#19978;&#65292;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#19978;&#25913;&#21892;&#20102;&#24378;&#34892;&#20026;&#20811;&#38534;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#19982;LLMs&#30452;&#25509;&#39044;&#27979;&#34892;&#21160;&#30456;&#27604;&#65292;&#20351;&#29992;LFMs&#22312;LLM&#36755;&#20986;&#26631;&#35760;&#30340;&#25968;&#37327;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#31532;&#19977;&#65292;LFMs&#36866;&#24212;&#26410;&#35265;&#29615;&#22659;&#65292;&#36890;&#36807;&#19968;&#36718;&#36866;&#24212;&#20351;&#20219;&#21153;&#23436;&#25104;&#29575;&#25552;&#39640;&#20102;3.5-12.0&#65285;&#12290;&#26368;&#21518;&#65292;&#21487;&#20197;&#20462;&#25913;LFM&#20197;&#25552;&#20379;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#21453;&#39304;&#65292;&#26080;&#38656;&#24615;&#33021;&#25439;&#22833;&#65292;&#20174;&#32780;&#20801;&#35768;&#20154;&#31867;&#39564;&#35777;&#27169;&#20223;&#23398;&#20064;&#30340;&#26399;&#26395;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20219;&#21153;&#26465;&#20214;&#30340;&#33258;&#36866;&#24212;&#22120;&#65292;&#22312;&#22810;&#20219;&#21153;&#31574;&#30053;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#65292;&#24182;&#19988;&#26080;&#38656;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;&#26435;&#37325;&#12290;</title><link>https://arxiv.org/abs/2402.07739</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#31574;&#30053;&#23398;&#20064;&#20013;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#30340;&#35270;&#35273;&#29305;&#24449;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Task-conditioned adaptation of visual features in multi-task policy learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20219;&#21153;&#26465;&#20214;&#30340;&#33258;&#36866;&#24212;&#22120;&#65292;&#22312;&#22810;&#20219;&#21153;&#31574;&#30053;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#65292;&#24182;&#19988;&#26080;&#38656;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#22320;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#26159;&#33258;&#20027;&#20195;&#29702;&#30340;&#26680;&#24515;&#33021;&#21147;&#65292;&#36825;&#38656;&#35201;&#28789;&#27963;&#22320;&#35843;&#25972;&#24213;&#23618;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#24182;&#19988;&#22914;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25152;&#25552;&#20986;&#30340;&#65292;&#36824;&#38656;&#35201;&#35843;&#25972;&#24213;&#23618;&#30340;&#24863;&#30693;&#27169;&#22359;&#12290;&#19968;&#20010;&#31867;&#27604;&#30340;&#35770;&#35777;&#26159;&#20154;&#31867;&#30340;&#35270;&#35273;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#33258;&#19978;&#32780;&#19979;&#30340;&#20449;&#21495;&#26469;&#19987;&#27880;&#20110;&#24403;&#21069;&#20219;&#21153;&#12290;&#31867;&#20284;&#22320;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#22810;&#20219;&#21153;&#31574;&#30053;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#26469;&#35843;&#25972;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#35270;&#35273;&#27169;&#22411;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#30340;&#36866;&#37197;&#22120;&#65292;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#20219;&#20309;&#39044;&#20808;&#35757;&#32451;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;&#36890;&#36807;&#34892;&#20026;&#20811;&#38534;&#35757;&#32451;&#30340;&#21333;&#19968;&#31574;&#30053;&#32467;&#21512;&#20351;&#29992;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#31574;&#30053;&#21644;&#35270;&#35273;&#36866;&#37197;&#22120;&#19978;&#26681;&#25454;&#20219;&#21153;&#23884;&#20837;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#22914;&#26524;&#20219;&#21153;&#26159;&#24050;&#30693;&#30340;&#65292;&#21017;&#21487;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36873;&#25321;&#20219;&#21153;&#23884;&#20837;&#65292;&#21542;&#21017;&#21487;&#20197;&#20174;&#19968;&#32452;&#31034;&#20363;&#28436;&#31034;&#20013;&#36827;&#34892;&#25512;&#26029;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#22312;...&#65288;&#25688;&#35201;&#26410;&#23436;&#25104;&#65289;
&lt;/p&gt;
&lt;p&gt;
Successfully addressing a wide variety of tasks is a core ability of autonomous agents, which requires flexibly adapting the underlying decision-making strategies and, as we argue in this work, also adapting the underlying perception modules. An analogical argument would be the human visual system, which uses top-down signals to focus attention determined by the current task. Similarly, in this work, we adapt pre-trained large vision models conditioned on specific downstream tasks in the context of multi-task policy learning. We introduce task-conditioned adapters that do not require finetuning any pre-trained weights, combined with a single policy trained with behavior cloning and capable of addressing multiple tasks. We condition the policy and visual adapters on task embeddings, which can be selected at inference if the task is known, or alternatively inferred from a set of example demonstrations. To this end, we propose a new optimization-based estimator. We evaluate the method on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;SHAP&#21644;NSHAP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29305;&#24449;&#20998;&#25104;&#26174;&#33879;&#20132;&#20114;&#30340;&#37096;&#20998;&#65292;&#26500;&#24314;&#20102;&#19968;&#31181;&#31616;&#26126;&#12289;&#26131;&#35299;&#37322;&#30340;&#21152;&#24615;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#26816;&#39564;&#21098;&#26525;&#27425;&#20248;&#35299;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#36816;&#34892;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05566</link><description>&lt;p&gt;
&#31616;&#26126;&#32771;&#34385;&#20132;&#20114;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Succint Interaction-Aware Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;SHAP&#21644;NSHAP&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#29305;&#24449;&#20998;&#25104;&#26174;&#33879;&#20132;&#20114;&#30340;&#37096;&#20998;&#65292;&#26500;&#24314;&#20102;&#19968;&#31181;&#31616;&#26126;&#12289;&#26131;&#35299;&#37322;&#30340;&#21152;&#24615;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#26816;&#39564;&#21098;&#26525;&#27425;&#20248;&#35299;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#36816;&#34892;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SHAP&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#35299;&#37322;&#40657;&#31665;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25581;&#31034;&#21508;&#20010;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#26469;&#36827;&#34892;&#35299;&#37322;&#12290;&#30001;&#20110;&#24573;&#30053;&#20102;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;SHAP&#30340;&#35299;&#37322;&#21487;&#33021;&#20250;&#20196;&#20154;&#22256;&#24785;&#29978;&#33267;&#35823;&#23548;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;NSHAP&#25253;&#21578;&#20102;&#25152;&#26377;&#29305;&#24449;&#23376;&#38598;&#30340;&#21152;&#24615;&#37325;&#35201;&#24615;&#12290;&#34429;&#28982;&#36825;&#21253;&#21547;&#20102;&#25152;&#26377;&#30456;&#20114;&#20316;&#29992;&#30340;&#29305;&#24449;&#38598;&#65292;&#20294;&#20063;&#23548;&#33268;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#22823;&#23567;&#30340;&#38590;&#20197;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#36825;&#20004;&#20010;&#26041;&#27861;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#30340;&#26041;&#27861;&#65292;&#23558;&#29305;&#24449;&#20998;&#25104;&#26174;&#33879;&#20132;&#20114;&#30340;&#37096;&#20998;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#37096;&#20998;&#26500;&#25104;&#31616;&#26126;&#12289;&#26131;&#35299;&#37322;&#30340;&#21152;&#24615;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#26469;&#34913;&#37327;&#36825;&#31181;&#20998;&#21306;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#20195;&#34920;&#24615;&#65292;&#25240;&#34935;&#20110;&#24471;&#21040;&#30340;&#35299;&#37322;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#39640;&#25928;&#22320;&#20174;&#36229;&#25351;&#25968;&#25968;&#37327;&#20013;&#25214;&#21040;&#26368;&#20339;&#20998;&#21306;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#32479;&#35745;&#26816;&#39564;&#26469;&#21098;&#26525;&#27425;&#20248;&#35299;&#65292;&#19981;&#20165;&#25552;&#39640;&#20102;&#36816;&#34892;&#26102;&#38388;&#65292;&#36824;&#26377;&#21161;&#20110;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
SHAP is a popular approach to explain black-box models by revealing the importance of individual features. As it ignores feature interactions, SHAP explanations can be confusing up to misleading. NSHAP, on the other hand, reports the additive importance for all subsets of features. While this does include all interacting sets of features, it also leads to an exponentially sized, difficult to interpret explanation. In this paper, we propose to combine the best of these two worlds, by partitioning the features into parts that significantly interact, and use these parts to compose a succinct, interpretable, additive explanation. We derive a criterion by which to measure the representativeness of such a partition for a models behavior, traded off against the complexity of the resulting explanation. To efficiently find the best partition out of super-exponentially many, we show how to prune sub-optimal solutions using a statistical test, which not only improves runtime but also helps to det
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20219;&#24847;&#20998;&#24067;&#35745;&#31639;&#20026;&#39044;&#23450;&#20041;&#20998;&#24067;&#30340;&#37327;&#23376;&#26631;&#20934;&#21270;&#27969;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2402.02866</link><description>&lt;p&gt;
&#37327;&#23376;&#26631;&#20934;&#21270;&#27969;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Quantum Normalizing Flows for Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20219;&#24847;&#20998;&#24067;&#35745;&#31639;&#20026;&#39044;&#23450;&#20041;&#20998;&#24067;&#30340;&#37327;&#23376;&#26631;&#20934;&#21270;&#27969;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#21270;&#27969;&#23558;&#20219;&#24847;&#20998;&#24067;&#35745;&#31639;&#20026;&#39044;&#23450;&#20041;&#65288;&#20363;&#22914;&#27491;&#24577;&#65289;&#20998;&#24067;&#30340;&#21452;&#23556;&#26144;&#23556;&#12290;&#19968;&#26086;&#23398;&#20064;&#21040;&#36825;&#26679;&#30340;&#26144;&#23556;&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#24322;&#24120;&#26816;&#27979;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#37327;&#23376;&#26550;&#26500;&#30340;&#26631;&#20934;&#21270;&#27969;&#65292;&#25551;&#36848;&#20102;&#22914;&#20309;&#24314;&#27169;&#21644;&#20248;&#21270;&#36825;&#26679;&#30340;&#27969;&#65292;&#24182;&#22312;&#31034;&#20363;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20986;&#19982;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;&#23396;&#31435;&#26862;&#26519;&#12289;&#23616;&#37096;&#31163;&#32676;&#22240;&#23376;&#65288;LOF&#65289;&#25110;&#21333;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#65289;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#23436;&#20840;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Normalizing Flow computes a bijective mapping from an arbitrary distribution to a predefined (e.g. normal) distribution. Such a flow can be used to address different tasks, e.g. anomaly detection, once such a mapping has been learned. In this work we introduce Normalizing Flows for Quantum architectures, describe how to model and optimize such a flow and evaluate our method on example datasets. Our proposed models show competitive performance for anomaly detection compared to classical methods, e.g. based on isolation forests, the local outlier factor (LOF) or single-class SVMs, while being fully executable on a quantum computer.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;EBCL&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#20013;&#20851;&#38190;&#20107;&#20214;&#21069;&#21518;&#30340;&#25968;&#25454;&#32534;&#30721;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;EBCL&#33021;&#22815;&#20135;&#29983;&#24615;&#33021;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#23545;&#20110;&#24515;&#21147;&#34928;&#31469;&#38431;&#21015;&#30340;&#20851;&#38190;&#19979;&#28216;&#20219;&#21153;&#20855;&#26377;&#26356;&#22909;&#30340;&#24494;&#35843;&#24615;&#33021;&#65292;&#24182;&#33021;&#26377;&#25928;&#22320;&#23558;&#20855;&#26377;&#30456;&#20284;&#39118;&#38505;&#30340;&#24739;&#32773;&#36827;&#34892;&#32858;&#31867;&#12290;</title><link>https://arxiv.org/abs/2312.10308</link><description>&lt;p&gt;
&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Event-Based Contrastive Learning for Medical Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;EBCL&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#20013;&#20851;&#38190;&#20107;&#20214;&#21069;&#21518;&#30340;&#25968;&#25454;&#32534;&#30721;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;EBCL&#33021;&#22815;&#20135;&#29983;&#24615;&#33021;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#23545;&#20110;&#24515;&#21147;&#34928;&#31469;&#38431;&#21015;&#30340;&#20851;&#38190;&#19979;&#28216;&#20219;&#21153;&#20855;&#26377;&#26356;&#22909;&#30340;&#24494;&#35843;&#24615;&#33021;&#65292;&#24182;&#33021;&#26377;&#25928;&#22320;&#23558;&#20855;&#26377;&#30456;&#20284;&#39118;&#38505;&#30340;&#24739;&#32773;&#36827;&#34892;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#32463;&#24120;&#38656;&#35201;&#30830;&#23450;&#26576;&#20010;&#20851;&#38190;&#21307;&#23398;&#20107;&#20214;&#21518;&#24739;&#32773;&#26159;&#21542;&#22788;&#20110;&#19981;&#33391;&#32467;&#26524;&#30340;&#39640;&#39118;&#38505;&#29366;&#24577;&#65292;&#20363;&#22914;&#24515;&#21147;&#34928;&#31469;&#20837;&#38498;&#21518;&#30340;&#30701;&#26399;&#27515;&#20129;&#39118;&#38505;&#12290;&#36825;&#20010;&#20219;&#21153;&#30001;&#20110;&#38271;&#26399;&#21307;&#23398;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12289;&#21464;&#24322;&#24615;&#21644;&#24322;&#36136;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20687;&#24515;&#21147;&#34928;&#31469;&#36825;&#26679;&#30340;&#24930;&#24615;&#30142;&#30149;&#24739;&#32773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20107;&#20214;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;EBCL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#19981;&#21516;&#31867;&#22411;&#24739;&#32773;&#25968;&#25454;&#30340;&#23884;&#20837;&#34920;&#31034;&#65292;&#20197;&#20445;&#30041;&#20851;&#38190;&#32034;&#24341;&#20107;&#20214;&#21069;&#21518;&#30340;&#26102;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;EBCL&#20135;&#29983;&#30340;&#27169;&#22411;&#22312;&#24515;&#21147;&#34928;&#31469;&#38431;&#21015;&#30340;&#20851;&#38190;&#19979;&#28216;&#20219;&#21153;&#65288;&#21253;&#25324;30&#22825;&#20877;&#20837;&#38498;&#12289;1&#24180;&#27515;&#20129;&#29575;&#21644;1&#21608;&#20303;&#38498;&#22825;&#25968;&#65289;&#30340;&#24494;&#35843;&#24615;&#33021;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;EBCL&#39044;&#35757;&#32451;&#21333;&#29420;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#20855;&#26377;&#30456;&#20284;&#27515;&#20129;&#29575;&#21644;&#20877;&#20837;&#38498;&#39118;&#38505;&#30340;&#24739;&#32773;&#36827;&#34892;&#32858;&#31867;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In clinical practice, one often needs to identify whether a patient is at high risk of adverse outcomes after some key medical event; for example, the short-term risk of death after an admission for heart failure. This task is challenging due to the complexity, variability, and heterogeneity of longitudinal medical data, especially for individuals suffering from chronic diseases like heart failure. In this paper, we introduce Event-Based Contrastive Learning (EBCL), a method for learning embeddings of heterogeneous patient data that preserves temporal information before and after key index events. We demonstrate that EBCL produces models that yield better fine-tuning performance on critical downstream tasks for a heart failure cohort, including 30-day readmission, 1-year mortality, and 1-week length of stay, relative to other pretraining methods. Our findings also reveal that EBCL pretraining alone can effectively cluster patients with similar mortality and readmission risks, offering 
&lt;/p&gt;</description></item><item><title>FedSSA&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24230;&#30340;&#32858;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#12290;&#23427;&#36890;&#36807;&#24322;&#26500; feature extractor &#21644;&#21516;&#36136; classification header &#23558;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#25286;&#20998;&#65292;&#24182;&#36890;&#36807;&#35821;&#20041;&#30456;&#20284;&#24230;&#36827;&#34892;&#22836;&#37096;&#21442;&#25968;&#32858;&#21512;&#23454;&#29616;&#26412;&#22320;&#21040;&#20840;&#23616;&#30340;&#30693;&#35782;&#20256;&#36755;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21442;&#25968;&#31283;&#23450;&#31574;&#30053;&#23454;&#29616;&#20102;&#20840;&#23616;&#21040;&#26412;&#22320;&#30340;&#30693;&#35782;&#20256;&#36755;&#12290;</title><link>https://arxiv.org/abs/2312.09006</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24230;&#30340;&#32858;&#21512;&#30340;FedSSA: &#29992;&#20110;&#39640;&#25928;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedSSA: Semantic Similarity-based Aggregation for Efficient Model-Heterogeneous Personalized Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09006
&lt;/p&gt;
&lt;p&gt;
FedSSA&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24230;&#30340;&#32858;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#12290;&#23427;&#36890;&#36807;&#24322;&#26500; feature extractor &#21644;&#21516;&#36136; classification header &#23558;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#25286;&#20998;&#65292;&#24182;&#36890;&#36807;&#35821;&#20041;&#30456;&#20284;&#24230;&#36827;&#34892;&#22836;&#37096;&#21442;&#25968;&#32858;&#21512;&#23454;&#29616;&#26412;&#22320;&#21040;&#20840;&#23616;&#30340;&#30693;&#35782;&#20256;&#36755;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21442;&#25968;&#31283;&#23450;&#31574;&#30053;&#23454;&#29616;&#20102;&#20840;&#23616;&#21040;&#26412;&#22320;&#30340;&#30693;&#35782;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#12290;&#20256;&#32479;&#30340;FL&#35201;&#27714;&#25152;&#26377;&#25968;&#25454;&#25152;&#26377;&#32773;&#65288;&#21363;FL&#23458;&#25143;&#31471;&#65289;&#35757;&#32451;&#30456;&#21516;&#30340;&#26412;&#22320;&#27169;&#22411;&#12290;&#36825;&#31181;&#35774;&#35745;&#24182;&#19981;&#36866;&#29992;&#20110;&#28041;&#21450;&#25968;&#25454;&#21644;/&#25110;&#31995;&#32479;&#24322;&#26500;&#30340;&#22330;&#26223;&#12290;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;FL&#65288;MHPFL&#65289;&#24050;&#32463;&#20986;&#29616;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;MHPFL&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20855;&#26377;&#30456;&#21516;&#23398;&#20064;&#20219;&#21153;&#24615;&#36136;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#25110;&#32773;&#20250;&#20135;&#29983;&#39640;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Federated Semantic Similarity Aggregation&#65288;FedSSA&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#20998;&#20026;&#24322;&#26500;&#65288;&#32467;&#26500;&#19981;&#21516;&#65289;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#21516;&#36136;&#65288;&#32467;&#26500;&#30456;&#21516;&#65289;&#20998;&#31867;&#22836;&#37096;&#12290;&#23427;&#36890;&#36807;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24230;&#30340;&#22836;&#37096;&#21442;&#25968;&#32858;&#21512;&#23454;&#29616;&#20102;&#26412;&#22320;&#21040;&#20840;&#23616;&#30340;&#30693;&#35782;&#20256;&#36755;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21442;&#25968;&#31283;&#23450;&#31574;&#30053;&#23454;&#29616;&#20102;&#20840;&#23616;&#21040;&#26412;&#22320;&#30340;&#30693;&#35782;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a privacy-preserving collaboratively machine learning paradigm. Traditional FL requires all data owners (a.k.a. FL clients) to train the same local model. This design is not well-suited for scenarios involving data and/or system heterogeneity. Model-Heterogeneous Personalized FL (MHPFL) has emerged to address this challenge. Existing MHPFL approaches often rely on having a public dataset with the same nature of the learning task, or incur high computation and communication costs. To address these limitations, we propose the Federated Semantic Similarity Aggregation (FedSSA) approach, which splits each client's model into a heterogeneous (structure-different) feature extractor and a homogeneous (structure-same) classification header. It performs local-to-global knowledge transfer via semantic similarity-based header parameter aggregation. In addition, global-to-local knowledge transfer is achieved via an adaptive parameter stabilization strategy which fuses th
&lt;/p&gt;</description></item><item><title>RefinedFields&#26159;&#31532;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#25913;&#21892;&#26080;&#32422;&#26463;&#22330;&#26223;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20248;&#21270;&#25351;&#23548;&#21644;&#20132;&#26367;&#35757;&#32451;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#30340;&#20808;&#39564;&#26465;&#20214;&#20013;&#25552;&#21462;&#26356;&#20016;&#23500;&#30340;&#32454;&#33410;&#65292;&#24182;&#22312;&#26032;&#35270;&#35282;&#21512;&#25104;&#20219;&#21153;&#20013;&#20248;&#20110;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.00639</link><description>&lt;p&gt;
RefinedFields: &#23545;&#26080;&#32422;&#26463;&#22330;&#26223;&#30340;&#36752;&#23556;&#22330;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
RefinedFields: Radiance Fields Refinement for Unconstrained Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00639
&lt;/p&gt;
&lt;p&gt;
RefinedFields&#26159;&#31532;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#25913;&#21892;&#26080;&#32422;&#26463;&#22330;&#26223;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20248;&#21270;&#25351;&#23548;&#21644;&#20132;&#26367;&#35757;&#32451;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#30340;&#20808;&#39564;&#26465;&#20214;&#20013;&#25552;&#21462;&#26356;&#20016;&#23500;&#30340;&#32454;&#33410;&#65292;&#24182;&#22312;&#26032;&#35270;&#35282;&#21512;&#25104;&#20219;&#21153;&#20013;&#20248;&#20110;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26080;&#32422;&#26463;&#30340;&#22270;&#20687;&#20013;&#24314;&#27169;&#22823;&#22330;&#26223;&#34987;&#35777;&#26126;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#22788;&#29702;&#37326;&#22806;&#22330;&#26223;&#24314;&#27169;&#26159;&#22312;&#23553;&#38381;&#30340;&#29615;&#22659;&#20013;&#65292;&#27809;&#26377;&#23545;&#20174;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#33719;&#24471;&#30340;&#20808;&#39564;&#26465;&#20214;&#36827;&#34892;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RefinedFields&#65292;&#36825;&#26159;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25913;&#21892;&#37326;&#22806;&#22330;&#26223;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#32593;&#32476;&#36890;&#36807;&#20248;&#21270;&#25351;&#23548;&#20351;&#29992;&#20132;&#26367;&#35757;&#32451;&#36807;&#31243;&#26469;&#32454;&#21270;K-Planes&#34920;&#31034;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#25105;&#20204;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#26053;&#28216;&#29031;&#29255;&#38598;&#19978;&#30340;&#20248;&#28857;&#12290;RefinedFields&#22686;&#24378;&#20102;&#28210;&#26579;&#22330;&#26223;&#30340;&#32454;&#33410;&#65292;&#20248;&#20110;&#20197;&#24448;&#22312;&#37326;&#22806;&#36827;&#34892;&#26032;&#35270;&#35282;&#21512;&#25104;&#20219;&#21153;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#39029;&#38754;&#21487;&#20197;&#22312;https://refinedfields.github.io&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling large scenes from unconstrained images has proven to be a major challenge in computer vision. Existing methods tackling in-the-wild scene modeling operate in closed-world settings, where no conditioning on priors acquired from real-world images is present. We propose RefinedFields, which is, to the best of our knowledge, the first method leveraging pre-trained models to improve in-the-wild scene modeling. We employ pre-trained networks to refine K-Planes representations via optimization guidance using an alternating training procedure. We carry out extensive experiments and verify the merit of our method on synthetic data and real tourism photo collections. RefinedFields enhances rendered scenes with richer details and outperforms previous work on the task of novel view synthesis in the wild. Our project page can be found at https://refinedfields.github.io .
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#22825;&#27668;&#39044;&#25253;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#22823;&#31361;&#30772;&#65292;&#23427;&#20811;&#26381;&#20102;&#20256;&#32479;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#26377;&#28508;&#21147;&#24341;&#39046;&#22825;&#27668;&#39044;&#25253;&#30340;&#31532;&#20108;&#27425;&#38761;&#21629;&#12290;</title><link>http://arxiv.org/abs/2401.16669</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26159;&#21542;&#20026;&#22825;&#27668;&#39044;&#25253;&#24102;&#26469;&#20102;&#31532;&#20108;&#27425;&#38761;&#21629;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Artificial Intelligence Providing the Second Revolution for Weather Forecasting?. (arXiv:2401.16669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16669
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#22825;&#27668;&#39044;&#25253;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#22823;&#31361;&#30772;&#65292;&#23427;&#20811;&#26381;&#20102;&#20256;&#32479;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#26377;&#28508;&#21147;&#24341;&#39046;&#22825;&#27668;&#39044;&#25253;&#30340;&#31532;&#20108;&#27425;&#38761;&#21629;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#36817;&#24180;&#26469;&#65292;&#23548;&#33268;&#20102;&#20960;&#31181;&#22823;&#21442;&#25968;&#20154;&#24037;&#26234;&#33021;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#36825;&#20123;&#27169;&#22411;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#22823;&#31361;&#30772;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#34920;&#26126;&#20102;&#22825;&#27668;&#39044;&#25253;&#21487;&#33021;&#36814;&#26469;&#31532;&#20108;&#27425;&#38761;&#21629;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#20123;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#39044;&#25253;&#27169;&#22411;&#30340;&#28436;&#21464;&#65292;&#24182;&#22312;&#30830;&#23450;&#30340;&#20849;&#21516;&#28857;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#23427;&#20204;&#30340;&#21457;&#23637;&#30340;&#8220;&#19977;&#22823;&#35268;&#21017;&#8221;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#38761;&#21629;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#31616;&#35201;&#27010;&#36848;&#20102;&#28508;&#22312;&#30340;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#26410;&#26469;&#21457;&#23637;&#21069;&#26223;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#23558;&#25972;&#20010;&#25968;&#20540;&#39044;&#25253;&#36807;&#31243;&#36827;&#34892;&#25972;&#21512;&#12290;&#36890;&#36807;&#23558;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#19982;&#20854;&#20182;&#20449;&#24687;&#32508;&#21512;&#65292;&#32473;&#20986;&#20102;&#19968;&#20010;&#24212;&#29992;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of artificial intelligence technologies, particularly in recent years, has led to the emergence of several large parameter artificial intelligence weather forecast models. These models represent a significant breakthrough, overcoming the limitations of traditional numerical weather prediction models and indicating a potential second revolution for weather forecast. This study explores the evolution of these advanced artificial intelligence forecast models, and based on the identified commonalities, proposes the "Three Large Rules" for their development. We discuss the potential of artificial intelligence in revolutionizing numerical weather prediction, briefly outlining the underlying reasons for this potential. Additionally, we explore key areas for future development prospects for large artificial intelligence weather forecast models, integrating the entire numerical prediction process. Through an example that combines a large artificial intelligence model with 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#22522;&#20110;OSINT&#30340;&#32593;&#32476;&#23041;&#32961;&#24847;&#35782;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#32593;&#32476;&#23433;&#20840;&#30340;&#20108;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.15127</link><description>&lt;p&gt;
&#35780;&#20272;&#29992;&#20110;&#22522;&#20110;OSINT&#30340;&#32593;&#32476;&#23041;&#32961;&#24847;&#35782;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Evaluation of LLM Chatbots for OSINT-based Cyberthreat Awareness. (arXiv:2401.15127v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#22522;&#20110;OSINT&#30340;&#32593;&#32476;&#23041;&#32961;&#24847;&#35782;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#32593;&#32476;&#23433;&#20840;&#30340;&#20108;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#65292;&#20851;&#20110;&#26032;&#20852;&#23041;&#32961;&#30340;&#30693;&#35782;&#20849;&#20139;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#26500;&#25104;&#20102;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#30340;&#22522;&#30784;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#26426;&#36935;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#12289;GPT4all&#12289;Dolly&#12289;Stanford Alpaca&#12289;Alpaca-LoRA&#21644;Falcon&#31561;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#35782;&#21035;&#24320;&#28304;&#24773;&#25253;&#20013;&#19982;&#32593;&#32476;&#23433;&#20840;&#30456;&#20851;&#30340;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#29616;&#26377;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20108;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20316;&#20026;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20174;Twitter&#25910;&#38598;&#30340;&#32463;&#36807;&#20805;&#20998;&#39564;&#35777;&#30340;&#25968;&#25454;&#65292;&#35813;&#25968;&#25454;&#26469;&#28304;&#20110;&#20197;&#24448;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#30340;&#20108;&#20998;&#31867;&#38382;&#39064;&#26041;&#38754;&#65292;&#21830;&#19994;&#27169;&#22411;Chatbot GPT-4&#23454;&#29616;&#20102;&#21487;&#25509;&#21463;&#30340;F1&#20998;&#25968;0.94&#65292;&#32780;&#24320;&#28304;&#27169;&#22411;GPT4all&#23454;&#29616;&#20102;F1&#20998;&#25968;0.90&#12290;&#28982;&#32780;&#65292;&#23601;&#32593;&#32476;&#23433;&#20840;&#23454;&#20307;&#35782;&#21035;&#32780;&#35328;&#65292;
&lt;/p&gt;
&lt;p&gt;
Knowledge sharing about emerging threats is crucial in the rapidly advancing field of cybersecurity and forms the foundation of Cyber Threat Intelligence. In this context, Large Language Models are becoming increasingly significant in the field of cybersecurity, presenting a wide range of opportunities. This study explores the capability of chatbots such as ChatGPT, GPT4all, Dolly,Stanford Alpaca, Alpaca-LoRA, and Falcon to identify cybersecurity-related text within Open Source Intelligence. We assess the capabilities of existing chatbot models for Natural Language Processing tasks. We consider binary classification and Named Entity Recognition as tasks. This study analyzes well-established data collected from Twitter, derived from previous research efforts. Regarding cybersecurity binary classification, Chatbot GPT-4 as a commercial model achieved an acceptable F1-score of 0.94, and the open-source GPT4all model achieved an F1-score of 0.90. However, concerning cybersecurity entity re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24418;&#24335;&#21270;&#39564;&#35777;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#31616;&#21270;&#39564;&#35777;&#36807;&#31243;&#24182;&#26377;&#25928;&#35757;&#32451;&#20986;&#26131;&#20110;&#39564;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2401.14961</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-To-End Set-Based Training for Neural Network Verification. (arXiv:2401.14961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24418;&#24335;&#21270;&#39564;&#35777;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#31616;&#21270;&#39564;&#35777;&#36807;&#31243;&#24182;&#26377;&#25928;&#35757;&#32451;&#20986;&#26131;&#20110;&#39564;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#21363;&#24494;&#23567;&#30340;&#36755;&#20837;&#25200;&#21160;&#21487;&#33021;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#20135;&#29983;&#37325;&#22823;&#21464;&#21270;&#12290;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#38656;&#35201;&#23545;&#36755;&#20837;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#21644;&#24418;&#24335;&#21270;&#39564;&#35777;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#39318;&#27425;&#37319;&#29992;&#31471;&#21040;&#31471;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#35813;&#35757;&#32451;&#26041;&#27861;&#33021;&#22815;&#35757;&#32451;&#20986;&#21487;&#36827;&#34892;&#24418;&#24335;&#21270;&#39564;&#35777;&#30340;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#33021;&#22815;&#22823;&#22823;&#31616;&#21270;&#24050;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#32493;&#24418;&#24335;&#21270;&#40065;&#26834;&#24615;&#39564;&#35777;&#36807;&#31243;&#12290;&#30456;&#27604;&#20110;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#38598;&#21512;&#30340;&#35745;&#31639;&#26469;&#35757;&#32451;&#25972;&#20010;&#25200;&#21160;&#36755;&#20837;&#38598;&#21512;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35757;&#32451;&#20986;&#26131;&#20110;&#39564;&#35777;&#30340;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are vulnerable to adversarial attacks, i.e., small input perturbations can result in substantially different outputs of a neural network. Safety-critical environments require neural networks that are robust against input perturbations. However, training and formally verifying robust neural networks is challenging. We address this challenge by employing, for the first time, a end-to-end set-based training procedure that trains robust neural networks for formal verification. Our training procedure drastically simplifies the subsequent formal robustness verification of the trained neural network. While previous research has predominantly focused on augmenting neural network training with adversarial attacks, our approach leverages set-based computing to train neural networks with entire sets of perturbed inputs. Moreover, we demonstrate that our set-based training procedure effectively trains robust neural networks, which are easier to verify. In many cases, set-based trai
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24130;&#28388;&#27874;&#22120;&#31070;&#32463;&#32593;&#32476; (GPFN)&#65292;&#36890;&#36807;&#20351;&#29992;&#24130;&#32423;&#25968;&#22270;&#28388;&#27874;&#22120;&#26469;&#22686;&#24378;&#33410;&#28857;&#20998;&#31867;&#12290;GPFN&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#25910;&#25947;&#24130;&#32423;&#25968;&#30340;&#20855;&#26377;&#26080;&#38480;&#25509;&#25910;&#22495;&#30340;&#22270;&#28388;&#27874;&#22120;&#26500;&#24314;&#26041;&#27861;&#65292;&#24182;&#33021;&#38598;&#25104;&#20219;&#20309;&#24130;&#32423;&#25968;&#24182;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.09943</link><description>&lt;p&gt;
&#26080;&#38480;&#26102;&#22495;&#22270;&#28388;&#27874;&#22120;&#65306;&#21033;&#29992;&#24130;&#32423;&#25968;&#22686;&#24378;&#31232;&#30095;&#20449;&#24687;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Infinite-Horizon Graph Filters: Leveraging Power Series to Enhance Sparse Information Aggregation. (arXiv:2401.09943v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09943
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24130;&#28388;&#27874;&#22120;&#31070;&#32463;&#32593;&#32476; (GPFN)&#65292;&#36890;&#36807;&#20351;&#29992;&#24130;&#32423;&#25968;&#22270;&#28388;&#27874;&#22120;&#26469;&#22686;&#24378;&#33410;&#28857;&#20998;&#31867;&#12290;GPFN&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#25910;&#25947;&#24130;&#32423;&#25968;&#30340;&#20855;&#26377;&#26080;&#38480;&#25509;&#25910;&#22495;&#30340;&#22270;&#28388;&#27874;&#22120;&#26500;&#24314;&#26041;&#27861;&#65292;&#24182;&#33021;&#38598;&#25104;&#20219;&#20309;&#24130;&#32423;&#25968;&#24182;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#30456;&#24403;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#24120;&#24120;&#21463;&#21040;&#26377;&#38480;&#25509;&#25910;&#22495;&#30340;&#38480;&#21046;&#65292;&#22312;&#31232;&#30095;&#22270;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#25361;&#25112;&#21464;&#24471;&#26356;&#21152;&#20005;&#23803;&#12290;&#37492;&#20110;&#20855;&#26377;&#26080;&#38480;&#25193;&#23637;&#33021;&#21147;&#30340;&#24130;&#32423;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#24130;&#28388;&#27874;&#22120;&#31070;&#32463;&#32593;&#32476; (GPFN)&#65292;&#36890;&#36807;&#20351;&#29992;&#24130;&#32423;&#25968;&#22270;&#28388;&#27874;&#22120;&#26469;&#22686;&#24378;&#33410;&#28857;&#20998;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;GPFN&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#25910;&#25947;&#24130;&#32423;&#25968;&#30340;&#20855;&#26377;&#26080;&#38480;&#25509;&#25910;&#22495;&#30340;&#22270;&#28388;&#27874;&#22120;&#26500;&#24314;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#39057;&#35889;&#21644;&#31354;&#38388;&#22495;&#20013;&#36827;&#34892;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;GPFN&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#38598;&#25104;&#20219;&#20309;&#24130;&#32423;&#25968;&#24182;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;GPFN&#22312;&#31232;&#30095;&#22270;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have shown considerable effectiveness in a variety of graph learning tasks, particularly those based on the message-passing approach in recent years. However, their performance is often constrained by a limited receptive field, a challenge that becomes more acute in the presence of sparse graphs. In light of the power series, which possesses infinite expansion capabilities, we propose a novel \underline{G}raph \underline{P}ower \underline{F}ilter \underline{N}eural Network (GPFN) that enhances node classification by employing a power series graph filter to augment the receptive field. Concretely, our GPFN designs a new way to build a graph filter with an infinite receptive field based on the convergence power series, which can be analyzed in the spectral and spatial domains. Besides, we theoretically prove that our GPFN is a general framework that can integrate any power series and capture long-range dependencies. Finally, experimental results on three data
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;FESS-GDA&#65292;&#21033;&#29992;&#24179;&#28369;&#25216;&#26415;&#36827;&#34892;&#32852;&#37030;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#12290;&#36890;&#36807;&#35299;&#20915;&#19981;&#21516;&#31867;&#22411;&#30340;&#32852;&#37030;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;FESS-GDA&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#23454;&#38469;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.00944</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#24179;&#28369;&#26799;&#24230;&#19978;&#21319;&#19979;&#38477;&#27861;&#30340;&#32852;&#37030;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Stochastic Smoothed Gradient Descent Ascent for Federated Minimax Optimization. (arXiv:2311.00944v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;FESS-GDA&#65292;&#21033;&#29992;&#24179;&#28369;&#25216;&#26415;&#36827;&#34892;&#32852;&#37030;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#12290;&#36890;&#36807;&#35299;&#20915;&#19981;&#21516;&#31867;&#22411;&#30340;&#32852;&#37030;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;FESS-GDA&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#23454;&#38469;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#32852;&#37030;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#22312;&#38598;&#20013;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#20013;&#65292;&#24179;&#28369;&#20132;&#26367;&#26799;&#24230;&#19978;&#21319;&#19979;&#38477;&#65288;Smoothed-AGDA&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#25104;&#21151;&#20043;&#22788;&#65292;&#20294;&#24179;&#28369;&#25216;&#26415;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#30340;&#20316;&#29992;&#21644;&#26159;&#21542;&#26377;&#25152;&#24110;&#21161;&#23578;&#26410;&#34987;&#25506;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#31216;&#20026;&#32852;&#37030;&#38543;&#26426;&#24179;&#28369;&#26799;&#24230;&#19978;&#21319;&#19979;&#38477;&#65288;FESS-GDA&#65289;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#24179;&#28369;&#25216;&#26415;&#36827;&#34892;&#32852;&#37030;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;FESS-GDA&#21487;&#20197;&#32479;&#19968;&#35299;&#20915;&#20960;&#31867;&#32852;&#37030;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#24182;&#20026;&#36825;&#20123;&#35774;&#32622;&#25552;&#20379;&#20102;&#26032;&#30340;&#25110;&#26356;&#22909;&#30340;&#25910;&#25947;&#32467;&#26524;&#20998;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;FESS-GDA&#22312;&#23454;&#38469;&#32852;&#37030;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#35757;&#32451;&#21644;&#20844;&#24179;&#20998;&#31867;&#20013;&#30340;&#23454;&#38469;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, federated minimax optimization has attracted growing interest due to its extensive applications in various machine learning tasks. While Smoothed Alternative Gradient Descent Ascent (Smoothed-AGDA) has proved its success in centralized nonconvex minimax optimization, how and whether smoothing technique could be helpful in federated setting remains unexplored. In this paper, we propose a new algorithm termed Federated Stochastic Smoothed Gradient Descent Ascent (FESS-GDA), which utilizes the smoothing technique for federated minimax optimization. We prove that FESS-GDA can be uniformly used to solve several classes of federated minimax problems and prove new or better analytical convergence results for these settings. We showcase the practical efficiency of FESS-GDA in practical federated learning tasks of training generative adversarial networks (GANs) and fair classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31867;&#38750;&#32447;&#24615;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#36793;&#30028;&#12290;&#23545;&#20110;&#20855;&#26377;Lipschitz&#36830;&#32493;&#26799;&#24230;&#30340;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;&#20351;&#22122;&#22768;&#26159;&#37325;&#23614;&#30340;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#23545;&#22833;&#36133;&#27010;&#29575;&#30340;&#23545;&#25968;&#20381;&#36182;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;&#21098;&#20999;&#12289;&#24402;&#19968;&#21270;&#21644;&#37327;&#21270;&#31561;&#20219;&#20309;&#20855;&#26377;&#26377;&#30028;&#36755;&#20986;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.18784</link><description>&lt;p&gt;
&#39640;&#27010;&#29575;&#25910;&#25947;&#36793;&#30028;&#19979;&#30340;&#38750;&#32447;&#24615;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#37325;&#23614;&#22122;&#22768;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
High-probability Convergence Bounds for Nonlinear Stochastic Gradient Descent Under Heavy-tailed Noise. (arXiv:2310.18784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31867;&#38750;&#32447;&#24615;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#36793;&#30028;&#12290;&#23545;&#20110;&#20855;&#26377;Lipschitz&#36830;&#32493;&#26799;&#24230;&#30340;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;&#20351;&#22122;&#22768;&#26159;&#37325;&#23614;&#30340;&#65292;&#32467;&#26524;&#35777;&#26126;&#20102;&#23545;&#22833;&#36133;&#27010;&#29575;&#30340;&#23545;&#25968;&#20381;&#36182;&#12290;&#36825;&#20123;&#32467;&#26524;&#36866;&#29992;&#20110;&#21098;&#20999;&#12289;&#24402;&#19968;&#21270;&#21644;&#37327;&#21270;&#31561;&#20219;&#20309;&#20855;&#26377;&#26377;&#30028;&#36755;&#20986;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#20010;&#30740;&#31350;&#24037;&#20316;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21450;&#20854;&#21098;&#20999;&#21464;&#20307;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#12290;&#19982;&#26222;&#36890;&#30340;SGD&#30456;&#27604;&#65292;&#21098;&#20999;SGD&#22312;&#23454;&#38469;&#20013;&#26356;&#21152;&#31283;&#23450;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#23545;&#25968;&#20381;&#36182;&#20110;&#22833;&#36133;&#27010;&#29575;&#30340;&#39069;&#22806;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#20854;&#20182;&#23454;&#38469;&#38750;&#32447;&#24615;SGD&#21464;&#20307;&#65288;&#22914;&#31526;&#21495;SGD&#12289;&#37327;&#21270;SGD&#21644;&#24402;&#19968;&#21270;SGD&#65289;&#30340;&#25910;&#25947;&#24615;&#29702;&#35299;&#35201;&#23569;&#24471;&#22810;&#65292;&#36825;&#20123;&#26041;&#27861;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#36890;&#20449;&#25928;&#29575;&#25110;&#21152;&#36895;&#25910;&#25947;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#24191;&#20041;&#38750;&#32447;&#24615;SGD&#26041;&#27861;&#30340;&#39640;&#27010;&#29575;&#25910;&#25947;&#36793;&#30028;&#12290;&#23545;&#20110;&#20855;&#26377;Lipschitz&#36830;&#32493;&#26799;&#24230;&#30340;&#24378;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;&#20351;&#22122;&#22768;&#26159;&#37325;&#23614;&#30340;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#22833;&#36133;&#27010;&#29575;&#30340;&#23545;&#25968;&#20381;&#36182;&#12290;&#19982;&#21098;&#20999;SGD&#30340;&#32467;&#26524;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26356;&#20026;&#19968;&#33324;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#26377;&#30028;&#36755;&#20986;&#30340;&#20219;&#20309;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#22914;&#21098;&#20999;&#12289;&#24402;&#19968;&#21270;&#21644;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several recent works have studied the convergence \textit{in high probability} of stochastic gradient descent (SGD) and its clipped variant. Compared to vanilla SGD, clipped SGD is practically more stable and has the additional theoretical benefit of logarithmic dependence on the failure probability. However, the convergence of other practical nonlinear variants of SGD, e.g., sign SGD, quantized SGD and normalized SGD, that achieve improved communication efficiency or accelerated convergence is much less understood. In this work, we study the convergence bounds \textit{in high probability} of a broad class of nonlinear SGD methods. For strongly convex loss functions with Lipschitz continuous gradients, we prove a logarithmic dependence on the failure probability, even when the noise is heavy-tailed. Strictly more general than the results for clipped SGD, our results hold for any nonlinearity with bounded (component-wise or joint) outputs, such as clipping, normalization, and quantizati
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#20855;&#26377;&#24418;&#29366;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#32553;&#25918;&#26497;&#38480;&#21487;&#20197;&#30001;&#24494;&#20998;&#26041;&#31243;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#26410;&#32463;&#24418;&#29366;&#22788;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20449;&#24687;&#23578;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#26410;&#32463;&#24418;&#29366;&#22788;&#29702;&#30340;&#32593;&#32476;&#65292;&#21457;&#29616;&#23427;&#20204;&#20063;&#21487;&#20197;&#30001;&#31867;&#20284;&#30340;&#24494;&#20998;&#26041;&#31243;&#26469;&#25551;&#36848;&#65292;&#24182;&#32473;&#20986;&#20102;&#23427;&#20204;&#30340;&#19968;&#20123;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2310.12079</link><description>&lt;p&gt;
&#24418;&#29366;&#21644;&#38750;&#24418;&#29366;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#20998;&#26041;&#31243;&#32553;&#25918;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Differential Equation Scaling Limits of Shaped and Unshaped Neural Networks. (arXiv:2310.12079v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12079
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#20855;&#26377;&#24418;&#29366;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#32553;&#25918;&#26497;&#38480;&#21487;&#20197;&#30001;&#24494;&#20998;&#26041;&#31243;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#26410;&#32463;&#24418;&#29366;&#22788;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20449;&#24687;&#23578;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#26410;&#32463;&#24418;&#29366;&#22788;&#29702;&#30340;&#32593;&#32476;&#65292;&#21457;&#29616;&#23427;&#20204;&#20063;&#21487;&#20197;&#30001;&#31867;&#20284;&#30340;&#24494;&#20998;&#26041;&#31243;&#26469;&#25551;&#36848;&#65292;&#24182;&#32473;&#20986;&#20102;&#23427;&#20204;&#30340;&#19968;&#20123;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20855;&#26377;&#24418;&#29366;&#28608;&#27963;&#20989;&#25968;&#65288;&#21363;&#38543;&#30528;&#32593;&#32476;&#35268;&#27169;&#22686;&#22823;&#32780;&#32553;&#25918;&#30340;&#28608;&#27963;&#20989;&#25968;&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#23427;&#20204;&#20855;&#26377;&#30001;&#24494;&#20998;&#26041;&#31243;&#25551;&#36848;&#30340;&#32553;&#25918;&#26497;&#38480;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32467;&#26524;&#19981;&#39044;&#20808;&#21578;&#35785;&#25105;&#20204;&#20851;&#20110;&#8220;&#26222;&#36890;&#8221;&#38750;&#24418;&#29366;&#32593;&#32476;&#30340;&#20219;&#20309;&#20449;&#24687;&#65292;&#20854;&#20013;&#28608;&#27963;&#20989;&#25968;&#22312;&#32593;&#32476;&#35268;&#27169;&#22686;&#22823;&#26102;&#20445;&#25345;&#19981;&#21464;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#20004;&#31181;&#31867;&#22411;&#30340;&#38750;&#24418;&#29366;&#32593;&#32476;&#25214;&#21040;&#20102;&#31867;&#20284;&#30340;&#22522;&#20110;&#24494;&#20998;&#26041;&#31243;&#30340;&#28176;&#36817;&#29305;&#24449;&#25551;&#36848;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20197;&#19979;&#20004;&#31181;&#26550;&#26500;&#22312;&#21021;&#22987;&#21270;&#26102;&#20250;&#25910;&#25947;&#21040;&#30456;&#21516;&#30340;&#26080;&#38480;&#28145;&#24230;&#21644;&#23485;&#24230;&#26497;&#38480;&#65306;&#65288;i&#65289;&#24102;&#26377;&#27531;&#24046;&#20998;&#25903;&#19978;&#30340; $d^{-1/2}$ &#22240;&#23376;&#30340;&#20840;&#36830;&#25509; ResNet&#65292;&#20854;&#20013; $d$ &#26159;&#32593;&#32476;&#30340;&#28145;&#24230;&#65307;&#65288;ii&#65289;&#24102;&#26377;&#28145;&#24230; $d \ll$ &#23485;&#24230; $n$ &#21644;&#24418;&#29366; ReLU &#28608;&#27963;&#20989;&#25968; (activation) &#30340;&#22810;&#23618;&#24863;&#30693;&#26426; (MLP)&#65292;&#20197; $d^{-1/2}$ &#30340;&#36895;&#29575;&#12290;&#20854;&#27425;&#65292;&#23545;&#20110;&#21021;&#22987;&#21270;&#30340;&#38750;&#24418;&#29366; MLP&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#23618;&#38388;&#30456;&#20851;&#24615;&#30340;&#19968;&#38454;&#28176;&#36817;&#20462;&#27491;&#12290;&#29305;&#21035;&#22320;&#65292;&#22914;&#26524; $\rho_\ell$ &#26159;&#31532; $\ell$ &#23618;&#30340;&#30456;&#20851;&#24615;&#65292;&#21017;...
&lt;/p&gt;
&lt;p&gt;
Recent analyses of neural networks with shaped activations (i.e. the activation function is scaled as the network size grows) have led to scaling limits described by differential equations. However, these results do not a priori tell us anything about "ordinary" unshaped networks, where the activation is unchanged as the network size grows. In this article, we find similar differential equation based asymptotic characterization for two types of unshaped networks.  Firstly, we show that the following two architectures converge to the same infinite-depth-and-width limit at initialization: (i) a fully connected ResNet with a $d^{-1/2}$ factor on the residual branch, where $d$ is the network depth. (ii) a multilayer perceptron (MLP) with depth $d \ll$ width $n$ and shaped ReLU activation at rate $d^{-1/2}$.  Secondly, for an unshaped MLP at initialization, we derive the first order asymptotic correction to the layerwise correlation. In particular, if $\rho_\ell$ is the correlation at layer
&lt;/p&gt;</description></item><item><title>RandCom&#26159;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#38543;&#26426;&#36890;&#20449;&#36339;&#36291;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#36890;&#36807;&#27010;&#29575;&#24615;&#26412;&#22320;&#26356;&#26032;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#35774;&#32622;&#20013;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2310.07983</link><description>&lt;p&gt;
RandCom&#65306;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#36890;&#20449;&#36339;&#36291;&#26041;&#27861;&#29992;&#20110;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
RandCom: Random Communication Skipping Method for Decentralized Stochastic Optimization. (arXiv:2310.07983v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07983
&lt;/p&gt;
&lt;p&gt;
RandCom&#26159;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#38543;&#26426;&#36890;&#20449;&#36339;&#36291;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#36890;&#36807;&#27010;&#29575;&#24615;&#26412;&#22320;&#26356;&#26032;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#35774;&#32622;&#20013;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#38543;&#26426;&#36890;&#20449;&#36339;&#36807;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#26041;&#27861;&#22240;&#20854;&#22312;&#21152;&#36895;&#36890;&#20449;&#22797;&#26434;&#24615;&#26041;&#38754;&#20855;&#26377;&#30340;&#20248;&#21183;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24378;&#20984;&#30830;&#23450;&#24615;&#35774;&#32622;&#30340;&#38598;&#20013;&#24335;&#36890;&#20449;&#21327;&#35758;&#19978;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RandCom&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#20102;&#27010;&#29575;&#24615;&#30340;&#26412;&#22320;&#26356;&#26032;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;RandCom&#22312;&#38543;&#26426;&#38750;&#20984;&#12289;&#20984;&#21644;&#24378;&#20984;&#35774;&#32622;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#33021;&#22815;&#36890;&#36807;&#36890;&#20449;&#27010;&#29575;&#26469;&#28176;&#36817;&#22320;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#24403;&#33410;&#28857;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;RandCom&#33021;&#22815;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#12290;&#22312;&#38543;&#26426;&#24378;&#20984;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;RandCom&#21487;&#20197;&#36890;&#36807;&#29420;&#31435;&#20110;&#32593;&#32476;&#30340;&#27493;&#38271;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;RandCom&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#30340;&#28508;&#21147;&#30340;&#31215;&#26497;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed optimization methods with random communication skips are gaining increasing attention due to their proven benefits in accelerating communication complexity. Nevertheless, existing research mainly focuses on centralized communication protocols for strongly convex deterministic settings. In this work, we provide a decentralized optimization method called RandCom, which incorporates probabilistic local updates. We analyze the performance of RandCom in stochastic non-convex, convex, and strongly convex settings and demonstrate its ability to asymptotically reduce communication overhead by the probability of communication. Additionally, we prove that RandCom achieves linear speedup as the number of nodes increases. In stochastic strongly convex settings, we further prove that RandCom can achieve linear speedup with network-independent stepsizes. Moreover, we apply RandCom to federated learning and provide positive results concerning the potential for achieving linear speedup and
&lt;/p&gt;</description></item><item><title>ProbTS&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#21327;&#21516;&#21644;&#27604;&#36739;&#23450;&#21046;&#31070;&#32463;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#12289;&#20248;&#21183;&#21644;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.07446</link><description>&lt;p&gt;
ProbTS&#65306;&#19968;&#31181;&#29992;&#20110;&#25506;&#32034;&#28145;&#24230;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#32479;&#19968;&#24037;&#20855;&#21253;&#12290;
&lt;/p&gt;
&lt;p&gt;
ProbTS: A Unified Toolkit to Probe Deep Time-series Forecasting. (arXiv:2310.07446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07446
&lt;/p&gt;
&lt;p&gt;
ProbTS&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#21327;&#21516;&#21644;&#27604;&#36739;&#23450;&#21046;&#31070;&#32463;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#12289;&#20248;&#21183;&#21644;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#36825;&#20010;&#39046;&#22495;&#20998;&#21270;&#25104;&#20102;&#20004;&#20010;&#26174;&#33879;&#30340;&#20998;&#25903;&#65306;&#19968;&#20010;&#19987;&#27880;&#20110;&#20026;&#26102;&#38388;&#24207;&#21015;&#23450;&#21046;&#29305;&#23450;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#21478;&#19968;&#20010;&#21033;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#12290;&#34429;&#28982;&#36825;&#20004;&#20010;&#20998;&#25903;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#25968;&#25454;&#24773;&#26223;&#12289;&#26041;&#27861;&#35770;&#28966;&#28857;&#21644;&#35299;&#30721;&#26041;&#26696;&#19978;&#30340;&#24046;&#24322;&#25552;&#20986;&#20102;&#28145;&#20837;&#32780;&#26410;&#34987;&#25506;&#32034;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30693;&#35782;&#40511;&#27807;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ProbTS&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#21327;&#21516;&#21644;&#27604;&#36739;&#36825;&#20004;&#20010;&#19981;&#21516;&#30340;&#20998;&#25903;&#12290;ProbTS&#20855;&#22791;&#32479;&#19968;&#30340;&#25968;&#25454;&#27169;&#22359;&#12289;&#27169;&#22359;&#21270;&#30340;&#27169;&#22411;&#27169;&#22359;&#21644;&#20840;&#38754;&#30340;&#35780;&#20272;&#22120;&#27169;&#22359;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#37325;&#26032;&#23457;&#35270;&#21644;&#22522;&#20934;&#27979;&#35797;&#36825;&#20004;&#20010;&#20998;&#25903;&#30340;&#39046;&#20808;&#26041;&#27861;&#12290;&#36890;&#36807;ProbTS&#30340;&#23457;&#26597;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#29305;&#28857;&#12289;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#20197;&#21450;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-series forecasting serves as a linchpin in a myriad of applications, spanning various domains. With the growth of deep learning, this arena has bifurcated into two salient branches: one focuses on crafting specific neural architectures tailored for time series, and the other harnesses advanced deep generative models for probabilistic forecasting. While both branches have made significant progress, their differences across data scenarios, methodological focuses, and decoding schemes pose profound, yet unexplored, research questions. To bridge this knowledge chasm, we introduce ProbTS, a pioneering toolkit developed to synergize and compare these two distinct branches. Endowed with a unified data module, a modularized model module, and a comprehensive evaluator module, ProbTS allows us to revisit and benchmark leading methods from both branches. The scrutiny with ProbTS highlights their distinct characteristics, relative strengths and weaknesses, and areas that need further explorat
&lt;/p&gt;</description></item><item><title>Lion&#26159;&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#23613;&#31649;&#20854;&#29702;&#35770;&#22522;&#30784;&#19981;&#26126;&#30830;&#65292;&#20294;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2310.05898</link><description>&lt;p&gt;
&#29422;&#23376;&#31192;&#23494;&#22320;&#35299;&#20915;&#21463;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#65306;&#27491;&#22914;&#26446;&#38597;&#26222;&#35834;&#22827;&#25152;&#39044;&#27979;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts. (arXiv:2310.05898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05898
&lt;/p&gt;
&lt;p&gt;
Lion&#26159;&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#23613;&#31649;&#20854;&#29702;&#35770;&#22522;&#30784;&#19981;&#26126;&#30830;&#65292;&#20294;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31243;&#24207;&#25628;&#32034;&#21457;&#29616;&#30340;&#26032;&#20248;&#21270;&#22120;Lion&#65288;&#36827;&#21270;&#30340;&#31526;&#21495;&#21160;&#37327;&#65289;&#22312;&#35757;&#32451;&#22823;&#22411;AI&#27169;&#22411;&#26041;&#38754;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#23427;&#22312;&#35757;&#32451;&#25928;&#26524;&#19978;&#19982;AdamW&#30456;&#24403;&#25110;&#26356;&#22909;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#20869;&#23384;&#25928;&#29575;&#12290;&#27491;&#22914;&#25105;&#20204;&#21487;&#20197;&#20174;&#38543;&#26426;&#25628;&#32034;&#31243;&#24207;&#30340;&#32467;&#26524;&#20013;&#26399;&#24453;&#30340;&#65292;Lion&#38598;&#25104;&#20102;&#20960;&#20010;&#29616;&#26377;&#31639;&#27861;&#30340;&#20803;&#32032;&#65292;&#21253;&#25324;&#31526;&#21495;&#21160;&#37327;&#12289;&#29420;&#31435;&#30340;&#26435;&#37325;&#34928;&#20943;&#12289;Polak&#21644;Nesterov&#21160;&#37327;&#65292;&#20294;&#21448;&#19981;&#23646;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#29702;&#35770;&#22522;&#30784;&#20248;&#21270;&#22120;&#31867;&#21035;&#12290;&#22240;&#27492;&#65292;&#23613;&#31649;Lion&#20316;&#20026;&#24191;&#27867;&#20219;&#21153;&#30340;&#36890;&#29992;&#20248;&#21270;&#22120;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20854;&#29702;&#35770;&#22522;&#30784;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#36825;&#31181;&#32570;&#20047;&#29702;&#35770;&#30340;&#26126;&#30830;&#24615;&#38480;&#21046;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#21644;&#25193;&#23637;Lion&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#25581;&#24320;Lion&#30340;&#31070;&#31192;&#38754;&#32433;&#12290;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;Lion&#26159;&#19968;&#31181;&#29702;&#35770;&#19978;&#26032;&#39062;&#19988;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#26368;&#23567;&#21270;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;$f(x)$&#30340;&#21516;&#26102;&#24378;&#21046;&#25191;&#34892;&#36793;&#30028;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lion (Evolved Sign Momentum), a new optimizer discovered through program search, has shown promising results in training large AI models. It performs comparably or favorably to AdamW but with greater memory efficiency. As we can expect from the results of a random search program, Lion incorporates elements from several existing algorithms, including signed momentum, decoupled weight decay, Polak, and Nesterov momentum, but does not fit into any existing category of theoretically grounded optimizers. Thus, even though Lion appears to perform well as a general-purpose optimizer for a wide range of tasks, its theoretical basis remains uncertain. This lack of theoretical clarity limits opportunities to further enhance and expand Lion's efficacy.  This work aims to demystify Lion. Based on both continuous-time and discrete-time analysis, we demonstrate that Lion is a theoretically novel and principled approach for minimizing a general loss function $f(x)$ while enforcing a bound constraint 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#39046;&#22495;&#36827;&#34892;&#26426;&#22120;&#20154;&#33258;&#24314;&#27169;&#21644;&#36816;&#21160;&#35268;&#21010;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;2D&#22270;&#20687;&#21644;&#30456;&#26426;&#23039;&#24577;&#36827;&#34892;&#23398;&#20064;&#65292;&#26080;&#38656;&#28145;&#24230;&#22270;&#20687;&#25110;&#20960;&#20309;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#23545;&#39640;&#33258;&#30001;&#24230;&#29289;&#20307;&#30340;&#24314;&#27169;&#12290;&#22312;7&#33258;&#30001;&#24230;&#26426;&#22120;&#20154;&#27979;&#35797;&#20013;&#65292;&#25152;&#23398;&#30340;&#33258;&#24314;&#27169;&#19982;&#30495;&#23454;&#27169;&#22411;&#30340;&#24046;&#36317;&#20165;&#20026;2%&#12290;</title><link>http://arxiv.org/abs/2310.03624</link><description>&lt;p&gt;
&#29992;&#20110;&#26426;&#22120;&#20154;&#33258;&#24314;&#27169;&#21644;&#36816;&#21160;&#35268;&#21010;&#30340;&#39640;&#33258;&#30001;&#24230;&#21160;&#24577;&#31070;&#32463;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling and Motion Planning. (arXiv:2310.03624v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#39046;&#22495;&#36827;&#34892;&#26426;&#22120;&#20154;&#33258;&#24314;&#27169;&#21644;&#36816;&#21160;&#35268;&#21010;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;2D&#22270;&#20687;&#21644;&#30456;&#26426;&#23039;&#24577;&#36827;&#34892;&#23398;&#20064;&#65292;&#26080;&#38656;&#28145;&#24230;&#22270;&#20687;&#25110;&#20960;&#20309;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#23545;&#39640;&#33258;&#30001;&#24230;&#29289;&#20307;&#30340;&#24314;&#27169;&#12290;&#22312;7&#33258;&#30001;&#24230;&#26426;&#22120;&#20154;&#27979;&#35797;&#20013;&#65292;&#25152;&#23398;&#30340;&#33258;&#24314;&#27169;&#19982;&#30495;&#23454;&#27169;&#22411;&#30340;&#24046;&#36317;&#20165;&#20026;2%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#33258;&#24314;&#27169;&#26159;&#26426;&#22120;&#20154;&#29289;&#29702;&#24418;&#24577;&#30340;&#20219;&#21153;&#26080;&#20851;&#34920;&#31034;&#65292;&#21487;&#29992;&#20110;&#22312;&#27809;&#26377;&#32463;&#20856;&#20960;&#20309;&#36816;&#21160;&#23398;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#36816;&#21160;&#35268;&#21010;&#20219;&#21153;&#12290;&#29305;&#21035;&#26159;&#24403;&#21518;&#32773;&#24456;&#38590;&#24037;&#31243;&#21270;&#25110;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#23398;&#20986;&#29616;&#24847;&#22806;&#21464;&#21270;&#26102;&#65292;&#26080;&#20154;&#33258;&#24314;&#27169;&#26159;&#30495;&#27491;&#33258;&#20027;&#25511;&#21046;&#26426;&#22120;&#20154;&#30340;&#24517;&#35201;&#21151;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#39046;&#22495;&#26469;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20197;&#31070;&#32463;&#38544;&#24335;&#26597;&#35810;&#27169;&#22411;&#30340;&#24418;&#24335;&#33258;&#24314;&#27169;&#20854;&#36816;&#21160;&#23398;&#65292;&#35813;&#27169;&#22411;&#20165;&#36890;&#36807;&#24102;&#26377;&#30456;&#26426;&#23039;&#24577;&#21644;&#37197;&#32622;&#30340;2D&#22270;&#20687;&#36827;&#34892;&#23398;&#20064;&#12290;&#36825;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#22240;&#20026;&#23427;&#19981;&#20381;&#36182;&#28145;&#24230;&#22270;&#20687;&#25110;&#20960;&#20309;&#30693;&#35782;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#26032;&#22411;&#21160;&#24577;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#30340;&#31070;&#32463;&#23494;&#24230;&#22330;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#21487;&#20197;&#36866;&#24212;&#39640;&#33258;&#30001;&#24230;&#65288;DOFs&#65289;&#30340;&#26465;&#20214;&#12290;&#22312;&#19968;&#20010;7&#33258;&#30001;&#24230;&#30340;&#26426;&#22120;&#20154;&#27979;&#35797;&#35774;&#32622;&#20013;&#65292;&#23398;&#24471;&#30340;&#33258;&#24314;&#27169;&#19982;&#30495;&#23454;&#27169;&#22411;&#20043;&#38388;&#30340;Chamfer-L2&#36317;&#31163;&#20165;&#20026;2%&#12290;
&lt;/p&gt;
&lt;p&gt;
A robot self-model is a task-agnostic representation of the robot's physical morphology that can be used for motion planning tasks in absence of classical geometric kinematic models. In particular, when the latter are hard to engineer or the robot's kinematics change unexpectedly, human-free self-modeling is a necessary feature of truly autonomous agents. In this work, we leverage neural fields to allow a robot to self-model its kinematics as a neural-implicit query model learned only from 2D images annotated with camera poses and configurations. This enables significantly greater applicability than existing approaches which have been dependent on depth images or geometry knowledge. To this end, alongside a curricular data sampling strategy, we propose a new encoder-based neural density field architecture for dynamic object-centric scenes conditioned on high numbers of degrees of freedom (DOFs). In a 7-DOF robot test setup, the learned self-model achieves a Chamfer-L2 distance of 2% of
&lt;/p&gt;</description></item><item><title>AutoCast++&#26159;&#19968;&#20010;&#38646;-shot&#22522;&#20110;&#25490;&#21517;&#30340;&#19978;&#19979;&#25991;&#26816;&#32034;&#31995;&#32479;&#65292;&#29992;&#20110;&#20174;&#24191;&#27867;&#30340;&#26032;&#38395;&#25991;&#26723;&#38598;&#21512;&#20013;&#36827;&#34892;&#20107;&#20214;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.01880</link><description>&lt;p&gt;
AutoCast++&#65306;&#21033;&#29992;&#38646;-shot&#22522;&#20110;&#25490;&#21517;&#30340;&#19978;&#19979;&#25991;&#26816;&#32034;&#22686;&#24378;&#19990;&#30028;&#20107;&#20214;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
AutoCast++: Enhancing World Event Prediction with Zero-shot Ranking-based Context Retrieval. (arXiv:2310.01880v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01880
&lt;/p&gt;
&lt;p&gt;
AutoCast++&#26159;&#19968;&#20010;&#38646;-shot&#22522;&#20110;&#25490;&#21517;&#30340;&#19978;&#19979;&#25991;&#26816;&#32034;&#31995;&#32479;&#65292;&#29992;&#20110;&#20174;&#24191;&#27867;&#30340;&#26032;&#38395;&#25991;&#26723;&#38598;&#21512;&#20013;&#36827;&#34892;&#20107;&#20214;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#30340;&#23454;&#26102;&#20107;&#20214;&#39044;&#27979;&#22240;&#20854;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#33021;&#25552;&#20379;&#30693;&#24773;&#20915;&#31574;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#39044;&#27979;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#22914;&#26102;&#38388;&#24207;&#21015;&#65292;&#32780;&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#36827;&#34892;&#39044;&#27979;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AutoCast++&#65292;&#23427;&#26159;&#19968;&#20010;&#38646;-shot&#22522;&#20110;&#25490;&#21517;&#30340;&#19978;&#19979;&#25991;&#26816;&#32034;&#31995;&#32479;&#65292;&#26088;&#22312;&#38024;&#23545;&#24191;&#27867;&#30340;&#26032;&#38395;&#25991;&#26723;&#38598;&#21512;&#36827;&#34892;&#20107;&#20214;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#26681;&#25454;&#38646;-shot&#30340;&#38382;&#39064;-&#27573;&#33853;&#30456;&#20851;&#24615;&#23545;&#25991;&#31456;&#36827;&#34892;&#37325;&#26032;&#25490;&#21517;&#65292;&#20174;&#32780;&#25214;&#21040;&#19982;&#35821;&#20041;&#30456;&#20851;&#30340;&#26032;&#38395;&#12290;&#20043;&#21518;&#65292;&#25152;&#36873;&#30340;&#25991;&#31456;&#23558;&#36827;&#34892;&#38646;-shot&#22788;&#29702;&#29992;&#20110;&#20107;&#20214;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-based prediction of real-world events is garnering attention due to its potential for informed decision-making. Whereas traditional forecasting predominantly hinges on structured data like time-series, recent breakthroughs in language models enable predictions using unstructured text. In particular, (Zou et al., 2022) unveils AutoCast, a new benchmark that employs news articles for answering forecasting queries. Nevertheless, existing methods still trail behind human performance. The cornerstone of accurate forecasting, we argue, lies in identifying a concise, yet rich subset of news snippets from a vast corpus. With this motivation, we introduce AutoCast++, a zero-shot ranking-based context retrieval system, tailored to sift through expansive news document collections for event forecasting. Our approach first re-ranks articles based on zero-shot question-passage relevance, honing in on semantically pertinent news. Following this, the chosen articles are subjected to zero-shot 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ChannelViT&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;ViT&#26550;&#26500;&#30340;&#20462;&#25913;&#21644;&#24341;&#20837;&#20998;&#23618;&#36890;&#36947;&#37319;&#26679;&#25216;&#26415;&#65292;&#22686;&#24378;&#20102;&#23545;&#22810;&#36890;&#36947;&#22270;&#20687;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#26174;&#24494;&#38236;&#21644;&#21355;&#26143;&#25104;&#20687;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.16108</link><description>&lt;p&gt;
&#39057;&#36947;&#35270;&#35273;Transformer&#65306;&#19968;&#24352;&#22270;&#20540;C x 16 x 16&#20010;&#35789;
&lt;/p&gt;
&lt;p&gt;
Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words. (arXiv:2309.16108v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ChannelViT&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;ViT&#26550;&#26500;&#30340;&#20462;&#25913;&#21644;&#24341;&#20837;&#20998;&#23618;&#36890;&#36947;&#37319;&#26679;&#25216;&#26415;&#65292;&#22686;&#24378;&#20102;&#23545;&#22810;&#36890;&#36947;&#22270;&#20687;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#26174;&#24494;&#38236;&#21644;&#21355;&#26143;&#25104;&#20687;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#22312;&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26576;&#20123;&#22270;&#20687;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#22914;&#26174;&#24494;&#38236;&#21644;&#21355;&#26143;&#25104;&#20687;&#65292;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#22270;&#20687;&#36890;&#24120;&#21253;&#21547;&#22810;&#20010;&#36890;&#36947;&#65292;&#27599;&#20010;&#36890;&#36947;&#37117;&#25658;&#24102;&#30528;&#35821;&#20041;&#19978;&#19981;&#21516;&#21644;&#29420;&#31435;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#24517;&#39035;&#23545;&#36755;&#20837;&#36890;&#36947;&#30340;&#31232;&#30095;&#24615;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#65292;&#22312;&#35757;&#32451;&#25110;&#27979;&#35797;&#36807;&#31243;&#20013;&#21487;&#33021;&#27809;&#26377;&#23494;&#38598;&#21487;&#29992;&#30340;&#36890;&#36947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;ViT&#26550;&#26500;&#30340;&#20462;&#25913;&#65292;&#22686;&#24378;&#20102;&#23545;&#36755;&#20837;&#36890;&#36947;&#20043;&#38388;&#30340;&#25512;&#29702;&#65292;&#24182;&#24341;&#20837;&#20102;&#20998;&#23618;&#36890;&#36947;&#37319;&#26679;(HCS)&#20316;&#20026;&#19968;&#31181;&#38468;&#21152;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20197;&#30830;&#20445;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#20165;&#20986;&#29616;&#37096;&#20998;&#36890;&#36947;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;ChannelViT&#29420;&#31435;&#22320;&#26500;&#24314;&#34917;&#19969;&#20196;&#29260;&#24182;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#36890;&#36947;&#23884;&#20837;&#23558;&#20854;&#28155;&#21152;&#21040;&#34917;&#19969;&#20196;&#29260;&#20013;&#65292;&#31867;&#20284;&#20110;&#20301;&#32622;&#23884;&#20837;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer (ViT) has emerged as a powerful architecture in the realm of modern computer vision. However, its application in certain imaging fields, such as microscopy and satellite imaging, presents unique challenges. In these domains, images often contain multiple channels, each carrying semantically distinct and independent information. Furthermore, the model must demonstrate robustness to sparsity in input channels, as they may not be densely available during training or testing. In this paper, we propose a modification to the ViT architecture that enhances reasoning across the input channels and introduce Hierarchical Channel Sampling (HCS) as an additional regularization technique to ensure robustness when only partial channels are presented during test time. Our proposed model, ChannelViT, constructs patch tokens independently from each input channel and utilizes a learnable channel embedding that is added to the patch tokens, similar to positional embeddings. We evaluate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{bio}$FAME&#30340;&#39057;&#29575;&#24863;&#30693;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#29983;&#29289;&#20449;&#21495;&#30340;&#39044;&#35757;&#32451;&#12290;&#20854;&#36890;&#36807;&#22312;&#39057;&#29575;&#31354;&#38388;&#20013;&#23545;&#29983;&#29289;&#20449;&#21495;&#36827;&#34892;&#34920;&#31034;&#21442;&#25968;&#21270;&#65292;&#21033;&#29992;&#22266;&#23450;&#22823;&#23567;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#36816;&#31639;&#31526;&#36827;&#34892;&#20840;&#23616;&#20196;&#29260;&#28151;&#21512;&#65292;&#24182;&#36890;&#36807;&#39057;&#29575;&#32500;&#25345;&#39044;&#35757;&#32451;&#31574;&#30053;&#20445;&#25345;&#27599;&#20010;&#36755;&#20837;&#36890;&#36947;&#20013;&#30340;&#39057;&#29575;&#25104;&#20998;&#12290;</title><link>http://arxiv.org/abs/2309.05927</link><description>&lt;p&gt;
&#38024;&#23545;&#29983;&#29289;&#20449;&#21495;&#30340;&#39057;&#29575;&#24863;&#30693;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Frequency-Aware Masked Autoencoders for Multimodal Pretraining on Biosignals. (arXiv:2309.05927v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{bio}$FAME&#30340;&#39057;&#29575;&#24863;&#30693;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#29983;&#29289;&#20449;&#21495;&#30340;&#39044;&#35757;&#32451;&#12290;&#20854;&#36890;&#36807;&#22312;&#39057;&#29575;&#31354;&#38388;&#20013;&#23545;&#29983;&#29289;&#20449;&#21495;&#36827;&#34892;&#34920;&#31034;&#21442;&#25968;&#21270;&#65292;&#21033;&#29992;&#22266;&#23450;&#22823;&#23567;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#36816;&#31639;&#31526;&#36827;&#34892;&#20840;&#23616;&#20196;&#29260;&#28151;&#21512;&#65292;&#24182;&#36890;&#36807;&#39057;&#29575;&#32500;&#25345;&#39044;&#35757;&#32451;&#31574;&#30053;&#20445;&#25345;&#27599;&#20010;&#36755;&#20837;&#36890;&#36947;&#20013;&#30340;&#39057;&#29575;&#25104;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#26469;&#33258;&#29983;&#29289;&#20449;&#21495;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#23545;&#20154;&#20204;&#30340;&#36523;&#24515;&#29366;&#24577;&#36827;&#34892;&#32508;&#21512;&#24314;&#27169;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#29983;&#29289;&#20449;&#21495;&#36890;&#24120;&#22312;&#39044;&#35757;&#32451;&#21644;&#25512;&#26029;&#25968;&#25454;&#38598;&#20043;&#38388;&#23384;&#22312;&#37325;&#22823;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#36825;&#28304;&#20110;&#20219;&#21153;&#35268;&#33539;&#30340;&#21464;&#21270;&#25110;&#32773;&#27169;&#24577;&#32452;&#21512;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#22312;&#28508;&#22312;&#20998;&#24067;&#20559;&#31227;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39057;&#29575;&#24863;&#30693;&#30340;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;$\texttt{bio}$FAME&#65289;&#65292;&#35813;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#22312;&#39057;&#29575;&#31354;&#38388;&#20013;&#23545;&#29983;&#29289;&#20449;&#21495;&#30340;&#34920;&#31034;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;$\texttt{bio}$FAME&#21253;&#21547;&#19968;&#20010;&#39057;&#29575;&#24863;&#30693;&#21464;&#21387;&#22120;&#65292;&#21033;&#29992;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#22266;&#23450;&#22823;&#23567;&#30340;&#36816;&#31639;&#31526;&#36827;&#34892;&#20840;&#23616;&#20196;&#29260;&#28151;&#21512;&#65292;&#19982;&#36755;&#20837;&#30340;&#38271;&#24230;&#21644;&#37319;&#26679;&#29575;&#26080;&#20851;&#12290;&#20026;&#20102;&#20445;&#25345;&#27599;&#20010;&#36755;&#20837;&#36890;&#36947;&#20013;&#30340;&#39057;&#29575;&#25104;&#20998;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#39057;&#29575;&#32500;&#25345;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#28508;&#31354;&#38388;&#20013;&#25191;&#34892;&#25513;&#30721;&#33258;&#32534;&#30721;&#12290;&#26368;&#32456;&#30340;&#26550;&#26500;&#26377;&#25928;&#22320;&#25429;&#33719;&#19981;&#21516;&#20219;&#21153;&#38388;&#30340;&#39057;&#29575;&#29305;&#24449;&#21644;&#27169;&#24577;&#32452;&#21512;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging multimodal information from biosignals is vital for building a comprehensive representation of people's physical and mental states. However, multimodal biosignals often exhibit substantial distributional shifts between pretraining and inference datasets, stemming from changes in task specification or variations in modality compositions. To achieve effective pretraining in the presence of potential distributional shifts, we propose a frequency-aware masked autoencoder ($\texttt{bio}$FAME) that learns to parameterize the representation of biosignals in the frequency space. $\texttt{bio}$FAME incorporates a frequency-aware transformer, which leverages a fixed-size Fourier-based operator for global token mixing, independent of the length and sampling rate of inputs. To maintain the frequency components within each input channel, we further employ a frequency-maintain pretraining strategy that performs masked autoencoding in the latent space. The resulting architecture effectivel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#21270;&#19988;&#21487;&#20256;&#36882;&#30340;&#35774;&#35745;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#20048;&#39640;&#25805;&#32437;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#31934;&#30830;&#24615;&#35201;&#27714;&#12290;&#36890;&#36807;&#30828;&#20214;&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#21644;&#36827;&#21270;&#31574;&#30053;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21487;&#38752;&#30340;&#20048;&#39640;&#25805;&#32437;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#35774;&#35745;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#20256;&#36882;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02354</link><description>&lt;p&gt;
&#19968;&#31181;&#36731;&#37327;&#21270;&#19988;&#21487;&#20256;&#36882;&#30340;&#35774;&#35745;&#29992;&#20110;&#31283;&#20581;&#30340;&#20048;&#39640;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
A Lightweight and Transferable Design for Robust LEGO Manipulation. (arXiv:2309.02354v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#21270;&#19988;&#21487;&#20256;&#36882;&#30340;&#35774;&#35745;&#65292;&#29992;&#20110;&#35299;&#20915;&#26426;&#22120;&#20154;&#20048;&#39640;&#25805;&#32437;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#31934;&#30830;&#24615;&#35201;&#27714;&#12290;&#36890;&#36807;&#30828;&#20214;&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#21644;&#36827;&#21270;&#31574;&#30053;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21487;&#38752;&#30340;&#20048;&#39640;&#25805;&#32437;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#35774;&#35745;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#20256;&#36882;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20048;&#39640;&#26159;&#19968;&#20010;&#29992;&#20110;&#21407;&#22411;&#21270;&#20687;&#32032;&#21270;&#23545;&#35937;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#24179;&#21488;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#20154;&#20048;&#39640;&#21407;&#22411;&#21270;&#65288;&#21363;&#25805;&#32437;&#20048;&#39640;&#31215;&#26408;&#65289;&#30001;&#20110;&#32039;&#23494;&#30340;&#36830;&#25509;&#21644;&#31934;&#30830;&#24615;&#35201;&#27714;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23433;&#20840;&#39640;&#25928;&#30340;&#26426;&#22120;&#20154;&#20048;&#39640;&#25805;&#32437;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#36890;&#36807;&#30828;&#20214;&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#20943;&#23569;&#20102;&#25805;&#32437;&#30340;&#22797;&#26434;&#24615;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#26411;&#31471;&#24037;&#20855;&#65288;EOAT&#65289;&#65292;&#23427;&#20943;&#23569;&#20102;&#38382;&#39064;&#32500;&#24230;&#65292;&#20351;&#22823;&#22411;&#24037;&#19994;&#26426;&#22120;&#20154;&#33021;&#22815;&#36731;&#26494;&#25805;&#32437;&#20048;&#39640;&#31215;&#26408;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#20351;&#29992;&#36827;&#21270;&#31574;&#30053;&#23433;&#20840;&#22320;&#20248;&#21270;&#26426;&#22120;&#20154;&#36816;&#21160;&#65292;&#29992;&#20110;&#20048;&#39640;&#25805;&#32437;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;EOAT&#22312;&#25805;&#32437;&#20048;&#39640;&#31215;&#26408;&#26041;&#38754;&#34920;&#29616;&#21487;&#38752;&#65292;&#32780;&#23398;&#20064;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#19988;&#23433;&#20840;&#22320;&#23558;&#25805;&#32437;&#24615;&#33021;&#25552;&#39640;&#21040;100%&#30340;&#25104;&#21151;&#29575;&#12290;&#25152;&#35774;&#35745;&#30340;&#21327;&#21516;&#35774;&#35745;&#24050;&#32463;&#37096;&#32626;&#21040;&#22810;&#21488;&#26426;&#22120;&#20154;&#65288;FANUC LR-mate 200id/7L&#21644;Yaskawa GP4&#65289;&#19978;&#65292;&#20197;&#23637;&#31034;&#20854;&#26222;&#36866;&#24615;&#21644;&#21487;&#20256;&#36882;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26412;&#30740;&#31350;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
LEGO is a well-known platform for prototyping pixelized objects. However, robotic LEGO prototyping (i.e. manipulating LEGO bricks) is challenging due to the tight connections and accuracy requirement. This paper investigates safe and efficient robotic LEGO manipulation. In particular, this paper reduces the complexity of the manipulation by hardware-software co-design. An end-of-arm tool (EOAT) is designed, which reduces the problem dimension and allows large industrial robots to easily manipulate LEGO bricks. In addition, this paper uses evolution strategy to safely optimize the robot motion for LEGO manipulation. Experiments demonstrate that the EOAT performs reliably in manipulating LEGO bricks and the learning framework can effectively and safely improve the manipulation performance to a 100% success rate. The co-design is deployed to multiple robots (i.e. FANUC LR-mate 200id/7L and Yaskawa GP4) to demonstrate its generalizability and transferability. In the end, we show that the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20102;&#26356;&#28789;&#27963;&#30340;&#32534;&#30721;&#29305;&#24449;&#32858;&#21512;&#26041;&#26696;&#65292;&#33021;&#22815;&#32039;&#23494;&#22320;&#19979;&#30028;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#12290;</title><link>http://arxiv.org/abs/2309.00380</link><description>&lt;p&gt;
&#29992;&#25490;&#24207;&#19981;&#21464;&#30340;&#32534;&#30721;&#22120;&#21644;&#26356;&#32039;&#30340;&#21464;&#20998;&#36793;&#30028;&#23398;&#20064;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning multi-modal generative models with permutation-invariant encoders and tighter variational bounds. (arXiv:2309.00380v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20102;&#26356;&#28789;&#27963;&#30340;&#32534;&#30721;&#29305;&#24449;&#32858;&#21512;&#26041;&#26696;&#65292;&#33021;&#22815;&#32039;&#23494;&#22320;&#19979;&#30028;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#19968;&#30452;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20027;&#39064;&#12290;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120; (VAE) &#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#65292;&#23427;&#23398;&#20064;&#33021;&#22815;&#20849;&#21516;&#35299;&#37322;&#22810;&#31181;&#27169;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#21508;&#31181;&#23458;&#35266;&#20989;&#25968;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#24448;&#24448;&#20197;&#22810;&#27169;&#24577;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#30340;&#19979;&#30028;&#20197;&#21450;&#20449;&#24687;&#35770;&#26041;&#38754;&#30340;&#32771;&#34385;&#20026;&#21160;&#26426;&#12290;&#20026;&#20102;&#23545;&#19981;&#21516;&#27169;&#24577;&#23376;&#38598;&#36827;&#34892;&#32534;&#30721;&#65292;&#25105;&#20204;&#32463;&#24120;&#20351;&#29992;&#24182;&#23637;&#31034;&#20102;&#20135;&#21697;&#22411;&#19987;&#23478; (PoE) &#25110;&#32773;&#28151;&#21512;&#22411;&#19987;&#23478; (MoE) &#32858;&#21512;&#26041;&#26696;&#65292;&#36825;&#20123;&#26041;&#26696;&#22312;&#29983;&#25104;&#36136;&#37327;&#25110;&#32773;&#22810;&#27169;&#24577;&#19968;&#33268;&#24615;&#31561;&#26041;&#38754;&#20855;&#26377;&#19981;&#21516;&#30340;&#26435;&#34913;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#33021;&#22815;&#32039;&#23494;&#22320;&#19979;&#30028;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#30340;&#21464;&#20998;&#36793;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#32534;&#30721;&#29305;&#24449;&#32452;&#21512;&#36215;&#26469;&#65292;&#24320;&#21457;&#20102;&#26356;&#28789;&#27963;&#30340;&#32858;&#21512;&#26041;&#26696;&#65292;&#36825;&#20123;&#26041;&#26696;&#25512;&#24191;&#20102; PoE &#25110;&#32773; MoE &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Devising deep latent variable models for multi-modal data has been a long-standing theme in machine learning research. Multi-modal Variational Autoencoders (VAEs) have been a popular generative model class that learns latent representations which jointly explain multiple modalities. Various objective functions for such models have been suggested, often motivated as lower bounds on the multi-modal data log-likelihood or from information-theoretic considerations. In order to encode latent variables from different modality subsets, Product-of-Experts (PoE) or Mixture-of-Experts (MoE) aggregation schemes have been routinely used and shown to yield different trade-offs, for instance, regarding their generative quality or consistency across multiple modalities. In this work, we consider a variational bound that can tightly lower bound the data log-likelihood. We develop more flexible aggregation schemes that generalise PoE or MoE approaches by combining encoded features from different modali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24341;&#20837;&#20102;&#19981;&#21487;&#35299;&#37322;&#24615;&#24102;&#30340;&#27010;&#24565;&#65292;&#20026;&#29702;&#35299;&#22914;&#20309;&#26500;&#24314;&#36127;&#36131;&#20219;&#19988;&#21487;&#36861;&#31350;&#30340;&#37327;&#23376;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2308.11098</link><description>&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#24615;&#19982;&#19981;&#21487;&#35299;&#37322;&#24615;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Explicability and Inexplicability in the Interpretation of Quantum Neural Networks. (arXiv:2308.11098v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24341;&#20837;&#20102;&#19981;&#21487;&#35299;&#37322;&#24615;&#24102;&#30340;&#27010;&#24565;&#65292;&#20026;&#29702;&#35299;&#22914;&#20309;&#26500;&#24314;&#36127;&#36131;&#20219;&#19988;&#21487;&#36861;&#31350;&#30340;&#37327;&#23376;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;(AI)&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;AI&#25903;&#25345;&#30340;&#31995;&#32479;&#24448;&#24448;&#20855;&#26377;&#19981;&#21487;&#35299;&#37322;&#30340;&#34892;&#20026;&#12290;&#35299;&#37322;&#36825;&#31181;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#26159;&#26500;&#24314;&#21487;&#20449;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#35768;&#22810;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#26126;&#26174;&#22320;&#25512;&#24191;&#21040;&#37327;&#23376;&#29615;&#22659;&#20013;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#37327;&#23376;&#21644;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#30340;&#23616;&#37096;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#24615;&#25351;&#26631;&#26469;&#25506;&#32034;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#21487;&#35299;&#37322;&#24615;&#24102;&#30340;&#27010;&#24565;&#65292;&#34920;&#31034;&#22312;&#35813;&#21306;&#22495;&#20869;&#30340;&#25968;&#25454;&#26679;&#26412;&#27809;&#26377;&#35299;&#37322;&#65292;&#24456;&#21487;&#33021;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#38543;&#26426;&#37327;&#23376;&#27979;&#37327;&#30340;&#21463;&#23475;&#32773;&#12290;&#25105;&#20204;&#23558;&#27492;&#35270;&#20026;&#29702;&#35299;&#22914;&#20309;&#26500;&#24314;&#36127;&#36131;&#20219;&#19988;&#21487;&#36861;&#31350;&#30340;&#37327;&#23376;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability of artificial intelligence (AI) methods, particularly deep neural networks, is of great interest due to the widespread use of AI-backed systems, which often have unexplainable behavior. The interpretability of such models is a crucial component of building trusted systems. Many methods exist to approach this problem, but they do not obviously generalize to the quantum setting. Here we explore the interpretability of quantum neural networks using local model-agnostic interpretability measures of quantum and classical neural networks. We introduce the concept of the band of inexplicability, representing the interpretable region in which data samples have no explanation, likely victims of inherently random quantum measurements. We see this as a step toward understanding how to build responsible and accountable quantum AI models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;IGNNet&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#26102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#20174;&#21407;&#22987;&#36755;&#20837;&#29305;&#24449;&#31934;&#30830;&#35745;&#31639;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2308.08945</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#26684;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Interpretable Graph Neural Networks for Tabular Data. (arXiv:2308.08945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;IGNNet&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#26102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#20174;&#21407;&#22987;&#36755;&#20837;&#29305;&#24449;&#31934;&#30830;&#35745;&#31639;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#34920;&#26684;&#26684;&#24335;&#30340;&#25968;&#25454;&#32463;&#24120;&#20986;&#29616;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36817;&#26399;&#34987;&#25193;&#23637;&#20197;&#26377;&#25928;&#22788;&#29702;&#27492;&#31867;&#25968;&#25454;&#65292;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#25429;&#25417;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26412;&#36136;&#19978;&#20135;&#29983;&#20102;&#40657;&#30418;&#27169;&#22411;&#65292;&#20197;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24418;&#24335;&#23384;&#22312;&#65292;&#20351;&#24471;&#29992;&#25143;&#26080;&#27861;&#29702;&#35299;&#27169;&#22411;&#39044;&#27979;&#30340;&#36923;&#36753;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;IGNNet&#65288;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#34920;&#26684;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#38480;&#21046;&#23398;&#20064;&#31639;&#27861;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;&#21407;&#22987;&#36755;&#20837;&#29305;&#24449;&#20934;&#30830;&#35745;&#31639;&#39044;&#27979;&#32467;&#26524;&#12290;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;IGNNet&#19982;&#38754;&#21521;&#34920;&#26684;&#25968;&#25454;&#30340;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#21253;&#25324;XGBoost&#65292;Random Forests&#21644;TabNet&#65289;&#24615;&#33021;&#30456;&#24403;&#12290;&#21516;&#26102;&#65292;&#32467;&#26524;&#26174;&#31034;&#20174;IGNNet&#33719;&#24471;&#30340;&#35299;&#37322;&#19982;&#30495;&#23454;&#24773;&#20917;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data in tabular format is frequently occurring in real-world applications. Graph Neural Networks (GNNs) have recently been extended to effectively handle such data, allowing feature interactions to be captured through representation learning. However, these approaches essentially produce black-box models, in the form of deep neural networks, precluding users from following the logic behind the model predictions. We propose an approach, called IGNNet (Interpretable Graph Neural Network for tabular data), which constrains the learning algorithm to produce an interpretable model, where the model shows how the predictions are exactly computed from the original input features. A large-scale empirical investigation is presented, showing that IGNNet is performing on par with state-of-the-art machine-learning algorithms that target tabular data, including XGBoost, Random Forests, and TabNet. At the same time, the results show that the explanations obtained from IGNNet are aligned with the true
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35266;&#27979;&#20195;&#20215;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#27979;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DMSOA&#65292;&#24182;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;DMSOA&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#20915;&#31574;&#27493;&#39588;&#21644;&#27979;&#37327;&#27425;&#25968;&#23398;&#21040;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2307.02620</link><description>&lt;p&gt;
&#22312;&#35266;&#27979;&#20195;&#20215;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21160;&#24577;&#35266;&#27979;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Dynamic Observation Policies in Observation Cost-Sensitive Reinforcement Learning. (arXiv:2307.02620v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35266;&#27979;&#20195;&#20215;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#27979;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DMSOA&#65292;&#24182;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;DMSOA&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#20915;&#31574;&#27493;&#39588;&#21644;&#27979;&#37327;&#27425;&#25968;&#23398;&#21040;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#23398;&#20064;&#22797;&#26434;&#20219;&#21153;&#30340;&#39640;&#32423;&#25511;&#21046;&#31574;&#30053;&#65292;&#21253;&#25324;&#28216;&#25103;&#12289;&#26426;&#22120;&#20154;&#12289;&#20379;&#26262;&#19982;&#21046;&#20919;&#31995;&#32479;&#21644;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21160;&#20316;-&#24863;&#30693;&#24490;&#29615;&#36890;&#24120;&#20551;&#35774;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#37117;&#21487;&#20197;&#33719;&#24471;&#23545;&#29615;&#22659;&#29366;&#24577;&#30340;&#27979;&#37327;&#65292;&#19988;&#19981;&#20135;&#29983;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#22312;&#28145;&#28023;&#21644;&#34892;&#26143;&#26426;&#22120;&#20154;&#25506;&#32034;&#12289;&#26448;&#26009;&#35774;&#35745;&#21644;&#21307;&#23398;&#31561;&#24212;&#29992;&#20013;&#65292;&#27979;&#37327;&#25110;&#32773;&#36817;&#20284;&#29615;&#22659;&#29366;&#24577;&#21487;&#33021;&#20250;&#20135;&#29983;&#39640;&#26114;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#36817;&#26469;&#19981;&#26029;&#22686;&#38271;&#30340;&#25991;&#29486;&#65292;&#37319;&#21462;&#20102;RL&#20195;&#29702;&#21487;&#33021;&#19981;&#38656;&#35201;&#25110;&#32773;&#19981;&#24819;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#36827;&#34892;&#26114;&#36149;&#27979;&#37327;&#30340;&#35266;&#28857;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Deep Dynamic Multi-Step Observationless Agent (DMSOA)&#65292;&#24182;&#23558;&#20854;&#19982;&#25991;&#29486;&#36827;&#34892;&#23545;&#27604;&#65292;&#24182;&#22312;OpenAI gym&#21644;Atari Pong&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;DMSOA&#33021;&#22815;&#20197;&#26356;&#23569;&#30340;&#20915;&#31574;&#27493;&#39588;&#21644;&#27979;&#37327;&#27425;&#25968;&#23398;&#21040;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has been shown to learn sophisticated control policies for complex tasks including games, robotics, heating and cooling systems and text generation. The action-perception cycle in RL, however, generally assumes that a measurement of the state of the environment is available at each time step without a cost. In applications such as deep-sea and planetary robot exploration, materials design and medicine, however, there can be a high cost associated with measuring, or even approximating, the state of the environment. In this paper, we survey the recently growing literature that adopts the perspective that an RL agent might not need, or even want, a costly measurement at each time step. Within this context, we propose the Deep Dynamic Multi-Step Observationless Agent (DMSOA), contrast it with the literature and empirically evaluate it on OpenAI gym and Atari Pong environments. Our results, show that DMSOA learns a better policy with fewer decision steps and meas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;SGD-like&#31639;&#27861;&#65292;&#27880;&#20837;&#38543;&#26426;&#22122;&#22768;&#24182;&#21033;&#29992;&#20998;&#24067;&#23545;&#31216;&#24615;&#26469;&#20943;&#23569;&#26041;&#24046;&#65292;&#20197;&#23547;&#25214;&#20855;&#26377;&#20302;&#28023;&#26862;&#30697;&#38453;&#36857;&#30340;&#24179;&#22374;&#26497;&#23567;&#20540;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25910;&#25947;&#36895;&#29575;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.08553</link><description>&lt;p&gt;
&#22122;&#22768;&#31283;&#23450;&#20248;&#21270;&#23545;&#20110;&#20855;&#26377;&#26368;&#20248;&#25910;&#25947;&#29575;&#30340;&#24179;&#22374;&#26497;&#23567;&#20540;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Noise Stability Optimization for Flat Minima with Optimal Convergence Rates. (arXiv:2306.08553v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;SGD-like&#31639;&#27861;&#65292;&#27880;&#20837;&#38543;&#26426;&#22122;&#22768;&#24182;&#21033;&#29992;&#20998;&#24067;&#23545;&#31216;&#24615;&#26469;&#20943;&#23569;&#26041;&#24046;&#65292;&#20197;&#23547;&#25214;&#20855;&#26377;&#20302;&#28023;&#26862;&#30697;&#38453;&#36857;&#30340;&#24179;&#22374;&#26497;&#23567;&#20540;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25910;&#25947;&#36895;&#29575;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#21152;&#20837;&#21152;&#26435;&#25200;&#21160;&#26469;&#25214;&#21040;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#12290;&#32473;&#23450;&#19968;&#20010;&#38750;&#20984;&#20989;&#25968;$f:\mathbb{R}^d\rightarrow \mathbb{R}$&#21644;&#19968;&#20010;$d$&#32500;&#20998;&#24067;$\mathcal{P}$&#65292;&#25105;&#20204;&#25200;&#21160;$f$&#30340;&#26435;&#37325;&#65292;&#24182;&#23450;&#20041;$F(W)=\mathbb{E}[f({W+U})]$&#65292;&#20854;&#20013;$U$&#26159;&#19968;&#20010;&#20174;$\mathcal{P}$&#20013;&#38543;&#26426;&#25277;&#21462;&#30340;&#26679;&#26412;&#12290;&#36825;&#20010;&#36807;&#31243;&#36890;&#36807;$f$&#30340;&#28023;&#26862;&#30697;&#38453;&#30340;&#36857;&#26469;&#35825;&#23548;&#27491;&#21017;&#21270;&#65292;&#20197;&#36866;&#24212;&#20110;&#23567;&#30340;&#12289;&#21508;&#21521;&#21516;&#24615;&#30340;&#39640;&#26031;&#25200;&#21160;&#12290;&#22240;&#27492;&#65292;&#21152;&#26435;&#25200;&#21160;&#30340;&#20989;&#25968;&#20559;&#21521;&#20110;&#24102;&#26377;&#20302;&#28023;&#26862;&#30697;&#38453;&#36857;&#30340;&#26497;&#23567;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;SGD&#30340;&#31639;&#27861;&#65292;&#22312;&#35745;&#31639;&#26799;&#24230;&#20043;&#21069;&#27880;&#20837;&#38543;&#26426;&#22122;&#22768;&#65292;&#21516;&#26102;&#21033;&#29992;$\mathcal{P}$&#30340;&#23545;&#31216;&#24615;&#26469;&#20943;&#23569;&#26041;&#24046;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We consider finding flat, local minimizers by adding average weight perturbations. Given a nonconvex function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ and a $d$-dimensional distribution $\mathcal{P}$ which is symmetric at zero, we perturb the weight of $f$ and define $F(W) = \mathbb{E}[f({W + U})]$, where $U$ is a random sample from $\mathcal{P}$. This injection induces regularization through the Hessian trace of $f$ for small, isotropic Gaussian perturbations. Thus, the weight-perturbed function biases to minimizers with low Hessian trace. Several prior works have studied settings related to this weight-perturbed function by designing algorithms to improve generalization. Still, convergence rates are not known for finding minima under the average perturbations of the function $F$. This paper considers an SGD-like algorithm that injects random noise before computing gradients while leveraging the symmetry of $\mathcal{P}$ to reduce variance. We then provide a rigorous analysis, showing
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;Med-DDPM&#65292;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21270;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#25511;&#21046;&#20687;&#32032;&#32423;&#25513;&#30721;&#26631;&#31614;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#30340;&#21307;&#23398;&#22270;&#20687;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;GAN&#25216;&#26415;&#65292;&#20063;&#20248;&#20110;&#20256;&#32479;&#30340;&#22686;&#24378;&#25216;&#26415;&#21644;GAN&#21512;&#25104;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2305.18453</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#21270;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Conditional Diffusion Models for Semantic 3D Medical Image Synthesis. (arXiv:2305.18453v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18453
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;Med-DDPM&#65292;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21270;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#25511;&#21046;&#20687;&#32032;&#32423;&#25513;&#30721;&#26631;&#31614;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#30340;&#21307;&#23398;&#22270;&#20687;&#65292;&#24182;&#19988;&#22312;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;GAN&#25216;&#26415;&#65292;&#20063;&#20248;&#20110;&#20256;&#32479;&#30340;&#22686;&#24378;&#25216;&#26415;&#21644;GAN&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Med-DDPM&#65292;&#23427;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#21270;&#30340;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#65292;&#35299;&#20915;&#20102;&#21307;&#23398;&#25104;&#20687;&#20013;&#25968;&#25454;&#31232;&#32570;&#12289;&#37319;&#38598;&#26041;&#27861;&#19981;&#19968;&#33268;&#21644;&#38544;&#31169;&#38382;&#39064;&#31561;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#37117;&#36229;&#36807;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#12290;Med-DDPM&#30340;&#29420;&#29305;&#29305;&#28857;&#22312;&#20110;&#20351;&#29992;&#35821;&#20041;&#26465;&#20214;&#36827;&#34892;&#19977;&#32500;&#22270;&#20687;&#21512;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#36890;&#36807;&#25511;&#21046;&#20687;&#32032;&#32423;&#25513;&#30721;&#26631;&#31614;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#23427;&#20415;&#20110;&#21019;&#24314;&#36924;&#30495;&#30340;&#21307;&#23398;&#22270;&#20687;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;Med-DDPM&#22312;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#31561;&#25351;&#26631;&#19978;&#20248;&#20110;GAN&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;Med-DDPM&#22312;&#22686;&#24378;&#20998;&#21106;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#20063;&#20248;&#20110;&#20256;&#32479;&#30340;&#22686;&#24378;&#25216;&#26415;&#21644;GAN&#21512;&#25104;&#22270;&#20687;&#12290;&#23427;&#35299;&#20915;&#20102;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#38590;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Med-DDPM, an innovative solution using diffusion models for semantic 3D medical image synthesis, addressing the prevalent issues in medical imaging such as data scarcity, inconsistent acquisition methods, and privacy concerns. Experimental evidence illustrates that diffusion models surpass Generative Adversarial Networks (GANs) in stability and performance, generating high-quality, realistic 3D medical images. The distinct feature of Med-DDPM is its use of semantic conditioning for the diffusion model in 3D image synthesis. By controlling the generation process through pixel-level mask labels, it facilitates the creation of realistic medical images. Empirical evaluations underscore the superior performance of Med-DDPM over GAN techniques in metrics such as accuracy, stability, and versatility. Furthermore, Med-DDPM outperforms traditional augmentation techniques and synthetic GAN images in enhancing the accuracy of segmentation models. It addresses challenges such
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#34880;&#28082;&#26816;&#26597;&#25968;&#20540;&#30340;&#30149;&#27602;&#19982;&#32454;&#33740;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#35782;&#21035;&#24863;&#26579;&#31867;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;CRP&#27700;&#24179;10-40 mg/L&#33539;&#22260;&#20869;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21306;&#20998;&#32454;&#33740;&#21644;&#30149;&#27602;&#24863;&#26579;&#30340;&#20934;&#30830;&#24615;&#65292;&#35777;&#26126;&#20102;&#22810;&#31181;&#34880;&#28082;&#21442;&#25968;&#23545;&#20110;&#35786;&#26029;&#20915;&#31574;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07877</link><description>&lt;p&gt;
&#22522;&#20110;&#20363;&#34892;&#34880;&#28082;&#26816;&#26597;&#25968;&#20540;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37492;&#21035;&#30149;&#27602;&#21644;&#32454;&#33740;&#24863;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiating Viral and Bacterial Infections: A Machine Learning Model Based on Routine Blood Test Values. (arXiv:2305.07877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#34880;&#28082;&#26816;&#26597;&#25968;&#20540;&#30340;&#30149;&#27602;&#19982;&#32454;&#33740;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#35782;&#21035;&#24863;&#26579;&#31867;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;CRP&#27700;&#24179;10-40 mg/L&#33539;&#22260;&#20869;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21306;&#20998;&#32454;&#33740;&#21644;&#30149;&#27602;&#24863;&#26579;&#30340;&#20934;&#30830;&#24615;&#65292;&#35777;&#26126;&#20102;&#22810;&#31181;&#34880;&#28082;&#21442;&#25968;&#23545;&#20110;&#35786;&#26029;&#20915;&#31574;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25239;&#29983;&#32032;&#32784;&#33647;&#24615;&#26085;&#30410;&#23041;&#32961;&#65292;&#27491;&#30830;&#21306;&#20998;&#32454;&#33740;&#21644;&#30149;&#27602;&#24863;&#26579;&#20197;&#36827;&#34892;&#27491;&#30830;&#30340;&#25239;&#29983;&#32032;&#20351;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;16&#20010;&#20363;&#34892;&#34880;&#28082;&#26816;&#26597;&#32467;&#26524;&#12289;C-&#21453;&#24212;&#34507;&#30333;&#27700;&#24179;&#12289;&#29983;&#29289;&#24615;&#21035;&#21644;&#24180;&#40836;&#30340;&#30149;&#27602;&#19982;&#32454;&#33740;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#21306;&#20998;&#36825;&#20123;&#24863;&#26579;&#31867;&#22411;&#12290;&#20351;&#29992;&#21333;&#20010;&#21307;&#30103;&#20013;&#24515;&#30340;44,120&#20010;&#26696;&#20363;&#25968;&#25454;&#38598;&#65292;"&#30149;&#27602; vs. &#32454;&#33740;"&#27169;&#22411;&#34920;&#29616;&#20986;&#20196;&#20154;&#30633;&#30446;&#30340;82.2%&#30340;&#20934;&#30830;&#29575;&#65292;0.129&#30340;Brier&#24471;&#20998;&#21644;0.91&#30340;ROC&#26354;&#32447;&#19979;&#38754;&#31215;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;CRP&#20915;&#31574;&#35268;&#21017;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#22312;CRP&#33539;&#22260;&#20026;10-40 mg/L&#26102;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#20934;&#30830;&#24615;&#65292;&#36825;&#20010;&#33539;&#22260;&#20869;&#20165;&#38752;CRP&#26080;&#27861;&#20026;&#32454;&#33740;&#21644;&#30149;&#27602;&#24863;&#26579;&#36827;&#34892;&#21306;&#20998;&#30340;&#35786;&#26029;&#20215;&#20540;&#26377;&#38480;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#35786;&#26029;&#20915;&#31574;&#20013;&#32771;&#34385;&#22810;&#31181;&#34880;&#28082;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24314;&#35758;&#30149;&#27602; vs. &#32454;&#33740;&#27169;&#22411;&#20351;&#24471;&#24212;&#29992;&#20110;&#20020;&#24202;&#20915;&#31574;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing threat of antibiotic resistance necessitates accurate differentiation between bacterial and viral infections for proper antibiotic administration. In this study, a Virus vs. Bacteria machine learning model was developed to discern between these infection types using 16 routine blood test results, C-reactive protein levels, biological sex, and age. With a dataset of 44,120 cases from a single medical center, the Virus vs. Bacteria model demonstrated remarkable accuracy of 82.2%, a Brier score of 0.129, and an area under the ROC curve of 0.91, surpassing the performance of traditional CRP decision rule models. The model demonstrates substantially improved accuracy within the CRP range of 10 40 mg/L, an interval in which CRP alone offers limited diagnostic value for distinguishing between bacterial and viral infections. These findings underscore the importance of considering multiple blood parameters for diagnostic decision-making and suggest that the Virus vs. Bacteria model 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#24179;&#31561;&#21270;&#21463;&#20445;&#25252;&#23376;&#32676;&#20307;&#20043;&#38388;&#30340;&#39044;&#27979;&#20998;&#24067;&#26469;&#23454;&#29616;&#32452;&#20844;&#24179;&#21644;&#35270;&#30456;&#20284;&#20010;&#20307;&#21516;&#31561;&#23545;&#24453;&#23454;&#29616;&#20010;&#20154;&#20844;&#27491;&#26159;&#19981;&#20860;&#23481;&#30340;&#12290; &#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#36830;&#32493;&#27010;&#29575;&#20989;&#25968;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#26469;&#23454;&#29616;&#32452;&#21644;&#20010;&#20154;&#20844;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.09779</link><description>&lt;p&gt;
&#24179;&#31561;&#25912;&#20851;&#19981;&#31561;&#20110;&#24179;&#31561;&#20010;&#20154;&#20960;&#29575;: &#29992;&#20110;&#32452;&#21644;&#20010;&#20154;&#20844;&#24179;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Equalised Odds is not Equal Individual Odds: Post-processing for Group and Individual Fairness. (arXiv:2304.09779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09779
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#24179;&#31561;&#21270;&#21463;&#20445;&#25252;&#23376;&#32676;&#20307;&#20043;&#38388;&#30340;&#39044;&#27979;&#20998;&#24067;&#26469;&#23454;&#29616;&#32452;&#20844;&#24179;&#21644;&#35270;&#30456;&#20284;&#20010;&#20307;&#21516;&#31561;&#23545;&#24453;&#23454;&#29616;&#20010;&#20154;&#20844;&#27491;&#26159;&#19981;&#20860;&#23481;&#30340;&#12290; &#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#36830;&#32493;&#27010;&#29575;&#20989;&#25968;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#26469;&#23454;&#29616;&#32452;&#21644;&#20010;&#20154;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#20844;&#24179;&#36890;&#36807;&#24179;&#34913;&#21463;&#20445;&#25252;&#23376;&#32676;&#20307;&#20043;&#38388;&#30340;&#39044;&#27979;&#20998;&#24067;&#26469;&#23454;&#29616;&#65307;&#20010;&#20154;&#20844;&#24179;&#35201;&#27714;&#23558;&#30456;&#20284;&#30340;&#20010;&#20307;&#35270;&#20026;&#21516;&#31561;&#23545;&#24453;&#12290;&#28982;&#32780;&#65292;&#24403;&#35780;&#20998;&#27169;&#22411;&#36890;&#36807;&#19981;&#36830;&#32493;&#30340;&#27010;&#29575;&#20989;&#25968;&#36827;&#34892;&#26657;&#20934;&#26102;&#65292;&#36825;&#20004;&#20010;&#30446;&#26631;&#26159;&#19981;&#20860;&#23481;&#30340;&#65292;&#20854;&#20013;&#20010;&#20307;&#21487;&#33021;&#20250;&#38543;&#26426;&#20998;&#37197;&#30001;&#22266;&#23450;&#27010;&#29575;&#30830;&#23450;&#30340;&#32467;&#26524;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#33021;&#20250;&#20351;&#26469;&#33258;&#21516;&#19968;&#21463;&#20445;&#25252;&#32452;&#30340;&#20004;&#20010;&#30456;&#20284;&#20010;&#20307;&#30340;&#20998;&#31867;&#20960;&#29575;&#24046;&#21035;&#26126;&#26174;&#19981;&#21516;&#65292;&#36825;&#26159;&#20010;&#20154;&#20844;&#24179;&#30340;&#26126;&#26174;&#36829;&#21453;&#12290;&#20026;&#27599;&#20010;&#21463;&#20445;&#25252;&#23376;&#32676;&#20307;&#20998;&#37197;&#21807;&#19968;&#30340;&#20960;&#29575;&#20063;&#21487;&#33021;&#20250;&#38459;&#27490;&#19968;&#20010;&#23376;&#32676;&#20307;&#30340;&#25104;&#21592;&#25509;&#21040;&#21478;&#19968;&#20010;&#23376;&#32676;&#20307;&#26377;&#27491;&#38754;&#32467;&#26524;&#30340;&#24179;&#31561;&#26426;&#20250;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#21478;&#19968;&#31181;&#31216;&#20026;&#20010;&#20154;&#20960;&#29575;&#30340;&#19981;&#20844;&#24179;&#31867;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#21463;&#32676;&#20307;&#38408;&#20540;&#32422;&#26463;&#30340;&#36830;&#32493;&#27010;&#29575;&#20989;&#25968;&#26469;&#35299;&#20915;&#25152;&#26377;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20445;&#30041;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group fairness is achieved by equalising prediction distributions between protected sub-populations; individual fairness requires treating similar individuals alike. These two objectives, however, are incompatible when a scoring model is calibrated through discontinuous probability functions, where individuals can be randomly assigned an outcome determined by a fixed probability. This procedure may provide two similar individuals from the same protected group with classification odds that are disparately different -- a clear violation of individual fairness. Assigning unique odds to each protected sub-population may also prevent members of one sub-population from ever receiving equal chances of a positive outcome to another, which we argue is another type of unfairness called individual odds. We reconcile all this by constructing continuous probability functions between group thresholds that are constrained by their Lipschitz constant. Our solution preserves the model's predictive powe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#31639;&#27861;&#36924;&#36817;&#32852;&#30431;&#32467;&#26500;&#32676;&#20307;&#35299;&#37322;&#22120;&#30340;&#22797;&#26434;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#24555;&#36895;&#12289;&#26131;&#20110;&#23454;&#29616;&#19988;&#19982;&#29305;&#23450;&#27169;&#22411;&#26080;&#20851;&#30340;&#26032;&#22411;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#20005;&#23494;&#30340;&#32479;&#35745;&#20998;&#26512;&#21450;&#35823;&#24046;&#30028;&#30340;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2303.10216</link><description>&lt;p&gt;
&#21033;&#29992;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#30340;&#32852;&#30431;&#32467;&#26500;&#32676;&#20307;&#35299;&#37322;&#22120;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Approximation of group explainers with coalition structure using Monte Carlo sampling on the product space of coalitions and features. (arXiv:2303.10216v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#31639;&#27861;&#36924;&#36817;&#32852;&#30431;&#32467;&#26500;&#32676;&#20307;&#35299;&#37322;&#22120;&#30340;&#22797;&#26434;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#24555;&#36895;&#12289;&#26131;&#20110;&#23454;&#29616;&#19988;&#19982;&#29305;&#23450;&#27169;&#22411;&#26080;&#20851;&#30340;&#26032;&#22411;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#20005;&#23494;&#30340;&#32479;&#35745;&#20998;&#26512;&#21450;&#35823;&#24046;&#30028;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#35299;&#37322;&#25216;&#26415;&#37117;&#37319;&#29992;&#20102;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#30340;&#24605;&#24819;&#12290;&#36825;&#20123;&#28216;&#25103;&#29702;&#35770;&#35299;&#37322;&#22120;&#30001;&#20110;&#22797;&#26434;&#24230;&#39640;&#65292;&#22312;&#23454;&#38469;&#29615;&#22659;&#19979;&#26080;&#27861;&#31934;&#30830;&#35745;&#31639;&#12290;&#26412;&#25991;&#37319;&#29992;&#36866;&#24403;&#30340;&#26679;&#26412;&#31354;&#38388;&#20272;&#35745;&#26399;&#26395;&#20540;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#31639;&#27861;, &#26469;&#20197;&#20381;&#36182;&#20110;&#32972;&#26223;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#20272;&#35745;&#29305;&#23450;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#39044;&#27979;&#21521;&#37327;&#30340;&#36793;&#38469;&#21338;&#24328;&#30340;&#32852;&#30431;&#20540;&#65292;&#36825;&#31181;&#26041;&#27861;&#24555;&#36895;&#12289;&#26131;&#20110;&#25191;&#34892;&#19988;&#19982;&#20854;&#20182;&#24050;&#30693;&#30340;&#35768;&#22810;&#26356;&#22797;&#26434;&#21644;&#29305;&#23450;&#20110;&#27169;&#22411;&#30340;&#20272;&#35745;&#25216;&#26415;&#20855;&#26377;&#31867;&#20284;&#30340;&#32479;&#35745;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#23494;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, many Machine Learning (ML) explanation techniques have been designed using ideas from cooperative game theory. These game-theoretic explainers suffer from high complexity, hindering their exact computation in practical settings. In our work, we focus on a wide class of linear game values, as well as coalitional values, for the marginal game based on a given ML model and predictor vector. By viewing these explainers as expectations over appropriate sample spaces, we design a novel Monte Carlo sampling algorithm that estimates them at a reduced complexity that depends linearly on the size of the background dataset. We set up a rigorous framework for the statistical analysis and obtain error bounds for our sampling methods. The advantage of this approach is that it is fast, easily implementable, and model-agnostic. Furthermore, it has similar statistical accuracy as other known estimation techniques that are more complex and model-specific. We provide rigorous proofs of s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#27169;&#22411;A-VBANet&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#27425;&#24615;&#23398;&#20064;&#25552;&#20379;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#25163;&#26415;&#25216;&#33021;&#20998;&#31867;&#65292;&#25104;&#21151;&#22320;&#36866;&#24212;&#20102;&#27169;&#25311;&#20219;&#21153;&#21644;&#33145;&#33108;&#38236;&#32966;&#22218;&#20999;&#38500;&#26415;&#65292;&#20026;&#22522;&#20110;&#35270;&#39057;&#30340;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#25552;&#20379;&#20102;&#39046;&#22495;&#19981;&#21487;&#30693;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2301.00812</link><description>&lt;p&gt;
&#19968;&#27425;&#24615;&#39046;&#22495;&#33258;&#36866;&#24212;&#22312;&#22522;&#20110;&#35270;&#39057;&#30340;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
One-shot domain adaptation in video-based assessment of surgical skills. (arXiv:2301.00812v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#27169;&#22411;A-VBANet&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#27425;&#24615;&#23398;&#20064;&#25552;&#20379;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#25163;&#26415;&#25216;&#33021;&#20998;&#31867;&#65292;&#25104;&#21151;&#22320;&#36866;&#24212;&#20102;&#27169;&#25311;&#20219;&#21153;&#21644;&#33145;&#33108;&#38236;&#32966;&#22218;&#20999;&#38500;&#26415;&#65292;&#20026;&#22522;&#20110;&#35270;&#39057;&#30340;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#25552;&#20379;&#20102;&#39046;&#22495;&#19981;&#21487;&#30693;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a meta-learning model, A-VBANet, that can deliver domain-agnostic surgical skill classification via one-shot learning. The model successfully adapts to simulated tasks and laparoscopic cholecystectomy, providing a domain-agnostic procedure for video-based assessment of surgical skills.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#23454;&#29616;&#20102;&#25163;&#26415;&#25216;&#33021;&#30340;&#33258;&#21160;&#21644;&#23458;&#35266;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#65292;&#24182;&#19988;&#21463;&#38480;&#20110;&#20854;&#35757;&#32451;&#39046;&#22495;&#12290;&#36825;&#38459;&#27490;&#20102;&#23427;&#20204;&#36807;&#28193;&#21040;&#25968;&#25454;&#26377;&#38480;&#30340;&#26032;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#39046;&#22495;&#33258;&#36866;&#24212;&#23545;&#20110;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#27169;&#22411;A-VBANet&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#19968;&#27425;&#24615;&#23398;&#20064;&#25552;&#20379;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#25163;&#26415;&#25216;&#33021;&#20998;&#31867;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#33145;&#33108;&#38236;&#21644;&#26426;&#22120;&#20154;&#25163;&#26415;&#27169;&#25311;&#22120;&#19978;&#24320;&#21457;&#20102;A-VBANet&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#33145;&#33108;&#38236;&#32966;&#22218;&#20999;&#38500;&#26415;&#30340;&#25163;&#26415;&#23460;&#35270;&#39057;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#22320;&#36866;&#24212;&#20102;&#27169;&#25311;&#20219;&#21153;&#65292;&#20934;&#30830;&#29575;&#39640;&#36798;99.5%&#65288;&#19968;&#27425;&#24615;&#65289;&#21644;99.9%&#65288;&#23569;&#37327;&#26679;&#26412;&#65289;&#65292;&#22312;&#33145;&#33108;&#38236;&#32966;&#22218;&#20999;&#38500;&#26415;&#20013;&#30340;&#20934;&#30830;&#29575;&#20026;89.7%&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20379;&#20102;&#22522;&#20110;&#35270;&#39057;&#30340;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#30340;&#39046;&#22495;&#19981;&#21487;&#30693;&#31243;&#24207;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#24433;&#21709;&#26159;&#23427;&#20801;&#35768;&#20351;&#29992;&#26469;&#33258;&#25163;&#26415;&#27169;&#25311;&#22120;&#30340;&#25968;&#25454;&#26469;&#35780;&#20272;&#25163;&#26415;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) has achieved automatic and objective assessment of surgical skills. However, DL models are data-hungry and restricted to their training domain. This prevents them from transitioning to new tasks where data is limited. Hence, domain adaptation is crucial to implement DL in real life. Here, we propose a meta-learning model, A-VBANet, that can deliver domain-agnostic surgical skill classification via one-shot learning. We develop the A-VBANet on five laparoscopic and robotic surgical simulators. Additionally, we test it on operating room (OR) videos of laparoscopic cholecystectomy. Our model successfully adapts with accuracies up to 99.5% in one-shot and 99.9% in few-shot settings for simulated tasks and 89.7% for laparoscopic cholecystectomy. For the first time, we provide a domain-agnostic procedure for video-based assessment of surgical skills. A significant implication of this approach is that it allows the use of data from surgical simulators to assess performance 
&lt;/p&gt;</description></item></channel></rss>