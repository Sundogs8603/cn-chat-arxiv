<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>InseRF&#26159;&#19968;&#31181;&#22312;NeRF&#37325;&#24314;&#30340;3D&#22330;&#26223;&#20013;&#36827;&#34892;&#29983;&#25104;&#29289;&#20307;&#25554;&#20837;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#21644;2D&#36793;&#30028;&#26694;&#65292;&#23454;&#29616;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#38590;&#20197;&#23454;&#29616;&#30340;&#29983;&#25104;&#26032;&#29289;&#20307;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.05335</link><description>&lt;p&gt;
InseRF: &#22522;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;&#31070;&#32463;3D&#22330;&#26223;&#20013;&#30340;&#29983;&#25104;&#29289;&#20307;&#25554;&#20837;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes. (arXiv:2401.05335v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05335
&lt;/p&gt;
&lt;p&gt;
InseRF&#26159;&#19968;&#31181;&#22312;NeRF&#37325;&#24314;&#30340;3D&#22330;&#26223;&#20013;&#36827;&#34892;&#29983;&#25104;&#29289;&#20307;&#25554;&#20837;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#21644;2D&#36793;&#30028;&#26694;&#65292;&#23454;&#29616;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#38590;&#20197;&#23454;&#29616;&#30340;&#29983;&#25104;&#26032;&#29289;&#20307;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;InseRF&#65292;&#19968;&#31181;&#22312;3D&#22330;&#26223;&#30340;NeRF&#37325;&#24314;&#20013;&#36827;&#34892;&#29983;&#25104;&#29289;&#20307;&#25554;&#20837;&#30340;&#26032;&#26041;&#27861;&#12290;&#22522;&#20110;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#21644;&#21442;&#32771;&#35270;&#28857;&#20013;&#30340;2D&#36793;&#30028;&#26694;&#65292;InseRF&#22312;3D&#22330;&#26223;&#20013;&#29983;&#25104;&#26032;&#30340;&#29289;&#20307;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;3D&#29983;&#25104;&#24314;&#27169;&#20013;&#20351;&#29992;&#24378;&#22823;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20351;&#24471;3D&#22330;&#26223;&#32534;&#36753;&#30340;&#26041;&#27861;&#21457;&#29983;&#20102;&#28145;&#21051;&#30340;&#36716;&#21464;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#36890;&#36807;&#39118;&#26684;&#21644;&#22806;&#35266;&#30340;&#25913;&#21464;&#25110;&#32773;&#31227;&#38500;&#29616;&#26377;&#29289;&#20307;&#26469;&#26377;&#25928;&#32534;&#36753;3D&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#31181;&#26041;&#27861;&#65292;&#29983;&#25104;&#26032;&#30340;&#29289;&#20307;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#22312;&#26412;&#30740;&#31350;&#20013;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;3D&#29289;&#20307;&#25554;&#20837;&#19982;&#21442;&#32771;&#35270;&#22270;&#20013;&#30340;2D&#29289;&#20307;&#25554;&#20837;&#36827;&#34892;&#23545;&#25509;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#21333;&#35270;&#22270;&#29289;&#20307;&#37325;&#24314;&#26041;&#27861;&#23558;2D&#32534;&#36753;&#25552;&#21319;&#20026;3D&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#30340;&#20808;&#39564;&#30693;&#35782;&#23558;&#37325;&#24314;&#30340;&#29289;&#20307;&#25554;&#20837;&#21040;&#22330;&#26223;&#20013;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;3D&#22330;&#26223;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce InseRF, a novel method for generative object insertion in the NeRF reconstructions of 3D scenes. Based on a user-provided textual description and a 2D bounding box in a reference viewpoint, InseRF generates new objects in 3D scenes. Recently, methods for 3D scene editing have been profoundly transformed, owing to the use of strong priors of text-to-image diffusion models in 3D generative modeling. Existing methods are mostly effective in editing 3D scenes via style and appearance changes or removing existing objects. Generating new objects, however, remains a challenge for such methods, which we address in this study. Specifically, we propose grounding the 3D object insertion to a 2D object insertion in a reference view of the scene. The 2D edit is then lifted to 3D using a single-view object reconstruction method. The reconstructed object is then inserted into the scene, guided by the priors of monocular depth estimation methods. We evaluate our method on various 3D scene
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#31359;&#26797;&#24052;&#22763;&#30340;&#21040;&#36798;&#26102;&#38388;&#39044;&#27979;&#31995;&#32479;&#65292;&#21033;&#29992;&#20998;&#21035;&#29992;&#20110;&#20572;&#30041;&#26102;&#38388;&#21644;&#36816;&#34892;&#26102;&#38388;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#23454;&#38469;&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#12290;&#36890;&#36807;&#38598;&#25104;&#31354;&#38388;&#25968;&#25454;&#21644;&#20351;&#29992;&#23618;&#27425;&#21270;&#27169;&#22411;&#22788;&#29702;&#32469;&#36807;&#31449;&#28857;&#30340;&#24773;&#20917;&#65292;&#24471;&#21040;&#20102;&#21487;&#38752;&#30340;AT&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.05322</link><description>&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#20026;&#33258;&#21160;&#31359;&#26797;&#24052;&#22763;&#26381;&#21153;&#30340;&#21040;&#36798;&#26102;&#38388;&#39044;&#27979;: &#26469;&#33258;&#20116;&#20010;&#22478;&#24066;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Arrival Time Prediction for Autonomous Shuttle Services in the Real World: Evidence from Five Cities. (arXiv:2401.05322v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#31359;&#26797;&#24052;&#22763;&#30340;&#21040;&#36798;&#26102;&#38388;&#39044;&#27979;&#31995;&#32479;&#65292;&#21033;&#29992;&#20998;&#21035;&#29992;&#20110;&#20572;&#30041;&#26102;&#38388;&#21644;&#36816;&#34892;&#26102;&#38388;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#23454;&#38469;&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#12290;&#36890;&#36807;&#38598;&#25104;&#31354;&#38388;&#25968;&#25454;&#21644;&#20351;&#29992;&#23618;&#27425;&#21270;&#27169;&#22411;&#22788;&#29702;&#32469;&#36807;&#31449;&#28857;&#30340;&#24773;&#20917;&#65292;&#24471;&#21040;&#20102;&#21487;&#38752;&#30340;AT&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20849;&#20139;&#12289;&#36830;&#25509;&#21644;&#21327;&#20316;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#20986;&#29616;&#65292;&#22478;&#24066;&#31227;&#21160;&#24615;&#27491;&#22788;&#20110;&#36716;&#22411;&#30340;&#36793;&#32536;&#12290;&#28982;&#32780;&#65292;&#35201;&#24819;&#34987;&#23458;&#25143;&#25509;&#21463;&#65292;&#23545;&#23427;&#20204;&#30340;&#20934;&#28857;&#24615;&#30340;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201;&#12290;&#35768;&#22810;&#35797;&#28857;&#39033;&#30446;&#27809;&#26377;&#22266;&#23450;&#26102;&#38388;&#34920;&#65292;&#22240;&#27492;&#21487;&#38752;&#30340;&#21040;&#36798;&#26102;&#38388;&#65288;AT&#65289;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#24471;&#21040;&#20102;&#22686;&#24378;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#33258;&#21160;&#31359;&#26797;&#24052;&#22763;&#30340;AT&#39044;&#27979;&#31995;&#32479;&#65292;&#21033;&#29992;&#20998;&#21035;&#29992;&#20110;&#20572;&#30041;&#26102;&#38388;&#21644;&#36816;&#34892;&#26102;&#38388;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#26469;&#33258;&#20116;&#20010;&#22478;&#24066;&#30340;&#23454;&#38469;&#25968;&#25454;&#36827;&#34892;&#39564;&#35777;&#12290;&#38500;&#20102;&#24120;&#29992;&#30340;&#26041;&#27861;&#22914;XGBoost&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#38598;&#25104;&#31354;&#38388;&#25968;&#25454;&#30340;&#30410;&#22788;&#12290;&#20026;&#20102;&#20934;&#30830;&#22788;&#29702;&#31359;&#26797;&#24052;&#22763;&#32469;&#36807;&#31449;&#28857;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#21644;GNN&#12290;&#26368;&#32456;&#30340;AT&#39044;&#27979;&#32467;&#26524;&#24456;&#26377;&#21069;&#26223;&#65292;&#21363;&#20351;&#39044;&#27979;&#25968;&#20010;&#31449;&#28857;&#20043;&#21069;&#20063;&#26174;&#31034;&#20986;&#36739;&#20302;&#30340;&#35823;&#24046;&#12290;&#28982;&#32780;&#65292;&#24182;&#27809;&#26377;&#21333;&#19968;&#30340;&#27169;&#22411;&#26174;&#38706;&#20986;&#26222;&#36941;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#29305;&#24449;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Urban mobility is on the cusp of transformation with the emergence of shared, connected, and cooperative automated vehicles. Yet, for them to be accepted by customers, trust in their punctuality is vital. Many pilot initiatives operate without a fixed schedule, thus enhancing the importance of reliable arrival time (AT) predictions. This study presents an AT prediction system for autonomous shuttles, utilizing separate models for dwell and running time predictions, validated on real-world data from five cities. Alongside established methods such as XGBoost, we explore the benefits of integrating spatial data using graph neural networks (GNN). To accurately handle the case of a shuttle bypassing a stop, we propose a hierarchical model combining a random forest classifier and a GNN. The results for the final AT prediction are promising, showing low errors even when predicting several stops ahead. Yet, no single model emerges as universally superior, and we provide insights into the chara
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#31354;&#24179;&#21488;&#31449;&#65288;HAPS&#65289;&#20351;&#33021;&#30340;&#22402;&#30452;&#24322;&#26500;&#32593;&#32476;&#20013;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#38382;&#39064;&#30340;&#25112;&#30053;&#23458;&#25143;&#36873;&#25321;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#30340;&#32593;&#32476;&#27969;&#37327;&#34892;&#20026;&#39044;&#27979;&#21644;&#20998;&#31867;&#65292;&#20248;&#20808;&#36873;&#25321;&#25968;&#25454;&#21576;&#29616;&#30456;&#20284;&#27169;&#24335;&#30340;&#23458;&#25143;&#21442;&#19982;&#65292;&#20197;&#25552;&#39640;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.05308</link><description>&lt;p&gt;
&#38754;&#23545;HAPS&#20351;&#33021;&#30340;FL&#32593;&#32476;&#20013;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#65292;&#25112;&#30053;&#23458;&#25143;&#36873;&#25321;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Strategic Client Selection to Address Non-IIDness in HAPS-enabled FL Networks. (arXiv:2401.05308v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05308
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#31354;&#24179;&#21488;&#31449;&#65288;HAPS&#65289;&#20351;&#33021;&#30340;&#22402;&#30452;&#24322;&#26500;&#32593;&#32476;&#20013;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#38382;&#39064;&#30340;&#25112;&#30053;&#23458;&#25143;&#36873;&#25321;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#30340;&#32593;&#32476;&#27969;&#37327;&#34892;&#20026;&#39044;&#27979;&#21644;&#20998;&#31867;&#65292;&#20248;&#20808;&#36873;&#25321;&#25968;&#25454;&#21576;&#29616;&#30456;&#20284;&#27169;&#24335;&#30340;&#23458;&#25143;&#21442;&#19982;&#65292;&#20197;&#25552;&#39640;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30001;&#39640;&#31354;&#24179;&#21488;&#31449;&#65288;HAPS&#65289;&#20351;&#33021;&#30340;&#22402;&#30452;&#24322;&#26500;&#32593;&#32476;&#20013;&#37096;&#32626;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#20026;&#21508;&#31181;&#19981;&#21516;&#36890;&#20449;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#23458;&#25143;&#25552;&#20379;&#20102;&#21442;&#19982;&#30340;&#26426;&#20250;&#12290;&#36825;&#31181;&#22810;&#26679;&#24615;&#19981;&#20165;&#25552;&#39640;&#20102;FL&#27169;&#22411;&#30340;&#35757;&#32451;&#31934;&#24230;&#65292;&#36824;&#21152;&#24555;&#20102;&#20854;&#25910;&#25947;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#24191;&#38420;&#30340;&#32593;&#32476;&#20013;&#24212;&#29992;FL&#23384;&#22312;&#26174;&#33879;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#38382;&#39064;&#12290;&#36825;&#31181;&#25968;&#25454;&#24322;&#36136;&#24615;&#24448;&#24448;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#21644;&#27169;&#22411;&#35757;&#32451;&#24615;&#33021;&#30340;&#38477;&#20302;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#27492;&#38382;&#39064;&#30340;&#23458;&#25143;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#29992;&#25143;&#32593;&#32476;&#27969;&#37327;&#34892;&#20026;&#36827;&#34892;&#39044;&#27979;&#21644;&#20998;&#31867;&#12290;&#35813;&#31574;&#30053;&#36890;&#36807;&#25112;&#30053;&#24615;&#36873;&#25321;&#25968;&#25454;&#21576;&#29616;&#30456;&#20284;&#27169;&#24335;&#30340;&#23458;&#25143;&#21442;&#19982;&#65292;&#21516;&#26102;&#20248;&#20808;&#32771;&#34385;&#29992;&#25143;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment of federated learning (FL) within vertical heterogeneous networks, such as those enabled by high-altitude platform station (HAPS), offers the opportunity to engage a wide array of clients, each endowed with distinct communication and computational capabilities. This diversity not only enhances the training accuracy of FL models but also hastens their convergence. Yet, applying FL in these expansive networks presents notable challenges, particularly the significant non-IIDness in client data distributions. Such data heterogeneity often results in slower convergence rates and reduced effectiveness in model training performance. Our study introduces a client selection strategy tailored to address this issue, leveraging user network traffic behaviour. This strategy involves the prediction and classification of clients based on their network usage patterns while prioritizing user privacy. By strategically selecting clients whose data exhibit similar patterns for participation
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25506;&#35752;&#20102;&#20869;&#23481;&#25512;&#33616;&#31995;&#32479;&#21487;&#33021;&#23545;&#29992;&#25143;&#20135;&#29983;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#25351;&#20986;&#36825;&#31181;&#24433;&#21709;&#19981;&#20165;&#21487;&#33021;&#30001;&#24179;&#21488;&#30446;&#26631;&#19982;&#29992;&#25143;&#31119;&#21033;&#19981;&#19968;&#33268;&#24341;&#36215;&#65292;&#36824;&#21487;&#33021;&#30001;&#23398;&#20064;&#31639;&#27861;&#23545;&#19981;&#21516;&#20869;&#23481;&#30340;&#21453;&#39304;&#29575;&#24046;&#24322;&#36896;&#25104;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#22810;&#33218;&#36172;&#21338;&#26426;&#26694;&#26550;&#21644;&#27010;&#29575;&#21453;&#39304;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.05304</link><description>&lt;p&gt;
&#33021;&#21542;&#29992;&#27010;&#29575;&#21453;&#39304;&#25512;&#21160;&#22312;&#32447;&#24179;&#21488;&#23545;&#29992;&#25143;&#20135;&#29983;&#24433;&#21709;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Probabilistic Feedback Drive User Impacts in Online Platforms?. (arXiv:2401.05304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05304
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#35752;&#20102;&#20869;&#23481;&#25512;&#33616;&#31995;&#32479;&#21487;&#33021;&#23545;&#29992;&#25143;&#20135;&#29983;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#25351;&#20986;&#36825;&#31181;&#24433;&#21709;&#19981;&#20165;&#21487;&#33021;&#30001;&#24179;&#21488;&#30446;&#26631;&#19982;&#29992;&#25143;&#31119;&#21033;&#19981;&#19968;&#33268;&#24341;&#36215;&#65292;&#36824;&#21487;&#33021;&#30001;&#23398;&#20064;&#31639;&#27861;&#23545;&#19981;&#21516;&#20869;&#23481;&#30340;&#21453;&#39304;&#29575;&#24046;&#24322;&#36896;&#25104;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#22810;&#33218;&#36172;&#21338;&#26426;&#26694;&#26550;&#21644;&#27010;&#29575;&#21453;&#39304;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35265;&#30340;&#35299;&#37322;&#26159;&#20869;&#23481;&#25512;&#33616;&#31995;&#32479;&#23545;&#29992;&#25143;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#26159;&#30001;&#20110;&#24179;&#21488;&#30446;&#26631;&#19982;&#29992;&#25143;&#31119;&#21033;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24179;&#21488;&#30446;&#26631;&#19981;&#19968;&#33268;&#24182;&#19981;&#26159;&#23545;&#29992;&#25143;&#20135;&#29983;&#24847;&#22806;&#24433;&#21709;&#30340;&#21807;&#19968;&#28508;&#22312;&#21407;&#22240;&#65306;&#21363;&#20351;&#24179;&#21488;&#30446;&#26631;&#23436;&#20840;&#19982;&#29992;&#25143;&#31119;&#21033;&#19968;&#33268;&#65292;&#23398;&#20064;&#31639;&#27861;&#20063;&#21487;&#33021;&#23545;&#29992;&#25143;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#36825;&#20123;&#29992;&#25143;&#24433;&#21709;&#30340;&#26469;&#28304;&#26159;&#19981;&#21516;&#20869;&#23481;&#21487;&#33021;&#20197;&#19981;&#21516;&#30340;&#36895;&#29575;&#20135;&#29983;&#21487;&#35266;&#23519;&#30340;&#29992;&#25143;&#21453;&#24212;&#65288;&#21453;&#39304;&#20449;&#24687;&#65289;&#65307;&#36825;&#20123;&#21453;&#39304;&#36895;&#29575;&#21487;&#33021;&#19982;&#24433;&#21709;&#29992;&#25143;&#20307;&#39564;&#30340;&#20869;&#23481;&#23646;&#24615;&#65288;&#22914;&#20105;&#35758;&#24615;&#25110;&#21019;&#20316;&#32773;&#30340;&#20154;&#21475;&#30456;&#20284;&#24230;&#65289;&#30456;&#20851;&#12290;&#30001;&#20110;&#21453;&#39304;&#36895;&#29575;&#30340;&#24046;&#24322;&#21487;&#33021;&#20250;&#24433;&#21709;&#23398;&#20064;&#31639;&#27861;&#19982;&#19981;&#21516;&#20869;&#23481;&#30340;&#20132;&#20114;&#39057;&#29575;&#65292;&#23398;&#20064;&#31639;&#27861;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#25512;&#24191;&#20855;&#26377;&#26576;&#20123;&#29305;&#23450;&#23646;&#24615;&#30340;&#20869;&#23481;&#12290;&#20351;&#29992;&#22810;&#33218;&#36172;&#21338;&#26426;&#26694;&#26550;&#19982;&#27010;&#29575;&#21453;&#39304;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common explanation for negative user impacts of content recommender systems is misalignment between the platform's objective and user welfare. In this work, we show that misalignment in the platform's objective is not the only potential cause of unintended impacts on users: even when the platform's objective is fully aligned with user welfare, the platform's learning algorithm can induce negative downstream impacts on users. The source of these user impacts is that different pieces of content may generate observable user reactions (feedback information) at different rates; these feedback rates may correlate with content properties, such as controversiality or demographic similarity of the creator, that affect the user experience. Since differences in feedback rates can impact how often the learning algorithm engages with different content, the learning algorithm may inadvertently promote content with certain such properties. Using the multi-armed bandit framework with probabilistic f
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#31890;&#23376;&#25506;&#27979;&#22120;&#32570;&#20047;&#33033;&#20914;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20855;&#26377;&#30456;&#21516;&#24418;&#29366;&#30340;&#33033;&#20914;&#65292;&#24182;&#19982;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#30456;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2401.05295</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#20174;&#31890;&#23376;&#25506;&#27979;&#22120;&#21512;&#25104;&#33033;&#20914;
&lt;/p&gt;
&lt;p&gt;
Synthesis of pulses from particle detectors with a Generative Adversarial Network (GAN). (arXiv:2401.05295v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05295
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#31890;&#23376;&#25506;&#27979;&#22120;&#32570;&#20047;&#33033;&#20914;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20855;&#26377;&#30456;&#21516;&#24418;&#29366;&#30340;&#33033;&#20914;&#65292;&#24182;&#19982;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#22312;&#24320;&#21457;&#30456;&#20851;&#30340;&#30005;&#23376;&#35774;&#22791;&#26399;&#38388;&#31890;&#23376;&#25506;&#27979;&#22120;&#21487;&#33021;&#32570;&#20047;&#25110;&#23436;&#20840;&#27809;&#26377;&#33033;&#20914;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#19981;&#22833;&#21435;&#30495;&#23454;&#33033;&#20914;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#23427;&#20204;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#21363;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#20854;&#35757;&#32451;&#26041;&#27861;&#21644;&#20351;&#29992;&#26469;&#33258;&#26174;&#31034;&#38378;&#28865;&#26230;&#20307;&#30001;${}^{137}$Cs&#21644;${}^{22}$Na&#25918;&#23556;&#28304;&#25509;&#25910;&#30340;&#30495;&#23454;&#33033;&#20914;&#26469;&#35757;&#32451;GAN&#30340;&#26041;&#27861;&#12290;&#29983;&#25104;&#22120;&#34987;&#23433;&#35013;&#22312;Xilinx&#30340;&#29255;&#19978;&#31995;&#32479;&#65288;SoC&#65289;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#32593;&#32476;&#33021;&#22815;&#29983;&#25104;&#19982;&#30495;&#23454;&#33033;&#20914;&#30456;&#21516;&#24418;&#29366;&#30340;&#33033;&#20914;&#65292;&#29978;&#33267;&#19982;&#21407;&#22987;&#33033;&#20914;&#39640;&#24230;&#30452;&#26041;&#22270;&#25968;&#25454;&#30340;&#25968;&#25454;&#20998;&#24067;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address the possible lack or total absence of pulses from particle detectors during the development of its associate electronics, we propose a model that can generate them without losing the features of the real ones. This model is based on artificial neural networks, namely Generative Adversarial Networks (GAN). We describe the proposed network architecture, its training methodology and the approach to train the GAN with real pulses from a scintillator receiving radiation from sources of ${}^{137}$Cs and ${}^{22}$Na. The Generator was installed in a Xilinx's System-On-Chip (SoC). We show how the network is capable of generating pulses with the same shape as the real ones that even match the data distributions in the original pulse-height histogram data.
&lt;/p&gt;</description></item><item><title>AUTOACT&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#21512;&#25104;&#36712;&#36857;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05268</link><description>&lt;p&gt;
AUTOACT&#65306;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#23454;&#29616;&#30340;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AUTOACT: Automatic Agent Learning from Scratch via Self-Planning. (arXiv:2401.05268v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05268
&lt;/p&gt;
&lt;p&gt;
AUTOACT&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#21512;&#25104;&#36712;&#36857;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#19981;&#26029;&#30340;&#25506;&#32034;&#65292;&#20294;&#29616;&#26377;&#30340;&#35821;&#35328;&#20195;&#29702;&#31995;&#32479;&#20173;&#28982;&#38754;&#20020;&#26114;&#36149;&#12289;&#19981;&#21487;&#37325;&#22797;&#30340;&#25968;&#25454;&#20381;&#36182;&#38382;&#39064;&#65292;&#24182;&#19988;&#38754;&#20020;&#23558;&#21333;&#19968;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#20010;&#21151;&#33021;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoAct&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#21644;&#26469;&#33258;&#38381;&#28304;&#27169;&#22411;&#65288;&#22914;GPT-4&#65289;&#30340;&#21512;&#25104;&#36712;&#36857;&#12290;&#32473;&#23450;&#26377;&#38480;&#30340;&#25968;&#25454;&#21644;&#24037;&#20855;&#24211;&#65292;AutoAct&#39318;&#20808;&#33258;&#21160;&#21512;&#25104;&#35268;&#21010;&#36712;&#36857;&#65292;&#19981;&#38656;&#35201;&#20154;&#31867;&#25110;&#24378;&#38381;&#28304;&#27169;&#22411;&#30340;&#20219;&#20309;&#36741;&#21161;&#12290;&#28982;&#21518;&#65292;AutoAct&#21033;&#29992;&#20998;&#24037;&#31574;&#30053;&#65292;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#20449;&#24687;&#21644;&#21512;&#25104;&#36712;&#36857;&#33258;&#21160;&#21306;&#20998;&#65292;&#20135;&#29983;&#19968;&#20010;&#23376;&#20195;&#29702;&#32452;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#31181;LLMs&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;AutoAct&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#25110;&#19982;&#20854;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to var
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;N&#32500;B&#26679;&#26465;&#20960;&#20309;&#36827;&#34892;&#25511;&#21046;&#22120;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#33258;&#20027;&#20915;&#23450;&#25511;&#21046;&#22120;&#21442;&#25968;&#30340;&#35843;&#25972;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#31616;&#21270;&#23545;&#20110;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#25511;&#21046;&#22120;&#21442;&#25968;&#21270;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2401.05251</link><description>&lt;p&gt;
ReACT: &#20351;&#29992;B&#26679;&#26465;&#20960;&#20309;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#25511;&#21046;&#22120;&#21442;&#25968;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
ReACT: Reinforcement Learning for Controller Parametrization using B-Spline Geometries. (arXiv:2401.05251v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;N&#32500;B&#26679;&#26465;&#20960;&#20309;&#36827;&#34892;&#25511;&#21046;&#22120;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#33258;&#20027;&#20915;&#23450;&#25511;&#21046;&#22120;&#21442;&#25968;&#30340;&#35843;&#25972;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#31616;&#21270;&#23545;&#20110;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#25511;&#21046;&#22120;&#21442;&#25968;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#22823;&#19988;&#39640;&#25928;&#30340;&#25511;&#21046;&#22120;&#23545;&#20110;&#24037;&#19994;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#31995;&#32479;&#23548;&#20986;&#25511;&#21046;&#22120;&#21442;&#25968;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#32791;&#26102;&#30340;&#12290;&#20026;&#20102;&#20415;&#20110;&#33258;&#21160;&#25511;&#21046;&#22120;&#21442;&#25968;&#21270;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#21644;N&#32500;B&#26679;&#26465;&#20960;&#20309;&#65288;BSG&#65289;&#12290;&#25105;&#20204;&#20851;&#27880;&#21442;&#25968;&#21464;&#21270;&#31995;&#32479;&#30340;&#25511;&#21046;&#65292;&#36825;&#26159;&#19968;&#31867;&#34892;&#20026;&#22797;&#26434;&#19988;&#21462;&#20915;&#20110;&#36816;&#34892;&#26465;&#20214;&#30340;&#31995;&#32479;&#12290;&#23545;&#20110;&#36825;&#19968;&#31995;&#32479;&#31867;&#21035;&#65292;&#30001;&#20110;&#20854;&#24050;&#30693;&#30340;&#35774;&#35745;&#21407;&#29702;&#65292;&#22686;&#30410;&#35843;&#24230;&#25511;&#21046;&#32467;&#26500;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#34892;&#19994;&#30340;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#31616;&#21270;&#23545;&#20110;&#36825;&#20123;&#25511;&#21046;&#32467;&#26500;&#30340;&#25511;&#21046;&#22120;&#21442;&#25968;&#21270;&#20219;&#21153;&#65292;&#25105;&#20204;&#37096;&#32626;&#20102;&#19968;&#20010;DRL&#20195;&#29702;&#12290;&#22522;&#20110;&#25511;&#21046;&#31995;&#32479;&#35266;&#27979;&#65292;&#20195;&#29702;&#33258;&#20027;&#20915;&#23450;&#22914;&#20309;&#35843;&#25972;&#25511;&#21046;&#22120;&#21442;&#25968;&#12290;&#36890;&#36807;&#24341;&#20837;BSG&#26469;&#26144;&#23556;&#21487;&#33021;&#20381;&#36182;&#20110;&#22810;&#20010;&#21464;&#37327;&#30340;&#25511;&#21046;&#22120;&#21442;&#25968;&#65292;&#25105;&#20204;&#20351;&#24471;&#36866;&#24212;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust and performant controllers are essential for industrial applications. However, deriving controller parameters for complex and nonlinear systems is challenging and time-consuming. To facilitate automatic controller parametrization, this work presents a novel approach using deep reinforcement learning (DRL) with N-dimensional B-spline geometries (BSGs). We focus on the control of parameter-variant systems, a class of systems with complex behavior which depends on the operating conditions. For this system class, gain-scheduling control structures are widely used in applications across industries due to well-known design principles. Facilitating the expensive controller parametrization task regarding these control structures, we deploy an DRL agent. Based on control system observations, the agent autonomously decides how to adapt the controller parameters. We make the adaptation process more efficient by introducing BSGs to map the controller parameters which may depend on numerous 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;Hamilton&#31070;&#32463;&#32593;&#32476;&#30340;Monte Carlo&#37319;&#26679;&#30340;&#23376;&#38598;&#27169;&#25311;&#26041;&#27861;&#26469;&#36827;&#34892;&#22797;&#26434;&#31995;&#32479;&#21487;&#38752;&#24615;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20248;&#36234;&#30340;&#37319;&#26679;&#21644;&#39640;&#25928;&#30340;&#26799;&#24230;&#35780;&#20272;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#25509;&#21463;&#29575;&#21644;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#19981;&#21516;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20854;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19982;&#20256;&#32479;&#30340;Hamilton Monte Carlo&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#26041;&#27861;&#22312;&#26799;&#24230;&#35780;&#20272;&#26041;&#38754;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.05244</link><description>&lt;p&gt;
&#20351;&#29992;Hamilton&#31070;&#32463;&#32593;&#32476;&#30340;&#23376;&#38598;&#27169;&#25311;&#36827;&#34892;&#22797;&#26434;&#31995;&#32479;&#21487;&#38752;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Reliability Analysis of Complex Systems using Subset Simulations with Hamiltonian Neural Networks. (arXiv:2401.05244v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05244
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;Hamilton&#31070;&#32463;&#32593;&#32476;&#30340;Monte Carlo&#37319;&#26679;&#30340;&#23376;&#38598;&#27169;&#25311;&#26041;&#27861;&#26469;&#36827;&#34892;&#22797;&#26434;&#31995;&#32479;&#21487;&#38752;&#24615;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20248;&#36234;&#30340;&#37319;&#26679;&#21644;&#39640;&#25928;&#30340;&#26799;&#24230;&#35780;&#20272;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#25509;&#21463;&#29575;&#21644;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#19981;&#21516;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20854;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19982;&#20256;&#32479;&#30340;Hamilton Monte Carlo&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#26041;&#27861;&#22312;&#26799;&#24230;&#35780;&#20272;&#26041;&#38754;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#22522;&#20110;Hamilton&#31070;&#32463;&#32593;&#32476;&#30340;Monte Carlo&#37319;&#26679;&#30340;&#23376;&#38598;&#27169;&#25311;&#26041;&#27861;&#65292;&#29992;&#20110;&#21487;&#38752;&#24615;&#20998;&#26512;&#12290;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#23558;Hamilton Monte Carlo&#26041;&#27861;&#30340;&#20248;&#36234;&#37319;&#26679;&#19982;&#20351;&#29992;Hamilton&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35745;&#31639;&#39640;&#25928;&#26799;&#24230;&#35780;&#20272;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#32452;&#21512;&#29305;&#21035;&#26377;&#20248;&#21183;&#65292;&#22240;&#20026;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20445;&#25345;&#20102;Hamiltonian&#30340;&#29305;&#24615;&#65292;&#32780;Hamiltonian&#23450;&#20041;&#20102;Hamilton Monte Carlo&#37319;&#26679;&#22120;&#30340;&#25509;&#21463;&#20934;&#21017;&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#31574;&#30053;&#22312;&#20302;&#35745;&#31639;&#25104;&#26412;&#19979;&#21487;&#20197;&#23454;&#29616;&#39640;&#25509;&#21463;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#23376;&#38598;&#27169;&#25311;&#26469;&#20272;&#35745;&#23567;&#27010;&#29575;&#22833;&#25928;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#27010;&#29575;&#26679;&#26412;&#21306;&#22495;&#20013;&#65292;&#26799;&#24230;&#35780;&#20272;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#31574;&#30053;&#30340;&#26174;&#33879;&#20934;&#30830;&#24615;&#65292;&#24182;&#23558;&#20854;&#25928;&#29575;&#19982;&#20256;&#32479;&#30340;Hamilton Monte Carlo&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#26799;&#24230;&#35780;&#20272;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new Subset Simulation approach using Hamiltonian neural network-based Monte Carlo sampling for reliability analysis. The proposed strategy combines the superior sampling of the Hamiltonian Monte Carlo method with computationally efficient gradient evaluations using Hamiltonian neural networks. This combination is especially advantageous because the neural network architecture conserves the Hamiltonian, which defines the acceptance criteria of the Hamiltonian Monte Carlo sampler. Hence, this strategy achieves high acceptance rates at low computational cost. Our approach estimates small failure probabilities using Subset Simulations. However, in low-probability sample regions, the gradient evaluation is particularly challenging. The remarkable accuracy of the proposed strategy is demonstrated on different reliability problems, and its efficiency is compared to the traditional Hamiltonian Monte Carlo method. We note that this approach can reach its limitations for gradient es
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20998;&#31867;&#22120;&#26657;&#20934;&#26469;&#23454;&#29616;&#21453;&#27450;&#35784;&#39044;&#38450;&#20013;&#30340;&#20915;&#31574;&#35299;&#32806;&#12290;&#36890;&#36807;&#20351;&#29992;&#26657;&#20934;&#31574;&#30053;&#65292;&#20182;&#20204;&#21457;&#29616;&#31561;&#36317;&#21644;&#36125;&#22612;&#26657;&#20934;&#26041;&#27861;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#21457;&#29983;&#21464;&#21270;&#30340;&#22330;&#26223;&#19979;&#34920;&#29616;&#31361;&#20986;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;&#20248;&#21270;&#35299;&#32806;&#21162;&#21147;&#30340;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.05240</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#31867;&#22120;&#26657;&#20934;&#23454;&#29616;&#21453;&#27450;&#35784;&#39044;&#38450;&#30340;&#20915;&#31574;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;
Decoupling Decision-Making in Fraud Prevention through Classifier Calibration for Business Logic Action. (arXiv:2401.05240v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05240
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20998;&#31867;&#22120;&#26657;&#20934;&#26469;&#23454;&#29616;&#21453;&#27450;&#35784;&#39044;&#38450;&#20013;&#30340;&#20915;&#31574;&#35299;&#32806;&#12290;&#36890;&#36807;&#20351;&#29992;&#26657;&#20934;&#31574;&#30053;&#65292;&#20182;&#20204;&#21457;&#29616;&#31561;&#36317;&#21644;&#36125;&#22612;&#26657;&#20934;&#26041;&#27861;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#21457;&#29983;&#21464;&#21270;&#30340;&#22330;&#26223;&#19979;&#34920;&#29616;&#31361;&#20986;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;&#20248;&#21270;&#35299;&#32806;&#21162;&#21147;&#30340;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#19987;&#27880;&#20110;&#29305;&#23450;&#30446;&#26631;&#65292;&#27604;&#22914;&#21019;&#24314;&#20998;&#31867;&#22120;&#65292;&#36890;&#24120;&#22522;&#20110;&#21830;&#19994;&#29615;&#22659;&#20013;&#24050;&#30693;&#30340;&#20154;&#32676;&#29305;&#24449;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#20010;&#20307;&#29305;&#24449;&#30340;&#27169;&#22411;&#38543;&#26102;&#38388;&#32780;&#36866;&#24212;&#65292;&#20197;&#25552;&#39640;&#31934;&#24230;&#65292;&#24341;&#20837;&#35299;&#32806;&#30340;&#27010;&#24565;&#65306;&#20174;&#28857;&#35780;&#20272;&#36716;&#21521;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#20351;&#29992;&#26657;&#20934;&#31574;&#30053;&#20316;&#20026;&#35299;&#32806;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20998;&#31867;&#22120;&#19982;&#22522;&#20110;&#24471;&#20998;&#30340;&#19994;&#21153;&#36923;&#36753;&#26694;&#26550;&#20013;&#30340;&#34892;&#21160;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#31574;&#30053;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#30495;&#23454;&#30340;&#21830;&#19994;&#22330;&#26223;&#21644;&#22810;&#20010;ML&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#20986;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26435;&#34913;&#21644;&#24615;&#33021;&#24433;&#21709;&#65292;&#24182;&#20026;&#23547;&#27714;&#20248;&#21270;&#35299;&#32806;&#21162;&#21147;&#30340;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#36716;&#21464;&#30340;&#24773;&#20917;&#19979;&#65292;&#31561;&#36317;&#21644;&#36125;&#22612;&#26657;&#20934;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models typically focus on specific targets like creating classifiers, often based on known population feature distributions in a business context. However, models calculating individual features adapt over time to improve precision, introducing the concept of decoupling: shifting from point evaluation to data distribution. We use calibration strategies as strategy for decoupling machine learning (ML) classifiers from score-based actions within business logic frameworks. To evaluate these strategies, we perform a comparative analysis using a real-world business scenario and multiple ML models. Our findings highlight the trade-offs and performance implications of the approach, offering valuable insights for practitioners seeking to optimize their decoupling efforts. In particular, the Isotonic and Beta calibration methods stand out for scenarios in which there is shift between training and testing data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22312;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20013;&#20998;&#26512;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#35774;&#32622;&#20013;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;&#20998;&#26512;&#21457;&#29616;&#20102;&#20004;&#20010;&#31283;&#23450;&#24615;&#23646;&#24615;&#65292;&#19982;&#20540;&#20989;&#25968;&#21644;/&#25110;&#31574;&#30053;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#36125;&#23572;&#26364;&#31639;&#23376;&#21644;&#21344;&#25454;&#24230;&#27979;&#24230;&#30456;&#20851;&#12290;&#36825;&#20123;&#23646;&#24615;&#22312;&#35768;&#22810;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#25104;&#31435;&#65292;&#24182;&#19988;&#22312;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#26041;&#27861;&#19979;&#33258;&#28982;&#20135;&#29983;&#12290;&#20998;&#26512;&#36824;&#25581;&#31034;&#20102;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#24754;&#35266;&#20027;&#20041;&#21644;&#20048;&#35266;&#20027;&#20041;&#30340;&#35282;&#33394;&#65292;&#20197;&#21450;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.05233</link><description>&lt;p&gt;
&#39535;&#26381;&#8220;&#25968;&#25454;&#39269;&#28212;&#8221;&#30340;&#24378;&#21270;&#23398;&#20064;&#65311;&#22312;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Taming "data-hungry" reinforcement learning? Stability in continuous state-action spaces. (arXiv:2401.05233v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22312;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20013;&#20998;&#26512;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#35774;&#32622;&#20013;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;&#20998;&#26512;&#21457;&#29616;&#20102;&#20004;&#20010;&#31283;&#23450;&#24615;&#23646;&#24615;&#65292;&#19982;&#20540;&#20989;&#25968;&#21644;/&#25110;&#31574;&#30053;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#36125;&#23572;&#26364;&#31639;&#23376;&#21644;&#21344;&#25454;&#24230;&#27979;&#24230;&#30456;&#20851;&#12290;&#36825;&#20123;&#23646;&#24615;&#22312;&#35768;&#22810;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#25104;&#31435;&#65292;&#24182;&#19988;&#22312;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#26041;&#27861;&#19979;&#33258;&#28982;&#20135;&#29983;&#12290;&#20998;&#26512;&#36824;&#25581;&#31034;&#20102;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#24754;&#35266;&#20027;&#20041;&#21644;&#20048;&#35266;&#20027;&#20041;&#30340;&#35282;&#33394;&#65292;&#20197;&#21450;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22312;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#20013;&#20998;&#26512;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#35777;&#26126;&#31163;&#32447;&#21644;&#22312;&#32447;&#35774;&#32622;&#20013;&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#30340;&#31283;&#23450;&#24615;&#23646;&#24615;&#65292;&#28041;&#21450;&#20540;&#20989;&#25968;&#21644;/&#25110;&#31574;&#30053;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#36125;&#23572;&#26364;&#31639;&#23376;&#21644;&#21344;&#25454;&#24230;&#27979;&#24230;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#23646;&#24615;&#22312;&#35768;&#22810;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#25104;&#31435;&#65292;&#24182;&#28436;&#31034;&#20102;&#22312;&#20351;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#26041;&#27861;&#26102;&#22914;&#20309;&#33258;&#28982;&#22320;&#20135;&#29983;&#36825;&#20123;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#20851;&#20110;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#24754;&#35266;&#20027;&#20041;&#21644;&#20048;&#35266;&#20027;&#20041;&#30340;&#26032;&#35270;&#35282;&#65292;&#24182;&#24378;&#35843;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel framework for analyzing reinforcement learning (RL) in continuous state-action spaces, and use it to prove fast rates of convergence in both off-line and on-line settings. Our analysis highlights two key stability properties, relating to how changes in value functions and/or policies affect the Bellman operator and occupation measures. We argue that these properties are satisfied in many continuous state-action Markov decision processes, and demonstrate how they arise naturally when using linear function approximation methods. Our analysis offers fresh perspectives on the roles of pessimism and optimism in off-line and on-line RL, and highlights the connection between off-line RL and transfer learning.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#65292;&#21457;&#29616;&#26410;&#23545;&#40784;&#21644;&#23545;&#40784;&#30340;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#22312;&#35821;&#20041;&#19978;&#26159;&#30456;&#20284;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21305;&#37197;&#26410;&#23545;&#40784;&#32534;&#30721;&#22120;&#65292;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2401.05224</link><description>&lt;p&gt;
&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#26159;&#21542;&#20197;&#30456;&#20284;&#26041;&#24335;&#34920;&#31034;&#19990;&#30028;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Vision and Language Encoders Represent the World Similarly?. (arXiv:2401.05224v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05224
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#65292;&#21457;&#29616;&#26410;&#23545;&#40784;&#21644;&#23545;&#40784;&#30340;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#22312;&#35821;&#20041;&#19978;&#26159;&#30456;&#20284;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21305;&#37197;&#26410;&#23545;&#40784;&#32534;&#30721;&#22120;&#65292;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#25104;&#20026;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#20107;&#23454;&#19978;&#30340;&#27169;&#22411;&#30340;&#23545;&#40784;&#30340;&#25991;&#26412;-&#22270;&#20687;&#32534;&#30721;&#22120;&#65288;&#22914;CLIP&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#27169;&#24577;&#29305;&#23450;&#30340;&#32534;&#30721;&#22120;&#22312;&#21508;&#33258;&#39046;&#22495;&#20013;&#20063;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65306;&#30001;&#20110;&#23427;&#20204;&#22522;&#26412;&#19978;&#34920;&#31034;&#21516;&#19968;&#20010;&#29289;&#29702;&#19990;&#30028;&#65292;&#21333;&#27169;&#24577;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#20043;&#38388;&#26159;&#21542;&#23384;&#22312;&#23545;&#40784;&#65311;&#36890;&#36807;&#20351;&#29992;&#20013;&#24515;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#20998;&#26512;&#22270;&#20687;-&#26631;&#39064;&#22522;&#20934;&#19978;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#65292;&#25105;&#20204;&#21457;&#29616;&#26410;&#23545;&#40784;&#21644;&#23545;&#40784;&#30340;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#22312;&#35821;&#20041;&#19978;&#26159;&#30456;&#20284;&#30340;&#12290;&#22312;&#20687;CLIP&#36825;&#26679;&#30340;&#23545;&#40784;&#32534;&#30721;&#22120;&#20013;&#32570;&#20047;&#32479;&#35745;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#26174;&#31034;&#20102;&#21487;&#33021;&#23384;&#22312;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#30340;&#26410;&#23545;&#40784;&#32534;&#30721;&#22120;&#30340;&#21305;&#37197;&#12290;&#25105;&#20204;&#23558;&#36825;&#35270;&#20026;&#21033;&#29992;&#22270;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26377;&#31181;&#23376;&#22270;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861; - &#24555;&#36895;&#20108;&#27425;&#20998;&#37197;&#38382;&#39064;&#20248;&#21270;&#21644;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#30340;&#23616;&#37096;CKA&#24230;&#37327;&#30340;&#21305;&#37197;/&#26816;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligned text-image encoders such as CLIP have become the de facto model for vision-language tasks. Furthermore, modality-specific encoders achieve impressive performances in their respective domains. This raises a central question: does an alignment exist between uni-modal vision and language encoders since they fundamentally represent the same physical world? Analyzing the latent spaces structure of vision and language models on image-caption benchmarks using the Centered Kernel Alignment (CKA), we find that the representation spaces of unaligned and aligned encoders are semantically similar. In the absence of statistical similarity in aligned encoders like CLIP, we show that a possible matching of unaligned encoders exists without any training. We frame this as a seeded graph-matching problem exploiting the semantic similarity between graphs and propose two methods - a Fast Quadratic Assignment Problem optimization, and a novel localized CKA metric-based matching/retrieval. We demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;ICP&#21407;&#21017;&#65292;&#32771;&#34385;&#20102;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20855;&#26377;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#30340;&#19981;&#21464;&#22240;&#26524;&#39044;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#25552;&#20379;&#22240;&#26524;&#29238;&#33410;&#28857;&#30340;&#21487;&#36776;&#35782;&#24615;&#26465;&#20214;&#21644;&#24341;&#20837;LoLICaP&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#35266;&#23519;&#25968;&#25454;&#20013;&#35782;&#21035;&#30446;&#26631;&#21464;&#37327;&#30340;&#22240;&#26524;&#29238;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.05218</link><description>&lt;p&gt;
&#20855;&#26377;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#30340;&#19981;&#21464;&#22240;&#26524;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Invariant Causal Prediction with Locally Linear Models. (arXiv:2401.05218v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;ICP&#21407;&#21017;&#65292;&#32771;&#34385;&#20102;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20855;&#26377;&#23616;&#37096;&#32447;&#24615;&#27169;&#22411;&#30340;&#19981;&#21464;&#22240;&#26524;&#39044;&#27979;&#20219;&#21153;&#12290;&#36890;&#36807;&#25552;&#20379;&#22240;&#26524;&#29238;&#33410;&#28857;&#30340;&#21487;&#36776;&#35782;&#24615;&#26465;&#20214;&#21644;&#24341;&#20837;LoLICaP&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#35266;&#23519;&#25968;&#25454;&#20013;&#35782;&#21035;&#30446;&#26631;&#21464;&#37327;&#30340;&#22240;&#26524;&#29238;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#36890;&#36807;&#35266;&#23519;&#25968;&#25454;&#65292;&#20174;&#19968;&#32452;&#20505;&#36873;&#21464;&#37327;&#20013;&#35782;&#21035;&#20986;&#30446;&#26631;&#21464;&#37327;&#30340;&#22240;&#26524;&#29238;&#33410;&#28857;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20551;&#35774;&#26159;&#20505;&#36873;&#21464;&#37327;&#22312;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#34987;&#35266;&#23519;&#21040;&#65292;&#36825;&#20123;&#29615;&#22659;&#21487;&#20197;&#23545;&#24212;&#20110;&#26426;&#22120;&#30340;&#19981;&#21516;&#35774;&#32622;&#25110;&#32773;&#21160;&#24577;&#36807;&#31243;&#20013;&#30340;&#19981;&#21516;&#26102;&#38388;&#38388;&#38548;&#31561;&#12290;&#22312;&#19968;&#23450;&#30340;&#20551;&#35774;&#26465;&#20214;&#19979;&#65292;&#19981;&#21516;&#30340;&#29615;&#22659;&#21487;&#20197;&#34987;&#35270;&#20026;&#23545;&#35266;&#23519;&#31995;&#32479;&#30340;&#24178;&#39044;&#12290;&#25105;&#20204;&#20551;&#35774;&#30446;&#26631;&#21464;&#37327;&#21644;&#21327;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#32447;&#24615;&#20851;&#31995;&#65292;&#22312;&#27599;&#20010;&#29615;&#22659;&#19979;&#21487;&#33021;&#19981;&#21516;&#65292;&#20294;&#22240;&#26524;&#32467;&#26500;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#26159;&#19981;&#21464;&#30340;&#12290;&#36825;&#26159;Peters&#31561;&#20154;[2016]&#25552;&#20986;&#30340;ICP&#65288;&#19981;&#21464;&#22240;&#26524;&#39044;&#27979;&#65289;&#21407;&#21017;&#30340;&#25193;&#23637;&#65292;&#21518;&#32773;&#20551;&#35774;&#25152;&#26377;&#29615;&#22659;&#19979;&#23384;&#22312;&#19968;&#20010;&#22266;&#23450;&#30340;&#32447;&#24615;&#20851;&#31995;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#22240;&#26524;&#29238;&#33410;&#28857;&#21487;&#36776;&#35782;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;LoLICaP&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the task of identifying the causal parents of a target variable among a set of candidate variables from observational data. Our main assumption is that the candidate variables are observed in different environments which may, for example, correspond to different settings of a machine or different time intervals in a dynamical process. Under certain assumptions different environments can be regarded as interventions on the observed system. We assume a linear relationship between target and covariates, which can be different in each environment with the only restriction that the causal structure is invariant across environments. This is an extension of the ICP ($\textbf{I}$nvariant $\textbf{C}$ausal $\textbf{P}$rediction) principle by Peters et al. [2016], who assumed a fixed linear relationship across all environments. Within our proposed setting we provide sufficient conditions for identifiability of the causal parents and introduce a practical method called LoLICaP ($\text
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#39640;&#38454;&#38544;&#24335;&#40857;&#26684;-&#24211;&#22612;&#26041;&#27861;&#26469;&#20272;&#35745;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#30340;&#35823;&#24046;&#65292;&#22312;&#36712;&#36857;&#30340;&#22810;&#20010;&#28857;&#19978;&#25552;&#20379;&#31934;&#30830;&#30340;&#35823;&#24046;&#20272;&#35745;&#12290;&#36890;&#36807;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#35823;&#24046;&#39640;&#24230;&#30456;&#20851;&#65292;&#22686;&#21152;IRK&#26041;&#27861;&#30340;&#38454;&#25968;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05211</link><description>&lt;p&gt;
&#20351;&#29992;&#38544;&#24335;&#40857;&#26684;-&#24211;&#22612;&#26041;&#27861;&#30340;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#30340;&#35823;&#24046;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Error estimation for physics-informed neural networks with implicit Runge-Kutta methods. (arXiv:2401.05211v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#39640;&#38454;&#38544;&#24335;&#40857;&#26684;-&#24211;&#22612;&#26041;&#27861;&#26469;&#20272;&#35745;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#30340;&#35823;&#24046;&#65292;&#22312;&#36712;&#36857;&#30340;&#22810;&#20010;&#28857;&#19978;&#25552;&#20379;&#31934;&#30830;&#30340;&#35823;&#24046;&#20272;&#35745;&#12290;&#36890;&#36807;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#35823;&#24046;&#39640;&#24230;&#30456;&#20851;&#65292;&#22686;&#21152;IRK&#26041;&#27861;&#30340;&#38454;&#25968;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#36817;&#20284;&#21160;&#21147;&#31995;&#32479;&#30340;&#36712;&#36857;&#33021;&#22815;&#23454;&#29616;&#23545;&#20854;&#20998;&#26512;&#12289;&#39044;&#27979;&#21644;&#25511;&#21046;&#12290;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36817;&#20284;&#20855;&#26377;&#24555;&#36895;&#35780;&#20272;&#21644;&#38271;&#26102;&#38388;&#27493;&#38271;&#30340;&#20934;&#30830;&#24615;&#65292;&#22240;&#27492;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#19982;&#20256;&#32479;&#30340;&#40857;&#26684;-&#24211;&#22612;&#26041;&#27861;&#31561;&#25968;&#20540;&#36817;&#20284;&#26041;&#26696;&#30456;&#27604;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36817;&#20284;&#35823;&#24046;&#20272;&#35745;&#30456;&#23545;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#32467;&#26524;&#22312;&#39640;&#38454;&#38544;&#24335;&#40857;&#26684;-&#24211;&#22612; (IRK)&#26041;&#27861;&#20013;&#12290;&#38544;&#24335;&#26041;&#31243;&#32452;&#20013;&#30340;&#27531;&#24046;&#21487;&#20197;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#35823;&#24046;&#30456;&#20851;&#32852;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#22312;&#36712;&#36857;&#30340;&#22810;&#20010;&#28857;&#19978;&#25552;&#20379;&#35823;&#24046;&#20272;&#35745;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#20010;&#35823;&#24046;&#20272;&#35745;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#35823;&#24046;&#39640;&#24230;&#30456;&#20851;&#65292;&#24182;&#19988;&#22686;&#21152;IRK&#26041;&#27861;&#30340;&#38454;&#25968;&#21487;&#20197;&#25913;&#21892;&#36825;&#20010;&#20272;&#35745;&#12290;&#25105;&#20204;&#20197;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476; (PINNs)&#22312;&#36923;&#36753;&#26041;&#31243;&#19978;&#36827;&#34892;&#20102;&#36825;&#20010;&#20272;&#35745;&#26041;&#27861;&#30340;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to accurately approximate trajectories of dynamical systems enables their analysis, prediction, and control. Neural network (NN)-based approximations have attracted significant interest due to fast evaluation with good accuracy over long integration time steps. In contrast to established numerical approximation schemes such as Runge-Kutta methods, the estimation of the error of the NN-based approximations proves to be difficult. In this work, we propose to use the NN's predictions in a high-order implicit Runge-Kutta (IRK) method. The residuals in the implicit system of equations can be related to the NN's prediction error, hence, we can provide an error estimate at several points along a trajectory. We find that this error estimate highly correlates with the NN's prediction error and that increasing the order of the IRK method improves this estimate. We demonstrate this estimation methodology for Physics-Informed Neural Network (PINNs) on the logistic equation as an illust
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#34920;&#38754;&#25705;&#25830;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24471;&#21040;&#31526;&#21512;&#25705;&#25830;&#35201;&#27714;&#30340;&#34920;&#38754;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2401.05206</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23450;&#21046;&#34920;&#38754;&#25705;&#25830;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Tailoring Frictional Properties of Surfaces Using Diffusion Models. (arXiv:2401.05206v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#34920;&#38754;&#25705;&#25830;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24471;&#21040;&#31526;&#21512;&#25705;&#25830;&#35201;&#27714;&#30340;&#34920;&#38754;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#20855;&#20307;&#26159;&#25193;&#25955;&#21435;&#22122;&#27010;&#29575;&#27169;&#22411;DDPM&#65289;&#31934;&#30830;&#35774;&#35745;&#34920;&#38754;&#25705;&#25830;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#30001;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#30830;&#23450;&#25705;&#25830;&#24615;&#36136;&#30340;&#21512;&#25104;&#34920;&#38754;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#35757;&#32451;DDPM&#26469;&#39044;&#27979;&#19982;&#25152;&#26399;&#26395;&#30340;&#25705;&#25830;&#24615;&#33021;&#30456;&#23545;&#24212;&#30340;&#34920;&#38754;&#32467;&#26500;&#12290;&#19982;&#20256;&#32479;&#30340;&#35797;&#38169;&#21644;&#25968;&#20540;&#20248;&#21270;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20197;&#39640;&#20934;&#30830;&#24615;&#21644;&#39640;&#25928;&#29575;&#30452;&#25509;&#24471;&#21040;&#31526;&#21512;&#25351;&#23450;&#25705;&#25830;&#26631;&#20934;&#30340;&#34920;&#38754;&#35774;&#35745;&#12290;&#36825;&#19968;&#26448;&#26009;&#34920;&#38754;&#24037;&#31243;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#20943;&#23569;&#34920;&#38754;&#35774;&#35745;&#36807;&#31243;&#20013;&#36845;&#20195;&#24615;&#36136;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#19981;&#20165;&#20026;&#31934;&#30830;&#23450;&#21046;&#34920;&#38754;&#24615;&#33021;&#25552;&#20379;&#20102;&#19968;&#26465;&#26032;&#36884;&#24452;&#65292;&#32780;&#19988;&#36824;&#25552;&#20986;&#20102;&#22312;&#34920;&#38754;&#29305;&#24615;&#20851;&#38190;&#30340;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#30340;&#26356;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This Letter introduces an approach for precisely designing surface friction properties using a conditional generative machine learning model, specifically a diffusion denoising probabilistic model (DDPM). We created a dataset of synthetic surfaces with frictional properties determined by molecular dynamics simulations, which trained the DDPM to predict surface structures from desired frictional outcomes. Unlike traditional trial-and-error and numerical optimization methods, our approach directly yields surface designs meeting specified frictional criteria with high accuracy and efficiency. This advancement in material surface engineering demonstrates the potential of machine learning in reducing the iterative nature of surface design processes. Our findings not only provide a new pathway for precise surface property tailoring but also suggest broader applications in material science where surface characteristics are critical.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#19978;&#19979;&#25991;&#20851;&#32852;&#36172;&#21338;&#38382;&#39064;&#20013;&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#36827;&#34892;&#23454;&#39564;&#35268;&#21010;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#19982;&#20989;&#25968;&#36924;&#36817;&#20860;&#23481;&#30340;&#23454;&#39564;&#35268;&#21010;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.05193</link><description>&lt;p&gt;
&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#36827;&#34892;&#23454;&#39564;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Experiment Planning with Function Approximation. (arXiv:2401.05193v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#19978;&#19979;&#25991;&#20851;&#32852;&#36172;&#21338;&#38382;&#39064;&#20013;&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#36827;&#34892;&#23454;&#39564;&#35268;&#21010;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#19982;&#20989;&#25968;&#36924;&#36817;&#20860;&#23481;&#30340;&#23454;&#39564;&#35268;&#21010;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19978;&#19979;&#25991;&#20851;&#32852;&#36172;&#21338;&#38382;&#39064;&#20013;&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#36827;&#34892;&#23454;&#39564;&#35268;&#21010;&#30340;&#38382;&#39064;&#12290;&#22312;&#23384;&#22312;&#37096;&#32626;&#33258;&#36866;&#24212;&#31639;&#27861;&#30340;&#26174;&#33879;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#24403;&#25191;&#34892;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#38656;&#35201;&#20998;&#24067;&#24335;&#25110;&#38656;&#35201;&#20154;&#24037;&#21442;&#19982;&#26102;&#65292;&#25552;&#21069;&#29983;&#25104;&#19968;&#32452;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#22823;&#22411;&#19978;&#19979;&#25991;&#25968;&#25454;&#38598;&#21487;&#29992;&#20294;&#22870;&#21169;&#25968;&#25454;&#19981;&#21487;&#29992;&#30340;&#24773;&#26223;&#65292;&#23398;&#20064;&#32773;&#21487;&#20197;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#35774;&#35745;&#19968;&#20010;&#26377;&#25928;&#30340;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#12290;&#34429;&#28982;&#24403;&#22870;&#21169;&#26159;&#32447;&#24615;&#30340;&#26102;&#20505;&#65292;&#36825;&#20010;&#38382;&#39064;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#20173;&#28982;&#32570;&#20047;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#19982;&#20989;&#25968;&#36924;&#36817;&#20860;&#23481;&#30340;&#23454;&#39564;&#35268;&#21010;&#31574;&#30053;&#12290;&#31532;&#19968;&#31181;&#26159;&#36867;&#36991;&#32773;&#35268;&#21010;&#21644;&#37319;&#26679;&#36807;&#31243;&#65292;&#21487;&#20197;&#26681;&#25454;&#36867;&#36991;&#32773;&#32500;&#24230;&#30340;&#22870;&#21169;&#20989;&#25968;&#31867;&#33719;&#24471;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#23545;&#20110;&#31532;&#20108;&#31181;&#31574;&#30053;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
We study the problem of experiment planning with function approximation in contextual bandit problems. In settings where there is a significant overhead to deploying adaptive algorithms -- for example, when the execution of the data collection policies is required to be distributed, or a human in the loop is needed to implement these policies -- producing in advance a set of policies for data collection is paramount. We study the setting where a large dataset of contexts but not rewards is available and may be used by the learner to design an effective data collection strategy. Although when rewards are linear this problem has been well studied, results are still missing for more complex reward models. In this work we propose two experiment planning strategies compatible with function approximation. The first is an eluder planning and sampling procedure that can recover optimality guarantees depending on the eluder dimension of the reward function class. For the second, we show that a 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#32852;&#37030;&#36951;&#24536;&#30340;&#27010;&#24565;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#21644;&#35774;&#35745;&#20934;&#21017;&#65292;&#26088;&#22312;&#20026;&#32852;&#37030;&#23398;&#20064;&#20013;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#21644;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.05146</link><description>&lt;p&gt;
&#32852;&#37030;&#36951;&#24536;&#65306;&#26041;&#27861;&#12289;&#35774;&#35745;&#20934;&#21017;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Federated Unlearning: A Survey on Methods, Design Guidelines, and Evaluation Metrics. (arXiv:2401.05146v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05146
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#32852;&#37030;&#36951;&#24536;&#30340;&#27010;&#24565;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#21644;&#35774;&#35745;&#20934;&#21017;&#65292;&#26088;&#22312;&#20026;&#32852;&#37030;&#23398;&#20064;&#20013;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#21644;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20351;&#24471;&#22810;&#20010;&#21442;&#19982;&#26041;&#33021;&#22815;&#21327;&#21516;&#35757;&#32451;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20445;&#30041;&#25968;&#25454;&#22312;&#26412;&#22320;&#23384;&#20648;&#65292;&#20174;&#32780;&#32500;&#25252;&#20102;&#29992;&#25143;&#21644;&#26426;&#26500;&#30340;&#38544;&#31169;&#12290;&#19982;&#38598;&#20013;&#21270;&#21407;&#22987;&#25968;&#25454;&#19981;&#21516;&#65292;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#20132;&#25442;&#26412;&#22320;&#20248;&#21270;&#30340;&#27169;&#22411;&#21442;&#25968;&#26469;&#36880;&#27493;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#12290;&#23613;&#31649;&#32852;&#37030;&#23398;&#20064;&#26356;&#21152;&#31526;&#21512;&#26032;&#20852;&#35268;&#23450;&#65292;&#22914;&#27431;&#27954;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#65288;GDPR&#65289;&#65292;&#20294;&#22312;&#27492;&#32972;&#26223;&#19979;&#30830;&#20445;&#36951;&#24536;&#26435;&#8212;&#8212;&#20801;&#35768;&#32852;&#37030;&#23398;&#20064;&#21442;&#19982;&#26041;&#20174;&#23398;&#20064;&#30340;&#27169;&#22411;&#20013;&#21024;&#38500;&#20182;&#20204;&#30340;&#25968;&#25454;&#36129;&#29486;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#27492;&#22806;&#65292;&#20154;&#20204;&#35748;&#35782;&#21040;&#24694;&#24847;&#23458;&#25143;&#31471;&#21487;&#33021;&#36890;&#36807;&#26356;&#26032;&#23558;&#21518;&#38376;&#27880;&#20837;&#20840;&#23616;&#27169;&#22411;&#65292;&#20363;&#22914;&#23545;&#29305;&#21046;&#25968;&#25454;&#31034;&#20363;&#36827;&#34892;&#38169;&#35823;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26426;&#21046;&#26469;&#30830;&#20445;&#20010;&#20154;&#26377;&#21487;&#33021;&#22312;&#32858;&#21512;&#21518;&#31227;&#38500;&#20182;&#20204;&#30340;&#25968;&#25454;&#24182;&#28165;&#38500;&#24694;&#24847;&#36129;&#29486;&#65292;&#32780;&#19981;&#25439;&#23475;&#24050;&#33719;&#24471;&#30340;"&#20840;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) enables collaborative training of a Machine Learning (ML) model across multiple parties, facilitating the preservation of users' and institutions' privacy by keeping data stored locally. Instead of centralizing raw data, FL exchanges locally refined model parameters to build a global model incrementally. While FL is more compliant with emerging regulations such as the European General Data Protection Regulation (GDPR), ensuring the right to be forgotten in this context - allowing FL participants to remove their data contributions from the learned model - remains unclear. In addition, it is recognized that malicious clients may inject backdoors into the global model through updates, e.g. to generate mispredictions on specially crafted data examples. Consequently, there is the need for mechanisms that can guarantee individuals the possibility to remove their data and erase malicious contributions even after aggregation, without compromising the already acquired "g
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#30196;&#21574;&#30740;&#31350;&#30340;&#36716;&#21270;&#28508;&#21147;&#65292;&#36825;&#39033;&#24320;&#21019;&#24615;&#30740;&#31350;&#26377;&#26395;&#35299;&#20915;&#30196;&#21574;&#30740;&#31350;&#20174;&#22522;&#30784;&#21457;&#29616;&#21040;&#23454;&#38469;&#24212;&#29992;&#36716;&#21270;&#36895;&#24230;&#36739;&#24930;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.05145</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25512;&#21160;&#36716;&#21270;&#30740;&#31350;&#65306;&#39044;&#27979;&#30196;&#21574;&#30740;&#31350;&#20013;&#30340;&#19987;&#21033;&#21644;&#20020;&#24202;&#35797;&#39564;&#25910;&#24405;
&lt;/p&gt;
&lt;p&gt;
Machine Learning to Promote Translational Research: Predicting Patent and Clinical Trial Inclusion in Dementia Research. (arXiv:2401.05145v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05145
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#30196;&#21574;&#30740;&#31350;&#30340;&#36716;&#21270;&#28508;&#21147;&#65292;&#36825;&#39033;&#24320;&#21019;&#24615;&#30740;&#31350;&#26377;&#26395;&#35299;&#20915;&#30196;&#21574;&#30740;&#31350;&#20174;&#22522;&#30784;&#21457;&#29616;&#21040;&#23454;&#38469;&#24212;&#29992;&#36716;&#21270;&#36895;&#24230;&#36739;&#24930;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35745;&#21040;2040&#24180;&#65292;&#30196;&#21574;&#30151;&#23558;&#24433;&#21709;&#33521;&#22269;160&#19975;&#20154;&#65292;&#24182;&#27599;&#24180;&#32791;&#36153;250&#20159;&#33521;&#38225;&#65292;&#32473;&#31038;&#20250;&#24102;&#26469;&#26085;&#30410;&#22686;&#38271;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#26159;&#19968;&#39033;&#24320;&#21019;&#24615;&#30340;&#23581;&#35797;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#30196;&#21574;&#30740;&#31350;&#30340;&#36716;&#21270;&#28508;&#21147;&#65292;&#24182;&#24076;&#26395;&#35299;&#20915;&#23613;&#31649;&#30196;&#21574;&#30151;&#23545;&#31038;&#20250;&#21644;&#32463;&#27982;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#20294;&#22522;&#30784;&#21457;&#29616;&#36716;&#21270;&#20026;&#23454;&#38469;&#24212;&#29992;&#30340;&#36895;&#24230;&#36739;&#24930;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;Dimensions&#25968;&#25454;&#24211;&#25552;&#21462;&#20102;1990-2023&#24180;&#20043;&#38388;43,091&#31687;&#33521;&#22269;&#30196;&#21574;&#30740;&#31350;&#20986;&#29256;&#29289;&#30340;&#25968;&#25454;&#65292;&#21253;&#25324;&#20803;&#25968;&#25454;&#65288;&#20316;&#32773;&#12289;&#20986;&#29256;&#24180;&#31561;&#65289;&#12289;&#35770;&#25991;&#20013;&#25552;&#21040;&#30340;&#27010;&#24565;&#20197;&#21450;&#35770;&#25991;&#25688;&#35201;&#12290;&#20026;&#20102;&#20026;&#26426;&#22120;&#23398;&#20064;&#20934;&#22791;&#25968;&#25454;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#35832;&#22914;one hot&#32534;&#30721;&#21644;/&#25110;&#35789;&#23884;&#20837;&#31561;&#26041;&#27861;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;CatBoost&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#19968;&#31687;&#35770;&#25991;&#26159;&#21542;&#20250;&#34987;&#24341;&#29992;&#22312;&#26410;&#26469;&#30340;&#19987;&#21033;&#25110;&#20020;&#24202;&#35797;&#39564;&#20013;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#21464;&#20307;&#12290;&#32467;&#21512;&#20803;&#25968;&#25454;&#12289;&#27010;&#24565;&#21644;&#25688;&#35201;&#30340;&#23884;&#20837;&#27169;&#22411;&#20135;&#29983;&#20102;&#26368;&#39640;&#30340;per
&lt;/p&gt;
&lt;p&gt;
Projected to impact 1.6 million people in the UK by 2040 and costing {\pounds}25 billion annually, dementia presents a growing challenge to society. This study, a pioneering effort to predict the translational potential of dementia research using machine learning, hopes to address the slow translation of fundamental discoveries into practical applications despite dementia's significant societal and economic impact. We used the Dimensions database to extract data from 43,091 UK dementia research publications between the years 1990-2023, specifically metadata (authors, publication year etc.), concepts mentioned in the paper, and the paper abstract. To prepare the data for machine learning we applied methods such as one hot encoding and/or word embeddings. We trained a CatBoost Classifier to predict if a publication will be cited in a future patent or clinical trial. We trained several model variations. The model combining metadata, concept, and abstract embeddings yielded the highest per
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#38544;&#31169;&#20445;&#25252;&#30340;&#35270;&#35273;Transformer&#27169;&#22411;&#65292;&#24182;&#36991;&#20813;&#20102;&#20351;&#29992;&#21152;&#23494;&#22270;&#20687;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#35813;&#26041;&#27861;&#22312;CIFAR-10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.05126</link><description>&lt;p&gt;
&#39640;&#25928;&#39046;&#22495;&#36866;&#24212;&#19979;&#30340;&#38544;&#31169;&#20445;&#25252;&#35270;&#35273;Transformer&#30340;&#31934;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Fine-Tuning with Domain Adaptation for Privacy-Preserving Vision Transformer. (arXiv:2401.05126v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#38544;&#31169;&#20445;&#25252;&#30340;&#35270;&#35273;Transformer&#27169;&#22411;&#65292;&#24182;&#36991;&#20813;&#20102;&#20351;&#29992;&#21152;&#23494;&#22270;&#20687;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#35813;&#26041;&#27861;&#22312;CIFAR-10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#29992;&#35270;&#35273;&#20445;&#25252;&#30340;&#22270;&#20687;&#35757;&#32451;&#27169;&#22411;&#24182;&#36827;&#34892;&#27979;&#35797;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#36991;&#20813;&#20351;&#29992;&#21152;&#23494;&#22270;&#20687;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#32780;&#20256;&#32479;&#26041;&#27861;&#19981;&#33021;&#36991;&#20813;&#22270;&#20687;&#21152;&#23494;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23545;&#20351;&#29992;&#21152;&#23494;&#22270;&#20687;&#30340;ViT&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;CIFAR-10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel method for privacy-preserving deep neural networks (DNNs) with the Vision Transformer (ViT). The method allows us not only to train models and test with visually protected images but to also avoid the performance degradation caused from the use of encrypted images, whereas conventional methods cannot avoid the influence of image encryption. A domain adaptation method is used to efficiently fine-tune ViT with encrypted images. In experiments, the method is demonstrated to outperform conventional methods in an image classification task on the CIFAR-10 and ImageNet datasets in terms of classification accuracy.
&lt;/p&gt;</description></item><item><title>&#20809;&#23376;&#38598;&#25104;&#30005;&#36335;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#24471;&#20197;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#26041;&#38754;&#20855;&#26377;&#39640;&#33021;&#37327;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#30828;&#20214;&#21046;&#36896;&#21644;&#22522;&#30784;&#35774;&#26045;&#20250;&#36896;&#25104;&#22823;&#37327;&#30899;&#25490;&#25918;&#65292;&#22240;&#27492;&#38656;&#35201;&#23545;&#20809;&#23376;&#23398;&#30340;&#21046;&#36896;&#21644;&#36816;&#33829;&#30899;&#25104;&#26412;&#36827;&#34892;&#32771;&#34385;&#65292;&#20197;&#30830;&#23450;&#20854;&#22312;&#21487;&#25345;&#32493;&#26410;&#26469;&#20013;&#26159;&#21542;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2401.05121</link><description>&lt;p&gt;
&#20809;&#23376;&#23398;&#29992;&#20110;&#21487;&#25345;&#32493;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Photonics for Sustainable Computing. (arXiv:2401.05121v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05121
&lt;/p&gt;
&lt;p&gt;
&#20809;&#23376;&#38598;&#25104;&#30005;&#36335;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#24471;&#20197;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#26041;&#38754;&#20855;&#26377;&#39640;&#33021;&#37327;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#30828;&#20214;&#21046;&#36896;&#21644;&#22522;&#30784;&#35774;&#26045;&#20250;&#36896;&#25104;&#22823;&#37327;&#30899;&#25490;&#25918;&#65292;&#22240;&#27492;&#38656;&#35201;&#23545;&#20809;&#23376;&#23398;&#30340;&#21046;&#36896;&#21644;&#36816;&#33829;&#30899;&#25104;&#26412;&#36827;&#34892;&#32771;&#34385;&#65292;&#20197;&#30830;&#23450;&#20854;&#22312;&#21487;&#25345;&#32493;&#26410;&#26469;&#20013;&#26159;&#21542;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23376;&#38598;&#25104;&#30005;&#36335;&#22312;&#20809;&#23398;&#25910;&#21457;&#22120;&#12289;&#28608;&#20809;&#38647;&#36798;&#12289;&#29983;&#29289;&#20256;&#24863;&#12289;&#20809;&#23376;&#37327;&#23376;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#31561;&#22810;&#20010;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#29305;&#21035;&#26159;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35268;&#27169;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#22522;&#20110;&#20809;&#23376;&#30340;&#21152;&#36895;&#22120;&#22240;&#33021;&#22815;&#27604;&#22522;&#20110;CMOS&#30340;&#21152;&#36895;&#22120;&#20197;&#22810;&#20010;&#25968;&#37327;&#32423;&#26356;&#39640;&#30340;&#33021;&#37327;&#25928;&#29575;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#32780;&#21463;&#21040;&#29305;&#21035;&#20851;&#27880;&#65292;&#25104;&#20026;&#21487;&#25345;&#32493;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#30828;&#20214;&#21046;&#36896;&#21644;&#22522;&#30784;&#35774;&#26045;&#23545;&#35745;&#31639;&#35774;&#22791;&#30340;&#30899;&#36275;&#36857;&#26377;&#30528;&#26174;&#33879;&#36129;&#29486;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#20351;&#29992;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#25490;&#25918;&#12290;&#20363;&#22914;&#65292;2019&#24180;&#33529;&#26524;&#20844;&#21496;&#30340;&#24635;&#30899;&#25490;&#25918;&#37327;&#20013;&#65292;&#21046;&#36896;&#36807;&#31243;&#21344;&#25454;&#20102;74&#65285;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#20135;&#29983;&#19968;&#20010;&#38382;&#39064; - &#22914;&#26524;&#25105;&#20204;&#32771;&#34385;&#20809;&#23376;&#23398;&#30340;&#21046;&#36896;&#21644;&#36816;&#33829;&#30899;&#25104;&#26412;&#65292;&#23427;&#26159;&#21542;&#30830;&#23454;&#26159;&#21487;&#25345;&#32493;&#26410;&#26469;&#30340;&#36884;&#24452;&#65311;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#30899;&#36275;&#36857;&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
Photonic integrated circuits are finding use in a variety of applications including optical transceivers, LIDAR, bio-sensing, photonic quantum computing, and Machine Learning (ML). In particular, with the exponentially increasing sizes of ML models, photonics-based accelerators are getting special attention as a sustainable solution because they can perform ML inferences with multiple orders of magnitude higher energy efficiency than CMOS-based accelerators. However, recent studies have shown that hardware manufacturing and infrastructure contribute significantly to the carbon footprint of computing devices, even surpassing the emissions generated during their use. For example, the manufacturing process accounts for 74% of the total carbon emissions from Apple in 2019. This prompts us to ask -- if we consider both the embodied (manufacturing) and operational carbon cost of photonics, is it indeed a viable avenue for a sustainable future? So, in this paper, we build a carbon footprint m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25239;&#22122;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20013;&#24341;&#20837;&#36866;&#37197;&#22120;&#65292;&#24182;&#20351;&#29992;&#24102;&#26377;&#22122;&#22768;&#21442;&#32771;&#35821;&#38899;&#23545;TTS&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21450;&#37319;&#29992;&#35821;&#38899;&#22686;&#24378;&#21069;&#31471;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#21512;&#25104;&#65292;&#23545;&#21442;&#32771;&#35821;&#38899;&#20013;&#30340;&#22122;&#22768;&#20855;&#26377;&#39640;&#24230;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05111</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#24449;&#27169;&#22411;&#30340;&#25239;&#22122;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Noise-robust zero-shot text-to-speech synthesis conditioned on self-supervised speech-representation model with adapters. (arXiv:2401.05111v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25239;&#22122;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20013;&#24341;&#20837;&#36866;&#37197;&#22120;&#65292;&#24182;&#20351;&#29992;&#24102;&#26377;&#22122;&#22768;&#21442;&#32771;&#35821;&#38899;&#23545;TTS&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21450;&#37319;&#29992;&#35821;&#38899;&#22686;&#24378;&#21069;&#31471;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#21512;&#25104;&#65292;&#23545;&#21442;&#32771;&#35821;&#38899;&#20013;&#30340;&#22122;&#22768;&#20855;&#26377;&#39640;&#24230;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#35821;&#38899;&#34920;&#24449;&#25552;&#21462;&#30340;&#35828;&#35805;&#20154;&#23884;&#20837;&#21521;&#37327;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#26041;&#27861;&#21487;&#20197;&#38750;&#24120;&#20934;&#30830;&#22320;&#22797;&#21046;&#35828;&#35805;&#20154;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#24403;&#21442;&#32771;&#35821;&#38899;&#20013;&#21547;&#26377;&#22122;&#22768;&#26102;&#65292;&#35813;&#26041;&#27861;&#30340;&#35821;&#38899;&#21512;&#25104;&#36136;&#37327;&#20250;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25239;&#22122;&#38646;&#26679;&#26412;TTS&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36866;&#37197;&#22120;&#24341;&#20837;&#21040;SSL&#27169;&#22411;&#20013;&#65292;&#24182;&#20351;&#29992;&#24102;&#26377;&#22122;&#22768;&#21442;&#32771;&#35821;&#38899;&#30340;TTS&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#35821;&#38899;&#22686;&#24378;&#65288;SE&#65289;&#21069;&#31471;&#12290;&#36890;&#36807;&#36825;&#20123;&#25913;&#36827;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;SSL&#30340;&#38646;&#26679;&#26412;TTS&#22312;&#22122;&#22768;&#21442;&#32771;&#35821;&#38899;&#19979;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#21512;&#25104;&#12290;&#36890;&#36807;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;&#35813;&#26041;&#27861;&#23545;&#21442;&#32771;&#35821;&#38899;&#20013;&#30340;&#22122;&#22768;&#20855;&#26377;&#39640;&#24230;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#19982;SE&#32467;&#21512;&#26377;&#25928;&#22320;&#36816;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
The zero-shot text-to-speech (TTS) method, based on speaker embeddings extracted from reference speech using self-supervised learning (SSL) speech representations, can reproduce speaker characteristics very accurately. However, this approach suffers from degradation in speech synthesis quality when the reference speech contains noise. In this paper, we propose a noise-robust zero-shot TTS method. We incorporated adapters into the SSL model, which we fine-tuned with the TTS model using noisy reference speech. In addition, to further improve performance, we adopted a speech enhancement (SE) front-end. With these improvements, our proposed SSL-based zero-shot TTS achieved high-quality speech synthesis with noisy reference speech. Through the objective and subjective evaluations, we confirmed that the proposed method is highly robust to noise in reference speech, and effectively works in combination with SE.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#8220;&#20219;&#24847;&#26041;&#24335;&#8221;&#23398;&#20064;&#33539;&#24335;&#35299;&#20915;&#20102;&#20803;&#23398;&#20064;&#20013;&#22266;&#23450;&#22522;&#25968;&#30340;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#20174;&#26631;&#31614;&#20998;&#37197;&#20013;&#20986;&#29616;&#30340;&#8220;&#26631;&#31614;&#31561;&#20215;&#24615;&#8221;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#24357;&#34917;&#26631;&#31614;&#31561;&#20215;&#24615;&#24102;&#26469;&#30340;&#35821;&#20041;&#20449;&#24687;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2401.05097</link><description>&lt;p&gt;
&#20219;&#24847;&#26041;&#24335;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Any-Way Meta Learning. (arXiv:2401.05097v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#8220;&#20219;&#24847;&#26041;&#24335;&#8221;&#23398;&#20064;&#33539;&#24335;&#35299;&#20915;&#20102;&#20803;&#23398;&#20064;&#20013;&#22266;&#23450;&#22522;&#25968;&#30340;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#20174;&#26631;&#31614;&#20998;&#37197;&#20013;&#20986;&#29616;&#30340;&#8220;&#26631;&#31614;&#31561;&#20215;&#24615;&#8221;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#24357;&#34917;&#26631;&#31614;&#31561;&#20215;&#24615;&#24102;&#26469;&#30340;&#35821;&#20041;&#20449;&#24687;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20803;&#23398;&#20064;&#22312;&#24555;&#36895;&#36866;&#24212;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#21463;&#21040;&#22266;&#23450;&#22522;&#25968;&#30340;&#38480;&#21046;&#12290;&#24403;&#38754;&#20020;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#22522;&#25968;&#19981;&#21516;&#30340;&#20219;&#21153;&#26102;&#65292;&#27169;&#22411;&#23601;&#26080;&#27861;&#32988;&#20219;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#20174;&#38543;&#26426;&#25968;&#20540;&#26631;&#31614;&#20998;&#37197;&#20013;&#20986;&#29616;&#30340;&#8220;&#26631;&#31614;&#31561;&#20215;&#24615;&#8221;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#36136;&#30097;&#8220;&#30495;&#27491;&#30340;&#8221;&#20803;&#23398;&#20064;&#30340;&#23450;&#20041;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#20219;&#24847;&#26041;&#24335;&#8221;&#23398;&#20064;&#33539;&#24335;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#25670;&#33073;&#20102;&#22266;&#23450;&#22522;&#25968;&#30340;&#38480;&#21046;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20010;&#27169;&#22411;&#19981;&#20165;&#22312;&#24615;&#33021;&#12289;&#25910;&#25947;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#19982;&#20256;&#32479;&#30340;&#22266;&#23450;&#26041;&#24335;&#27169;&#22411;&#30456;&#21305;&#37197;&#65292;&#32780;&#19988;&#36890;&#24120;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;&#36825;&#39072;&#35206;&#20102;&#20851;&#20110;&#39046;&#22495;&#27867;&#21270;&#30340;&#24050;&#26377;&#35266;&#24565;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#20026;&#22266;&#26377;&#30340;&#26631;&#31614;&#31561;&#20215;&#24615;&#33258;&#28982;&#22320;&#32570;&#20047;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#24357;&#34917;&#26631;&#31614;&#31561;&#20215;&#24615;&#24102;&#26469;&#30340;&#36825;&#31181;&#35821;&#20041;&#20449;&#24687;&#24046;&#36317;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although meta-learning seems promising performance in the realm of rapid adaptability, it is constrained by fixed cardinality. When faced with tasks of varying cardinalities that were unseen during training, the model lacks its ability. In this paper, we address and resolve this challenge by harnessing `label equivalence' emerged from stochastic numeric label assignments during episodic task sampling. Questioning what defines ``true" meta-learning, we introduce the ``any-way" learning paradigm, an innovative model training approach that liberates model from fixed cardinality constraints. Surprisingly, this model not only matches but often outperforms traditional fixed-way models in terms of performance, convergence speed, and stability. This disrupts established notions about domain generalization. Furthermore, we argue that the inherent label equivalence naturally lacks semantic information. To bridge this semantic information gap arising from label equivalence, we further propose a m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20998;&#31867;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#32844;&#20301;&#24191;&#21578;&#35201;&#27714;&#21644;&#27178;&#21521;&#25216;&#33021;&#38598;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#39044;&#27979;&#20010;&#21035;&#24037;&#20316;&#25551;&#36848;&#25152;&#38656;&#30340;&#25216;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#23618;&#27425;&#20998;&#31867;&#21644;&#22810;&#26631;&#31614;&#31574;&#30053;&#65292;&#24182;&#37319;&#29992;&#22686;&#24378;&#25216;&#26415;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#27431;&#27954;&#23601;&#19994;&#24066;&#22330;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.05073</link><description>&lt;p&gt;
&#22522;&#20110;&#21477;&#23376;&#23884;&#20837;&#30340;&#32844;&#20301;&#24191;&#21578;&#20013;&#27178;&#21521;&#25216;&#33021;&#30340;&#23618;&#27425;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Classification of Transversal Skills in Job Ads Based on Sentence Embeddings. (arXiv:2401.05073v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20998;&#31867;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#32844;&#20301;&#24191;&#21578;&#35201;&#27714;&#21644;&#27178;&#21521;&#25216;&#33021;&#38598;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#39044;&#27979;&#20010;&#21035;&#24037;&#20316;&#25551;&#36848;&#25152;&#38656;&#30340;&#25216;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#23618;&#27425;&#20998;&#31867;&#21644;&#22810;&#26631;&#31614;&#31574;&#30053;&#65292;&#24182;&#37319;&#29992;&#22686;&#24378;&#25216;&#26415;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#27431;&#27954;&#23601;&#19994;&#24066;&#22330;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#32844;&#20301;&#24191;&#21578;&#35201;&#27714;&#21644;&#27178;&#21521;&#25216;&#33021;&#38598;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#37325;&#28857;&#26159;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20010;&#21035;&#24037;&#20316;&#25551;&#36848;&#25152;&#38656;&#30340;&#25216;&#33021;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#25968;&#25454;&#25910;&#38598;&#12289;&#39044;&#22788;&#29702;&#21644;&#20351;&#29992;ESCO&#65288;&#27431;&#27954;&#25216;&#33021;&#12289;&#33021;&#21147;&#21644;&#32844;&#19994;&#65289;&#20998;&#31867;&#31995;&#32479;&#36827;&#34892;&#26631;&#27880;&#12290;&#22312;&#25216;&#33021;&#35782;&#21035;&#26041;&#38754;&#65292;&#37319;&#29992;&#20102;&#23618;&#27425;&#20998;&#31867;&#21644;&#22810;&#26631;&#31614;&#31574;&#30053;&#65292;&#32780;&#22686;&#24378;&#25216;&#26415;&#21017;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#36890;&#36807;&#27604;&#36739;&#20351;&#29992;&#33521;&#35821;&#29305;&#23450;&#21644;&#22810;&#35821;&#35328;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#24471;&#21040;&#30340;&#32467;&#26524;&#65292;&#21457;&#29616;&#20854;&#20934;&#30830;&#24230;&#30456;&#36817;&#12290;&#23454;&#39564;&#26696;&#20363;&#30740;&#31350;&#35814;&#32454;&#35828;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#37197;&#32622;&#12289;&#36229;&#21442;&#25968;&#21644;&#20132;&#21449;&#39564;&#35777;&#32467;&#26524;&#65292;&#31361;&#26174;&#20102;&#23618;&#27425;&#21270;&#26041;&#27861;&#30340;&#21151;&#25928;&#20197;&#21450;&#22810;&#35821;&#35328;&#27169;&#22411;&#36866;&#29992;&#20110;&#22810;&#26679;&#21270;&#30340;&#27431;&#27954;&#23601;&#19994;&#24066;&#22330;&#12290;&#22240;&#27492;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#23618;&#27425;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a classification framework aimed at identifying correlations between job ad requirements and transversal skill sets, with a focus on predicting the necessary skills for individual job descriptions using a deep learning model. The approach involves data collection, preprocessing, and labeling using ESCO (European Skills, Competences, and Occupations) taxonomy. Hierarchical classification and multi-label strategies are used for skill identification, while augmentation techniques address data imbalance, enhancing model robustness. A comparison between results obtained with English-specific and multi-language sentence embedding models reveals close accuracy. The experimental case studies detail neural network configurations, hyperparameters, and cross-validation results, highlighting the efficacy of the hierarchical approach and the suitability of the multi-language model for the diverse European job market. Thus, a new approach is proposed for the hierarchical classifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;&#22810;&#31867;&#21487;&#35299;&#37322;&#35780;&#20998;&#31995;&#32479;&#65288;MISS&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21487;&#23558;&#35780;&#20998;&#31995;&#32479;&#36716;&#25442;&#20026;&#31867;&#21035;&#27010;&#29575;&#65292;&#20855;&#26377;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#20248;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05069</link><description>&lt;p&gt;
MISS: &#22810;&#31867;&#21487;&#35299;&#37322;&#35780;&#20998;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MISS: Multiclass Interpretable Scoring Systems. (arXiv:2401.05069v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;&#22810;&#31867;&#21487;&#35299;&#37322;&#35780;&#20998;&#31995;&#32479;&#65288;MISS&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21487;&#23558;&#35780;&#20998;&#31995;&#32479;&#36716;&#25442;&#20026;&#31867;&#21035;&#27010;&#29575;&#65292;&#20855;&#26377;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#20248;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#22810;&#31867;&#21487;&#35299;&#37322;&#35780;&#20998;&#31995;&#32479;&#65288;MISS&#65289;- &#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21333;&#19968;&#12289;&#31232;&#30095;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#30340;&#35780;&#20998;&#31995;&#32479;&#12290;&#35780;&#20998;&#31995;&#32479;&#36890;&#24120;&#29992;&#20316;&#21307;&#30103;&#20445;&#20581;&#12289;&#21009;&#20107;&#21496;&#27861;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#20915;&#31574;&#25903;&#25345;&#27169;&#22411;&#65292;&#20854;&#20013;&#39044;&#27979;&#30340;&#35299;&#37322;&#24615;&#21644;&#26131;&#29992;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#20197;&#21069;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#22914;SLIM&#65288;&#36229;&#31232;&#30095;&#32447;&#24615;&#25972;&#25968;&#27169;&#22411;&#65289;&#65292;&#20165;&#38480;&#20110;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#65292;&#32780;&#23545;&#20110;&#22810;&#31867;&#39046;&#22495;&#30340;&#25193;&#23637;&#20027;&#35201;&#36890;&#36807;&#19968;&#23545;&#22810;&#25216;&#26415;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#35780;&#20998;&#21487;&#20197;&#36890;&#36807;softmax&#20989;&#25968;&#36731;&#26494;&#36716;&#25442;&#20026;&#31867;&#21035;&#27010;&#29575;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#38477;&#32500;&#21644;&#21551;&#21457;&#24335;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#24182;&#20943;&#23569;&#20102;&#26368;&#20339;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#35813;&#24046;&#36317;&#21487;&#20197;&#35777;&#26126;&#27169;&#22411;&#30340;&#26368;&#20339;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a novel, machine-learning approach for constructing Multiclass Interpretable Scoring Systems (MISS) - a fully data-driven methodology for generating single, sparse, and user-friendly scoring systems for multiclass classification problems. Scoring systems are commonly utilized as decision support models in healthcare, criminal justice, and other domains where interpretability of predictions and ease of use are crucial. Prior methods for data-driven scoring, such as SLIM (Supersparse Linear Integer Model), were limited to binary classification tasks and extensions to multiclass domains were primarily accomplished via one-versus-all-type techniques. The scores produced by our method can be easily transformed into class probabilities via the softmax function. We demonstrate techniques for dimensionality reduction and heuristics that enhance the training efficiency and decrease the optimality gap, a measure that can certify the optimality of the model. Our approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#25216;&#26415;&#22312;&#22823;&#37327;&#30340;&#29420;&#31435;&#38899;&#36712;&#19978;&#35757;&#32451;&#65292;&#20197;&#25552;&#21462;&#36866;&#29992;&#20110;&#21809;&#27468;&#30456;&#20851;&#20219;&#21153;&#30340;&#39640;&#36136;&#37327;&#27468;&#25163;&#36523;&#20221;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#34920;&#31034;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#20855;&#22791;&#39046;&#22495;&#22806;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05064</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#30417;&#30563;&#25216;&#26415;&#36827;&#34892;&#27468;&#25163;&#36523;&#20221;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Singer Identity Representation Learning using Self-Supervised Techniques. (arXiv:2401.05064v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#25216;&#26415;&#22312;&#22823;&#37327;&#30340;&#29420;&#31435;&#38899;&#36712;&#19978;&#35757;&#32451;&#65292;&#20197;&#25552;&#21462;&#36866;&#29992;&#20110;&#21809;&#27468;&#30456;&#20851;&#20219;&#21153;&#30340;&#39640;&#36136;&#37327;&#27468;&#25163;&#36523;&#20221;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#34920;&#31034;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#20855;&#22791;&#39046;&#22495;&#22806;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#35821;&#38899;&#25968;&#25454;&#21019;&#24314;&#22768;&#38899;&#36523;&#20221;&#34920;&#31034;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21809;&#27468;&#22768;&#38899;&#65292;&#36824;&#27809;&#26377;&#21462;&#24471;&#21516;&#26679;&#30340;&#36827;&#23637;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#27468;&#25163;&#36523;&#20221;&#32534;&#30721;&#22120;&#65292;&#20197;&#25552;&#21462;&#36866;&#29992;&#20110;&#21508;&#31181;&#21809;&#27468;&#30456;&#20851;&#20219;&#21153;&#65288;&#22914;&#21809;&#27468;&#22768;&#38899;&#30456;&#20284;&#24615;&#21644;&#21512;&#25104;&#65289;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#30340;&#29420;&#31435;&#38899;&#36712;&#19978;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#22312;&#35757;&#32451;&#26399;&#38388;&#24212;&#29992;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#30830;&#20445;&#34920;&#31034;&#23545;&#38899;&#39640;&#21644;&#20869;&#23481;&#21464;&#21270;&#19981;&#21464;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#24471;&#21040;&#34920;&#31034;&#30340;&#36136;&#37327;&#65292;&#29305;&#21035;&#27880;&#37325;&#39046;&#22495;&#22806;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;44.1 kHz&#26465;&#20214;&#19979;&#20135;&#29983;&#20102;&#39640;&#36136;&#37327;&#30340;&#23884;&#20837;&#65292;&#20248;&#20110;&#35828;&#35805;&#20154;&#39564;&#35777;&#21644;wav2vec 2.0&#39044;&#35757;&#32451;&#22522;&#32447;&#65292;&#24182;&#22312;&#21809;&#27468;&#22768;&#38899;&#19978;&#25805;&#20316;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Significant strides have been made in creating voice identity representations using speech data. However, the same level of progress has not been achieved for singing voices. To bridge this gap, we suggest a framework for training singer identity encoders to extract representations suitable for various singing-related tasks, such as singing voice similarity and synthesis. We explore different self-supervised learning techniques on a large collection of isolated vocal tracks and apply data augmentations during training to ensure that the representations are invariant to pitch and content variations. We evaluate the quality of the resulting representations on singer similarity and identification tasks across multiple datasets, with a particular emphasis on out-of-domain generalization. Our proposed framework produces high-quality embeddings that outperform both speaker verification and wav2vec 2.0 pre-trained baselines on singing voice while operating at 44.1 kHz. We release our code and
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#22270;&#20687;&#20462;&#22797;&#27969;&#31243;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#27169;&#22411;&#12289;&#23545;&#35937;&#32423;&#21035;&#30340;&#20462;&#22797;&#21644;&#29992;&#25143;&#33258;&#23450;&#20041;&#30340;&#27493;&#39588;&#39034;&#24207;&#65292;&#20197;&#21450;&#28145;&#24230;&#24863;&#30693;&#20248;&#21270;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#25972;&#20010;&#20462;&#22797;&#36807;&#31243;&#30340;&#23436;&#20840;&#29992;&#25143;&#25511;&#21046;&#12290;&#36825;&#20010;&#31995;&#32479;&#20855;&#26377;&#24456;&#24378;&#30340;&#36866;&#24212;&#24615;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#30340;&#23545;&#35937;&#31867;&#21035;&#23454;&#29616;&#22270;&#20687;&#20462;&#22797;&#12290;</title><link>http://arxiv.org/abs/2401.05049</link><description>&lt;p&gt;
&#20869;&#23481;&#24863;&#30693;&#30340;&#28145;&#24230;&#33258;&#36866;&#24212;&#22270;&#20687;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Content-Aware Depth-Adaptive Image Restoration. (arXiv:2401.05049v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05049
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#22270;&#20687;&#20462;&#22797;&#27969;&#31243;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#27169;&#22411;&#12289;&#23545;&#35937;&#32423;&#21035;&#30340;&#20462;&#22797;&#21644;&#29992;&#25143;&#33258;&#23450;&#20041;&#30340;&#27493;&#39588;&#39034;&#24207;&#65292;&#20197;&#21450;&#28145;&#24230;&#24863;&#30693;&#20248;&#21270;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#25972;&#20010;&#20462;&#22797;&#36807;&#31243;&#30340;&#23436;&#20840;&#29992;&#25143;&#25511;&#21046;&#12290;&#36825;&#20010;&#31995;&#32479;&#20855;&#26377;&#24456;&#24378;&#30340;&#36866;&#24212;&#24615;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#30340;&#23545;&#35937;&#31867;&#21035;&#23454;&#29616;&#22270;&#20687;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30340;&#37325;&#28857;&#26159;&#24314;&#31435;&#19968;&#20010;&#27169;&#22359;&#21270;&#30340;&#27969;&#31243;&#65292;&#21033;&#29992;&#29616;&#26377;&#27169;&#22411;&#26469;&#31995;&#32479;&#22320;&#20462;&#22797;&#22270;&#20687;&#65292;&#32780;&#19981;&#26159;&#20174;&#22836;&#24320;&#22987;&#21019;&#24314;&#26032;&#30340;&#20462;&#22797;&#27169;&#22411;&#12290;&#20462;&#22797;&#26159;&#22312;&#23545;&#35937;&#32423;&#21035;&#36827;&#34892;&#30340;&#65292;&#27599;&#20010;&#23545;&#35937;&#20351;&#29992;&#20854;&#23545;&#24212;&#30340;&#31867;&#21035;&#26631;&#31614;&#20449;&#24687;&#36827;&#34892;&#37325;&#26032;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#23436;&#20840;&#29992;&#25143;&#25511;&#21046;&#25972;&#20010;&#20462;&#22797;&#36807;&#31243;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#19987;&#38376;&#30340;&#20462;&#22797;&#27493;&#39588;&#27169;&#22411;&#65292;&#33258;&#23450;&#20041;&#27493;&#39588;&#30340;&#39034;&#24207;&#20197;&#28385;&#36275;&#20182;&#20204;&#30340;&#38656;&#27714;&#65292;&#24182;&#21033;&#29992;&#28145;&#24230;&#24863;&#30693;&#23545;&#29983;&#25104;&#30340;&#22270;&#20687;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#20248;&#21270;&#12290;&#30740;&#31350;&#25552;&#20379;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#22270;&#20687;&#20462;&#22797;&#23454;&#26045;&#36335;&#24452;&#65292;&#20197;&#27604;&#36739;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#21183;&#21644;&#38480;&#21046;&#12290;&#36825;&#20010;&#22810;&#21151;&#33021;&#31995;&#32479;&#26368;&#21560;&#24341;&#20154;&#30340;&#26041;&#38754;&#26159;&#23427;&#30340;&#36866;&#24212;&#24615;&#12290;&#36825;&#31181;&#36866;&#24212;&#24615;&#20351;&#29992;&#25143;&#33021;&#22815;&#38024;&#23545;&#29305;&#23450;&#30340;&#23545;&#35937;&#31867;&#21035;&#65288;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#65289;&#25552;&#20379;&#22312;&#36825;&#20123;&#23545;&#35937;&#31867;&#21035;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work prioritizes building a modular pipeline that utilizes existing models to systematically restore images, rather than creating new restoration models from scratch. Restoration is carried out at an object-specific level, with each object regenerated using its corresponding class label information. The approach stands out by providing complete user control over the entire restoration process. Users can select models for specialized restoration steps, customize the sequence of steps to meet their needs, and refine the resulting regenerated image with depth awareness. The research provides two distinct pathways for implementing image regeneration, allowing for a comparison of their respective strengths and limitations. The most compelling aspect of this versatile system is its adaptability. This adaptability enables users to target particular object categories, including medical images, by providing models that are trained on those object classes.
&lt;/p&gt;</description></item><item><title>CreINNs&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;Credal-Set Interval Neural Networks&#65292;&#36890;&#36807;&#20445;&#30041;&#20256;&#32479;&#30340;&#21306;&#38388;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#25429;&#25417;&#26435;&#37325;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#21306;&#38388;&#30340;&#25968;&#23398;&#26694;&#26550;&#39044;&#27979;&#21487;&#20449;&#21306;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CreINNs&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#38598;&#25104;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2401.05043</link><description>&lt;p&gt;
CreINNs: Credal-Set Interval Neural Networks&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
CreINNs: Credal-Set Interval Neural Networks for Uncertainty Estimation in Classification Tasks. (arXiv:2401.05043v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05043
&lt;/p&gt;
&lt;p&gt;
CreINNs&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;Credal-Set Interval Neural Networks&#65292;&#36890;&#36807;&#20445;&#30041;&#20256;&#32479;&#30340;&#21306;&#38388;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#25429;&#25417;&#26435;&#37325;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#21306;&#38388;&#30340;&#25968;&#23398;&#26694;&#26550;&#39044;&#27979;&#21487;&#20449;&#21306;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CreINNs&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#38598;&#25104;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#20110;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#36234;&#26469;&#36234;&#26377;&#21560;&#24341;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;Credal-Set Interval Neural Networks&#65288;CreINNs&#65289;&#65292;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#12290;CreINNs&#20445;&#30041;&#20102;&#20256;&#32479;&#30340;&#21306;&#38388;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#36890;&#36807;&#30830;&#23450;&#24615;&#21306;&#38388;&#25429;&#25417;&#26435;&#37325;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#27010;&#29575;&#21306;&#38388;&#30340;&#25968;&#23398;&#26694;&#26550;&#39044;&#27979;&#21487;&#20449;&#21306;&#38388;&#12290;&#22312;&#19968;&#20010;&#36229;&#20986;&#20998;&#21457;&#26816;&#27979;&#22522;&#20934;&#65288;CIFAR10 vs SVHN&#65289;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20013;&#65292;CreINNs&#30456;&#27604;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#21644;&#28145;&#24230;&#38598;&#25104;&#65288;DEs&#65289;&#65292;&#22312;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#19982;&#21464;&#20998;BNNs&#30456;&#27604;&#65292;CreINNs&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#26174;&#33879;&#38477;&#20302;&#65292;&#24182;&#19988;&#27604;DEs&#20855;&#26377;&#36739;&#23567;&#30340;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation is increasingly attractive for improving the reliability of neural networks. In this work, we present novel credal-set interval neural networks (CreINNs) designed for classification tasks. CreINNs preserve the traditional interval neural network structure, capturing weight uncertainty through deterministic intervals, while forecasting credal sets using the mathematical framework of probability intervals. Experimental validations on an out-of-distribution detection benchmark (CIFAR10 vs SVHN) showcase that CreINNs outperform epistemic uncertainty estimation when compared to variational Bayesian neural networks (BNNs) and deep ensembles (DEs). Furthermore, CreINNs exhibit a notable reduction in computational complexity compared to variational BNNs and demonstrate smaller model sizes than DEs.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#23398;&#35268;&#21010;&#23398;&#20064;&#26469;&#37197;&#32622;&#25968;&#23398;&#35268;&#21010;&#27714;&#35299;&#22120;&#65292;&#35299;&#20915;&#20102;&#21442;&#25968;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.05041</link><description>&lt;p&gt;
&#23398;&#20064;&#36890;&#36807;&#25968;&#23398;&#35268;&#21010;&#26469;&#37197;&#32622;&#25968;&#23398;&#35268;&#21010;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Learning to Configure Mathematical Programming Solvers by Mathematical Programming. (arXiv:2401.05041v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05041
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#23398;&#35268;&#21010;&#23398;&#20064;&#26469;&#37197;&#32622;&#25968;&#23398;&#35268;&#21010;&#27714;&#35299;&#22120;&#65292;&#35299;&#20915;&#20102;&#21442;&#25968;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35752;&#35770;&#20102;&#20026;&#29305;&#23450;&#38382;&#39064;&#23454;&#20363;&#25214;&#21040;&#19968;&#20010;&#22909;&#30340;&#25968;&#23398;&#35268;&#21010;&#27714;&#35299;&#22120;&#37197;&#32622;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#38382;&#39064;&#23454;&#20363;&#12289;&#37197;&#32622;&#21644;&#37197;&#32622;&#27714;&#35299;&#22120;&#22312;&#38382;&#39064;&#23454;&#20363;&#19978;&#30340;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23398;&#20064;&#19968;&#20010;&#22909;&#30340;&#27714;&#35299;&#22120;&#37197;&#32622;&#30340;&#19968;&#20010;&#29305;&#27530;&#22256;&#38590;&#26159;&#21442;&#25968;&#35774;&#32622;&#21487;&#33021;&#19981;&#26159;&#20840;&#37096;&#29420;&#31435;&#30340;&#65307;&#36825;&#38656;&#35201;&#24378;&#21046;&#65288;&#30828;&#65289;&#32422;&#26463;&#26465;&#20214;&#65292;&#32780;&#35768;&#22810;&#24191;&#27867;&#20351;&#29992;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#19981;&#33021;&#21407;&#29983;&#22320;&#23454;&#29616;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#29992;&#23398;&#21040;&#30340;&#20449;&#24687;&#26500;&#24314;&#21644;&#35299;&#20915;&#20102;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#26174;&#24335;&#22320;&#34920;&#31034;&#20102;&#37197;&#32622;&#21442;&#25968;&#35774;&#32622;&#30340;&#20381;&#36182;/&#19968;&#33268;&#24615;&#32422;&#26463;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#20004;&#20010;&#19981;&#21516;&#23454;&#20363;&#19978;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#30340;&#35745;&#31639;&#32467;&#26524;&#65292;&#36825;&#20123;&#23454;&#20363;&#26159;&#22312;&#27700;&#24211;&#35895;&#22320;&#30701;&#26399;&#35268;&#21010;&#20013;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We discuss the issue of finding a good mathematical programming solver configuration for a particular instance of a given problem, and we propose a two-phase approach to solve it. In the first phase we learn the relationships between the instance, the configuration and the performance of the configured solver on the given instance. A specific difficulty of learning a good solver configuration is that parameter settings may not all be independent; this requires enforcing (hard) constraints, something that many widely used supervised learning methods cannot natively achieve. We tackle this issue in the second phase of our approach, where we use the learnt information to construct and solve an optimization problem having an explicit representation of the dependency/consistency constraints on the configuration parameter settings. We discuss computational results for two different instantiations of this approach on a unit commitment problem arising in the short-term planning of hydro valley
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21464;&#20998;&#20449;&#24687;&#30340;&#20114;&#21160;&#22522;&#30784;&#23398;&#20064;&#65288;VI-IGL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#24378;&#21046;&#25191;&#34892;&#26465;&#20214;&#29420;&#31435;&#24615;&#20551;&#35774;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#22870;&#21169;&#35299;&#30721;&#22120;&#26469;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;-&#21160;&#20316;&#65288;X&#65292;A&#65289;&#21644;&#21453;&#39304;&#21464;&#37327;Y&#20043;&#38388;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.05015</link><description>&lt;p&gt;
&#20449;&#24687;&#35770;&#26041;&#27861;&#22312;&#22522;&#20110;&#20114;&#21160;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
An Information Theoretic Approach to Interaction-Grounded Learning. (arXiv:2401.05015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21464;&#20998;&#20449;&#24687;&#30340;&#20114;&#21160;&#22522;&#30784;&#23398;&#20064;&#65288;VI-IGL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#24378;&#21046;&#25191;&#34892;&#26465;&#20214;&#29420;&#31435;&#24615;&#20551;&#35774;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#22870;&#21169;&#35299;&#30721;&#22120;&#26469;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;-&#21160;&#20316;&#65288;X&#65292;A&#65289;&#21644;&#21453;&#39304;&#21464;&#37327;Y&#20043;&#38388;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20960;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#23398;&#20064;&#32773;&#35797;&#22270;&#20174;&#19968;&#20123;&#21453;&#39304;&#21464;&#37327;&#20013;&#25512;&#26029;&#20986;&#26410;&#35266;&#27979;&#21040;&#30340;&#22870;&#21169;&#30340;&#38382;&#39064;&#12290;&#20114;&#21160;&#22522;&#30784;&#23398;&#20064;&#65288;IGL&#65289;&#26159;&#36825;&#31181;&#22522;&#20110;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#19968;&#20010;&#20363;&#23376;&#65292;&#23398;&#20064;&#32773;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20114;&#21160;&#25512;&#26029;&#20986;&#28508;&#22312;&#30340;&#20108;&#20540;&#22870;&#21169;&#26469;&#20248;&#21270;&#36820;&#22238;&#12290;&#22312;IGL&#35774;&#32622;&#20013;&#65292;RL&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#19968;&#20010;&#30456;&#20851;&#20551;&#35774;&#26159;&#65292;&#22312;&#32473;&#23450;&#28508;&#22312;&#22870;&#21169;R&#30340;&#24773;&#20917;&#19979;&#65292;&#21453;&#39304;&#21464;&#37327;Y&#22312;&#19978;&#19979;&#25991;-&#21160;&#20316;&#65288;X&#65292;A&#65289;&#19978;&#26159;&#26465;&#20214;&#29420;&#31435;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#20449;&#24687;&#30340;IGL&#65288;VI-IGL&#65289;&#26041;&#27861;&#65292;&#20197;&#24212;&#29992;&#20110;IGL&#30340;RL&#38382;&#39064;&#20013;&#65292;&#20197;&#24378;&#21046;&#25191;&#34892;&#26465;&#20214;&#29420;&#31435;&#24615;&#20551;&#35774;&#12290;VI-IGL&#26694;&#26550;&#20351;&#29992;&#22522;&#20110;&#26465;&#20214;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#30340;&#20449;&#24687;&#30446;&#26631;&#26469;&#23398;&#20064;&#22870;&#21169;&#35299;&#30721;&#22120;&#65292;&#35813;&#20449;&#24687;&#30446;&#26631;&#34913;&#37327;&#20102;&#20174;&#29615;&#22659;&#20013;&#35266;&#23519;&#21040;&#30340;&#19978;&#19979;&#25991;-&#21160;&#20316;&#65288;X&#65292;A&#65289;&#21644;&#21453;&#39304;&#21464;&#37327;Y&#20043;&#38388;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) problems where the learner attempts to infer an unobserved reward from some feedback variables have been studied in several recent papers. The setting of Interaction-Grounded Learning (IGL) is an example of such feedback-based reinforcement learning tasks where the learner optimizes the return by inferring latent binary rewards from the interaction with the environment. In the IGL setting, a relevant assumption used in the RL literature is that the feedback variable $Y$ is conditionally independent of the context-action $(X,A)$ given the latent reward $R$. In this work, we propose Variational Information-based IGL (VI-IGL) as an information-theoretic method to enforce the conditional independence assumption in the IGL-based RL problem. The VI-IGL framework learns a reward decoder using an information-based objective based on the conditional mutual information (MI) between the context-action $(X,A)$ and the feedback variable $Y$ observed from the environment.
&lt;/p&gt;</description></item><item><title>HiMTM&#26159;&#19968;&#31181;&#38754;&#21521;&#38271;&#26399;&#39044;&#27979;&#30340;&#20998;&#23618;&#22810;&#23610;&#24230;&#23631;&#34109;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#23618;&#22810;&#23610;&#24230;&#21464;&#21387;&#22120;&#65292;&#35299;&#32806;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65292;&#22810;&#23610;&#24230;&#23631;&#34109;&#37325;&#26500;&#21644;&#36328;&#23610;&#24230;&#27880;&#24847;&#24494;&#35843;&#31561;&#32452;&#25104;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2401.05012</link><description>&lt;p&gt;
HiMTM: &#38754;&#21521;&#38271;&#26399;&#39044;&#27979;&#30340;&#20998;&#23618;&#22810;&#23610;&#24230;&#23631;&#34109;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
HiMTM: Hierarchical Multi-Scale Masked Time Series Modeling for Long-Term Forecasting. (arXiv:2401.05012v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05012
&lt;/p&gt;
&lt;p&gt;
HiMTM&#26159;&#19968;&#31181;&#38754;&#21521;&#38271;&#26399;&#39044;&#27979;&#30340;&#20998;&#23618;&#22810;&#23610;&#24230;&#23631;&#34109;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#21253;&#25324;&#20998;&#23618;&#22810;&#23610;&#24230;&#21464;&#21387;&#22120;&#65292;&#35299;&#32806;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65292;&#22810;&#23610;&#24230;&#23631;&#34109;&#37325;&#26500;&#21644;&#36328;&#23610;&#24230;&#27880;&#24847;&#24494;&#35843;&#31561;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#33267;&#20851;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36817;&#26399;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36259;&#28608;&#22686;&#65292;&#36825;&#20123;&#27169;&#22411;&#36866;&#24212;&#21508;&#31181;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#36825;&#26159;&#20540;&#24471;&#27880;&#24847;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#23610;&#24230;&#24615;&#36136;&#65292;&#36825;&#26159;&#36827;&#34892;&#31934;&#30830;&#39044;&#27979;&#25152;&#24517;&#38656;&#30340;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HiMTM&#65292;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#38271;&#26399;&#39044;&#27979;&#35774;&#35745;&#30340;&#20998;&#23618;&#22810;&#23610;&#24230;&#23631;&#34109;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#12290;&#20855;&#20307;&#21253;&#25324;&#22235;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#20998;&#23618;&#22810;&#23610;&#24230;&#21464;&#21387;&#22120;&#65288;HMT&#65289;&#25429;&#25417;&#19981;&#21516;&#23610;&#24230;&#30340;&#26102;&#38388;&#20449;&#24687;&#65307;&#65288;2&#65289;&#35299;&#32806;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;DED&#65289;&#35753;&#32534;&#30721;&#22120;&#19987;&#27880;&#20110;&#29305;&#24449;&#25552;&#21462;&#65292;&#32780;&#35299;&#30721;&#22120;&#19987;&#27880;&#20110;&#20551;&#35774;&#20219;&#21153;&#65307;&#65288;3&#65289;&#22810;&#23610;&#24230;&#23631;&#34109;&#37325;&#26500;&#65288;MMR&#65289;&#20026;&#39044;&#35757;&#32451;&#25552;&#20379;&#22810;&#38454;&#27573;&#30340;&#30417;&#30563;&#20449;&#21495;&#65307;&#65288;4&#65289;&#36328;&#23610;&#24230;&#27880;&#24847;&#24494;&#35843;&#65288;CSA-FT&#65289;&#20197;&#25429;&#25417;&#19981;&#21516;&#23610;&#24230;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#36827;&#34892;&#39044;&#27979;&#12290;&#32508;&#19978;&#25152;&#36848;&#65292;&#36825;&#20123;&#37096;&#20214;&#20849;&#21516;&#26500;&#25104;&#20102;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting is crucial and challenging in the real world. The recent surge in interest regarding time series foundation models, which cater to a diverse array of downstream tasks, is noteworthy. However, existing methods often overlook the multi-scale nature of time series, an aspect crucial for precise forecasting. To bridge this gap, we propose HiMTM, a hierarchical multi-scale masked time series modeling method designed for long-term forecasting. Specifically, it comprises four integral components: (1) hierarchical multi-scale transformer (HMT) to capture temporal information at different scales; (2) decoupled encoder-decoder (DED) forces the encoder to focus on feature extraction, while the decoder to focus on pretext tasks; (3) multi-scale masked reconstruction (MMR) provides multi-stage supervision signals for pre-training; (4) cross-scale attention fine-tuning (CSA-FT) to capture dependencies between different scales for forecasting. Collectively, these components en
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;&#20840;&#29699;&#28798;&#23475;&#39118;&#38505;&#36827;&#34892;&#20102;&#26102;&#38388;&#20998;&#26512;&#65292;&#21457;&#29616;&#23613;&#31649;&#26377;&#25345;&#32493;&#30340;&#21162;&#21147;&#65292;&#20840;&#29699;&#20173;&#28982;&#20998;&#20026;&#39640;&#26131;&#24863;&#24615;&#21644;&#20013;&#31561;&#26131;&#24863;&#24615;&#20004;&#20010;&#20027;&#35201;&#38598;&#32676;&#12290;</title><link>http://arxiv.org/abs/2401.05007</link><description>&lt;p&gt;
&#19990;&#30028;&#28798;&#23475;&#39118;&#38505;&#30340;&#26102;&#38388;&#20998;&#26512;&#65306;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38598;&#32676;&#21160;&#24577;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Temporal Analysis of World Disaster Risk:A Machine Learning Approach to Cluster Dynamics. (arXiv:2401.05007v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;&#20840;&#29699;&#28798;&#23475;&#39118;&#38505;&#36827;&#34892;&#20102;&#26102;&#38388;&#20998;&#26512;&#65292;&#21457;&#29616;&#23613;&#31649;&#26377;&#25345;&#32493;&#30340;&#21162;&#21147;&#65292;&#20840;&#29699;&#20173;&#28982;&#20998;&#20026;&#39640;&#26131;&#24863;&#24615;&#21644;&#20013;&#31561;&#26131;&#24863;&#24615;&#20004;&#20010;&#20027;&#35201;&#38598;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#34892;&#21160;&#30340;&#24433;&#21709;&#23545;&#20110;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#20840;&#29699;&#33539;&#22260;&#20869;&#20943;&#36731;&#39118;&#38505;&#21644;&#21019;&#24314;&#23433;&#20840;&#29615;&#22659;&#25152;&#32771;&#34385;&#30340;&#21162;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#35266;&#23519;&#22312;&#29305;&#23450;&#30701;&#26102;&#38388;&#20869;&#25913;&#21892;&#30340;&#27010;&#29575;&#26469;&#34913;&#37327;&#36825;&#31181;&#24433;&#21709;&#12290;&#21033;&#29992;&#19990;&#30028;&#39118;&#38505;&#25351;&#25968;&#65292;&#25105;&#20204;&#23545;2011&#24180;&#33267;2021&#24180;&#20840;&#29699;&#28798;&#23475;&#39118;&#38505;&#21160;&#24577;&#36827;&#34892;&#20102;&#26102;&#38388;&#20998;&#26512;&#12290;&#36890;&#36807;&#19990;&#30028;&#39118;&#38505;&#25351;&#25968;&#30340;&#35270;&#35282;&#36827;&#34892;&#30340;&#36825;&#31181;&#26102;&#38388;&#25506;&#32034;&#25581;&#31034;&#20102;&#28798;&#23475;&#39118;&#38505;&#30340;&#22797;&#26434;&#21160;&#24577;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#36827;&#34892;&#20102;&#25345;&#32493;&#21162;&#21147;&#65292;&#20840;&#29699;&#26684;&#23616;&#20173;&#28982;&#20998;&#20026;&#20004;&#20010;&#20027;&#35201;&#38598;&#32676;&#65306;&#39640;&#26131;&#24863;&#24615;&#21644;&#20013;&#31561;&#26131;&#24863;&#24615;&#65292;&#19982;&#22320;&#29702;&#20301;&#32622;&#26080;&#20851;&#12290;&#36890;&#36807;&#21322;&#30417;&#30563;&#26041;&#27861;&#20351;&#29992;&#26631;&#31614;&#25193;&#25955;&#31639;&#27861;&#36827;&#34892;&#20102;&#35813;&#32858;&#31867;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;98%&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#22312;&#26412;&#30740;&#31350;&#25152;&#32771;&#34385;&#30340;&#26399;&#38388;&#65288;&#19968;&#24180;&#12289;&#19977;&#24180;&#21644;&#20116;&#24180;&#65289;&#65292;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#39044;&#27979;&#30340;&#38598;&#32676;&#25345;&#32493;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
he evaluation of the impact of actions undertaken is essential in management. This paper assesses the impact of efforts considered to mitigate risk and create safe environments on a global scale. We measure this impact by looking at the probability of improvement over a specific short period of time. Using the World Risk Index, we conduct a temporal analysis of global disaster risk dynamics from 2011 to 2021. This temporal exploration through the lens of the World Risk Index provides insights into the complex dynamics of disaster risk. We found that, despite sustained efforts, the global landscape remains divided into two main clusters: high susceptibility and moderate susceptibility, regardless of geographical location. This clustering was achieved using a semi-supervised approach through the Label Spreading algorithm, with 98% accuracy. We also found that the prediction of clusters achieved through supervised learning on the period considered in this study (one, three, and five years
&lt;/p&gt;</description></item><item><title>AdaFed&#26159;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#20844;&#20849;&#19979;&#38477;&#26041;&#21521;&#23454;&#29616;&#20844;&#24179;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#26381;&#21153;&#22120;&#30340;&#26356;&#26032;&#26041;&#21521;&#26469;&#30830;&#20445;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#25439;&#22833;&#20989;&#25968;&#20943;&#23567;&#65292;&#24182;&#19988;&#26356;&#22823;&#20540;&#30340;&#23458;&#25143;&#31471;&#30340;&#20943;&#23567;&#36895;&#29575;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.04993</link><description>&lt;p&gt;
AdaFed&#65306;&#36890;&#36807;&#33258;&#36866;&#24212;&#20844;&#20849;&#19979;&#38477;&#26041;&#21521;&#23454;&#29616;&#20844;&#24179;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AdaFed: Fair Federated Learning via Adaptive Common Descent Direction. (arXiv:2401.04993v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04993
&lt;/p&gt;
&lt;p&gt;
AdaFed&#26159;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#20844;&#20849;&#19979;&#38477;&#26041;&#21521;&#23454;&#29616;&#20844;&#24179;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#26381;&#21153;&#22120;&#30340;&#26356;&#26032;&#26041;&#21521;&#26469;&#30830;&#20445;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#25439;&#22833;&#20989;&#25968;&#20943;&#23567;&#65292;&#24182;&#19988;&#26356;&#22823;&#20540;&#30340;&#23458;&#25143;&#31471;&#30340;&#20943;&#23567;&#36895;&#29575;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#35813;&#25216;&#26415;&#65292;&#19968;&#20123;&#36793;&#32536;&#35774;&#22791;/&#23458;&#25143;&#31471;&#22312;&#26381;&#21153;&#22120;&#30340;&#21327;&#35843;&#19979;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#19981;&#20844;&#24179;&#30340;&#27169;&#22411;&#23398;&#20064;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#33021;&#23545;&#26576;&#20123;&#35774;&#22791;&#20135;&#29983;&#19981;&#20844;&#24179;&#30340;&#20248;&#21183;&#25110;&#21155;&#21183;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;AdaFed&#12290;AdaFed&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#26381;&#21153;&#22120;&#26356;&#26032;&#26041;&#21521;&#65292;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#65292;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#25439;&#22833;&#20989;&#25968;&#37117;&#22312;&#20943;&#23567;&#65292;&#24182;&#19988;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25439;&#22833;&#20989;&#25968;&#20540;&#36739;&#22823;&#30340;&#23458;&#25143;&#31471;&#30340;&#20943;&#23567;&#36895;&#29575;&#26356;&#39640;&#12290;AdaFed&#26681;&#25454;&#26412;&#22320;&#26799;&#24230;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#20540;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#36825;&#20010;&#20844;&#20849;&#26041;&#21521;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#32852;&#37030;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;AdaFed&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;AdaFed&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a promising technology via which some edge devices/clients collaboratively train a machine learning model orchestrated by a server. Learning an unfair model is known as a critical problem in federated learning, where the trained model may unfairly advantage or disadvantage some of the devices. To tackle this problem, in this work, we propose AdaFed. The goal of AdaFed is to find an updating direction for the server along which (i) all the clients' loss functions are decreasing; and (ii) more importantly, the loss functions for the clients with larger values decrease with a higher rate. AdaFed adaptively tunes this common direction based on the values of local gradients and loss functions. We validate the effectiveness of AdaFed on a suite of federated datasets, and demonstrate that AdaFed outperforms state-of-the-art fair FL methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25345;&#32467;&#26500;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#20445;&#25345;&#32467;&#26500;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24212;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;PINNs&#30340;&#24615;&#33021;&#21644;&#24212;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2401.04986</link><description>&lt;p&gt;
&#20445;&#25345;&#32467;&#26500;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65306;&#22522;&#20110;&#33021;&#37327;&#25110;&#26446;&#38597;&#26222;&#35834;&#22827;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Structure-Preserving Physics-Informed Neural Networks With Energy or Lyapunov Structure. (arXiv:2401.04986v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04986
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25345;&#32467;&#26500;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#20445;&#25345;&#32467;&#26500;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24212;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;PINNs&#30340;&#24615;&#33021;&#21644;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#35299;&#20915;&#24494;&#20998;&#26041;&#31243;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#20197;&#36866;&#24403;&#30340;&#26041;&#24335;&#20445;&#25345;&#33021;&#37327;&#21644;&#31283;&#23450;&#24615;&#31561;&#32467;&#26500;&#65292;&#23578;&#26410;&#24471;&#21040;&#30830;&#35748;&#12290;&#36825;&#31181;&#38480;&#21046;&#21487;&#33021;&#26159;PINNs&#23398;&#20064;&#36807;&#31243;&#25928;&#29575;&#19981;&#39640;&#21644;&#25968;&#20540;&#32467;&#26524;&#21487;&#33021;&#34920;&#29616;&#20986;&#38750;&#29289;&#29702;&#34892;&#20026;&#30340;&#28508;&#22312;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#23427;&#20204;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#36824;&#24456;&#23569;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20445;&#25345;&#32467;&#26500;&#30340;PINNs&#65292;&#20197;&#25552;&#39640;&#23427;&#20204;&#30340;&#24615;&#33021;&#24182;&#25193;&#23637;&#23427;&#20204;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#29289;&#29702;&#31995;&#32479;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#20445;&#25345;&#32467;&#26500;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24110;&#21161;PINN&#23398;&#20064;&#28508;&#22312;&#32467;&#26500;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#20445;&#25345;&#32467;&#26500;&#30340;PINN&#36827;&#34892;&#40065;&#26834;&#22270;&#20687;&#35782;&#21035;&#30340;&#26694;&#26550;&#12290;&#22312;&#36825;&#37324;&#65292;&#20445;&#25345;&#24213;&#23618;&#31995;&#32479;&#30340;&#26446;&#38597;&#26222;&#35834;&#22827;&#32467;&#26500;&#30830;&#20445;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been growing interest in using physics-informed neural networks (PINNs) to solve differential equations. However, the preservation of structure, such as energy and stability, in a suitable manner has yet to be established. This limitation could be a potential reason why the learning process for PINNs is not always efficient and the numerical results may suggest nonphysical behavior. Besides, there is little research on their applications on downstream tasks. To address these issues, we propose structure-preserving PINNs to improve their performance and broaden their applications for downstream tasks. Firstly, by leveraging prior knowledge about the physical system, a structure-preserving loss function is designed to assist the PINN in learning the underlying structure. Secondly, a framework that utilizes structure-preserving PINN for robust image recognition is proposed. Here, preserving the Lyapunov structure of the underlying system ensures the stability of the sy
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36870;&#35299;&#20915;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#27969;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26082;&#20445;&#35777;&#20102;&#21487;&#36870;&#24615;&#21448;&#38477;&#20302;&#20102;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04979</link><description>&lt;p&gt;
&#21487;&#36870;&#35299;&#20915;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Invertible Solution of Neural Differential Equations for Analysis of Irregularly-Sampled Time Series. (arXiv:2401.04979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04979
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36870;&#35299;&#20915;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#27969;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26082;&#20445;&#35777;&#20102;&#21487;&#36870;&#24615;&#21448;&#38477;&#20302;&#20102;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22788;&#29702;&#38750;&#35268;&#21017;&#21644;&#19981;&#23436;&#25972;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#65288;NDE&#65289;&#30340;&#21487;&#36870;&#35299;&#20915;&#26041;&#26696;&#12290;&#34429;&#28982;&#22522;&#20110;NDE&#30340;&#26041;&#27861;&#26159;&#20998;&#26512;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#19968;&#31181;&#24378;&#22823;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#19981;&#33021;&#20445;&#35777;&#22312;&#20854;&#26631;&#20934;&#24418;&#24335;&#19979;&#36827;&#34892;&#21487;&#36870;&#21464;&#25442;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#35758;&#20351;&#29992;&#20855;&#26377;&#31070;&#32463;&#27969;&#30340;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#65288;Neural CDEs&#65289;&#30340;&#21464;&#31181;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#36739;&#20302;&#30340;&#35745;&#31639;&#36127;&#25285;&#30340;&#21516;&#26102;&#30830;&#20445;&#20102;&#21487;&#36870;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#35757;&#32451;&#21452;&#37325;&#28508;&#22312;&#31354;&#38388;&#65292;&#22686;&#24378;&#20102;&#23545;&#21160;&#24577;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#22312;&#20998;&#31867;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#22686;&#24378;&#22411;&#21452;&#37325;&#28508;&#22312;&#29366;&#24577;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#20013;&#25552;&#39640;&#31934;&#24230;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
To handle the complexities of irregular and incomplete time series data, we propose an invertible solution of Neural Differential Equations (NDE)-based method. While NDE-based methods are a powerful method for analyzing irregularly-sampled time series, they typically do not guarantee reversible transformations in their standard form. Our method suggests the variation of Neural Controlled Differential Equations (Neural CDEs) with Neural Flow, which ensures invertibility while maintaining a lower computational burden. Additionally, it enables the training of a dual latent space, enhancing the modeling of dynamic temporal dynamics. Our research presents an advanced framework that excels in both classification and interpolation tasks. At the core of our approach is an enhanced dual latent states architecture, carefully designed for high precision across various time series tasks. Empirical analysis demonstrates that our method significantly outperforms existing models. This work significan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#38381;&#24335;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#33258;&#21160;&#21270;&#31185;&#23398;&#21457;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#21040;&#19968;&#32452;&#22522;&#20110;&#30456;&#21516;&#37327;&#30340;&#31561;&#20215;&#31867;&#20013;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#35813;&#31561;&#20215;&#31867;&#19982;&#31526;&#21495;&#22238;&#24402;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#26041;&#31243;&#30340;&#20132;&#38598;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2401.04978</link><description>&lt;p&gt;
&#29992;&#31526;&#21495;&#22238;&#24402;&#26799;&#24230;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#38381;&#24335;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Closed-Form Interpretation of Neural Network Classifiers with Symbolic Regression Gradients. (arXiv:2401.04978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#38381;&#24335;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#33258;&#21160;&#21270;&#31185;&#23398;&#21457;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#21040;&#19968;&#32452;&#22522;&#20110;&#30456;&#21516;&#37327;&#30340;&#31561;&#20215;&#31867;&#20013;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#35813;&#31561;&#20215;&#31867;&#19982;&#31526;&#21495;&#22238;&#24402;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#26041;&#31243;&#30340;&#20132;&#38598;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#31185;&#23398;&#21457;&#29616;&#12290;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22238;&#24402;&#19981;&#21516;&#65292;&#23545;&#20110;&#20998;&#31867;&#32780;&#35328;&#65292;&#21363;&#20351;&#31070;&#32463;&#32593;&#32476;&#26412;&#36523;&#30340;&#20998;&#31867;&#22522;&#20110;&#21487;&#20197;&#34920;&#31034;&#20026;&#38381;&#24335;&#26041;&#31243;&#30340;&#37327;&#65292;&#20063;&#19968;&#33324;&#26080;&#27861;&#25214;&#21040;&#20174;&#31070;&#32463;&#32593;&#32476;&#21040;&#31526;&#21495;&#26041;&#31243;&#30340;&#19968;&#23545;&#19968;&#26144;&#23556;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#23558;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#21040;&#19968;&#20010;&#31561;&#20215;&#31867;&#20013;&#65292;&#36825;&#20010;&#31561;&#20215;&#31867;&#30340;&#20998;&#31867;&#20989;&#25968;&#30340;&#20915;&#31574;&#37117;&#22522;&#20110;&#30456;&#21516;&#30340;&#37327;&#12290;&#25105;&#36890;&#36807;&#25214;&#21040;&#36825;&#20010;&#31561;&#20215;&#31867;&#19982;&#30001;&#31526;&#21495;&#22238;&#24402;&#25628;&#32034;&#31354;&#38388;&#23450;&#20041;&#30340;&#21487;&#35835;&#30340;&#26041;&#31243;&#30340;&#20132;&#38598;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38480;&#20110;&#20998;&#31867;&#22120;&#25110;&#23436;&#25972;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#38544;&#34255;&#23618;&#25110;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20219;&#24847;&#31070;&#32463;&#20803;&#65292;&#25110;&#31616;&#21270;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#22120;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
I introduce a unified framework for interpreting neural network classifiers tailored toward automated scientific discovery. In contrast to neural network-based regression, for classification, it is in general impossible to find a one-to-one mapping from the neural network to a symbolic equation even if the neural network itself bases its classification on a quantity that can be written as a closed-form equation. In this paper, I embed a trained neural network into an equivalence class of classifying functions that base their decisions on the same quantity. I interpret neural networks by finding an intersection between this equivalence class and human-readable equations defined by the search space of symbolic regression. The approach is not limited to classifiers or full neural networks and can be applied to arbitrary neurons in hidden layers or latent spaces or to simplify the process of interpreting neural network regressors.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ConvConcatNet&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;EEG&#20013;&#37325;&#24314;Mel&#39057;&#35889;&#65292;&#24182;&#22312;Auditory EEG Challenge&#30340;Task 2&#20013;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#65292;&#37325;&#24314;&#21644;&#30446;&#26631;Mel&#39057;&#35889;&#20043;&#38388;&#30340;&#30382;&#23572;&#36874;&#30456;&#20851;&#31995;&#25968;&#36798;&#21040;0.0420&#12290;</title><link>http://arxiv.org/abs/2401.04965</link><description>&lt;p&gt;
ConvConcatNet: &#19968;&#20010;&#29992;&#20110;&#20174;&#33041;&#30005;&#22270; (EEG) &#37325;&#24314; Mel &#39057;&#35889;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ConvConcatNet: a deep convolutional neural network to reconstruct mel spectrogram from the EEG. (arXiv:2401.04965v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ConvConcatNet&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;EEG&#20013;&#37325;&#24314;Mel&#39057;&#35889;&#65292;&#24182;&#22312;Auditory EEG Challenge&#30340;Task 2&#20013;&#21462;&#24471;&#20102;&#31532;&#19968;&#21517;&#65292;&#37325;&#24314;&#21644;&#30446;&#26631;Mel&#39057;&#35889;&#20043;&#38388;&#30340;&#30382;&#23572;&#36874;&#30456;&#20851;&#31995;&#25968;&#36798;&#21040;0.0420&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30740;&#31350;&#22823;&#33041;&#23545;&#35821;&#38899;&#30340;&#22788;&#29702;&#65292;&#36890;&#24120;&#20351;&#29992;&#31616;&#21333;&#30340;&#32447;&#24615;&#27169;&#22411;&#26469;&#24314;&#31435;&#33041;&#20449;&#21495;&#21644;&#35821;&#38899;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32447;&#24615;&#27169;&#22411;&#19981;&#36866;&#29992;&#20110;&#27169;&#25311;&#22823;&#33041;&#36825;&#26679;&#39640;&#24230;&#21160;&#24577;&#21644;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;&#23613;&#31649;&#26368;&#36817;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#26041;&#27861;&#65292;&#20294;&#20174;&#26410;&#35265;&#36807;&#30340;&#21463;&#35797;&#32773;&#30340; EEG &#20013;&#37325;&#24314;&#26410;&#30693;&#21050;&#28608;&#20173;&#28982;&#26159;&#19968;&#39033;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; ConvConcatNet&#65292;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#24191;&#27867;&#30340;&#20018;&#25509;&#25805;&#20316;&#65292;&#20174; EEG &#20013;&#37325;&#24314; Mel &#39057;&#35889;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340; ConvConcatNet &#27169;&#22411;&#65292;&#37325;&#24314;&#30340; Mel &#39057;&#35889;&#19982;&#30446;&#26631; Mel &#39057;&#35889;&#20043;&#38388;&#30340;&#30382;&#23572;&#36874;&#30456;&#20851;&#31995;&#25968;&#21487;&#36798;&#21040; 0.0420&#65292;&#22312;&#21548;&#35273; EEG &#25361;&#25112;&#36187;&#30340;&#20219;&#21153;2&#20013;&#25490;&#21517;&#31532;&#19968;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#27169;&#22411;&#23558;&#22312; Github &#19978;&#25552;&#20379;&#65306;https://github.com/xuxiran/ConvConcatNet
&lt;/p&gt;
&lt;p&gt;
To investigate the processing of speech in the brain, simple linear models are commonly used to establish a relationship between brain signals and speech features. However, these linear models are ill-equipped to model a highly dynamic and complex non-linear system like the brain. Although non-linear methods with neural networks have been developed recently, reconstructing unseen stimuli from unseen subjects' EEG is still a highly challenging task. This work presents a novel method, ConvConcatNet, to reconstruct mel-specgrams from EEG, in which the deep convolution neural network and extensive concatenation operation were combined. With our ConvConcatNet model, the Pearson correlation between the reconstructed and the target mel-spectrogram can achieve 0.0420, which was ranked as No.1 in the Task 2 of the Auditory EEG Challenge. The codes and models to implement our work will be available on Github: https://github.com/xuxiran/ConvConcatNet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22235;&#26059;&#32764;&#22120;&#22312;&#21463;&#21040;&#38459;&#21147;&#21147;&#24433;&#21709;&#19979;&#30340;&#36712;&#36857;&#29983;&#25104;&#21644;&#25511;&#21046;&#35774;&#35745;&#38382;&#39064;&#12290;&#36890;&#36807;&#20445;&#25345;&#25511;&#21046;&#22120;&#22266;&#23450;&#65292;&#21482;&#25913;&#21464;&#36712;&#36857;&#29983;&#25104;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#39118;&#38459;&#30340;&#35268;&#21010;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22235;&#26059;&#32764;&#22120;&#31995;&#32479;&#30340;&#36712;&#36857;&#36319;&#36394;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04960</link><description>&lt;p&gt;
&#24403;&#20320;&#33021;&#25913;&#21464;&#20320;&#30340;&#35268;&#21010;&#22120;&#20026;&#20309;&#35201;&#25913;&#21464;&#20320;&#30340;&#25511;&#21046;&#22120;&#65306;&#38024;&#23545;&#22235;&#26059;&#32764;&#31995;&#32479;&#30340;&#32771;&#34385;&#39118;&#38459;&#36712;&#36857;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Why Change Your Controller When You Can Change Your Planner: Drag-Aware Trajectory Generation for Quadrotor Systems. (arXiv:2401.04960v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22235;&#26059;&#32764;&#22120;&#22312;&#21463;&#21040;&#38459;&#21147;&#21147;&#24433;&#21709;&#19979;&#30340;&#36712;&#36857;&#29983;&#25104;&#21644;&#25511;&#21046;&#35774;&#35745;&#38382;&#39064;&#12290;&#36890;&#36807;&#20445;&#25345;&#25511;&#21046;&#22120;&#22266;&#23450;&#65292;&#21482;&#25913;&#21464;&#36712;&#36857;&#29983;&#25104;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#39118;&#38459;&#30340;&#35268;&#21010;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22235;&#26059;&#32764;&#22120;&#31995;&#32479;&#30340;&#36712;&#36857;&#36319;&#36394;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#22235;&#26059;&#32764;&#22120;&#22312;&#36733;&#33655;&#20256;&#36882;&#20013;&#30340;&#36234;&#26469;&#36234;&#24191;&#27867;&#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#22235;&#26059;&#32764;&#22120;&#30340;&#32852;&#21512;&#36712;&#36857;&#29983;&#25104;&#21644;&#21453;&#39304;&#25511;&#21046;&#35774;&#35745;&#38382;&#39064;&#65292;&#32771;&#34385;&#20102;&#22235;&#26059;&#32764;&#22120;&#22312;&#31354;&#27668;&#21160;&#21147;&#23398;&#21147;&#30340;&#20316;&#29992;&#19979;&#30340;&#36816;&#21160;&#12290;&#25658;&#24102;&#30340;&#36733;&#33655;&#20135;&#29983;&#30340;&#26410;&#24314;&#27169;&#31354;&#27668;&#21160;&#21147;&#23398;&#38459;&#21147;&#21147;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#30340;&#21518;&#26524;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#23558;&#31354;&#27668;&#21160;&#21147;&#23398;&#25928;&#24212;&#24314;&#27169;&#20026;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#21097;&#20313;&#21160;&#21147;&#23398;&#25110;&#22806;&#37096;&#24178;&#25200;&#65292;&#24341;&#23548;&#20986;&#19968;&#31181;&#21487;&#33021;&#36896;&#25104;&#28798;&#38590;&#30340;&#21453;&#24212;&#24615;&#25919;&#31574;&#12290;&#27492;&#22806;&#65292;&#22312;&#30828;&#20214;&#24179;&#21488;&#19978;&#37325;&#26032;&#35774;&#35745;&#25511;&#21046;&#22120;&#21644;&#35843;&#25972;&#25511;&#21046;&#22686;&#30410;&#26159;&#19968;&#39033;&#36153;&#26102;&#30340;&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#20445;&#25345;&#25511;&#21046;&#22120;&#22266;&#23450;&#65292;&#21482;&#25913;&#21464;&#36712;&#36857;&#29983;&#25104;&#37096;&#20998;&#33021;&#22815;&#25552;&#39640;&#22312;&#21463;&#21040;&#38459;&#21147;&#21147;&#24433;&#21709;&#19979;&#30340;&#22235;&#26059;&#32764;&#22120;&#31995;&#32479;&#30340;&#36712;&#36857;&#36319;&#36394;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#26368;&#20248;&#22235;&#26059;&#32764;&#22120;&#25511;&#21046;&#38382;&#39064;&#24212;&#29992;&#36866;&#24403;&#30340;&#26494;&#24347;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#34913;&#37327;&#25511;&#21046;&#22120;&#36319;&#36394;&#21442;&#32771;&#36712;&#36857;&#33021;&#21147;&#30340;&#36319;&#36394;&#25104;&#26412;&#20989;&#25968;&#65292;&#20174;&#32780;&#24418;&#25104;&#19968;&#20010;&#32771;&#34385;&#39118;&#38459;&#30340;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the increasing use of quadrotors for payload delivery, we consider a joint trajectory generation and feedback control design problem for a quadrotor experiencing aerodynamic wrenches. Unmodeled aerodynamic drag forces from carried payloads can lead to catastrophic outcomes. Prior work model aerodynamic effects as residual dynamics or external disturbances in the control problem leading to a reactive policy that could be catastrophic. Moreover, redesigning controllers and tuning control gains on hardware platforms is a laborious effort. In this paper, we argue that adapting the trajectory generation component keeping the controller fixed can improve trajectory tracking for quadrotor systems experiencing drag forces. To achieve this, we formulate a drag-aware planning problem by applying a suitable relaxation to an optimal quadrotor control problem, introducing a tracking cost function which measures the ability of a controller to follow a reference trajectory. This tracking
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;Q&#23398;&#20064;&#24378;&#21270;&#31639;&#27861;&#22312;PhysioNet/Computing in Cardiology Challenge&#65288;CinC&#65289;&#25552;&#20379;&#30340;&#22810;&#31181;ECG&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#65292;&#30740;&#31350;&#20102;P&#27874;&#21644;PR&#38388;&#26399;&#22312;Lead II&#21644;Lead V1&#19978;&#30340;&#19981;&#21516;&#21464;&#24322;&#12290;&#36890;&#36807;Q-Agent&#30340;&#20998;&#31867;&#33021;&#22815;&#22312;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;90.4&#65285;&#30340;&#24773;&#20917;&#19979;&#23545;&#24739;&#32773;&#30340;&#24515;&#25615;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#24179;&#22343;&#27721;&#26126;&#25439;&#22833;&#29575;&#21644;&#36739;&#24555;&#30340;&#20998;&#31867;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.04938</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#25552;&#21319;ECG&#35786;&#26029;&#65306;&#38024;&#23545;P&#27874;&#21644;PR&#38388;&#26399;&#30340;&#20840;&#23616;&#27874;&#24418;&#21464;&#24322;(arXiv&#65306;2401.04938v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
Advancing ECG Diagnosis Using Reinforcement Learning on Global Waveform Variations Related to P Wave and PR Interval. (arXiv:2401.04938v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;Q&#23398;&#20064;&#24378;&#21270;&#31639;&#27861;&#22312;PhysioNet/Computing in Cardiology Challenge&#65288;CinC&#65289;&#25552;&#20379;&#30340;&#22810;&#31181;ECG&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#65292;&#30740;&#31350;&#20102;P&#27874;&#21644;PR&#38388;&#26399;&#22312;Lead II&#21644;Lead V1&#19978;&#30340;&#19981;&#21516;&#21464;&#24322;&#12290;&#36890;&#36807;Q-Agent&#30340;&#20998;&#31867;&#33021;&#22815;&#22312;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;90.4&#65285;&#30340;&#24773;&#20917;&#19979;&#23545;&#24739;&#32773;&#30340;&#24515;&#25615;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#24179;&#22343;&#27721;&#26126;&#25439;&#22833;&#29575;&#21644;&#36739;&#24555;&#30340;&#20998;&#31867;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20998;&#26512;&#21487;&#38752;&#22320;&#35786;&#26029;&#24515;&#33039;&#30149;&#24773;&#65292;&#20851;&#38190;&#35201;&#20934;&#30830;&#26816;&#27979;P&#27874;&#21644;&#27979;&#37327;PR&#38388;&#26399;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;ECG&#20449;&#21495;&#20013;&#23384;&#22312;&#30340;&#20840;&#23616;&#21464;&#24322;&#65292;&#23454;&#29616;&#36328;&#22810;&#26679;&#21270;&#20154;&#32676;&#30340;&#19968;&#33268;&#21644;&#21487;&#25512;&#24191;&#30340;&#35786;&#26029;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#23558;Q&#23398;&#20064;&#24378;&#21270;&#31639;&#27861;&#24212;&#29992;&#20110;PhysioNet/Computing in Cardiology Challenge&#65288;CinC&#65289;&#20013;&#21487;&#29992;&#30340;&#21508;&#31181;ECG&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#20102;Lead II&#21644;Lead V1&#19978;P&#27874;&#21644;PR&#38388;&#26399;&#30340;&#22810;&#26679;&#24615;&#65292;&#21253;&#21547;&#20116;&#20010;ECG&#24515;&#25615;&#31867;&#22411;&#65292;&#21253;&#25324;&#27491;&#24120;&#31398;&#24615;&#24515;&#24459;&#12289;&#24515;&#25151;&#25169;&#21160;&#12289;&#24515;&#25151;&#32420;&#39076;&#12289;&#19968;&#24230;&#25151;&#23460;&#20256;&#23548;&#38459;&#28382;&#21644;&#24038;&#25151;&#25193;&#22823;&#12290;Q-Agent&#23545;8,867&#21517;&#24739;&#32773;&#30340;71,672&#20010;&#24515;&#25615;&#26679;&#26412;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24179;&#22343;&#20934;&#30830;&#29575;&#20026;90.4&#65285;&#65292;&#36890;&#36807;&#38169;&#35823;&#20998;&#31867;&#30340;&#24179;&#22343;&#27721;&#26126;&#25439;&#22833;&#29575;&#20165;&#20026;9.6&#65285;&#12290;&#22312;&#22788;&#29702;&#32422;40,000&#20010;&#26679;&#26412;&#30340;&#31532;100&#20010;episode&#20013;&#65292;&#24179;&#22343;&#20998;&#31867;&#26102;&#38388;&#20026;0.04&#31186;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reliable diagnosis of cardiac conditions through electrocardiogram (ECG) analysis critically depends on accurately detecting P waves and measuring the PR interval. However, achieving consistent and generalizable diagnoses across diverse populations presents challenges due to the inherent global variations observed in ECG signals. This paper is focused on applying the Q learning reinforcement algorithm to the various ECG datasets available in the PhysioNet/Computing in Cardiology Challenge (CinC). Five ECG beats, including Normal Sinus Rhythm, Atrial Flutter, Atrial Fibrillation, 1st Degree Atrioventricular Block, and Left Atrial Enlargement, are included to study variations of P waves and PR Interval on Lead II and Lead V1. Q-Agent classified 71,672 beat samples in 8,867 patients with an average accuracy of 90.4% and only 9.6% average hamming loss over misclassification. The average classification time at the 100th episode containing around 40,000 samples is 0.04 seconds. An averag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#23436;&#20840;&#20998;&#25955;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#20004;&#31181;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#31639;&#27861;&#20197;&#21450;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.04934</link><description>&lt;p&gt;
&#23436;&#20840;&#20998;&#25955;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Fully Decentralized Cooperative Multi-Agent Reinforcement Learning: A Survey. (arXiv:2401.04934v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#23436;&#20840;&#20998;&#25955;&#30340;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#20004;&#31181;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#31639;&#27861;&#20197;&#21450;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26159;&#35299;&#20915;&#35768;&#22810;&#23454;&#38469;&#21512;&#20316;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#29616;&#23454;&#24212;&#29992;&#30340;&#38480;&#21046;&#21487;&#33021;&#35201;&#27714;&#20197;&#23436;&#20840;&#20998;&#25955;&#30340;&#26041;&#24335;&#35757;&#32451;&#26234;&#33021;&#20307;&#12290;&#30001;&#20110;&#32570;&#20047;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#20449;&#24687;&#65292;&#35201;&#22312;&#23436;&#20840;&#20998;&#25955;&#30340;&#24773;&#20917;&#19979;&#24471;&#20986;&#21487;&#20197;&#25910;&#25947;&#21040;&#26368;&#20248;&#32852;&#21512;&#31574;&#30053;&#30340;&#31639;&#27861;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#36824;&#27809;&#26377;&#24471;&#21040;&#28145;&#20837;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#20004;&#31181;&#35774;&#32622;&#30340;&#23436;&#20840;&#20998;&#25955;&#26041;&#27861;&#65306;&#26368;&#22823;&#21270;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#20849;&#20139;&#22870;&#21169;&#21644;&#26368;&#22823;&#21270;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#20010;&#20307;&#22870;&#21169;&#30340;&#24635;&#21644;&#65292;&#24182;&#35752;&#35770;&#20102;&#24320;&#25918;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cooperative multi-agent reinforcement learning is a powerful tool to solve many real-world cooperative tasks, but restrictions of real-world applications may require training the agents in a fully decentralized manner. Due to the lack of information about other agents, it is challenging to derive algorithms that can converge to the optimal joint policy in a fully decentralized setting. Thus, this research area has not been thoroughly studied. In this paper, we seek to systematically review the fully decentralized methods in two settings: maximizing a shared reward of all agents and maximizing the sum of individual rewards of all agents, and discuss open questions and future research directions.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20284;&#28982;&#36335;&#24452;&#21407;&#29702;&#21644;&#26032;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#26412;&#30740;&#31350;&#38024;&#23545;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#30340;&#26465;&#20214;&#20284;&#28982;&#24615;&#25552;&#20379;&#20102;&#38750;&#28176;&#36817;&#21487;&#35777;&#26126;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.04933</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#27979;&#35797;&#26102;&#20284;&#28982;&#24615;&#65306;&#20284;&#28982;&#36335;&#24452;&#21407;&#29702;&#21450;&#20854;&#22312;OOD&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Rethinking Test-time Likelihood: The Likelihood Path Principle and Its Application to OOD Detection. (arXiv:2401.04933v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04933
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20284;&#28982;&#36335;&#24452;&#21407;&#29702;&#21644;&#26032;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#26412;&#30740;&#31350;&#38024;&#23545;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#30340;&#26465;&#20214;&#20284;&#28982;&#24615;&#25552;&#20379;&#20102;&#38750;&#28176;&#36817;&#21487;&#35777;&#26126;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20284;&#28982;&#24615;&#22312;&#29702;&#35770;&#19978;&#24456;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#26159;&#36890;&#36807;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;DGM&#65289;&#20272;&#35745;&#30340;&#20284;&#28982;&#24615;&#22312;&#23454;&#36341;&#20013;&#32463;&#24120;&#20986;&#29616;&#38382;&#39064;&#65292;&#23545;&#20110;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#34920;&#29616;&#19981;&#20339;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#24037;&#20316;&#24320;&#22987;&#32771;&#34385;&#26367;&#20195;&#24615;&#35780;&#20998;&#65292;&#24182;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#65292;&#20063;&#19981;&#28165;&#26970;&#23427;&#20204;&#30340;&#36873;&#25321;&#26159;&#21542;&#25552;&#21462;&#20102;&#36275;&#22815;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#23581;&#35797;&#25913;&#21464;&#36825;&#31181;&#24773;&#20917;&#65292;&#36890;&#36807;&#23545;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20284;&#28982;&#36335;&#24452;&#65288;LPath&#65289;&#21407;&#29702;&#65292;&#25512;&#24191;&#20102;&#20284;&#28982;&#24615;&#21407;&#29702;&#12290;&#36825;&#23558;&#25628;&#32034;&#26377;&#29992;&#30340;&#25688;&#35201;&#32479;&#35745;&#37327;&#32553;&#23567;&#21040;VAEs&#26465;&#20214;&#20284;&#28982;&#24615;&#30340;&#26368;&#23567;&#20805;&#20998;&#32479;&#35745;&#37327;&#12290;&#20854;&#27425;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#22914;&#20960;&#20046;&#26377;&#25928;&#25903;&#25345;&#12289;&#22522;&#26412;&#36317;&#31163;&#21644;&#20849;-Lipschitz&#24615;&#65292;&#25105;&#20204;&#20026;&#26576;&#20123;&#26368;&#23567;&#20805;&#20998;&#32479;&#35745;&#37327;&#30340;&#25688;&#35201;&#25552;&#20379;&#20102;&#38750;&#28176;&#36817;&#21487;&#35777;&#26126;&#30340;OOD&#26816;&#27979;&#20445;&#35777;&#12290;&#30456;&#24212;&#30340;LPath&#31639;&#27861;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
While likelihood is attractive in theory, its estimates by deep generative models (DGMs) are often broken in practice, and perform poorly for out of distribution (OOD) Detection. Various recent works started to consider alternative scores and achieved better performances. However, such recipes do not come with provable guarantees, nor is it clear that their choices extract sufficient information.  We attempt to change this by conducting a case study on variational autoencoders (VAEs). First, we introduce the likelihood path (LPath) principle, generalizing the likelihood principle. This narrows the search for informative summary statistics down to the minimal sufficient statistics of VAEs' conditional likelihoods. Second, introducing new theoretic tools such as nearly essential support, essential distance and co-Lipschitzness, we obtain non-asymptotic provable OOD detection guarantees for certain distillation of the minimal sufficient statistics. The corresponding LPath algorithm demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#26174;&#33879;&#25552;&#39640;&#20302;FPR&#19979;&#30340;TPR&#65292;&#20197;&#39564;&#35777;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2401.04929</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#25552;&#21319;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning-Based Difficulty Calibration for Enhanced Membership Inference Attacks. (arXiv:2401.04929v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#26174;&#33879;&#25552;&#39640;&#20302;FPR&#19979;&#30340;TPR&#65292;&#20197;&#39564;&#35777;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#30446;&#21069;&#26159;&#21508;&#31181;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20174;&#21307;&#30103;&#20445;&#20581;&#21040;&#37329;&#34701;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#25935;&#24863;&#25968;&#25454;&#26469;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#24341;&#21457;&#20102;&#23545;&#38544;&#31169;&#21644;&#23433;&#20840;&#30340;&#25285;&#24551;&#12290;&#19968;&#31181;&#39564;&#35777;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#27861;&#26159;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIA&#65289;&#65292;&#23427;&#20801;&#35768;&#23545;&#25163;&#30830;&#23450;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#26159;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#12290;&#34429;&#28982;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;MIA&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#33021;&#22815;&#22312;&#20302;&#20551;&#38451;&#24615;&#29575;&#65288;FPR&#65289;&#21306;&#22495;&#65288;0.01%~1%&#65289;&#23454;&#29616;&#36739;&#39640;&#30340;&#30495;&#38451;&#24615;&#29575;&#65288;TPR&#65289;&#12290;&#36825;&#26159;&#23454;&#38469;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;MIA&#24517;&#39035;&#32771;&#34385;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MIA&#26041;&#27861;&#65292;&#26088;&#22312;&#26174;&#33879;&#25552;&#39640;&#20302;FPR&#30340;TPR&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#65288;LDC-MIA&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#23558;&#25968;&#25454;&#35760;&#24405;&#20197;&#20854;&#38590;&#24230;&#32423;&#21035;&#36827;&#34892;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models, in particular deep neural networks, are currently an integral part of various applications, from healthcare to finance. However, using sensitive data to train these models raises concerns about privacy and security. One method that has emerged to verify if the trained models are privacy-preserving is Membership Inference Attacks (MIA), which allows adversaries to determine whether a specific data point was part of a model's training dataset. While a series of MIAs have been proposed in the literature, only a few can achieve high True Positive Rates (TPR) in the low False Positive Rate (FPR) region (0.01%~1%). This is a crucial factor to consider for an MIA to be practically useful in real-world settings. In this paper, we present a novel approach to MIA that is aimed at significantly improving TPR at low FPRs. Our method, named learning-based difficulty calibration for MIA(LDC-MIA), characterizes data records by their hardness levels using a neural network clas
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#24322;&#26500;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#25918;&#26494;&#30340;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#65292;&#38450;&#27490;&#34920;&#31034;&#22349;&#32553;&#65292;&#22686;&#24378;&#29305;&#24449;&#30340;&#21487;&#20256;&#36882;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.04928</link><description>&lt;p&gt;
&#25918;&#26494;&#30340;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Relaxed Contrastive Learning for Federated Learning. (arXiv:2401.04928v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04928
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#24322;&#26500;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#25918;&#26494;&#30340;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#65292;&#38450;&#27490;&#34920;&#31034;&#22349;&#32553;&#65292;&#22686;&#24378;&#29305;&#24449;&#30340;&#21487;&#20256;&#36882;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#26377;&#25928;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#24322;&#26500;&#24615;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#26412;&#22320;&#35757;&#32451;&#20013;&#23458;&#25143;&#31471;&#20043;&#38388;&#26799;&#24230;&#26356;&#26032;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24314;&#31435;&#20854;&#19982;&#29305;&#24449;&#34920;&#31034;&#20998;&#24067;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#23548;&#20986;&#20102;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;SCL&#65289;&#30446;&#26631;&#26469;&#20943;&#36731;&#23616;&#37096;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;SCL&#30340;&#26420;&#32032;&#24212;&#29992;&#20250;&#23548;&#33268;&#34920;&#31034;&#22349;&#32553;&#65292;&#23548;&#33268;&#25910;&#25947;&#32531;&#24930;&#21644;&#24615;&#33021;&#25552;&#21319;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25918;&#26494;&#30340;&#23545;&#27604;&#23398;&#20064;&#25439;&#22833;&#65292;&#23545;&#27599;&#20010;&#31867;&#21035;&#20869;&#36807;&#20110;&#30456;&#20284;&#30340;&#26679;&#26412;&#23545;&#26045;&#21152;&#21457;&#25955;&#24809;&#32602;&#12290;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#38450;&#27490;&#34920;&#31034;&#22349;&#32553;&#65292;&#22686;&#24378;&#29305;&#24449;&#30340;&#21487;&#20256;&#36882;&#24615;&#65292;&#20419;&#36827;&#21327;&#20316;&#35757;&#32451;&#65292;&#24182;&#23548;&#33268;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#25152;&#26377;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20013;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel contrastive learning framework to effectively address the challenges of data heterogeneity in federated learning. We first analyze the inconsistency of gradient updates across clients during local training and establish its dependence on the distribution of feature representations, leading to the derivation of the supervised contrastive learning (SCL) objective to mitigate local deviations. In addition, we show that a na\"ive adoption of SCL in federated learning leads to representation collapse, resulting in slow convergence and limited performance gains. To address this issue, we introduce a relaxed contrastive learning loss that imposes a divergence penalty on excessively similar sample pairs within each class. This strategy prevents collapsed representations and enhances feature transferability, facilitating collaborative training and leading to significant performance improvements. Our framework outperforms all existing federated learning approaches by huge marg
&lt;/p&gt;</description></item><item><title>NEAT&#26159;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#20027;&#21160;&#24320;&#25918;&#38598;&#26631;&#27880;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#26631;&#31614;&#30340;&#21487;&#32858;&#31867;&#24615;&#26469;&#35782;&#21035;&#24050;&#30693;&#31867;&#21035;&#65292;&#24182;&#20174;&#24050;&#30693;&#21644;&#26410;&#30693;&#31867;&#21035;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#36873;&#25321;&#20855;&#26377;&#20449;&#24687;&#37327;&#30340;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#12290;</title><link>http://arxiv.org/abs/2401.04923</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#19968;&#33268;&#24615;&#30340;&#25968;&#25454;&#20013;&#24515;&#20027;&#21160;&#24320;&#25918;&#38598;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Inconsistency-Based Data-Centric Active Open-Set Annotation. (arXiv:2401.04923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04923
&lt;/p&gt;
&lt;p&gt;
NEAT&#26159;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#20027;&#21160;&#24320;&#25918;&#38598;&#26631;&#27880;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#26631;&#31614;&#30340;&#21487;&#32858;&#31867;&#24615;&#26469;&#35782;&#21035;&#24050;&#30693;&#31867;&#21035;&#65292;&#24182;&#20174;&#24050;&#30693;&#21644;&#26410;&#30693;&#31867;&#21035;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#36873;&#25321;&#20855;&#26377;&#20449;&#24687;&#37327;&#30340;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20943;&#23569;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#26631;&#35760;&#24037;&#20316;&#37327;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#25928;&#26524;&#21463;&#21040;&#20854;&#23553;&#38381;&#19990;&#30028;&#20551;&#35774;&#30340;&#38480;&#21046;&#65292;&#21363;&#20551;&#35774;&#26410;&#26631;&#35760;&#25968;&#25454;&#27744;&#20013;&#30340;&#25152;&#26377;&#25968;&#25454;&#26469;&#33258;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#24050;&#30693;&#31867;&#21035;&#12290;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#20551;&#35774;&#36890;&#24120;&#26159;&#26080;&#25928;&#30340;&#65292;&#22240;&#20026;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#21487;&#33021;&#23384;&#22312;&#26410;&#30693;&#31867;&#21035;&#65292;&#23548;&#33268;&#20027;&#21160;&#24320;&#25918;&#38598;&#26631;&#27880;&#38382;&#39064;&#12290;&#25968;&#25454;&#20013;&#23384;&#22312;&#26410;&#30693;&#31867;&#21035;&#20250;&#26174;&#33879;&#24433;&#21709;&#29616;&#26377;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#20854;&#24341;&#20837;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25968;&#25454;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;NEAT&#65292;&#23427;&#21487;&#20197;&#20027;&#21160;&#26631;&#27880;&#24320;&#25918;&#38598;&#25968;&#25454;&#12290;NEAT&#26088;&#22312;&#20174;&#24050;&#30693;&#21644;&#26410;&#30693;&#31867;&#21035;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#27744;&#20013;&#26631;&#35760;&#24050;&#30693;&#31867;&#21035;&#30340;&#25968;&#25454;&#65292;&#21033;&#29992;&#26631;&#31614;&#30340;&#21487;&#32858;&#31867;&#24615;&#26469;&#35782;&#21035;&#26410;&#26631;&#35760;&#27744;&#20013;&#30340;&#24050;&#30693;&#31867;&#21035;&#65292;&#24182;&#36873;&#25321;&#20855;&#26377;&#20449;&#24687;&#37327;&#30340;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning is a commonly used approach that reduces the labeling effort required to train deep neural networks. However, the effectiveness of current active learning methods is limited by their closed-world assumptions, which assume that all data in the unlabeled pool comes from a set of predefined known classes. This assumption is often not valid in practical situations, as there may be unknown classes in the unlabeled data, leading to the active open-set annotation problem. The presence of unknown classes in the data can significantly impact the performance of existing active learning methods due to the uncertainty they introduce. To address this issue, we propose a novel data-centric active learning method called NEAT that actively annotates open-set data. NEAT is designed to label known classes data from a pool of both known and unknown classes unlabeled data. It utilizes the clusterability of labels to identify the known classes from the unlabeled pool and selects informative
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21483;&#20570;&#20809;&#35889;&#21464;&#25442;&#22120;&#65288;SPT&#65289;&#30340;&#26694;&#26550;&#26469;&#39044;&#27979;&#32418;&#24040;&#26143;&#30340;&#24180;&#40836;&#21644;&#36136;&#37327;&#65292;&#20854;&#20013;&#20851;&#38190;&#30340;&#32452;&#25104;&#37096;&#20998;&#26159;&#20026;&#20809;&#35889;&#35774;&#35745;&#30340;&#22810;&#22836;&#21704;&#36798;&#29595;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#20102;&#39532;&#27663;&#36317;&#31163;&#21644;&#33945;&#29305;&#21345;&#27931;Dropout&#26469;&#35299;&#20915;&#38382;&#39064;&#24182;&#20998;&#26512;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04900</link><description>&lt;p&gt;
SPT: &#20809;&#35889;&#21464;&#25442;&#22120;&#29992;&#20110;&#32418;&#24040;&#26143;&#24180;&#40836;&#21644;&#36136;&#37327;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
SPT: Spectral Transformer for Red Giant Stars Age and Mass Estimation. (arXiv:2401.04900v1 [astro-ph.SR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04900
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21483;&#20570;&#20809;&#35889;&#21464;&#25442;&#22120;&#65288;SPT&#65289;&#30340;&#26694;&#26550;&#26469;&#39044;&#27979;&#32418;&#24040;&#26143;&#30340;&#24180;&#40836;&#21644;&#36136;&#37327;&#65292;&#20854;&#20013;&#20851;&#38190;&#30340;&#32452;&#25104;&#37096;&#20998;&#26159;&#20026;&#20809;&#35889;&#35774;&#35745;&#30340;&#22810;&#22836;&#21704;&#36798;&#29595;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#21516;&#26102;&#36824;&#20351;&#29992;&#20102;&#39532;&#27663;&#36317;&#31163;&#21644;&#33945;&#29305;&#21345;&#27931;Dropout&#26469;&#35299;&#20915;&#38382;&#39064;&#24182;&#20998;&#26512;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32418;&#24040;&#26143;&#30340;&#24180;&#40836;&#21644;&#36136;&#37327;&#23545;&#20110;&#29702;&#35299;&#38134;&#27827;&#31995;&#30340;&#32467;&#26500;&#21644;&#28436;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#31561;&#26102;&#32447;&#26041;&#27861;&#22312;&#20272;&#31639;&#20013;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#20026;&#22312;&#36203;&#32599;&#22270;&#20013;&#31561;&#26102;&#32447;&#37325;&#21472;&#65292;&#32780;&#22825;&#20307;&#22768;&#23398;&#34429;&#28982;&#26356;&#31934;&#30830;&#65292;&#20294;&#38656;&#35201;&#39640;&#31934;&#24230;&#12289;&#38271;&#26399;&#35266;&#27979;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#8212;&#8212;&#20809;&#35889;&#21464;&#25442;&#22120;&#65288;Spectral Transformer&#65292;SPT&#65289;&#65292;&#36890;&#36807;&#32418;&#24040;&#26143;&#30340;&#20809;&#35889;&#39044;&#27979;&#20854;&#19982;&#22825;&#20307;&#22768;&#23398;&#19968;&#33268;&#30340;&#24180;&#40836;&#21644;&#36136;&#37327;&#12290;SPT&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#20026;&#20809;&#35889;&#29305;&#21035;&#35774;&#35745;&#30340;&#22810;&#22836;&#21704;&#36798;&#29595;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#21487;&#20197;&#25429;&#25417;&#19981;&#21516;&#27874;&#38271;&#19978;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#39532;&#27663;&#36317;&#31163;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#23610;&#24230;&#19981;&#24179;&#34913;&#21644;&#20132;&#20114;&#27169;&#24335;&#25439;&#22833;&#65292;&#24182;&#32467;&#21512;&#20102;&#33945;&#29305;&#21345;&#27931;Dropout&#26469;&#23450;&#37327;&#20998;&#26512;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#26469;&#33258;LAMOST&#30340;3,880&#20010;&#32418;&#24040;&#26143;&#20809;&#35889;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;SPT&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24180;&#40836;
&lt;/p&gt;
&lt;p&gt;
The age and mass of red giants are essential for understanding the structure and evolution of the Milky Way. Traditional isochrone methods for these estimations are inherently limited due to overlapping isochrones in the Hertzsprung-Russell diagram, while asteroseismology, though more precise, requires high-precision, long-term observations. In response to these challenges, we developed a novel framework, Spectral Transformer (SPT), to predict the age and mass of red giants aligned with asteroseismology from their spectra. A key component of SPT, the Multi-head Hadamard Self-Attention mechanism, designed specifically for spectra, can capture complex relationships across different wavelength. Further, we introduced a Mahalanobis distance-based loss function to address scale imbalance and interaction mode loss, and incorporated Monte Carlo dropout for quantitative analysis of prediction uncertainty.Trained and tested on 3,880 red giant spectra from LAMOST, the SPT achieved remarkable age
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#26426;&#21046;&#31232;&#30095;&#24615;&#27491;&#21017;&#21270;&#30340;&#35299;&#32544;&#21407;&#21017;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#28508;&#22312;&#22240;&#32032;&#21644;&#35299;&#37322;&#23427;&#20204;&#30340;&#31232;&#30095;&#22240;&#26524;&#22270;&#27169;&#22411;&#26469;&#23454;&#29616;&#35299;&#32544;&#12290;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#38750;&#21442;&#25968;&#21270;&#21487;&#36776;&#35782;&#24615;&#29702;&#35770;&#35777;&#26126;&#20102;&#36825;&#19968;&#21407;&#21017;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22270;&#24418;&#20934;&#21017;&#26469;&#20445;&#35777;&#23436;&#20840;&#35299;&#32544;&#12290;</title><link>http://arxiv.org/abs/2401.04890</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#21046;&#31232;&#30095;&#24615;&#36827;&#34892;&#38750;&#21442;&#25968;&#21270;&#37096;&#20998;&#35299;&#32544;: &#31232;&#30095;&#21160;&#20316;, &#24178;&#39044;&#21644;&#31232;&#30095;&#26102;&#38388;&#20381;&#36182;&#24615;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Partial Disentanglement via Mechanism Sparsity: Sparse Actions, Interventions and Sparse Temporal Dependencies. (arXiv:2401.04890v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#26426;&#21046;&#31232;&#30095;&#24615;&#27491;&#21017;&#21270;&#30340;&#35299;&#32544;&#21407;&#21017;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#28508;&#22312;&#22240;&#32032;&#21644;&#35299;&#37322;&#23427;&#20204;&#30340;&#31232;&#30095;&#22240;&#26524;&#22270;&#27169;&#22411;&#26469;&#23454;&#29616;&#35299;&#32544;&#12290;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#38750;&#21442;&#25968;&#21270;&#21487;&#36776;&#35782;&#24615;&#29702;&#35770;&#35777;&#26126;&#20102;&#36825;&#19968;&#21407;&#21017;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22270;&#24418;&#20934;&#21017;&#26469;&#20445;&#35777;&#23436;&#20840;&#35299;&#32544;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#35299;&#32544;&#21407;&#21017;&#65292;&#21363;&#26426;&#21046;&#31232;&#30095;&#35268;&#21017;&#65292;&#35813;&#35268;&#21017;&#36866;&#29992;&#20110;&#24863;&#20852;&#36259;&#30340;&#28508;&#22312;&#22240;&#32032;&#22312;&#35266;&#23519;&#36741;&#21161;&#21464;&#37327;&#21644;/&#25110;&#36807;&#21435;&#28508;&#22312;&#22240;&#32032;&#19978;&#31232;&#30095;&#20381;&#36182;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#28508;&#22312;&#22240;&#32032;&#21644;&#35299;&#37322;&#23427;&#20204;&#30340;&#31232;&#30095;&#22240;&#26524;&#22270;&#27169;&#22411;&#26469;&#24341;&#23548;&#35299;&#32544;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#21270;&#21487;&#36776;&#35782;&#24615;&#29702;&#35770;&#26469;&#24418;&#24335;&#21270;&#36825;&#19968;&#21407;&#21017;&#65292;&#24182;&#35777;&#26126;&#36890;&#36807;&#23558;&#23398;&#20064;&#21040;&#30340;&#22240;&#26524;&#22270;&#31232;&#30095;&#21270;&#65292;&#21487;&#20197;&#24674;&#22797;&#28508;&#22312;&#22240;&#32032;&#12290;&#26356;&#30830;&#20999;&#22320;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#31561;&#20215;&#20851;&#31995;"&#19968;&#33268;&#24615;"&#26469;&#25551;&#36848;&#33021;&#22815;&#20445;&#25345;&#19968;&#20123;&#28508;&#22312;&#22240;&#32032;&#32416;&#32544;&#30340;&#37096;&#20998;&#35299;&#32544;&#36807;&#31243;&#12290;&#20026;&#20102;&#25551;&#36848;&#32416;&#32544;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32416;&#32544;&#22270;&#21644;&#22270;&#20445;&#25345;&#20989;&#25968;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#22270;&#24418;&#20934;&#21017;&#65292;&#29992;&#20110;&#20445;&#35777;&#23436;&#20840;&#35299;&#32544;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces a novel principle for disentanglement we call mechanism sparsity regularization, which applies when the latent factors of interest depend sparsely on observed auxiliary variables and/or past latent factors. We propose a representation learning method that induces disentanglement by simultaneously learning the latent factors and the sparse causal graphical model that explains them. We develop a nonparametric identifiability theory that formalizes this principle and shows that the latent factors can be recovered by regularizing the learned causal graph to be sparse. More precisely, we show identifiablity up to a novel equivalence relation we call "consistency", which allows some latent factors to remain entangled (hence the term partial disentanglement). To describe the structure of this entanglement, we introduce the notions of entanglement graphs and graph preserving functions. We further provide a graphical criterion which guarantees complete disentanglement, that
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#32593;&#32476;&#26041;&#27861;&#21450;&#24212;&#29992;&#26159;&#23558;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#29305;&#24449;&#36830;&#25509;&#25104;&#22270;&#24418;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#20989;&#25968;&#25805;&#20316;&#29983;&#25104;&#26032;&#30340;&#29305;&#24449;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#22270;&#20687;&#22788;&#29702;&#21644;&#35745;&#31639;&#29983;&#29289;&#23398;&#20013;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2401.04874</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#32593;&#32476;&#26041;&#27861;&#21450;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Feature Network Methods in Machine Learning and Applications. (arXiv:2401.04874v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04874
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#32593;&#32476;&#26041;&#27861;&#21450;&#24212;&#29992;&#26159;&#23558;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#29305;&#24449;&#36830;&#25509;&#25104;&#22270;&#24418;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#20989;&#25968;&#25805;&#20316;&#29983;&#25104;&#26032;&#30340;&#29305;&#24449;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#22270;&#20687;&#22788;&#29702;&#21644;&#35745;&#31639;&#29983;&#29289;&#23398;&#20013;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#32593;&#32476;&#26159;&#19968;&#20010;&#23558;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#29305;&#24449;&#22522;&#20110;&#30456;&#20284;&#24615;&#36830;&#25509;&#36215;&#26469;&#30340;&#22270;&#24418;&#34920;&#31034;&#12290;&#36825;&#31181;&#32593;&#32476;&#34920;&#31034;&#20801;&#35768;&#25105;&#20204;&#23558;&#29305;&#24449;&#21521;&#37327;&#35270;&#20026;&#32593;&#32476;&#19978;&#30340;&#20989;&#25968;&#12290;&#36890;&#36807;&#21033;&#29992;&#20613;&#37324;&#21494;&#20998;&#26512;&#21644;&#20989;&#25968;&#20998;&#26512;&#20013;&#30340;&#20989;&#25968;&#25805;&#20316;&#65292;&#25105;&#20204;&#21487;&#20197;&#36731;&#26494;&#22320;&#29983;&#25104;&#26032;&#30340;&#29305;&#24449;&#65292;&#21033;&#29992;&#29305;&#24449;&#21521;&#37327;&#19978;&#25152;&#26045;&#21152;&#30340;&#22270;&#24418;&#32467;&#26500;&#12290;&#36825;&#26679;&#30340;&#32593;&#32476;&#32467;&#26500;&#22312;&#22270;&#20687;&#22788;&#29702;&#21644;&#35745;&#31639;&#29983;&#29289;&#23398;&#20013;&#24050;&#32463;&#34987;&#38544;&#24335;&#30740;&#31350;&#36807;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#29305;&#24449;&#32593;&#32476;&#25551;&#36848;&#20026;&#34987;&#26045;&#21152;&#22312;&#29305;&#24449;&#21521;&#37327;&#19978;&#30340;&#22270;&#24418;&#32467;&#26500;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#25552;&#20379;&#24212;&#29992;&#12290;&#20854;&#20013;&#19968;&#20010;&#24212;&#29992;&#28041;&#21450;&#22522;&#20110;&#22270;&#24418;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#24191;&#65292;&#28041;&#21450;&#20855;&#26377;&#19981;&#21516;&#28145;&#24230;&#25110;&#22797;&#26434;&#24230;&#30340;&#29305;&#24449;&#30340;&#23618;&#27425;&#21270;&#32467;&#26500;&#21270;&#28145;&#24230;&#23398;&#20064;&#12290;&#36825;&#36824;&#25193;&#23637;&#21040;&#33021;&#22815;&#29983;&#25104;&#26377;&#29992;&#30340;&#26032;&#30340;&#22810;&#23618;&#32423;&#29305;&#24449;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29305;&#24449;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A machine learning (ML) feature network is a graph that connects ML features in learning tasks based on their similarity. This network representation allows us to view feature vectors as functions on the network. By leveraging function operations from Fourier analysis and from functional analysis, one can easily generate new and novel features, making use of the graph structure imposed on the feature vectors. Such network structures have previously been studied implicitly in image processing and computational biology. We thus describe feature networks as graph structures imposed on feature vectors, and provide applications in machine learning. One application involves graph-based generalizations of convolutional neural networks, involving structured deep learning with hierarchical representations of features that have varying depth or complexity. This extends also to learning algorithms that are able to generate useful new multilevel features. Additionally, we discuss the use of featur
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#22270;&#36716;&#25442;&#22120;&#32467;&#26500;&#65292;&#36890;&#36807;&#35774;&#35745;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22359;&#26469;&#25552;&#39640;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#32771;&#34385;&#36328;&#25968;&#25454;&#38598;&#24207;&#21015;&#30340;&#39069;&#22806;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.04872</link><description>&lt;p&gt;
&#30693;&#35782;&#24863;&#30693;&#22270;&#36716;&#25442;&#22120;&#29992;&#20110;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Knowledge-aware Graph Transformer for Pedestrian Trajectory Prediction. (arXiv:2401.04872v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04872
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#22270;&#36716;&#25442;&#22120;&#32467;&#26500;&#65292;&#36890;&#36807;&#35774;&#35745;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22359;&#26469;&#25552;&#39640;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#32771;&#34385;&#36328;&#25968;&#25454;&#38598;&#24207;&#21015;&#30340;&#39069;&#22806;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#34892;&#20154;&#36816;&#21160;&#36712;&#36857;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#36335;&#24452;&#35268;&#21010;&#21644;&#36816;&#21160;&#25511;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#19981;&#21516;&#29615;&#22659;&#19979;&#20154;&#31867;&#36816;&#21160;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20934;&#30830;&#39044;&#27979;&#32676;&#20307;&#36712;&#36857;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#36712;&#36857;&#21382;&#21490;&#21644;&#34892;&#20154;&#20043;&#38388;&#30340;&#20132;&#20114;&#31561;&#20449;&#24687;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38480;&#21046;&#20102;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#24046;&#24322;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#22320;&#34701;&#21512;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#36716;&#25442;&#22120;&#32467;&#26500;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#25429;&#25417;&#25968;&#25454;&#38598;&#20013;&#19981;&#21516;&#22330;&#25152;&#21644;&#22330;&#26223;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22359;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#32771;&#34385;&#36328;&#25968;&#25454;&#38598;&#24207;&#21015;&#30340;&#39069;&#22806;&#25351;&#26631;&#20197;&#36827;&#34892;&#36712;&#36857;&#39044;&#27979;&#24615;&#33021;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting pedestrian motion trajectories is crucial for path planning and motion control of autonomous vehicles. Accurately forecasting crowd trajectories is challenging due to the uncertain nature of human motions in different environments. For training, recent deep learning-based prediction approaches mainly utilize information like trajectory history and interactions between pedestrians, among others. This can limit the prediction performance across various scenarios since the discrepancies between training datasets have not been properly incorporated. To overcome this limitation, this paper proposes a graph transformer structure to improve prediction performance, capturing the differences between the various sites and scenarios contained in the datasets. In particular, a self-attention mechanism and a domain adaption module have been designed to improve the generalization ability of the model. Moreover, an additional metric considering cross-dataset sequences is introduced for tra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#23884;&#20837;&#27169;&#22359;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#29992;&#25143;&#21382;&#21490;&#35760;&#24405;&#65292;&#24182;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.04858</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#35821;&#35328;&#25552;&#31034;&#30340;&#29992;&#25143;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
User Embedding Model for Personalized Language Prompting. (arXiv:2401.04858v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#23884;&#20837;&#27169;&#22359;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#29992;&#25143;&#21382;&#21490;&#35760;&#24405;&#65292;&#24182;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#30340;&#27169;&#22411;&#65292;&#24314;&#27169;&#38271;&#26102;&#38388;&#30340;&#21382;&#21490;&#35760;&#24405;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#33021;&#22815;&#25429;&#25417;&#29992;&#25143;&#19981;&#26029;&#28436;&#21464;&#30340;&#20559;&#22909;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#20934;&#30830;&#21644;&#20010;&#24615;&#21270;&#30340;&#25512;&#33616;&#12290;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#20559;&#22909;&#29702;&#35299;&#20013;&#24314;&#27169;&#38271;&#29992;&#25143;&#21382;&#21490;&#35760;&#24405;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#25143;&#23884;&#20837;&#27169;&#22359;(UEM)&#65292;&#36890;&#36807;&#23558;&#29992;&#25143;&#21382;&#21490;&#35760;&#24405;&#20197;&#23884;&#20837;&#24418;&#24335;&#21387;&#32553;&#21644;&#34920;&#31034;&#65292;&#23558;&#20854;&#20316;&#20026;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#36719;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25552;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#22788;&#29702;&#26174;&#33879;&#26356;&#38271;&#30340;&#21382;&#21490;&#35760;&#24405;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;&#35813;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#20102;&#20351;&#29992;&#34920;&#31034;&#20026;&#23884;&#20837;&#30340;&#29992;&#25143;&#20449;&#21495;&#26469;&#20559;&#32622;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling long histories plays a pivotal role in enhancing recommendation systems, allowing to capture user's evolving preferences, resulting in more precise and personalized recommendations. In this study we tackle the challenges of modeling long user histories for preference understanding in natural language. Specifically, we introduce a new User Embedding Module (UEM) that efficiently processes user history in free-form text by compressing and representing them as embeddings, to use them as soft prompts to a LM. Our experiments demonstrate the superior capability of this approach in handling significantly longer histories compared to conventional text based prompting methods, yielding substantial improvements in predictive performance. The main contribution of this research is to demonstrate the ability to bias language models with user signals represented as embeddings.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#21464;&#25442;&#30340;&#26032;&#22411;&#32479;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20132;&#36890;&#24066;&#22330;&#21033;&#29575;&#30340;&#39044;&#27979;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#23646;&#24615;&#21644;&#29305;&#24449;&#21464;&#25442;&#26680;&#20989;&#25968;&#65292;&#33021;&#22815;&#39640;&#25928;&#29983;&#25104;&#29305;&#24449;&#65292;&#24182;&#22312;&#39044;&#27979;&#36807;&#31243;&#20013;&#20934;&#30830;&#35782;&#21035;&#23395;&#33410;&#24615;&#21644;&#21046;&#24230;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2401.04857</link><description>&lt;p&gt;
&#20351;&#29992;&#29305;&#24449;&#21464;&#25442;&#36827;&#34892;&#20132;&#36890;&#24066;&#22330;&#21033;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transportation Market Rate Forecast Using Signature Transform. (arXiv:2401.04857v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#21464;&#25442;&#30340;&#26032;&#22411;&#32479;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20132;&#36890;&#24066;&#22330;&#21033;&#29575;&#30340;&#39044;&#27979;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#23646;&#24615;&#21644;&#29305;&#24449;&#21464;&#25442;&#26680;&#20989;&#25968;&#65292;&#33021;&#22815;&#39640;&#25928;&#29983;&#25104;&#29305;&#24449;&#65292;&#24182;&#22312;&#39044;&#27979;&#36807;&#31243;&#20013;&#20934;&#30830;&#35782;&#21035;&#23395;&#33410;&#24615;&#21644;&#21046;&#24230;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20122;&#39532;&#36874;&#22312;&#20132;&#36890;&#24066;&#22330;&#21033;&#29575;&#39044;&#27979;&#19978;&#20381;&#36182;&#31532;&#19977;&#26041;&#65292;&#23613;&#31649;&#36825;&#20123;&#39044;&#27979;&#36136;&#37327;&#24046;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#34429;&#28982;&#20132;&#36890;&#24066;&#22330;&#21033;&#29575;&#36890;&#24120;&#24456;&#38590;&#20934;&#30830;&#39044;&#27979;&#65292;&#20294;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#21464;&#25442;&#30340;&#26032;&#22411;&#32479;&#35745;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#39044;&#27979;&#21644;&#33258;&#36866;&#24212;&#27169;&#22411;&#26469;&#39044;&#27979;&#24066;&#22330;&#21033;&#29575;&#12290;&#36825;&#31181;&#26032;&#25216;&#26415;&#22522;&#20110;&#29305;&#24449;&#21464;&#25442;&#30340;&#20004;&#20010;&#20851;&#38190;&#23646;&#24615;&#12290;&#31532;&#19968;&#20010;&#26159;&#20854;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#65292;&#23427;&#32447;&#24615;&#21270;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#23558;&#39044;&#27979;&#38382;&#39064;&#36716;&#21270;&#20026;&#32447;&#24615;&#22238;&#24402;&#20998;&#26512;&#65307;&#31532;&#20108;&#20010;&#26159;&#29305;&#24449;&#21464;&#25442;&#26680;&#20989;&#25968;&#65292;&#23427;&#20801;&#35768;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#36827;&#34892;&#35745;&#31639;&#26377;&#25928;&#30340;&#30456;&#20284;&#24615;&#27604;&#36739;&#12290;&#32467;&#21512;&#36215;&#26469;&#65292;&#36825;&#20123;&#23646;&#24615;&#20801;&#35768;&#36827;&#34892;&#39640;&#25928;&#30340;&#29305;&#24449;&#29983;&#25104;&#65292;&#24182;&#22312;&#39044;&#27979;&#36807;&#31243;&#20013;&#26356;&#31934;&#30830;&#22320;&#35782;&#21035;&#23395;&#33410;&#24615;&#21644;&#21046;&#24230;&#36716;&#25442;&#12290;&#27169;&#22411;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26032;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#24066;&#22330;&#21033;&#29575;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, Amazon relies on third parties for transportation marketplace rate forecasts, despite the poor quality and lack of interpretability of these forecasts. While transportation marketplace rates are typically very challenging to forecast accurately, we have developed a novel signature-based statistical technique to address these challenges and built a predictive and adaptive model to forecast marketplace rates. This novel technique is based on two key properties of the signature transform. The first is its universal nonlinearity which linearizes the feature space and hence translates the forecasting problem into a linear regression analysis; the second is the signature kernel which allows for comparing computationally efficiently similarities between time series data. Combined, these properties allow for efficient feature generation and more precise identification of seasonality and regime switching in the forecasting process. Preliminary result by the model shows that this new 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21453;&#20363;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#35780;&#20998;&#20989;&#25968;&#23398;&#20064;&#33391;&#22909;&#65292;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGMs&#65289;&#20173;&#28982;&#26080;&#27861;&#29983;&#25104;&#25509;&#36817;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#21482;&#33021;&#20135;&#29983;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#39640;&#26031;&#27169;&#31946;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.04856</link><description>&lt;p&gt;
&#19968;&#20010;&#22909;&#30340;&#35780;&#20998;&#24182;&#19981;&#20250;&#23548;&#33268;&#19968;&#20010;&#22909;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Good Score Does not Lead to A Good Generative Model. (arXiv:2401.04856v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21453;&#20363;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#35780;&#20998;&#20989;&#25968;&#23398;&#20064;&#33391;&#22909;&#65292;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGMs&#65289;&#20173;&#28982;&#26080;&#27861;&#29983;&#25104;&#25509;&#36817;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#21482;&#33021;&#20135;&#29983;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#39640;&#26031;&#27169;&#31946;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGMs&#65289;&#26159;&#29983;&#25104;&#24314;&#27169;&#20013;&#30340;&#19968;&#31181;&#20027;&#35201;&#26041;&#27861;&#65292;&#20197;&#20854;&#33021;&#22815;&#20174;&#22797;&#26434;&#30340;&#39640;&#32500;&#25968;&#25454;&#20998;&#24067;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#32780;&#38395;&#21517;&#12290;&#35813;&#26041;&#27861;&#22312;&#32463;&#39564;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#24182;&#19988;&#26377;&#30528;&#20005;&#26684;&#30340;&#29702;&#35770;&#25910;&#25947;&#24615;&#36136;&#30340;&#25903;&#25345;&#12290;&#29305;&#21035;&#26159;&#24050;&#32463;&#35777;&#26126;&#65292;&#22914;&#26524;&#23398;&#20064;&#21040;&#30340;&#24213;&#23618;&#35780;&#20998;&#20989;&#25968;&#33391;&#22909;&#65292;SGMs&#33021;&#22815;&#29983;&#25104;&#25509;&#36817;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#36825;&#34920;&#26126;&#20102;SGM&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#25104;&#21151;&#20043;&#22788;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21453;&#20363;&#12290;&#36890;&#36807;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#35780;&#20998;&#20989;&#25968;&#23398;&#20064;&#24471;&#24456;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;SGMs&#21482;&#33021;&#36755;&#20986;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#39640;&#26031;&#27169;&#31946;&#26679;&#26412;&#65292;&#27169;&#25311;&#26680;&#23494;&#24230;&#20272;&#35745;&#30340;&#25928;&#26524;&#12290;&#36825;&#19968;&#21457;&#29616;&#19982;&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#21457;&#29616;&#30456;&#19968;&#33268;&#65292;&#25581;&#31034;&#20102;SGMs&#21487;&#33021;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#35760;&#24518;&#25928;&#24212;&#24182;&#19988;&#26080;&#27861;&#29983;&#25104;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based Generative Models (SGMs) is one leading method in generative modeling, renowned for their ability to generate high-quality samples from complex, high-dimensional data distributions. The method enjoys empirical success and is supported by rigorous theoretical convergence properties. In particular, it has been shown that SGMs can generate samples from a distribution that is close to the ground-truth if the underlying score function is learned well, suggesting the success of SGM as a generative model. We provide a counter-example in this paper. Through the sample complexity argument, we provide one specific setting where the score function is learned well. Yet, SGMs in this setting can only output samples that are Gaussian blurring of training data points, mimicking the effects of kernel density estimation. The finding resonates a series of recent finding that reveal that SGMs can demonstrate strong memorization effect and fail to generate.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#24863;&#30693;-&#34892;&#21160;-&#36890;&#20449;(LPAC)&#26550;&#26500;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#29615;&#22659;&#24863;&#30693;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#27969;&#65292;&#27973;&#23618;&#22810;&#23618;&#24863;&#30693;&#26426;&#35745;&#31639;&#26426;&#22120;&#20154;&#30340;&#21160;&#20316;&#12290;&#20351;&#29992;&#38598;&#20013;&#24335;&#26174;&#24494;&#31639;&#27861;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#32676;&#20307;&#30340;&#21327;&#20316;&#12290;</title><link>http://arxiv.org/abs/2401.04855</link><description>&lt;p&gt;
LPAC: &#21487;&#23398;&#20064;&#30340;&#24863;&#30693;-&#34892;&#21160;-&#36890;&#20449;&#24490;&#29615;&#21450;&#20854;&#22312;&#35206;&#30422;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
LPAC: Learnable Perception-Action-Communication Loops with Applications to Coverage Control. (arXiv:2401.04855v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04855
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#24863;&#30693;-&#34892;&#21160;-&#36890;&#20449;(LPAC)&#26550;&#26500;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#29615;&#22659;&#24863;&#30693;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#27969;&#65292;&#27973;&#23618;&#22810;&#23618;&#24863;&#30693;&#26426;&#35745;&#31639;&#26426;&#22120;&#20154;&#30340;&#21160;&#20316;&#12290;&#20351;&#29992;&#38598;&#20013;&#24335;&#26174;&#24494;&#31639;&#27861;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#32676;&#20307;&#30340;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35206;&#30422;&#25511;&#21046;&#26159;&#25351;&#23548;&#26426;&#22120;&#20154;&#32676;&#20307;&#21327;&#21516;&#30417;&#27979;&#26410;&#30693;&#30340;&#24863;&#20852;&#36259;&#29305;&#24449;&#25110;&#29616;&#35937;&#30340;&#38382;&#39064;&#12290;&#22312;&#26377;&#38480;&#30340;&#36890;&#20449;&#21644;&#24863;&#30693;&#33021;&#21147;&#30340;&#20998;&#25955;&#35774;&#32622;&#20013;&#65292;&#36825;&#20010;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#24863;&#30693;-&#34892;&#21160;-&#36890;&#20449;(LPAC)&#26550;&#26500;&#26469;&#35299;&#20915;&#35206;&#30422;&#25511;&#21046;&#38382;&#39064;&#12290;&#22312;&#35813;&#35299;&#20915;&#26041;&#26696;&#20013;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#22788;&#29702;&#20102;&#29615;&#22659;&#30340;&#23616;&#37096;&#24863;&#30693;&#65307;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#23454;&#29616;&#20102;&#37051;&#36817;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#30456;&#20851;&#20449;&#24687;&#36890;&#20449;&#65307;&#26368;&#21518;&#65292;&#27973;&#23618;&#22810;&#23618;&#24863;&#30693;&#26426;(MLP)&#35745;&#31639;&#26426;&#22120;&#20154;&#30340;&#21160;&#20316;&#12290;&#36890;&#20449;&#27169;&#22359;&#20013;&#30340;GNN&#36890;&#36807;&#35745;&#31639;&#24212;&#35813;&#19982;&#37051;&#23621;&#36890;&#20449;&#21738;&#20123;&#20449;&#24687;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#37319;&#21462;&#36866;&#24403;&#30340;&#34892;&#21160;&#26469;&#23454;&#29616;&#26426;&#22120;&#20154;&#32676;&#20307;&#30340;&#21327;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#30693;&#26195;&#25972;&#20010;&#29615;&#22659;&#30340;&#38598;&#20013;&#24335;&#26174;&#24494;&#31639;&#27861;&#26469;&#36827;&#34892;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coverage control is the problem of navigating a robot swarm to collaboratively monitor features or a phenomenon of interest not known a priori. The problem is challenging in decentralized settings with robots that have limited communication and sensing capabilities. This paper proposes a learnable Perception-Action-Communication (LPAC) architecture for the coverage control problem. In the proposed solution, a convolution neural network (CNN) processes localized perception of the environment; a graph neural network (GNN) enables communication of relevant information between neighboring robots; finally, a shallow multi-layer perceptron (MLP) computes robot actions. The GNN in the communication module enables collaboration in the robot swarm by computing what information to communicate with neighbors and how to use received information to take appropriate actions. We train models using imitation learning with a centralized clairvoyant algorithm that is aware of the entire environment. Eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#35268;&#21010;&#22478;&#24066;&#31354;&#20013;&#31227;&#21160;&#26426;&#38431;&#30340;&#26102;&#38388;&#34920;&#21644;&#30446;&#30340;&#22320;&#65292;&#32771;&#34385;&#21040;&#25805;&#20316;&#38480;&#21046;&#12289;&#38656;&#27714;&#21464;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#31561;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#26032;&#30340;&#31574;&#30053;&#26550;&#26500;&#21644;&#20351;&#29992;&#22270;&#33014;&#22218;&#36716;&#25442;&#32593;&#32476;&#12289;&#36716;&#25442;&#22120;&#23618;&#21644;Multi-head Attention-based&#35299;&#30721;&#22120;&#31561;&#32452;&#20214;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#29616;&#26377;&#26426;&#38431;&#35268;&#21010;&#23454;&#26045;&#20013;&#23384;&#22312;&#30340;&#22797;&#26434;&#24615;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#29616;&#23454;&#24615;&#21644;&#36924;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.04851</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#22478;&#24066;&#31354;&#20013;&#31227;&#21160;&#26426;&#38431;&#35843;&#24230;&#22312;&#25805;&#20316;&#38480;&#21046;&#12289;&#38656;&#27714;&#21464;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Graph Learning-based Fleet Scheduling for Urban Air Mobility under Operational Constraints, Varying Demand &amp; Uncertainties. (arXiv:2401.04851v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#35268;&#21010;&#22478;&#24066;&#31354;&#20013;&#31227;&#21160;&#26426;&#38431;&#30340;&#26102;&#38388;&#34920;&#21644;&#30446;&#30340;&#22320;&#65292;&#32771;&#34385;&#21040;&#25805;&#20316;&#38480;&#21046;&#12289;&#38656;&#27714;&#21464;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#31561;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#26032;&#30340;&#31574;&#30053;&#26550;&#26500;&#21644;&#20351;&#29992;&#22270;&#33014;&#22218;&#36716;&#25442;&#32593;&#32476;&#12289;&#36716;&#25442;&#22120;&#23618;&#21644;Multi-head Attention-based&#35299;&#30721;&#22120;&#31561;&#32452;&#20214;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#20915;&#29616;&#26377;&#26426;&#38431;&#35268;&#21010;&#23454;&#26045;&#20013;&#23384;&#22312;&#30340;&#22797;&#26434;&#24615;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#29616;&#23454;&#24615;&#21644;&#36924;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;,&#29992;&#20110;&#22312;&#32447;&#35268;&#21010;&#30005;&#21160;&#39134;&#34892;&#22120;&#30340;&#26102;&#38388;&#34920;&#21644;&#30446;&#30340;&#22320;,&#35813;&#39134;&#34892;&#22120;&#32452;&#25104;&#20102;&#19968;&#20010;&#36328;&#22810;&#20010;&#22402;&#30452;&#28207;&#21475;&#36816;&#33829;&#30340;&#22478;&#24066;&#31354;&#20013;&#31227;&#21160;&#65288;UAM&#65289;&#26426;&#38431;&#12290;&#36825;&#20010;&#26426;&#38431;&#35843;&#24230;&#38382;&#39064;&#30340;&#21046;&#23450;&#32771;&#34385;&#20102;&#26102;&#38388;&#21464;&#21270;&#30340;&#38656;&#27714;&#12289;&#22402;&#30452;&#28207;&#21475;&#23481;&#37327;&#12289;&#39134;&#34892;&#22120;&#23481;&#37327;&#21644;&#31354;&#22495;&#23433;&#20840;&#20934;&#21017;&#30340;&#32422;&#26463;&#65292;&#20197;&#21450;&#36215;&#39134;&#24310;&#35823;&#12289;&#22825;&#27668;&#24341;&#36215;&#30340;&#36335;&#32447;&#20851;&#38381;&#21644;&#24773;&#20917;&#26410;&#30693;&#30340;&#39134;&#34892;&#22120;&#20572;&#26426;&#26102;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#26679;&#30340;&#21046;&#23450;&#26041;&#24335;&#27604;&#29616;&#26377;&#30340;UAM&#26426;&#38431;&#35268;&#21010;&#23454;&#26045;&#26356;&#21152;&#22797;&#26434;&#65292;&#21487;&#33021;&#22686;&#21152;&#20102;&#26356;&#22810;&#30340;&#29616;&#23454;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#22797;&#26434;&#24615;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#31574;&#30053;&#26550;&#26500;&#65292;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#21253;&#25324;&#65306;&#20316;&#20026;&#22270;&#30340;&#25277;&#35937;&#65292;&#29992;&#20110;&#32534;&#30721;&#22402;&#30452;&#28207;&#21475;&#21644;&#39134;&#34892;&#22120;&#26426;&#38431;&#29366;&#24577;&#30340;&#22270;&#33014;&#22218;&#36716;&#25442;&#32593;&#32476;&#65307;&#32534;&#30721;&#38656;&#27714;&#21644;&#20056;&#23458;&#31080;&#20215;&#30340;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#30340;&#36716;&#25442;&#22120;&#23618;&#65307;&#20197;&#21450;&#20351;&#29992;&#32534;&#30721;&#22120;&#30340;Multi-head Attention-based&#35299;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper develops a graph reinforcement learning approach to online planning of the schedule and destinations of electric aircraft that comprise an urban air mobility (UAM) fleet operating across multiple vertiports. This fleet scheduling problem is formulated to consider time-varying demand, constraints related to vertiport capacity, aircraft capacity and airspace safety guidelines, uncertainties related to take-off delay, weather-induced route closures, and unanticipated aircraft downtime. Collectively, such a formulation presents greater complexity, and potentially increased realism, than in existing UAM fleet planning implementations. To address these complexities, a new policy architecture is constructed, primary components of which include: graph capsule conv-nets for encoding vertiport and aircraft-fleet states both abstracted as graphs; transformer layers encoding time series information on demand and passenger fare; and a Multi-head Attention-based decoder that uses the enco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;&#24191;&#20041;&#31561;&#22686;&#36882;&#24402;&#20998;&#21106;&#31639;&#27861;&#65288;GIRP&#65289;&#65292;&#22312;&#21487;&#20998;&#31163;&#20984;&#25439;&#22833;&#21644;&#19981;&#21487;&#24494;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;&#20102;&#31561;&#22686;&#22238;&#24402;&#38382;&#39064;&#30340;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36882;&#24402;&#20108;&#20998;&#20998;&#21106;&#30340;&#26041;&#27861;&#26469;&#25214;&#21040;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.04847</link><description>&lt;p&gt;
&#20851;&#20110;&#24191;&#20041;&#31561;&#22686;&#36882;&#24402;&#20998;&#21106;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Correctness of the Generalized Isotonic Recursive Partitioning Algorithm. (arXiv:2401.04847v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;&#24191;&#20041;&#31561;&#22686;&#36882;&#24402;&#20998;&#21106;&#31639;&#27861;&#65288;GIRP&#65289;&#65292;&#22312;&#21487;&#20998;&#31163;&#20984;&#25439;&#22833;&#21644;&#19981;&#21487;&#24494;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;&#20102;&#31561;&#22686;&#22238;&#24402;&#38382;&#39064;&#30340;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36882;&#24402;&#20108;&#20998;&#20998;&#21106;&#30340;&#26041;&#27861;&#26469;&#25214;&#21040;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#24191;&#20041;&#31561;&#22686;&#36882;&#24402;&#20998;&#21106;&#31639;&#27861;&#65288;GIRP&#65289;&#65292;&#35813;&#31639;&#27861;&#29992;&#20110;&#25311;&#21512;&#21487;&#20998;&#31163;&#20984;&#25439;&#22833;&#19979;&#30340;&#31561;&#22686;&#27169;&#22411;&#65292;&#35813;&#31639;&#27861;&#30001;Luss&#21644;Rosset&#25552;&#20986; [J. Comput. Graph. Statist., 23 (2014), pp. 192--201] &#24182;&#30001;Painsky&#21644;Rosset [IEEE Trans. Pattern Anal. Mach. Intell., 38 (2016), pp. 308-321] &#25193;&#23637;&#36866;&#29992;&#20110;&#19981;&#21487;&#24494;&#25439;&#22833;&#12290;GIRP&#31639;&#27861;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#29305;&#28857;&#65292;&#21363;&#22312;&#31639;&#27861;&#30340;&#27599;&#19968;&#27493;&#20013;&#65292;&#20013;&#38388;&#35299;&#28385;&#36275;&#31561;&#22686;&#32422;&#26463;&#12290;&#25991;&#31456;&#20197;&#19968;&#20010;&#20363;&#23376;&#24320;&#22987;&#65292;&#23637;&#31034;&#20102;&#25991;&#29486;&#20013;&#25551;&#36848;&#30340;GIRP&#31639;&#27861;&#21487;&#33021;&#26080;&#27861;&#20135;&#29983;&#31561;&#22686;&#27169;&#22411;&#30340;&#24773;&#20917;&#65292;&#34920;&#26126;&#24517;&#39035;&#20180;&#32454;&#35752;&#35770;&#31561;&#22686;&#22238;&#24402;&#38382;&#39064;&#30340;&#35299;&#30340;&#23384;&#22312;&#24615;&#21644;&#21807;&#19968;&#24615;&#12290;&#25991;&#31456;&#25509;&#30528;&#23637;&#31034;&#65292;&#21487;&#33021;&#23384;&#22312;&#35768;&#22810;&#35299;&#20043;&#19968;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#35266;&#23519;&#25968;&#25454;&#38598;&#36827;&#34892;&#36882;&#24402;&#20108;&#20998;&#20998;&#21106;&#26469;&#25214;&#21040;&#35299;&#12290;&#19968;&#20010;&#23567;&#30340;&#20462;&#25913;
&lt;/p&gt;
&lt;p&gt;
This paper presents an in-depth analysis of the generalized isotonic recursive partitioning (GIRP) algorithm for fitting isotonic models under separable convex losses, proposed by Luss and Rosset [J. Comput. Graph. Statist., 23 (2014), pp. 192--201] for differentiable losses and extended by Painsky and Rosset [IEEE Trans. Pattern Anal. Mach. Intell., 38 (2016), pp. 308-321] for nondifferentiable losses. The GIRP algorithm poseses an attractive feature that in each step of the algorithm, the intermediate solution satisfies the isotonicity constraint. The paper begins with an example showing that the GIRP algorithm as described in the literature may fail to produce an isotonic model, suggesting that the existence and uniqueness of the solution to the isotonic regression problem must be carefully addressed. It proceeds with showing that, among possibly many solutions, there indeed exists a solution that can be found by recursive binary partitioning of the set of observed data. A small mod
&lt;/p&gt;</description></item><item><title>T-PRIME&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#21327;&#35758;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#23398;&#20064;&#20256;&#36755;&#24103;&#30340;&#32467;&#26500;&#35774;&#35745;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#28145;&#24230;&#23398;&#20064;&#30828;&#20214;&#38480;&#21046;&#19979;&#30340;&#23454;&#26102;&#21487;&#34892;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2401.04837</link><description>&lt;p&gt;
T-PRIME: &#22522;&#20110;Transformer&#30340;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#21327;&#35758;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
T-PRIME: Transformer-based Protocol Identification for Machine-learning at the Edge. (arXiv:2401.04837v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04837
&lt;/p&gt;
&lt;p&gt;
T-PRIME&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#36793;&#32536;&#26426;&#22120;&#23398;&#20064;&#21327;&#35758;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#23398;&#20064;&#20256;&#36755;&#24103;&#30340;&#32467;&#26500;&#35774;&#35745;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#28145;&#24230;&#23398;&#20064;&#30828;&#20214;&#38480;&#21046;&#19979;&#30340;&#23454;&#26102;&#21487;&#34892;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39057;&#35889;&#20849;&#20139;&#20801;&#35768;&#30456;&#21516;&#26631;&#20934;&#65288;&#20363;&#22914;802.11&#31995;&#21015;&#65289;&#25110;&#19981;&#21516;&#26631;&#20934;&#65288;&#20363;&#22914;LTE&#21644;DVB&#65289;&#30340;&#19981;&#21516;&#21327;&#35758;&#22312;&#37325;&#21472;&#30340;&#39057;&#27573;&#20013;&#20849;&#23384;&#12290;&#38543;&#30528;&#36825;&#31181;&#33539;&#24335;&#30340;&#25512;&#24191;&#65292;&#26080;&#32447;&#31995;&#32479;&#24517;&#39035;&#22312;&#25925;&#24847;&#25439;&#22351;&#21069;&#23548;&#30721;&#12289;&#26497;&#20302;&#20449;&#22122;&#27604;&#21644;&#25361;&#25112;&#24615;&#30340;&#20449;&#36947;&#26465;&#20214;&#19979;&#23454;&#26102;&#35782;&#21035;&#27963;&#21160;&#21457;&#23556;&#22120;&#21644;&#26410;&#32463;&#25480;&#26435;&#30340;&#27874;&#24418;&#12290;&#36890;&#36807;&#35774;&#35745;T-PRIME&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#21069;&#23548;&#30721;&#21305;&#37197;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;T-PRIME&#36890;&#36807;&#20854;&#27880;&#24847;&#26426;&#21046;&#23398;&#20064;&#20256;&#36755;&#24103;&#30340;&#32467;&#26500;&#35774;&#35745;&#65292;&#26597;&#30475;&#36229;&#20986;&#21069;&#23548;&#30721;&#30340;&#24207;&#21015;&#27169;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#23545;&#27604;Transformer&#27169;&#22411;&#24182;&#35777;&#26126;&#20854;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20854;&#27425;&#65292;&#20005;&#26684;&#20998;&#26512;&#20102;T-PRIME&#22312;&#28145;&#24230;&#23398;&#20064;&#30828;&#20214;&#38480;&#21046;&#19979;&#30340;&#23454;&#26102;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectrum sharing allows different protocols of the same standard (e.g., 802.11 family) or different standards (e.g., LTE and DVB) to coexist in overlapping frequency bands. As this paradigm continues to spread, wireless systems must also evolve to identify active transmitters and unauthorized waveforms in real time under intentional distortion of preambles, extremely low signal-to-noise ratios and challenging channel conditions. We overcome limitations of correlation-based preamble matching methods in such conditions through the design of T-PRIME: a Transformer-based machine learning approach. T-PRIME learns the structural design of transmitted frames through its attention mechanism, looking at sequence patterns that go beyond the preamble alone. The paper makes three contributions: First, it compares Transformer models and demonstrates their superiority over traditional methods and state-of-the-art neural networks. Second, it rigorously analyzes T-PRIME's real-time feasibility on Deep
&lt;/p&gt;</description></item><item><title>GNNShap&#26159;&#19968;&#31181;&#20351;&#29992;Shapley&#20540;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#33021;&#22815;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;GNNShap&#36890;&#36807;&#25277;&#26679;&#12289;&#24182;&#34892;&#21270;&#35745;&#31639;&#31561;&#25216;&#26415;&#25552;&#39640;&#20102;&#35299;&#37322;&#36895;&#24230;&#21644;&#31934;&#32454;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.04829</link><description>&lt;p&gt;
GNNShap: &#20351;&#29992;Shapley&#20540;&#24555;&#36895;&#32780;&#20934;&#30830;&#35299;&#37322;GNN&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
GNNShap: Fast and Accurate GNN Explanations using Shapley Values. (arXiv:2401.04829v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04829
&lt;/p&gt;
&lt;p&gt;
GNNShap&#26159;&#19968;&#31181;&#20351;&#29992;Shapley&#20540;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#33021;&#22815;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;GNNShap&#36890;&#36807;&#25277;&#26679;&#12289;&#24182;&#34892;&#21270;&#35745;&#31639;&#31561;&#25216;&#26415;&#25552;&#39640;&#20102;&#35299;&#37322;&#36895;&#24230;&#21644;&#31934;&#32454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#26159;&#19968;&#31181;&#22312;&#31185;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;GNN&#34987;&#35748;&#20026;&#26159;&#40657;&#30418;&#27169;&#22411;&#65292;&#24456;&#38590;&#29702;&#35299;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#39044;&#27979;&#12290;&#22522;&#20110;&#21338;&#24328;&#35770;&#30340;Shapley&#20540;&#26041;&#27861;&#22312;&#20854;&#20182;&#39046;&#22495;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#37322;&#27169;&#22411;&#65292;&#20294;&#22312;&#22270;&#39046;&#22495;&#20013;&#30740;&#31350;&#36739;&#23569;&#12290;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#22522;&#20110;Shapley&#20540;&#30340;GNN&#35299;&#37322;&#26041;&#27861;&#65292;&#28982;&#32780;&#23427;&#20204;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#23427;&#20204;&#21482;&#32771;&#34385;&#20102;&#26377;&#38480;&#30340;&#26679;&#26412;&#26469;&#36817;&#20284;Shapley&#20540;&#65307;&#26377;&#20123;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#23567;&#21644;&#22823;&#30340;&#32852;&#30431;&#22823;&#23567;&#65292;&#24182;&#19988;&#23427;&#20204;&#27604;&#20854;&#20182;&#35299;&#37322;&#26041;&#27861;&#24930;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#20013;&#31561;&#35268;&#27169;&#30340;&#22270;&#20013;&#26080;&#27861;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GNNShap&#65292;&#23427;&#25552;&#20379;&#36793;&#30340;&#35299;&#37322;&#65292;&#22240;&#20026;&#23427;&#20204;&#23545;&#22270;&#25552;&#20379;&#20102;&#26356;&#33258;&#28982;&#21644;&#31934;&#32454;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#25152;&#26377;&#32852;&#30431;&#22823;&#23567;&#36827;&#34892;&#25277;&#26679;&#12289;&#22312;GPU&#19978;&#24182;&#34892;&#25277;&#26679;&#21644;&#21152;&#36895;&#27169;&#22411;&#31561;&#26041;&#38754;&#20811;&#26381;&#20102;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are popular machine learning models for graphs with many applications across scientific domains. However, GNNs are considered black box models, and it is challenging to understand how the model makes predictions. Game theory-based Shapley value approaches are popular explanation methods in other domains but are not well-studied for graphs. Some studies have proposed Shapley value-based GNN explanations, yet they have several limitations: they consider limited samples to approximate Shapley values; some mainly focus on small and large coalition sizes, and they are an order of magnitude slower than other explanation methods, making them inapplicable to even moderate-size graphs. In this work, we propose GNNShap, which provides explanations for edges since they provide more natural explanations for graphs and more fine-grained explanations. We overcome the limitations by sampling from all coalition sizes, parallelizing the sampling on GPUs, and speeding up mod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;Agent-Based&#27169;&#22411;&#27169;&#25311;&#20102;&#33647;&#29289;&#12289;&#34892;&#20026;&#21644;&#25968;&#23383;&#24178;&#39044;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#24314;&#35758;&#32508;&#21512;&#36816;&#29992;&#36825;&#20123;&#24178;&#39044;&#25514;&#26045;&#24212;&#23545;&#22823;&#27969;&#34892;&#30123;&#24773;&#12290;&#36890;&#36807;&#20998;&#26512;&#21457;&#29616;&#65292;&#26368;&#21021;&#30340;100&#22825;&#23545;&#20915;&#23450;&#30123;&#24773;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#24378;&#35843;&#20102;&#36805;&#36895;&#20915;&#31574;&#21644;&#39640;&#25928;&#25919;&#31574;&#21046;&#23450;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04795</link><description>&lt;p&gt;
&#22823;&#27969;&#34892;&#30340;&#21069;100&#22825;&#65307;&#33647;&#29289;&#12289;&#34892;&#20026;&#21644;&#25968;&#23383;&#24178;&#39044;&#30340;&#30456;&#20114;&#20316;&#29992;&#8212;&#8212;&#22522;&#20110;Agent-Based&#27169;&#22411;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
First 100 days of pandemic; an interplay of pharmaceutical, behavioral and digital interventions -- A study using agent based modeling. (arXiv:2401.04795v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;Agent-Based&#27169;&#22411;&#27169;&#25311;&#20102;&#33647;&#29289;&#12289;&#34892;&#20026;&#21644;&#25968;&#23383;&#24178;&#39044;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#24314;&#35758;&#32508;&#21512;&#36816;&#29992;&#36825;&#20123;&#24178;&#39044;&#25514;&#26045;&#24212;&#23545;&#22823;&#27969;&#34892;&#30123;&#24773;&#12290;&#36890;&#36807;&#20998;&#26512;&#21457;&#29616;&#65292;&#26368;&#21021;&#30340;100&#22825;&#23545;&#20915;&#23450;&#30123;&#24773;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#24378;&#35843;&#20102;&#36805;&#36895;&#20915;&#31574;&#21644;&#39640;&#25928;&#25919;&#31574;&#21046;&#23450;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#27969;&#34892;&#65292;&#29305;&#21035;&#26159;&#26368;&#36817;&#30340;COVID-19&#30123;&#24773;&#29190;&#21457;&#65292;&#23545;&#20844;&#20849;&#21355;&#29983;&#21644;&#20840;&#29699;&#32463;&#27982;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#30142;&#30149;&#21457;&#23637;&#36235;&#21183;&#21644;&#39640;&#25928;&#30340;&#24212;&#23545;&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#28508;&#22312;&#30340;&#26410;&#26469;&#30123;&#24773;&#29190;&#21457;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#20195;&#29702;&#20154;&#27169;&#22411;&#65288;Agent-Based Models&#65292;ABM&#65289;&#22312;&#25429;&#25417;&#22797;&#26434;&#30340;&#24863;&#26579;&#21160;&#24577;&#21644;&#29702;&#35299;&#24178;&#39044;&#25514;&#26045;&#24433;&#21709;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#27169;&#25311;&#20102;&#21453;&#26144;&#29616;&#23454;&#25919;&#31574;&#37319;&#32435;&#20013;&#30340;&#25361;&#25112;&#30340;&#30495;&#23454;&#33647;&#29289;&#12289;&#34892;&#20026;&#21644;&#25968;&#23383;&#24178;&#39044;&#65292;&#24182;&#25552;&#20986;&#20102;&#36825;&#20123;&#24178;&#39044;&#30340;&#25972;&#20307;&#32452;&#21512;&#29992;&#20110;&#22823;&#27969;&#34892;&#30123;&#24773;&#24212;&#23545;&#12290;&#36890;&#36807;&#36825;&#20123;&#27169;&#25311;&#65292;&#25105;&#20204;&#26681;&#25454;&#21326;&#30427;&#39039;&#24030;&#37329;&#26031;&#21439;&#30340;&#30495;&#23454;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#21644;&#22320;&#29702;&#26222;&#26597;&#25968;&#25454;&#65292;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#20154;&#21475;&#30340;&#26032;&#20852;&#34892;&#20026;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#26368;&#21021;100&#22825;&#22312;&#20915;&#23450;&#22823;&#27969;&#34892;&#30123;&#24773;&#36208;&#21183;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24378;&#35843;&#20102;&#24555;&#36895;&#20915;&#31574;&#21644;&#39640;&#25928;&#25919;&#31574;&#21046;&#23450;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pandemics, notably the recent COVID-19 outbreak, have impacted both public health and the global economy. A profound understanding of disease progression and efficient response strategies is thus needed to prepare for potential future outbreaks. In this paper, we emphasize the potential of Agent-Based Models (ABM) in capturing complex infection dynamics and understanding the impact of interventions. We simulate realistic pharmaceutical, behavioral, and digital interventions that mirror challenges in real-world policy adoption and suggest a holistic combination of these interventions for pandemic response. Using these simulations, we study the trends of emergent behavior on a large-scale population based on real-world socio-demographic and geo-census data from Kings County in Washington. Our analysis reveals the pivotal role of the initial 100 days in dictating a pandemic's course, emphasizing the importance of quick decision-making and efficient policy development. Further, we highligh
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#21452;&#26354;&#32447;&#38381;&#21253;&#27169;&#22411;&#65292;&#29992;&#20110;BGK&#21160;&#21147;&#27169;&#22411;&#30340;Grad&#30697;&#23637;&#24320;&#65292;&#20197;&#23454;&#29616;&#26368;&#39640;&#30697;&#30340;&#26799;&#24230;&#30340;&#31934;&#30830;&#38381;&#21512;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.04783</link><description>&lt;p&gt;
BGK&#26041;&#31243;&#30340;&#21452;&#26354;&#32447;&#26426;&#22120;&#23398;&#20064;&#30697;&#38381;&#21253;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Machine Learning Moment Closures for the BGK Equations. (arXiv:2401.04783v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04783
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#21452;&#26354;&#32447;&#38381;&#21253;&#27169;&#22411;&#65292;&#29992;&#20110;BGK&#21160;&#21147;&#27169;&#22411;&#30340;Grad&#30697;&#23637;&#24320;&#65292;&#20197;&#23454;&#29616;&#26368;&#39640;&#30697;&#30340;&#26799;&#24230;&#30340;&#31934;&#30830;&#38381;&#21512;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#22312;BGK&#21160;&#21147;&#27169;&#22411;&#30340;&#30697;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24341;&#20837;&#20102;&#23545;Bhatnagar-Gross-Krook&#65288;BGK&#65289;&#21160;&#21147;&#27169;&#22411;&#30340;Grad&#30697;&#23637;&#24320;&#30340;&#21452;&#26354;&#32447;&#38381;&#21253;&#12290;&#36825;&#20010;&#38381;&#21253;&#26159;&#22522;&#20110;&#25105;&#20204;&#22312;&#36755;&#36816;&#23553;&#38381;&#20013;&#23548;&#20986;&#30340;&#33258;&#30001;&#27969;&#26497;&#38480;&#30340;&#31934;&#30830;&#23553;&#38381;&#20851;&#31995;&#32780;&#25552;&#20986;&#30340;&#12290;&#36825;&#20010;&#31934;&#30830;&#23553;&#38381;&#20851;&#31995;&#23558;&#26368;&#39640;&#30697;&#30340;&#26799;&#24230;&#19982;&#22235;&#20010;&#36739;&#20302;&#30697;&#30340;&#26799;&#24230;&#30456;&#20851;&#32852;&#12290;&#19982;&#25105;&#20204;&#36807;&#21435;&#30340;&#24037;&#20316;&#19968;&#26679;&#65292;&#36825;&#37324;&#20171;&#32461;&#30340;&#27169;&#22411;&#36890;&#36807;&#36739;&#20302;&#30697;&#30340;&#26799;&#24230;&#31995;&#25968;&#26469;&#23398;&#20064;&#26368;&#39640;&#30697;&#30340;&#26799;&#24230;&#12290;&#36825;&#24847;&#21619;&#30528;&#24471;&#21040;&#30340;&#21452;&#26354;&#31995;&#32479;&#22312;&#26368;&#39640;&#30697;&#19978;&#24182;&#38750;&#23432;&#24658;&#12290;&#20026;&#20102;&#31283;&#23450;&#24615;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#23618;&#34987;&#35774;&#35745;&#25104;&#24378;&#21046;&#21452;&#26354;&#24615;&#21644;Galileo&#19981;&#21464;&#24615;&#12290;&#36825;&#30830;&#20445;&#27169;&#22411;&#33021;&#22815;&#22312;NN&#30340;&#35757;&#32451;&#31383;&#21475;&#20043;&#22806;&#36816;&#34892;&#12290;&#19982;&#25105;&#20204;&#20197;&#21069;&#22788;&#29702;&#32447;&#24615;&#27169;&#22411;&#30340;&#36752;&#23556;&#36755;&#36816;&#24037;&#20316;&#19981;&#21516;&#65292;BGK&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#24615;&#35201;&#27714;&#26356;&#39640;&#32423;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a hyperbolic closure for the Grad moment expansion of the Bhatnagar-Gross-Krook's (BGK) kinetic model using a neural network (NN) trained on BGK's moment data. This closure is motivated by the exact closure for the free streaming limit that we derived in our paper on closures in transport \cite{Huang2022-RTE1}. The exact closure relates the gradient of the highest moment to the gradient of four lower moments. As with our past work, the model presented here learns the gradient of the highest moment in terms of the coefficients of gradients for all lower ones. By necessity, this means that the resulting hyperbolic system is not conservative in the highest moment. For stability, the output layers of the NN are designed to enforce hyperbolicity and Galilean invariance. This ensures the model can be run outside of the training window of the NN. Unlike our previous work on radiation transport that dealt with linear models, the BGK model's nonlinearity demanded advanced training 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#29305;&#24449;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#26222;&#36866;&#19988;&#26080;&#38656;&#20551;&#35774;&#30340;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#12290;&#30740;&#31350;&#22522;&#20110;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#24230;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#20851;&#36924;&#36817;&#36136;&#37327;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.04778</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#29305;&#24449;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Generative neural networks for characteristic functions. (arXiv:2401.04778v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#29305;&#24449;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#26222;&#36866;&#19988;&#26080;&#38656;&#20551;&#35774;&#30340;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#12290;&#30740;&#31350;&#22522;&#20110;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#24230;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#20851;&#36924;&#36817;&#36136;&#37327;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#31639;&#27861;&#26469;&#20174;&#19968;&#20010;&#65288;&#22810;&#20803;&#65289;&#29305;&#24449;&#20989;&#25968;&#20013;&#27169;&#25311;&#65292;&#35813;&#29305;&#24449;&#20989;&#25968;&#20165;&#20197;&#40657;&#30418;&#26684;&#24335;&#21487;&#35775;&#38382;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#25439;&#22833;&#20989;&#25968;&#21033;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#24230;&#37327;&#30340;&#29305;&#23450;&#34920;&#31034;&#65292;&#30452;&#25509;&#32467;&#21512;&#30446;&#26631;&#29305;&#24449;&#20989;&#25968;&#12290;&#36825;&#31181;&#26500;&#36896;&#20855;&#26377;&#26222;&#36941;&#24615;&#65292;&#19981;&#20381;&#36182;&#20110;&#32500;&#24230;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#23545;&#32473;&#23450;&#29305;&#24449;&#20989;&#25968;&#36827;&#34892;&#20219;&#20309;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#36824;&#24471;&#20986;&#20102;&#20851;&#20110;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#24230;&#37327;&#30340;&#36924;&#36817;&#36136;&#37327;&#30340;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#30701;&#26399;&#27169;&#25311;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we provide a simulation algorithm to simulate from a (multivariate) characteristic function, which is only accessible in a black-box format. We construct a generative neural network, whose loss function exploits a specific representation of the Maximum-Mean-Discrepancy metric to directly incorporate the targeted characteristic function. The construction is universal in the sense that it is independent of the dimension and that it does not require any assumptions on the given characteristic function. Furthermore, finite sample guarantees on the approximation quality in terms of the Maximum-Mean Discrepancy metric are derived. The method is illustrated in a short simulation study.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#21313;&#19968;&#20010;&#26368;&#36817;&#30340;&#27169;&#22411;&#26550;&#26500;&#22312;&#20116;&#20010;&#25968;&#37327;&#32423;&#30340;&#35745;&#31639;&#35268;&#27169;&#19978;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30740;&#31350;&#65292;&#21457;&#29616;&#23558;&#35768;&#22810;&#20010;&#20307;&#20219;&#21153;&#21644;&#35780;&#20272;&#32858;&#21512;&#22312;&#19968;&#36215;&#30340;&#24179;&#22343;&#22522;&#20934;&#24615;&#33021;&#21487;&#20197;&#21512;&#29702;&#39044;&#27979;&#65292;&#20294;&#22312;&#20010;&#21035;&#20219;&#21153;&#20013;&#30340;&#39044;&#27979;&#24615;&#33021;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.04757</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#24615;&#33021;&#26377;&#22810;&#21487;&#39044;&#27979;&#65311;
&lt;/p&gt;
&lt;p&gt;
How predictable is language model benchmark performance?. (arXiv:2401.04757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#21313;&#19968;&#20010;&#26368;&#36817;&#30340;&#27169;&#22411;&#26550;&#26500;&#22312;&#20116;&#20010;&#25968;&#37327;&#32423;&#30340;&#35745;&#31639;&#35268;&#27169;&#19978;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30740;&#31350;&#65292;&#21457;&#29616;&#23558;&#35768;&#22810;&#20010;&#20307;&#20219;&#21153;&#21644;&#35780;&#20272;&#32858;&#21512;&#22312;&#19968;&#36215;&#30340;&#24179;&#22343;&#22522;&#20934;&#24615;&#33021;&#21487;&#20197;&#21512;&#29702;&#39044;&#27979;&#65292;&#20294;&#22312;&#20010;&#21035;&#20219;&#21153;&#20013;&#30340;&#39044;&#27979;&#24615;&#33021;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#21313;&#19968;&#20010;&#26368;&#36817;&#30340;&#27169;&#22411;&#26550;&#26500;&#22312;&#20116;&#20010;&#25968;&#37327;&#32423;&#30340;&#35745;&#31639;&#35268;&#27169;&#19978;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23558;&#35768;&#22810;&#20010;&#20307;&#20219;&#21153;&#21644;&#35780;&#20272;&#32858;&#21512;&#22312;&#19968;&#36215;&#65292;&#23601;&#20687;&#24120;&#29992;&#30340;BIG-Bench&#25968;&#25454;&#38598;&#19968;&#26679;&#65292;&#24179;&#22343;&#22522;&#20934;&#24615;&#33021;&#22312;&#35757;&#32451;&#35745;&#31639;&#35268;&#27169;&#30340;&#20989;&#25968;&#19979;&#26159;&#21487;&#20197;&#21512;&#29702;&#39044;&#27979;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24403;&#22312;&#35745;&#31639;&#20013;&#25193;&#22823;&#19968;&#20010;&#25968;&#37327;&#32423;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;BIG-Bench Hard&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026;6&#20010;&#30334;&#20998;&#28857;&#65288;pp&#65289;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#35745;&#31639;&#20013;&#25193;&#22823;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#20010;&#21035;BIG-Bench&#20219;&#21153;&#30340;&#22806;&#25512;&#24179;&#22343;&#35823;&#24046;&#20026;18pp&#12290;&#19981;&#36807;&#65292;&#20010;&#21035;&#20219;&#21153;&#30340;&#24615;&#33021;&#20173;&#28982;&#27604;&#38543;&#26426;&#32467;&#26524;&#26356;&#21487;&#39044;&#27979;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35745;&#31639;&#35268;&#27169;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#22810;&#26679;&#21270;&#22522;&#20934;&#20013;&#30340;AI&#33021;&#21147;&#65292;&#20294;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#39044;&#27979;&#24615;&#33021;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate large language model performance across five orders of magnitude of compute scaling in eleven recent model architectures. We show that average benchmark performance, aggregating over many individual tasks and evaluations as in the commonly-used BIG-Bench dataset, is decently predictable as a function of training compute scale. Specifically, when extrapolating BIG-Bench Hard performance across one order of magnitude in compute, we observe average absolute errors of 6 percentage points (pp). By contrast, extrapolation for individual BIG-Bench tasks across an order of magnitude in compute yields higher average errors of 18pp. Nonetheless, individual task performance remains significantly more predictable than chance. Overall, our work suggests compute scaling provides a promising basis to forecast AI capabilities in diverse benchmarks, though predicting performance in specific tasks poses challenges.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;KMeans&#32858;&#31867;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#35782;&#21035;&#20986;&#20102;&#24863;&#24212;&#28809;&#20013;&#30340;&#26368;&#20339;&#29076;&#21270;&#27169;&#24335;&#65292;&#24182;&#21033;&#29992;&#22810;&#20934;&#21017;&#20915;&#31574;&#26041;&#27861;&#30830;&#23450;&#20102;&#26368;&#20339;&#23454;&#36341;&#31751;&#12290;</title><link>http://arxiv.org/abs/2401.04751</link><description>&lt;p&gt;
&#35782;&#21035;&#24863;&#24212;&#28809;&#26368;&#20339;&#29076;&#21270;&#27169;&#24335;&#65306;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;KMeans&#32858;&#31867;&#21644;&#22810;&#26631;&#20934;&#20915;&#31574;&#26041;&#27861;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Identifying Best Practice Melting Patterns in Induction Furnaces: A Data-Driven Approach Using Time Series KMeans Clustering and Multi-Criteria Decision Making. (arXiv:2401.04751v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;KMeans&#32858;&#31867;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#35782;&#21035;&#20986;&#20102;&#24863;&#24212;&#28809;&#20013;&#30340;&#26368;&#20339;&#29076;&#21270;&#27169;&#24335;&#65292;&#24182;&#21033;&#29992;&#22810;&#20934;&#21017;&#20915;&#31574;&#26041;&#27861;&#30830;&#23450;&#20102;&#26368;&#20339;&#23454;&#36341;&#31751;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#24037;&#19994;&#29983;&#20135;&#36807;&#31243;&#30340;&#33021;&#28304;&#25928;&#29575;&#23545;&#20110;&#31454;&#20105;&#21147;&#21644;&#31526;&#21512;&#27668;&#20505;&#25919;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#24863;&#24212;&#28809;&#20013;&#30340;&#26368;&#20339;&#29076;&#21270;&#27169;&#24335;&#12290;&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;KMeans&#32858;&#31867;&#65292;&#21487;&#20197;&#26681;&#25454;&#28201;&#24230;&#26354;&#32447;&#23558;&#29076;&#21270;&#27169;&#24335;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#31751;&#12290;&#20351;&#29992;&#24367;&#26354;&#25296;&#28857;&#27861;&#65292;&#30830;&#23450;&#20102;12&#20010;&#31751;&#65292;&#20195;&#34920;&#20102;&#29076;&#21270;&#27169;&#24335;&#30340;&#33539;&#22260;&#12290;&#38024;&#23545;&#27599;&#20010;&#31751;&#24314;&#31435;&#20102;&#29076;&#21270;&#26102;&#38388;&#12289;&#33021;&#28304;&#29305;&#23450;&#24615;&#33021;&#21644;&#30899;&#25104;&#26412;&#31561;&#24615;&#33021;&#21442;&#25968;&#65292;&#25351;&#31034;&#20102;&#28809;&#23376;&#30340;&#25928;&#29575;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;&#21033;&#29992;&#31616;&#21333;&#21152;&#26435;&#12289;&#20056;&#27861;&#25351;&#25968;&#21152;&#26435;&#12289;TOPSIS&#27861;&#12289;&#25913;&#36827;&#30340;TOPSIS&#27861;&#21644;VlseKriterijumska Optimizacija I Kompromisno Resenje&#31561;&#22810;&#20934;&#21017;&#20915;&#31574;&#26041;&#27861;&#26469;&#30830;&#23450;&#26368;&#20339;&#23454;&#36341;&#31751;&#12290;&#30740;&#31350;&#25104;&#21151;&#22320;&#35782;&#21035;&#20102;&#26368;&#20339;&#23454;&#36341;&#31751;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving energy efficiency in industrial production processes is crucial for competitiveness, and compliance with climate policies. This paper introduces a data-driven approach to identify optimal melting patterns in induction furnaces. Through time-series K-means clustering the melting patterns could be classified into distinct clusters based on temperature profiles. Using the elbow method, 12 clusters were identified, representing the range of melting patterns. Performance parameters such as melting time, energy-specific performance, and carbon cost were established for each cluster, indicating furnace efficiency and environmental impact. Multiple criteria decision-making methods including Simple Additive Weighting, Multiplicative Exponential Weighting, Technique for Order of Preference by Similarity to Ideal Solution, modified TOPSIS, and VlseKriterijumska Optimizacija I Kompromisno Resenje were utilized to determine the best-practice cluster. The study successfully identified the 
&lt;/p&gt;</description></item><item><title>LogFormer&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#21644;&#35843;&#20248;&#27969;&#31243;&#65292;&#33021;&#22815;&#25552;&#39640;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#22312;&#28304;&#39046;&#22495;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#24182;&#21033;&#29992;&#20849;&#20139;&#21442;&#25968;&#23558;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#21516;&#26102;&#24341;&#20837;Log-Attention&#27169;&#22359;&#26469;&#34917;&#20805;&#34987;&#26085;&#24535;&#37197;&#23545;&#24573;&#30053;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.04749</link><description>&lt;p&gt;
LogFormer&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#30340;&#39044;&#35757;&#32451;&#21644;&#35843;&#20248;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
LogFormer: A Pre-train and Tuning Pipeline for Log Anomaly Detection. (arXiv:2401.04749v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04749
&lt;/p&gt;
&lt;p&gt;
LogFormer&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#21644;&#35843;&#20248;&#27969;&#31243;&#65292;&#33021;&#22815;&#25552;&#39640;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23427;&#36890;&#36807;&#22312;&#28304;&#39046;&#22495;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#24182;&#21033;&#29992;&#20849;&#20139;&#21442;&#25968;&#23558;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#21516;&#26102;&#24341;&#20837;Log-Attention&#27169;&#22359;&#26469;&#34917;&#20805;&#34987;&#26085;&#24535;&#37197;&#23545;&#24573;&#30053;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26159;&#20154;&#24037;&#26234;&#33021;&#36816;&#32500;&#65288;AIOps&#65289;&#39046;&#22495;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#32771;&#34385;&#21040;&#19981;&#21516;&#39046;&#22495;&#30340;&#26085;&#24535;&#25968;&#25454;&#65292;&#22312;&#23454;&#38469;&#24037;&#19994;&#22330;&#26223;&#20013;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#32593;&#32476;&#20197;&#36866;&#24212;&#26410;&#30693;&#39046;&#22495;&#26159;&#20302;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#28145;&#24230;&#27169;&#22411;&#20165;&#20851;&#27880;&#20110;&#22312;&#21516;&#19968;&#39046;&#22495;&#20013;&#25552;&#21462;&#26085;&#24535;&#24207;&#21015;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#23548;&#33268;&#22312;&#22810;&#39046;&#22495;&#26085;&#24535;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;Log&#24322;&#24120;&#26816;&#27979;&#32479;&#19968;&#26694;&#26550;(LogFormer)&#65292;&#20197;&#25913;&#21892;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#24314;&#31435;&#20102;&#21253;&#25324;&#39044;&#35757;&#32451;&#21644;&#22522;&#20110;adapter&#30340;&#35843;&#20248;&#38454;&#27573;&#30340;&#20004;&#38454;&#27573;&#27969;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#39318;&#20808;&#22312;&#28304;&#39046;&#22495;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#33719;&#21462;&#26085;&#24535;&#25968;&#25454;&#30340;&#20849;&#20139;&#35821;&#20041;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20849;&#20139;&#21442;&#25968;&#23558;&#36825;&#31181;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#39046;&#22495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Log-Attention&#27169;&#22359;&#65292;&#29992;&#20110;&#34917;&#20805;&#34987;&#26085;&#24535;&#37197;&#23545;&#24573;&#30053;&#30340;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#26159;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Log anomaly detection is a key component in the field of artificial intelligence for IT operations (AIOps). Considering log data of variant domains, retraining the whole network for unknown domains is inefficient in real industrial scenarios. However, previous deep models merely focused on extracting the semantics of log sequences in the same domain, leading to poor generalization on multi-domain logs. To alleviate this issue, we propose a unified Transformer-based framework for Log anomaly detection (LogFormer) to improve the generalization ability across different domains, where we establish a two-stage process including the pre-training and adapter-based tuning stage. Specifically, our model is first pre-trained on the source domain to obtain shared semantic knowledge of log data. Then, we transfer such knowledge to the target domain via shared parameters. Besides, the Log-Attention module is proposed to supplement the information ignored by the log-paring. The proposed method is ev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26080;&#25511;&#21046;&#20892;&#22330;&#29615;&#22659;&#19979;&#26816;&#27979;&#40657;&#33683;&#26524;&#23454;&#30340;&#32454;&#24494;&#25104;&#29087;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#24320;&#22987;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#25552;&#21462;&#40657;&#33683;&#26524;&#23454;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#20197;&#21028;&#26029;&#20854;&#25104;&#29087;&#24230;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;VGG16&#27169;&#22411;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#36755;&#20837;CNN&#36827;&#34892;&#38598;&#25104;&#20998;&#31867;&#65292;&#20197;&#35299;&#20915;&#40657;&#33683;&#26524;&#23454;&#25104;&#29087;&#24230;&#26816;&#27979;&#20013;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.04748</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23398;&#20064;&#30340;&#26080;&#25511;&#21046;&#20892;&#22330;&#29615;&#22659;&#19979;&#30340;&#39640;&#20809;&#35889;&#22270;&#20687;&#40657;&#33683;&#26524;&#23454;&#25104;&#29087;&#24230;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Network Ensemble Learning for Hyperspectral Imaging-based Blackberry Fruit Ripeness Detection in Uncontrolled Farm Environment. (arXiv:2401.04748v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26080;&#25511;&#21046;&#20892;&#22330;&#29615;&#22659;&#19979;&#26816;&#27979;&#40657;&#33683;&#26524;&#23454;&#30340;&#32454;&#24494;&#25104;&#29087;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#24320;&#22987;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#25552;&#21462;&#40657;&#33683;&#26524;&#23454;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#20197;&#21028;&#26029;&#20854;&#25104;&#29087;&#24230;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;VGG16&#27169;&#22411;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#36755;&#20837;CNN&#36827;&#34892;&#38598;&#25104;&#20998;&#31867;&#65292;&#20197;&#35299;&#20915;&#40657;&#33683;&#26524;&#23454;&#25104;&#29087;&#24230;&#26816;&#27979;&#20013;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#26524;&#23454;&#25104;&#29087;&#24230;&#20272;&#35745;&#27169;&#22411;&#19968;&#30452;&#20381;&#36182;&#20110;&#20809;&#35889;&#25351;&#25968;&#29305;&#24449;&#25110;&#22522;&#20110;&#39068;&#33394;&#30340;&#29305;&#24449;&#65292;&#22914;&#22343;&#20540;&#12289;&#26631;&#20934;&#24046;&#12289;&#20559;&#24230;&#12289;&#39068;&#33394;&#30697;&#21644;/&#25110;&#30452;&#26041;&#22270;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#24320;&#22987;&#25506;&#32034;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#40657;&#33683;&#26524;&#23454;&#30340;&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#26469;&#21028;&#26029;&#20854;&#25104;&#29087;&#24230;&#12290;&#28982;&#32780;&#65292;&#40657;&#33683;&#26524;&#23454;&#22312;&#25104;&#29087;&#26102;&#27809;&#26377;&#26126;&#26174;&#21487;&#38752;&#30340;&#21487;&#35265;&#24615;&#29305;&#24449;&#65292;&#22240;&#27492;&#23545;&#37319;&#25688;&#32773;&#26469;&#35828;&#20855;&#26377;&#24456;&#22823;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#24037;&#31243;&#24212;&#29992;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#36755;&#20837;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#38598;&#25104;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#26816;&#27979;&#40657;&#33683;&#26524;&#23454;&#25104;&#29087;&#24230;&#30340;&#32454;&#24494;&#29305;&#24449;&#12290;&#22810;&#36755;&#20837;CNN&#26159;&#30001;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#20960;&#20309;&#32452;16&#23618;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#65288;VGG16&#65289;&#27169;&#22411;&#21019;&#24314;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fruit ripeness estimation models have for decades depended on spectral index features or colour-based features, such as mean, standard deviation, skewness, colour moments, and/or histograms for learning traits of fruit ripeness. Recently, few studies have explored the use of deep learning techniques to extract features from images of fruits with visible ripeness cues. However, the blackberry (Rubus fruticosus) fruit does not show obvious and reliable visible traits of ripeness when mature and therefore poses great difficulty to fruit pickers. The mature blackberry, to the human eye, is black before, during, and post-ripening. To address this engineering application challenge, this paper proposes a novel multi-input convolutional neural network (CNN) ensemble classifier for detecting subtle traits of ripeness in blackberry fruits. The multi-input CNN was created from a pre-trained visual geometry group 16-layer deep convolutional network (VGG16) model trained on the ImageNet dataset. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Vision Transformer&#36827;&#34892;&#30382;&#32932;&#30284;&#20998;&#21106;&#21644;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#33258;&#27880;&#24847;&#26426;&#21046;&#25429;&#25417;&#22797;&#26434;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#20248;&#20110;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#39640;&#20934;&#30830;&#29575;&#21644;&#28508;&#22312;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.04746</link><description>&lt;p&gt;
&#20351;&#29992;Vision Transformer&#36827;&#34892;&#30382;&#32932;&#30284;&#20998;&#21106;&#21644;&#20998;&#31867;&#65292;&#29992;&#20110;&#22522;&#20110;&#30382;&#32932;&#38236;&#30340;&#38750;&#20405;&#20837;&#24335;&#25968;&#23383;&#31995;&#32479;&#30340;&#33258;&#21160;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Skin Cancer Segmentation and Classification Using Vision Transformer for Automatic Analysis in Dermatoscopy-based Non-invasive Digital System. (arXiv:2401.04746v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Vision Transformer&#36827;&#34892;&#30382;&#32932;&#30284;&#20998;&#21106;&#21644;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#33258;&#27880;&#24847;&#26426;&#21046;&#25429;&#25417;&#22797;&#26434;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#20248;&#20110;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#39640;&#20934;&#30830;&#29575;&#21644;&#28508;&#22312;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30382;&#32932;&#30284;&#26159;&#20840;&#29699;&#20581;&#24247;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#26089;&#26399;&#21644;&#20934;&#30830;&#30340;&#35786;&#26029;&#20197;&#25552;&#39640;&#24739;&#32773;&#39044;&#21518;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#30382;&#32932;&#30284;&#20998;&#31867;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;Vision Transformer&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#21508;&#31181;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#30340;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#21033;&#29992;HAM10000&#25968;&#25454;&#38598;&#30340;10,015&#20010;&#31934;&#30830;&#27880;&#37322;&#30340;&#30382;&#32932;&#30149;&#21464;&#22270;&#20687;&#65292;&#27169;&#22411;&#32463;&#36807;&#39044;&#22788;&#29702;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#36866;&#24212;&#20110;&#30382;&#32932;&#30284;&#20998;&#31867;&#20219;&#21153;&#30340;Vision Transformer&#21033;&#29992;&#33258;&#27880;&#24847;&#26426;&#21046;&#25429;&#25417;&#22797;&#26434;&#30340;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20855;&#26377;&#21331;&#36234;&#24615;&#33021;&#12290;Segment Anything Model&#26377;&#21161;&#20110;&#31934;&#30830;&#20998;&#21106;&#30284;&#21464;&#21306;&#22495;&#65292;&#36798;&#21040;&#39640;IOU&#21644;Dice&#31995;&#25968;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;Google&#30340;ViT patch-32&#21464;&#20307;&#65292;&#20854;&#20934;&#30830;&#29575;&#36798;&#21040;96.15%&#65292;&#23637;&#31034;&#20102;&#20316;&#20026;&#26377;&#25928;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skin cancer is a global health concern, necessitating early and accurate diagnosis for improved patient outcomes. This study introduces a groundbreaking approach to skin cancer classification, employing the Vision Transformer, a state-of-the-art deep learning architecture renowned for its success in diverse image analysis tasks. Utilizing the HAM10000 dataset of 10,015 meticulously annotated skin lesion images, the model undergoes preprocessing for enhanced robustness. The Vision Transformer, adapted to the skin cancer classification task, leverages the self-attention mechanism to capture intricate spatial dependencies, achieving superior performance over traditional deep learning architectures. Segment Anything Model aids in precise segmentation of cancerous areas, attaining high IOU and Dice Coefficient. Extensive experiments highlight the model's supremacy, particularly the Google-based ViT patch-32 variant, which achieves 96.15% accuracy and showcases potential as an effective tool
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;Dropout&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#33258;&#26059;&#30005;&#23376;&#23398;&#20013;&#27169;&#31946;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#20934;&#30830;&#24615;&#65292;&#20026;&#36164;&#28304;&#21463;&#38480;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04744</link><description>&lt;p&gt;
&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;Dropout&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#26059;&#30005;&#23376;&#23398;&#20013;&#30340;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Testing Spintronics Implemented Monte Carlo Dropout-Based Bayesian Neural Networks. (arXiv:2401.04744v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;Dropout&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#33258;&#26059;&#30005;&#23376;&#23398;&#20013;&#27169;&#31946;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#20934;&#30830;&#24615;&#65292;&#20026;&#36164;&#28304;&#21463;&#38480;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BayNNs&#65289;&#21487;&#20197;&#20869;&#22312;&#22320;&#20272;&#35745;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26041;&#20415;&#20915;&#31574;&#21046;&#23450;&#12290;&#22522;&#20110;Dropout&#30340;BayNNs&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#33258;&#26059;&#30005;&#23376;&#23398;&#35745;&#31639;&#20869;&#23384;&#26550;&#26500;&#20013;&#65292;&#29992;&#20110;&#36164;&#28304;&#21463;&#38480;&#20294;&#24615;&#33021;&#35201;&#27714;&#39640;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#12290;&#23613;&#31649;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24456;&#37325;&#35201;&#65292;&#20294;&#29616;&#26377;&#30340;&#24037;&#20316;&#24573;&#35270;&#20102;Dropout&#29983;&#25104;&#21644;BayNN&#35745;&#31639;&#30340;&#21487;&#38752;&#24615;&#23545;&#20110;&#30446;&#26631;&#24212;&#29992;&#21516;&#26679;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#27979;&#35797;BayNNs&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#38543;&#26426;&#24615;&#36136;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#26059;&#30005;&#23376;&#23398;Dropout&#27169;&#22359;&#30340;&#38750;&#29702;&#24819;&#24615;&#27169;&#22411;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#23545;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#37325;&#22797;&#24615;&#25490;&#21517;&#30340;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;Dropout-based BayNN&#65292;&#21487;&#20197;&#36798;&#21040;100%&#30340;&#25925;&#38556;&#35206;&#30422;&#29575;&#65292;&#21516;&#26102;&#20165;&#20351;&#29992;0.2%&#30340;&#35757;&#32451;&#25968;&#25454;&#20316;&#20026;&#27979;&#35797;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Neural Networks (BayNNs) can inherently estimate predictive uncertainty, facilitating informed decision-making. Dropout-based BayNNs are increasingly implemented in spintronics-based computation-in-memory architectures for resource-constrained yet high-performance safety-critical applications. Although uncertainty estimation is important, the reliability of Dropout generation and BayNN computation is equally important for target applications but is overlooked in existing works. However, testing BayNNs is significantly more challenging compared to conventional NNs, due to their stochastic nature. In this paper, we present for the first time the model of the non-idealities of the spintronics-based Dropout module and analyze their impact on uncertainty estimates and accuracy. Furthermore, we propose a testing framework based on repeatability ranking for Dropout-based BayNN with up to $100\%$ fault coverage while using only $0.2\%$ of training data as test vectors.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#20837;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#21644;&#25913;&#36827;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#39044;&#23450;&#20041;&#30340;&#32858;&#31867;&#25968;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#25928;&#30340;&#22270;&#32858;&#31867;&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.04741</link><description>&lt;p&gt;
&#26080;&#39044;&#23450;&#20041;&#32858;&#31867;&#25968;k&#30340;&#22270;&#32858;&#31867;&#30340;Masked AutoEncoder
&lt;/p&gt;
&lt;p&gt;
Masked AutoEncoder for Graph Clustering without Pre-defined Cluster Number k. (arXiv:2401.04741v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#20837;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#21644;&#25913;&#36827;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#39044;&#23450;&#20041;&#30340;&#32858;&#31867;&#25968;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#25928;&#30340;&#22270;&#32858;&#31867;&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#32467;&#26500;&#30340;&#22270;&#32858;&#31867;&#31639;&#27861;&#22240;&#20854;&#39640;&#25928;&#24615;&#33021;&#21644;&#20302;&#35757;&#32451;&#25104;&#26412;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22522;&#20110;GCN&#25110;GAT&#30340;&#29616;&#26377;&#22270;&#33258;&#32534;&#30721;&#32858;&#31867;&#31639;&#27861;&#65292;&#23427;&#20204;&#19981;&#20165;&#32570;&#20047;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19988;&#38590;&#20197;&#33258;&#21160;&#30830;&#23450;&#30001;&#27492;&#31867;&#33258;&#32534;&#30721;&#27169;&#22411;&#32858;&#31867;&#30340;&#31751;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;Masked Autoencoders&#36827;&#34892;&#22270;&#32858;&#31867;&#65288;GCMA&#65289;&#12290;&#23427;&#37319;&#29992;&#22522;&#20110;&#22270;&#23631;&#34109;&#26041;&#27861;&#30340;&#34701;&#21512;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#22270;&#30340;&#34701;&#21512;&#32534;&#30721;&#12290;&#23427;&#24341;&#20837;&#25105;&#20204;&#25913;&#36827;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#20316;&#20026;&#31532;&#20108;&#20010;&#35299;&#30721;&#22120;&#65292;&#22312;&#22810;&#30446;&#26631;&#37325;&#24314;&#26102;&#35299;&#30721;&#12290;&#36890;&#36807;&#35299;&#30721;&#25513;&#30721;&#23884;&#20837;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#25429;&#33719;&#26356;&#24191;&#20041;&#21644;&#20840;&#38754;&#30340;&#30693;&#35782;&#12290;&#22312;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#30340;&#21516;&#26102;&#21487;&#20197;&#36755;&#20986;&#32858;&#31867;&#25968;&#21644;&#32858;&#31867;&#32467;&#26524;&#12290;&#20316;&#20026;&#19968;&#31181;&#38750;&#21442;&#25968;&#31867;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
Graph clustering algorithms with autoencoder structures have recently gained popularity due to their efficient performance and low training cost. However, for existing graph autoencoder clustering algorithms based on GCN or GAT, not only do they lack good generalization ability, but also the number of clusters clustered by such autoencoder models is difficult to determine automatically. To solve this problem, we propose a new framework called Graph Clustering with Masked Autoencoders (GCMA). It employs our designed fusion autoencoder based on the graph masking method for the fusion coding of graph. It introduces our improved density-based clustering algorithm as a second decoder while decoding with multi-target reconstruction. By decoding the mask embedding, our model can capture more generalized and comprehensive knowledge. The number of clusters and clustering results can be output end-to-end while improving the generalization ability. As a nonparametric class method, extensive exper
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#23454;&#26102;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;LLM&#23884;&#20837;&#19982;&#38144;&#21806;&#26448;&#26009;&#36827;&#34892;&#21305;&#37197;&#65292;&#25552;&#20379;&#32473;&#38144;&#21806;&#20154;&#21592;&#23454;&#26102;&#25512;&#33616;&#65292;&#20174;&#32780;&#25552;&#39640;&#38144;&#21806;&#20154;&#21592;&#30340;&#24037;&#20316;&#25928;&#29575;&#12290;&#36825;&#19968;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#36820;&#22238;&#26368;&#30456;&#20851;&#30340;&#20869;&#23481;&#25512;&#33616;&#65292;&#21363;&#20351;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20063;&#26159;&#22914;&#27492;&#12290;&#36825;&#19968;&#25512;&#33616;&#31995;&#32479;&#24050;&#25104;&#21151;&#38598;&#25104;&#21040;&#24494;&#36719;&#38144;&#21806;&#20154;&#21592;&#27599;&#26085;&#20351;&#29992;&#30340;Dynamics CRM&#30340;&#29983;&#20135;&#29256;&#26412;&#20013;&#12290;</title><link>http://arxiv.org/abs/2401.04732</link><description>&lt;p&gt;
MSX&#38144;&#21806;&#21327;&#21516;&#21161;&#25163;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#26696;&#20363;&#30740;&#31350;: &#36890;&#36807;&#23454;&#26102;&#38382;&#31572;&#31995;&#32479;&#25913;&#21892;&#38144;&#21806;&#20154;&#21592;&#30340;&#24037;&#20316;&#25928;&#29575;&#20197;&#23454;&#29616;&#20869;&#23481;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
A case study of Generative AI in MSX Sales Copilot: Improving seller productivity with a real-time question-answering system for content recommendation. (arXiv:2401.04732v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#23454;&#26102;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;LLM&#23884;&#20837;&#19982;&#38144;&#21806;&#26448;&#26009;&#36827;&#34892;&#21305;&#37197;&#65292;&#25552;&#20379;&#32473;&#38144;&#21806;&#20154;&#21592;&#23454;&#26102;&#25512;&#33616;&#65292;&#20174;&#32780;&#25552;&#39640;&#38144;&#21806;&#20154;&#21592;&#30340;&#24037;&#20316;&#25928;&#29575;&#12290;&#36825;&#19968;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#36820;&#22238;&#26368;&#30456;&#20851;&#30340;&#20869;&#23481;&#25512;&#33616;&#65292;&#21363;&#20351;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20063;&#26159;&#22914;&#27492;&#12290;&#36825;&#19968;&#25512;&#33616;&#31995;&#32479;&#24050;&#25104;&#21151;&#38598;&#25104;&#21040;&#24494;&#36719;&#38144;&#21806;&#20154;&#21592;&#27599;&#26085;&#20351;&#29992;&#30340;Dynamics CRM&#30340;&#29983;&#20135;&#29256;&#26412;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#23454;&#26102;&#38382;&#31572;&#31995;&#32479;&#65292;&#19987;&#38376;&#20026;&#38144;&#21806;&#20154;&#21592;&#25552;&#20379;&#26377;&#20851;&#26448;&#26009;/&#25991;&#26723;&#30340;&#23454;&#26102;&#25512;&#33616;&#65292;&#20197;&#20415;&#19982;&#23458;&#25143;&#20998;&#20139;&#25110;&#22312;&#30005;&#35805;&#20013;&#21442;&#32771;&#12290;&#36890;&#36807;&#20351;&#29992;Seismic&#38144;&#21806;&#36164;&#26009;&#30340;&#30456;&#23545;&#36739;&#22823;&#35268;&#27169;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#21334;&#26041;&#26597;&#35810;&#30340;LLM&#23884;&#20837;&#19982;&#30456;&#20851;&#20869;&#23481;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;&#35814;&#32454;&#30340;&#26041;&#24335;&#35774;&#35745;&#25552;&#31034;&#35821;&#65292;&#24182;&#21033;&#29992;&#21487;&#29992;&#30340;&#20016;&#23500;&#30340;&#25991;&#26723;&#21644;&#38144;&#21806;&#32773;&#20803;&#29305;&#24449;&#38598;&#65292;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#20132;&#21449;&#32534;&#30721;&#22120;&#37325;&#25490;&#24207;&#22120;&#26550;&#26500;&#30340;&#21452;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#35299;&#20915;&#26041;&#26696;&#22312;&#20165;&#20960;&#31186;&#38047;&#20869;&#21363;&#21487;&#36820;&#22238;&#26368;&#30456;&#20851;&#30340;&#20869;&#23481;&#25512;&#33616;&#65292;&#21363;&#20351;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#30340;&#25512;&#33616;&#31995;&#32479;&#24050;&#37096;&#32626;&#20026;&#29992;&#20110;&#23454;&#26102;&#25512;&#29702;&#30340;AML&#31471;&#28857;&#65292;&#24182;&#24050;&#38598;&#25104;&#21040;Copilot&#30028;&#38754;&#20013;&#65292;&#35813;&#30028;&#38754;&#29616;&#24050;&#37096;&#32626;&#22312;&#27599;&#26085;&#30001;&#24494;&#36719;&#38144;&#21806;&#20154;&#21592;&#20351;&#29992;&#30340;Dynamics CRM&#30340;&#29983;&#20135;&#29256;&#26412;&#20013;(MSX&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we design a real-time question-answering system specifically targeted for helping sellers get relevant material/documentation they can share live with their customers or refer to during a call. Taking the Seismic content repository as a relatively large scale example of a diverse dataset of sales material, we demonstrate how LLM embeddings of sellers' queries can be matched with the relevant content. We achieve this by engineering prompts in an elaborate fashion that makes use of the rich set of meta-features available for documents and sellers. Using a bi-encoder with cross-encoder re-ranker architecture, we show how the solution returns the most relevant content recommendations in just a few seconds even for large datasets. Our recommender system is deployed as an AML endpoint for real-time inferencing and has been integrated into a Copilot interface that is now deployed in the production version of the Dynamics CRM, known as MSX, used daily by Microsoft sellers.
&lt;/p&gt;</description></item><item><title>RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04679</link><description>&lt;p&gt;
RoSA: &#36890;&#36807;&#40065;&#26834;&#36866;&#24212;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04679
&lt;/p&gt;
&lt;p&gt;
RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#32972;&#26223;&#19979;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#39044;&#31639;&#19979;&#25552;&#20379;&#33391;&#22909;&#20934;&#30830;&#24615;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843; (PEFT) &#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#31216;&#20026;RoSA&#65292;&#21463;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512; (PCA) &#30340;&#21551;&#21457;&#65292;&#23427;&#22312;&#19968;&#32452;&#22266;&#23450;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#20849;&#21516;&#35757;&#32451;$\textit{&#20302;&#31209;}$&#21644;$\textit{&#39640;&#24230;&#31232;&#30095;}$&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#65288;FFT&#65289;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RoSA&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#23567;&#23398;&#25968;&#23398;&#21644;SQL&#26597;&#35810;&#29983;&#25104;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#20197;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#30456;&#21516;&#30340;&#21442;&#25968;&#39044;&#31639;&#19979;&#65292;RoSA&#20248;&#20110;LoRA&#21644;&#32431;&#31929;&#30340;&#31232;&#30095;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#31232;&#30095;GPU&#20869;&#26680;&#20026;RoSA&#25552;&#20379;&#31995;&#32479;&#25903;&#25345;&#65292;&#20197;&#34917;&#20805;&#35757;&#32451;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;https://github.com/IST-DASLab&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memoryand computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedDEP&#65292;&#29992;&#20110;&#35299;&#20915;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#20256;&#25773;&#19981;&#23436;&#25972;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25216;&#26415;&#35774;&#35745;&#65292;&#21253;&#25324;&#28145;&#24230;&#37051;&#23621;&#29983;&#25104;&#21644;&#39640;&#25928;&#30340;&#31169;&#23494;&#39046;&#22495;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2401.04336</link><description>&lt;p&gt;
&#28145;&#24230;&#39640;&#25928;&#30340;&#31169;&#23494;&#39046;&#22495;&#29983;&#25104;&#29992;&#20110;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Efficient Private Neighbor Generation for Subgraph Federated Learning. (arXiv:2401.04336v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedDEP&#65292;&#29992;&#20110;&#35299;&#20915;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#20256;&#25773;&#19981;&#23436;&#25972;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25216;&#26415;&#35774;&#35745;&#65292;&#21253;&#25324;&#28145;&#24230;&#37051;&#23621;&#29983;&#25104;&#21644;&#39640;&#25928;&#30340;&#31169;&#23494;&#39046;&#22495;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#24040;&#22823;&#22270;&#36890;&#24120;&#20197;&#38750;&#20013;&#24515;&#21270;&#23376;&#22270;&#30340;&#24418;&#24335;&#30001;&#22810;&#20010;&#25968;&#25454;&#25152;&#26377;&#32773;&#20998;&#25955;&#23384;&#20648;&#12290;&#20026;&#20102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#22312;&#19981;&#25439;&#23475;&#25968;&#25454;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#65292;&#32771;&#34385;&#21040;&#23376;&#22270;&#32852;&#37030;&#23398;&#20064;&#65288;subgraph FL&#65289;&#22330;&#26223;&#26159;&#24456;&#33258;&#28982;&#30340;&#65292;&#20854;&#20013;&#27599;&#20010;&#26412;&#22320;&#23458;&#25143;&#31471;&#25345;&#26377;&#25972;&#20010;&#20840;&#23616;&#22270;&#30340;&#23376;&#22270;&#65292;&#20197;&#33719;&#21462;&#20840;&#23616;&#19968;&#33324;&#21270;&#30340;&#22270;&#25366;&#25496;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#30001;&#20110;&#32570;&#23569;&#36328;&#23376;&#22270;&#37051;&#23621;&#32780;&#23548;&#33268;&#30340;&#23616;&#37096;&#23376;&#22270;&#19978;&#30340;&#20449;&#24687;&#20256;&#25773;&#19981;&#23436;&#25972;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20197;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#32570;&#22833;&#37051;&#23621;&#29983;&#25104;&#22120;&#21644;GNN&#30340;&#32852;&#21512;FL&#26469;&#22686;&#21152;&#26412;&#22320;&#37051;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;FL&#30340;&#25928;&#29992;&#24615;&#12289;&#25928;&#29575;&#24615;&#21644;&#38544;&#31169;&#30446;&#26631;&#26041;&#38754;&#23384;&#22312;&#28145;&#23618;&#27425;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedDEP&#26469;&#20840;&#38754;&#35299;&#20915;&#23376;&#22270;FL&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#12290;FedDEP&#21253;&#25324;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#25216;&#26415;&#35774;&#35745;&#65306;(1) &#21033;&#29992;&#28508;&#22312;&#32570;&#22833;&#37051;&#23621;&#30340;GNN&#23884;&#20837;&#36827;&#34892;&#28145;&#24230;&#37051;&#23621;&#29983;&#25104;&#65307;(2) Effic...
&lt;/p&gt;
&lt;p&gt;
Behemoth graphs are often fragmented and separately stored by multiple data owners as distributed subgraphs in many realistic applications. Without harming data privacy, it is natural to consider the subgraph federated learning (subgraph FL) scenario, where each local client holds a subgraph of the entire global graph, to obtain globally generalized graph mining models. To overcome the unique challenge of incomplete information propagation on local subgraphs due to missing cross-subgraph neighbors, previous works resort to the augmentation of local neighborhoods through the joint FL of missing neighbor generators and GNNs. Yet their technical designs have profound limitations regarding the utility, efficiency, and privacy goals of FL. In this work, we propose FedDEP to comprehensively tackle these challenges in subgraph FL. FedDEP consists of a series of novel technical designs: (1) Deep neighbor generation through leveraging the GNN embeddings of potential missing neighbors; (2) Effic
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;PLUTO:&#19968;&#31181;&#25554;&#25300;&#24335;&#27169;&#22359;&#21270;&#30340;&#27979;&#35797;&#26102;&#39046;&#22495;&#36866;&#24212;&#31574;&#30053;&#65292;&#36890;&#36807;&#39044;&#20808;&#35757;&#32451;&#19968;&#31995;&#21015;&#38024;&#23545;&#19981;&#21516;&#28304;&#39046;&#22495;&#30340;&#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#21019;&#24314;&#20102;&#19968;&#20010;"&#27169;&#22359;&#23384;&#20648;&#24211;"&#12290;&#37319;&#29992;&#26080;&#30417;&#30563;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20174;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#31232;&#30095;&#30340;&#30456;&#20851;&#27169;&#22359;&#30340;&#23376;&#38598;&#65292;&#24182;&#21019;&#24314;&#36873;&#20013;&#27169;&#22359;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#39046;&#22495;&#30340;&#33258;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2401.04130</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#65306;&#36890;&#36807;&#25554;&#20837;&#21644;&#25773;&#25918;&#21464;&#25442;&#22120;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Test-Time Adaptation via Plug-and-Play Transformer Modules. (arXiv:2401.04130v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04130
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;PLUTO:&#19968;&#31181;&#25554;&#25300;&#24335;&#27169;&#22359;&#21270;&#30340;&#27979;&#35797;&#26102;&#39046;&#22495;&#36866;&#24212;&#31574;&#30053;&#65292;&#36890;&#36807;&#39044;&#20808;&#35757;&#32451;&#19968;&#31995;&#21015;&#38024;&#23545;&#19981;&#21516;&#28304;&#39046;&#22495;&#30340;&#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#21019;&#24314;&#20102;&#19968;&#20010;"&#27169;&#22359;&#23384;&#20648;&#24211;"&#12290;&#37319;&#29992;&#26080;&#30417;&#30563;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20174;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#31232;&#30095;&#30340;&#30456;&#20851;&#27169;&#22359;&#30340;&#23376;&#38598;&#65292;&#24182;&#21019;&#24314;&#36873;&#20013;&#27169;&#22359;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#39046;&#22495;&#30340;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;(PET)&#26041;&#27861;&#65292;&#22914;LoRA&#12289;Adapter&#21644;Visual Prompt Tuning(VPT)&#65292;&#36890;&#36807;&#35843;&#25972;&#21464;&#25442;&#22120;&#27169;&#22411;&#20013;&#30340;&#23567;&#27169;&#22359;&#65292;&#22312;&#20351;&#36866;&#24212;&#26032;&#39046;&#22495;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#39046;&#22495;&#25968;&#37327;&#21487;&#33021;&#38750;&#24120;&#22823;&#65292;&#25968;&#25454;&#36890;&#24120;&#26159;&#26080;&#26631;&#31614;&#30340;&#12290;&#22240;&#27492;&#65292;&#36866;&#24212;&#26032;&#39046;&#22495;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#20063;&#19981;&#29616;&#23454;&#20026;&#27599;&#20010;&#36825;&#26679;&#30340;&#39046;&#22495;&#29983;&#25104;&#23450;&#21046;&#30340;&#35843;&#25972;&#27169;&#22359;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;PLUTO&#65306;&#19968;&#31181;&#25554;&#25300;&#27169;&#22359;&#21270;&#30340;&#27979;&#35797;&#26102;&#39046;&#22495;&#36866;&#24212;&#31574;&#30053;&#12290;&#25105;&#20204;&#39044;&#35757;&#32451;&#20102;&#19968;&#31995;&#21015;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#19987;&#20026;&#19981;&#21516;&#30340;&#28304;&#39046;&#22495;&#36827;&#34892;&#20102;&#19987;&#38376;&#35774;&#35745;&#65292;&#26377;&#25928;&#22320;&#21019;&#24314;&#20102;&#19968;&#20010;"&#27169;&#22359;&#23384;&#20648;&#24211;"&#12290;&#32473;&#23450;&#19968;&#20010;&#24102;&#26377;&#23569;&#26679;&#26412;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#30446;&#26631;&#22495;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;(TTA)&#26041;&#27861;&#65292;&#26469;(1)&#20174;&#24211;&#20013;&#36873;&#25321;&#20986;&#31232;&#30095;&#30340;&#30456;&#20851;&#27169;&#22359;&#30340;&#23376;&#38598;&#65292;&#24182;&#19988;(2)&#22312;&#19981;&#35843;&#25972;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#21019;&#24314;&#36873;&#20013;&#27169;&#22359;&#30340;&#21152;&#26435;&#32452;&#21512;&#12290;&#36825;&#31181;&#25554;&#25300;&#24335;&#30340;&#29305;&#24615;&#20351;&#24471;&#23427;&#21487;===
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient tuning (PET) methods such as LoRA, Adapter, and Visual Prompt Tuning (VPT) have found success in enabling adaptation to new domains by tuning small modules within a transformer model. However, the number of domains encountered during test time can be very large, and the data is usually unlabeled. Thus, adaptation to new domains is challenging; it is also impractical to generate customized tuned modules for each such domain. Toward addressing these challenges, this work introduces PLUTO: a Plug-and-pLay modUlar Test-time domain adaptatiOn strategy. We pre-train a large set of modules, each specialized for different source domains, effectively creating a ``module store''. Given a target domain with few-shot unlabeled data, we introduce an unsupervised test-time adaptation (TTA) method to (1) select a sparse subset of relevant modules from this store and (2) create a weighted combination of selected modules without tuning their weights. This plug-and-play nature enable
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#31070;&#32463;&#36864;&#34892;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;AD&#21644;MCI&#30340;&#35782;&#21035;&#12290;&#35813;&#32593;&#32476;&#32771;&#34385;&#20102;&#23616;&#37096;&#32467;&#26500;&#29305;&#24449;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#36827;&#34892;&#26089;&#26399;AD&#30340;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2401.03922</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#31070;&#32463;&#36864;&#34892;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#24314;&#27169;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Structure-focused Neurodegeneration Convolutional Neural Network for Modeling and Classification of Alzheimer's Disease. (arXiv:2401.03922v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#31070;&#32463;&#36864;&#34892;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;AD&#21644;MCI&#30340;&#35782;&#21035;&#12290;&#35813;&#32593;&#32476;&#32771;&#34385;&#20102;&#23616;&#37096;&#32467;&#26500;&#29305;&#24449;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#36827;&#34892;&#26089;&#26399;AD&#30340;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#20316;&#20026;&#20027;&#35201;&#30340;&#30196;&#21574;&#24418;&#24335;&#65292;&#23545;&#20840;&#29699;&#26500;&#25104;&#20102;&#19968;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#20102;&#20934;&#30830;&#21644;&#26089;&#26399;&#35786;&#26029;&#30340;&#32039;&#36843;&#24615;&#12290;&#20020;&#24202;&#25216;&#26415;&#20013;&#65292;&#25918;&#23556;&#31185;&#21307;&#29983;&#22312;&#20351;&#29992;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#21306;&#20998;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;MCI&#65289;&#21644;AD&#26102;&#36935;&#21040;&#20102;&#22256;&#38590;&#65292;&#22240;&#20026;&#36825;&#20123;&#26041;&#27861;&#19981;&#19968;&#33268;&#19988;&#19981;&#21487;&#38752;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#26377;&#26395;&#25552;&#20379;&#26089;&#26399;AD&#35786;&#26029;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#32454;&#31890;&#24230;&#30340;&#23616;&#37096;&#29305;&#24449;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#21040;&#21487;&#20197;&#25552;&#20379;&#22823;&#33041;&#30382;&#23618;&#31070;&#32463;&#36864;&#34892;&#20449;&#24687;&#30340;&#23616;&#37096;&#32467;&#26500;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#38598;&#25104;&#20102;Gamma&#26657;&#27491;&#65288;&#19968;&#31181;&#22270;&#20687;&#22686;&#24378;&#25216;&#26415;&#65289;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;SNeurodCNN&#30340;&#32467;&#26500;&#21270;&#31070;&#32463;&#36864;&#34892;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#21306;&#20998;AD&#21644;MCI&#12290;&#35813;ML&#26694;&#26550;&#21033;&#29992;&#20102;&#20013;&#30690;&#29366;&#38754;&#21644;&#26049;&#30690;&#29366;&#38754;&#30340;&#22823;&#33041;&#22270;&#20687;&#35270;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's disease (AD), the predominant form of dementia, poses a growing global challenge and underscores the urgency of accurate and early diagnosis. The clinical technique radiologists adopt for distinguishing between mild cognitive impairment (MCI) and AD using Machine Resonance Imaging (MRI) encounter hurdles because they are not consistent and reliable. Machine learning has been shown to offer promise for early AD diagnosis. However, existing models focused on focal fine-grain features without considerations to focal structural features that give off information on neurodegeneration of the brain cerebral cortex. Therefore, this paper proposes a machine learning (ML) framework that integrates Gamma correction, an image enhancement technique, and includes a structure-focused neurodegeneration convolutional neural network (CNN) architecture called SNeurodCNN for discriminating between AD and MCI. The ML framework leverages the mid-sagittal and para-sagittal brain image viewpoints 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#27835;&#30103;&#25512;&#33616;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#22266;&#23450;&#39044;&#31639;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23454;&#39564;&#35774;&#35745;&#21644;&#31574;&#30053;&#23398;&#20064;&#26469;&#25512;&#33616;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26399;&#26395;&#31616;&#21333;&#36951;&#25022;&#26469;&#34913;&#37327;&#25512;&#33616;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.03756</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#22266;&#23450;&#39044;&#31639;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#65306;&#36866;&#24212;&#24615;&#23454;&#39564;&#35774;&#35745;&#19982;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contextual Fixed-Budget Best Arm Identification: Adaptive Experimental Design with Policy Learning. (arXiv:2401.03756v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03756
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#27835;&#30103;&#25512;&#33616;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#22266;&#23450;&#39044;&#31639;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23454;&#39564;&#35774;&#35745;&#21644;&#31574;&#30053;&#23398;&#20064;&#26469;&#25512;&#33616;&#26368;&#20339;&#27835;&#30103;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26399;&#26395;&#31616;&#21333;&#36951;&#25022;&#26469;&#34913;&#37327;&#25512;&#33616;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#27835;&#30103;&#25512;&#33616;&#26159;&#22522;&#20110;&#35777;&#25454;&#30340;&#20915;&#31574;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#20219;&#21153;&#20316;&#20026;&#19968;&#20010;&#24102;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22266;&#23450;&#39044;&#31639;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;Best Arm Identification, BAI&#65289;&#38382;&#39064;&#26469;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#32473;&#23450;&#22810;&#20010;&#27835;&#30103;&#33218;&#30340;&#33258;&#36866;&#24212;&#35797;&#39564;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#20915;&#31574;&#32773;&#35266;&#23519;&#19968;&#20010;&#21051;&#30011;&#23454;&#39564;&#21333;&#20301;&#30340;&#19978;&#19979;&#25991;&#65288;&#21327;&#21464;&#37327;&#65289;&#65292;&#24182;&#23558;&#35813;&#21333;&#20301;&#20998;&#37197;&#32473;&#20854;&#20013;&#19968;&#20010;&#27835;&#30103;&#33218;&#12290;&#22312;&#23454;&#39564;&#32467;&#26463;&#26102;&#65292;&#20915;&#31574;&#32773;&#25512;&#33616;&#19968;&#20010;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#26465;&#20214;&#19979;&#39044;&#35745;&#20135;&#29983;&#26368;&#39640;&#26399;&#26395;&#32467;&#26524;&#30340;&#27835;&#30103;&#33218;&#65288;&#26368;&#20339;&#27835;&#30103;&#33218;&#65289;&#12290;&#35813;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26399;&#26395;&#31616;&#21333;&#36951;&#25022;&#65288;&#31574;&#30053;&#36951;&#25022;&#65289;&#26469;&#34913;&#37327;&#65292;&#35813;&#36951;&#25022;&#34920;&#31034;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#26465;&#20214;&#19979;&#65292;&#26368;&#20339;&#27835;&#30103;&#33218;&#21644;&#25512;&#33616;&#27835;&#30103;&#33218;&#30340;&#26465;&#20214;&#26399;&#26395;&#32467;&#26524;&#20043;&#38388;&#30340;&#26368;&#22823;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#21021;&#22987;&#27493;&#39588;&#26159;&#25512;&#23548;&#26368;&#22351;&#24773;&#20917;&#19979;&#26399;&#26395;&#31616;&#21333;&#36951;&#25022;&#30340;&#28176;&#36817;&#19979;&#30028;&#65292;&#35813;&#19979;&#30028;&#36824;&#26263;&#31034;&#30528;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#19968;&#20123;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individualized treatment recommendation is a crucial task in evidence-based decision-making. In this study, we formulate this task as a fixed-budget best arm identification (BAI) problem with contextual information. In this setting, we consider an adaptive experiment given multiple treatment arms. At each round, a decision-maker observes a context (covariate) that characterizes an experimental unit and assigns the unit to one of the treatment arms. At the end of the experiment, the decision-maker recommends a treatment arm estimated to yield the highest expected outcome conditioned on a context (best treatment arm). The effectiveness of this decision is measured in terms of the worst-case expected simple regret (policy regret), which represents the largest difference between the conditional expected outcomes of the best and recommended treatment arms given a context. Our initial step is to derive asymptotic lower bounds for the worst-case expected simple regret, which also implies idea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20197;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#26368;&#22823;&#20551;&#35774;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#65292;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24320;&#21457;&#20013;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#22312;&#35813;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#30340;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#25506;&#35752;&#20102;&#20351;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35782;&#21035;&#20551;&#35774;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.03653</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24320;&#21457;&#20013;&#33258;&#21160;&#35782;&#21035;&#20551;&#35774;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An exploratory study on automatic identification of assumptions in the development of deep learning frameworks. (arXiv:2401.03653v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20197;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#26368;&#22823;&#20551;&#35774;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#65292;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24320;&#21457;&#20013;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#30740;&#31350;&#12290;&#22312;&#35813;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#30340;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#25506;&#35752;&#20102;&#20351;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35782;&#21035;&#20551;&#35774;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#30410;&#30456;&#20851;&#26041;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24320;&#21457;&#20013;&#32463;&#24120;&#20570;&#20986;&#20551;&#35774;&#12290;&#36825;&#20123;&#20551;&#35774;&#28041;&#21450;&#21508;&#31181;&#36719;&#20214;&#26500;&#20214;&#65288;&#20363;&#22914;&#38656;&#27714;&#12289;&#35774;&#35745;&#20915;&#31574;&#21644;&#25216;&#26415;&#20538;&#21153;&#65289;&#65292;&#21487;&#33021;&#20250;&#34987;&#35777;&#26126;&#26080;&#25928;&#65292;&#20174;&#32780;&#23548;&#33268;&#31995;&#32479;&#25925;&#38556;&#12290;&#29616;&#26377;&#30340;&#20551;&#35774;&#31649;&#29702;&#26041;&#27861;&#21644;&#24037;&#20855;&#36890;&#24120;&#20381;&#36182;&#20110;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#20551;&#35774;&#20998;&#25955;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24320;&#21457;&#30340;&#21508;&#31181;&#28304;&#22836;&#65288;&#20363;&#22914;&#20195;&#30721;&#27880;&#37322;&#12289;&#25552;&#20132;&#12289;&#25289;&#21462;&#35831;&#27714;&#21644;&#38382;&#39064;&#65289;&#20013;&#65292;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#25104;&#26412;&#36739;&#39640;&#65288;&#20363;&#22914;&#26102;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#24320;&#21457;&#20013;&#25163;&#21160;&#35782;&#21035;&#20551;&#35774;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#24182;&#19988;&#26368;&#22823;&#30340;&#20551;&#35774;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;AssuEval&#65289;&#65292;&#35813;&#25968;&#25454;&#38598;&#25910;&#38598;&#33258;GitHub&#19978;&#30340;TensorFlow&#21644;Keras&#20195;&#30721;&#24211;&#65307;&#25105;&#20204;&#25506;&#35752;&#20102;&#19971;&#20010;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#20363;&#22914;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;&#20998;&#31867;&#22238;&#24402;&#26641;&#65289;&#21644;&#19968;&#20010;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stakeholders constantly make assumptions in the development of deep learning (DL) frameworks. These assumptions are related to various types of software artifacts (e.g., requirements, design decisions, and technical debt) and can turn out to be invalid, leading to system failures. Existing approaches and tools for assumption management usually depend on manual identification of assumptions. However, assumptions are scattered in various sources (e.g., code comments, commits, pull requests, and issues) of DL framework development, and manually identifying assumptions has high costs (e.g., time and resources). To overcome the issues of manually identifying assumptions in DL framework development, we constructed a new and largest dataset (i.e., AssuEval) of assumptions collected from the TensorFlow and Keras repositories on GitHub; explored the performance of seven traditional machine learning models (e.g., Support Vector Machine, Classification and Regression Trees), a popular DL model (i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#33322;&#29677;&#23618;&#27425;&#30340;&#20056;&#23458;&#27969;&#37327;&#65292;&#30456;&#27604;&#20256;&#32479;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;&#35813;&#27169;&#22411;&#26377;&#25928;&#25972;&#21512;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#25968;&#25454;&#20869;&#37096;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#31354;&#38388;&#20851;&#31995;&#26469;&#22686;&#24378;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.03397</link><description>&lt;p&gt;
&#39044;&#27979;&#22825;&#31354;&#65306;&#19968;&#31181;&#29992;&#20110;&#33322;&#29677;&#23618;&#27425;&#30340;&#20056;&#23458;&#27969;&#37327;&#39044;&#27979;&#30340;&#26032;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Predicting the Skies: A Novel Model for Flight-Level Passenger Traffic Forecasting. (arXiv:2401.03397v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#33322;&#29677;&#23618;&#27425;&#30340;&#20056;&#23458;&#27969;&#37327;&#65292;&#30456;&#27604;&#20256;&#32479;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;&#35813;&#27169;&#22411;&#26377;&#25928;&#25972;&#21512;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#25968;&#25454;&#20869;&#37096;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#31354;&#38388;&#20851;&#31995;&#26469;&#22686;&#24378;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#33322;&#29677;&#23618;&#27425;&#30340;&#20056;&#23458;&#27969;&#37327;&#22312;&#33322;&#31354;&#20844;&#21496;&#36816;&#33829;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#24433;&#21709;&#20174;&#23450;&#20215;&#21040;&#36335;&#32447;&#20248;&#21270;&#31561;&#20851;&#38190;&#20915;&#31574;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#33322;&#29677;&#23618;&#27425;&#20056;&#23458;&#27969;&#37327;&#39044;&#27979;&#30340;&#25361;&#25112;&#65292;&#30456;&#27604;&#20256;&#32479;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#12290;&#21033;&#29992;&#32654;&#22269;&#33322;&#31354;&#20844;&#21496;&#30340;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21560;&#25910;&#20102;&#21382;&#21490;&#27969;&#37327;&#25968;&#25454;&#12289;&#31080;&#20215;&#20851;&#38381;&#20449;&#24687;&#21644;&#27599;&#20010;&#33322;&#29677;&#29305;&#23450;&#30340;&#23395;&#33410;&#24615;&#23646;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#23558;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20248;&#21183;&#36827;&#34892;&#20102;&#25972;&#21512;&#65292;&#21033;&#29992;&#25968;&#25454;&#20869;&#37096;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#31354;&#38388;&#20851;&#31995;&#26469;&#22686;&#24378;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#25104;&#21151;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#20840;&#38754;&#30340;&#25968;&#25454;&#22788;&#29702;&#31574;&#30053;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;3D&#24352;&#37327;&#26469;&#34920;&#31034;&#25968;&#25454;&#65292;&#24212;&#29992;&#20102;&#31934;&#32454;&#30340;&#25513;&#34109;&#31574;&#30053;&#26469;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#65292;&#24182;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate prediction of flight-level passenger traffic is of paramount importance in airline operations, influencing key decisions from pricing to route optimization. This study introduces a novel, multimodal deep learning approach to the challenge of predicting flight-level passenger traffic, yielding substantial accuracy improvements compared to traditional models. Leveraging an extensive dataset from American Airlines, our model ingests historical traffic data, fare closure information, and seasonality attributes specific to each flight. Our proposed neural network integrates the strengths of Recurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN), exploiting the temporal patterns and spatial relationships within the data to enhance prediction performance. Crucial to the success of our model is a comprehensive data processing strategy. We construct 3D tensors to represent data, apply careful masking strategies to mirror real-world dynamics, and employ data augmentatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#23454;&#29616;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#30340;&#23433;&#20840;&#39640;&#25928;&#33258;&#21160;&#39550;&#39542;&#12290;&#35813;&#26041;&#27861;&#23558;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#21516;&#26102;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#20197;&#36991;&#20813;&#20107;&#25925;&#65292;&#24182;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03160</link><description>&lt;p&gt;
&#20154;&#20316;&#20026;AI&#23548;&#24072;&#65306;&#22686;&#24378;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#20197;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving. (arXiv:2401.03160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#23454;&#29616;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#30340;&#23433;&#20840;&#39640;&#25928;&#33258;&#21160;&#39550;&#39542;&#12290;&#35813;&#26041;&#27861;&#23558;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#21516;&#26102;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#20197;&#36991;&#20813;&#20107;&#25925;&#65292;&#24182;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30830;&#20445;AVs&#30340;&#23433;&#20840;&#24615;&#21644;&#20132;&#36890;&#27969;&#25928;&#29575;&#30340;&#39550;&#39542;&#31574;&#30053;&#30340;&#21457;&#23637;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#20154;&#20316;&#20026;AI&#23548;&#24072;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;HAIM-DRL&#65289;&#26694;&#26550;&#65292;&#20197;&#22312;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;&#12290;&#20174;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#26377;&#25928;&#22320;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#65292;&#31216;&#20026;&#20154;&#20316;&#20026;AI&#23548;&#24072;&#65288;HAIM&#65289;&#12290;&#22312;&#36825;&#20010;&#33539;&#24335;&#20013;&#65292;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#20026;AI&#20195;&#29702;&#25552;&#20379;&#24110;&#21161;&#12290;&#22312;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#20805;&#20998;&#25506;&#32034;&#30340;&#21516;&#26102;&#65292;&#20154;&#31867;&#19987;&#23478;&#21487;&#20197;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#65292;&#24182;&#23637;&#31034;&#27491;&#30830;&#30340;&#34892;&#21160;&#20197;&#36991;&#20813;&#28508;&#22312;&#20107;&#25925;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21487;&#20197;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20174;&#32780;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant progress in autonomous vehicles (AVs), the development of driving policies that ensure both the safety of AVs and traffic flow efficiency has not yet been fully explored. In this paper, we propose an enhanced human-in-the-loop reinforcement learning method, termed the Human as AI mentor-based deep reinforcement learning (HAIM-DRL) framework, which facilitates safe and efficient autonomous driving in mixed traffic platoon. Drawing inspiration from the human learning process, we first introduce an innovative learning paradigm that effectively injects human intelligence into AI, termed Human as AI mentor (HAIM). In this paradigm, the human expert serves as a mentor to the AI agent. While allowing the agent to sufficiently explore uncertain environments, the human expert can take control in dangerous situations and demonstrate correct actions to avoid potential accidents. On the other hand, the agent could be guided to minimize traffic flow disturbance, thereby optimizi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22788;&#29702;&#22810;&#30446;&#26631;&#36319;&#36394;&#30340;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22312;&#26234;&#33021;&#20307;&#25968;&#37327;&#23569;&#20110;&#30446;&#26631;&#25968;&#37327;&#26102;&#23454;&#29616;&#20027;&#21160;&#25628;&#32034;&#21644;&#36319;&#36394;&#65292;&#24182;&#20351;&#29992;&#24322;&#27493;&#26234;&#33021;&#20307;&#36890;&#20449;&#26469;&#21327;&#35843;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2401.03154</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#20027;&#21160;&#25628;&#32034;&#21644;&#36319;&#36394;&#24403;&#30446;&#26631;&#36229;&#36807;&#26234;&#33021;&#20307;&#25968;&#37327;&#26102;
&lt;/p&gt;
&lt;p&gt;
Decentralized Multi-Agent Active Search and Tracking when Targets Outnumber Agents. (arXiv:2401.03154v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03154
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22788;&#29702;&#22810;&#30446;&#26631;&#36319;&#36394;&#30340;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22312;&#26234;&#33021;&#20307;&#25968;&#37327;&#23569;&#20110;&#30446;&#26631;&#25968;&#37327;&#26102;&#23454;&#29616;&#20027;&#21160;&#25628;&#32034;&#21644;&#36319;&#36394;&#65292;&#24182;&#20351;&#29992;&#24322;&#27493;&#26234;&#33021;&#20307;&#36890;&#20449;&#26469;&#21327;&#35843;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#22810;&#30446;&#26631;&#36319;&#36394;&#22312;&#37326;&#29983;&#21160;&#29289;&#24033;&#36923;&#12289;&#23433;&#20840;&#30417;&#25511;&#25110;&#29615;&#22659;&#30417;&#27979;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#29616;&#26377;&#31639;&#27861;&#24120;&#24120;&#20570;&#20986;&#19968;&#20123;&#38480;&#21046;&#24615;&#20551;&#35774;&#65306;&#30446;&#26631;&#25968;&#37327;&#21644;&#21021;&#22987;&#20301;&#32622;&#24050;&#30693;&#65292;&#25110;&#32773;&#26234;&#33021;&#20307;&#24050;&#34987;&#39044;&#20998;&#37197;&#21040;&#30417;&#25511;&#29615;&#22659;&#30340;&#19981;&#37325;&#21472;&#20998;&#21306;&#65292;&#20943;&#36731;&#20102;&#25506;&#32034;&#30340;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#24403;&#26234;&#33021;&#20307;&#25968;&#37327;&#23569;&#20110;&#30446;&#26631;&#25968;&#37327;&#26102;&#65292;&#36825;&#31181;&#20551;&#35774;&#20250;&#38480;&#21046;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#22240;&#20026;&#26234;&#33021;&#20307;&#26080;&#27861;&#25345;&#32493;&#36319;&#36394;&#20854;&#35270;&#37326;&#20013;&#30340;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#22810;&#26234;&#33021;&#20307;&#36319;&#36394;&#31639;&#27861;&#36824;&#20551;&#35774;&#26234;&#33021;&#20307;&#38388;&#35266;&#27979;&#30340;&#21516;&#27493;&#65292;&#25110;&#32773;&#38656;&#35201;&#19968;&#20010;&#20013;&#22830;&#25511;&#21046;&#22120;&#26469;&#21327;&#35843;&#32852;&#21512;&#21160;&#20316;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20851;&#27880;&#20110;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#12289;&#22810;&#30446;&#26631;&#12289;&#21516;&#26102;&#20027;&#21160;&#25628;&#32034;&#21644;&#36319;&#36394;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#38388;&#36890;&#20449;&#26159;&#24322;&#27493;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;DecSTER&#20351;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#20551;&#35774;&#23494;&#24230;&#28388;&#27874;&#22120;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent multi-target tracking has a wide range of applications, including wildlife patrolling, security surveillance or environment monitoring. Such algorithms often make restrictive assumptions: the number of targets and/or their initial locations may be assumed known, or agents may be pre-assigned to monitor disjoint partitions of the environment, reducing the burden of exploration. This also limits applicability when there are fewer agents than targets, since agents are unable to continuously follow the targets in their fields of view. Multi-agent tracking algorithms additionally assume inter-agent synchronization of observations, or the presence of a central controller to coordinate joint actions. Instead, we focus on the setting of decentralized multi-agent, multi-target, simultaneous active search-and-tracking with asynchronous inter-agent communication. Our proposed algorithm DecSTER uses a sequential monte carlo implementation of the probability hypothesis density filter fo
&lt;/p&gt;</description></item><item><title>Powerformer&#26159;&#19968;&#31181;&#36866;&#24212;&#19981;&#21516;&#20256;&#36755;&#21306;&#27573;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#31283;&#20581;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#34920;&#31034;&#12290;&#23427;&#36890;&#36807;&#24320;&#21457;&#19987;&#29992;&#30340;&#21306;&#27573;&#33258;&#36866;&#24212;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#24341;&#20837;&#22270;&#31070;&#32463;&#32593;&#32476;&#20256;&#25773;&#21644;&#22810;&#22240;&#32032;&#27880;&#24847;&#26426;&#21046;&#26469;&#25552;&#20379;&#26356;&#21152;&#31283;&#20581;&#30340;&#29366;&#24577;&#34920;&#31034;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#30005;&#21147;&#31995;&#32479;&#22330;&#26223;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.02771</link><description>&lt;p&gt;
Powerformer&#65306;&#36866;&#24212;&#19981;&#21516;&#20256;&#36755;&#21306;&#27573;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#29992;&#20110;&#30005;&#21147;&#27969;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Powerformer: A Section-adaptive Transformer for Power Flow Adjustment. (arXiv:2401.02771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02771
&lt;/p&gt;
&lt;p&gt;
Powerformer&#26159;&#19968;&#31181;&#36866;&#24212;&#19981;&#21516;&#20256;&#36755;&#21306;&#27573;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#31283;&#20581;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#34920;&#31034;&#12290;&#23427;&#36890;&#36807;&#24320;&#21457;&#19987;&#29992;&#30340;&#21306;&#27573;&#33258;&#36866;&#24212;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#24341;&#20837;&#22270;&#31070;&#32463;&#32593;&#32476;&#20256;&#25773;&#21644;&#22810;&#22240;&#32032;&#27880;&#24847;&#26426;&#21046;&#26469;&#25552;&#20379;&#26356;&#21152;&#31283;&#20581;&#30340;&#29366;&#24577;&#34920;&#31034;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#30005;&#21147;&#31995;&#32479;&#22330;&#26223;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#23398;&#20064;&#31283;&#20581;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#34920;&#31034;&#32780;&#37327;&#36523;&#23450;&#21046;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#26088;&#22312;&#20248;&#21270;&#36328;&#19981;&#21516;&#20256;&#36755;&#21306;&#27573;&#30340;&#30005;&#21147;&#35843;&#24230;&#20197;&#36827;&#34892;&#30005;&#21147;&#27969;&#35843;&#25972;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26041;&#27861;&#21517;&#20026;Powerformer&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#19987;&#29992;&#30340;&#21306;&#27573;&#33258;&#36866;&#24212;&#27880;&#24847;&#26426;&#21046;&#65292;&#19982;&#20256;&#32479;&#21464;&#21387;&#22120;&#20013;&#20351;&#29992;&#30340;&#33258;&#27880;&#24847;&#20998;&#31163;&#24320;&#26469;&#12290;&#35813;&#26426;&#21046;&#26377;&#25928;&#22320;&#23558;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#19982;&#20256;&#36755;&#21306;&#27573;&#20449;&#24687;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#31283;&#20581;&#30340;&#29366;&#24577;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32771;&#34385;&#30005;&#21147;&#31995;&#32479;&#30340;&#22270;&#25299;&#25169;&#21644;&#27597;&#32447;&#33410;&#28857;&#30340;&#30005;&#27668;&#23646;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#23450;&#21046;&#31574;&#30053;&#26469;&#36827;&#19968;&#27493;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#20256;&#25773;&#21644;&#22810;&#22240;&#32032;&#27880;&#24847;&#26426;&#21046;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#30005;&#21147;&#31995;&#32479;&#22330;&#26223;&#65288;&#21253;&#25324;IEEE 118&#33410;&#28857;&#31995;&#32479;&#12289;&#20013;&#22269;&#23454;&#38469;300&#33410;&#28857;&#31995;&#32479;&#21644;&#19968;&#20010;&#22823;&#22411;&#31995;&#32479;&#65289;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel transformer architecture tailored for learning robust power system state representations, which strives to optimize power dispatch for the power flow adjustment across different transmission sections. Specifically, our proposed approach, named Powerformer, develops a dedicated section-adaptive attention mechanism, separating itself from the self-attention used in conventional transformers. This mechanism effectively integrates power system states with transmission section information, which facilitates the development of robust state representations. Furthermore, by considering the graph topology of power system and the electrical attributes of bus nodes, we introduce two customized strategies to further enhance the expressiveness: graph neural network propagation and multi-factor attention mechanism. Extensive evaluations are conducted on three power system scenarios, including the IEEE 118-bus system, a realistic 300-bus system in China, and a large-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#22810;&#20809;&#35889;&#26080;&#20154;&#26426;&#24433;&#20687;&#21644;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#26680;&#26691;&#27700;&#20998;&#32961;&#36843;&#27979;&#32472;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#26080;&#20154;&#26426;&#24433;&#20687;&#21644;&#22825;&#27668;&#25968;&#25454;&#65292;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#26377;&#25928;&#20272;&#35745;&#20102;&#26680;&#26691;&#26641;&#30340;&#33550;&#27700;&#21183;&#65292;&#20026;&#26680;&#26691;&#31934;&#20934;&#28748;&#28297;&#31649;&#29702;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21442;&#32771;&#20381;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.01375</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#22810;&#20809;&#35889;&#26080;&#20154;&#26426;&#24433;&#20687;&#21644;&#26426;&#22120;&#23398;&#20064;&#26469;&#36827;&#34892;&#26680;&#26691;&#27700;&#20998;&#32961;&#36843;&#30340;&#27979;&#32472;
&lt;/p&gt;
&lt;p&gt;
Mapping Walnut water Stress with High Resolution Multispectral UAV Imagery and Machine Learning. (arXiv:2401.01375v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#22810;&#20809;&#35889;&#26080;&#20154;&#26426;&#24433;&#20687;&#21644;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#26680;&#26691;&#27700;&#20998;&#32961;&#36843;&#27979;&#32472;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#26080;&#20154;&#26426;&#24433;&#20687;&#21644;&#22825;&#27668;&#25968;&#25454;&#65292;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#26377;&#25928;&#20272;&#35745;&#20102;&#26680;&#26691;&#26641;&#30340;&#33550;&#27700;&#21183;&#65292;&#20026;&#26680;&#26691;&#31934;&#20934;&#28748;&#28297;&#31649;&#29702;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#21442;&#32771;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30417;&#27979;&#26680;&#26691;&#30340;&#27700;&#20998;&#29366;&#24577;&#21644;&#32961;&#36843;&#27700;&#24179;&#23545;&#20110;&#21152;&#21033;&#31119;&#23612;&#20122;&#24030;&#37325;&#35201;&#20892;&#20316;&#29289;&#26680;&#26691;&#30340;&#31934;&#20934;&#28748;&#28297;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#27169;&#22411;&#32467;&#21512;&#26080;&#20154;&#26426;&#33322;&#25293;&#30340;&#39640;&#20998;&#36776;&#29575;&#22810;&#20809;&#35889;&#36965;&#24863;&#24433;&#20687;&#21644;&#22825;&#27668;&#25968;&#25454;&#26469;&#27979;&#32472;&#33550;&#27700;&#21183;&#65288;SWP&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#20174;2017&#24180;&#21040;2018&#24180;&#65292;&#20351;&#29992;&#19968;&#26550;&#35013;&#22791;&#26377;&#19971;&#27874;&#27573;&#22810;&#20809;&#35889;&#30456;&#26426;&#30340;&#26080;&#20154;&#26426;&#65292;&#22312;&#19968;&#20010;&#21830;&#19994;&#26680;&#26691;&#22253;&#36827;&#34892;&#20102;&#20116;&#27425;&#39134;&#34892;&#65292;&#21516;&#26102;&#20276;&#38543;&#23545;&#25277;&#26679;&#26680;&#26691;&#26893;&#26666;&#30340;&#22320;&#38754;&#27979;&#37327;&#12290;RF&#22238;&#24402;&#27169;&#22411;&#21033;&#29992;&#26469;&#33258;&#27491;&#23556;&#26080;&#20154;&#26426;&#24433;&#20687;&#21644;&#22825;&#27668;&#25968;&#25454;&#30340;&#26893;&#34987;&#25351;&#25968;&#65292;&#26377;&#25928;&#22320;&#20272;&#35745;&#20102;&#22320;&#38754;&#27979;&#37327;&#30340;SWPs&#65292;&#36798;&#21040;&#20102;0.63&#30340;R^2&#20540;&#21644;0.80&#24052;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#12290;&#22825;&#27668;&#25968;&#25454;&#30340;&#25972;&#21512;&#23588;&#20026;&#37325;&#35201;&#65292;&#20197;&#25972;&#21512;&#19981;&#21516;&#39134;&#34892;&#26085;&#26399;&#30340;&#25968;&#25454;&#12290;&#26174;&#33879;&#30340;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Effective monitoring of walnut water status and stress level across the whole orchard is an essential step towards precision irrigation management of walnuts, a significant crop in California. This study presents a machine learning approach using Random Forest (RF) models to map stem water potential (SWP) by integrating high-resolution multispectral remote sensing imagery from Unmanned Aerial Vehicle (UAV) flights with weather data. From 2017 to 2018, five flights of an UAV equipped with a seven-band multispectral camera were conducted over a commercial walnut orchard, paired with concurrent ground measurements of sampled walnut plants. The RF regression model, utilizing vegetation indices derived from orthomosaiced UAV imagery and weather data, effectively estimated ground-measured SWPs, achieving an $R^2$ of 0.63 and a mean absolute error (MAE) of 0.80 bars. The integration of weather data was particularly crucial for consolidating data across various flight dates. Significant variab
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#31181;&#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#32858;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#26410;&#26469;&#20540;&#24182;&#24418;&#25104;&#32858;&#31867;&#31354;&#38388;&#26469;&#21019;&#24314;&#24739;&#32773;&#36164;&#26009;&#65292;&#20854;&#20013;&#19968;&#31181;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#21160;&#24577;&#32676;&#32452;&#24402;&#23646;&#12290;</title><link>http://arxiv.org/abs/2312.17286</link><description>&lt;p&gt;
&#36830;&#25509;&#21307;&#30103;&#35774;&#22791;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#32858;&#31867;&#27169;&#22411;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparative study of clustering models for multivariate time series from connected medical devices. (arXiv:2312.17286v2 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17286
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#31181;&#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#32858;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#26410;&#26469;&#20540;&#24182;&#24418;&#25104;&#32858;&#31867;&#31354;&#38388;&#26469;&#21019;&#24314;&#24739;&#32773;&#36164;&#26009;&#65292;&#20854;&#20013;&#19968;&#31181;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#21160;&#24577;&#32676;&#32452;&#24402;&#23646;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#24739;&#32773;&#25968;&#25454;&#36890;&#24120;&#20197;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#24418;&#24335;&#25910;&#38598;&#65292;&#21487;&#20197;&#20840;&#38754;&#22320;&#21453;&#26144;&#24739;&#32773;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#20581;&#24247;&#29366;&#20917;&#12290;&#34429;&#28982;&#36825;&#20123;&#25968;&#25454;&#21487;&#33021;&#26159;&#31232;&#30095;&#30340;&#65292;&#20294;&#36830;&#25509;&#30340;&#35774;&#22791;&#21487;&#20197;&#22686;&#21152;&#25968;&#25454;&#39057;&#29575;&#12290;&#30446;&#26631;&#26159;&#20174;&#36825;&#20123;&#26102;&#38388;&#24207;&#21015;&#20013;&#21019;&#24314;&#24739;&#32773;&#36164;&#26009;&#12290;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;&#39044;&#27979;&#27169;&#22411;&#26469;&#39044;&#27979;&#26410;&#26469;&#20540;&#65292;&#21516;&#26102;&#24418;&#25104;&#19968;&#20010;&#28508;&#22312;&#30340;&#32858;&#31867;&#31354;&#38388;&#65292;&#24182;&#20197;&#39044;&#27979;&#24615;&#33021;&#20026;&#35780;&#20215;&#25351;&#26631;&#12290;&#25105;&#20204;&#22312;Withing&#30340;&#25968;&#25454;&#38598;&#19978;&#27604;&#36739;&#20102;&#20004;&#20010;&#27169;&#22411;&#65292;M AGMAC LUST&#21487;&#20197;&#23545;&#25972;&#20010;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32858;&#31867;&#65292;&#32780;DGM${}^2$&#20801;&#35768;&#20010;&#20307;&#30340;&#32676;&#32452;&#24402;&#23646;&#38543;&#26102;&#38388;&#21464;&#21270;&#65288;&#21160;&#24577;&#32858;&#31867;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In healthcare, patient data is often collected as multivariate time series, providing a comprehensive view of a patient's health status over time. While this data can be sparse, connected devices may enhance its frequency. The goal is to create patient profiles from these time series. In the absence of labels, a predictive model can be used to predict future values while forming a latent cluster space, evaluated based on predictive performance. We compare two models on Withing's datasets, M AGMAC LUST which clusters entire time series and DGM${}^2$ which allows the group affiliation of an individual to change over time (dynamic clustering).
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BOSS&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20248;&#21270;&#27493;&#38271;&#21644;&#29983;&#25104;&#36335;&#24452;&#65292;&#25552;&#21319;&#20102;&#20302;&#36164;&#28304;&#22330;&#26223;&#19979;&#27969;&#21305;&#37197;&#29983;&#25104;&#27169;&#22411;&#30340;&#22270;&#20687;&#36136;&#37327;&#21644;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2312.16414</link><description>&lt;p&gt;
Bellman&#26368;&#20339;&#27493;&#38271;&#30452;&#32447;&#21270;&#27969;&#21305;&#37197;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bellman Optimal Step-size Straightening of Flow-Matching Models. (arXiv:2312.16414v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;BOSS&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20248;&#21270;&#27493;&#38271;&#21644;&#29983;&#25104;&#36335;&#24452;&#65292;&#25552;&#21319;&#20102;&#20302;&#36164;&#28304;&#22330;&#26223;&#19979;&#27969;&#21305;&#37197;&#29983;&#25104;&#27169;&#22411;&#30340;&#22270;&#20687;&#36136;&#37327;&#21644;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#21305;&#37197;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#23588;&#20854;&#26159;&#22270;&#20687;&#21512;&#25104;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24378;&#22823;&#35745;&#31639;&#38656;&#27714;&#65292;&#23588;&#20854;&#22312;&#24494;&#35843;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#20013;&#65292;&#32473;&#20302;&#36164;&#28304;&#22330;&#26223;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;Bellman&#26368;&#20339;&#27493;&#38271;&#30452;&#32447;&#21270;&#65288;BOSS&#65289;&#25216;&#26415;&#26469;&#25552;&#28860;&#27969;&#21305;&#37197;&#29983;&#25104;&#27169;&#22411;&#65306;&#23427;&#38024;&#23545;&#30340;&#26159;&#22312;&#35745;&#31639;&#39044;&#31639;&#32422;&#26463;&#19979;&#36827;&#34892;&#23569;&#25968;&#27493;&#39588;&#30340;&#39640;&#25928;&#22270;&#20687;&#37319;&#26679;&#12290;&#39318;&#20808;&#65292;&#35813;&#25216;&#26415;&#28041;&#21450;&#19968;&#20010;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#65292;&#20248;&#21270;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#27493;&#38271;&#12290;&#28982;&#21518;&#65292;&#23427;&#36890;&#36807;&#20248;&#21270;&#36895;&#24230;&#32593;&#32476;&#20197;&#21305;&#37197;&#26368;&#20339;&#27493;&#38271;&#26469;&#25913;&#36827;&#29983;&#25104;&#36335;&#24452;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;BOSS&#22312;&#36164;&#28304;&#21033;&#29992;&#21644;&#22270;&#20687;&#36136;&#37327;&#26041;&#38754;&#37117;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;BOSS&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flow matching is a powerful framework for generating high-quality samples in various applications, especially image synthesis. However, the intensive computational demands of these models, especially during the fine-tuning process and sampling processes, pose significant challenges for low-resource scenarios. This paper introduces Bellman Optimal Step-size Straightening (BOSS) technique for distilling flow-matching generative models: it aims specifically for a few-step efficient image sampling while adhering to a computational budget constraint. First, this technique involves a dynamic programming algorithm that optimizes the step sizes of the pretrained network. Then, it refines the velocity network to match the optimal step sizes, aiming to straighten the generation paths. Extensive experimental evaluations across image generation tasks demonstrate the efficacy of BOSS in terms of both resource utilization and image quality. Our results reveal that BOSS achieves substantial gains in 
&lt;/p&gt;</description></item><item><title>I-CEE&#26159;&#19968;&#20010;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#20026;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;&#23450;&#21046;&#20102;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#36890;&#36807;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#30340;&#31034;&#20363;&#22270;&#20687;&#12289;&#23616;&#37096;&#35299;&#37322;&#21644;&#27169;&#22411;&#20915;&#31574;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2312.12102</link><description>&lt;p&gt;
I-CEE: &#23558;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#35299;&#37322;&#23450;&#21046;&#20026;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
I-CEE: Tailoring Explanations of Image Classification Models to User Expertise. (arXiv:2312.12102v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12102
&lt;/p&gt;
&lt;p&gt;
I-CEE&#26159;&#19968;&#20010;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#20026;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;&#23450;&#21046;&#20102;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#36890;&#36807;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#30340;&#31034;&#20363;&#22270;&#20687;&#12289;&#23616;&#37096;&#35299;&#37322;&#21644;&#27169;&#22411;&#20915;&#31574;&#26469;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#35299;&#37322;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#23545;&#20110;&#20381;&#36182;&#23427;&#20204;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#36127;&#36131;&#20219;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#35782;&#21035;&#21040;&#20854;&#37325;&#35201;&#24615;&#65292;&#21487;&#20197;&#29983;&#25104;&#36825;&#20123;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#25552;&#20379;&#20102;&#20960;&#31181;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#19968;&#19981;&#26029;&#21457;&#23637;&#30340;&#24037;&#20316;&#20013;&#65292;&#23545;&#29992;&#25143;&#65288;&#35299;&#37322;&#23545;&#35937;&#65289;&#30340;&#20851;&#27880;&#30456;&#23545;&#36739;&#23569;&#65292;&#22823;&#22810;&#25968;XAI&#25216;&#26415;&#20135;&#29983;&#30340;&#26159;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#23454;&#29616;&#26356;&#21152;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;XAI&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;I-CEE&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;&#23450;&#21046;&#22270;&#20687;&#20998;&#31867;&#35299;&#37322;&#30340;&#26694;&#26550;&#12290;&#21463;&#21040;&#29616;&#26377;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;I-CEE&#36890;&#36807;&#20026;&#29992;&#25143;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#65288;&#21363;&#31034;&#20363;&#22270;&#20687;&#65289;&#12289;&#30456;&#24212;&#30340;&#23616;&#37096;&#35299;&#37322;&#21644;&#27169;&#22411;&#20915;&#31574;&#26469;&#35299;&#37322;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#19982;&#27492;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;I-CEE&#27169;&#25311;&#20102;&#31034;&#20363;&#22270;&#20687;&#30340;&#20449;&#24687;&#37327;&#20381;&#36182;&#20110;&#29992;&#25143;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#20026;&#19981;&#21516;&#30340;&#29992;&#25143;&#25552;&#20379;&#19981;&#21516;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effectively explaining decisions of black-box machine learning models is critical to responsible deployment of AI systems that rely on them. Recognizing their importance, the field of explainable AI (XAI) provides several techniques to generate these explanations. Yet, there is relatively little emphasis on the user (the explainee) in this growing body of work and most XAI techniques generate "one-size-fits-all" explanations. To bridge this gap and achieve a step closer towards human-centered XAI, we present I-CEE, a framework that provides Image Classification Explanations tailored to User Expertise. Informed by existing work, I-CEE explains the decisions of image classification models by providing the user with an informative subset of training data (i.e., example images), corresponding local explanations, and model decisions. However, unlike prior work, I-CEE models the informativeness of the example images to depend on user expertise, resulting in different examples for different u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#23884;&#20837;&#22312;&#38750;&#27431;&#20960;&#37324;&#24503;&#27969;&#24418;&#31354;&#38388;&#20013;&#30340;&#31354;&#38388;&#32593;&#32476;&#30340;&#34920;&#31034;&#65292;&#36890;&#36807;&#25552;&#21462;&#36793;&#19978;&#30340;&#28040;&#24687;&#23558;&#22270;&#25299;&#25169;&#21644;&#31354;&#38388;&#20960;&#20309;&#32467;&#21512;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2312.10808</link><description>&lt;p&gt;
&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Non-Euclidean Spatial Graph Neural Network. (arXiv:2312.10808v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#23884;&#20837;&#22312;&#38750;&#27431;&#20960;&#37324;&#24503;&#27969;&#24418;&#31354;&#38388;&#20013;&#30340;&#31354;&#38388;&#32593;&#32476;&#30340;&#34920;&#31034;&#65292;&#36890;&#36807;&#25552;&#21462;&#36793;&#19978;&#30340;&#28040;&#24687;&#23558;&#22270;&#25299;&#25169;&#21644;&#31354;&#38388;&#20960;&#20309;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#32593;&#32476;&#26159;&#20854;&#22270;&#25299;&#25169;&#21463;&#23884;&#20837;&#31354;&#38388;&#32422;&#26463;&#30340;&#32593;&#32476;&#12290;&#29702;&#35299;&#32806;&#21512;&#30340;&#31354;&#38388;&#22270;&#23646;&#24615;&#23545;&#20110;&#20174;&#31354;&#38388;&#32593;&#32476;&#20013;&#25552;&#21462;&#24378;&#22823;&#34920;&#31034;&#38750;&#24120;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#20165;&#20165;&#32467;&#21512;&#20010;&#21035;&#30340;&#31354;&#38388;&#21644;&#32593;&#32476;&#34920;&#31034;&#26080;&#27861;&#25581;&#31034;&#31354;&#38388;&#32593;&#32476;&#30340;&#28508;&#22312;&#20132;&#20114;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#31354;&#38388;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#21482;&#33021;&#32771;&#34385;&#23884;&#20837;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#32593;&#32476;&#65292;&#26080;&#27861;&#24456;&#22909;&#22320;&#21033;&#29992;&#19981;&#35268;&#21017;&#21644;&#38750;&#22343;&#21248;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#25152;&#25658;&#24102;&#30340;&#20016;&#23500;&#20960;&#20309;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#23398;&#20064;&#23884;&#20837;&#22312;&#38750;&#27431;&#20960;&#37324;&#24503;&#27969;&#24418;&#31354;&#38388;&#20013;&#30340;&#31354;&#38388;&#32593;&#32476;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#32467;&#21512;&#22270;&#25299;&#25169;&#21644;&#31354;&#38388;&#20960;&#20309;&#65292;&#20854;&#20013;&#31354;&#38388;&#20960;&#20309;&#34987;&#25552;&#21462;&#20026;&#36793;&#19978;&#30340;&#28040;&#24687;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20445;&#35777;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Spatial networks are networks whose graph topology is constrained by their embedded spatial space. Understanding the coupled spatial-graph properties is crucial for extracting powerful representations from spatial networks. Therefore, merely combining individual spatial and network representations cannot reveal the underlying interaction mechanism of spatial networks. Besides, existing spatial network representation learning methods can only consider networks embedded in Euclidean space, and can not well exploit the rich geometric information carried by irregular and non-uniform non-Euclidean space. In order to address this issue, in this paper we propose a novel generic framework to learn the representation of spatial networks that are embedded in non-Euclidean manifold space. Specifically, a novel message-passing-based neural network is proposed to combine graph topology and spatial geometry, where spatial geometry is extracted as messages on the edges. We theoretically guarantee tha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#20855;&#26377;&#23454;&#20363;&#30456;&#20851;&#22122;&#22768;&#30340;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedBeat&#30340;&#26032;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;&#23454;&#20363;&#30456;&#20851;&#22122;&#22768;&#36716;&#31227;&#30697;&#38453;&#26500;&#24314;&#20840;&#23616;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;&#32852;&#37030;&#25968;&#25454;&#25552;&#21462;&#21644;&#36716;&#31227;&#30697;&#38453;&#20272;&#35745;&#20004;&#20010;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2312.10324</link><description>&lt;p&gt;
&#20855;&#26377;&#23454;&#20363;&#30456;&#20851;&#22122;&#22768;&#26631;&#31614;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning with Instance-Dependent Noisy Label. (arXiv:2312.10324v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#20855;&#26377;&#23454;&#20363;&#30456;&#20851;&#22122;&#22768;&#30340;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedBeat&#30340;&#26032;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#20351;&#29992;&#23454;&#20363;&#30456;&#20851;&#22122;&#22768;&#36716;&#31227;&#30697;&#38453;&#26500;&#24314;&#20840;&#23616;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;&#32852;&#37030;&#25968;&#25454;&#25552;&#21462;&#21644;&#36716;&#31227;&#30697;&#38453;&#20272;&#35745;&#20004;&#20010;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#22788;&#29702;&#20013;&#24515;&#21270;&#23398;&#20064;&#20013;&#22122;&#22768;&#26631;&#31614;&#30340;&#26041;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#24448;&#24448;&#22833;&#25928;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#25968;&#25454;&#38598;&#35268;&#27169;&#23567;&#21644;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#23581;&#35797;&#20102;&#19968;&#20123;&#35299;&#20915;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20294;&#20027;&#35201;&#38598;&#20013;&#22312;&#28041;&#21450;&#31867;&#26465;&#20214;&#22122;&#22768;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#32852;&#37030;&#23398;&#20064;&#20013;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#23454;&#38469;&#38382;&#39064;&#65292;&#21363;&#23454;&#20363;&#30456;&#20851;&#22122;&#22768;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;FedBeat&#65288;&#32852;&#37030;&#23398;&#20064;&#19982;&#36125;&#21494;&#26031;&#38598;&#25104;&#36741;&#21161;&#36716;&#31227;&#30697;&#38453;&#20272;&#35745;&#65289;&#65292;&#26088;&#22312;&#20351;&#29992;&#23454;&#20363;&#30456;&#20851;&#22122;&#22768;&#36716;&#31227;&#30697;&#38453;&#65288;IDNTM&#65289;&#26500;&#24314;&#20840;&#23616;&#32479;&#19968;&#30340;&#20998;&#31867;&#22120;&#12290;&#35813;&#31639;&#27861;&#21253;&#25324;&#19977;&#20010;&#21327;&#21516;&#27493;&#39588;&#65306;&#65288;1&#65289;&#32852;&#37030;&#25968;&#25454;&#25552;&#21462;&#27493;&#39588;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#26500;&#24314;&#24369;&#20840;&#23616;&#27169;&#22411;&#24182;&#25552;&#21462;&#39640;&#32622;&#20449;&#24230;&#25968;&#25454;&#65307;&#65288;2&#65289;&#32852;&#37030;&#36716;&#31227;&#30697;&#38453;&#20272;&#35745;&#27493;&#39588;&#65307; in
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) with noisy labels poses a significant challenge. Existing methods designed for handling noisy labels in centralized learning tend to lose their effectiveness in the FL setting, mainly due to the small dataset size and the heterogeneity of client data. While some attempts have been made to tackle FL with noisy labels, they primarily focused on scenarios involving class-conditional noise. In this paper, we study the more challenging and practical issue of instance-dependent noise (IDN) in FL. We introduce a novel algorithm called FedBeat (Federated Learning with Bayesian Ensemble-Assisted Transition Matrix Estimation). FedBeat aims to build a global statistically consistent classifier using the IDN transition matrix (IDNTM), which encompasses three synergistic steps: (1) A federated data extraction step that constructs a weak global model and extracts high-confidence data using a Bayesian model ensemble method. (2) A federated transition matrix estimation step in 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;$\beta$-Predictive Bayes&#30340;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#39044;&#27979;&#21518;&#39564;&#30340;&#28151;&#21512;&#21644;&#20056;&#31215;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#65292;&#36890;&#36807;&#35843;&#25972;&#21442;&#25968;$\beta$&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.09817</link><description>&lt;p&gt;
&#22312;&#39044;&#27979;&#31354;&#38388;&#20013;&#24102;&#26377;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26657;&#20934;&#19968;&#36718;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Calibrated One Round Federated Learning with Bayesian Inference in the Predictive Space. (arXiv:2312.09817v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;$\beta$-Predictive Bayes&#30340;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#39044;&#27979;&#21518;&#39564;&#30340;&#28151;&#21512;&#21644;&#20056;&#31215;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#65292;&#36890;&#36807;&#35843;&#25972;&#21442;&#25968;$\beta$&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#25351;&#22312;&#20998;&#24067;&#22312;&#23458;&#25143;&#31471;&#20013;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#38598;&#26159;&#26412;&#22320;&#21270;&#19988;&#21487;&#33021;&#26159;&#24322;&#36136;&#30340;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#23567;&#32780;&#22122;&#22768;&#30340;&#25968;&#25454;&#38598;&#24456;&#24120;&#35265;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#33021;&#22815;&#34920;&#31034;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#33391;&#22909;&#26657;&#20934;&#27169;&#22411;&#12290;&#26368;&#25509;&#36817;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#26159;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#20174;&#23616;&#37096;&#21518;&#39564;&#20013;&#25910;&#38598;&#21442;&#25968;&#26679;&#26412;&#65292;&#24182;&#23558;&#23427;&#20204;&#32858;&#21512;&#20197;&#36817;&#20284;&#20840;&#23616;&#21518;&#39564;&#12290;&#20026;&#20102;&#25552;&#39640;&#26356;&#22823;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#36125;&#21494;&#26031;&#26041;&#27861;&#36890;&#24120;&#26159;&#36890;&#36807;&#23558;&#23616;&#37096;&#39044;&#27979;&#21518;&#39564;&#30456;&#20056;&#26469;&#36817;&#20284;&#20840;&#23616;&#39044;&#27979;&#21518;&#39564;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#23548;&#33268;&#31995;&#32479;&#24615;&#30340;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$\beta$-Predictive Bayes&#30340;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#22312;&#39044;&#27979;&#21518;&#39564;&#30340;&#28151;&#21512;&#21644;&#20056;&#31215;&#20043;&#38388;&#36827;&#34892;&#25554;&#20540;&#65292;&#20351;&#29992;&#19968;&#20010;&#21487;&#35843;&#21442;&#25968;$\beta$&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) involves training a model over a dataset distributed among clients, with the constraint that each client's dataset is localized and possibly heterogeneous. In FL, small and noisy datasets are common, highlighting the need for well-calibrated models that represent the uncertainty of predictions. The closest FL techniques to achieving such goals are the Bayesian FL methods which collect parameter samples from local posteriors, and aggregate them to approximate the global posterior. To improve scalability for larger models, one common Bayesian approach is to approximate the global predictive posterior by multiplying local predictive posteriors. In this work, we demonstrate that this method gives systematically overconfident predictions, and we remedy this by proposing $\beta$-Predictive Bayes, a Bayesian FL algorithm that interpolates between a mixture and product of the predictive posteriors, using a tunable parameter $\beta$. This parameter is tuned to improve th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; KwaiAgents&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#20449;&#24687;&#25628;&#32034;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#30693;&#26680;&#24515;&#65292;&#29702;&#35299;&#29992;&#25143;&#30340;&#26597;&#35810;&#65292;&#34892;&#20026;&#20934;&#21017;&#24182;&#21442;&#32771;&#22806;&#37096;&#25991;&#26723;&#65292;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2312.04889</link><description>&lt;p&gt;
KwaiAgents&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#20449;&#24687;&#25628;&#32034;&#26234;&#33021;&#20307;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
KwaiAgents: Generalized Information-seeking Agent System with Large Language Models. (arXiv:2312.04889v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; KwaiAgents&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#20449;&#24687;&#25628;&#32034;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#30693;&#26680;&#24515;&#65292;&#29702;&#35299;&#29992;&#25143;&#30340;&#26597;&#35810;&#65292;&#34892;&#20026;&#20934;&#21017;&#24182;&#21442;&#32771;&#22806;&#37096;&#25991;&#26723;&#65292;&#20197;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30001;&#20110;&#22909;&#22855;&#24515;&#30340;&#39537;&#20351;&#65292;&#19981;&#26029;&#25506;&#32034;&#21644;&#29702;&#35299;&#21608;&#22260;&#30340;&#19990;&#30028;&#65292;&#20174;&#32780;&#21457;&#26126;&#20102;&#21508;&#31181;&#24037;&#20855;&#26469;&#28385;&#36275;&#36825;&#31181;&#22909;&#22855;&#24515;&#12290;&#23613;&#31649;&#20154;&#31867;&#26080;&#27861;&#22312;&#22823;&#33041;&#20013;&#22788;&#29702;&#21644;&#35760;&#24518;&#22823;&#37327;&#20449;&#24687;&#65292;&#20294;&#22312;&#25209;&#21028;&#24605;&#32500;&#12289;&#35268;&#21010;&#12289;&#21453;&#24605;&#20197;&#21450;&#21033;&#29992;&#29616;&#26377;&#24037;&#20855;&#19982;&#19990;&#30028;&#36827;&#34892;&#20132;&#20114;&#21644;&#35299;&#37322;&#26041;&#38754;&#21331;&#36234;&#20986;&#33394;&#65292;&#20351;&#20854;&#33021;&#22815;&#39640;&#25928;&#22320;&#23547;&#25214;&#31572;&#26696;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#27493;&#34920;&#26126;&#65292;&#26426;&#22120;&#21487;&#33021;&#20063;&#20855;&#22791;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#21363;&#20351;&#21442;&#25968;&#25968;&#37327;&#21463;&#38480;&#65292;&#20063;&#33021;&#23637;&#31034;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; KwaiAgents&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#36890;&#29992;&#20449;&#24687;&#25628;&#32034;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#22312; KwaiAgents &#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LLM&#20316;&#20026;&#35748;&#30693;&#26680;&#24515;&#30340;&#26234;&#33021;&#20307;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#26597;&#35810;&#12289;&#34892;&#20026;&#20934;&#21017;&#21644;&#21442;&#32771;&#22806;&#37096;&#25991;&#26723;&#12290;&#26234;&#33021;&#20307;&#36824;&#21487;&#20197;&#26356;&#26032;&#26597;&#35810;&#32467;&#26524;&#65292;&#19982;&#29992;&#25143;&#36827;&#34892;&#20114;&#21160;&#65292;&#24182;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#30693;&#35782;&#21644;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driven by curiosity, humans have continually sought to explore and understand the world around them, leading to the invention of various tools to satiate this inquisitiveness. Despite not having the capacity to process and memorize vast amounts of information in their brains, humans excel in critical thinking, planning, reflection, and harnessing available tools to interact with and interpret the world, enabling them to find answers efficiently. The recent advancements in large language models (LLMs) suggest that machines might also possess the aforementioned human-like capabilities, allowing them to exhibit powerful abilities even with a constrained parameter count. In this paper, we introduce KwaiAgents, a generalized information-seeking agent system based on LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its cognitive core, which is capable of understanding a user's query, behavior guidelines, and referencing external documents. The agent can also update an
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#30340;&#21160;&#24577;&#20998;&#23618;Transformer&#27169;&#22411;(DHT)&#65292;&#36890;&#36807;&#35299;&#20915;&#19978;&#19979;&#25991;Bandit&#38382;&#39064;&#21160;&#24577;&#37197;&#32622;&#23618;&#21644;&#22836;&#30340;&#25968;&#37327;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;DHT&#19981;&#20165;&#22312;&#35757;&#32451;&#20013;&#33021;&#22815;&#33258;&#36866;&#24212;&#20248;&#21270;&#32593;&#32476;&#26550;&#26500;&#65292;&#32780;&#19988;&#20855;&#26377;&#28789;&#27963;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2312.03038</link><description>&lt;p&gt;
&#22522;&#20110;&#26679;&#26412;&#30340;&#21160;&#24577;&#20998;&#23618;Transformer&#36890;&#36807;&#19978;&#19979;&#25991;Bandit&#23454;&#29616;&#23618;&#21644;&#22836;&#30340;&#28789;&#27963;&#24615;
&lt;/p&gt;
&lt;p&gt;
Sample-based Dynamic Hierarchical Transformer with Layer and Head Flexibility via Contextual Bandit. (arXiv:2312.03038v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03038
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#30340;&#21160;&#24577;&#20998;&#23618;Transformer&#27169;&#22411;(DHT)&#65292;&#36890;&#36807;&#35299;&#20915;&#19978;&#19979;&#25991;Bandit&#38382;&#39064;&#21160;&#24577;&#37197;&#32622;&#23618;&#21644;&#22836;&#30340;&#25968;&#37327;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;DHT&#19981;&#20165;&#22312;&#35757;&#32451;&#20013;&#33021;&#22815;&#33258;&#36866;&#24212;&#20248;&#21270;&#32593;&#32476;&#26550;&#26500;&#65292;&#32780;&#19988;&#20855;&#26377;&#28789;&#27963;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#38656;&#35201;&#22266;&#23450;&#25968;&#37327;&#30340;&#23618;&#21644;&#22836;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#23545;&#21333;&#20010;&#26679;&#26412;&#30340;&#22797;&#26434;&#24615;&#19981;&#28789;&#27963;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#20013;&#37117;&#24456;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#30340;&#21160;&#24577;&#20998;&#23618;Transformer&#65288;DHT&#65289;&#27169;&#22411;&#65292;&#23427;&#30340;&#23618;&#21644;&#22836;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;&#19978;&#19979;&#25991;Bandit&#38382;&#39064;&#21160;&#24577;&#37197;&#32622;&#12290;&#20026;&#20102;&#30830;&#23450;&#23618;&#25968;&#21644;&#22836;&#25968;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22343;&#21248;&#32622;&#20449;&#19978;&#30028;&#65292;&#32780;&#22312;&#32473;&#23450;&#22836;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#37319;&#29992;&#32452;&#21512;Thompson&#25277;&#26679;&#26469;&#36873;&#25321;&#29305;&#23450;&#30340;&#22836;&#32452;&#21512;&#12290;&#19982;&#20043;&#21069;&#21482;&#20851;&#27880;&#21387;&#32553;&#35757;&#32451;&#32593;&#32476;&#20197;&#29992;&#20110;&#25512;&#26029;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;DHT&#19981;&#20165;&#22312;&#35757;&#32451;&#26399;&#38388;&#33021;&#22815;&#33258;&#36866;&#24212;&#20248;&#21270;&#24213;&#23618;&#32593;&#32476;&#26550;&#26500;&#65292;&#32780;&#19988;&#36824;&#20855;&#26377;&#28789;&#27963;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#25512;&#26029;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#27809;&#26377;&#20219;&#20309;&#39069;&#22806;&#36741;&#21161;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#21160;&#24577;&#31995;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#21160;&#24577;transformer&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer requires a fixed number of layers and heads which makes them inflexible to the complexity of individual samples and expensive in training and inference. To address this, we propose a sample-based Dynamic Hierarchical Transformer (DHT) model whose layers and heads can be dynamically configured with single data samples via solving contextual bandit problems. To determine the number of layers and heads, we use the Uniform Confidence Bound while we deploy combinatorial Thompson Sampling in order to select specific head combinations given their number. Different from previous work that focuses on compressing trained networks for inference only, DHT is not only advantageous for adaptively optimizing the underlying network architecture during training but also has a flexible network for efficient inference. To the best of our knowledge, this is the first comprehensive data-driven dynamic transformer without any additional auxiliary neural networks that implement the dynamic system
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedEmb&#30340;&#36890;&#29992;&#31639;&#27861;&#65292;&#29992;&#20110;&#36827;&#34892;&#22402;&#30452;&#21644;&#28151;&#21512;&#30340;&#22522;&#20110;DNN&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25512;&#29702;&#20934;&#30830;&#29575;&#12289;&#38544;&#31169;&#20445;&#25252;&#24615;&#33021;&#26356;&#24378;&#20197;&#21450;&#36739;&#20302;&#30340;&#36890;&#20449;&#24102;&#23485;&#38656;&#27714;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FedEmb&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20998;&#24067;&#24335;&#38382;&#39064;&#65292;&#24182;&#22312;&#26377;&#38480;&#30340;&#38544;&#31169;&#27844;&#38706;&#19979;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2312.00102</link><description>&lt;p&gt;
FedEmb:&#19968;&#31181;&#20351;&#29992;&#32593;&#32476;&#21644;&#29305;&#24449;&#23884;&#20837;&#32858;&#21512;&#30340;&#22402;&#30452;&#21644;&#28151;&#21512;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FedEmb: A Vertical and Hybrid Federated Learning Algorithm using Network And Feature Embedding Aggregation. (arXiv:2312.00102v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.00102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedEmb&#30340;&#36890;&#29992;&#31639;&#27861;&#65292;&#29992;&#20110;&#36827;&#34892;&#22402;&#30452;&#21644;&#28151;&#21512;&#30340;&#22522;&#20110;DNN&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25512;&#29702;&#20934;&#30830;&#29575;&#12289;&#38544;&#31169;&#20445;&#25252;&#24615;&#33021;&#26356;&#24378;&#20197;&#21450;&#36739;&#20302;&#30340;&#36890;&#20449;&#24102;&#23485;&#38656;&#27714;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;FedEmb&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20998;&#24067;&#24335;&#38382;&#39064;&#65292;&#24182;&#22312;&#26377;&#38480;&#30340;&#38544;&#31169;&#27844;&#38706;&#19979;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(FL)&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#21435;&#20013;&#24515;&#21270;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#33539;&#20363;&#65292;&#23427;&#22312;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#65292;&#32780;&#19981;&#23558;&#25968;&#25454;&#20256;&#36755;&#32473;&#20013;&#22830;&#26381;&#21153;&#22120;&#12290;&#23398;&#20064;&#26041;&#26696;&#21487;&#20197;&#26159;&#27700;&#24179;&#30340;&#12289;&#22402;&#30452;&#30340;&#25110;&#28151;&#21512;&#30340;(&#22402;&#30452;&#21644;&#27700;&#24179;&#37117;&#26377;)&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#24314;&#27169;&#30340;&#30740;&#31350;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#27700;&#24179;&#25968;&#25454;&#20998;&#24067;&#19978;&#65292;&#32780;&#22402;&#30452;&#21644;&#28151;&#21512;&#26041;&#26696;&#30740;&#31350;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31639;&#27861;FedEmb,&#29992;&#20110;&#24314;&#27169;&#22402;&#30452;&#21644;&#28151;&#21512;&#30340;&#22522;&#20110;DNN&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#31639;&#27861;&#30340;&#24605;&#24819;&#20855;&#26377;&#26356;&#39640;&#30340;&#25512;&#29702;&#20934;&#30830;&#29575;&#12289;&#26356;&#24378;&#30340;&#38544;&#31169;&#20445;&#25252;&#24615;&#33021;&#21644;&#26356;&#20302;&#30340;&#23458;&#25143;&#31471;-&#26381;&#21153;&#22120;&#36890;&#20449;&#24102;&#23485;&#38656;&#27714;&#65292;&#19982;&#29616;&#26377;&#24037;&#20316;&#30456;&#27604;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FedEmb&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20998;&#21106;&#29305;&#24449;&#21644;&#20027;&#39064;&#31354;&#38388;&#20998;&#25955;&#38382;&#39064;&#65292;&#22312;&#26377;&#38480;&#30340;&#38544;&#31169;&#26292;&#38706;&#19979;&#65292;&#26174;&#31034;&#20102;0.3%&#21040;4.2%&#30340;&#25512;&#29702;&#20934;&#30830;&#24230;&#25552;&#39640;&#65292;&#36866;&#29992;&#20110;&#23384;&#20648;&#22312;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is an emerging paradigm for decentralized training of machine learning models on distributed clients, without revealing the data to the central server. The learning scheme may be horizontal, vertical or hybrid (both vertical and horizontal). Most existing research work with deep neural network (DNN) modelling is focused on horizontal data distributions, while vertical and hybrid schemes are much less studied. In this paper, we propose a generalized algorithm FedEmb, for modelling vertical and hybrid DNN-based learning. The idea of our algorithm is characterised by higher inference accuracy, stronger privacy-preserving properties, and lower client-server communication bandwidth demands as compared with existing work. The experimental results show that FedEmb is an effective method to tackle both split feature &amp; subject space decentralized problems, shows 0.3% to 4.2% inference accuracy improvement with limited privacy revealing for datasets stored in local client
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#23494;&#24230;&#20272;&#35745;&#30340;&#35282;&#24230;&#35299;&#37322;&#23398;&#20064;&#25104;&#23545;&#20154;&#31867;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#35757;&#32451;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#26377;&#25928;&#22320;&#27169;&#25311;&#27880;&#37322;&#32773;&#30340;&#38544;&#21547;&#20559;&#22909;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2311.14115</link><description>&lt;p&gt;
&#20174;&#25104;&#23545;&#20154;&#31867;&#20559;&#22909;&#23398;&#20064;&#30340;&#23494;&#24230;&#20272;&#35745;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A density estimation perspective on learning from pairwise human preferences. (arXiv:2311.14115v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.14115
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#23494;&#24230;&#20272;&#35745;&#30340;&#35282;&#24230;&#35299;&#37322;&#23398;&#20064;&#25104;&#23545;&#20154;&#31867;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#35757;&#32451;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#26377;&#25928;&#22320;&#27169;&#25311;&#27880;&#37322;&#32773;&#30340;&#38544;&#21547;&#20559;&#22909;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;LHF&#65289;--&#23588;&#20854;&#26159;&#20174;&#25104;&#23545;&#20559;&#22909;&#23398;&#20064;--&#26368;&#36817;&#22312;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#25104;&#20026;&#35768;&#22810;&#30740;&#31350;&#30340;&#20027;&#39064;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#22823;&#22810;&#23558;&#20854;&#26694;&#26550;&#20026;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#25104;&#23545;&#20559;&#22909;&#25968;&#25454;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#23558;LLM&#35270;&#20026;&#19968;&#20010;&#31574;&#30053;&#65292;&#24182;&#22312;&#39069;&#22806;&#30340;&#27491;&#21017;&#21270;&#32422;&#26463;&#19979;&#36827;&#34892;&#35843;&#25972;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#35299;&#37322;&#65292;&#23427;&#20197;&#25104;&#23545;&#20559;&#22909;&#30340;&#29983;&#25104;&#36807;&#31243;&#20026;&#20013;&#24515;&#65292;&#24182;&#23558;LHF&#35270;&#20026;&#19968;&#20010;&#23494;&#24230;&#20272;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#34920;&#26126;&#23545;&#20110;&#36890;&#36807;&#20559;&#22909;&#34892;&#20026;&#20998;&#24067;&#26041;&#31243;&#23450;&#20041;&#30340;&#19968;&#31867;&#29983;&#25104;&#36807;&#31243;&#65292;&#36890;&#36807;&#25104;&#23545;&#20559;&#22909;&#35757;&#32451;&#22870;&#21169;&#20989;&#25968;&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;&#27880;&#37322;&#32773;&#30340;&#38544;&#21547;&#20559;&#22909;&#20998;&#24067;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#8220;&#26631;&#27880;&#32773;&#38169;&#35823;&#8221;&#30340;&#30740;&#31350;&#32467;&#26524;--&#21363;&#38169;&#35823;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from human feedback (LHF) -- and in particular learning from pairwise preferences -- has recently become a crucial ingredient in training large language models (LLMs), and has been the subject of much research. Most recent works frame it as a reinforcement learning problem, where a reward function is learned from pairwise preference data and the LLM is treated as a policy which is adapted to maximize the rewards, often under additional regularization constraints. We propose an alternative interpretation which centers on the generative process for pairwise preferences and treats LHF as a density estimation problem. We provide theoretical and empirical results showing that for a family of generative processes defined via preference behavior distribution equations, training a reward function on pairwise preferences effectively models an annotator's implicit preference distribution. Finally, we discuss and present findings on "annotator misspecification" -failure cases where wro
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#20020;&#24202;&#21644;RT-PCR&#25968;&#25454;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#26469;&#39044;&#27979;COVID-19&#24739;&#32773;&#24247;&#22797;&#25110;&#27515;&#20129;&#39118;&#38505;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.13925</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#20915;&#31574;&#26862;&#26519;&#65306;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;COVID-19&#24739;&#32773;&#24247;&#22797;&#25110;&#27515;&#20129;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#20020;&#24202;&#21644;RT-PCR&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Decision Forest: A Novel Approach for Predicting Recovery or Decease of COVID-19 Patients with Clinical and RT-PCR. (arXiv:2311.13925v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13925
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#20020;&#24202;&#21644;RT-PCR&#25968;&#25454;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#26469;&#39044;&#27979;COVID-19&#24739;&#32773;&#24247;&#22797;&#25110;&#27515;&#20129;&#39118;&#38505;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;&#23459;&#24067;&#22823;&#27969;&#34892;&#24050;&#32463;&#32467;&#26463;&#65292;&#20294;COVID-19&#20173;&#28982;&#34987;&#35270;&#20026;&#19968;&#31181;&#22320;&#26041;&#24615;&#30142;&#30149;&#12290;&#36825;&#27425;&#22823;&#27969;&#34892;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#26041;&#24335;&#25171;&#20081;&#20102;&#20154;&#20204;&#30340;&#29983;&#27963;&#24182;&#23548;&#33268;&#24191;&#27867;&#30340;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#12290;&#22240;&#27492;&#65292;&#32039;&#24613;&#21307;&#29983;&#26377;&#24517;&#35201;&#30830;&#23450;&#39640;&#39118;&#38505;&#27515;&#20129;&#24739;&#32773;&#65292;&#20197;&#20415;&#20248;&#20808;&#32771;&#34385;&#21307;&#38498;&#35774;&#22791;&#30340;&#20998;&#37197;&#65292;&#23588;&#20854;&#26159;&#22312;&#21307;&#30103;&#36164;&#28304;&#26377;&#38480;&#30340;&#22320;&#21306;&#12290;&#23613;&#31649;&#23384;&#22312;&#21738;&#31181;&#25968;&#25454;&#26368;&#20934;&#30830;&#30340;&#39044;&#27979;&#30340;&#38382;&#39064;&#65292;&#20294;&#24739;&#32773;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#23545;&#20110;&#39044;&#27979;COVID-19&#30149;&#20363;&#30340;&#32467;&#26524;&#26159;&#26377;&#30410;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#23454;&#29616;&#20004;&#20010;&#20027;&#35201;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24819;&#35201;&#26816;&#26597;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#26159;&#21542;&#33021;&#22815;&#39044;&#27979;&#24739;&#32773;&#30340;&#27515;&#20129;&#29575;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20020;&#24202;&#21644;RT-PCR&#23545;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#20197;&#30830;&#23450;&#21738;&#20010;&#26356;&#21487;&#38752;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#22235;&#20010;&#19981;&#21516;&#29305;&#24449;&#38598;&#30340;&#38454;&#27573;&#65292;&#24182;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26500;&#24314;&#20102;&#30456;&#24212;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
COVID-19 continues to be considered an endemic disease in spite of the World Health Organization's declaration that the pandemic is over. This pandemic has disrupted people's lives in unprecedented ways and caused widespread morbidity and mortality. As a result, it is important for emergency physicians to identify patients with a higher mortality risk in order to prioritize hospital equipment, especially in areas with limited medical services. The collected data from patients is beneficial to predict the outcome of COVID-19 cases, although there is a question about which data makes the most accurate predictions. Therefore, this study aims to accomplish two main objectives. First, we want to examine whether deep learning algorithms can predict a patient's morality. Second, we investigated the impact of Clinical and RT-PCR on prediction to determine which one is more reliable. We defined four stages with different feature sets and used interpretable deep learning methods to build appropr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignedCoT&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27597;&#35821;&#39118;&#26684;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.13538</link><description>&lt;p&gt;
&#23398;&#20250;&#35828;&#27597;&#35821;&#65306;&#20197;&#27597;&#35821;&#39118;&#26684;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Speak Like a Native: Prompting Large Language Models in a Native Style. (arXiv:2311.13538v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignedCoT&#30340;&#26032;&#39062;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27597;&#35821;&#39118;&#26684;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#24050;&#25104;&#20026;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#29616;&#20195;&#24037;&#20855;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#25991;&#26412;&#39118;&#26684;&#22914;&#20309;&#24433;&#21709;LLMs&#30340;&#24615;&#33021;&#20173;&#28982;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlignedCoT&#30340;&#26032;&#39062;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;LLMs&#30340;&#27597;&#35821;&#39118;&#26684;&#23545;&#40784;&#26469;&#25552;&#39640;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290; "&#27597;&#35821;"&#26159;&#25351;LLMs&#30340;&#22266;&#26377;&#29305;&#24449;&#65292;&#21487;&#20197;&#36890;&#36807;&#38646;-shot&#22330;&#26223;&#25506;&#27979;&#12290; AlignedCoT&#24191;&#27867;&#36866;&#29992;&#20110;ICL&#26041;&#27861;&#65292;&#21487;&#20197;&#36731;&#26494;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#32467;&#21512;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#25968;&#23398;&#38382;&#31572;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#25991;&#26412;&#29702;&#35299;&#31561;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#32780;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;AlignedCoT&#30456;&#27604;&#31934;&#24515;&#25163;&#24037;&#21046;&#20316;&#30340;&#28436;&#31034;&#25991;&#31295;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) with large language models (LLMs) has become the modern tools of choice for many natural language processing tasks. However, how the text style of in-context examples influences the performance of LLMs still remains under-explored. This paper presents a novel and effective approach, named \textbf{AlignedCoT}, to improve the reasoning capability of LLMs by aligning the in-context examples with the native style of LLMs.''Native'' refers to the inherent characteristic of LLMs which can be probed by zero-shot scenarios.AlignedCoT is widely applicable to ICL methods, making it easy to combine with state-of-the-art techniques to further improve the LLMs' performance. We conduct extensive and comprehensive experiments on several benchmarks on mathematical question-answering, common-sense reasoning, and text understanding. The empirical results demonstrate that our AlignedCoT significantly improves performance over the carefully handcrafted demonstrations. Specificall
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20984;&#20248;&#21270;&#20013;&#31639;&#27861;&#30340;&#21487;&#37325;&#29616;&#24615;&#21644;&#26799;&#24230;&#22797;&#26434;&#24230;&#38382;&#39064;&#12290;&#20182;&#20204;&#25361;&#25112;&#20102;&#20043;&#21069;&#30340;&#35266;&#28857;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#24179;&#28369;&#20984;&#20248;&#21270;&#21644;&#24179;&#28369;&#20984;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#20248;&#30340;&#21487;&#37325;&#29616;&#24615;&#21644;&#25509;&#36817;&#26368;&#20248;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#20182;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#19981;&#21516;&#30340;oracle&#35774;&#32622;&#19979;&#65292;&#19982;&#19981;&#21516;&#31867;&#22411;&#30340;oracle&#30456;&#21305;&#37197;&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17759</link><description>&lt;p&gt;
&#22312;&#20984;&#20248;&#21270;&#20013;&#30340;&#31639;&#27861;&#21487;&#37325;&#29616;&#24615;&#21644;&#26799;&#24230;&#22797;&#26434;&#24230;&#30340;&#26368;&#20248;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Optimal Guarantees for Algorithmic Reproducibility and Gradient Complexity in Convex Optimization. (arXiv:2310.17759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17759
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20984;&#20248;&#21270;&#20013;&#31639;&#27861;&#30340;&#21487;&#37325;&#29616;&#24615;&#21644;&#26799;&#24230;&#22797;&#26434;&#24230;&#38382;&#39064;&#12290;&#20182;&#20204;&#25361;&#25112;&#20102;&#20043;&#21069;&#30340;&#35266;&#28857;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#24179;&#28369;&#20984;&#20248;&#21270;&#21644;&#24179;&#28369;&#20984;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#20248;&#30340;&#21487;&#37325;&#29616;&#24615;&#21644;&#25509;&#36817;&#26368;&#20248;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#20182;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#19981;&#21516;&#30340;oracle&#35774;&#32622;&#19979;&#65292;&#19982;&#19981;&#21516;&#31867;&#22411;&#30340;oracle&#30456;&#21305;&#37197;&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#21487;&#37325;&#29616;&#24615;&#34913;&#37327;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#31245;&#24494;&#25913;&#21464;&#26102;&#36755;&#20986;&#30340;&#20559;&#24046;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19968;&#38454;&#26041;&#27861;&#38656;&#35201;&#22312;&#25910;&#25947;&#36895;&#24230;&#65288;&#26799;&#24230;&#22797;&#26434;&#24230;&#65289;&#21644;&#26356;&#22909;&#30340;&#21487;&#37325;&#29616;&#24615;&#20043;&#38388;&#20570;&#20986;&#26435;&#34913;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#36825;&#31181;&#30475;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#23481;&#26131;&#20986;&#38169;&#30340;oracle&#35774;&#32622;&#19979;&#65292;&#23545;&#20110;&#24179;&#28369;&#20984;&#20248;&#21270;&#21644;&#24179;&#28369;&#20984;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#20248;&#30340;&#21487;&#37325;&#29616;&#24615;&#21644;&#25509;&#36817;&#26368;&#20248;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#23588;&#20854;&#26159;&#65292;&#22312;&#19981;&#31934;&#30830;&#30340;&#21021;&#22987;&#21270;oracle&#32473;&#23450;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#21487;&#37325;&#29616;&#24615;&#21644;&#25509;&#36817;&#26368;&#20248;&#30340;&#26799;&#24230;&#22797;&#26434;&#24230;-&#23545;&#20110;&#26368;&#23567;&#21270;&#21644;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#12290;&#23545;&#20110;&#19981;&#31934;&#30830;&#30340;&#26799;&#24230;oracle&#65292;&#25509;&#36817;&#26368;&#20248;&#30340;&#20445;&#35777;&#20063;&#36866;&#29992;&#20110;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#38543;&#26426;&#26799;&#24230;oracle&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#22312;&#21487;&#37325;&#29616;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#37117;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic reproducibility measures the deviation in outputs of machine learning algorithms upon minor changes in the training process. Previous work suggests that first-order methods would need to trade-off convergence rate (gradient complexity) for better reproducibility. In this work, we challenge this perception and demonstrate that both optimal reproducibility and near-optimal convergence guarantees can be achieved for smooth convex minimization and smooth convex-concave minimax problems under various error-prone oracle settings. Particularly, given the inexact initialization oracle, our regularization-based algorithms achieve the best of both worlds optimal reproducibility and near-optimal gradient complexity - for minimization and minimax optimization. With the inexact gradient oracle, the near-optimal guarantees also hold for minimax optimization. Additionally, with the stochastic gradient oracle, we show that stochastic gradient descent ascent is optimal in terms of both re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#22522;&#36203;&#38669;&#22827;&#30005;&#27969;&#23450;&#24459;&#19982;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#36830;&#32493;&#28145;&#24230;&#32593;&#32476;&#24314;&#31435;&#36830;&#25509;&#12290;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;98.86%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#19988;&#20855;&#26377;&#22312;&#30828;&#20214;&#19978;&#23454;&#29616;&#30340;&#28508;&#21147;&#12290;&#26080;&#35770;&#32593;&#32476;&#21442;&#25968;&#25968;&#37327;&#22914;&#20309;&#65292;&#20854;&#27491;&#21521;&#35745;&#31639;&#37117;&#21487;&#20197;&#22312;1/f&#31186;&#20869;&#23436;&#25104;&#65292;&#20855;&#26377;&#24555;&#36895;&#35745;&#31639;&#30340;&#30828;&#20214;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15872</link><description>&lt;p&gt;
KirchhoffNet&#65306;&#19968;&#31181;&#36830;&#25509;&#28040;&#24687;&#20256;&#36882;&#21644;&#36830;&#32493;&#28145;&#24230;&#27169;&#22411;&#30340;&#30005;&#36335;&#26725;&#25509;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
KirchhoffNet: A Circuit Bridging Message Passing and Continuous-Depth Models. (arXiv:2310.15872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#22522;&#36203;&#38669;&#22827;&#30005;&#27969;&#23450;&#24459;&#19982;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#36830;&#32493;&#28145;&#24230;&#32593;&#32476;&#24314;&#31435;&#36830;&#25509;&#12290;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;98.86%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#19988;&#20855;&#26377;&#22312;&#30828;&#20214;&#19978;&#23454;&#29616;&#30340;&#28508;&#21147;&#12290;&#26080;&#35770;&#32593;&#32476;&#21442;&#25968;&#25968;&#37327;&#22914;&#20309;&#65292;&#20854;&#27491;&#21521;&#35745;&#31639;&#37117;&#21487;&#20197;&#22312;1/f&#31186;&#20869;&#23436;&#25104;&#65292;&#20855;&#26377;&#24555;&#36895;&#35745;&#31639;&#30340;&#30828;&#20214;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#27169;&#25311;&#30005;&#36335;&#30340;&#22522;&#26412;&#21407;&#29702;&#22522;&#36203;&#38669;&#22827;&#30005;&#27969;&#23450;&#24459;&#65292;&#24341;&#20837;&#20102;&#19968;&#31867;&#29420;&#29305;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#31216;&#20026;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#12290;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#19982;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#36830;&#32493;&#28145;&#24230;&#32593;&#32476;&#24314;&#31435;&#20102;&#23494;&#20999;&#32852;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#20219;&#20309;&#20256;&#32479;&#23618;&#65288;&#22914;&#21367;&#31215;&#12289;&#27744;&#21270;&#25110;&#32447;&#24615;&#23618;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;98.86%&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#30456;&#24403;&#12290;&#35753;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#26356;&#21152;&#26377;&#36259;&#30340;&#26159;&#20854;&#22312;&#30828;&#20214;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#24403;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#37096;&#32626;&#22312;GPU&#19978;&#12290;&#30456;&#21453;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#27169;&#25311;&#30005;&#36335;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26080;&#35770;&#22312;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#20869;&#26377;&#22810;&#23569;&#21442;&#25968;&#65292;&#20854;&#27491;&#21521;&#35745;&#31639;&#37117;&#21487;&#20197;&#22312;1/f&#31186;&#20869;&#23436;&#25104;&#65292;&#20854;&#20013;f&#34920;&#31034;&#30828;&#20214;&#30340;&#26102;&#38047;&#39057;&#29575;&#12290;&#36825;&#31181;&#29305;&#24615;&#34920;&#26126;&#65292;&#22522;&#36203;&#38669;&#22827;&#32593;&#32476;&#20855;&#26377;&#28508;&#21147;&#23454;&#29616;&#24555;&#36895;&#35745;&#31639;&#30340;&#30828;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we exploit a fundamental principle of analog electronic circuitry, Kirchhoff's current law, to introduce a unique class of neural network models that we refer to as KirchhoffNet. KirchhoffNet establishes close connections with message passing neural networks and continuous-depth networks. We demonstrate that even in the absence of any traditional layers (such as convolution, pooling, or linear layers), KirchhoffNet attains 98.86% test accuracy on the MNIST dataset, comparable with state of the art (SOTA) results. What makes KirchhoffNet more intriguing is its potential in the realm of hardware. Contemporary deep neural networks are conventionally deployed on GPUs. In contrast, KirchhoffNet can be physically realized by an analog electronic circuit. Moreover, we justify that irrespective of the number of parameters within a KirchhoffNet, its forward calculation can always be completed within 1/f seconds, with f representing the hardware's clock frequency. This characteris
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QUAVE&#30340;&#22235;&#20803;&#25968;&#23567;&#27874;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#25552;&#21462;&#26174;&#33879;&#29305;&#24449;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#25110;&#32508;&#21512;&#20219;&#21153;&#32467;&#21512;&#20351;&#29992;&#65292;&#24182;&#25512;&#24191;&#20102;&#23545;&#21333;&#36890;&#36947;&#25968;&#25454;&#30340;&#37319;&#29992;&#12290;&#36890;&#36807;&#22235;&#20803;&#25968;&#23567;&#27874;&#21464;&#25442;&#21644;&#21152;&#26435;&#22788;&#29702;&#65292;QUAVE&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#36739;&#22823;&#21464;&#21270;&#30340;&#21307;&#23398;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.10224</link><description>&lt;p&gt;
&#36890;&#36807;&#22235;&#20803;&#25968;&#23567;&#27874;&#32593;&#32476;&#25512;&#24191;&#21307;&#23398;&#22270;&#20687;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Generalizing Medical Image Representations via Quaternion Wavelet Networks. (arXiv:2310.10224v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QUAVE&#30340;&#22235;&#20803;&#25968;&#23567;&#27874;&#32593;&#32476;&#65292;&#21487;&#20197;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#25552;&#21462;&#26174;&#33879;&#29305;&#24449;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#25110;&#32508;&#21512;&#20219;&#21153;&#32467;&#21512;&#20351;&#29992;&#65292;&#24182;&#25512;&#24191;&#20102;&#23545;&#21333;&#36890;&#36947;&#25968;&#25454;&#30340;&#37319;&#29992;&#12290;&#36890;&#36807;&#22235;&#20803;&#25968;&#23567;&#27874;&#21464;&#25442;&#21644;&#21152;&#26435;&#22788;&#29702;&#65292;QUAVE&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#36739;&#22823;&#21464;&#21270;&#30340;&#21307;&#23398;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#21644;&#21508;&#31181;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#26085;&#30410;&#22686;&#21152;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#36866;&#24615;&#25104;&#20026;&#19968;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#24403;&#22788;&#29702;&#21307;&#23398;&#25968;&#25454;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#23588;&#20026;&#24191;&#27867;&#65292;&#22240;&#20026;&#32570;&#20047;&#26041;&#27861;&#35770;&#26631;&#20934;&#23548;&#33268;&#19981;&#21516;&#30340;&#25104;&#20687;&#20013;&#24515;&#25110;&#20351;&#29992;&#19981;&#21516;&#35774;&#22791;&#21644;&#36741;&#21161;&#22240;&#32032;&#33719;&#21462;&#30340;&#25968;&#25454;&#23384;&#22312;&#36739;&#22823;&#21464;&#21270;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#26222;&#36866;&#30340;&#12289;&#25968;&#25454;-&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#25552;&#21462;&#26174;&#33879;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#22235;&#20803;&#25968;&#23567;&#27874;&#32593;&#32476;&#65288;QUAVE&#65289;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#25110;&#32508;&#21512;&#20219;&#21153;&#30456;&#32467;&#21512;&#65292;&#24182;&#19988;&#21487;&#20197;&#32467;&#21512;&#23454;&#38469;&#12289;&#22235;&#20803;&#25968;&#25110;&#36229;&#22797;&#20540;&#27169;&#22411;&#65292;&#25512;&#24191;&#23427;&#20204;&#23545;&#21333;&#36890;&#36947;&#25968;&#25454;&#30340;&#37319;&#29992;&#12290;QUAVE&#39318;&#20808;&#36890;&#36807;&#22235;&#20803;&#25968;&#23567;&#27874;&#21464;&#25442;&#25552;&#21462;&#19981;&#21516;&#30340;&#23376;&#24102;&#65292;&#24471;&#21040;&#20302;&#39057;/&#36817;&#20284;&#39057;&#24102;&#21644;&#39640;&#39057;/&#32454;&#31890;&#24230;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#23427;&#23545;&#26368;&#26377;&#20195;&#34920;&#24615;&#30340;&#29305;&#24449;&#36827;&#34892;&#21152;&#26435;&#22788;&#29702;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#29305;&#24449;&#37325;&#35201;&#24615;&#19981;&#22343;&#21248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network generalizability is becoming a broad research field due to the increasing availability of datasets from different sources and for various tasks. This issue is even wider when processing medical data, where a lack of methodological standards causes large variations being provided by different imaging centers or acquired with various devices and cofactors. To overcome these limitations, we introduce a novel, generalizable, data- and task-agnostic framework able to extract salient features from medical images. The proposed quaternion wavelet network (QUAVE) can be easily integrated with any pre-existing medical image analysis or synthesis task, and it can be involved with real, quaternion, or hypercomplex-valued models, generalizing their adoption to single-channel data. QUAVE first extracts different sub-bands through the quaternion wavelet transform, resulting in both low-frequency/approximation bands and high-frequency/fine-grained features. Then, it weighs the most repr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;&#20013;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#65292;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05365</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Molecular De Novo Design through Transformer-based Reinforcement Learning. (arXiv:2310.05365v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;&#20013;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#65292;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#21033;&#29992;Transformer&#30456;&#23545;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#20248;&#36234;&#24207;&#21015;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;RNN&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#39044;&#27979;&#23545;&#22810;&#31181;&#29983;&#29289;&#38774;&#28857;&#20855;&#26377;&#27963;&#24615;&#30340;&#21270;&#21512;&#29289;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#25429;&#25417;&#20102;&#20998;&#23376;&#32467;&#26500;&#24207;&#21015;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#21253;&#25324;&#29983;&#25104;&#19982;&#26597;&#35810;&#32467;&#26500;&#31867;&#20284;&#30340;&#20998;&#23376;&#21644;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#21270;&#21512;&#29289;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#22522;&#32447;&#30340;&#22522;&#20110;RNN&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#26725;&#25509;&#21270;&#23398;&#12289;&#20174;&#21333;&#20010;&#20998;&#23376;&#24320;&#22987;&#25193;&#23637;&#24211;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#39640;&#39044;&#27979;&#27963;&#24615;&#30340;&#21270;&#21512;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a method to fine-tune a Transformer-based generative model for molecular de novo design. Leveraging the superior sequence learning capacity of Transformers over Recurrent Neural Networks (RNNs), our model can generate molecular structures with desired properties effectively. In contrast to the traditional RNN-based models, our proposed method exhibits superior performance in generating compounds predicted to be active against various biological targets, capturing long-term dependencies in the molecular structure sequence. The model's efficacy is demonstrated across numerous tasks, including generating analogues to a query structure and producing compounds with particular attributes, outperforming the baseline RNN-based methods. Our approach can be used for scaffold hopping, library expansion starting from a single molecule, and generating compounds with high predicted activity against biological targets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27969;&#21305;&#37197;&#36827;&#34892;&#32479;&#19968;&#30340;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#30340;&#26550;&#26500;&#65292;&#30456;&#27604;&#20110;&#20808;&#21069;&#30340;&#25216;&#26415;&#65292;&#23427;&#26356;&#31616;&#21333;&#12289;&#21344;&#29992;&#20869;&#23384;&#26356;&#23567;&#65292;&#24182;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#27169;&#24577;&#12290;&#26032;&#30340;&#35757;&#32451;&#26426;&#21046;&#22312;&#23569;&#37327;&#27493;&#39588;&#20013;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#21512;&#25104;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#20027;&#35266;&#27979;&#35797;&#35777;&#26126;&#20102;&#22312;&#35821;&#38899;&#33258;&#28982;&#24230;&#12289;&#25163;&#21183;&#20154;&#31867;&#21270;&#21644;&#36328;&#27169;&#24577;&#36866;&#24403;&#24615;&#26041;&#38754;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.05181</link><description>&lt;p&gt;
&#20351;&#29992;&#27969;&#21305;&#37197;&#36827;&#34892;&#32479;&#19968;&#30340;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unified speech and gesture synthesis using flow matching. (arXiv:2310.05181v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27969;&#21305;&#37197;&#36827;&#34892;&#32479;&#19968;&#30340;&#35821;&#38899;&#21644;&#25163;&#21183;&#21512;&#25104;&#30340;&#26550;&#26500;&#65292;&#30456;&#27604;&#20110;&#20808;&#21069;&#30340;&#25216;&#26415;&#65292;&#23427;&#26356;&#31616;&#21333;&#12289;&#21344;&#29992;&#20869;&#23384;&#26356;&#23567;&#65292;&#24182;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#35821;&#38899;&#21644;&#25163;&#21183;&#27169;&#24577;&#12290;&#26032;&#30340;&#35757;&#32451;&#26426;&#21046;&#22312;&#23569;&#37327;&#27493;&#39588;&#20013;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#21512;&#25104;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#20027;&#35266;&#27979;&#35797;&#35777;&#26126;&#20102;&#22312;&#35821;&#38899;&#33258;&#28982;&#24230;&#12289;&#25163;&#21183;&#20154;&#31867;&#21270;&#21644;&#36328;&#27169;&#24577;&#36866;&#24403;&#24615;&#26041;&#38754;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25991;&#26412;&#36716;&#35821;&#38899;&#25216;&#26415;&#22312;&#26391;&#35835;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;&#33258;&#28982;&#24615;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#35821;&#35328;&#21644;&#38750;&#35821;&#35328;&#20132;&#27969;&#34892;&#20026;&#30340;&#22810;&#27169;&#24577;&#21512;&#25104;&#65292;&#20363;&#22914;&#33258;&#21457;&#35821;&#35328;&#21644;&#30456;&#20851;&#30340;&#36523;&#20307;&#25163;&#21183;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#26550;&#26500;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#20013;&#32852;&#21512;&#21512;&#25104;&#35821;&#38899;&#22768;&#23398;&#21644;&#22522;&#20110;&#39592;&#39612;&#30340;&#19977;&#32500;&#25163;&#21183;&#36816;&#21160;&#65292;&#35757;&#32451;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#26465;&#20214;&#27969;&#21305;&#37197;&#65288;OT-CFM&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#27604;&#20808;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#26356;&#31616;&#21333;&#65292;&#21344;&#29992;&#30340;&#20869;&#23384;&#26356;&#23567;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#35821;&#38899;&#21644;&#25163;&#21183;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#22312;&#19968;&#20010;&#21333;&#19968;&#36807;&#31243;&#20013;&#29983;&#25104;&#20004;&#31181;&#27169;&#24577;&#12290;&#19982;&#20197;&#21069;&#30456;&#27604;&#65292;&#26032;&#30340;&#35757;&#32451;&#26426;&#21046;&#22312;&#26356;&#23569;&#30340;&#27493;&#39588;&#65288;&#32593;&#32476;&#35780;&#20272;&#65289;&#20013;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#21512;&#25104;&#36136;&#37327;&#12290;&#21333;&#19968;&#21644;&#22810;&#27169;&#24577;&#30340;&#20027;&#35266;&#27979;&#35797;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#22522;&#20934;&#30456;&#27604;&#65292;&#35821;&#38899;&#33258;&#28982;&#24230;&#12289;&#25163;&#21183;&#20154;&#31867;&#21270;&#21644;&#36328;&#27169;&#24577;&#30340;&#36866;&#24403;&#24615;&#37117;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
As text-to-speech technologies achieve remarkable naturalness in read-aloud tasks, there is growing interest in multimodal synthesis of verbal and non-verbal communicative behaviour, such as spontaneous speech and associated body gestures. This paper presents a novel, unified architecture for jointly synthesising speech acoustics and skeleton-based 3D gesture motion from text, trained using optimal-transport conditional flow matching (OT-CFM). The proposed architecture is simpler than the previous state of the art, has a smaller memory footprint, and can capture the joint distribution of speech and gestures, generating both modalities together in one single process. The new training regime, meanwhile, enables better synthesis quality in much fewer steps (network evaluations) than before. Uni- and multimodal subjective tests demonstrate improved speech naturalness, gesture human-likeness, and cross-modal appropriateness compared to existing benchmarks. Please see https://shivammehta25.g
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33258;&#21160;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#23558;VQA&#35780;&#20272;&#26684;&#24335;&#21270;&#20026;&#22238;&#31572;&#35780;&#20998;&#20219;&#21153;&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#19978;&#35780;&#20998;&#20505;&#36873;&#31572;&#26696;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#20248;&#20110;&#29616;&#26377;&#24230;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02567</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33258;&#21160;VQA&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Improving Automatic VQA Evaluation Using Large Language Models. (arXiv:2310.02567v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02567
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33258;&#21160;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#23558;VQA&#35780;&#20272;&#26684;&#24335;&#21270;&#20026;&#22238;&#31572;&#35780;&#20998;&#20219;&#21153;&#65292;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20934;&#30830;&#24230;&#19978;&#35780;&#20998;&#20505;&#36873;&#31572;&#26696;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#20248;&#20110;&#29616;&#26377;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25552;&#20986;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;8&#24180;&#21518;&#65292;&#20934;&#30830;&#29575;&#20173;&#28982;&#26159;&#33258;&#21160;&#35780;&#20272;&#30340;&#20027;&#35201;&#25351;&#26631;&#12290;&#22312;IID&#35780;&#20272;&#35774;&#32622;&#20013;&#65292;VQA&#20934;&#30830;&#24230;&#19968;&#30452;&#24456;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#31038;&#21306;&#27491;&#22312;&#36716;&#21521;&#24320;&#25918;&#24335;&#29983;&#25104;&#27169;&#22411;&#21644;OOD&#35780;&#20272;&#12290;&#22312;&#36825;&#31181;&#26032;&#30340;&#33539;&#24335;&#20013;&#65292;&#29616;&#26377;&#30340;VQA&#20934;&#30830;&#24230;&#25351;&#26631;&#36807;&#20110;&#20005;&#26684;&#65292;&#20302;&#20272;&#20102;VQA&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#33258;&#21160;VQA&#24230;&#37327;&#65292;&#20316;&#20026;&#20154;&#31867;&#21028;&#26029;&#30340;&#20195;&#29702;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#26500;&#24314;&#26356;&#22909;&#30340;VQA&#24230;&#37327;&#12290;&#25105;&#20204;&#23558;VQA&#35780;&#20272;&#26684;&#24335;&#21270;&#20026;&#19968;&#20010;&#22238;&#31572;&#35780;&#20998;&#20219;&#21153;&#65292;&#21363;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#25351;&#31034;&#26681;&#25454;&#19968;&#32452;&#21442;&#32771;&#31572;&#26696;&#35780;&#20998;&#20505;&#36873;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#20248;&#20110;&#29616;&#26377;&#24230;&#37327;&#22312;&#20960;&#20010;VQA&#27169;&#22411;&#21644;&#22522;&#20934;&#27979;&#35797;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
8 years after the visual question answering (VQA) task was proposed, accuracy remains the primary metric for automatic evaluation. VQA Accuracy has been effective so far in the IID evaluation setting. However, our community is undergoing a shift towards open-ended generative models and OOD evaluation. In this new paradigm, the existing VQA Accuracy metric is overly stringent and underestimates the performance of VQA systems. Thus, there is a need to develop more robust automatic VQA metrics that serve as a proxy for human judgment. In this work, we propose to leverage the in-context learning capabilities of instruction-tuned large language models (LLMs) to build a better VQA metric. We formulate VQA evaluation as an answer-rating task where the LLM is instructed to score the accuracy of a candidate answer given a set of reference answers. We demonstrate the proposed metric better correlates with human judgment compared to existing metrics across several VQA models and benchmarks. We ho
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SGNNs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#21147;&#21327;&#21516;&#36215;&#26469;&#65292;&#29992;&#20110;&#39044;&#27979;&#23398;&#29983;&#22312;&#23398;&#20064;&#32773;&#25552;&#20379;&#30340;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26377;&#31526;&#21495;&#20108;&#20998;&#22270;&#20840;&#38754;&#24314;&#27169;&#23398;&#29983;&#22238;&#31572;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#22686;&#24378;&#20102;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13500</link><description>&lt;p&gt;
&#22312;&#23398;&#20064;&#32773;&#25552;&#20379;&#30340;&#38382;&#39064;&#19978;&#22686;&#24378;&#23398;&#29983;&#34920;&#29616;&#39044;&#27979;&#30340;SGNN-LLM&#21327;&#21516;
&lt;/p&gt;
&lt;p&gt;
Enhancing Student Performance Prediction on Learnersourced Questions with SGNN-LLM Synergy. (arXiv:2309.13500v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13500
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SGNNs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#21147;&#21327;&#21516;&#36215;&#26469;&#65292;&#29992;&#20110;&#39044;&#27979;&#23398;&#29983;&#22312;&#23398;&#20064;&#32773;&#25552;&#20379;&#30340;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26377;&#31526;&#21495;&#20108;&#20998;&#22270;&#20840;&#38754;&#24314;&#27169;&#23398;&#29983;&#22238;&#31572;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#22686;&#24378;&#20102;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#29983;&#20869;&#23481;&#21019;&#20316;&#65292;&#23398;&#20064;&#32773;&#21512;&#20316;&#20855;&#26377;&#21487;&#25193;&#23637;&#25945;&#32946;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#23398;&#29983;&#22312;&#23398;&#20064;&#32773;&#25552;&#20379;&#30340;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#23545;&#20110;&#20010;&#24615;&#21270;&#23398;&#20064;&#20307;&#39564;&#33267;&#20851;&#37325;&#35201;&#65292;&#30001;&#20110;&#23398;&#29983;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#22122;&#22768;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#21487;&#20197;&#25429;&#33719;&#23398;&#29983;&#21644;&#38382;&#39064;&#20132;&#20114;&#30340;&#22797;&#26434;&#32593;&#32476;&#65292;&#20294;&#22312;&#20919;&#21551;&#21160;&#26465;&#20214;&#19979;&#65292;&#20854;&#20013;&#23398;&#29983;&#23545;&#38382;&#39064;&#30340;&#26377;&#38480;&#21442;&#19982;&#23548;&#33268;&#25968;&#25454;&#31232;&#30095;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#31574;&#30053;&#65292;&#23558;&#25972;&#21512;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SGNNs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#21147;&#21327;&#21516;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#26377;&#31526;&#21495;&#20108;&#20998;&#22270;&#20840;&#38754;&#24314;&#27169;&#23398;&#29983;&#22238;&#31572;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#22686;&#24378;&#20102;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;LLM&#30340;&#36129;&#29486;&#22312;&#20110;&#29983;&#25104;&#22522;&#30784;&#38382;&#39064;&#23884;&#20837;&#65292;&#29305;&#21035;&#26159;&#35777;&#26126;&#20102;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learnersourcing offers great potential for scalable education through student content creation. However, predicting student performance on learnersourced questions, which is essential for personalizing the learning experience, is challenging due to the inherent noise in student-generated data. Moreover, while conventional graph-based methods can capture the complex network of student and question interactions, they often fall short under cold start conditions where limited student engagement with questions yields sparse data. To address both challenges, we introduce an innovative strategy that synergizes the potential of integrating Signed Graph Neural Networks (SGNNs) and Large Language Model (LLM) embeddings. Our methodology employs a signed bipartite graph to comprehensively model student answers, complemented by a contrastive learning framework that enhances noise resilience. Furthermore, LLM's contribution lies in generating foundational question embeddings, proving especially adv
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22686;&#21152;&#26497;&#28857;&#26469;&#36991;&#20813;&#26681;&#26597;&#25214;&#21644;&#20248;&#21270;&#20013;&#19981;&#38656;&#35201;&#30340;&#28857;&#65292;&#26041;&#27861;&#26159;&#23558;&#20195;&#20215;&#20989;&#25968;&#38500;&#20197;&#21040;&#30446;&#26631;&#28857;&#30340;&#36317;&#31163;&#20989;&#25968;&#30340;&#36866;&#24403;&#24130;&#12290;</title><link>http://arxiv.org/abs/2309.11475</link><description>&lt;p&gt;
&#36991;&#20813;&#22312;&#26681;&#26597;&#25214;&#21644;&#20248;&#21270;&#20013;&#20986;&#29616;&#19981;&#38656;&#35201;&#30340;&#28857;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#22686;&#21152;&#26497;&#28857;
&lt;/p&gt;
&lt;p&gt;
Multiplying poles to avoid unwanted points in root finding and optimization. (arXiv:2309.11475v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11475
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#26497;&#28857;&#26469;&#36991;&#20813;&#26681;&#26597;&#25214;&#21644;&#20248;&#21270;&#20013;&#19981;&#38656;&#35201;&#30340;&#28857;&#65292;&#26041;&#27861;&#26159;&#23558;&#20195;&#20215;&#20989;&#25968;&#38500;&#20197;&#21040;&#30446;&#26631;&#28857;&#30340;&#36317;&#31163;&#20989;&#25968;&#30340;&#36866;&#24403;&#24130;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26681;&#26597;&#25214;&#21644;&#20248;&#21270;&#20013;&#65292;&#23384;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#33021;&#26080;&#27861;&#20445;&#35777;&#33258;&#24049;&#36873;&#25321;&#30340;&#26041;&#27861;&#26500;&#36896;&#30340;&#24207;&#21015;&#25910;&#25947;&#20110;&#19968;&#20010;&#38381;&#38598;&#21512;A&#65288;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24182;&#19981;&#20551;&#35774;A&#26377;&#20854;&#20182;&#38468;&#21152;&#23646;&#24615;&#65292;&#22914;&#20984;&#24615;&#25110;&#36830;&#36890;&#24615;&#65289;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24076;&#26395;&#26377;&#19968;&#20010;&#26426;&#21046;&#26469;&#36991;&#20813;&#22312;&#31639;&#27861;&#30340;&#19979;&#19968;&#27425;&#36816;&#34892;&#20013;&#20877;&#27425;&#36935;&#21040;&#36825;&#20010;&#28857;z*&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65306;&#25105;&#20204;&#23558;&#20195;&#20215;&#20989;&#25968;&#38500;&#20197;&#21040;A&#30340;&#36317;&#31163;&#20989;&#25968;&#30340;&#36866;&#24403;&#24130;&#12290;&#36825;&#20010;&#24819;&#27861;&#21463;&#21040;&#20102;&#22312;&#19968;&#32500;&#20989;&#25968;&#20013;&#23581;&#35797;&#25214;&#21040;&#25152;&#26377;&#26681;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#39318;&#20808;&#35299;&#37322;&#20102;&#22312;&#20195;&#20215;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#24688;&#22909;&#20026;0&#30340;&#24773;&#20917;&#19979;&#36825;&#31181;&#26041;&#27861;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#28982;&#21518;&#35299;&#37322;&#20102;&#22914;&#26524;&#26368;&#23567;&#20540;&#19981;&#20026;&#38646;&#35813;&#22914;&#20309;&#36827;&#34892;&#65288;&#21516;&#26102;&#20801;&#35768;&#27491;&#30340;&#26368;&#23567;&#20540;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In root finding and optimization, there are many cases where there is a closed set $A$ one does not the sequence constructed by one's favourite method will converge to A (here, we do not assume extra properties on $A$ such as being convex or connected). For example, if one wants to find roots, and one chooses initial points in the basin of attraction for 1 root $x^*$ (a fact which one may not know before hand), then one will always end up in that root. In this case, one would like to have a mechanism to avoid this point $z^*$ in the next runs of one's algorithm.  In this paper, we propose a new method aiming to achieve this: we divide the cost function by an appropriate power of the distance function to $A$. This idea is inspired by how one would try to find all roots of a function in 1 variable. We first explain the heuristic for this method in the case where the minimum of the cost function is exactly 0, and then explain how to proceed if the minimum is non-zero (allowing both positi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#23454;&#29616;&#40065;&#26834;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#20445;&#30041;&#36807;&#21435;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#23481;&#37327;&#30340;&#20869;&#23384;&#26469;&#20445;&#23384;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#29615;&#22659;&#20449;&#24687;&#65292;&#24182;&#20174;&#20869;&#23384;&#20013;&#37319;&#26679;&#25968;&#25454;&#28857;&#26469;&#33719;&#24471;&#23545;&#26410;&#30693;&#21464;&#21270;&#40065;&#26834;&#30340;&#39044;&#27979;&#22120;&#12290;&#35813;&#20998;&#26512;&#23637;&#31034;&#20102;&#35760;&#24518;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#32780;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10149</link><description>&lt;p&gt;
AI&#20195;&#29702;&#30340;&#35760;&#24518;&#21644;&#27867;&#21270;&#33021;&#21147;&#20998;&#26512;&#65306;&#36830;&#32493;&#23398;&#20064;&#32773;&#26159;&#21542;&#20855;&#26377;&#40065;&#26834;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Analysis of the Memorization and Generalization Capabilities of AI Agents: Are Continual Learners Robust?. (arXiv:2309.10149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#23454;&#29616;&#40065;&#26834;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#20445;&#30041;&#36807;&#21435;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#23481;&#37327;&#30340;&#20869;&#23384;&#26469;&#20445;&#23384;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#29615;&#22659;&#20449;&#24687;&#65292;&#24182;&#20174;&#20869;&#23384;&#20013;&#37319;&#26679;&#25968;&#25454;&#28857;&#26469;&#33719;&#24471;&#23545;&#26410;&#30693;&#21464;&#21270;&#40065;&#26834;&#30340;&#39044;&#27979;&#22120;&#12290;&#35813;&#20998;&#26512;&#23637;&#31034;&#20102;&#35760;&#24518;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#32780;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;AI&#20195;&#29702;&#65288;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25110;&#26426;&#22120;&#20154;&#65289;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#20174;&#38750;&#31283;&#24577;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#12290;&#23545;&#20110;&#36825;&#31867;&#24212;&#29992;&#30340;&#23454;&#38469;&#37096;&#32626;&#65292;&#20445;&#35777;&#23545;&#26410;&#30693;&#29615;&#22659;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#20445;&#30041;&#36807;&#21435;&#30340;&#32463;&#39564;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#23454;&#29616;&#40065;&#26834;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#20445;&#30041;&#36807;&#21435;&#30340;&#30693;&#35782;&#12290;&#32771;&#34385;&#21040;&#36830;&#32493;&#23398;&#20064;&#20195;&#29702;&#20351;&#29992;&#26377;&#38480;&#23481;&#37327;&#30340;&#20869;&#23384;&#26469;&#20445;&#23384;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#29615;&#22659;&#20449;&#24687;&#65292;&#20197;&#20943;&#36731;&#36951;&#24536;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#20174;&#20869;&#23384;&#20013;&#37319;&#26679;&#25968;&#25454;&#28857;&#65292;&#20197;&#20272;&#35745;&#29615;&#22659;&#21464;&#21270;&#30340;&#39118;&#38505;&#20998;&#24067;&#65292;&#20174;&#32780;&#24471;&#21040;&#23545;&#26410;&#30693;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#39044;&#27979;&#22120;&#12290;&#23545;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#27867;&#21270;&#21644;&#35760;&#24518;&#24615;&#33021;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#35813;&#20998;&#26512;&#23637;&#31034;&#20102;&#38543;&#20869;&#23384;&#22823;&#23567;&#30340;&#35760;&#24518;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In continual learning (CL), an AI agent (e.g., autonomous vehicles or robotics) learns from non-stationary data streams under dynamic environments. For the practical deployment of such applications, it is important to guarantee robustness to unseen environments while maintaining past experiences. In this paper, a novel CL framework is proposed to achieve robust generalization to dynamic environments while retaining past knowledge. The considered CL agent uses a capacity-limited memory to save previously observed environmental information to mitigate forgetting issues. Then, data points are sampled from the memory to estimate the distribution of risks over environmental change so as to obtain predictors that are robust with unseen changes. The generalization and memorization performance of the proposed framework are theoretically analyzed. This analysis showcases the tradeoff between memorization and generalization with the memory size. Experiments show that the proposed algorithm outpe
&lt;/p&gt;</description></item><item><title>RaTrack&#26159;&#19968;&#31181;&#38024;&#23545;&#38647;&#36798;&#36319;&#36394;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36816;&#21160;&#20998;&#21106;&#21644;&#32858;&#31867;&#20197;&#21450;&#36816;&#21160;&#20272;&#35745;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23545;&#31227;&#21160;&#29289;&#20307;&#30340;&#31934;&#30830;&#36319;&#36394;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.09737</link><description>&lt;p&gt;
RaTrack: &#24102;&#26377;4D&#38647;&#36798;&#28857;&#20113;&#30340;&#36816;&#21160;&#29289;&#20307;&#26816;&#27979;&#19982;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud. (arXiv:2309.09737v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09737
&lt;/p&gt;
&lt;p&gt;
RaTrack&#26159;&#19968;&#31181;&#38024;&#23545;&#38647;&#36798;&#36319;&#36394;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36816;&#21160;&#20998;&#21106;&#21644;&#32858;&#31867;&#20197;&#21450;&#36816;&#21160;&#20272;&#35745;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23545;&#31227;&#21160;&#29289;&#20307;&#30340;&#31934;&#30830;&#36319;&#36394;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#33258;&#20027;&#24615;&#20381;&#36182;&#20110;&#23545;&#21160;&#24577;&#29615;&#22659;&#30340;&#31934;&#30830;&#24863;&#30693;&#12290;&#22312;3D&#19990;&#30028;&#20013;&#31283;&#23450;&#22320;&#36319;&#36394;&#31227;&#21160;&#29289;&#20307;&#22240;&#27492;&#23545;&#20110;&#36712;&#36857;&#39044;&#27979;&#12289;&#36991;&#38556;&#21644;&#36335;&#24452;&#35268;&#21010;&#31561;&#24212;&#29992;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#21033;&#29992;LiDAR&#25110;&#30456;&#26426;&#36827;&#34892;&#22810;&#30446;&#26631;&#36319;&#36394;&#65288;MOT&#65289;&#65292;&#20294;4D&#25104;&#20687;&#38647;&#36798;&#30340;&#33021;&#21147;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#35748;&#35782;&#21040;4D&#38647;&#36798;&#25968;&#25454;&#20013;&#30340;&#38647;&#36798;&#22122;&#22768;&#21644;&#28857;&#31232;&#30095;&#24615;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RaTrack&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#22522;&#20110;&#38647;&#36798;&#30340;&#36319;&#36394;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25682;&#24323;&#20102;&#23545;&#29305;&#23450;&#23545;&#35937;&#31867;&#22411;&#21644;3D&#36793;&#30028;&#26694;&#30340;&#20381;&#36182;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#36816;&#21160;&#20998;&#21106;&#21644;&#32858;&#31867;&#65292;&#24182;&#37197;&#20197;&#36816;&#21160;&#20272;&#35745;&#27169;&#22359;&#12290;&#22312;View-of-Delft&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;RaTrack&#23637;&#31034;&#20986;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#36816;&#21160;&#29289;&#20307;&#36319;&#36394;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile autonomy relies on the precise perception of dynamic environments. Robustly tracking moving objects in 3D world thus plays a pivotal role for applications like trajectory prediction, obstacle avoidance, and path planning. While most current methods utilize LiDARs or cameras for Multiple Object Tracking (MOT), the capabilities of 4D imaging radars remain largely unexplored. Recognizing the challenges posed by radar noise and point sparsity in 4D radar data, we introduce RaTrack, an innovative solution tailored for radar-based tracking. Bypassing the typical reliance on specific object types and 3D bounding boxes, our method focuses on motion segmentation and clustering, enriched by a motion estimation module. Evaluated on the View-of-Delft dataset, RaTrack showcases superior tracking precision of moving objects, largely surpassing the performance of the state of the art.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#36830;&#25509;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FC-STGNN&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#24314;&#27169;&#22810;&#21464;&#37327;&#26102;&#24207;&#25968;&#25454;&#20013;&#30340;&#26102;&#31354;&#20381;&#36182;&#24615;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#19981;&#21516;&#26102;&#38388;&#25139;&#19978;&#19981;&#21516;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#38754;&#24314;&#27169;&#26102;&#31354;&#20381;&#36182;&#24615;&#30340;&#26032;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2309.05305</link><description>&lt;p&gt;
&#20840;&#36830;&#25509;&#30340;&#26102;&#31354;&#22270;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#24207;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Fully-Connected Spatial-Temporal Graph for Multivariate Time-Series Data. (arXiv:2309.05305v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#36830;&#25509;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FC-STGNN&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#24314;&#27169;&#22810;&#21464;&#37327;&#26102;&#24207;&#25968;&#25454;&#20013;&#30340;&#26102;&#31354;&#20381;&#36182;&#24615;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#19981;&#21516;&#26102;&#38388;&#25139;&#19978;&#19981;&#21516;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#38754;&#24314;&#27169;&#26102;&#31354;&#20381;&#36182;&#24615;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#24207;&#65288;MTS&#65289;&#25968;&#25454;&#22312;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#37117;&#24456;&#37325;&#35201;&#12290;&#30001;&#20110;&#20854;&#24207;&#21015;&#24615;&#21644;&#22810;&#28304;&#24615;&#65288;&#22810;&#20010;&#20256;&#24863;&#22120;&#65289;&#65292;MTS&#25968;&#25454;&#22266;&#26377;&#22320;&#23637;&#29616;&#20102;&#26102;&#31354;&#20381;&#36182;&#24615;&#65292;&#21253;&#25324;&#26102;&#38388;&#25139;&#20043;&#38388;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#21644;&#27599;&#20010;&#26102;&#38388;&#25139;&#20013;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#65292;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65288;GNNs&#65289;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20998;&#21035;&#25429;&#33719;&#31354;&#38388;&#20381;&#36182;&#24615;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#26080;&#27861;&#25429;&#25417;&#19981;&#21516;&#26102;&#38388;&#25139;&#19978;&#19981;&#21516;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#24573;&#35270;&#36825;&#26679;&#30340;&#30456;&#20851;&#24615;&#38480;&#21046;&#20102;&#22312;MTS&#25968;&#25454;&#20013;&#20840;&#38754;&#24314;&#27169;&#26102;&#31354;&#20381;&#36182;&#24615;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#29616;&#26377;GNNs&#23398;&#20064;&#26377;&#25928;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#20840;&#36830;&#25509;&#30340;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;FC-STGNN&#65289;&#65292;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;FC&#22270;&#26500;&#24314;&#21644;FC&#22270;&#21367;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate Time-Series (MTS) data is crucial in various application fields. With its sequential and multi-source (multiple sensors) properties, MTS data inherently exhibits Spatial-Temporal (ST) dependencies, involving temporal correlations between timestamps and spatial correlations between sensors in each timestamp. To effectively leverage this information, Graph Neural Network-based methods (GNNs) have been widely adopted. However, existing approaches separately capture spatial dependency and temporal dependency and fail to capture the correlations between Different sEnsors at Different Timestamps (DEDT). Overlooking such correlations hinders the comprehensive modelling of ST dependencies within MTS data, thus restricting existing GNNs from learning effective representations. To address this limitation, we propose a novel method called Fully-Connected Spatial-Temporal Graph Neural Network (FC-STGNN), including two key components namely FC graph construction and FC graph convolutio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;(MTS)&#20998;&#31867;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24573;&#35270;&#20102;MTS&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#19968;&#33268;&#24615;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22270;&#25193;&#22686;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#20445;&#25345;&#20256;&#24863;&#22120;&#30340;&#31283;&#23450;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#25968;&#25454;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.05202</link><description>&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#22270;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph-Aware Contrasting for Multivariate Time-Series Classification. (arXiv:2309.05202v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05202
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;(MTS)&#20998;&#31867;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#24573;&#35270;&#20102;MTS&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#19968;&#33268;&#24615;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22270;&#25193;&#22686;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#20445;&#25345;&#20256;&#24863;&#22120;&#30340;&#31283;&#23450;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#25968;&#25454;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#65292;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#20998;&#31867;&#20013;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#23427;&#30830;&#20445;&#20102;&#26410;&#26631;&#35760;&#26679;&#26412;&#30340;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#28982;&#21518;&#23398;&#20064;&#36825;&#20123;&#26679;&#26412;&#30340;&#26377;&#25928;&#34920;&#31034;&#12290;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#26102;&#38388;&#25193;&#22686;&#21644;&#23545;&#27604;&#25216;&#26415;&#23454;&#29616;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#26088;&#22312;&#20445;&#25252;MTS&#25968;&#25454;&#30340;&#26102;&#38388;&#27169;&#24335;&#19981;&#21463;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24573;&#35270;&#20102;&#38656;&#35201;&#30830;&#20445;&#20256;&#24863;&#22120;&#30340;&#31283;&#23450;&#24615;&#21644;&#23427;&#20204;&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#31354;&#38388;&#19968;&#33268;&#24615;&#12290;&#30001;&#20110;MTS&#25968;&#25454;&#36890;&#24120;&#26469;&#33258;&#22810;&#20010;&#20256;&#24863;&#22120;&#65292;&#30830;&#20445;&#31354;&#38388;&#19968;&#33268;&#24615;&#23545;&#20110;&#23545;&#27604;&#23398;&#20064;&#22312;MTS&#25968;&#25454;&#19978;&#30340;&#25972;&#20307;&#24615;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;MTS&#25968;&#25454;&#30340;&#31354;&#38388;&#19968;&#33268;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21253;&#25324;&#33410;&#28857;&#21644;&#36793;&#25193;&#22686;&#22312;&#20869;&#30340;&#22270;&#25193;&#22686;&#65292;&#20197;&#20445;&#25345;&#20256;&#24863;&#22120;&#30340;&#31283;&#23450;&#24615;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#28982;&#21518;&#24212;&#29992;&#22270;&#23545;&#27604;&#30446;&#26631;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning, as a self-supervised learning paradigm, becomes popular for Multivariate Time-Series (MTS) classification. It ensures the consistency across different views of unlabeled samples and then learns effective representations for these samples. Existing contrastive learning methods mainly focus on achieving temporal consistency with temporal augmentation and contrasting techniques, aiming to preserve temporal patterns against perturbations for MTS data. However, they overlook spatial consistency that requires the stability of individual sensors and their correlations. As MTS data typically originate from multiple sensors, ensuring spatial consistency becomes essential for the overall performance of contrastive learning on MTS data. Thus, we propose Graph-Aware Contrasting for spatial consistency across MTS data. Specifically, we propose graph augmentations including node and edge augmentations to preserve the stability of sensors and their correlations, followed by grap
&lt;/p&gt;</description></item><item><title>CenTime&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#20107;&#20214;&#26465;&#20214;&#23457;&#26597;&#26426;&#21046;&#30452;&#25509;&#20272;&#35745;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#38388;&#65292;&#22312;&#22788;&#29702;&#26410;&#34987;&#23457;&#26597;&#30340;&#25968;&#25454;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03851</link><description>&lt;p&gt;
CenTime: &#20107;&#20214;&#26465;&#20214;&#27169;&#22411;&#22312;&#29983;&#23384;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
CenTime: Event-Conditional Modelling of Censoring in Survival Analysis. (arXiv:2309.03851v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03851
&lt;/p&gt;
&lt;p&gt;
CenTime&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#20107;&#20214;&#26465;&#20214;&#23457;&#26597;&#26426;&#21046;&#30452;&#25509;&#20272;&#35745;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#38388;&#65292;&#22312;&#22788;&#29702;&#26410;&#34987;&#23457;&#26597;&#30340;&#25968;&#25454;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22522;&#20110;&#22522;&#32447;&#35266;&#27979;&#26469;&#20272;&#35745;&#29305;&#23450;&#20107;&#20214;&#65288;&#22914;&#27515;&#20129;&#25110;&#30284;&#30151;&#22797;&#21457;&#65289;&#21457;&#29983;&#30340;&#26102;&#38388;&#12290;&#36825;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#38750;&#24120;&#26377;&#29992;&#65292;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#25968;&#25454;&#39044;&#27979;&#20020;&#24202;&#37325;&#35201;&#20107;&#20214;&#30340;&#39044;&#21518;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24120;&#24120;&#23384;&#22312;&#23616;&#38480;&#24615;&#65307;&#26377;&#20123;&#26041;&#27861;&#21482;&#20851;&#27880;&#23558;&#24739;&#32773;&#25353;&#29983;&#23384;&#33021;&#21147;&#36827;&#34892;&#25490;&#21517;&#65292;&#24573;&#35270;&#20102;&#23545;&#23454;&#38469;&#20107;&#20214;&#26102;&#38388;&#30340;&#20272;&#35745;&#65307;&#32780;&#20854;&#20182;&#26041;&#27861;&#23558;&#38382;&#39064;&#35270;&#20026;&#20998;&#31867;&#20219;&#21153;&#65292;&#24573;&#35270;&#20102;&#20107;&#20214;&#30340;&#26102;&#38388;&#39034;&#24207;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#26377;&#25928;&#21033;&#29992;&#34987;&#23457;&#26597;&#26679;&#26412;&#65288;&#35757;&#32451;&#25968;&#25454;&#28857;&#65292;&#20854;&#20013;&#30830;&#20999;&#20107;&#20214;&#26102;&#38388;&#19981;&#21487;&#30693;&#65289;&#23545;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CenTime&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#30452;&#25509;&#20272;&#35745;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#21019;&#26032;&#30340;&#20107;&#20214;&#26465;&#20214;&#23457;&#26597;&#26426;&#21046;&#65292;&#21363;&#20351;&#27809;&#26377;&#26410;&#34987;&#23457;&#26597;&#30340;&#25968;&#25454;&#65292;&#20063;&#33021;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival analysis is a valuable tool for estimating the time until specific events, such as death or cancer recurrence, based on baseline observations. This is particularly useful in healthcare to prognostically predict clinically important events based on patient data. However, existing approaches often have limitations; some focus only on ranking patients by survivability, neglecting to estimate the actual event time, while others treat the problem as a classification task, ignoring the inherent time-ordered structure of the events. Furthermore, the effective utilization of censored samples - training data points where the exact event time is unknown - is essential for improving the predictive accuracy of the model. In this paper, we introduce CenTime, a novel approach to survival analysis that directly estimates the time to event. Our method features an innovative event-conditional censoring mechanism that performs robustly even when uncensored data is scarce. We demonstrate that ou
&lt;/p&gt;</description></item><item><title>Matcha-TTS&#26159;&#19968;&#31181;&#24555;&#36895;TTS&#26550;&#26500;&#65292;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#26465;&#20214;&#27969;&#21305;&#37197;&#35757;&#32451;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#36755;&#20986;&#21644;&#24555;&#36895;&#21512;&#25104;&#27493;&#39588;&#12290;&#23427;&#19981;&#38656;&#35201;&#22806;&#37096;&#23545;&#40784;&#65292;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#36895;&#24230;&#26356;&#24555;&#65292;&#24182;&#22312;&#21548;&#35273;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#35780;&#20998;&#12290;</title><link>http://arxiv.org/abs/2309.03199</link><description>&lt;p&gt;
Matcha-TTS: &#19968;&#31181;&#20855;&#26377;&#26465;&#20214;&#27969;&#21305;&#37197;&#30340;&#24555;&#36895;TTS&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Matcha-TTS: A fast TTS architecture with conditional flow matching. (arXiv:2309.03199v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03199
&lt;/p&gt;
&lt;p&gt;
Matcha-TTS&#26159;&#19968;&#31181;&#24555;&#36895;TTS&#26550;&#26500;&#65292;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#26465;&#20214;&#27969;&#21305;&#37197;&#35757;&#32451;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#36755;&#20986;&#21644;&#24555;&#36895;&#21512;&#25104;&#27493;&#39588;&#12290;&#23427;&#19981;&#38656;&#35201;&#22806;&#37096;&#23545;&#40784;&#65292;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#36895;&#24230;&#26356;&#24555;&#65292;&#24182;&#22312;&#21548;&#35273;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Matcha-TTS&#65292;&#19968;&#31181;&#26032;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#36895;TTS&#22768;&#23398;&#24314;&#27169;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#26368;&#20248;&#20256;&#36755;&#26465;&#20214;&#27969;&#21305;&#37197;&#65288;OT-CFM&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20351;&#24471;&#22522;&#20110;ODE&#30340;&#35299;&#30721;&#22120;&#33021;&#22815;&#22312;&#27604;&#20351;&#29992;&#24471;&#20998;&#21305;&#37197;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#23569;&#30340;&#21512;&#25104;&#27493;&#39588;&#20013;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#36755;&#20986;&#12290;&#31934;&#24515;&#35774;&#35745;&#30340;&#36873;&#25321;&#30830;&#20445;&#27599;&#20010;&#21512;&#25104;&#27493;&#39588;&#30340;&#36816;&#34892;&#36895;&#24230;&#24555;&#12290;&#35813;&#26041;&#27861;&#26159;&#27010;&#29575;&#30340;&#12289;&#38750;&#33258;&#22238;&#24402;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#33258;&#20027;&#23398;&#20064;&#35828;&#35805;&#65292;&#26080;&#38656;&#22806;&#37096;&#23545;&#40784;&#12290;&#19982;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#65292;Matcha-TTS&#31995;&#32479;&#20855;&#26377;&#26368;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#19982;&#26368;&#24555;&#27169;&#22411;&#22312;&#38271;&#35821;&#38899;&#29255;&#27573;&#19978;&#30340;&#36895;&#24230;&#30456;&#24403;&#65292;&#24182;&#22312;&#19968;&#39033;&#21548;&#35273;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#35780;&#20998;&#12290;&#35831;&#35775;&#38382;https://shivammehta25.github.io/Matcha-TTS/ &#26597;&#30475;&#38899;&#39057;&#31034;&#20363;&#12289;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Matcha-TTS, a new encoder-decoder architecture for speedy TTS acoustic modelling, trained using optimal-transport conditional flow matching (OT-CFM). This yields an ODE-based decoder capable of high output quality in fewer synthesis steps than models trained using score matching. Careful design choices additionally ensure each synthesis step is fast to run. The method is probabilistic, non-autoregressive, and learns to speak from scratch without external alignments. Compared to strong pre-trained baseline models, the Matcha-TTS system has the smallest memory footprint, rivals the speed of the fastest models on long utterances, and attains the highest mean opinion score in a listening test. Please see https://shivammehta25.github.io/Matcha-TTS/ for audio examples, code, and pre-trained models.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#21307;&#23398;&#22270;&#20687;&#37197;&#20934;&#21644;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#32508;&#36848;&#12290;&#23427;&#21253;&#25324;&#20102;&#22270;&#20687;&#37197;&#20934;&#30340;&#23450;&#20041;&#21644;&#31526;&#21495;&#34920;&#31034;&#65292;&#20197;&#21450;&#21508;&#31181;&#22270;&#20687;&#21464;&#25442;&#21644;&#21307;&#23398;&#22270;&#20687;&#37197;&#20934;&#31639;&#27861;&#12290;&#36824;&#35752;&#35770;&#20102;&#22522;&#20110;&#22270;&#35889;&#30340;&#37197;&#20934;&#21644;&#22810;&#38454;&#27573;&#22270;&#20687;&#37197;&#20934;&#25216;&#26415;&#65292;&#20197;&#21450;&#35780;&#20272;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.00727</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#37197;&#20934;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#65306;&#20171;&#32461;&#19982;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep learning in medical image registration: introduction and survey. (arXiv:2309.00727v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00727
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#21307;&#23398;&#22270;&#20687;&#37197;&#20934;&#21644;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#32508;&#36848;&#12290;&#23427;&#21253;&#25324;&#20102;&#22270;&#20687;&#37197;&#20934;&#30340;&#23450;&#20041;&#21644;&#31526;&#21495;&#34920;&#31034;&#65292;&#20197;&#21450;&#21508;&#31181;&#22270;&#20687;&#21464;&#25442;&#21644;&#21307;&#23398;&#22270;&#20687;&#37197;&#20934;&#31639;&#27861;&#12290;&#36824;&#35752;&#35770;&#20102;&#22522;&#20110;&#22270;&#35889;&#30340;&#37197;&#20934;&#21644;&#22810;&#38454;&#27573;&#22270;&#20687;&#37197;&#20934;&#25216;&#26415;&#65292;&#20197;&#21450;&#35780;&#20272;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#37197;&#20934;&#65288;IR&#65289;&#26159;&#19968;&#31181;&#23558;&#22270;&#20687;&#21464;&#24418;&#20197;&#20351;&#20854;&#30456;&#23545;&#20110;&#21442;&#32771;&#31354;&#38388;&#23545;&#40784;&#30340;&#36807;&#31243;&#65292;&#20351;&#21307;&#21153;&#20154;&#21592;&#33021;&#22815;&#22312;&#26631;&#20934;&#21270;&#30340;&#21442;&#32771;&#26694;&#26550;&#20013;&#26816;&#26597;&#21508;&#31181;&#21307;&#23398;&#22270;&#20687;&#65292;&#22914;&#20855;&#26377;&#30456;&#21516;&#30340;&#26059;&#36716;&#21644;&#32553;&#25918;&#12290;&#26412;&#25991;&#20197;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#20540;&#31034;&#20363;&#20171;&#32461;&#20102;&#22270;&#20687;&#37197;&#20934;&#65292;&#24182;&#25552;&#20379;&#20102;&#22270;&#20687;&#37197;&#20934;&#30340;&#23450;&#20041;&#20197;&#21450;&#31354;&#38388;&#26041;&#21521;&#30340;&#31526;&#21495;&#34920;&#31034;&#12290;&#26412;&#32508;&#36848;&#28085;&#30422;&#20102;&#21508;&#31181;&#22270;&#20687;&#21464;&#25442;&#30340;&#26041;&#38754;&#65292;&#21253;&#25324;&#20223;&#23556;&#12289;&#21487;&#21464;&#24418;&#12289;&#21487;&#36870;&#21644;&#21452;&#21521;&#21464;&#25442;&#65292;&#20197;&#21450;&#21307;&#23398;&#22270;&#20687;&#37197;&#20934;&#31639;&#27861;&#65292;&#22914;Voxelmorph&#12289;Demons&#12289;SyN&#12289;Iterative Closest Point&#21644;SynthMorph&#12290;&#36824;&#25506;&#35752;&#20102;&#22522;&#20110;&#22270;&#35889;&#30340;&#37197;&#20934;&#21644;&#22810;&#38454;&#27573;&#22270;&#20687;&#37197;&#20934;&#25216;&#26415;&#65292;&#21253;&#25324;&#31895;-&#32454;&#21644;&#37329;&#23383;&#22612;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#32508;&#36848;&#36824;&#35752;&#35770;&#20102;&#21307;&#23398;&#22270;&#20687;&#37197;&#20934;&#30340;&#20998;&#31867;&#20307;&#31995;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#22914;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#24230;&#37327;&#21644;&#22522;&#20110;&#20998;&#21106;&#30340;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image registration (IR) is a process that deforms images to align them with respect to a reference space, making it easier for medical practitioners to examine various medical images in a standardized reference frame, such as having the same rotation and scale. This document introduces image registration using a simple numeric example. It provides a definition of image registration along with a space-oriented symbolic representation. This review covers various aspects of image transformations, including affine, deformable, invertible, and bidirectional transformations, as well as medical image registration algorithms such as Voxelmorph, Demons, SyN, Iterative Closest Point, and SynthMorph. It also explores atlas-based registration and multistage image registration techniques, including coarse-fine and pyramid approaches. Furthermore, this survey paper discusses medical image registration taxonomies, datasets, evaluation measures, such as correlation-based metrics, segmentation-based me
&lt;/p&gt;</description></item><item><title>&#22810;&#20445;&#30495;&#24230;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#22320;&#36136;&#30899;&#20648;&#23384;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#32463;&#27982;&#24615;&#26356;&#39640;&#30340;&#22810;&#20445;&#30495;&#24230;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#20197;&#19982;&#39640;&#20445;&#30495;&#24230;&#27169;&#22411;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.09113</link><description>&lt;p&gt;
&#22810;&#20445;&#30495;&#24230;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#29992;&#20110;&#24555;&#36895;&#24314;&#27169;&#22823;&#35268;&#27169;&#22320;&#36136;&#30899;&#20648;&#23384;
&lt;/p&gt;
&lt;p&gt;
Multi-fidelity Fourier Neural Operator for Fast Modeling of Large-Scale Geological Carbon Storage. (arXiv:2308.09113v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09113
&lt;/p&gt;
&lt;p&gt;
&#22810;&#20445;&#30495;&#24230;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#22320;&#36136;&#30899;&#20648;&#23384;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#32463;&#27982;&#24615;&#26356;&#39640;&#30340;&#22810;&#20445;&#30495;&#24230;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#20197;&#19982;&#39640;&#20445;&#30495;&#24230;&#27169;&#22411;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#20195;&#29702;&#27169;&#22411;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#22320;&#36136;&#30899;&#20648;&#23384;&#65288;GCS&#65289;&#38382;&#39064;&#65292;&#20197;&#21152;&#24555;&#39044;&#27979;&#20648;&#21387;&#21644;&#20108;&#27687;&#21270;&#30899;&#20113;&#23618;&#31227;&#21160;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#22823;&#35268;&#27169;&#19977;&#32500;&#38382;&#39064;&#30340;&#21487;&#29992;&#35757;&#32451;&#25968;&#25454;&#22987;&#32456;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22810;&#20445;&#30495;&#24230;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;GCS&#38382;&#39064;&#65292;&#21033;&#29992;&#26356;&#20855;&#32463;&#27982;&#24615;&#30340;&#22810;&#20445;&#30495;&#24230;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#20855;&#26377;&#33391;&#22909;&#30340;&#32593;&#26684;&#19981;&#21464;&#24615;&#65292;&#31616;&#21270;&#20102;&#19981;&#21516;&#31163;&#25955;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#36801;&#31227;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#19968;&#20010;GCS&#20648;&#23618;&#27169;&#22411;&#19978;&#36827;&#34892;&#27169;&#22411;&#26377;&#25928;&#24615;&#27979;&#35797;&#65292;&#35813;&#27169;&#22411;&#34987;&#21010;&#20998;&#20026;110,000&#20010;&#32593;&#26684;&#21333;&#20803;&#12290;&#22810;&#20445;&#30495;&#24230;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#21487;&#19982;&#39640;&#20445;&#30495;&#24230;&#27169;&#22411;&#30340;&#35757;&#32451;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based surrogate models have been widely applied in geological carbon storage (GCS) problems to accelerate the prediction of reservoir pressure and CO2 plume migration. Large amounts of data from physics-based numerical simulators are required to train a model to accurately predict the complex physical behaviors associated with this process. In practice, the available training data are always limited in large-scale 3D problems due to the high computational cost. Therefore, we propose to use a multi-fidelity Fourier Neural Operator to solve large-scale GCS problems with more affordable multi-fidelity training datasets. The Fourier Neural Operator has a desirable grid-invariant property, which simplifies the transfer learning procedure between datasets with different discretization. We first test the model efficacy on a GCS reservoir model being discretized into 110k grid cells. The multi-fidelity model can predict with accuracy comparable to a high-fidelity model trained wi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#32447;&#24615;&#12289;&#21453;&#39304;&#21644;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#20110;&#24378;&#21487;&#38752;&#24615;&#30340;k-Triangle Faithfulness&#30340;&#26367;&#20195;&#23450;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.07520</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#12289;&#21453;&#39304;&#21644;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Nonlinearity, Feedback and Uniform Consistency in Causal Structural Learning. (arXiv:2308.07520v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07520
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#32447;&#24615;&#12289;&#21453;&#39304;&#21644;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20013;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#20110;&#24378;&#21487;&#38752;&#24615;&#30340;k-Triangle Faithfulness&#30340;&#26367;&#20195;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#30340;&#30446;&#26631;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25214;&#21040;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#30340;&#33258;&#21160;&#21270;&#25628;&#32034;&#26041;&#27861;&#12290;&#26377;&#20123;&#24773;&#20917;&#19979;&#65292;&#24863;&#20852;&#36259;&#30340;&#22240;&#26524;&#26426;&#21046;&#30340;&#25152;&#26377;&#21464;&#37327;&#37117;&#24050;&#32463;&#34987;&#27979;&#37327;&#65292;&#20219;&#21153;&#26159;&#39044;&#27979;&#19968;&#20010;&#21464;&#37327;&#23545;&#21478;&#19968;&#20010;&#21464;&#37327;&#30340;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#26377;&#26102;&#20027;&#35201;&#20851;&#27880;&#30340;&#21464;&#37327;&#24182;&#38750;&#30452;&#25509;&#21487;&#35266;&#23519;&#65292;&#32780;&#26159;&#36890;&#36807;&#23427;&#20204;&#22312;&#25968;&#25454;&#20013;&#30340;&#34920;&#29616;&#26469;&#25512;&#29702;&#20986;&#26469;&#30340;&#12290;&#36825;&#20123;&#34987;&#31216;&#20026;&#28508;&#22312;&#21464;&#37327;&#12290;&#19968;&#20010;&#24191;&#27867;&#34987;&#30693;&#36947;&#30340;&#20363;&#23376;&#26159;&#24515;&#29702;&#26500;&#36896;&#30340;&#26234;&#21830;&#65292;&#22240;&#20026;&#26080;&#27861;&#30452;&#25509;&#27979;&#37327;&#65292;&#25152;&#20197;&#30740;&#31350;&#20154;&#21592;&#23581;&#35797;&#36890;&#36807;&#21508;&#31181;&#25351;&#26631;&#22914;&#26234;&#21830;&#27979;&#35797;&#26469;&#35780;&#20272;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#21487;&#20197;&#25581;&#31034;&#28508;&#22312;&#21464;&#37327;&#20043;&#38388;&#21644;&#28508;&#22312;&#21464;&#37327;&#19982;&#35266;&#23519;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#36830;&#25509;&#65292;&#20174;&#32780;&#21457;&#29616;&#28508;&#22312;&#30340;&#27169;&#24335;&#21644;&#32467;&#26500;&#12290;&#36825;&#31687;&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#20004;&#20010;&#38382;&#39064;&#65306;&#25552;&#20379;&#20102;&#19968;&#20010;&#24369;&#20110;&#24378;&#21487;&#38752;&#24615;&#30340;k-Triangle Faithfulness&#30340;&#26367;&#20195;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#32479;&#35745;&#19968;&#33268;&#24615;&#30340;&#26032;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of Causal Discovery is to find automated search methods for learning causal structures from observational data. In some cases all variables of the interested causal mechanism are measured, and the task is to predict the effects one measured variable has on another. In contrast, sometimes the variables of primary interest are not directly observable but instead inferred from their manifestations in the data. These are referred to as latent variables. One commonly known example is the psychological construct of intelligence, which cannot directly measured so researchers try to assess through various indicators such as IQ tests. In this case, casual discovery algorithms can uncover underlying patterns and structures to reveal the causal connections between the latent variables and between the latent and observed variables. This thesis focuses on two questions in causal discovery: providing an alternative definition of k-Triangle Faithfulness that (i) is weaker than strong faithfu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;SIR&#27169;&#22411;&#30740;&#31350;&#20102;2020&#24180;&#35199;&#37096;&#32654;&#22269;&#28779;&#28798;&#23395;&#30340;&#28798;&#23475;&#21709;&#24212;&#12290;&#30740;&#31350;&#21457;&#29616;Twitter&#29992;&#25143;&#20027;&#35201;&#20851;&#27880;&#20581;&#24247;&#24433;&#21709;&#12289;&#25439;&#22833;&#21644;&#25764;&#31163;&#19977;&#20010;&#20027;&#39064;&#65292;&#24182;&#20351;&#29992;SIR&#29702;&#35770;&#25506;&#32034;&#20102;&#36825;&#20123;&#20027;&#39064;&#22312;Twitter&#19978;&#30340;&#20256;&#25773;&#35268;&#27169;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.05281</link><description>&lt;p&gt;
&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;&#26131;&#24863;-&#24863;&#26579;-&#24247;&#22797;&#65288;SIR&#65289;&#27169;&#22411;&#30740;&#31350;&#28798;&#23475;&#21709;&#24212;&#65306;&#20197;2020&#24180;&#35199;&#37096;&#32654;&#22269;&#28779;&#28798;&#23395;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating disaster response through social media data and the Susceptible-Infected-Recovered (SIR) model: A case study of 2020 Western U.S. wildfire season. (arXiv:2308.05281v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05281
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;SIR&#27169;&#22411;&#30740;&#31350;&#20102;2020&#24180;&#35199;&#37096;&#32654;&#22269;&#28779;&#28798;&#23395;&#30340;&#28798;&#23475;&#21709;&#24212;&#12290;&#30740;&#31350;&#21457;&#29616;Twitter&#29992;&#25143;&#20027;&#35201;&#20851;&#27880;&#20581;&#24247;&#24433;&#21709;&#12289;&#25439;&#22833;&#21644;&#25764;&#31163;&#19977;&#20010;&#20027;&#39064;&#65292;&#24182;&#20351;&#29992;SIR&#29702;&#35770;&#25506;&#32034;&#20102;&#36825;&#20123;&#20027;&#39064;&#22312;Twitter&#19978;&#30340;&#20256;&#25773;&#35268;&#27169;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#28798;&#23475;&#21709;&#24212;&#23545;&#21463;&#24433;&#21709;&#30340;&#31038;&#21306;&#33267;&#20851;&#37325;&#35201;&#12290;&#24212;&#24613;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#22312;&#28798;&#23475;&#26399;&#38388;&#22312;&#20102;&#35299;&#31038;&#21306;&#25152;&#38754;&#20020;&#38382;&#39064;&#30340;&#21487;&#38752;&#21644;&#21450;&#26102;&#30340;&#25351;&#26631;&#19978;&#23558;&#21463;&#30410;&#20110;&#31038;&#20132;&#23186;&#20307;&#25552;&#20379;&#30340;&#20016;&#23500;&#25968;&#25454;&#26469;&#28304;&#12290;&#31038;&#20132;&#23186;&#20307;&#21487;&#20197;&#21453;&#26144;&#20844;&#20247;&#20851;&#27880;&#21644;&#38656;&#27714;&#65292;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#35265;&#65292;&#20197;&#20102;&#35299;&#19981;&#26029;&#28436;&#21464;&#30340;&#24773;&#20917;&#24182;&#20248;&#21270;&#36164;&#28304;&#37197;&#32622;&#12290;&#25105;&#20204;&#20351;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#36716;&#25442;&#65288;BERT&#65289;&#20027;&#39064;&#24314;&#27169;&#23545;Twitter&#25968;&#25454;&#36827;&#34892;&#20027;&#39064;&#32858;&#31867;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#26102;&#38388;-&#31354;&#38388;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#20027;&#39064;&#22312;2020&#24180;&#32654;&#22269;&#35199;&#37096;&#28779;&#28798;&#23395;&#26399;&#38388;&#22312;&#19981;&#21516;&#22320;&#21306;&#30340;&#20998;&#24067;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;Twitter&#29992;&#25143;&#20027;&#35201;&#20851;&#27880;&#19977;&#20010;&#20027;&#39064;&#65306;&#8220;&#20581;&#24247;&#24433;&#21709;&#8221;&#65292;&#8220;&#25439;&#22833;&#8221;&#65292;&#8220;&#25764;&#31163;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#26131;&#24863;-&#24863;&#26579;-&#24247;&#22797;&#65288;SIR&#65289;&#29702;&#35770;&#26469;&#25506;&#32034;&#20027;&#39064;&#22312;Twitter&#19978;&#30340;&#20256;&#25773;&#35268;&#27169;&#21644;&#36895;&#24230;&#12290;&#32467;&#26524;&#28165;&#26224;&#22320;&#26174;&#31034;&#20102;&#20027;&#39064;&#20256;&#25773;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective disaster response is critical for affected communities. Responders and decision-makers would benefit from reliable, timely measures of the issues impacting their communities during a disaster, and social media offers a potentially rich data source. Social media can reflect public concerns and demands during a disaster, offering valuable insights for decision-makers to understand evolving situations and optimize resource allocation. We used Bidirectional Encoder Representations from Transformers (BERT) topic modeling to cluster topics from Twitter data. Then, we conducted a temporal-spatial analysis to examine the distribution of these topics across different regions during the 2020 western U.S. wildfire season. Our results show that Twitter users mainly focused on three topics:"health impact," "damage," and "evacuation." We used the Susceptible-Infected-Recovered (SIR) theory to explore the magnitude and velocity of topic diffusion on Twitter. The results displayed a clear re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#22312;&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#21457;&#29616;&#31616;&#21333;&#27169;&#22411;&#22312;&#29983;&#25104;&#21333;&#20010;&#36712;&#36857;&#26102;&#20173;&#28982;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#26576;&#20123;&#36890;&#24120;&#34987;&#35748;&#20026;&#26377;&#29992;&#30340;&#29305;&#24449;&#23545;&#25972;&#20307;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2308.05194</link><description>&lt;p&gt;
&#35780;&#20272;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evaluating Pedestrian Trajectory Prediction Methods for the Application in Autonomous Driving. (arXiv:2308.05194v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#22312;&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#21457;&#29616;&#31616;&#21333;&#27169;&#22411;&#22312;&#29983;&#25104;&#21333;&#20010;&#36712;&#36857;&#26102;&#20173;&#28982;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#26576;&#20123;&#36890;&#24120;&#34987;&#35748;&#20026;&#26377;&#29992;&#30340;&#29305;&#24449;&#23545;&#25972;&#20307;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#39046;&#22495;&#30340;&#26368;&#26032;&#26041;&#27861;&#19982;&#24120;&#36895;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#35780;&#20272;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;ETH/UCY&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#65292;&#25253;&#21578;&#20102;&#24179;&#22343;&#20301;&#31227;&#35823;&#24046;&#65288;ADE&#65289;&#21644;&#26368;&#32456;&#20301;&#31227;&#35823;&#24046;&#65288;FDE&#65289;&#12290;&#20026;&#20102;&#31526;&#21512;&#23454;&#38469;&#24212;&#29992;&#30340;&#35201;&#27714;&#65292;&#23545;&#21021;&#22987;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#36827;&#34892;&#20102;&#20462;&#25913;&#12290;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;&#35266;&#23519;&#21040;&#30340;&#36816;&#21160;&#21382;&#21490;&#23545;&#39044;&#27979;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#24314;&#31435;&#26356;&#22909;&#30340;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#36824;&#27979;&#37327;&#20102;&#27599;&#20010;&#27169;&#22411;&#30340;&#25512;&#29702;&#26102;&#38388;&#65292;&#20197;&#35780;&#20272;&#38754;&#23545;&#19981;&#21516;&#25968;&#37327;&#20195;&#29702;&#26102;&#27599;&#20010;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#29983;&#25104;&#21333;&#20010;&#36712;&#36857;&#26102;&#65292;&#31616;&#21333;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#26576;&#20123;&#36890;&#24120;&#34987;&#35748;&#20026;&#26377;&#29992;&#30340;&#29305;&#24449;&#23545;&#25972;&#20307;&#24615;&#33021;&#24433;&#21709;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, the state of the art in the field of pedestrian trajectory prediction is evaluated alongside the constant velocity model (CVM) with respect to its applicability in autonomous vehicles. The evaluation is conducted on the widely-used ETH/UCY dataset where the Average Displacement Error (ADE) and the Final Displacement Error (FDE) are reported. To align with requirements in real-world applications, modifications are made to the input features of the initially proposed models. An ablation study is conducted to examine the influence of the observed motion history on the prediction performance, thereby establishing a better understanding of its impact. Additionally, the inference time of each model is measured to evaluate the scalability of each model when confronted with varying amounts of agents. The results demonstrate that simple models remain competitive when generating single trajectories, and certain features commonly thought of as useful have little impact on the overa
&lt;/p&gt;</description></item><item><title>SLEM&#26159;&#19968;&#31181;&#36335;&#24452;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#36229;&#32423;&#23398;&#20064;&#32773;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#19988;&#26080;&#20559;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#24182;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20851;&#31995;&#26102;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.04365</link><description>&lt;p&gt;
SLEM&#65306;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#36335;&#24452;&#24314;&#27169;&#21644;&#22240;&#26524;&#25512;&#26029;&#30340;&#36229;&#32423;&#23398;&#20064;&#32773;&#26041;&#31243;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling. (arXiv:2308.04365v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04365
&lt;/p&gt;
&lt;p&gt;
SLEM&#26159;&#19968;&#31181;&#36335;&#24452;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#36229;&#32423;&#23398;&#20064;&#32773;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#19988;&#26080;&#20559;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#24182;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20851;&#31995;&#26102;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#26159;&#31185;&#23398;&#30340;&#20851;&#38190;&#30446;&#26631;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#36890;&#36807;&#35266;&#23519;&#25968;&#25454;&#24471;&#20986;&#20851;&#20110;&#23545;&#20551;&#23450;&#24178;&#39044;&#30340;&#39044;&#27979;&#30340;&#26377;&#24847;&#20041;&#30340;&#32467;&#35770;&#12290;&#36335;&#24452;&#27169;&#22411;&#12289;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;(SEMs)&#20197;&#21450;&#26356;&#19968;&#33324;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs)&#33021;&#22815;&#26126;&#30830;&#22320;&#25351;&#23450;&#20851;&#20110;&#29616;&#35937;&#32972;&#21518;&#30340;&#22240;&#26524;&#32467;&#26500;&#30340;&#20551;&#35774;&#12290;&#19982;DAGs&#19981;&#21516;&#65292;SEMs&#20551;&#35774;&#32447;&#24615;&#20851;&#31995;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20989;&#25968;&#38169;&#35823;&#35268;&#33539;&#65292;&#20174;&#32780;&#38459;&#30861;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#21487;&#38752;&#30340;&#25928;&#26524;&#22823;&#23567;&#20272;&#35745;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36229;&#32423;&#23398;&#20064;&#32773;&#26041;&#31243;&#27169;&#22411;&#65288;SLEM&#65289;&#65292;&#19968;&#31181;&#38598;&#25104;&#20102;&#26426;&#22120;&#23398;&#20064;&#36229;&#32423;&#23398;&#20064;&#32773;&#38598;&#25104;&#30340;&#36335;&#24452;&#24314;&#27169;&#25216;&#26415;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;SLEM&#33021;&#22815;&#25552;&#20379;&#19968;&#33268;&#19988;&#26080;&#20559;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#22312;&#19982;SEMs&#36827;&#34892;&#32447;&#24615;&#27169;&#22411;&#27604;&#36739;&#26102;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20851;&#31995;&#26102;&#20248;&#20110;SEMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference is a crucial goal of science, enabling researchers to arrive at meaningful conclusions regarding the predictions of hypothetical interventions using observational data. Path models, Structural Equation Models (SEMs), and, more generally, Directed Acyclic Graphs (DAGs), provide a means to unambiguously specify assumptions regarding the causal structure underlying a phenomenon. Unlike DAGs, which make very few assumptions about the functional and parametric form, SEM assumes linearity. This can result in functional misspecification which prevents researchers from undertaking reliable effect size estimation. In contrast, we propose Super Learner Equation Modeling, a path modeling technique integrating machine learning Super Learner ensembles. We empirically demonstrate its ability to provide consistent and unbiased estimates of causal effects, its competitive performance for linear models when compared with SEM, and highlight its superiority over SEM when dealing with non
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#26368;&#21518;&#19968;&#23618;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#65292;Deep Feature Reweighting&#65288;DFR&#65289;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#25968;&#25454;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#21307;&#23398;&#25968;&#25454;&#26102;&#23384;&#22312;&#19968;&#23450;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00473</link><description>&lt;p&gt;
&#26368;&#21518;&#19968;&#23618;&#30340;&#35757;&#32451;&#26159;&#21542;&#36275;&#20197;&#24212;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Last Layer Re-Training Truly Sufficient for Robustness to Spurious Correlations?. (arXiv:2308.00473v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00473
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#26368;&#21518;&#19968;&#23618;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#65292;Deep Feature Reweighting&#65288;DFR&#65289;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#25968;&#25454;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#21307;&#23398;&#25968;&#25454;&#26102;&#23384;&#22312;&#19968;&#23450;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#35757;&#32451;&#30340;&#27169;&#22411;&#24050;&#34987;&#30693;&#26195;&#23398;&#20250;&#20381;&#36182;&#34394;&#20551;&#29305;&#24449;&#65292;&#21363;&#23427;&#20204;&#30340;&#39044;&#27979;&#22522;&#20110;&#19982;&#31867;&#21035;&#26631;&#31614;&#24378;&#30456;&#20851;&#20294;&#32570;&#20047;&#22240;&#26524;&#25512;&#29702;&#30340;&#38750;&#26399;&#26395;&#36741;&#21161;&#29305;&#24449;&#12290;&#36825;&#31181;&#34892;&#20026;&#23588;&#20854;&#22312;&#30456;&#20851;&#31867;&#21035;&#30340;&#26679;&#26412;&#32452;&#20013;&#65292;&#21487;&#33021;&#27809;&#26377;&#36825;&#20123;&#34394;&#20551;&#29305;&#24449;&#25110;&#32773;&#30456;&#21453;&#31867;&#21035;&#30340;&#26679;&#26412;&#20013;&#23384;&#22312;&#36825;&#20123;&#34394;&#20551;&#29305;&#24449;&#26102;&#65292;&#23548;&#33268;&#20934;&#30830;&#24615;&#30340;&#19979;&#38477;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#28145;&#24230;&#29305;&#24449;&#37325;&#21152;&#26435;&#65288;DFR&#65289;&#26041;&#27861;&#25552;&#39640;&#20102;&#36825;&#20123;&#26368;&#24046;&#26679;&#26412;&#32452;&#30340;&#20934;&#30830;&#24615;&#12290;&#22522;&#20110;ERM&#27169;&#22411;&#21487;&#20197;&#36275;&#22815;&#22909;&#22320;&#23398;&#20064;&#26680;&#24515;&#29305;&#24449;&#30340;&#20027;&#35201;&#35770;&#28857;&#65292;DFR&#21482;&#38656;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#26368;&#21518;&#19968;&#23618;&#36827;&#34892;&#23567;&#35268;&#27169;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26816;&#39564;&#20102;DFR&#22312;&#21307;&#23398;&#39046;&#22495;&#30495;&#23454;&#25968;&#25454;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#26368;&#21518;&#19968;&#23618;&#37325;&#26032;&#35757;&#32451;&#26377;&#25928;&#24615;&#32972;&#21518;&#30340;&#25512;&#29702;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#32467;&#26524;&#34920;&#26126;&#23613;&#31649;DFR&#20855;&#26377;&#25552;&#39640;&#26368;&#24046;&#26679;&#26412;&#32452;&#20934;&#30830;&#24615;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#23454;&#29616;&#26041;&#24335;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models trained with empirical risk minimization (ERM) are known to learn to rely on spurious features, i.e., their prediction is based on undesired auxiliary features which are strongly correlated with class labels but lack causal reasoning. This behavior particularly degrades accuracy in groups of samples of the correlated class that are missing the spurious feature or samples of the opposite class but with the spurious feature present. The recently proposed Deep Feature Reweighting (DFR) method improves accuracy of these worst groups. Based on the main argument that ERM mods can learn core features sufficiently well, DFR only needs to retrain the last layer of the classification model with a small group-balanced data set. In this work, we examine the applicability of DFR to realistic data in the medical domain. Furthermore, we investigate the reasoning behind the effectiveness of last-layer retraining and show that even though DFR has the potential to improve the accuracy of the wors
&lt;/p&gt;</description></item><item><title>MSQNet&#26159;&#19968;&#31181;&#26080;&#20851;&#28436;&#21592;&#30340;&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#21160;&#20316;&#31867;&#21035;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#38024;&#23545;&#29305;&#23450;&#28436;&#21592;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.10763</link><description>&lt;p&gt;
MSQNet: &#26080;&#20851;&#28436;&#21592;&#30340;&#22810;&#27169;&#24577;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MSQNet: Actor-agnostic Action Recognition with Multi-modal Query. (arXiv:2307.10763v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10763
&lt;/p&gt;
&lt;p&gt;
MSQNet&#26159;&#19968;&#31181;&#26080;&#20851;&#28436;&#21592;&#30340;&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#21160;&#20316;&#31867;&#21035;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#38024;&#23545;&#29305;&#23450;&#28436;&#21592;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#36890;&#24120;&#26159;&#38024;&#23545;&#29305;&#23450;&#28436;&#21592;&#30340;&#65292;&#22240;&#20026;&#28436;&#21592;&#20043;&#38388;&#20855;&#26377;&#22266;&#26377;&#30340;&#25299;&#25169;&#21644;&#26174;&#30528;&#24046;&#24322;&#12290;&#36825;&#23601;&#38656;&#35201;&#29305;&#23450;&#28436;&#21592;&#30340;&#23039;&#24577;&#20272;&#35745;&#65288;&#20363;&#22914;&#20154;&#31867;&#19982;&#21160;&#29289;&#65289;&#65292;&#23548;&#33268;&#27169;&#22411;&#35774;&#35745;&#22797;&#26434;&#24615;&#21644;&#39640;&#32500;&#25252;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36890;&#24120;&#21482;&#20851;&#27880;&#23398;&#20064;&#35270;&#35273;&#27169;&#24577;&#21644;&#21333;&#26631;&#31614;&#20998;&#31867;&#65292;&#24573;&#35270;&#20102;&#20854;&#20182;&#21487;&#29992;&#20449;&#24687;&#28304;&#65288;&#20363;&#22914;&#31867;&#21517;&#25991;&#26412;&#65289;&#21644;&#22810;&#20010;&#21160;&#20316;&#30340;&#21516;&#26102;&#21457;&#29983;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#26080;&#20851;&#28436;&#21592;&#30340;&#22810;&#27169;&#24577;&#22810;&#26631;&#31614;&#21160;&#20316;&#35782;&#21035;&#8221;&#65292;&#20026;&#21253;&#25324;&#20154;&#31867;&#21644;&#21160;&#29289;&#22312;&#20869;&#30340;&#21508;&#31181;&#31867;&#22411;&#30340;&#28436;&#21592;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#22522;&#20110;Transformer&#30340;&#30446;&#26631;&#26816;&#27979;&#26694;&#26550;&#65288;&#20363;&#22914;DETR&#65289;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#26597;&#35810;&#32593;&#32476;&#65288;MSQNet&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#26356;&#22909;&#22320;&#34920;&#31034;&#21160;&#20316;&#31867;&#21035;&#12290;&#28040;&#38500;&#20102;&#28436;&#21592;&#29305;&#23450;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing action recognition methods are typically actor-specific due to the intrinsic topological and apparent differences among the actors. This requires actor-specific pose estimation (e.g., humans vs. animals), leading to cumbersome model design complexity and high maintenance costs. Moreover, they often focus on learning the visual modality alone and single-label classification whilst neglecting other available information sources (e.g., class name text) and the concurrent occurrence of multiple actions. To overcome these limitations, we propose a new approach called 'actor-agnostic multi-modal multi-label action recognition,' which offers a unified solution for various types of actors, including humans and animals. We further formulate a novel Multi-modal Semantic Query Network (MSQNet) model in a transformer-based object detection framework (e.g., DETR), characterized by leveraging visual and textual modalities to represent the action classes better. The elimination of actor-spec
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#24322;&#26500;&#29305;&#24449;&#38598;&#25104;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#26500;&#24314;&#30340;&#23725;&#22238;&#24402;&#38598;&#25104;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#32467;&#26524;&#34920;&#26126;&#24322;&#26500;&#29305;&#24449;&#38598;&#25104;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03176</link><description>&lt;p&gt;
&#24322;&#26500;&#29305;&#24449;&#23376;&#37319;&#26679;&#30340;Ridge Ensemble&#30340;&#23398;&#20064;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Learning Curves for Heterogeneous Feature-Subsampled Ridge Ensembles. (arXiv:2307.03176v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03176
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#24322;&#26500;&#29305;&#24449;&#38598;&#25104;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#26500;&#24314;&#30340;&#23725;&#22238;&#24402;&#38598;&#25104;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#32467;&#26524;&#34920;&#26126;&#24322;&#26500;&#29305;&#24449;&#38598;&#25104;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#21253;&#35013;&#26159;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#22312;&#38543;&#26426;&#23376;&#26679;&#26412;&#25110;&#29305;&#24449;&#25237;&#24433;&#19978;&#35757;&#32451;&#20272;&#35745;&#22120;&#26469;&#20943;&#23569;&#39044;&#27979;&#26041;&#24046;&#30340;&#25104;&#29087;&#38598;&#25104;&#26041;&#27861;&#12290;&#36890;&#24120;&#65292;&#38598;&#25104;&#36873;&#25321;&#26159;&#21516;&#36136;&#30340;&#65292;&#21363;&#20272;&#35745;&#22120;&#21487;&#29992;&#30340;&#29305;&#24449;&#32500;&#25968;&#22312;&#25972;&#20010;&#38598;&#25104;&#20013;&#26159;&#22343;&#21248;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24322;&#26500;&#29305;&#24449;&#38598;&#25104;&#26041;&#27861;&#65292;&#20854;&#20013;&#30340;&#20272;&#35745;&#22120;&#22522;&#20110;&#21464;&#21160;&#30340;&#29305;&#24449;&#32500;&#25968;&#65292;&#24182;&#30740;&#31350;&#20854;&#22312;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#32447;&#24615;&#39044;&#27979;&#22120;&#30340;&#38598;&#25104;&#65292;&#27599;&#20010;&#39044;&#27979;&#22120;&#20351;&#29992;&#37096;&#20998;&#21487;&#29992;&#29305;&#24449;&#36827;&#34892;&#23725;&#22238;&#24402;&#25311;&#21512;&#12290;&#25105;&#20204;&#20801;&#35768;&#36825;&#20123;&#23376;&#38598;&#20013;&#21253;&#21547;&#30340;&#29305;&#24449;&#25968;&#37327;&#26377;&#25152;&#21464;&#21270;&#12290;&#21033;&#29992;&#32479;&#35745;&#29289;&#29702;&#20013;&#30340;&#22797;&#21046;&#25216;&#24039;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20855;&#26377;&#30830;&#23450;&#24615;&#32447;&#24615;&#25513;&#27169;&#30340;&#23725;&#22238;&#24402;&#38598;&#25104;&#30340;&#23398;&#20064;&#26354;&#32447;&#12290;&#23545;&#20110;&#20855;&#26377;&#21508;&#21521;&#21516;&#24615;&#29305;&#24449;&#22122;&#22768;&#30340;&#31561;&#30456;&#30456;&#20851;&#25968;&#25454;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#23398;&#20064;&#26354;&#32447;&#30340;&#26174;&#24335;&#34920;&#36798;&#24335;&#12290;&#21033;&#29992;&#36825;&#20123;&#25512;&#23548;&#34920;&#36798;&#24335;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38598;&#25104;&#22312;&#19981;&#21516;&#29305;&#24449;&#32500;&#25968;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature bagging is a well-established ensembling method which aims to reduce prediction variance by training estimators in an ensemble on random subsamples or projections of features. Typically, ensembles are chosen to be homogeneous, in the sense the the number of feature dimensions available to an estimator is uniform across the ensemble. Here, we introduce heterogeneous feature ensembling, with estimators built on varying number of feature dimensions, and consider its performance in a linear regression setting. We study an ensemble of linear predictors, each fit using ridge regression on a subset of the available features. We allow the number of features included in these subsets to vary. Using the replica trick from statistical physics, we derive learning curves for ridge ensembles with deterministic linear masks. We obtain explicit expressions for the learning curves in the case of equicorrelated data with an isotropic feature noise. Using the derived expressions, we investigate t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#32452;&#21512;&#24615;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#23618;&#27425;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#65292;&#21457;&#29616;&#28145;&#24230;CNN&#23398;&#20064;&#36825;&#20010;&#20219;&#21153;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#38543;&#30528;&#31867;&#21035;&#25968;&#12289;&#32452;&#21512;&#25968;&#21644;&#36845;&#20195;&#27425;&#25968;&#30340;&#22686;&#21152;&#32780;&#28176;&#36827;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2307.02129</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#23398;&#20064;&#32452;&#21512;&#24615;&#25968;&#25454;&#65306;&#38543;&#26426;&#23618;&#27425;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model. (arXiv:2307.02129v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#32452;&#21512;&#24615;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#23618;&#27425;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#65292;&#21457;&#29616;&#28145;&#24230;CNN&#23398;&#20064;&#36825;&#20010;&#20219;&#21153;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#38543;&#30528;&#31867;&#21035;&#25968;&#12289;&#32452;&#21512;&#25968;&#21644;&#36845;&#20195;&#27425;&#25968;&#30340;&#22686;&#21152;&#32780;&#28176;&#36827;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#19968;&#33324;&#39640;&#32500;&#20219;&#21153;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#19982;&#32500;&#24230;&#25104;&#25351;&#25968;&#22686;&#38271;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#25104;&#21151;&#12290;&#19968;&#31181;&#26222;&#36941;&#30340;&#20551;&#35774;&#26159;&#21487;&#23398;&#20064;&#20219;&#21153;&#20855;&#26377;&#39640;&#24230;&#32467;&#26500;&#21270;&#65292;CNN&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#24314;&#31435;&#20102;&#25968;&#25454;&#30340;&#20302;&#32500;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#23427;&#20204;&#38656;&#35201;&#22810;&#23569;&#35757;&#32451;&#25968;&#25454;&#20197;&#21450;&#36825;&#20010;&#25968;&#23383;&#22914;&#20309;&#21462;&#20915;&#20110;&#25968;&#25454;&#32467;&#26500;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#25991;&#22238;&#31572;&#20102;&#38024;&#23545;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#31867;&#20219;&#21153;&#30340;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#25429;&#25417;&#30495;&#23454;&#25968;&#25454;&#30340;&#30456;&#20851;&#26041;&#38754;&#65306;&#38543;&#26426;&#23618;&#27425;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;$n_c$&#20010;&#31867;&#21035;&#20013;&#30340;&#27599;&#19968;&#20010;&#23545;&#24212;&#20110;$m$&#20010;&#21516;&#20041;&#32452;&#21512;&#30340;&#39640;&#23618;&#27425;&#29305;&#24449;&#65292;&#24182;&#19988;&#36825;&#20123;&#29305;&#24449;&#21448;&#36890;&#36807;&#19968;&#20010;&#37325;&#22797;$L$&#27425;&#30340;&#36845;&#20195;&#36807;&#31243;&#30001;&#23376;&#29305;&#24449;&#32452;&#25104;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38656;&#35201;&#28145;&#24230;CNN&#23398;&#20064;&#36825;&#20010;&#20219;&#21153;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;$P^*$&#65288;i&#65289;&#38543;&#30528;$n_c m^L$&#30340;&#22686;&#38271;&#32780;&#28176;&#36827;&#22320;&#22686;&#38271;&#65292;&#36825;&#21482;&#26377;...
&lt;/p&gt;
&lt;p&gt;
Learning generic high-dimensional tasks is notably hard, as it requires a number of training data exponential in the dimension. Yet, deep convolutional neural networks (CNNs) have shown remarkable success in overcoming this challenge. A popular hypothesis is that learnable tasks are highly structured and that CNNs leverage this structure to build a low-dimensional representation of the data. However, little is known about how much training data they require, and how this number depends on the data structure. This paper answers this question for a simple classification task that seeks to capture relevant aspects of real data: the Random Hierarchy Model. In this model, each of the $n_c$ classes corresponds to $m$ synonymic compositions of high-level features, which are in turn composed of sub-features through an iterative process repeated $L$ times. We find that the number of training data $P^*$ required by deep CNNs to learn this task (i) grows asymptotically as $n_c m^L$, which is only
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BuildingsBench&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;900K&#24231;&#24314;&#31569;&#29289;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#20013;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35813;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#32463;&#36807;&#21512;&#25104;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.00142</link><description>&lt;p&gt;
BuildingsBench&#65306;&#19968;&#20010;&#21253;&#21547;900K&#24231;&#24314;&#31569;&#29289;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BuildingsBench: A Large-Scale Dataset of 900K Buildings and Benchmark for Short-Term Load Forecasting. (arXiv:2307.00142v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BuildingsBench&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;900K&#24231;&#24314;&#31569;&#29289;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#20013;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35813;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#32463;&#36807;&#21512;&#25104;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;(STLF)&#20013;&#32570;&#20047;&#24320;&#25918;&#12289;&#22823;&#35268;&#27169;&#12289;&#39640;&#24314;&#31569;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;BuildingsBench&#65292;&#21253;&#25324;1)&#21253;&#21547;900K&#20010;&#27169;&#25311;&#24314;&#31569;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;Buildings-900K&#65292;&#20197;&#27169;&#25311;&#32654;&#22269;&#30340;&#24314;&#31569;&#24211;&#23384;&#65292;&#20197;&#21450;2)&#25317;&#26377;&#26469;&#33258;7&#20010;&#24320;&#25918;&#25968;&#25454;&#38598;&#30340;&#36229;&#36807;1900&#20010;&#30495;&#23454;&#20303;&#23429;&#21644;&#21830;&#19994;&#24314;&#31569;&#29289;&#30340;&#35780;&#20272;&#24179;&#21488;&#12290;BuildingsBench&#20026;&#20004;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#20219;&#21153;&#25552;&#20379;&#20102;&#22522;&#20934;&#65306;&#38646;-shot STLF&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#24314;&#31569;&#19978;&#36827;&#34892;&#35780;&#20272;&#32780;&#26080;&#38656;&#24494;&#35843;&#65307;&#20197;&#21450;&#36801;&#31227;&#23398;&#20064;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#30446;&#26631;&#24314;&#31569;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#26412;&#27425;&#22522;&#20934;&#20998;&#26512;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#32463;&#36807;&#21512;&#25104;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#24847;&#22806;&#22320;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Short-term forecasting of residential and commercial building energy consumption is widely used in power systems and continues to grow in importance. Data-driven short-term load forecasting (STLF), although promising, has suffered from a lack of open, large-scale datasets with high building diversity. This has hindered exploring the pretrain-then-finetune paradigm for STLF. To help address this, we present BuildingsBench, which consists of 1) Buildings-900K, a large-scale dataset of 900K simulated buildings representing the U.S. building stock, and 2) an evaluation platform with over 1,900 real residential and commercial buildings from 7 open datasets. BuildingsBench benchmarks two under-explored tasks: zero-shot STLF, where a pretrained model is evaluated on unseen buildings without fine-tuning, and transfer learning, where a pretrained model is fine-tuned on a target building. The main finding of our benchmark analysis is that synthetically pretrained models generalize surprisingly w
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#32467;&#21512;&#23454;&#39564;&#25968;&#25454;&#21644;&#33258;&#20030;&#26679;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#35780;&#20272;&#21333;&#20809;&#23376;&#28304;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#22810;&#20809;&#23376;&#21457;&#23556;&#20107;&#20214;&#27010;&#29575;&#30340;&#19981;&#30830;&#23450;&#24615;&#23545;&#36136;&#37327;&#35780;&#20272;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.15683</link><description>&lt;p&gt;
&#24555;&#36895;&#30830;&#23450;&#21333;&#20809;&#23376;&#28304;&#36136;&#37327;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The Challenge of Quickly Determining the Quality of a Single-Photon Source. (arXiv:2306.15683v1 [physics.optics])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15683
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#32467;&#21512;&#23454;&#39564;&#25968;&#25454;&#21644;&#33258;&#20030;&#26679;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#35780;&#20272;&#21333;&#20809;&#23376;&#28304;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#22810;&#20809;&#23376;&#21457;&#23556;&#20107;&#20214;&#27010;&#29575;&#30340;&#19981;&#30830;&#23450;&#24615;&#23545;&#36136;&#37327;&#35780;&#20272;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#25991;&#29486;&#25552;&#20986;&#20102;&#26032;&#39062;&#26041;&#27861;&#26469;&#24555;&#36895;&#35780;&#20272;&#21333;&#20809;&#23376;&#28304;&#65288;SPS&#65289;&#30340;&#36136;&#37327;&#65292;&#20363;&#22914;&#37327;&#23376;&#28857;&#65292;&#20197;&#35299;&#20915;&#23454;&#39564;&#39564;&#35777;&#36890;&#36807;&#20809;&#24378;&#24178;&#28041;&#20202;&#39564;&#35777;&#25928;&#29575;&#20302;&#21644;&#32791;&#26102;&#38271;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#19981;&#30830;&#23450;&#24615;&#35752;&#35770;&#21644;&#21487;&#37325;&#22797;&#24615;&#32454;&#33410;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#24341;&#21457;&#20102;&#20851;&#20999;&#12290;&#26412;&#30740;&#31350;&#23545;&#20174;&#21457;&#23556;&#27874;&#38271;&#20026;1.3&#956;m&#65292;&#30001;80MHz&#28608;&#20809;&#22120;&#28608;&#21457;&#30340;InGaAs/GaAs&#22806;&#24310;&#37327;&#23376;&#28857;&#33719;&#24471;&#30340;&#20843;&#32452;&#25968;&#25454;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#23558;&#23454;&#39564;&#25968;&#25454;&#19982;&#33258;&#20030;&#26679;&#26412;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36129;&#29486;&#12290;&#23545;&#21512;&#25104;&#26679;&#26412;&#30340;&#39640;&#25928;&#30452;&#26041;&#22270;&#25311;&#21512;&#23548;&#20986;&#30340;SPS&#36136;&#37327;&#25351;&#26631;&#65292;&#21363;&#22810;&#20809;&#23376;&#21457;&#23556;&#20107;&#20214;&#30340;&#27010;&#29575;&#36827;&#34892;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#30001;&#25551;&#36848;&#25506;&#27979;&#29575;&#30340;&#27850;&#26494;&#36807;&#31243;&#20013;&#38543;&#26426;&#21464;&#24322;&#24615;&#36129;&#29486;&#30340;&#26174;&#33879;&#19981;&#30830;&#23450;&#24615;&#12290;&#24573;&#35270;&#36825;&#20010;&#22240;&#32032;&#21487;&#33021;&#23548;&#33268;&#23545;SPS&#36136;&#37327;&#30340;&#38169;&#35823;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Novel methods for rapidly estimating single-photon source (SPS) quality, e.g. of quantum dots, have been promoted in recent literature to address the expensive and time-consuming nature of experimental validation via intensity interferometry. However, the frequent lack of uncertainty discussions and reproducible details raises concerns about their reliability. This study investigates one such proposal on eight datasets obtained from an InGaAs/GaAs epitaxial quantum dot that emits at 1.3 {\mu}m and is excited by an 80 MHz laser. The study introduces a novel contribution by employing data augmentation, a machine learning technique, to supplement experimental data with bootstrapped samples. Analysis of the SPS quality metric, i.e. the probability of multi-photon emission events, as derived from efficient histogram fitting of the synthetic samples, reveals significant uncertainty contributed by stochastic variability in the Poisson processes that describe detection rates. Ignoring this sou
&lt;/p&gt;</description></item><item><title>TrustGuard&#26159;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#65292;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#65292;&#25239;&#20987;&#40065;&#26834;&#24182;&#25552;&#20379;&#35299;&#37322;&#33021;&#21147;&#65292;&#23427;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13339</link><description>&lt;p&gt;
TrustGuard: &#22522;&#20110;GNN&#30340;&#21160;&#24577;&#25903;&#25345;&#40065;&#26834;&#19988;&#21487;&#35299;&#37322;&#30340;&#20449;&#20219;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support. (arXiv:2306.13339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13339
&lt;/p&gt;
&lt;p&gt;
TrustGuard&#26159;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#65292;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#65292;&#25239;&#20987;&#40065;&#26834;&#24182;&#25552;&#20379;&#35299;&#37322;&#33021;&#21147;&#65292;&#23427;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#37117;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#20219;&#35780;&#20272;&#35780;&#20272;&#23454;&#20307;&#20043;&#38388;&#30340;&#20449;&#20219;&#20851;&#31995;&#24182;&#20419;&#36827;&#20915;&#31574;&#12290;&#26426;&#22120;&#23398;&#20064;&#30001;&#20110;&#20854;&#23398;&#20064;&#33021;&#21147;&#32780;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22240;&#27492;&#23545;&#20449;&#20219;&#35780;&#20272;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36817;&#24180;&#26469;&#65292;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22788;&#29702;&#22270;&#24418;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;&#36825;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#23558;&#20854;&#29992;&#20110;&#20449;&#20219;&#35780;&#20272;&#65292;&#22240;&#20026;&#23454;&#20307;&#20043;&#38388;&#30340;&#20449;&#20219;&#20851;&#31995;&#21487;&#20197;&#24314;&#27169;&#20026;&#22270;&#24418;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;GNN&#30340;&#24403;&#21069;&#20449;&#20219;&#35780;&#20272;&#26041;&#27861;&#26410;&#33021;&#23436;&#20840;&#28385;&#36275;&#20449;&#20219;&#30340;&#21160;&#24577;&#24615;&#65292;&#24573;&#30053;&#20102;&#25915;&#20987;&#23545;&#20449;&#20219;&#35780;&#20272;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#19988;&#26080;&#27861;&#25552;&#20379;&#20196;&#20154;&#20449;&#26381;&#30340;&#35780;&#20272;&#32467;&#26524;&#35299;&#37322;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrustGuard &#65306;&#19968;&#31181;&#25903;&#25345;&#20449;&#20219;&#21160;&#24577;&#24615;&#12289;&#25239;&#20987;&#40065;&#26834;&#19988;&#36890;&#36807;&#21487;&#35270;&#21270;&#25552;&#20379;&#35299;&#37322;&#30340;&#31934;&#30830;&#20449;&#20219;&#35780;&#20272;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TrustGuard &#35774;&#35745;&#20102;&#19968;&#20010;&#30001;&#21160;&#24577;&#24863;&#30693;&#33410;&#28857;&#23884;&#20837;&#23618;&#12289;&#22270;&#21367;&#31215;&#23618;&#12289;&#27880;&#24847;&#26426;&#21046;&#23618;&#21644;&#20449;&#20219;&#39044;&#27979;&#23618;&#32452;&#25104;&#30340;&#20998;&#23618;&#26550;&#26500;&#12290;&#20026;&#20102;&#35780;&#20272;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23558;TrustGuard&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TrustGuard &#22312;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trust evaluation assesses trust relationships between entities and facilitates decision-making. Machine Learning (ML) shows great potential for trust evaluation owing to its learning capabilities. In recent years, Graph Neural Networks (GNNs), as a new ML paradigm, have demonstrated superiority in dealing with graph data. This has motivated researchers to explore their use in trust evaluation, as trust relationships among entities can be modeled as a graph. However, current trust evaluation methods that employ GNNs fail to fully satisfy the dynamicity nature of trust, overlook the adverse effects of attacks on trust evaluation, and cannot provide convincing explanations on evaluation results. To address these problems, in this paper, we propose TrustGuard, a GNN-based accurate trust evaluation model that supports trust dynamicity, is robust against typical attacks, and provides explanations through visualization. Specifically, TrustGuard is designed with a layered architecture that con
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#32447;&#21160;&#24577;&#23376;&#27169;&#35268;&#21010;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#32447;&#23376;&#27169;&#36138;&#23146;&#31639;&#27861;&#65288;OSGA&#65289;&#21644;&#22312;&#32447;&#23376;&#27169;&#26144;&#23556;&#26799;&#24230;&#19979;&#38477;&#65288;OSPGD&#65289;&#31639;&#27861;&#20197;&#35299;&#20915;&#27492;&#31867;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#19981;&#21516;&#30340;&#30005;&#21147;&#31995;&#32479;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.10835</link><description>&lt;p&gt;
&#22312;&#32447;&#21160;&#24577;&#23376;&#27169;&#35268;&#21010;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Online Dynamic Submodular Optimization. (arXiv:2306.10835v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10835
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#32447;&#21160;&#24577;&#23376;&#27169;&#35268;&#21010;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#32447;&#23376;&#27169;&#36138;&#23146;&#31639;&#27861;&#65288;OSGA&#65289;&#21644;&#22312;&#32447;&#23376;&#27169;&#26144;&#23556;&#26799;&#24230;&#19979;&#38477;&#65288;OSPGD&#65289;&#31639;&#27861;&#20197;&#35299;&#20915;&#27492;&#31867;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#19981;&#21516;&#30340;&#30005;&#21147;&#31995;&#32479;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#21487;&#35777;&#26126;&#24615;&#33021;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#28385;&#36275;&#19968;&#33324;&#32422;&#26463;&#26465;&#20214;&#21644;&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;&#22312;&#32447;&#20108;&#20803;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#30446;&#26631;&#20989;&#25968;&#20026;&#23376;&#27169;&#35268;&#21010;&#30340;&#38382;&#39064;&#23376;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#32447;&#23376;&#27169;&#36138;&#23146;&#31639;&#27861;&#65288;OSGA&#65289;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#27714;&#35299;&#20808;&#21069;&#36718;&#25439;&#22833;&#20989;&#25968;&#30340;&#36817;&#20284;&#20540;&#26469;&#36991;&#20813;&#21407;&#38382;&#39064;&#30340;NP-&#22256;&#38590;&#24615;&#12290;&#25105;&#20204;&#23558;OSGA&#25193;&#23637;&#20026;&#36890;&#29992;&#30340;&#36817;&#20284;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;OSGA&#22312;&#26102;&#38388;&#38271;&#24230;&#21644;&#32047;&#31215;&#36718;&#27425;&#26368;&#20248;&#21464;&#21270;&#26041;&#38754;&#20855;&#26377;&#19982;&#22312;&#32447;&#20984;&#20248;&#21270;&#20013;&#26368;&#20005;&#26684;&#36793;&#30028;&#30456;&#20284;&#30340;&#21160;&#24577;&#36951;&#25022;&#30028;&#12290;&#23545;&#20110;&#27809;&#26377;&#36817;&#20284;&#35299;&#25110;&#38656;&#35201;&#26356;&#31616;&#21333;&#30340;&#23454;&#29616;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22312;&#32447;&#23376;&#27169;&#26144;&#23556;&#26799;&#24230;&#19979;&#38477;&#65288;OSPGD&#65289;&#31639;&#27861;&#65292;&#21033;&#29992;Lova\'sz&#25193;&#23637;&#12290;&#25105;&#20204;&#24471;&#21040;&#20102;&#31867;&#20284;&#20110;&#20256;&#32479;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#65288;OGD&#65289;&#30340;&#36951;&#25022;&#30028;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#30005;&#21147;&#31995;&#32479;&#20013;&#23545;&#31639;&#27861;&#36827;&#34892;&#20102;&#25968;&#20540;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose new algorithms with provable performance for online binary optimization subject to general constraints and in dynamic settings. We consider the subset of problems in which the objective function is submodular. We propose the online submodular greedy algorithm (OSGA) which solves to optimality an approximation of the previous round loss function to avoid the NP-hardness of the original problem. We extend OSGA to a generic approximation function. We show that OSGA has a dynamic regret bound similar to the tightest bounds in online convex optimization with respect to the time horizon and the cumulative round optimum variation. For instances where no approximation exists or a computationally simpler implementation is desired, we design the online submodular projected gradient descent (OSPGD) by leveraging the Lova\'sz extension. We obtain a regret bound that is akin to the conventional online gradient descent (OGD). Finally, we numerically test our algorithms in two power system
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;$K$&#26368;&#36817;&#37051;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#31639;&#21382;&#21490;&#25968;&#25454;&#20013;&#30001;&#19981;&#21516;&#31574;&#30053;&#29983;&#25104;&#30340;&#20915;&#31574;&#36807;&#31243;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#20013;&#30340;&#21453;&#20107;&#23454;&#20272;&#35745;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.04836</link><description>&lt;p&gt;
$K$&#26368;&#36817;&#37051;&#37325;&#37319;&#26679;&#29992;&#20110;&#38543;&#26426;&#25511;&#21046;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
$K$-Nearest-Neighbor Resampling for Off-Policy Evaluation in Stochastic Control. (arXiv:2306.04836v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04836
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;$K$&#26368;&#36817;&#37051;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#31639;&#21382;&#21490;&#25968;&#25454;&#20013;&#30001;&#19981;&#21516;&#31574;&#30053;&#29983;&#25104;&#30340;&#20915;&#31574;&#36807;&#31243;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#20013;&#30340;&#21453;&#20107;&#23454;&#20272;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;$K$&#26368;&#36817;&#37051;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#31639;&#21382;&#21490;&#25968;&#25454;&#20013;&#30001;&#19981;&#21516;&#31574;&#30053;&#29983;&#25104;&#30340;&#20915;&#31574;&#36807;&#31243;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20381;&#36182;&#20110;&#24403;&#21069;&#29366;&#24577;&#30340;&#21453;&#39304;&#31574;&#30053;&#65292;&#36825;&#31181;&#31574;&#30053;&#36866;&#29992;&#20110;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#21644;&#25152;&#36873;&#21160;&#20316;&#24433;&#21709;&#19979;&#30340;&#31995;&#32479;&#22266;&#26377;&#38543;&#26426;&#24615;&#30340;&#29615;&#22659;&#12290;&#36825;&#20123;&#35774;&#32622;&#22312;&#35768;&#22810;&#39640;&#39118;&#38505;&#24212;&#29992;&#31243;&#24207;&#20013;&#24456;&#24120;&#35265;&#65292;&#24182;&#22312;&#38543;&#26426;&#25511;&#21046;&#30340;&#19978;&#19979;&#25991;&#20013;&#31215;&#26497;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#36807;&#31243;&#21033;&#29992;&#20102;&#31867;&#20284;&#30340;&#29366;&#24577;/&#21160;&#20316;&#23545;&#65288;&#22312;&#24230;&#37327;&#24847;&#20041;&#19979;&#65289;&#19982;&#31867;&#20284;&#30340;&#22870;&#21169;&#21644;&#29366;&#24577;&#36716;&#25442;&#30456;&#20851;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#37325;&#37319;&#26679;&#36807;&#31243;&#36890;&#36807;&#31867;&#20284;&#20110;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#30340;&#36712;&#36857;&#27169;&#25311;&#26469;&#35299;&#20915;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#20013;&#30340;&#21453;&#20107;&#23454;&#20272;&#35745;&#38382;&#39064;&#12290;&#19982;&#20854;&#20182;OPE&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#38656;&#35201;&#20248;&#21270;&#65292;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#26641;&#30340;&#26368;&#36817;&#37051;&#25628;&#32034;&#39640;&#25928;&#23454;&#29616;&#65292;&#24182;&#19988;&#26412;&#36136;&#19978;&#26159;&#21487;&#24182;&#34892;&#21270;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#22312;&#22522;&#20934;&#29615;&#22659;&#19979;&#23637;&#31034;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#20248;&#36234;&#23454;&#39564;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel $K$-nearest neighbor resampling procedure for estimating the performance of a policy from historical data containing realized episodes of a decision process generated under a different policy. We focus on feedback policies that depend deterministically on the current state in environments with continuous state-action spaces and system-inherent stochasticity effected by chosen actions. Such settings are common in a wide range of high-stake applications and are actively investigated in the context of stochastic control. Our procedure exploits that similar state/action pairs (in a metric sense) are associated with similar rewards and state transitions. This enables our resampling procedure to tackle the counterfactual estimation problem underlying off-policy evaluation (OPE) by simulating trajectories similarly to Monte Carlo methods. Compared to other OPE methods, our algorithm does not require optimization, can be efficiently implemented via tree-based nearest neighbo
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Pointnet++&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#31232;&#30095;&#19981;&#35268;&#21017;&#28857;&#20113;&#20013;&#21494;&#29255;&#21644;&#26408;&#26448;&#30340;&#21306;&#20998;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.16963</link><description>&lt;p&gt;
&#31232;&#30095;&#19981;&#35268;&#21017;&#28857;&#20113;&#30340;&#21494;&#29255;/&#26408;&#26448;&#21306;&#20998;&#30340;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation of sparse irregular point clouds for leaf/wood discrimination. (arXiv:2305.16963v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16963
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Pointnet++&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#31232;&#30095;&#19981;&#35268;&#21017;&#28857;&#20113;&#20013;&#21494;&#29255;&#21644;&#26408;&#26448;&#30340;&#21306;&#20998;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LiDAR&#65288;&#20809;&#25506;&#27979;&#19982;&#27979;&#36317;&#65289;&#24050;&#25104;&#20026;&#36965;&#24863;&#24037;&#20855;&#31665;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#65292;&#29992;&#20110;&#29983;&#29289;&#22280;&#30417;&#27979;&#12290;&#29305;&#21035;&#26159;&#65292;LiDAR&#25552;&#20379;&#20102;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#20934;&#30830;&#24230;&#26469;&#32472;&#21046;&#26862;&#26519;&#21494;&#38754;&#31215;&#65292;&#32780;&#21494;&#38754;&#31215;&#19968;&#30452;&#26159;&#24433;&#21709;&#26893;&#34987;&#19982;&#22823;&#27668;&#38388;&#27668;&#20307;&#20132;&#25442;&#27169;&#22411;&#30340;&#37325;&#35201;&#19981;&#30830;&#23450;&#24615;&#28304;&#12290;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#26131;&#20110;&#37096;&#32626;&#65292;&#22240;&#27492;&#21487;&#20197;&#39057;&#32321;&#37325;&#35775;&#20197;&#36319;&#36394;&#26893;&#34987;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#35013;&#36733;&#22312;UAV&#19978;&#30340;&#24494;&#22411;&#20256;&#24863;&#22120;&#36890;&#24120;&#21482;&#25552;&#20379;&#26377;&#38480;&#23494;&#24230;&#30340;&#28857;&#20113;&#65292;&#32780;&#19988;&#30001;&#20110;&#36880;&#28176;&#22686;&#24378;&#30340;&#36974;&#25377;&#65292;&#20174;&#39030;&#37096;&#21040;&#24213;&#37096;&#30340;&#23494;&#24230;&#21576;&#29616;&#26126;&#26174;&#19979;&#38477;&#30340;&#36235;&#21183;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#24378;&#28872;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#31354;&#38388;&#19978;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#23494;&#24230;&#65292;&#23558;&#21494;&#29255;&#28857;&#21306;&#20998;&#20986;&#26469;&#26408;&#26448;&#28857;&#38754;&#20020;&#30528;&#24040;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Pointnet ++&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LiDAR (Light Detection and Ranging) has become an essential part of the remote sensing toolbox used for biosphere monitoring. In particular, LiDAR provides the opportunity to map forest leaf area with unprecedented accuracy, while leaf area has remained an important source of uncertainty affecting models of gas exchanges between the vegetation and the atmosphere. Unmanned Aerial Vehicles (UAV) are easy to mobilize and therefore allow frequent revisits to track the response of vegetation to climate change. However, miniature sensors embarked on UAVs usually provide point clouds of limited density, which are further affected by a strong decrease in density from top to bottom of the canopy due to progressively stronger occlusion. In such a context, discriminating leaf points from wood points presents a significant challenge due in particular to strong class imbalance and spatially irregular sampling intensity. Here we introduce a neural network model based on the Pointnet ++ architecture 
&lt;/p&gt;</description></item><item><title>FedZero&#26159;&#19968;&#20010;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#21487;&#20877;&#29983;&#22810;&#20313;&#33021;&#28304;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#33021;&#28304;&#21644;&#36127;&#36733;&#39044;&#27979;&#26469;&#35843;&#24230;&#35757;&#32451;&#20219;&#21153;&#65292;&#23558;&#30899;&#25490;&#25918;&#38477;&#20302;&#21040;&#38646;&#12290;</title><link>http://arxiv.org/abs/2305.15092</link><description>&lt;p&gt;
FedZero&#65306;&#21033;&#29992;&#21487;&#20877;&#29983;&#22810;&#20313;&#33021;&#28304;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;
&lt;/p&gt;
&lt;p&gt;
FedZero: Leveraging Renewable Excess Energy in Federated Learning. (arXiv:2305.15092v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15092
&lt;/p&gt;
&lt;p&gt;
FedZero&#26159;&#19968;&#20010;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#21487;&#20877;&#29983;&#22810;&#20313;&#33021;&#28304;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#33021;&#28304;&#21644;&#36127;&#36733;&#39044;&#27979;&#26469;&#35843;&#24230;&#35757;&#32451;&#20219;&#21153;&#65292;&#23558;&#30899;&#25490;&#25918;&#38477;&#20302;&#21040;&#38646;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#25968;&#25454;&#23396;&#23707;&#25110;&#36793;&#32536;&#35774;&#22791;&#20043;&#38388;&#36827;&#34892;&#20998;&#24067;&#24335;&#27169;&#22411;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#19982;&#38598;&#20013;&#24335;&#27169;&#22411;&#35757;&#32451;&#30456;&#27604;&#65292;FL&#19981;&#21487;&#36991;&#20813;&#22320;&#24341;&#20837;&#20102;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#36825;&#23558;&#36827;&#19968;&#27493;&#22686;&#21152;&#26410;&#26469;&#26426;&#22120;&#23398;&#20064;&#30340;&#39640;&#33021;&#32791;&#21644;&#30456;&#20851;&#30340;&#30899;&#25490;&#25918;&#12290;&#20943;&#23569;FL&#30340;&#30899;&#36275;&#36857;&#30340;&#19968;&#20010;&#24605;&#36335;&#26159;&#26681;&#25454;&#30005;&#32593;&#20013;&#26576;&#20123;&#26102;&#27573;&#21644;&#22320;&#28857;&#21487;&#33021;&#20986;&#29616;&#30340;&#21487;&#20877;&#29983;&#22810;&#20313;&#33021;&#28304;&#30340;&#21487;&#29992;&#24615;&#36827;&#34892;&#35843;&#24230;&#35757;&#32451;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#36825;&#31181;&#27874;&#21160;&#21644;&#19981;&#21487;&#38752;&#30340;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#30340;FL&#35843;&#24230;&#22120;&#19981;&#33021;&#22987;&#32456;&#20445;&#35777;&#24555;&#36895;&#12289;&#39640;&#25928;&#21644;&#20844;&#24179;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedZero&#30340;FL&#31995;&#32479;&#65292;&#23427;&#20165;&#22312;&#21487;&#20877;&#29983;&#22810;&#20313;&#33021;&#28304;&#21644;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#30340;&#38386;&#32622;&#23481;&#37327;&#19978;&#36816;&#34892;&#65292;&#23558;&#35757;&#32451;&#30340;&#25805;&#20316;&#30899;&#25490;&#25918;&#26377;&#25928;&#38477;&#20302;&#21040;&#38646;&#12290;&#36890;&#36807;&#33021;&#28304;&#21644;&#36127;&#36733;&#39044;&#27979;&#65292;FedZero&#21033;&#29992;&#22810;&#20313;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#26102;&#31354;&#21487;&#29992;&#24615;&#26469;&#36827;&#34892;&#35843;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is an emerging machine learning technique that enables distributed model training across data silos or edge devices without data sharing. Yet, FL inevitably introduces inefficiencies compared to centralized model training, which will further increase the already high energy usage and associated carbon emissions of machine learning in the future. One idea to reduce FL's carbon footprint is to schedule training jobs based on the availability of renewable excess energy that can occur at certain times and places in the grid. However, in the presence of such volatile and unreliable resources, existing FL schedulers cannot always ensure fast, efficient, and fair training.  We propose FedZero, an FL system that operates exclusively on renewable excess energy and spare capacity of compute infrastructure to effectively reduce a training's operational carbon emissions to zero. Using energy and load forecasts, FedZero leverages the spatio-temporal availability of excess re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35777;&#25454;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22788;&#29702;&#20284;&#28982;&#20989;&#25968;&#25110;&#20808;&#39564;&#20989;&#25968;&#19982;&#23884;&#22871;&#25277;&#26679;&#26080;&#27861;&#32988;&#20219;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36125;&#21494;&#26031;&#27169;&#22411;&#27604;&#36739;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#26356;&#24555;&#36895;&#22320;&#12289;&#26356;&#26377;&#25928;&#22320;&#20272;&#31639;&#36125;&#21494;&#26031;&#22240;&#23376;&#12290;</title><link>http://arxiv.org/abs/2305.11241</link><description>&lt;p&gt;
&#35777;&#25454;&#32593;&#32476;&#65306;&#29992;&#31616;&#21333;&#30340;&#25439;&#22833;&#20989;&#25968;&#24555;&#36895;&#12289;&#20998;&#25674;&#24335;&#22320;&#36827;&#34892;&#31070;&#32463;&#36125;&#21494;&#26031;&#27169;&#22411;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Evidence Networks: simple losses for fast, amortized, neural Bayesian model comparison. (arXiv:2305.11241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35777;&#25454;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22788;&#29702;&#20284;&#28982;&#20989;&#25968;&#25110;&#20808;&#39564;&#20989;&#25968;&#19982;&#23884;&#22871;&#25277;&#26679;&#26080;&#27861;&#32988;&#20219;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36125;&#21494;&#26031;&#27169;&#22411;&#27604;&#36739;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#26356;&#24555;&#36895;&#22320;&#12289;&#26356;&#26377;&#25928;&#22320;&#20272;&#31639;&#36125;&#21494;&#26031;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35777;&#25454;&#32593;&#32476;&#21487;&#22312;&#24403;&#29616;&#26377;&#30340;&#26041;&#27861;&#65288;&#22914;&#23884;&#22871;&#25277;&#26679;&#65289;&#22833;&#36133;&#12289;&#20284;&#28982;&#20989;&#25968;&#25110;&#20808;&#39564;&#20989;&#25968;&#38590;&#20197;&#22788;&#29702;&#25110;&#19981;&#30693;&#36947;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36125;&#21494;&#26031;&#27169;&#22411;&#27604;&#36739;&#12290;&#36125;&#21494;&#26031;&#27169;&#22411;&#27604;&#36739;&#21487;&#30475;&#20316;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#12290;&#34429;&#28982;&#29992;&#36125;&#21494;&#26031;&#27861;&#36827;&#34892;&#26368;&#20248;&#20998;&#31867;&#30340;&#35299;&#37322;&#24050;&#32463;&#20247;&#25152;&#21608;&#30693;&#65292;&#20294;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25913;&#21464;&#20102;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#20135;&#29983;&#24555;&#36895;&#12289;&#20998;&#25674;&#24335;&#30340;&#31070;&#32463;&#20272;&#35745;&#22120;&#65292;&#30452;&#25509;&#20272;&#31639;&#26041;&#20415;&#30340;&#36125;&#21494;&#26031;&#22240;&#23376;&#30340;&#20989;&#25968;&#12290;&#36825;&#20943;&#23569;&#20102;&#20272;&#31639;&#21333;&#20010;&#27169;&#22411;&#27010;&#29575;&#26102;&#30340;&#25968;&#23383;&#19981;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#28183;&#28431;&#22855; parity-odd power&#65288;l-POP&#65289;&#21464;&#25442;&#65292;&#24341;&#23548;&#20102;&#26032;&#30340;&#8220;l-Pop-Exponential&#8221;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#19981;&#21516;&#27169;&#22411;&#20013;&#23545;&#25968;&#25454;&#27010;&#29575;&#36827;&#34892;&#31070;&#32463;&#23494;&#24230;&#20272;&#35745;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#27604;&#35777;&#25454;&#32593;&#32476;&#30340;&#31934;&#24230;&#21644;&#21487;&#25193;&#23637;&#24615;&#37117;&#35201;&#20302;&#12290;&#22810;&#31181;&#23454;&#38469;&#21644;&#20154;&#36896;&#20363;&#23376;&#35777;&#26126;&#20102;&#35777;&#25454;&#32593;&#32476;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evidence Networks can enable Bayesian model comparison when state-of-the-art methods (e.g. nested sampling) fail and even when likelihoods or priors are intractable or unknown. Bayesian model comparison, i.e. the computation of Bayes factors or evidence ratios, can be cast as an optimization problem. Though the Bayesian interpretation of optimal classification is well-known, here we change perspective and present classes of loss functions that result in fast, amortized neural estimators that directly estimate convenient functions of the Bayes factor. This mitigates numerical inaccuracies associated with estimating individual model probabilities. We introduce the leaky parity-odd power (l-POP) transform, leading to the novel ``l-POP-Exponential'' loss function. We explore neural density estimation for data probability in different models, showing it to be less accurate and scalable than Evidence Networks. Multiple real-world and synthetic examples illustrate that Evidence Networks are e
&lt;/p&gt;</description></item><item><title>DualFL&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#20598;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20855;&#20307;&#23545;&#20598;&#24418;&#24335;&#35299;&#20915;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20445;&#35777;&#20102;&#21363;&#20351;&#20351;&#29992;&#19981;&#31934;&#30830;&#30340;&#26412;&#22320;&#35299;&#20915;&#26041;&#26696;&#20063;&#21487;&#20197;&#23454;&#29616;&#26368;&#20339;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.10294</link><description>&lt;p&gt;
DualFL&#65306;&#19968;&#31181;&#22522;&#20110;&#23545;&#20598;&#30340;Federated Learning&#31639;&#27861;&#21450;&#22312;&#19968;&#33324;&#20984;&#24773;&#24418;&#19979;&#21152;&#36895;&#36890;&#35759;
&lt;/p&gt;
&lt;p&gt;
DualFL: A Duality-based Federated Learning Algorithm with Communication Acceleration in the General Convex Regime. (arXiv:2305.10294v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10294
&lt;/p&gt;
&lt;p&gt;
DualFL&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#20598;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20855;&#20307;&#23545;&#20598;&#24418;&#24335;&#35299;&#20915;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20445;&#35777;&#20102;&#21363;&#20351;&#20351;&#29992;&#19981;&#31934;&#30830;&#30340;&#26412;&#22320;&#35299;&#20915;&#26041;&#26696;&#20063;&#21487;&#20197;&#23454;&#29616;&#26368;&#20339;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DualFL&#65288;Dualized Federated Learning&#65289;&#30340;&#26032;&#22411;&#35757;&#32451;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#30340;&#29305;&#23450;&#23545;&#20598;&#24418;&#24335;&#12290;DualFL&#22312;&#19981;&#21516;&#30340;&#20809;&#28369;&#24615;&#21644;&#24378;&#20984;&#24615;&#35774;&#32622;&#19979;&#23454;&#29616;&#36890;&#35759;&#21152;&#36895;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#29702;&#35770;&#19978;&#20445;&#35777;&#20351;&#29992;&#19981;&#31934;&#30830;&#30340;&#26412;&#22320;&#27714;&#35299;&#22120;&#65292;&#21363;&#20351;&#26159;&#20351;&#29992;&#19981;&#31934;&#30830;&#30340;&#26412;&#22320;&#35299;&#20915;&#26041;&#26696;&#65292;&#20063;&#21487;&#20197;&#20445;&#25345;&#20854;&#26368;&#20339;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;DualFL&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#36890;&#35759;&#21152;&#36895;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#21363;&#20351;&#25104;&#26412;&#20989;&#25968;&#26082;&#38750;&#20809;&#28369;&#20063;&#38750;&#24378;&#20984;&#65292;&#20063;&#21487;&#20197;&#20351;&#29992;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;DualFL&#30340;&#23454;&#38469;&#24615;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30456;&#24403;&#65292;&#24182;&#19988;&#23545;&#36229;&#21442;&#25968;&#35843;&#25972;&#26159;&#31283;&#20581;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel training algorithm called DualFL (Dualized Federated Learning), for solving a distributed optimization problem in federated learning. Our approach is based on a specific dual formulation of the federated learning problem. DualFL achieves communication acceleration under various settings on smoothness and strong convexity of the problem. Moreover, it theoretically guarantees the use of inexact local solvers, preserving its optimal communication complexity even with inexact local solutions. DualFL is the first federated learning algorithm that achieves communication acceleration, even when the cost function is either nonsmooth or non-strongly convex. Numerical results demonstrate that the practical performance of DualFL is comparable to those of state-of-the-art federated learning algorithms, and it is robust with respect to hyperparameter tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;&#25239;&#20307;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#65292;&#35299;&#20915;&#20960;&#20309;&#24314;&#27169;&#21644;&#20302;&#25928;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09480</link><description>&lt;p&gt;
&#20132;&#21449;&#38376;&#25511;&#22810;&#23618;&#24863;&#30693;&#26426;&#19979;&#30340;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#19981;&#21464;&#23884;&#20837;&#26159;&#19968;&#31181;&#19968;&#27425;&#24615;&#25239;&#20307;&#35774;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Protein Complex Invariant Embedding with Cross-Gate MLP is A One-Shot Antibody Designer. (arXiv:2305.09480v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;&#25239;&#20307;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#65292;&#35299;&#20915;&#20960;&#20309;&#24314;&#27169;&#21644;&#20302;&#25928;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#26159;&#30001;&#20813;&#30123;&#31995;&#32479;&#20135;&#29983;&#30340;&#38024;&#23545;&#22806;&#26469;&#29289;&#36136;&#25110;&#25239;&#21407;&#30340;&#37325;&#35201;&#34507;&#30333;&#36136;&#12290;&#25239;&#20307;&#30340;&#29305;&#24322;&#24615;&#30001;&#20854;&#20114;&#34917;&#20915;&#23450;&#21306;&#65288;CDR&#65289;&#20915;&#23450;&#65292;CDR&#20301;&#20110;&#25239;&#20307;&#38142;&#30340;&#21487;&#21464;&#21306;&#22495;&#20013;&#65292;&#24418;&#25104;&#19982;&#25239;&#21407;&#32467;&#21512;&#30340;&#20301;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#21033;&#29992;&#22797;&#26434;&#30340;&#25216;&#26415;&#29983;&#25104;CDR&#65292;&#20294;&#23427;&#20204;&#36973;&#21463;&#20102;&#20960;&#20309;&#24314;&#27169;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#24120;&#35265;&#30340;&#36845;&#20195;&#31934;&#21270;&#31574;&#30053;&#23548;&#33268;&#20102;&#20302;&#25928;&#30340;&#25512;&#26029;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#25239;&#20307;CDR&#35774;&#35745;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20960;&#20309;&#24314;&#27169;&#21644;&#65288;ii&#65289;&#24207;&#21015;&#32467;&#26500;&#20849;&#23398;&#20064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#19981;&#21464;&#23884;&#20837;&#65292;&#21487;&#25429;&#25417;&#34507;&#30333;&#36136;&#39592;&#26550;&#21407;&#23376;&#65288;&#21253;&#25324;C&#945;&#12289;N&#12289;C&#21644;O&#21407;&#23376;&#65289;&#20043;&#38388;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#32452;&#20998;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#23454;&#29616;&#20840;&#38754;&#30340;&#20960;&#20309;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antibodies are crucial proteins produced by the immune system in response to foreign substances or antigens. The specificity of an antibody is determined by its complementarity-determining regions (CDRs), which are located in the variable domains of the antibody chains and form the antigen-binding site. Previous studies have utilized complex techniques to generate CDRs, but they suffer from inadequate geometric modeling. Moreover, the common iterative refinement strategies lead to an inefficient inference. In this paper, we propose a deep generative model that can co-design 1D sequences and 3D structures of CDRs in a one-shot manner. To achieve this, we decouple the antibody CDR design into two stages: (i) geometric modeling of protein structures and (ii) sequence-structure co-learning. We develop a protein complex invariant embedding that captures both intra- and inter-component interactions among the backbone atoms including C$\alpha$, N, C, and O atoms to achieve comprehensive geome
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22359;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#22788;&#29702;&#21333;&#35843;&#32570;&#22833;&#25968;&#25454;&#30340;&#25554;&#20540;&#19982;&#38477;&#32500;&#26694;&#26550;&#65292;&#21487;&#20197;&#26174;&#33879;&#22320;&#20943;&#23569;&#25554;&#34917;&#26102;&#38388;&#65292;&#36866;&#29992;&#20110;&#22823;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.06042</link><description>&lt;p&gt;
&#22522;&#20110;&#22359;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#22788;&#29702;&#21333;&#35843;&#32570;&#22833;&#25968;&#25454;&#30340;&#25554;&#20540;&#19982;&#38477;&#32500;
&lt;/p&gt;
&lt;p&gt;
Blockwise Principal Component Analysis for monotone missing data imputation and dimensionality reduction. (arXiv:2305.06042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06042
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22359;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#22788;&#29702;&#21333;&#35843;&#32570;&#22833;&#25968;&#25454;&#30340;&#25554;&#20540;&#19982;&#38477;&#32500;&#26694;&#26550;&#65292;&#21487;&#20197;&#26174;&#33879;&#22320;&#20943;&#23569;&#25554;&#34917;&#26102;&#38388;&#65292;&#36866;&#29992;&#20110;&#22823;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#35843;&#32570;&#22833;&#25968;&#25454;&#26159;&#25968;&#25454;&#20998;&#26512;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#25554;&#20540;&#21644;&#38477;&#32500;&#30340;&#32452;&#21512;&#21487;&#33021;&#20250;&#38754;&#20020;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;BPI&#30340;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#20102;&#22359;&#20869;&#20027;&#25104;&#20998;&#20998;&#26512;&#30340;&#26041;&#27861;&#22788;&#29702;&#21333;&#35843;&#32570;&#22833;&#25968;&#25454;&#30340;&#25554;&#20540;&#20197;&#21450;&#38477;&#32500;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#19982;&#21508;&#31181;&#25554;&#34917;&#25216;&#26415;&#19968;&#36215;&#20351;&#29992;&#65292;&#24182;&#19988;&#19982;&#25554;&#34917;&#21518;&#30340;&#38477;&#32500;&#30456;&#27604;&#65292;&#20854;&#21487;&#20197;&#26174;&#33879;&#22320;&#20943;&#23569;&#25554;&#34917;&#26102;&#38388;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26694;&#26550;&#20855;&#26377;&#36739;&#39640;&#25928;&#29575;&#24182;&#19988;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#23558;MICE&#30452;&#25509;&#24212;&#29992;&#20110;&#32570;&#22833;&#25968;&#25454;&#21487;&#33021;&#19981;&#26159;&#26368;&#20339;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monotone missing data is a common problem in data analysis. However, imputation combined with dimensionality reduction can be computationally expensive, especially with the increasing size of datasets. To address this issue, we propose a Blockwise principal component analysis Imputation (BPI) framework for dimensionality reduction and imputation of monotone missing data. The framework conducts Principal Component Analysis (PCA) on the observed part of each monotone block of the data and then imputes on merging the obtained principal components using a chosen imputation technique. BPI can work with various imputation techniques and can significantly reduce imputation time compared to conducting dimensionality reduction after imputation. This makes it a practical and efficient approach for large datasets with monotone missing data. Our experiments validate the improvement in speed. In addition, our experiments also show that while applying MICE imputation directly on missing data may not
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#38543;&#26426;Lp&#33539;&#25968;&#22833;&#30495;&#23545;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#22686;&#24378;&#65292;&#24182;&#35780;&#20272;&#27169;&#22411;&#23545;&#19981;&#21487;&#24863;&#30693;&#38543;&#26426;&#22833;&#30495;&#30340;&#31283;&#20581;&#24615;&#65292;&#21457;&#29616;&#31283;&#20581;&#24615;&#21487;&#33021;&#20250;&#25552;&#39640;&#27169;&#22411;&#22312;&#38543;&#26426;&#22833;&#30495;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#20294;&#20063;&#21487;&#33021;&#20250;&#25439;&#23475;L&#8734;&#33539;&#25968;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05400</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;Lp&#33539;&#25968;&#22833;&#30495;&#25506;&#31350;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#33104;&#36133;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigating the Corruption Robustness of Image Classifiers with Random Lp-norm Corruptions. (arXiv:2305.05400v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#38543;&#26426;Lp&#33539;&#25968;&#22833;&#30495;&#23545;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#22686;&#24378;&#65292;&#24182;&#35780;&#20272;&#27169;&#22411;&#23545;&#19981;&#21487;&#24863;&#30693;&#38543;&#26426;&#22833;&#30495;&#30340;&#31283;&#20581;&#24615;&#65292;&#21457;&#29616;&#31283;&#20581;&#24615;&#21487;&#33021;&#20250;&#25552;&#39640;&#27169;&#22411;&#22312;&#38543;&#26426;&#22833;&#30495;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#20294;&#20063;&#21487;&#33021;&#20250;&#25439;&#23475;L&#8734;&#33539;&#25968;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#20581;&#24615;&#26159;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#23454;&#29616;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#22522;&#26412;&#23646;&#24615;&#12290;&#22312;&#23545;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#23545;&#25239;&#31283;&#20581;&#24615;&#21644;&#24418;&#24335;&#31283;&#20581;&#24615;&#39564;&#35777;&#39046;&#22495;&#20013;&#65292;&#31283;&#20581;&#24615;&#36890;&#24120;&#34987;&#23450;&#20041;&#20026;&#22312;Lp&#33539;&#25968;&#36317;&#31163;&#20869;&#23545;&#25152;&#26377;&#36755;&#20837;&#21464;&#21270;&#30340;&#31283;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#38543;&#26426;&#22833;&#30495;&#30340;&#31283;&#20581;&#24615;&#36890;&#24120;&#36890;&#36807;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#35266;&#23519;&#21040;&#30340;&#21464;&#21270;&#26469;&#25913;&#36827;&#21644;&#35780;&#20272;&#65292;&#32780;&#24456;&#23569;&#32771;&#34385;&#25968;&#23398;&#23450;&#20041;&#30340;Lp&#33539;&#25968;&#22833;&#30495;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#38543;&#26426;Lp&#33539;&#25968;&#22833;&#30495;&#26469;&#22686;&#24378;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#20511;&#37492;&#20102;&#23545;&#25239;&#31283;&#20581;&#24615;&#39046;&#22495;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#23545;&#19981;&#21487;&#24863;&#30693;&#38543;&#26426;&#22833;&#30495;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#23454;&#35777;&#21644;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;Lp&#33539;&#25968;&#20043;&#38388;&#31283;&#20581;&#24615;&#26159;&#21542;&#21487;&#36716;&#31227;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#21738;&#20123;Lp&#33539;&#25968;&#30340;&#22833;&#30495;&#24212;&#35813;&#29992;&#26469;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#20250;&#25552;&#39640;&#27169;&#22411;&#22312;&#38543;&#26426;&#22833;&#30495;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#20294;&#20063;&#21487;&#33021;&#20250;&#25439;&#23475;L&#8734;&#33539;&#25968;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustness is a fundamental property of machine learning classifiers to achieve safety and reliability. In the fields of adversarial robustness and formal robustness verification of image classification models, robustness is commonly defined as the stability to all input variations within an Lp-norm distance. However, robustness to random corruptions is usually improved and evaluated using variations observed in the real-world, while mathematically defined Lp-norm corruptions are rarely considered. This study investigates the use of random Lp-norm corruptions to augment the training and test data of image classifiers. We adapt an approach from the field of adversarial robustness to assess the model robustness to imperceptible random corruptions. We empirically and theoretically investigate whether robustness is transferable across different Lp-norms and derive conclusions on which Lp-norm corruptions a model should be trained and evaluated on. We find that training data augmentation wi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;LyaDEQ&#30340;&#40065;&#26834;DEQ&#27169;&#22411;&#65292;&#36890;&#36807;Lyapunov&#29702;&#35770;&#25552;&#20379;&#20102;&#20445;&#35777;&#30340;&#31283;&#23450;&#24615;&#20197;&#25269;&#25239;&#24494;&#23567;&#30340;&#21021;&#22987;&#25200;&#21160;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22266;&#23450;&#28857;&#20043;&#38388;&#21152;&#20837;&#20840;&#36830;&#25509;&#23618;&#20197;&#36991;&#20813;&#19981;&#33391;&#23545;&#25239;&#24615;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2304.12707</link><description>&lt;p&gt;
&#23398;&#20064;&#40065;&#26834;&#30340;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Robust Deep Equilibrium Models. (arXiv:2304.12707v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;LyaDEQ&#30340;&#40065;&#26834;DEQ&#27169;&#22411;&#65292;&#36890;&#36807;Lyapunov&#29702;&#35770;&#25552;&#20379;&#20102;&#20445;&#35777;&#30340;&#31283;&#23450;&#24615;&#20197;&#25269;&#25239;&#24494;&#23567;&#30340;&#21021;&#22987;&#25200;&#21160;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22266;&#23450;&#28857;&#20043;&#38388;&#21152;&#20837;&#20840;&#36830;&#25509;&#23618;&#20197;&#36991;&#20813;&#19981;&#33391;&#23545;&#25239;&#24615;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24179;&#34913;(DEQ)&#27169;&#22411;&#24050;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#20013;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#38544;&#24335;&#23618;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#35299;&#20915;&#21333;&#20010;&#38750;&#32447;&#24615;&#23618;&#30340;&#22266;&#23450;&#28857;&#26469;&#25918;&#24323;&#20102;&#20256;&#32479;&#28145;&#24230;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#24456;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#22266;&#23450;&#28857;&#30340;&#31283;&#23450;&#24615;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#26368;&#36817;&#65292;&#23558;Lyapunov&#29702;&#35770;&#24212;&#29992;&#20110;&#21478;&#19968;&#31181;&#31867;&#22411;&#30340;&#38544;&#24335;&#23618;&#27169;&#22411;&#8212;&#8212;&#31070;&#32463;ODE&#65292;&#21487;&#20197;&#36171;&#20104;&#20854;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23558;DEQ&#27169;&#22411;&#35270;&#20026;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LyaDEQ&#30340;&#40065;&#26834;DEQ&#27169;&#22411;&#65292;&#36890;&#36807;Lyapunov&#29702;&#35770;&#25552;&#20379;&#20102;&#20445;&#35777;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#30830;&#20445;DEQ&#27169;&#22411;&#30340;&#22266;&#23450;&#28857;&#26159;Lyapunov&#31283;&#23450;&#30340;&#65292;&#36825;&#20351;&#24471;LyaDEQ&#27169;&#22411;&#33021;&#22815;&#25269;&#25239;&#24494;&#23567;&#30340;&#21021;&#22987;&#25200;&#21160;&#12290;&#20026;&#20102;&#36991;&#20813;&#30001;&#20110;Lyapunov&#31283;&#23450;&#30340;&#22266;&#23450;&#28857;&#24444;&#27492;&#38752;&#36817;&#32780;&#23548;&#33268;&#30340;&#19981;&#33391;&#23545;&#25239;&#24615;&#38450;&#24481;&#65292;&#25105;&#20204;&#22312;Lyapunov&#31283;&#23450;&#24615;&#27169;&#22359;&#20043;&#21518;&#21152;&#20837;&#20102;&#19968;&#20010;&#27491;&#20132;&#30340;&#20840;&#36830;&#25509;&#23618;&#65292;&#20197;&#20998;&#31163;&#19981;&#21516;&#30340;&#22266;&#23450;&#28857;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;LyaDEQ&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep equilibrium (DEQ) models have emerged as a promising class of implicit layer models in deep learning, which abandon traditional depth by solving for the fixed points of a single nonlinear layer. Despite their success, the stability of the fixed points for these models remains poorly understood. Recently, Lyapunov theory has been applied to Neural ODEs, another type of implicit layer model, to confer adversarial robustness. By considering DEQ models as nonlinear dynamic systems, we propose a robust DEQ model named LyaDEQ with guaranteed provable stability via Lyapunov theory. The crux of our method is ensuring the fixed points of the DEQ models are Lyapunov stable, which enables the LyaDEQ models to resist the minor initial perturbations. To avoid poor adversarial defense due to Lyapunov-stable fixed points being located near each other, we add an orthogonal fully connected layer after the Lyapunov stability module to separate different fixed points. We evaluate LyaDEQ models on se
&lt;/p&gt;</description></item><item><title>MERMAIDE&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#20027;&#20307;&#24555;&#36895;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#23398;&#20064;&#31574;&#30053;&#21644;&#22870;&#21169;&#20989;&#25968;&#30340;&#36229;&#20986;&#20998;&#24067;&#20195;&#29702;&#65292;&#20197;&#23454;&#29616;&#29702;&#24819;&#32467;&#26524;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#23569;&#37327;&#26679;&#26412;&#20013;&#36866;&#24212;&#24182;&#20943;&#23569;&#24178;&#39044;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.04668</link><description>&lt;p&gt;
MERMAIDE: &#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
MERMAIDE: Learning to Align Learners using Model-Based Meta-Learning. (arXiv:2304.04668v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04668
&lt;/p&gt;
&lt;p&gt;
MERMAIDE&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#20027;&#20307;&#24555;&#36895;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#23398;&#20064;&#31574;&#30053;&#21644;&#22870;&#21169;&#20989;&#25968;&#30340;&#36229;&#20986;&#20998;&#24067;&#20195;&#29702;&#65292;&#20197;&#23454;&#29616;&#29702;&#24819;&#32467;&#26524;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#23569;&#37327;&#26679;&#26412;&#20013;&#36866;&#24212;&#24182;&#20943;&#23569;&#24178;&#39044;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#20027;&#20307;&#22914;&#20309;&#39640;&#25928;&#26377;&#25928;&#22320;&#24178;&#39044;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#23398;&#20064;&#20195;&#29702;&#30340;&#22870;&#21169;&#65292;&#20197;&#23454;&#29616;&#29702;&#24819;&#30340;&#32467;&#26524;&#12290;&#36825;&#23545;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#26223;&#65288;&#22914;&#25293;&#21334;&#25110;&#31246;&#25910;&#65289;&#26159;&#30456;&#20851;&#30340;&#65292;&#22240;&#20026;&#20027;&#20307;&#21487;&#33021;&#19981;&#30693;&#36947;&#30495;&#23454;&#20154;&#30340;&#23398;&#20064;&#34892;&#20026;&#21644;&#22870;&#21169;&#12290;&#27492;&#22806;&#65292;&#20027;&#20307;&#24212;&#35813;&#33021;&#22815;&#22312;&#23569;&#37327;&#26679;&#26412;&#20013;&#36866;&#24212;&#65292;&#24182;&#19988;&#23613;&#37327;&#20943;&#23569;&#24178;&#39044;&#30340;&#27425;&#25968;&#65292;&#22240;&#20026;&#24178;&#39044;&#36890;&#24120;&#26159;&#26114;&#36149;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MERMAIDE&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#23398;&#20064;&#31574;&#30053;&#21644;&#22870;&#21169;&#20989;&#25968;&#30340;&#36229;&#20986;&#20998;&#24067;&#20195;&#29702;&#30340;&#20027;&#20307;&#12290;&#25105;&#20204;&#36880;&#27493;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#22312;&#20855;&#26377;&#26368;&#20339;&#21709;&#24212;&#20195;&#29702;&#30340;&#26031;&#22612;&#20811;&#36125;&#26684;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20803;&#23398;&#20064;&#22312;&#27979;&#35797;&#26102;&#33021;&#22815;&#24555;&#36895;&#25910;&#25947;&#21040;&#29702;&#35770;&#24050;&#30693;&#30340;&#26031;&#22612;&#20811;&#36125;&#26684;&#22343;&#34913;&#65292;&#23613;&#31649;&#22122;&#22768;&#35266;&#27979;&#20005;&#37325;&#22686;&#21152;&#20102;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#25104;&#26412;&#25928;&#30410;&#39640;
&lt;/p&gt;
&lt;p&gt;
We study how a principal can efficiently and effectively intervene on the rewards of a previously unseen learning agent in order to induce desirable outcomes. This is relevant to many real-world settings like auctions or taxation, where the principal may not know the learning behavior nor the rewards of real people. Moreover, the principal should be few-shot adaptable and minimize the number of interventions, because interventions are often costly. We introduce MERMAIDE, a model-based meta-learning framework to train a principal that can quickly adapt to out-of-distribution agents with different learning strategies and reward functions. We validate this approach step-by-step. First, in a Stackelberg setting with a best-response agent, we show that meta-learning enables quick convergence to the theoretically known Stackelberg equilibrium at test time, although noisy observations severely increase the sample complexity. We then show that our model-based meta-learning approach is cost-eff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#29289;&#29702;&#24341;&#23548;&#37492;&#21035;&#22120;&#30340;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#26469;&#29983;&#25104;&#20154;&#36896;DIC&#20301;&#31227;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292; &#20197;&#35757;&#32451;&#26356;&#31934;&#30830;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#21487;&#38752;&#30340;&#30130;&#21171;&#35010;&#32441;&#22686;&#38271;&#35780;&#20272;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2303.15939</link><description>&lt;p&gt;
&#29289;&#29702;&#24341;&#23548;&#30340;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20154;&#36896;&#25968;&#23383;&#22270;&#20687;&#30456;&#20851;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Physics-guided adversarial networks for artificial digital image correlation data generation. (arXiv:2303.15939v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#29289;&#29702;&#24341;&#23548;&#37492;&#21035;&#22120;&#30340;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#26469;&#29983;&#25104;&#20154;&#36896;DIC&#20301;&#31227;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292; &#20197;&#35757;&#32451;&#26356;&#31934;&#30830;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#21487;&#38752;&#30340;&#30130;&#21171;&#35010;&#32441;&#22686;&#38271;&#35780;&#20272;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#22270;&#20687;&#30456;&#20851;&#65288;DIC&#65289;&#24050;&#25104;&#20026;&#35780;&#20272;&#21147;&#23398;&#23454;&#39564;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#30130;&#21171;&#35010;&#32441;&#22686;&#38271;&#23454;&#39564;&#12290;&#35780;&#20272;&#38656;&#35201;&#20934;&#30830;&#30340;&#35010;&#32441;&#36335;&#24452;&#21644;&#35010;&#32441;&#23574;&#31471;&#20301;&#32622;&#20449;&#24687;&#65292;&#30001;&#20110;&#22266;&#26377;&#22122;&#22768;&#21644;&#20266;&#24433;&#30340;&#21407;&#22240;&#65292;&#36825;&#24456;&#38590;&#33719;&#24471;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#32473;&#23450;&#26631;&#35760;&#30340;DIC&#20301;&#31227;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#27492;&#30456;&#20851;&#20449;&#24687;&#38750;&#24120;&#25104;&#21151;&#12290;&#20026;&#20102;&#35757;&#32451;&#20855;&#26377;&#24191;&#27867;&#27867;&#21270;&#33021;&#21147;&#30340;&#24378;&#22823;&#27169;&#22411;&#65292;&#38656;&#35201;&#22823;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#39564;&#26114;&#36149;&#19988;&#32791;&#26102;&#65292;&#26448;&#26009;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#30340;&#25968;&#25454;&#36890;&#24120;&#24456;&#23569;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#29289;&#29702;&#24341;&#23548;&#37492;&#21035;&#22120;&#30340;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#26469;&#29983;&#25104;&#20154;&#36896;DIC&#20301;&#31227;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#20915;&#23450;&#25968;&#25454;&#26679;&#26412;&#26159;&#30495;&#23454;&#36824;&#26159;&#20551;&#30340;&#65292;&#35813;&#37492;&#21035;&#22120;&#21478;&#22806;&#25509;&#25910;&#23548;&#20986;&#30340;von Mises&#31561;&#25928;&#24212;&#21464;&#12290;&#25105;&#20204;&#26174;&#31034;&#65292;&#36825;&#31181;&#29289;&#29702;&#24341;&#23548;&#26041;&#27861;&#30456;&#27604;&#20256;&#32479;GAN&#20135;&#29983;&#20102;&#26356;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#20135;&#29983;&#22823;&#37327;&#30340;&#20154;&#36896;DIC&#25968;&#25454;&#65292;&#20197;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#21487;&#38752;&#30340;&#30130;&#21171;&#35010;&#32441;&#22686;&#38271;&#35780;&#20272;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital image correlation (DIC) has become a valuable tool in the evaluation of mechanical experiments, particularly fatigue crack growth experiments. The evaluation requires accurate information of the crack path and crack tip position, which is difficult to obtain due to inherent noise and artefacts. Machine learning models have been extremely successful in recognizing this relevant information given labelled DIC displacement data. For the training of robust models, which generalize well, big data is needed. However, data is typically scarce in the field of material science and engineering because experiments are expensive and time-consuming. We present a method to generate synthetic DIC displacement data using generative adversarial networks with a physics-guided discriminator. To decide whether data samples are real or fake, this discriminator additionally receives the derived von Mises equivalent strain. We show that this physics-guided approach leads to improved results in terms 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#29615;Koopman&#31639;&#23376;&#36924;&#36817;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;Koopman&#31639;&#23376;&#30340;&#32447;&#24615;&#24615;&#36136;&#21644;&#23545;&#25511;&#21046;&#22120;&#21644;&#38381;&#29615;&#31995;&#32479;&#32467;&#26500;&#30340;&#20102;&#35299;&#65292;&#21487;&#20197;&#21516;&#26102;&#35782;&#21035;&#38381;&#29615;&#21644;&#35013;&#32622;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.15318</link><description>&lt;p&gt;
&#38381;&#29615;Koopman&#31639;&#23376;&#36924;&#36817;&#27861;
&lt;/p&gt;
&lt;p&gt;
Closed-Loop Koopman Operator Approximation. (arXiv:2303.15318v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#29615;Koopman&#31639;&#23376;&#36924;&#36817;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;Koopman&#31639;&#23376;&#30340;&#32447;&#24615;&#24615;&#36136;&#21644;&#23545;&#25511;&#21046;&#22120;&#21644;&#38381;&#29615;&#31995;&#32479;&#32467;&#26500;&#30340;&#20102;&#35299;&#65292;&#21487;&#20197;&#21516;&#26102;&#35782;&#21035;&#38381;&#29615;&#21644;&#35013;&#32622;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24050;&#30693;&#25511;&#21046;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#35782;&#21035;&#21453;&#39304;&#25511;&#21046;&#31995;&#32479;&#30340;Koopman&#27169;&#22411;&#12290;Koopman&#31639;&#23376;&#36890;&#36807;&#23558;&#38750;&#32447;&#24615;&#31995;&#32479;&#35270;&#20026;&#26080;&#38480;&#32500;&#32447;&#24615;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#26080;&#38480;&#25968;&#37327;&#30340;&#26144;&#23556;&#20989;&#25968;&#36827;&#34892;&#37325;&#20889;&#12290;&#36890;&#36807;&#36873;&#25321;&#26377;&#38480;&#25968;&#37327;&#30340;&#26144;&#23556;&#20989;&#25968;&#24182;&#22312;&#26144;&#23556;&#31354;&#38388;&#20013;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;Koopman&#31639;&#23376;&#30340;&#26377;&#38480;&#32500;&#36817;&#20284;&#12290;&#29616;&#26377;&#26041;&#27861;&#26088;&#22312;&#35782;&#21035;&#24320;&#29615;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26576;&#20123;&#31995;&#32479;&#65288;&#22914;&#19981;&#31283;&#23450;&#31995;&#32479;&#65289;&#65292;&#20197;&#24320;&#29615;&#26041;&#24335;&#36816;&#34892;&#23454;&#39564;&#26159;&#19981;&#20999;&#23454;&#38469;&#25110;&#19981;&#21487;&#33021;&#30340;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;Koopman&#31639;&#23376;&#30340;&#32447;&#24615;&#24615;&#36136;&#65292;&#32467;&#21512;&#25511;&#21046;&#22120;&#21644;&#38381;&#29615;&#31995;&#32479;&#30340;&#32467;&#26500;&#65292;&#21516;&#26102;&#35782;&#21035;&#38381;&#29615;&#21644;&#35013;&#32622;&#31995;&#32479;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#38381;&#29615;Koopman&#31639;&#23376;&#36924;&#36817;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method to identify a Koopman model of a feedback-controlled system given a known controller. The Koopman operator allows a nonlinear system to be rewritten as an infinite-dimensional linear system by viewing it in terms of an infinite set of lifting functions. A finite-dimensional approximation of the Koopman operator can be identified from data by choosing a finite subset of lifting functions and solving a regression problem in the lifted space. Existing methods are designed to identify open-loop systems. However, it is impractical or impossible to run experiments on some systems, such as unstable systems, in an open-loop fashion. The proposed method leverages the linearity of the Koopman operator, along with knowledge of the controller and the structure of the closed-loop system, to simultaneously identify the closed-loop and plant systems. The advantages of the proposed closed-loop Koopman operator approximation method are demonstrated experimentally using a ro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#20998;&#25955;&#30340;&#32852;&#37030;&#32456;&#36523;&#23398;&#20064; (ADFLL) &#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#20219;&#21153;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#38382;&#39064;&#65292;&#24182;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#22320;&#26631;&#23450;&#20301;&#26041;&#38754;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.06783</link><description>&lt;p&gt;
&#24322;&#27493;&#20998;&#25955;&#30340;&#32852;&#37030;&#32456;&#36523;&#23398;&#20064;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#22320;&#26631;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Decentralized Federated Lifelong Learning for Landmark Localization in Medical Imaging. (arXiv:2303.06783v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#20998;&#25955;&#30340;&#32852;&#37030;&#32456;&#36523;&#23398;&#20064; (ADFLL) &#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#22810;&#20010;&#20219;&#21153;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#38382;&#39064;&#65292;&#24182;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#22320;&#26631;&#23450;&#20301;&#26041;&#38754;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20010;&#26032;&#21457;&#23637;&#65292;&#23427;&#20801;&#35768;&#35774;&#22791;&#30340;&#31995;&#32479;&#22312;&#19968;&#20010;&#25110;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#23558;&#20854;&#25968;&#25454;&#20849;&#20139;&#21040;&#21333;&#20010;&#20301;&#32622;&#25110;&#35774;&#22791;&#12290;&#28982;&#32780;&#65292;&#35813;&#26694;&#26550;&#20173;&#28982;&#38656;&#35201;&#19968;&#20010;&#38598;&#20013;&#30340;&#20840;&#23616;&#27169;&#22411;&#26469;&#23558;&#20010;&#20307;&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#65292;&#24182;&#19988;&#35774;&#22791;&#36827;&#34892;&#21516;&#27493;&#35757;&#32451;&#65292;&#36825;&#20004;&#32773;&#37117;&#21487;&#33021;&#25104;&#20026;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#28508;&#22312;&#29942;&#39048;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#27493;&#20998;&#25955;&#30340;&#32852;&#37030;&#32456;&#36523;&#23398;&#20064; (ADFLL) &#26041;&#27861;&#65292;&#23427;&#32487;&#25215;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#21516;&#26102;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#20013;&#22830;&#33410;&#28857;&#25110;&#21516;&#27493;&#35757;&#32451;&#12290;&#20174;&#32780;&#20811;&#26381;&#20102;&#20256;&#32479;&#32852;&#37030;&#23398;&#20064;&#30340;&#28508;&#22312;&#32570;&#28857;&#12290;&#25105;&#20204;&#22312;&#33041;&#32959;&#30244;&#20998;&#21106; (BRATS) &#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#29992;&#20110;&#23450;&#20301;&#22810;&#20010;&#22270;&#20687;&#24207;&#21015;&#21644;&#22270;&#20687;&#26041;&#21521;&#30340;&#24038;&#23460;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#20195;&#29702;&#21830;&#20197;&#24179;&#22343;&#36317;&#31163;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a recent development in the machine learning area that allows a system of devices to train on one or more tasks without sharing their data to a single location or device. However, this framework still requires a centralized global model to consolidate individual models into one, and the devices train synchronously, which both can be potential bottlenecks for using federated learning. In this paper, we propose a novel method of asynchronous decentralized federated lifelong learning (ADFLL) method that inherits the merits of federated learning and can train on multiple tasks simultaneously without the need for a central node or synchronous training. Thus, overcoming the potential drawbacks of conventional federated learning. We demonstrate excellent performance on the brain tumor segmentation (BRATS) dataset for localizing the left ventricle on multiple image sequences and image orientation. Our framework allows agents to achieve the best performance with a mean dis
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FeDepth&#30340;&#20869;&#23384;&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#20869;&#23384;&#39044;&#31639;&#23558;&#23436;&#25972;&#27169;&#22411;&#33258;&#36866;&#24212;&#22320;&#20998;&#35299;&#25104;&#22359;&#65292;&#24182;&#20381;&#27425;&#35757;&#32451;&#36825;&#20123;&#22359;&#65292;&#20197;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#24322;&#26500;&#35774;&#22791;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.04887</link><description>&lt;p&gt;
&#21487;&#21464;&#28145;&#24230;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#20869;&#23384;&#33258;&#36866;&#24212;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Memory-adaptive Depth-wise Heterogenous Federated Learning. (arXiv:2303.04887v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04887
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FeDepth&#30340;&#20869;&#23384;&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#20869;&#23384;&#39044;&#31639;&#23558;&#23436;&#25972;&#27169;&#22411;&#33258;&#36866;&#24212;&#22320;&#20998;&#35299;&#25104;&#22359;&#65292;&#24182;&#20381;&#27425;&#35757;&#32451;&#36825;&#20123;&#22359;&#65292;&#20197;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#24322;&#26500;&#35774;&#22791;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#65292;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#19981;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23384;&#22312;&#24322;&#26500;&#35774;&#22791;&#65292;&#22914;&#25163;&#26426;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#20869;&#23384;&#33021;&#21147;&#19981;&#21516;&#65292;&#20250;&#38480;&#21046;&#27169;&#22411;&#33021;&#22815;&#35757;&#32451;&#30340;&#35268;&#27169;&#21644;&#24615;&#33021;&#12290;&#20027;&#35201;&#35299;&#20915;&#20869;&#23384;&#38480;&#21046;&#30340;&#26041;&#27861;&#38598;&#20013;&#22312;&#20943;&#23569;&#23485;&#24230;&#30340;&#25216;&#26415;&#19978;&#65292;&#21363;&#19981;&#21516;&#23458;&#25143;&#31471;&#22312;&#26412;&#22320;&#35757;&#32451;&#20943;&#23485;&#24230;&#30340;&#23376;&#32593;&#32476;&#65292;&#28982;&#21518;&#26381;&#21153;&#22120;&#32858;&#21512;&#36825;&#20123;&#23376;&#32593;&#32476;&#12290;&#30001;&#20110;&#22788;&#29702;&#32858;&#21512;&#38454;&#27573;&#20013;&#19981;&#21516;&#23376;&#32593;&#32476;&#23485;&#24230;&#21464;&#21270;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#36825;&#20123;&#26041;&#27861;&#20135;&#29983;&#30340;&#20840;&#23616;&#27169;&#22411;&#20250;&#21463;&#21040;&#24615;&#33021;&#30340;&#38477;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;FeDepth&#30340;&#20869;&#23384;&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#20869;&#23384;&#39044;&#31639;&#23558;&#23436;&#25972;&#27169;&#22411;&#33258;&#36866;&#24212;&#22320;&#20998;&#35299;&#25104;&#22359;&#65292;&#24182;&#20381;&#27425;&#35757;&#32451;&#36825;&#20123;&#22359;&#65292;&#20197;&#33719;&#21462;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a promising paradigm that allows multiple clients to collaboratively train a model without sharing the local data. However, the presence of heterogeneous devices in federated learning, such as mobile phones and IoT devices with varying memory capabilities, would limit the scale and hence the performance of the model could be trained. The mainstream approaches to address memory limitations focus on width-slimming techniques, where different clients train subnetworks with reduced widths locally and then the server aggregates the subnetworks. The global model produced from these methods suffers from performance degradation due to the negative impact of the actions taken to handle the varying subnetwork widths in the aggregation phase. In this paper, we introduce a memory-adaptive depth-wise learning solution in FL called FeDepth, which adaptively decomposes the full model into blocks according to the memory budgets of each client and trains blocks sequentially to obt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#24515;&#38598;&#36827;&#34892;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#21387;&#32553;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#21319;&#32456;&#36523;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#24212;&#29992;&#20110;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2302.11510</link><description>&lt;p&gt;
&#20351;&#29992;&#26680;&#24515;&#38598;&#30340;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#21387;&#32553;&#29992;&#20110;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20013;&#30340;&#32456;&#36523;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging. (arXiv:2302.11510v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#24515;&#38598;&#36827;&#34892;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#21387;&#32553;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#21319;&#32456;&#36523;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#24212;&#29992;&#20110;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#26159;&#23558;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#19982;&#32456;&#36523;&#23398;&#20064;&#32467;&#21512;&#30340;&#19968;&#31181;&#27969;&#34892;&#31574;&#30053;&#12290;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#26088;&#22312;&#37325;&#36848;&#20197;&#21069;&#20219;&#21153;&#20013;&#30340;&#36873;&#25321;&#24615;&#32463;&#39564;&#65292;&#20197;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#30340;&#25216;&#26415;&#26159;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#65292;&#24182;&#20801;&#35768;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#20849;&#20139;&#32463;&#39564;&#12290;&#28982;&#32780;&#65292;&#23384;&#20648;&#25152;&#26377;&#20197;&#21069;&#20219;&#21153;&#30340;&#32463;&#39564;&#20250;&#20351;&#24471;&#20351;&#29992;&#36873;&#25321;&#24615;&#32463;&#39564;&#22238;&#25918;&#30340;&#32456;&#36523;&#23398;&#20064;&#21464;&#24471;&#35745;&#31639;&#19978;&#38750;&#24120;&#26114;&#36149;&#21644;&#19981;&#20999;&#23454;&#38469;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selective experience replay is a popular strategy for integrating lifelong learning with deep reinforcement learning. Selective experience replay aims to recount selected experiences from previous tasks to avoid catastrophic forgetting. Furthermore, selective experience replay based techniques are model agnostic and allow experiences to be shared across different models. However, storing experiences from all previous tasks make lifelong learning using selective experience replay computationally very expensive and impractical as the number of tasks increase. To that end, we propose a reward distribution-preserving coreset compression technique for compressing experience replay buffers stored for selective experience replay.  We evaluated the coreset compression technique on the brain tumor segmentation (BRATS) dataset for the task of ventricle localization and on the whole-body MRI for localization of left knee cap, left kidney, right trochanter, left lung, and spleen. The coreset lifel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#38750;&#32447;&#24615;Ridge Bandits&#20013;&#29420;&#29305;&#30340;&#23398;&#20064;&#29616;&#35937;&#65292;&#25512;&#23548;&#20986;&#20102;&#26368;&#20248;&#28903;&#24405;&#25104;&#26412;&#30340;&#19978;&#19979;&#38480;&#21644;&#25972;&#20010;&#28903;&#24405;&#26399;&#38388;&#30340;&#23398;&#20064;&#36712;&#36857;&#30340;&#32479;&#35745;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;UCB&#21644;&#22522;&#20110;&#22238;&#24402;&#31070;&#32463;&#20803;&#30340;&#31639;&#27861;&#37117;&#26159;&#27425;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2302.06025</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;Ridge Bandits&#30340;&#32479;&#35745;&#22797;&#26434;&#24230;&#21644;&#26368;&#20248;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Statistical Complexity and Optimal Algorithms for Non-linear Ridge Bandits. (arXiv:2302.06025v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#38750;&#32447;&#24615;Ridge Bandits&#20013;&#29420;&#29305;&#30340;&#23398;&#20064;&#29616;&#35937;&#65292;&#25512;&#23548;&#20986;&#20102;&#26368;&#20248;&#28903;&#24405;&#25104;&#26412;&#30340;&#19978;&#19979;&#38480;&#21644;&#25972;&#20010;&#28903;&#24405;&#26399;&#38388;&#30340;&#23398;&#20064;&#36712;&#36857;&#30340;&#32479;&#35745;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;UCB&#21644;&#22522;&#20110;&#22238;&#24402;&#31070;&#32463;&#20803;&#30340;&#31639;&#27861;&#37117;&#26159;&#27425;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#20854;&#20013;&#24179;&#22343;&#32467;&#26524;&#26159;&#25152;&#36873;&#25321;&#21160;&#20316;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;&#19982;&#32447;&#24615;&#27169;&#22411;&#30456;&#27604;&#65292;&#38750;&#32447;&#24615;&#27169;&#22411;&#26377;&#20004;&#31181;&#22855;&#29305;&#29616;&#35937;&#65306;&#39318;&#20808;&#65292;&#38500;&#20102;&#20855;&#26377;&#26631;&#20934;&#21442;&#25968;&#29575;&#30340;&#8220;&#23398;&#20064;&#38454;&#27573;&#8221;&#20197;&#36827;&#34892;&#20272;&#35745;&#25110;&#21518;&#24724;&#22806;&#65292;&#36824;&#26377;&#19968;&#20010;&#30001;&#38750;&#32447;&#24615;&#20989;&#25968;&#30830;&#23450;&#30340;&#22266;&#23450;&#25104;&#26412;&#30340;&#8220;&#28903;&#24405;&#26399;&#8221;; &#20854;&#27425;&#65292;&#23454;&#29616;&#26368;&#23567;&#28903;&#24405;&#25104;&#26412;&#38656;&#35201;&#26032;&#30340;&#25506;&#32034;&#31639;&#27861;&#12290;&#38024;&#23545;&#19968;&#31867;&#21517;&#20026;ridge&#20989;&#25968;&#30340;&#29305;&#27530;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#25105;&#20204;&#36890;&#36807;&#24494;&#20998;&#26041;&#31243;&#25512;&#23548;&#20102;&#26368;&#20248;&#28903;&#24405;&#25104;&#26412;&#30340;&#19978;&#19979;&#38480;&#65292;&#27492;&#22806;&#36824;&#25512;&#23548;&#20102;&#25972;&#20010;&#28903;&#24405;&#26399;&#38388;&#30340;&#23398;&#20064;&#36712;&#36857;&#30340;&#19978;&#19979;&#38480;&#12290;&#29305;&#21035;&#22320;&#65292;&#19968;&#31181;&#20004;&#38454;&#27573;&#31639;&#27861;&#20808;&#25214;&#21040;&#19968;&#20010;&#22909;&#30340;&#21021;&#22987;&#34892;&#21160;&#65292;&#28982;&#21518;&#23558;&#38382;&#39064;&#35270;&#20026;&#23616;&#37096;&#32447;&#24615;&#65292;&#36825;&#26159;&#32479;&#35745;&#19978;&#26368;&#20248;&#30340;&#12290;&#30456;&#21453;&#65292;&#20960;&#31181;&#32463;&#20856;&#31639;&#27861;&#65292;&#20363;&#22914;UCB&#21644;&#20381;&#36182;&#20110;&#22238;&#24402;&#31070;&#32463;&#20803;&#30340;&#31639;&#27861;&#65292;&#20854;&#21487;&#35777;&#26126;&#26159;&#27425;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the sequential decision-making problem where the mean outcome is a non-linear function of the chosen action. Compared with the linear model, two curious phenomena arise in non-linear models: first, in addition to the "learning phase" with a standard parametric rate for estimation or regret, there is an "burn-in period" with a fixed cost determined by the non-linear function; second, achieving the smallest burn-in cost requires new exploration algorithms. For a special family of non-linear functions named ridge functions in the literature, we derive upper and lower bounds on the optimal burn-in cost, and in addition, on the entire learning trajectory during the burn-in period via differential equations. In particular, a two-stage algorithm that first finds a good initial action and then treats the problem as locally linear is statistically optimal. In contrast, several classical algorithms, such as UCB and algorithms relying on regression oracles, are provably suboptimal.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#22312;&#39640;&#23481;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#20013;&#65292;&#40723;&#21169;&#39044;&#27979;&#22810;&#26679;&#24615;&#24182;&#19981;&#24635;&#26159;&#26377;&#25928;&#30340;&#65292;&#29978;&#33267;&#21453;&#32780;&#20250;&#25439;&#23475;&#24615;&#33021;&#12290;&#30456;&#21453;&#22320;&#65292;&#38459;&#27490;&#39044;&#27979;&#22810;&#26679;&#24615;&#24448;&#24448;&#26159;&#26080;&#23475;&#30340;&#65292;&#36825;&#19982;&#20808;&#21069;&#30340;&#30452;&#35273;&#30456;&#21453;&#12290;</title><link>http://arxiv.org/abs/2302.00704</link><description>&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#20013;&#30340;&#39044;&#27979;&#22810;&#26679;&#24615;&#30149;&#24577;
&lt;/p&gt;
&lt;p&gt;
Pathologies of Predictive Diversity in Deep Ensembles. (arXiv:2302.00704v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#22312;&#39640;&#23481;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#20013;&#65292;&#40723;&#21169;&#39044;&#27979;&#22810;&#26679;&#24615;&#24182;&#19981;&#24635;&#26159;&#26377;&#25928;&#30340;&#65292;&#29978;&#33267;&#21453;&#32780;&#20250;&#25439;&#23475;&#24615;&#33021;&#12290;&#30456;&#21453;&#22320;&#65292;&#38459;&#27490;&#39044;&#27979;&#22810;&#26679;&#24615;&#24448;&#24448;&#26159;&#26080;&#23475;&#30340;&#65292;&#36825;&#19982;&#20808;&#21069;&#30340;&#30452;&#35273;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#40723;&#21169;&#39044;&#27979;&#22810;&#26679;&#24615;&#21487;&#20197;&#25552;&#39640;&#20302;&#23481;&#37327;&#27169;&#22411;&#30340;&#38598;&#25104;&#24615;&#33021;&#65292;&#20363;&#22914;&#36890;&#36807;Bagging&#25110;Boosting&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#30452;&#35273;&#22312;&#39640;&#23481;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#65288;&#28145;&#24230;&#38598;&#25104;&#65289;&#20013;&#24182;&#19981;&#36866;&#29992;&#65292;&#20107;&#23454;&#19978;&#65292;&#24448;&#24448;&#30456;&#21453;&#12290;&#36890;&#36807;&#23545;&#36817;600&#20010;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#38598;&#25104;&#30340;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#19968;&#31995;&#21015;&#24179;&#34913;&#32452;&#20214;&#27169;&#22411;&#24615;&#33021;&#19982;&#39044;&#27979;&#22810;&#26679;&#24615;&#20043;&#38388;&#20851;&#31995;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#34429;&#28982;&#36825;&#26679;&#30340;&#24178;&#39044;&#25514;&#26045;&#21487;&#20197;&#25913;&#21892;&#23567;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#30340;&#24615;&#33021;&#65288;&#31526;&#21512;&#26631;&#20934;&#30452;&#35273;&#65289;&#65292;&#20294;&#23427;&#20204;&#21364;&#20250;&#25439;&#23475;&#22312;&#23454;&#36341;&#20013;&#26368;&#24120;&#29992;&#30340;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#30340;&#24615;&#33021;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#22823;&#22411;&#32593;&#32476;&#38598;&#25104;&#20013;&#65292;&#38459;&#27490;&#39044;&#27979;&#22810;&#26679;&#24615;&#24448;&#24448;&#26159;&#26080;&#23475;&#30340;&#65292;&#23436;&#20840;&#39072;&#35206;&#20102;&#26631;&#20934;&#30340;&#30452;&#35273;&#12290;&#21363;&#20351;&#22810;&#26679;&#24615;&#20419;&#36827;&#30340;&#24178;&#39044;&#25514;&#26045;&#19981;&#29306;&#29298;&#32452;&#20214;&#27169;&#22411;&#30340;&#24615;&#33021;&#65288;&#20363;&#22914;&#20351;&#29992;&#24322;&#26500;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#65289;...
&lt;/p&gt;
&lt;p&gt;
Classic results establish that encouraging predictive diversity improves performance in ensembles of low-capacity models, e.g. through bagging or boosting. Here we demonstrate that these intuitions do not apply to high-capacity neural network ensembles (deep ensembles), and in fact the opposite is often true. In a large scale study of nearly 600 neural network classification ensembles, we examine a variety of interventions that trade off component model performance for predictive diversity. While such interventions can improve the performance of small neural network ensembles (in line with standard intuitions), they harm the performance of the large neural network ensembles most often used in practice. Surprisingly, we also find that discouraging predictive diversity is often benign in large-network ensembles, fully inverting standard intuitions. Even when diversity-promoting interventions do not sacrifice component model performance (e.g. using heterogeneous architectures and training
&lt;/p&gt;</description></item><item><title>&#26696;&#20363;&#22522;&#30784;&#31070;&#32463;&#32593;&#32476;&#65288;CBNNs&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#27169;&#25311;&#26102;&#38388;&#21464;&#21270;&#30340;&#20132;&#20114;&#21644;&#22797;&#26434;&#30340;&#22522;&#32447;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2301.06535</link><description>&lt;p&gt;
&#26696;&#20363;&#22522;&#30784;&#31070;&#32463;&#32593;&#32476;&#65306;&#20855;&#26377;&#26102;&#38388;&#21464;&#21270;&#30340;&#39640;&#38454;&#20132;&#20114;&#30340;&#29983;&#23384;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Case-Base Neural Networks: survival analysis with time-varying, higher-order interactions. (arXiv:2301.06535v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06535
&lt;/p&gt;
&lt;p&gt;
&#26696;&#20363;&#22522;&#30784;&#31070;&#32463;&#32593;&#32476;&#65288;CBNNs&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#27169;&#25311;&#26102;&#38388;&#21464;&#21270;&#30340;&#20132;&#20114;&#21644;&#22797;&#26434;&#30340;&#22522;&#32447;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22522;&#20110;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#21487;&#20197;&#27169;&#25311;&#25968;&#25454;&#39537;&#21160;&#30340;&#21327;&#21464;&#37327;&#20132;&#20114;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#27604;&#22238;&#24402;&#26041;&#27861;&#25552;&#20379;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#24182;&#19981;&#26159;&#25152;&#26377;&#30340;&#26041;&#27861;&#37117;&#21487;&#20197;&#27169;&#25311;&#26102;&#38388;&#21464;&#21270;&#30340;&#20132;&#20114;&#21644;&#22797;&#26434;&#30340;&#22522;&#32447;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26696;&#20363;&#22522;&#30784;&#31070;&#32463;&#32593;&#32476;&#65288;CBNNs&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;&#26696;&#20363;&#22522;&#30784;&#25277;&#26679;&#26694;&#26550;&#19982;&#28789;&#27963;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25277;&#26679;&#26041;&#26696;&#21644;&#25968;&#25454;&#22686;&#24378;&#26469;&#33258;&#28982;&#22320;&#32771;&#34385;&#21040;&#25130;&#23614;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#20197;&#25509;&#21463;&#26102;&#38388;&#36755;&#20837;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#12290;CBNNs&#36890;&#36807;&#39044;&#27979;&#22312;&#32473;&#23450;&#26102;&#21051;&#20107;&#20214;&#21457;&#29983;&#30340;&#27010;&#29575;&#26469;&#20272;&#35745;&#21361;&#38505;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21644;&#19977;&#20010;&#26696;&#20363;&#30740;&#31350;&#20351;&#29992;&#20004;&#20010;&#26102;&#38388;&#20381;&#36182;&#25351;&#26631;&#27604;&#36739;CBNNs&#19982;&#22238;&#24402;&#21644;&#31070;&#32463;&#32593;&#32476;&#22522;&#20110;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#28041;&#21450;&#22797;&#26434;&#22522;&#32447;&#39118;&#38505;&#21644;&#26102;&#38388;&#21464;&#21270;&#20132;&#20114;&#30340;&#27169;&#25311;&#26469;&#35780;&#20272;&#25152;&#26377;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;CBNNs&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network-based survival methods can model data-driven covariate interactions. While these methods can provide better predictive performance than regression-based approaches, not all can model time-varying interactions and complex baseline hazards. To address this, we propose Case-Base Neural Networks (CBNNs) as a new approach that combines the case-base sampling framework with flexible neural network architectures. Using a novel sampling scheme and data augmentation to naturally account for censoring, we construct a feed-forward neural network that may take time as an input. CBNNs predict the probability of an event occurring at a given moment to estimate the hazard function. We compare the performance of CBNNs to regression and neural network-based survival methods in a simulation and three case studies using two time-dependent metrics. First, we examine performance on a simulation involving a complex baseline hazard and time-varying interactions to assess all methods, with CBNN
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;SemPPL&#65292;&#36890;&#36807;&#39044;&#27979;&#20266;&#26631;&#31614;&#26469;&#25913;&#21892;&#23545;&#27604;&#34920;&#31034;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#23398;&#20064;&#22823;&#37327;&#26080;&#30417;&#30563;&#25968;&#25454;&#21644;&#23569;&#37327;&#30417;&#30563;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.05158</link><description>&lt;p&gt;
SemPPL: &#39044;&#27979;&#20266;&#26631;&#31614;&#20197;&#25913;&#21892;&#23545;&#27604;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
SemPPL: Predicting pseudo-labels for better contrastive representations. (arXiv:2301.05158v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;SemPPL&#65292;&#36890;&#36807;&#39044;&#27979;&#20266;&#26631;&#31614;&#26469;&#25913;&#21892;&#23545;&#27604;&#34920;&#31034;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#23398;&#20064;&#22823;&#37327;&#26080;&#30417;&#30563;&#25968;&#25454;&#21644;&#23569;&#37327;&#30417;&#30563;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22823;&#37327;&#26080;&#30417;&#30563;&#25968;&#25454;&#21644;&#23569;&#37327;&#30417;&#30563;&#25968;&#25454;&#20013;&#23398;&#20064;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;Semantic Positives via Pseudo-Labels (SemPPL)&#65292;&#23427;&#32467;&#21512;&#20102;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#30340;&#25968;&#25454;&#26469;&#23398;&#20064;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#36890;&#36807;&#36873;&#25321;&#27491;&#26679;&#26412;&#30340;&#26032;&#26041;&#27861;&#65292;&#26469;&#21306;&#20998;&#20004;&#20010;&#26679;&#26412;&#26159;&#21542;&#20195;&#34920;&#30456;&#21516;&#30340;&#22522;&#30784;&#25968;&#25454;&#12290;&#20026;&#20102;&#20016;&#23500;&#27491;&#26679;&#26412;&#38598;&#65292;&#25105;&#20204;&#21033;&#29992;&#23569;&#37327;&#24050;&#26377;&#30340;&#30495;&#23454;&#26631;&#31614;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;&#26377;&#26631;&#31614;&#25968;&#25454;&#30340;&#23884;&#20837;&#26469;&#39044;&#27979;&#32570;&#22833;&#30340;&#26631;&#31614;&#65292;&#36890;&#36807;k&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#23454;&#29616;&#12290;&#25105;&#20204;&#23558;&#20855;&#26377;&#30456;&#21516;&#20266;&#26631;&#31614;&#30340;&#25968;&#25454;&#28857;&#25193;&#23637;&#20026;&#27491;&#26679;&#26412;&#65292;&#24182;&#31216;&#20043;&#20026;&#35821;&#20041;&#27491;&#26679;&#26412;&#12290;&#25105;&#20204;&#21516;&#26102;&#23398;&#20064;&#34920;&#31034;&#21644;&#39044;&#27979;&#33258;&#21160;&#22686;&#24378;&#30340;&#20266;&#26631;&#31614;&#65292;&#24418;&#25104;&#19968;&#20010;&#24490;&#29615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from large amounts of unsupervised data and a small amount of supervision is an important open problem in computer vision. We propose a new semi-supervised learning method, Semantic Positives via Pseudo-Labels (SemPPL), that combines labelled and unlabelled data to learn informative representations. Our method extends self-supervised contrastive learning -where representations are shaped by distinguishing whether two samples represent the same underlying datum (positives) or not (negatives) -- with a novel approach to selecting positives. To enrich the set of positives, we leverage the few existing ground-truth labels to predict the missing ones through a $k$-nearest neighbours classifier by using the learned embeddings of the labelled data. We thus extend the set of positives with datapoints having the same pseudo-label and call these semantic positives. We jointly learn the representation and predict bootstrapped pseudo-labels. This creates a reinforcing cycle. Strong init
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#30862;&#29255;&#30340;&#20998;&#23376;&#34920;&#31034;&#26694;&#26550; t-SMILES&#65292;&#36890;&#36807;&#24341;&#20837; t-SMILES &#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#20998;&#23376;&#30340;&#34920;&#31034;&#25928;&#26524;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#20854;&#20182;&#32463;&#20856;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2301.01829</link><description>&lt;p&gt;
t-SMILES&#65306;&#29992;&#20110;&#20840;&#26032;&#20998;&#23376;&#29983;&#25104;&#30340;&#21487;&#25193;&#23637;&#22522;&#20110;&#30862;&#29255;&#30340;&#20998;&#23376;&#34920;&#31034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
t-SMILES: A Scalable Fragment-based Molecular Representation Framework for De Novo Molecule Generation. (arXiv:2301.01829v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#30862;&#29255;&#30340;&#20998;&#23376;&#34920;&#31034;&#26694;&#26550; t-SMILES&#65292;&#36890;&#36807;&#24341;&#20837; t-SMILES &#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#20998;&#23376;&#30340;&#34920;&#31034;&#25928;&#26524;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#20854;&#20182;&#32463;&#20856;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#30340;&#26377;&#25928;&#34920;&#31034;&#26159;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#12289;&#22522;&#20110;&#30862;&#29255;&#30340;&#22810;&#23610;&#24230;&#20998;&#23376;&#34920;&#31034;&#26694;&#26550; t-SMILES&#65288;&#22522;&#20110;&#26641;&#30340;SMILES&#65289;&#65292;&#35813;&#26694;&#26550;&#21253;&#21547;&#19977;&#31181;&#20195;&#30721;&#31639;&#27861;&#65306;TSSA&#65288;&#24102;&#26377;&#20849;&#20139;&#21407;&#23376;&#30340;t-SMILES&#65289;&#12289;TSDY&#65288;&#24102;&#26377;&#34394;&#25311;&#21407;&#23376;&#30340;t-SMILES&#65289;&#21644;TSID&#65288;&#24102;&#26377;ID&#30340;t-SMILES&#65289;&#12290;&#23427;&#20351;&#29992;&#20174;&#20998;&#23376;&#22270;&#30340;&#30862;&#29255;&#24418;&#25104;&#30340;&#20840;&#20108;&#21449;&#26641;&#19978;&#36827;&#34892;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#24471;&#21040;&#30340;SMILES&#31867;&#22411;&#23383;&#31526;&#20018;&#26469;&#25551;&#36848;&#20998;&#23376;&#12290;&#36890;&#36807;&#20351;&#29992;JTVAE&#12289;BRICS&#12289;MMPA&#21644;Scaffold&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#65292;&#26174;&#31034;&#20102;&#26500;&#24314;&#22810;&#20195;&#30721;&#20998;&#23376;&#25551;&#36848;&#31995;&#32479;&#30340;&#21487;&#34892;&#24615;&#65292;&#21508;&#31181;&#25551;&#36848;&#30456;&#20114;&#34917;&#20805;&#65292;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#26080;&#35770;&#27169;&#22411;&#26159;&#21407;&#22987;&#30340;&#12289;&#25968;&#25454;&#22686;&#24378;&#30340;&#36824;&#26159;&#39044;&#35757;&#32451;&#24494;&#35843;&#30340;&#12290;&#23427;&#22312;goa&#31561;&#20219;&#21153;&#20013;&#26126;&#26174;&#20248;&#20110;&#32463;&#20856;&#30340;SMILES&#12289;DeepSMILES&#12289;SELFIES&#21644;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective representation of molecules is a crucial factor affecting the performance of artificial intelligence models. This study introduces a flexible, fragment-based, multiscale molecular representation framework called t-SMILES (tree-based SMILES) with three code algorithms: TSSA (t-SMILES with Shared Atom), TSDY (t-SMILES with Dummy Atom) and TSID (t-SMILES with ID). It describes molecules using SMILES-type strings obtained by performing a breadth-first search on a full binary tree formed from a fragmented molecular graph. Systematic evaluations using JTVAE, BRICS, MMPA, and Scaffold show the feasibility to construct a multi-code molecular description system, where various descriptions complement each other, enhancing the overall performance. Additionally, it exhibits impressive performance on low-resource datasets, whether the model is original, data augmented, or pre-training fine-tuned. It significantly outperforms classical SMILES, DeepSMILES, SELFIES and baseline models in goa
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#32534;&#30721;&#22120;&#37096;&#20998;&#65292;&#35813;&#26041;&#27861;&#20559;&#22909;&#31561;&#36317;&#12289;&#22806;&#22312;&#24179;&#22374;&#23884;&#20837;&#65292;&#24182;&#20801;&#35768;&#21333;&#29420;&#23545;&#32534;&#30721;&#22120;&#36827;&#34892;&#35757;&#32451;&#12290;&#30740;&#31350;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#31215;&#20998;&#35745;&#31639;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#29992;&#23616;&#37096;Riemannian&#36317;&#31163;&#21644;&#23616;&#37096;Riemannian&#22343;&#20540;&#35780;&#20272;&#36755;&#20837;&#27969;&#24418;&#19978;&#30340;&#28857;&#23545;&#12290;</title><link>http://arxiv.org/abs/2208.10193</link><description>&lt;p&gt;
&#20855;&#26377;&#20302;&#24367;&#26354;&#21644;&#20302;&#30072;&#21464;&#30340;&#27969;&#24418;&#23884;&#20837;&#30340;&#25910;&#25947;&#33258;&#21160;&#32534;&#30721;&#22120;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Convergent autoencoder approximation of low bending and low distortion manifold embeddings. (arXiv:2208.10193v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10193
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#32534;&#30721;&#22120;&#37096;&#20998;&#65292;&#35813;&#26041;&#27861;&#20559;&#22909;&#31561;&#36317;&#12289;&#22806;&#22312;&#24179;&#22374;&#23884;&#20837;&#65292;&#24182;&#20801;&#35768;&#21333;&#29420;&#23545;&#32534;&#30721;&#22120;&#36827;&#34892;&#35757;&#32451;&#12290;&#30740;&#31350;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#31215;&#20998;&#35745;&#31639;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#29992;&#23616;&#37096;Riemannian&#36317;&#31163;&#21644;&#23616;&#37096;Riemannian&#22343;&#20540;&#35780;&#20272;&#36755;&#20837;&#27969;&#24418;&#19978;&#30340;&#28857;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#32534;&#30721;&#22120;&#30001;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#32452;&#25104;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24191;&#27867;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#30340;&#38477;&#32500;&#12290;&#32534;&#30721;&#22120;&#23558;&#36755;&#20837;&#25968;&#25454;&#27969;&#24418;&#23884;&#20837;&#21040;&#36739;&#20302;&#32500;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#32780;&#35299;&#30721;&#22120;&#21017;&#34920;&#31034;&#21453;&#21521;&#26144;&#23556;&#65292;&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#27969;&#24418;&#32473;&#20986;&#25968;&#25454;&#27969;&#24418;&#30340;&#21442;&#25968;&#21270;&#12290;&#33391;&#22909;&#30340;&#23884;&#20837;&#27969;&#24418;&#30340;&#35268;&#21017;&#24615;&#21644;&#32467;&#26500;&#24615;&#21487;&#20197;&#26497;&#22823;&#22320;&#31616;&#21270;&#36827;&#19968;&#27493;&#30340;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#65292;&#22914;&#32858;&#31867;&#20998;&#26512;&#25110;&#25968;&#25454;&#25554;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#32534;&#30721;&#22120;&#37096;&#20998;&#65306;&#19968;&#31181;&#20559;&#22909;&#31561;&#36317;&#12289;&#22806;&#22312;&#24179;&#22374;&#23884;&#20837;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20801;&#35768;&#21333;&#29420;&#23545;&#32534;&#30721;&#22120;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#36827;&#34892;&#35757;&#32451;&#65292;&#20551;&#35774;&#21487;&#20197;&#35780;&#20272;&#36755;&#20837;&#27969;&#24418;&#19978;&#38468;&#36817;&#28857;&#23545;&#30340;&#23616;&#37096;Riemannian&#36317;&#31163;&#21644;&#23616;&#37096;Riemannian&#22343;&#20540;&#12290;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#31215;&#20998;&#21644;&#19981;&#21516;&#30340;&#37319;&#26679;&#31574;&#30053;&#35745;&#31639;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoencoders, which consist of an encoder and a decoder, are widely used in machine learning for dimension reduction of high-dimensional data. The encoder embeds the input data manifold into a lower-dimensional latent space, while the decoder represents the inverse map, providing a parametrization of the data manifold by the manifold in latent space. A good regularity and structure of the embedded manifold may substantially simplify further data processing tasks such as cluster analysis or data interpolation. We propose and analyze a novel regularization for learning the encoder component of an autoencoder: a loss functional that prefers isometric, extrinsically flat embeddings and allows to train the encoder on its own. To perform the training it is assumed that for pairs of nearby points on the input manifold their local Riemannian distance and their local Riemannian average can be evaluated. The loss functional is computed via Monte Carlo integration with different sampling strategi
&lt;/p&gt;</description></item><item><title>GANDALF&#26159;&#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#39640;&#24615;&#33021;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#29305;&#28857;&#12290;&#36890;&#36807;&#24341;&#20837;&#38376;&#25511;&#29305;&#24449;&#23398;&#20064;&#21333;&#20803;&#65292;GANDALF&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#20844;&#24320;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#25110;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25345;&#24179;&#12290;</title><link>http://arxiv.org/abs/2207.08548</link><description>&lt;p&gt;
GANDALF: &#29992;&#20110;&#28145;&#24230;&#33258;&#21160;&#21270;&#29305;&#24449;&#23398;&#20064;&#30340;&#38376;&#25511;&#33258;&#36866;&#24212;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GANDALF: Gated Adaptive Network for Deep Automated Learning of Features. (arXiv:2207.08548v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08548
&lt;/p&gt;
&lt;p&gt;
GANDALF&#26159;&#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#39640;&#24615;&#33021;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#20855;&#26377;&#35299;&#37322;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#29305;&#28857;&#12290;&#36890;&#36807;&#24341;&#20837;&#38376;&#25511;&#29305;&#24449;&#23398;&#20064;&#21333;&#20803;&#65292;GANDALF&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#20844;&#24320;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#25110;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#12289;&#21442;&#25968;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#65292;&#31216;&#20026;GANDALF&#65288;Gated Adaptive Network for Deep Automated Learning of Features&#65289;&#12290;GANDALF&#20381;&#36182;&#20110;&#19968;&#20010;&#20855;&#26377;&#38376;&#25511;&#26426;&#21046;&#21644;&#20869;&#32622;&#29305;&#24449;&#36873;&#25321;&#30340;&#26032;&#30340;&#34920;&#26684;&#22788;&#29702;&#21333;&#20803;&#65292;&#31216;&#20026;&#38376;&#25511;&#29305;&#24449;&#23398;&#20064;&#21333;&#20803;&#65288;GFLU&#65289;&#65292;&#20316;&#20026;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#21333;&#20803;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#20844;&#24320;&#30340;&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;GANDALF&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#25110;&#19982;XGBoost&#12289;SAINT&#12289;FT-Transformers&#31561;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25345;&#24179;&#12290;&#25105;&#20204;&#24050;&#32463;&#23558;&#20195;&#30721;&#22312;github.com/manujosephv/pytorch_tabular&#19978;&#20197;MIT&#35768;&#21487;&#35777;&#30340;&#24418;&#24335;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel high-performance, interpretable, and parameter \&amp; computationally efficient deep learning architecture for tabular data, Gated Adaptive Network for Deep Automated Learning of Features (GANDALF). GANDALF relies on a new tabular processing unit with a gating mechanism and in-built feature selection called Gated Feature Learning Unit (GFLU) as a feature representation learning unit. We demonstrate that GANDALF outperforms or stays at-par with SOTA approaches like XGBoost, SAINT, FT-Transformers, etc. by experiments on multiple established public benchmarks. We have made available the code at github.com/manujosephv/pytorch_tabular under MIT License.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#26080;&#32447;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#20013;&#36827;&#34892;&#24863;&#30693;&#35774;&#35745;&#12290;&#36890;&#36807;&#22312;&#26234;&#33021;&#20256;&#24863;&#22120;&#38388;&#36827;&#34892;&#20915;&#31574;&#65292;&#21487;&#20197;&#22312;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#24182;&#33719;&#24471;&#26368;&#20248;&#30340;&#24863;&#30693;&#35774;&#35745;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2204.00703</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36164;&#28304;&#21463;&#38480;&#26080;&#32447;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#20013;&#30340;&#24863;&#30693;&#35774;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Reinforcement Learning Approach to Sensing Design in Resource-Constrained Wireless Networked Control Systems. (arXiv:2204.00703v5 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.00703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#26080;&#32447;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#20013;&#36827;&#34892;&#24863;&#30693;&#35774;&#35745;&#12290;&#36890;&#36807;&#22312;&#26234;&#33021;&#20256;&#24863;&#22120;&#38388;&#36827;&#34892;&#20915;&#31574;&#65292;&#21487;&#20197;&#22312;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#24182;&#33719;&#24471;&#26368;&#20248;&#30340;&#24863;&#30693;&#35774;&#35745;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31181;&#26234;&#33021;&#20256;&#24863;&#22120;&#65288;&#20195;&#29702;&#22120;&#20214;&#65289;&#32452;&#25104;&#30340;&#26080;&#32447;&#32593;&#32476;&#65292;&#29992;&#20110;&#30417;&#27979;&#21160;&#24577;&#36807;&#31243;&#24182;&#23558;&#27979;&#37327;&#32467;&#26524;&#21457;&#36865;&#32473;&#25191;&#34892;&#20840;&#23616;&#30417;&#27979;&#21644;&#20915;&#31574;&#30340;&#22522;&#31449;&#12290;&#26234;&#33021;&#20256;&#24863;&#22120;&#20855;&#22791;&#24863;&#30693;&#21644;&#35745;&#31639;&#33021;&#21147;&#65292;&#21487;&#20197;&#21457;&#36865;&#21407;&#22987;&#27979;&#37327;&#25968;&#25454;&#25110;&#22312;&#20256;&#36755;&#21069;&#36827;&#34892;&#22788;&#29702;&#12290;&#20195;&#29702;&#22120;&#20214;&#36164;&#28304;&#30340;&#38480;&#21046;&#23548;&#33268;&#20102;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#12290;&#19968;&#26041;&#38754;&#65292;&#21407;&#22987;&#27979;&#37327;&#25968;&#25454;&#20855;&#26377;&#19981;&#20934;&#30830;&#20294;&#20135;&#29983;&#24555;&#30340;&#29305;&#28857;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20026;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#24179;&#21488;&#19978;&#29983;&#25104;&#20934;&#30830;&#30340;&#27979;&#37327;&#25968;&#25454;&#65292;&#38656;&#35201;&#32791;&#36153;&#30456;&#24403;&#30340;&#35745;&#31639;&#24310;&#36831;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#22788;&#29702;&#21518;&#30340;&#25968;&#25454;&#20063;&#32463;&#36807;&#20102;&#21387;&#32553;&#65292;&#21017;&#30001;&#20110;&#26080;&#32447;&#36890;&#20449;&#24341;&#36215;&#30340;&#24310;&#36831;&#21487;&#33021;&#26356;&#39640;&#12290;&#22240;&#27492;&#65292;&#20915;&#23450;&#32593;&#32476;&#20013;&#30340;&#20256;&#24863;&#22120;&#20309;&#26102;&#20309;&#22320;&#20256;&#36755;&#21407;&#22987;&#27979;&#37327;&#25968;&#25454;&#25110;&#21033;&#29992;&#32791;&#26102;&#30340;&#26412;&#22320;&#22788;&#29702;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#35774;&#35745;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#19968;&#20010;&#26368;&#20248;&#30340;&#24863;&#30693;&#35774;&#35745;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider a wireless network of smart sensors (agents) that monitor a dynamical process and send measurements to a base station that performs global monitoring and decision-making. Smart sensors are equipped with both sensing and computation, and can either send raw measurements or process them prior to transmission. Constrained agent resources raise a fundamental latency-accuracy trade-off. On the one hand, raw measurements are inaccurate but fast to produce. On the other hand, data processing on resource-constrained platforms generates accurate measurements at the cost of non-negligible computation latency. Further, if processed data are also compressed, latency caused by wireless communication might be higher for raw measurements. Hence, it is challenging to decide when and where sensors in the network should transmit raw measurements or leverage time-consuming local processing. To tackle this design problem, we propose a Reinforcement Learning approach to learn an 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#20048;&#35266;&#26799;&#24230;&#26041;&#27861;&#35299;&#37322;&#20026;&#23545;&#37051;&#36817;&#28857;&#27861;&#30340;&#36817;&#20284;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#20048;&#35266;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#22797;&#21512;&#30446;&#26631;&#20989;&#25968;&#30340;&#32422;&#26463;&#38797;&#28857;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;Bregman&#36317;&#31163;&#22788;&#29702;&#20219;&#24847;&#33539;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#22238;&#28335;&#32447;&#25628;&#32034;&#26041;&#26696;&#65292;&#20197;&#36873;&#25321;&#27493;&#38271;&#65292;&#32780;&#19981;&#38656;&#35201;&#20102;&#35299;&#24179;&#28369;&#31995;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#19968;&#38454;&#12289;&#20108;&#38454;&#21644;&#26356;&#39640;&#38454;&#25968;&#23398;&#35268;&#21017;&#26102;&#65292;&#32473;&#20986;&#20102;&#24050;&#30693;&#30340;&#20840;&#23616;&#36845;&#20195;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2202.09674</link><description>&lt;p&gt;
&#24191;&#20041;&#20048;&#35266;&#26041;&#27861;&#29992;&#20110;&#20984;&#20985;&#38797;&#28857;&#38382;&#39064;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized Optimistic Methods for Convex-Concave Saddle Point Problems. (arXiv:2202.09674v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.09674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#20048;&#35266;&#26799;&#24230;&#26041;&#27861;&#35299;&#37322;&#20026;&#23545;&#37051;&#36817;&#28857;&#27861;&#30340;&#36817;&#20284;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#20048;&#35266;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#22797;&#21512;&#30446;&#26631;&#20989;&#25968;&#30340;&#32422;&#26463;&#38797;&#28857;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;Bregman&#36317;&#31163;&#22788;&#29702;&#20219;&#24847;&#33539;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#22238;&#28335;&#32447;&#25628;&#32034;&#26041;&#26696;&#65292;&#20197;&#36873;&#25321;&#27493;&#38271;&#65292;&#32780;&#19981;&#38656;&#35201;&#20102;&#35299;&#24179;&#28369;&#31995;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#19968;&#38454;&#12289;&#20108;&#38454;&#21644;&#26356;&#39640;&#38454;&#25968;&#23398;&#35268;&#21017;&#26102;&#65292;&#32473;&#20986;&#20102;&#24050;&#30693;&#30340;&#20840;&#23616;&#36845;&#20195;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20048;&#35266;&#26799;&#24230;&#26041;&#27861;&#22312;&#35299;&#20915;&#20984;&#20985;&#38797;&#28857;&#38382;&#39064;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#20026;&#20102;&#20998;&#26512;&#20854;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#26368;&#36817;&#30340;&#19968;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#35266;&#28857;&#65292;&#23558;&#36825;&#20010;&#26041;&#27861;&#35299;&#37322;&#20026;&#23545;&#37051;&#36817;&#28857;&#27861;&#30340;&#36817;&#20284;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36981;&#24490;&#36825;&#19968;&#26041;&#27861;&#65292;&#24182;&#23558;&#20048;&#35266;&#30340;&#24605;&#24819;&#31934;&#28860;&#20026;&#19968;&#20010;&#24191;&#20041;&#20048;&#35266;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20048;&#35266;&#26799;&#24230;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#36890;&#29992;&#26694;&#26550;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#22797;&#21512;&#30446;&#26631;&#20989;&#25968;&#30340;&#32422;&#26463;&#38797;&#28857;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;Bregman&#36317;&#31163;&#22788;&#29702;&#20219;&#24847;&#33539;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22238;&#28335;&#32447;&#25628;&#32034;&#26041;&#26696;&#65292;&#20197;&#36873;&#25321;&#27493;&#38271;&#65292;&#32780;&#19981;&#38656;&#35201;&#20102;&#35299;&#24179;&#28369;&#31995;&#25968;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#38454;&#12289;&#20108;&#38454;&#21644;&#26356;&#39640;&#38454;&#25968;&#23398;&#35268;&#21017;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#24050;&#30693;&#30340;&#20840;&#23616;&#36845;&#20195;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24179;&#22343;&#36845;&#20195;&#22312;$O(1/N)$&#30340;&#36895;&#24230;&#19979;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimistic gradient method has seen increasing popularity for solving convex-concave saddle point problems. To analyze its iteration complexity, a recent work [arXiv:1906.01115] proposed an interesting perspective that interprets this method as an approximation to the proximal point method. In this paper, we follow this approach and distill the underlying idea of optimism to propose a generalized optimistic method, which includes the optimistic gradient method as a special case. Our general framework can handle constrained saddle point problems with composite objective functions and can work with arbitrary norms using Bregman distances. Moreover, we develop a backtracking line search scheme to select the step sizes without knowledge of the smoothness coefficients. We instantiate our method with first-, second- and higher-order oracles and give best-known global iteration complexity bounds. For our first-order method, we show that the averaged iterates converge at a rate of $O(1/N)$
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#32447;&#24615;&#21453;&#21521;&#20256;&#25773;&#65288;LinBP&#65289;&#22312;&#31070;&#32463;&#32593;&#32476;&#30456;&#20851;&#30340;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#36136;&#65292;&#22312;&#23545;&#25239;&#25915;&#20987;&#21644;&#27169;&#22411;&#35757;&#32451;&#20013;&#65292;LinBP&#22312;&#30456;&#21516;&#30340;&#36229;&#21442;&#25968;&#35774;&#32622;&#19979;&#33021;&#22815;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2112.11018</link><description>&lt;p&gt;
&#32447;&#24615;&#21453;&#21521;&#20256;&#25773;&#21450;&#20854;&#25910;&#25947;&#24615;&#30340;&#29702;&#35770;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Theoretical View of Linear Backpropagation and Its Convergence. (arXiv:2112.11018v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.11018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#32447;&#24615;&#21453;&#21521;&#20256;&#25773;&#65288;LinBP&#65289;&#22312;&#31070;&#32463;&#32593;&#32476;&#30456;&#20851;&#30340;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#36136;&#65292;&#22312;&#23545;&#25239;&#25915;&#20987;&#21644;&#27169;&#22411;&#35757;&#32451;&#20013;&#65292;LinBP&#22312;&#30456;&#21516;&#30340;&#36229;&#21442;&#25968;&#35774;&#32622;&#19979;&#33021;&#22815;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#30340;&#26799;&#24230;&#12290;&#23427;&#36890;&#24120;&#19982;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#25110;&#20854;&#21464;&#31181;&#19968;&#36215;&#20351;&#29992;&#65292;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#34987;&#35270;&#20026;&#20107;&#23454;&#19978;&#30340;&#36873;&#25321;&#65292;&#21253;&#25324;DNN&#35757;&#32451;&#21644;&#23545;&#25239;&#25915;&#20987;/&#38450;&#24481;&#12290;&#26368;&#36817;&#65292;Guo&#31561;&#20154;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LinBP&#30340;&#32447;&#24615;BP&#21464;&#20307;&#65292;&#29992;&#20110;&#29983;&#25104;&#26356;&#20855;&#20256;&#36882;&#24615;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#20197;&#36827;&#34892;&#40657;&#30418;&#25915;&#20987;&#12290;&#34429;&#28982;&#32463;&#39564;&#35777;&#26126;&#22312;&#40657;&#30418;&#25915;&#20987;&#20013;&#26377;&#25928;&#65292;&#20294;&#23545;&#36825;&#31181;&#26041;&#27861;&#30340;&#29702;&#35770;&#30740;&#31350;&#21644;&#25910;&#25947;&#24615;&#20998;&#26512;&#36739;&#20026;&#32570;&#20047;&#12290;&#26412;&#25991;&#20316;&#20026;Guo&#31561;&#20154;&#35770;&#25991;&#30340;&#34917;&#20805;&#21644;&#25193;&#23637;&#65292;&#25552;&#20379;&#20102;&#23545;LinBP&#22312;&#28041;&#21450;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#21253;&#25324;&#23545;&#25239;&#25915;&#20987;&#21644;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#30456;&#21516;&#30340;&#36229;&#21442;&#25968;&#35774;&#32622;&#19979;&#65292;&#19982;BP&#30456;&#27604;&#65292;LinBP&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#33021;&#22815;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backpropagation (BP) is widely used for calculating gradients in deep neural networks (DNNs). Applied often along with stochastic gradient descent (SGD) or its variants, BP is considered as a de-facto choice in a variety of machine learning tasks including DNN training and adversarial attack/defense. Recently, a linear variant of BP named LinBP was introduced for generating more transferable adversarial examples for performing black-box attacks, by Guo et al. Although it has been shown empirically effective in black-box attacks, theoretical studies and convergence analyses of such a method is lacking. This paper serves as a complement and somewhat an extension to Guo et al.'s paper, by providing theoretical analyses on LinBP in neural-network-involved learning tasks, including adversarial attack and model training. We demonstrate that, somewhat surprisingly, LinBP can lead to faster convergence in these tasks in the same hyper-parameter settings, compared to BP. We confirm our theoreti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32852;&#21512;&#20998;&#24067;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#22823;&#37327;&#25968;&#25454;&#28857;&#20013;&#20272;&#35745;&#20302;&#32500;&#12289;&#24402;&#19968;&#21270;&#21644;&#27491;&#30340;Radon-Nikodym&#23548;&#25968;&#27169;&#22411;&#65292;&#24182;&#22312;&#19981;&#21516;&#23398;&#20064;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2110.04829</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#32852;&#21512;&#20998;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive joint distribution learning. (arXiv:2110.04829v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.04829
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32852;&#21512;&#20998;&#24067;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#22823;&#37327;&#25968;&#25454;&#28857;&#20013;&#20272;&#35745;&#20302;&#32500;&#12289;&#24402;&#19968;&#21270;&#21644;&#27491;&#30340;Radon-Nikodym&#23548;&#25968;&#27169;&#22411;&#65292;&#24182;&#22312;&#19981;&#21516;&#23398;&#20064;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#23884;&#20837;&#24352;&#37327;&#31215;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#20013;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#23481;&#32435;&#19968;&#20010;&#20302;&#32500;&#12289;&#24402;&#19968;&#21270;&#21644;&#27491;&#30340;Radon-Nikodym&#23548;&#25968;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20174;&#22810;&#36798;&#25968;&#30334;&#19975;&#20010;&#25968;&#25454;&#28857;&#30340;&#26679;&#26412;&#22823;&#23567;&#20013;&#36827;&#34892;&#20272;&#35745;&#65292;&#20943;&#36731;&#20102;RKHS&#24314;&#27169;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#28982;&#20135;&#29983;&#20102;&#23450;&#20041;&#33391;&#22909;&#30340;&#24402;&#19968;&#21270;&#21644;&#27491;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290;&#23884;&#20837;&#35745;&#31639;&#36895;&#24230;&#24555;&#19988;&#36866;&#29992;&#20110;&#20174;&#39044;&#27979;&#21040;&#20998;&#31867;&#30340;&#21508;&#31181;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#24471;&#21040;&#20102;&#26377;&#30410;&#30340;&#25968;&#20540;&#32467;&#26524;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a new framework for embedding joint probability distributions in tensor product reproducing kernel Hilbert spaces (RKHS). Our framework accommodates a low-dimensional, normalized and positive model of a Radon-Nikodym derivative, which we estimate from sample sizes of up to several million data points, alleviating the inherent limitations of RKHS modeling. Well-defined normalized and positive conditional distributions are natural by-products to our approach. The embedding is fast to compute and accommodates learning problems ranging from prediction to classification. Our theoretical findings are supplemented by favorable numerical results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#30456;&#20851;&#32858;&#31867;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#27491;&#36127;&#37197;&#23545;&#19981;&#30456;&#20284;&#24230;&#65292;&#24182;&#30740;&#31350;&#20102;&#20351;&#29992;&#27492;&#26041;&#27861;&#36827;&#34892;&#26080;&#30417;&#30563;&#34920;&#24449;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2002.07756</link><description>&lt;p&gt;
&#20998;&#23618;&#30456;&#20851;&#32858;&#31867;&#21644;&#32500;&#25345;&#26641;&#32467;&#26500;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Correlation Clustering and Tree Preserving Embedding. (arXiv:2002.07756v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.07756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#30456;&#20851;&#32858;&#31867;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#27491;&#36127;&#37197;&#23545;&#19981;&#30456;&#20284;&#24230;&#65292;&#24182;&#30740;&#31350;&#20102;&#20351;&#29992;&#27492;&#26041;&#27861;&#36827;&#34892;&#26080;&#30417;&#30563;&#34920;&#24449;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#30456;&#20851;&#32858;&#31867;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#33879;&#21517;&#30340;&#30456;&#20851;&#32858;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#20135;&#29983;&#36866;&#29992;&#20110;&#27491;&#36127;&#37197;&#23545;&#19981;&#30456;&#20284;&#24230;&#30340;&#20998;&#23618;&#32858;&#31867;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#36825;&#31181;&#20998;&#23618;&#30456;&#20851;&#32858;&#31867;&#30340;&#26080;&#30417;&#30563;&#34920;&#24449;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#23558;&#30456;&#24212;&#30340;&#20998;&#23618;&#23884;&#20837;&#29992;&#20110;&#32500;&#25345;&#26641;&#32467;&#26500;&#23884;&#20837;&#21644;&#29305;&#24449;&#25552;&#21462;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#23567;&#26368;&#22823;&#36317;&#31163;&#24230;&#37327;&#25193;&#23637;&#21040;&#30456;&#20851;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#20316;&#20026;&#21478;&#19968;&#31181;&#34920;&#24449;&#23398;&#20064;&#33539;&#24335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a hierarchical correlation clustering method that extends the well-known correlation clustering to produce hierarchical clusters applicable to both positive and negative pairwise dissimilarities. Then, in the following, we study unsupervised representation learning with such hierarchical correlation clustering. For this purpose, we first investigate embedding the respective hierarchy to be used for tree-preserving embedding and feature extraction. Thereafter, we study the extension of minimax distance measures to correlation clustering, as another representation learning paradigm. Finally, we demonstrate the performance of our methods on several datasets.
&lt;/p&gt;</description></item></channel></rss>