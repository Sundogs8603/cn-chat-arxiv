<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#22522;&#20110;&#20010;&#20307;&#25968;&#25454;&#32780;&#19981;&#26159;&#25972;&#20307;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#25104;&#21151;&#36816;&#29992;&#20110;HuMob Challenge&#31454;&#36187;&#20013;&#12290;&#37319;&#29992;&#20102;&#29305;&#24449;&#35774;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;SVR&#65292;&#36890;&#36807;&#31163;&#32447;&#35780;&#20272;&#21644;&#29305;&#24449;&#36873;&#25321;&#21644;&#21442;&#25968;&#35843;&#25972;&#65292;&#20351;&#39044;&#27979;&#20934;&#30830;&#24615;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.12900</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#26041;&#27861;&#21450;&#20854;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
Personalized human mobility prediction for HuMob challenge. (arXiv:2310.12900v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12900
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#22522;&#20110;&#20010;&#20307;&#25968;&#25454;&#32780;&#19981;&#26159;&#25972;&#20307;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#25104;&#21151;&#36816;&#29992;&#20110;HuMob Challenge&#31454;&#36187;&#20013;&#12290;&#37319;&#29992;&#20102;&#29305;&#24449;&#35774;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;SVR&#65292;&#36890;&#36807;&#31163;&#32447;&#35780;&#20272;&#21644;&#29305;&#24449;&#36873;&#25321;&#21644;&#21442;&#25968;&#35843;&#25972;&#65292;&#20351;&#39044;&#27979;&#20934;&#30830;&#24615;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#21019;&#24314;&#25552;&#20132;&#32473;HuMob Challenge&#30340;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#30340;&#25968;&#25454;&#20998;&#26512;&#31454;&#36187;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#27169;&#22411;&#22522;&#20110;&#20010;&#20307;&#30340;&#25968;&#25454;&#26469;&#39044;&#27979;&#20854;&#36816;&#21160;&#36712;&#36857;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#25972;&#20307;&#36816;&#21160;&#30340;&#39044;&#27979;&#65292;&#22522;&#20110;&#20154;&#31867;&#36816;&#21160;&#23545;&#20110;&#27599;&#20010;&#20154;&#32780;&#35328;&#26159;&#29420;&#29305;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#29305;&#24449;&#65292;&#22914;&#26085;&#26399;&#21644;&#26102;&#38388;&#65292;&#27963;&#21160;&#26102;&#38388;&#65292;&#21608;&#20960;&#65292;&#19968;&#22825;&#20013;&#30340;&#26102;&#38388;&#21644;POI&#65288;&#20852;&#36259;&#28857;&#65289;&#35775;&#38382;&#39057;&#29575;&#31561;&#12290;&#20316;&#20026;&#39069;&#22806;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#31867;&#26469;&#34701;&#21512;&#20855;&#26377;&#30456;&#20284;&#34892;&#20026;&#27169;&#24335;&#30340;&#20854;&#20182;&#20010;&#20307;&#30340;&#36816;&#21160;&#12290;&#25105;&#20204;&#37319;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;SVR&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#31163;&#32447;&#35780;&#20272;&#36827;&#34892;&#20934;&#30830;&#24615;&#26816;&#39564;&#65292;&#24182;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#21644;&#21442;&#25968;&#35843;&#25972;&#12290;&#23613;&#31649;&#24635;&#20307;&#25968;&#25454;&#38598;&#21253;&#21547;10&#19975;&#21517;&#29992;&#25143;&#30340;&#36712;&#36857;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#20351;&#29992;&#20102;2&#19975;&#21517;&#30446;&#26631;&#29992;&#25143;&#30340;&#25968;&#25454;&#65292;&#24182;&#19981;&#38656;&#35201;&#20351;&#29992;&#20854;&#20182;8&#19975;&#21517;&#29992;&#25143;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explain the methodology used to create the data submitted to HuMob Challenge, a data analysis competition for human mobility prediction. We adopted a personalized model to predict the individual's movement trajectory from their data, instead of predicting from the overall movement, based on the hypothesis that human movement is unique to each person. We devised the features such as the date and time, activity time, days of the week, time of day, and frequency of visits to POI (Point of Interest). As additional features, we incorporated the movement of other individuals with similar behavior patterns through the employment of clustering. The machine learning model we adopted was the Support Vector Regression (SVR). We performed accuracy through offline assessment and carried out feature selection and parameter tuning. Although overall dataset provided consists of 100,000 users trajectory, our method use only 20,000 target users data, and do not need to use other 80,000 data. Despite 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#37327;&#23376;&#20108;&#20998;&#20851;&#32852;&#22120;&#31639;&#27861;&#30340;&#30450;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21327;&#35758;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#21644;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#29305;&#28857;&#65292;&#24182;&#24341;&#20837;&#20102;&#20302;&#35745;&#31639;&#24320;&#38144;&#12289;&#19981;&#38656;&#35201;&#22797;&#26434;&#21152;&#23494;&#25216;&#26415;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#12290;&#35770;&#25991;&#36890;&#36807;&#22797;&#26434;&#24615;&#21644;&#38544;&#31169;&#20998;&#26512;&#39564;&#35777;&#20102;&#21327;&#35758;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#21457;&#23637;&#20998;&#24067;&#24335;&#37327;&#23376;&#35745;&#31639;&#21644;&#38544;&#31169;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12893</link><description>&lt;p&gt;
&#30450;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#19982;&#37327;&#23376;&#20108;&#20998;&#20851;&#32852;&#22120;
&lt;/p&gt;
&lt;p&gt;
Blind quantum machine learning with quantum bipartite correlator. (arXiv:2310.12893v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12893
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#37327;&#23376;&#20108;&#20998;&#20851;&#32852;&#22120;&#31639;&#27861;&#30340;&#30450;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21327;&#35758;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#21644;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#29305;&#28857;&#65292;&#24182;&#24341;&#20837;&#20102;&#20302;&#35745;&#31639;&#24320;&#38144;&#12289;&#19981;&#38656;&#35201;&#22797;&#26434;&#21152;&#23494;&#25216;&#26415;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#12290;&#35770;&#25991;&#36890;&#36807;&#22797;&#26434;&#24615;&#21644;&#38544;&#31169;&#20998;&#26512;&#39564;&#35777;&#20102;&#21327;&#35758;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#21457;&#23637;&#20998;&#24067;&#24335;&#37327;&#23376;&#35745;&#31639;&#21644;&#38544;&#31169;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#37327;&#23376;&#35745;&#31639;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35745;&#31639;&#33539;&#24335;&#65292;&#21487;&#20197;&#25191;&#34892;&#21333;&#20010;&#37327;&#23376;&#35774;&#22791;&#26080;&#27861;&#36798;&#21040;&#30340;&#35745;&#31639;&#12290;&#22312;&#20998;&#24067;&#24335;&#37327;&#23376;&#35745;&#31639;&#20013;&#65292;&#38544;&#31169;&#23545;&#20110;&#22312;&#19981;&#21487;&#20449;&#35745;&#31639;&#33410;&#28857;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#32500;&#25252;&#26426;&#23494;&#24615;&#21644;&#20445;&#25252;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#37327;&#23376;&#20108;&#20998;&#20851;&#32852;&#22120;&#31639;&#27861;&#30340;&#26032;&#22411;&#30450;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21327;&#35758;&#12290;&#25105;&#20204;&#30340;&#21327;&#35758;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#38477;&#20302;&#20102;&#36890;&#20449;&#24320;&#38144;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20855;&#26377;&#20302;&#35745;&#31639;&#24320;&#38144;&#19988;&#19981;&#38656;&#35201;&#22797;&#26434;&#21152;&#23494;&#25216;&#26415;&#30340;&#24378;&#38887;&#31639;&#27861;&#29305;&#23450;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22797;&#26434;&#24615;&#21644;&#38544;&#31169;&#20998;&#26512;&#39564;&#35777;&#20102;&#25152;&#25552;&#35758;&#21327;&#35758;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#21457;&#23637;&#20998;&#24067;&#24335;&#37327;&#23376;&#35745;&#31639;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#20026;&#37327;&#23376;&#25216;&#26415;&#26102;&#20195;&#19979;&#30340;&#38544;&#31169;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed quantum computing is a promising computational paradigm for performing computations that are beyond the reach of individual quantum devices. Privacy in distributed quantum computing is critical for maintaining confidentiality and protecting the data in the presence of untrusted computing nodes. In this work, we introduce novel blind quantum machine learning protocols based on the quantum bipartite correlator algorithm. Our protocols have reduced communication overhead while preserving the privacy of data from untrusted parties. We introduce robust algorithm-specific privacy-preserving mechanisms with low computational overhead that do not require complex cryptographic techniques. We then validate the effectiveness of the proposed protocols through complexity and privacy analysis. Our findings pave the way for advancements in distributed quantum computing, opening up new possibilities for privacy-aware machine learning applications in the era of quantum technologies.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#29983;&#25104;&#27169;&#22411;&#24494;&#35843;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#20219;&#21153;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;GPU&#21152;&#36895;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26679;&#26412;&#29983;&#25104;&#24555;&#36895;&#36866;&#24212;&#21040;&#26426;&#22120;&#20154;&#20219;&#21153;&#35266;&#27979;&#20013;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21644;&#26426;&#22120;&#20154;&#29615;&#22659;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#36805;&#36895;&#24494;&#35843;&#20197;&#36866;&#24212;&#19982;&#35266;&#27979;&#35777;&#25454;&#30456;&#31526;&#30340;&#29983;&#25104;&#26679;&#26412;&#26469;&#23454;&#29616;&#65292;&#20351;&#29992;&#20132;&#21449;&#29109;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#24182;&#19988;&#22312;&#29289;&#20307;&#24418;&#29366;&#25512;&#29702;&#12289;&#36870;&#21521;&#36816;&#21160;&#23398;&#35745;&#31639;&#21644;&#28857;&#20113;&#34917;&#20840;&#31561;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12862</link><description>&lt;p&gt;
&#23558;&#29983;&#25104;&#27169;&#22411;&#24494;&#35843;&#20316;&#20026;&#26426;&#22120;&#20154;&#20219;&#21153;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Generative Models as an Inference Method for Robotic Tasks. (arXiv:2310.12862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#29983;&#25104;&#27169;&#22411;&#24494;&#35843;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#20219;&#21153;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;GPU&#21152;&#36895;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26679;&#26412;&#29983;&#25104;&#24555;&#36895;&#36866;&#24212;&#21040;&#26426;&#22120;&#20154;&#20219;&#21153;&#35266;&#27979;&#20013;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21644;&#26426;&#22120;&#20154;&#29615;&#22659;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#36805;&#36895;&#24494;&#35843;&#20197;&#36866;&#24212;&#19982;&#35266;&#27979;&#35777;&#25454;&#30456;&#31526;&#30340;&#29983;&#25104;&#26679;&#26412;&#26469;&#23454;&#29616;&#65292;&#20351;&#29992;&#20132;&#21449;&#29109;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#24182;&#19988;&#22312;&#29289;&#20307;&#24418;&#29366;&#25512;&#29702;&#12289;&#36870;&#21521;&#36816;&#21160;&#23398;&#35745;&#31639;&#21644;&#28857;&#20113;&#34917;&#20840;&#31561;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#36866;&#24212;&#30340;&#27169;&#22411;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#21892;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#25805;&#20316;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#26032;&#39062;&#21644;&#22810;&#21464;&#30340;&#24773;&#20917;&#12290;&#34429;&#28982;&#35832;&#22914;&#36125;&#21494;&#26031;&#25512;&#29702;&#31561;&#26041;&#27861;&#24050;&#25104;&#20026;&#23558;&#27169;&#22411;&#36866;&#24212;&#20110;&#35777;&#25454;&#30340;&#30740;&#31350;&#26694;&#26550;&#65292;&#20294;&#25105;&#20204;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36825;&#20123;&#36827;&#23637;&#24050;&#32463;&#26497;&#22823;&#22320;&#24433;&#21709;&#20102;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#35768;&#22810;&#26041;&#38754;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;GPU&#21152;&#36895;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26679;&#26412;&#29983;&#25104;&#24555;&#36895;&#36866;&#24212;&#21040;&#26426;&#22120;&#20154;&#20219;&#21153;&#35266;&#27979;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21644;&#26426;&#22120;&#20154;&#29615;&#22659;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#20351;&#29992;&#20132;&#21449;&#29109;&#26041;&#27861;&#65292;&#23558;&#27169;&#22411;&#36805;&#36895;&#24494;&#35843;&#20197;&#36866;&#24212;&#19982;&#35266;&#27979;&#35777;&#25454;&#30456;&#31526;&#30340;&#29983;&#25104;&#26679;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#24212;&#29992;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#29289;&#20307;&#24418;&#29366;&#25512;&#29702;&#12289;&#36870;&#21521;&#36816;&#21160;&#23398;&#35745;&#31639;&#21644;&#28857;&#20113;&#34917;&#20840;&#20013;&#30340;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptable models could greatly benefit robotic agents operating in the real world, allowing them to deal with novel and varying conditions. While approaches such as Bayesian inference are well-studied frameworks for adapting models to evidence, we build on recent advances in deep generative models which have greatly affected many areas of robotics. Harnessing modern GPU acceleration, we investigate how to quickly adapt the sample generation of neural network models to observations in robotic tasks. We propose a simple and general method that is applicable to various deep generative models and robotic environments. The key idea is to quickly fine-tune the model by fitting it to generated samples matching the observed evidence, using the cross-entropy method. We show that our method can be applied to both autoregressive models and variational autoencoders, and demonstrate its usability in object shape inference from grasping, inverse kinematics calculation, and point cloud completion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38750;&#21018;&#24615;&#25991;&#26412;&#32534;&#36753;&#36827;&#34892;&#38899;&#39057;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20445;&#25345;&#36755;&#20837;&#38899;&#39057;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.12858</link><description>&lt;p&gt;
&#38750;&#21018;&#24615;&#25991;&#26412;&#25552;&#31034;&#30340;&#38899;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Audio Editing with Non-Rigid Text Prompts. (arXiv:2310.12858v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38750;&#21018;&#24615;&#25991;&#26412;&#32534;&#36753;&#36827;&#34892;&#38899;&#39057;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20445;&#25345;&#36755;&#20837;&#38899;&#39057;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#38750;&#21018;&#24615;&#25991;&#26412;&#32534;&#36753;&#36827;&#34892;&#38899;&#39057;&#32534;&#36753;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#32534;&#36753;&#27969;&#31243;&#33021;&#22815;&#21019;&#24314;&#19982;&#36755;&#20837;&#38899;&#39057;&#20445;&#25345;&#19968;&#33268;&#30340;&#38899;&#39057;&#32534;&#36753;&#32467;&#26524;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#33021;&#22815;&#36827;&#34892;&#28155;&#21152;&#12289;&#39118;&#26684;&#36716;&#25442;&#21644;&#20462;&#22797;&#30340;&#25991;&#26412;&#25552;&#31034;&#12290;&#25105;&#20204;&#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#35777;&#26126;&#20102;&#36825;&#20123;&#32534;&#36753;&#33021;&#22815;&#20248;&#20110;&#26368;&#36817;&#21457;&#24067;&#30340;&#25991;&#26412;&#25552;&#31034;&#38899;&#39057;&#29983;&#25104;&#27169;&#22411;Audio-LDM&#30340;&#32467;&#26524;&#12290;&#23545;&#32467;&#26524;&#30340;&#23450;&#24615;&#26816;&#26597;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32473;&#20986;&#20102;&#26356;&#21152;&#20445;&#25345;&#36755;&#20837;&#38899;&#39057;&#21407;&#22987;&#36215;&#22987;&#21644;&#32467;&#26463;&#30340;&#32534;&#36753;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore audio-editing with non-rigid text edits. We show that the proposed editing pipeline is able to create audio edits that remain faithful to the input audio. We explore text prompts that perform addition, style transfer, and in-painting. We quantitatively and qualitatively show that the edits are able to obtain results which outperform Audio-LDM, a recently released text-prompted audio generation model. Qualitative inspection of the results points out that the edits given by our approach remain more faithful to the input audio in terms of keeping the original onsets and offsets of the audio events.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#29616;&#26377;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#29702;&#35299;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#26469;&#28304;&#21644;&#32622;&#20449;&#24230;&#65292;&#24182;&#21033;&#29992;&#25913;&#32534;&#21518;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#12289;&#37096;&#20998;&#20381;&#36182;&#22270;&#21644;&#20010;&#20307;&#26465;&#20214;&#26399;&#26395;&#22270;&#31561;&#26041;&#27861;&#26469;&#27979;&#37327;&#29305;&#24449;&#23545;&#39044;&#27979;&#20998;&#24067;&#30340;&#29109;&#21644;&#22522;&#20110;&#30495;&#23454;&#26631;&#31614;&#30340;&#23545;&#25968;&#20284;&#28982;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.12842</link><description>&lt;p&gt;
&#38024;&#23545;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#27169;&#22411;&#26080;&#20851;&#21464;&#37327;&#37325;&#35201;&#24615;&#65306;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Model-agnostic variable importance for predictive uncertainty: an entropy-based approach. (arXiv:2310.12842v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#29616;&#26377;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#29702;&#35299;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#20013;&#30340;&#39044;&#27979;&#26469;&#28304;&#21644;&#32622;&#20449;&#24230;&#65292;&#24182;&#21033;&#29992;&#25913;&#32534;&#21518;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#12289;&#37096;&#20998;&#20381;&#36182;&#22270;&#21644;&#20010;&#20307;&#26465;&#20214;&#26399;&#26395;&#22270;&#31561;&#26041;&#27861;&#26469;&#27979;&#37327;&#29305;&#24449;&#23545;&#39044;&#27979;&#20998;&#24067;&#30340;&#29109;&#21644;&#22522;&#20110;&#30495;&#23454;&#26631;&#31614;&#30340;&#23545;&#25968;&#20284;&#28982;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30456;&#20449;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#24517;&#39035;&#29702;&#35299;&#23548;&#33268;&#36825;&#20123;&#39044;&#27979;&#30340;&#22240;&#32032;&#12290;&#23545;&#20110;&#27010;&#29575;&#21644;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#19981;&#20165;&#38656;&#35201;&#29702;&#35299;&#39044;&#27979;&#26412;&#36523;&#30340;&#21407;&#22240;&#65292;&#36824;&#35201;&#29702;&#35299;&#27169;&#22411;&#23545;&#36825;&#20123;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#29616;&#26377;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#25193;&#23637;&#21040;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27169;&#22411;&#65292;&#24182;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#25193;&#23637;&#26469;&#29702;&#35299;&#27169;&#22411;&#39044;&#27979;&#20998;&#24067;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;&#29305;&#21035;&#26159;&#36890;&#36807;&#25913;&#32534;&#25490;&#21015;&#29305;&#24449;&#37325;&#35201;&#24615;&#12289;&#37096;&#20998;&#20381;&#36182;&#22270;&#21644;&#20010;&#20307;&#26465;&#20214;&#26399;&#26395;&#22270;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#33719;&#24471;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#26469;&#34913;&#37327;&#29305;&#24449;&#23545;&#39044;&#27979;&#20998;&#24067;&#30340;&#29109;&#21644;&#22522;&#20110;&#35813;&#20998;&#24067;&#30340;&#30495;&#23454;&#26631;&#31614;&#30340;&#23545;&#25968;&#20284;&#28982;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to trust the predictions of a machine learning algorithm, it is necessary to understand the factors that contribute to those predictions. In the case of probabilistic and uncertainty-aware models, it is necessary to understand not only the reasons for the predictions themselves, but also the model's level of confidence in those predictions. In this paper, we show how existing methods in explainability can be extended to uncertainty-aware models and how such extensions can be used to understand the sources of uncertainty in a model's predictive distribution. In particular, by adapting permutation feature importance, partial dependence plots, and individual conditional expectation plots, we demonstrate that novel insights into model behaviour may be obtained and that these methods can be used to measure the impact of features on both the entropy of the predictive distribution and the log-likelihood of the ground truth labels under that distribution. With experiments using both s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#29420;&#31435;&#30340;&#39564;&#35777;&#22120;&#26469;&#26816;&#27979;&#27169;&#22411;&#36755;&#20986;&#21644;&#30693;&#35782;&#30340;&#38169;&#35823;&#65292;&#24182;&#36890;&#36807;&#26816;&#32034;&#26032;&#30693;&#35782;&#25110;&#20462;&#27491;&#25688;&#35201;&#26469;&#32416;&#27491;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2310.12836</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Augmented Language Model Verification. (arXiv:2310.12836v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12836
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#29420;&#31435;&#30340;&#39564;&#35777;&#22120;&#26469;&#26816;&#27979;&#27169;&#22411;&#36755;&#20986;&#21644;&#30693;&#35782;&#30340;&#38169;&#35823;&#65292;&#24182;&#36890;&#36807;&#26816;&#32034;&#26032;&#30693;&#35782;&#25110;&#20462;&#27491;&#25688;&#35201;&#26469;&#32416;&#27491;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#22791;&#20102;&#21442;&#25968;&#20013;&#20869;&#37096;&#21270;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#30693;&#35782;&#21487;&#33021;&#26159;&#19981;&#20934;&#30830;&#12289;&#19981;&#23436;&#25972;&#21644;&#36807;&#26102;&#30340;&#65292;&#22240;&#27492;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#20250;&#23545;&#32473;&#23450;&#26597;&#35810;&#29983;&#25104;&#20986;&#20107;&#23454;&#19978;&#19981;&#27491;&#30830;&#30340;&#22238;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#20174;&#22806;&#37096;&#30693;&#35782;&#28304;&#26816;&#32034;&#21040;&#30340;&#30693;&#35782;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26041;&#27861;&#32463;&#24120;&#22240;&#20026;&#20004;&#20010;&#21407;&#22240;&#32780;&#26174;&#31034;&#20986;&#27425;&#20248;&#30340;&#25991;&#26412;&#29983;&#25104;&#24615;&#33021;&#65306;1&#65289;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#26816;&#32034;&#21040;&#19982;&#32473;&#23450;&#26597;&#35810;&#30456;&#20851;&#30340;&#30693;&#35782;&#65307;2&#65289;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#22312;&#29983;&#25104;&#25991;&#26412;&#20013;&#24544;&#23454;&#22320;&#21453;&#26144;&#26816;&#32034;&#21040;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#19968;&#20010;&#21333;&#29420;&#30340;&#39564;&#35777;&#22120;&#26469;&#39564;&#35777;&#30693;&#35782;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#21644;&#30693;&#35782;&#65292;&#36825;&#20010;&#39564;&#35777;&#22120;&#26159;&#19968;&#20010;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#26041;&#24335;&#35757;&#32451;&#26469;&#26816;&#27979;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#38169;&#35823;&#12290;&#28982;&#21518;&#65292;&#24403;&#39564;&#35777;&#22120;&#26816;&#27979;&#21040;&#38169;&#35823;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#26816;&#32034;&#26032;&#30340;&#30693;&#35782;&#25110;&#33719;&#21462;&#26356;&#25913;&#25688;&#35201;&#26469;&#36827;&#34892;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent Language Models (LMs) have shown impressive capabilities in generating texts with the knowledge internalized in parameters. Yet, LMs often generate the factually incorrect responses to the given queries, since their knowledge may be inaccurate, incomplete, and outdated. To address this problem, previous works propose to augment LMs with the knowledge retrieved from an external knowledge source. However, such approaches often show suboptimal text generation performance due to two reasons: 1) the model may fail to retrieve the knowledge relevant to the given query, or 2) the model may not faithfully reflect the retrieved knowledge in the generated text. To overcome these, we propose to verify the output and the knowledge of the knowledge-augmented LMs with a separate verifier, which is a small LM that is trained to detect those two types of errors through instruction-finetuning. Then, when the verifier recognizes an error, we can rectify it by either retrieving new knowledge or ge
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;AgentTuning&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#25552;&#21319;LLMs&#30340;&#20195;&#29702;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#36890;&#29992;&#33021;&#21147;&#12290;&#36890;&#36807;&#26500;&#24314;AgentInstruct&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#28151;&#21512;&#35757;&#32451;&#26041;&#27861;&#65292;&#20316;&#32773;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#25552;&#39640;LLMs&#20195;&#29702;&#33021;&#21147;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.12823</link><description>&lt;p&gt;
AgentTuning: &#20026;LLMs&#23454;&#29616;&#36890;&#29992;&#20195;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
AgentTuning: Enabling Generalized Agent Abilities for LLMs. (arXiv:2310.12823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;AgentTuning&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#25552;&#21319;LLMs&#30340;&#20195;&#29702;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#36890;&#29992;&#33021;&#21147;&#12290;&#36890;&#36807;&#26500;&#24314;AgentInstruct&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#28151;&#21512;&#35757;&#32451;&#26041;&#27861;&#65292;&#20316;&#32773;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#25552;&#39640;LLMs&#20195;&#29702;&#33021;&#21147;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;LLMs&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#23427;&#20204;&#20316;&#20026;&#20195;&#29702;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24212;&#23545;&#22797;&#26434;&#20219;&#21153;&#26102;&#65292;&#23427;&#20204;&#36828;&#19981;&#21450;ChatGPT&#21644;GPT-4&#31561;&#21830;&#19994;&#27169;&#22411;&#12290;&#36825;&#20123;&#20195;&#29702;&#20219;&#21153;&#23558;LLMs&#20316;&#20026;&#36127;&#36131;&#35268;&#21010;&#12289;&#35760;&#24518;&#21644;&#24037;&#20855;&#21033;&#29992;&#30340;&#20013;&#22830;&#25511;&#21046;&#22120;&#65292;&#38656;&#35201;&#32454;&#31890;&#24230;&#30340;&#25552;&#31034;&#26041;&#27861;&#21644;&#24378;&#22823;&#30340;LLMs&#25165;&#33021;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25552;&#31034;&#26041;&#27861;&#26469;&#23436;&#25104;&#29305;&#23450;&#30340;&#20195;&#29702;&#20219;&#21153;&#65292;&#20294;&#32570;&#20047;&#30740;&#31350;&#19987;&#27880;&#20110;&#25552;&#39640;LLMs&#33258;&#36523;&#30340;&#20195;&#29702;&#33021;&#21147;&#32780;&#19981;&#25439;&#23475;&#20854;&#36890;&#29992;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AgentTuning&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21319;LLMs&#30340;&#20195;&#29702;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#36890;&#29992;&#30340;LLM&#33021;&#21147;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;AgentInstruct&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#39640;&#36136;&#37327;&#30340;&#20132;&#20114;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open large language models (LLMs) with great performance in various tasks have significantly advanced the development of LLMs. However, they are far inferior to commercial models such as ChatGPT and GPT-4 when acting as agents to tackle complex tasks in the real world. These agent tasks employ LLMs as the central controller responsible for planning, memorization, and tool utilization, necessitating both fine-grained prompting methods and robust LLMs to achieve satisfactory performance. Though many prompting methods have been proposed to complete particular agent tasks, there is lack of research focusing on improving the agent capabilities of LLMs themselves without compromising their general abilities. In this work, we present AgentTuning, a simple and general method to enhance the agent abilities of LLMs while maintaining their general LLM capabilities. We construct AgentInstruct, a lightweight instruction-tuning dataset containing high-quality interaction trajectories. We employ a hy
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#23398;&#20248;&#21270;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#38598;&#20307;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#20026;&#39640;&#39118;&#38505;&#20915;&#31574;&#22330;&#26223;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#35299;&#37322;&#24037;&#20855;&#12290;&#19982;&#20256;&#32479;&#30340;&#21333;&#23454;&#20363;&#35299;&#37322;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#38024;&#23545;&#19968;&#32452;&#23454;&#20363;&#25552;&#20379;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#25200;&#21160;&#30340;&#24635;&#25104;&#26412;&#26469;&#25552;&#20379;&#26368;&#20248;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2310.12822</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#23398;&#20248;&#21270;&#22312;&#22522;&#20110;&#35780;&#20998;&#30340;&#20998;&#31867;&#20013;&#29983;&#25104;&#38598;&#20307;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Generating collective counterfactual explanations in score-based classification via mathematical optimization. (arXiv:2310.12822v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12822
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#23398;&#20248;&#21270;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#38598;&#20307;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#20026;&#39640;&#39118;&#38505;&#20915;&#31574;&#22330;&#26223;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#35299;&#37322;&#24037;&#20855;&#12290;&#19982;&#20256;&#32479;&#30340;&#21333;&#23454;&#20363;&#35299;&#37322;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#38024;&#23545;&#19968;&#32452;&#23454;&#20363;&#25552;&#20379;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#25200;&#21160;&#30340;&#24635;&#25104;&#26412;&#26469;&#25552;&#20379;&#26368;&#20248;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#22330;&#26223;&#20013;&#30340;&#22686;&#21152;&#20351;&#29992;&#65292;&#20102;&#35299;&#27169;&#22411;&#22914;&#20309;&#20570;&#20986;&#20915;&#31574;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#30417;&#30563;&#20998;&#31867;&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#21453;&#20107;&#23454;&#20998;&#26512;&#33719;&#24471;&#35299;&#37322;&#65306;&#19968;&#20010;&#23454;&#20363;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#25351;&#31034;&#24212;&#35813;&#22914;&#20309;&#26368;&#23567;&#31243;&#24230;&#22320;&#20462;&#25913;&#36825;&#20010;&#23454;&#20363;&#65292;&#20351;&#24471;&#34987;&#25200;&#21160;&#30340;&#23454;&#20363;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#34987;&#20998;&#31867;&#21040;&#25152;&#26399;&#26395;&#30340;&#31867;&#21035;&#20013;&#12290;&#22823;&#37096;&#20998;&#21453;&#20107;&#23454;&#20998;&#26512;&#25991;&#29486;&#38598;&#20013;&#22312;&#21333;&#23454;&#20363;&#21333;&#21453;&#20107;&#23454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#38024;&#23545;&#19968;&#20010;&#21333;&#19968;&#23454;&#20363;&#26469;&#25552;&#20379;&#19968;&#20010;&#21333;&#19968;&#35299;&#37322;&#12290;&#20174;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#20171;&#32461;&#20102;&#25152;&#35859;&#30340;&#38598;&#20307;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#36890;&#36807;&#26032;&#39062;&#30340;&#25968;&#23398;&#20248;&#21270;&#27169;&#22411;&#65292;&#25105;&#20204;&#20026;&#24863;&#20852;&#36259;&#30340;&#19968;&#32452;&#23454;&#20363;&#25552;&#20379;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#20197;&#20351;&#25200;&#21160;&#30340;&#24635;&#25104;&#26412;&#26368;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the increasing use of Machine Learning models in high stakes decision making settings, it has become increasingly important to have tools to understand how models arrive at decisions. Assuming a trained Supervised Classification model, explanations can be obtained via counterfactual analysis: a counterfactual explanation of an instance indicates how this instance should be minimally modified so that the perturbed instance is classified in the desired class by the Machine Learning classification model. Most of the Counterfactual Analysis literature focuses on the single-instance single-counterfactual setting, in which the analysis is done for one single instance to provide one single explanation. Taking a stakeholder's perspective, in this paper we introduce the so-called collective counterfactual explanations. By means of novel Mathematical Optimization models, we provide a counterfactual explanation for each instance in a group of interest, so that the total cost of the perturb
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#36827;&#34892;&#39640;&#25928;&#35268;&#21010;&#65292;&#24182;&#20445;&#35777;&#20102;&#23436;&#25972;&#24615;&#12290;&#36890;&#36807;&#22312;&#39640;&#32423;&#25628;&#32034;&#20013;&#28155;&#21152;&#20302;&#32423;&#21160;&#20316;&#65292;&#35813;&#26041;&#27861;&#26082;&#20855;&#26377;&#39640;&#32423;&#25628;&#32034;&#30340;&#23454;&#38469;&#25928;&#29575;&#65292;&#21448;&#20855;&#26377;&#20302;&#32423;&#25628;&#32034;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12819</link><description>&lt;p&gt;
&#24102;&#26377;&#23436;&#25972;&#24615;&#20445;&#35777;&#30340;&#39640;&#25928;&#35268;&#21010;&#30340;&#28151;&#21512;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Hybrid Search for Efficient Planning with Completeness Guarantees. (arXiv:2310.12819v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#36827;&#34892;&#39640;&#25928;&#35268;&#21010;&#65292;&#24182;&#20445;&#35777;&#20102;&#23436;&#25972;&#24615;&#12290;&#36890;&#36807;&#22312;&#39640;&#32423;&#25628;&#32034;&#20013;&#28155;&#21152;&#20302;&#32423;&#21160;&#20316;&#65292;&#35813;&#26041;&#27861;&#26082;&#20855;&#26377;&#39640;&#32423;&#25628;&#32034;&#30340;&#23454;&#38469;&#25928;&#29575;&#65292;&#21448;&#20855;&#26377;&#20302;&#32423;&#25628;&#32034;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;&#35268;&#21010;&#38382;&#39064;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#22522;&#20110;&#23398;&#20064;&#30340;&#23376;&#30446;&#26631;&#25628;&#32034;&#26041;&#27861;&#22312;&#22788;&#29702;&#36825;&#20123;&#38382;&#39064;&#19978;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#32570;&#20047;&#23436;&#25972;&#24615;&#20445;&#35777;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#21487;&#33021;&#26080;&#27861;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#20351;&#23384;&#22312;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#23376;&#30446;&#26631;&#25628;&#32034;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#23436;&#25972;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#39640;&#32423;&#25628;&#32034;&#20013;&#28155;&#21152;&#20302;&#32423;&#21160;&#20316;&#26469;&#25191;&#34892;&#22810;&#23618;&#27425;&#65288;&#28151;&#21512;&#65289;&#25628;&#32034;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#23436;&#25972;&#30340;&#23376;&#30446;&#26631;&#25628;&#32034;&#12290;&#36825;&#20010;&#35299;&#20915;&#26041;&#26696;&#23454;&#29616;&#20102;&#39640;&#32423;&#25628;&#32034;&#30340;&#23454;&#38469;&#25928;&#29575;&#21644;&#20302;&#32423;&#25628;&#32034;&#30340;&#23436;&#25972;&#24615;&#30340;&#26368;&#20339;&#32467;&#21512;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#25628;&#32034;&#26041;&#27861;&#24212;&#29992;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#23376;&#30446;&#26631;&#25628;&#32034;&#31639;&#27861;&#65292;&#24182;&#35780;&#20272;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#35757;&#32451;&#30340;&#31639;&#27861;&#22312;&#22797;&#26434;&#30340;&#35268;&#21010;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#23436;&#25972;&#23376;&#30446;&#26631;&#25628;&#32034;&#19981;&#20165;&#20445;&#35777;&#20102;&#23436;&#25972;&#24615;&#65292;&#36824;&#21487;&#20197;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving complex planning problems has been a long-standing challenge in computer science. Learning-based subgoal search methods have shown promise in tackling these problems, but they often suffer from a lack of completeness guarantees, meaning that they may fail to find a solution even if one exists. In this paper, we propose an efficient approach to augment a subgoal search method to achieve completeness in discrete action spaces. Specifically, we augment the high-level search with low-level actions to execute a multi-level (hybrid) search, which we call complete subgoal search. This solution achieves the best of both worlds: the practical efficiency of high-level search and the completeness of low-level search. We apply the proposed search method to a recently proposed subgoal search algorithm and evaluate the algorithm trained on offline data on complex planning problems. We demonstrate that our complete subgoal search not only guarantees completeness but can even improve performan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21319;&#21442;&#25968;&#20849;&#20139;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#31616;&#21333;&#25216;&#26415;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#23454;&#29616;&#23436;&#20840;&#25110;&#37096;&#20998;&#20849;&#20139;&#30340;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#21442;&#25968;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.12818</link><description>&lt;p&gt;
&#25552;&#21319;&#25512;&#29702;&#25928;&#29575;&#65306;&#37322;&#25918;&#21442;&#25968;&#20849;&#20139;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Boosting Inference Efficiency: Unleashing the Power of Parameter-Shared Pre-trained Language Models. (arXiv:2310.12818v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21319;&#21442;&#25968;&#20849;&#20139;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#31616;&#21333;&#25216;&#26415;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#23454;&#29616;&#23436;&#20840;&#25110;&#37096;&#20998;&#20849;&#20139;&#30340;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#21442;&#25968;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#20849;&#20139;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#22312;&#36164;&#28304;&#26377;&#38480;&#29615;&#22659;&#20013;&#30340;&#25104;&#21151;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#19981;&#26174;&#33879;&#38477;&#20302;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#27169;&#22411;&#23384;&#20648;&#21644;&#20869;&#23384;&#25104;&#26412;&#30340;&#22823;&#24133;&#38477;&#20302;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#21442;&#25968;&#20849;&#20139;&#19981;&#33021;&#20943;&#36731;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#36825;&#20351;&#24471;&#22312;&#20855;&#26377;&#20005;&#26684;&#26102;&#24310;&#35201;&#27714;&#25110;&#35745;&#31639;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#23454;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#22522;&#20110;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#21442;&#25968;&#20849;&#20139;&#30340;PLMs&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#21487;&#20197;&#23454;&#29616;&#23436;&#20840;&#25110;&#37096;&#20998;&#20849;&#20139;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22823;&#30340;&#25512;&#29702;&#21152;&#36895;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#33258;&#22238;&#24402;&#21644;&#33258;&#32534;&#30721;PLMs&#37117;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#20026;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#21442;&#25968;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-shared pre-trained language models (PLMs) have emerged as a successful approach in resource-constrained environments, enabling substantial reductions in model storage and memory costs without significant performance compromise. However, it is important to note that parameter sharing does not alleviate computational burdens associated with inference, thus impeding its practicality in situations characterized by limited stringent latency requirements or computational resources. Building upon neural ordinary differential equations (ODEs), we introduce a straightforward technique to enhance the inference efficiency of parameter-shared PLMs. Additionally, we propose a simple pre-training technique that leads to fully or partially shared models capable of achieving even greater inference acceleration. The experimental results demonstrate the effectiveness of our methods on both autoregressive and autoencoding PLMs, providing novel insights into more efficient utilization of paramet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#32423;&#30417;&#30563;&#30340;2D-3D&#20132;&#38169;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#24369;&#30417;&#30563;&#28857;&#20113;&#20998;&#21106;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20004;&#20010;&#32534;&#30721;&#22120;&#35745;&#31639;2D&#21644;3D&#25968;&#25454;&#30340;&#33258;&#27880;&#24847;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#20132;&#26367;&#20999;&#25442;&#26597;&#35810;&#21644;&#38190;&#20540;&#23545;&#30340;&#35282;&#33394;&#65292;&#23454;&#29616;&#20102;2D&#21644;3D&#29305;&#24449;&#30340;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.12817</link><description>&lt;p&gt;
&#22522;&#20110;&#22330;&#26223;&#32423;&#30417;&#30563;&#30340;&#28857;&#20113;&#20998;&#21106;&#30340;2D-3D&#20132;&#38169;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level Supervision. (arXiv:2310.12817v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#32423;&#30417;&#30563;&#30340;2D-3D&#20132;&#38169;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#24369;&#30417;&#30563;&#28857;&#20113;&#20998;&#21106;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#20004;&#20010;&#32534;&#30721;&#22120;&#35745;&#31639;2D&#21644;3D&#25968;&#25454;&#30340;&#33258;&#27880;&#24847;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#20132;&#26367;&#20999;&#25442;&#26597;&#35810;&#21644;&#38190;&#20540;&#23545;&#30340;&#35282;&#33394;&#65292;&#23454;&#29616;&#20102;2D&#21644;3D&#29305;&#24449;&#30340;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#20132;&#38169;Transformer&#27169;&#22411;&#65288;MIT&#65289;&#65292;&#29992;&#20110;&#32771;&#34385;2D&#21644;3D&#25968;&#25454;&#36827;&#34892;&#24369;&#30417;&#30563;&#28857;&#20113;&#20998;&#21106;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;2D&#21644;3D&#29305;&#24449;&#22312;&#28857;&#20113;&#20998;&#21106;&#20013;&#20114;&#34917;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;2D&#27880;&#37322;&#26469;&#23454;&#29616;2D-3D&#20449;&#24687;&#34701;&#21512;&#12290;&#37492;&#20110;&#28857;&#20113;&#30340;&#39640;&#27880;&#37322;&#25104;&#26412;&#65292;&#22522;&#20110;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26377;&#25928;2D&#21644;3D&#29305;&#24449;&#34701;&#21512;&#38656;&#27714;&#38750;&#24120;&#36843;&#20999;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#20004;&#20010;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#35299;&#30721;&#22120;&#30340;Transformer&#27169;&#22411;&#65292;&#20165;&#20351;&#29992;&#22330;&#26223;&#32423;&#31867;&#26631;&#31614;&#36827;&#34892;&#24369;&#30417;&#30563;&#28857;&#20113;&#20998;&#21106;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20004;&#20010;&#32534;&#30721;&#22120;&#20998;&#21035;&#35745;&#31639;3D&#28857;&#20113;&#21644;2D&#22810;&#35270;&#22270;&#22270;&#20687;&#30340;&#33258;&#27880;&#24847;&#29305;&#24449;&#12290;&#35299;&#30721;&#22120;&#23454;&#29616;&#20132;&#38169;&#30340;2D-3D&#20132;&#21449;&#27880;&#24847;&#21147;&#65292;&#24182;&#36827;&#34892;&#38544;&#24335;2D&#21644;3D&#29305;&#24449;&#34701;&#21512;&#12290;&#25105;&#20204;&#22312;&#35299;&#30721;&#22120;&#23618;&#20013;&#20132;&#26367;&#20999;&#25442;&#26597;&#35810;&#21644;&#38190;&#20540;&#23545;&#30340;&#35282;&#33394;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;2D&#21644;3D&#29305;&#24449;&#26159;&#20114;&#34917;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a Multimodal Interlaced Transformer (MIT) that jointly considers 2D and 3D data for weakly supervised point cloud segmentation. Research studies have shown that 2D and 3D features are complementary for point cloud segmentation. However, existing methods require extra 2D annotations to achieve 2D-3D information fusion. Considering the high annotation cost of point clouds, effective 2D and 3D feature fusion based on weakly supervised learning is in great demand. To this end, we propose a transformer model with two encoders and one decoder for weakly supervised point cloud segmentation using only scene-level class tags. Specifically, the two encoders compute the self-attended features for 3D point clouds and 2D multi-view images, respectively. The decoder implements interlaced 2D-3D cross-attention and carries out implicit 2D and 3D feature fusion. We alternately switch the roles of queries and key-value pairs in the decoder layers. It turns out that the 2D and 3D features are 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.12815</link><description>&lt;p&gt;
LLM-&#38598;&#25104;&#24212;&#29992;&#20013;&#30340;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Prompt Injection Attacks and Defenses in LLM-Integrated Applications. (arXiv:2310.12815v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#31995;&#32479;&#21270;&#38450;&#24481;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20316;&#21508;&#31181;&#31216;&#20026;LLM-&#38598;&#25104;&#24212;&#29992;&#30340;&#23454;&#38469;&#24212;&#29992;&#31243;&#24207;&#30340;&#21518;&#31471;&#12290;&#26368;&#36817;&#30340;&#22810;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;LLM-&#38598;&#25104;&#24212;&#29992;&#23481;&#26131;&#21463;&#21040;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#23558;&#24694;&#24847;&#25351;&#20196;/&#25968;&#25454;&#27880;&#20837;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#30340;&#36755;&#20837;&#20013;&#65292;&#20197;&#36798;&#21040;&#25915;&#20987;&#32773;&#30340;&#39044;&#26399;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#26696;&#20363;&#30740;&#31350;&#65292;&#32570;&#20047;&#23545;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21450;&#20854;&#38450;&#24481;&#30340;&#31995;&#32479;&#29702;&#35299;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#24418;&#24335;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#65292;&#24182;&#23558;&#30740;&#31350;&#35770;&#25991;&#21644;&#21338;&#23458;&#25991;&#31456;&#20013;&#35752;&#35770;&#30340;&#29616;&#26377;&#25915;&#20987;&#35270;&#20026;&#25105;&#20204;&#26694;&#26550;&#30340;&#29305;&#20363;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#29616;&#26377;&#25915;&#20987;&#35774;&#35745;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#38450;&#24481;&#30340;&#26694;&#26550;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#38450;&#21644;&#32531;&#35299;&#36825;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly deployed as the backend for a variety of real-world applications called LLM-Integrated Applications. Multiple recent works showed that LLM-Integrated Applications are vulnerable to prompt injection attacks, in which an attacker injects malicious instruction/data into the input of those applications such that they produce results as the attacker desires. However, existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a general framework to formalize prompt injection attacks. Existing attacks, which are discussed in research papers and blog posts, are special cases in our framework. Our framework enables us to design a new attack by combining existing attacks. Moreover, we also propose a framework to systematize defenses against prompt injection attacks. Using our frameworks, we con
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#35268;&#27169;&#23618;&#27425;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31232;&#30095;&#25439;&#22833;&#20989;&#25968;&#30452;&#25509;&#20248;&#21270;&#23618;&#27425;&#20135;&#21697;&#21644;/&#25110;&#26102;&#38388;&#32467;&#26500;&#65292;&#20174;&#32780;&#20026;&#25968;&#30334;&#19975;&#20010;&#26102;&#38388;&#24207;&#21015;&#25552;&#20379;&#19968;&#33268;&#30340;&#39044;&#27979;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;M5&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;10%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.12809</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#23618;&#27425;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Forecasting at Scale. (arXiv:2310.12809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12809
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#35268;&#27169;&#23618;&#27425;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31232;&#30095;&#25439;&#22833;&#20989;&#25968;&#30452;&#25509;&#20248;&#21270;&#23618;&#27425;&#20135;&#21697;&#21644;/&#25110;&#26102;&#38388;&#32467;&#26500;&#65292;&#20174;&#32780;&#20026;&#25968;&#30334;&#19975;&#20010;&#26102;&#38388;&#24207;&#21015;&#25552;&#20379;&#19968;&#33268;&#30340;&#39044;&#27979;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;M5&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;10%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26102;&#38388;&#24207;&#21015;&#30340;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;&#29616;&#26377;&#30340;&#23618;&#27425;&#39044;&#27979;&#25216;&#26415;&#30340;&#25193;&#23637;&#24615;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#31232;&#30095;&#25439;&#22833;&#20989;&#25968;&#26469;&#23398;&#20064;&#21315;&#19975;&#20010;&#26102;&#38388;&#24207;&#21015;&#30340;&#19968;&#33268;&#39044;&#27979;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#30452;&#25509;&#20248;&#21270;&#23618;&#27425;&#20135;&#21697;&#21644;/&#25110;&#26102;&#38388;&#32467;&#26500;&#12290;&#25105;&#20204;&#31232;&#30095;&#23618;&#27425;&#25439;&#22833;&#20989;&#25968;&#30340;&#20248;&#28857;&#26159;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#23454;&#36341;&#32773;&#33021;&#22815;&#20135;&#29983;&#19982;&#20219;&#20309;&#36873;&#25321;&#30340;&#27178;&#21521;&#25110;&#26102;&#38388;&#23618;&#27425;&#19968;&#33268;&#30340;&#24213;&#23618;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#28040;&#38500;&#20256;&#32479;&#23618;&#27425;&#39044;&#27979;&#25216;&#26415;&#20013;&#38656;&#35201;&#30340;&#21518;&#22788;&#29702;&#27493;&#39588;&#65292;&#20943;&#23569;&#20102;&#39044;&#27979;&#27969;&#31243;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#20844;&#24320;&#30340;M5&#25968;&#25454;&#38598;&#19978;&#65292;&#19982;&#22522;&#20934;&#25439;&#22833;&#20989;&#25968;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31232;&#30095;&#23618;&#27425;&#25439;&#22833;&#20989;&#25968;&#24615;&#33021;&#25552;&#39640;&#20102;10%&#65288;RMSE&#65289;&#12290;&#25105;&#20204;&#23558;&#31232;&#30095;&#23618;&#27425;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#22312;bol&#36825;&#20010;&#22823;&#22411;&#27431;&#27954;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#29616;&#23384;&#30340;&#39044;&#27979;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing hierarchical forecasting techniques scale poorly when the number of time series increases. We propose to learn a coherent forecast for millions of time series with a single bottom-level forecast model by using a sparse loss function that directly optimizes the hierarchical product and/or temporal structure. The benefit of our sparse hierarchical loss function is that it provides practitioners a method of producing bottom-level forecasts that are coherent to any chosen cross-sectional or temporal hierarchy. In addition, removing the need for a post-processing step as required in traditional hierarchical forecasting techniques reduces the computational cost of the prediction phase in the forecasting pipeline. On the public M5 dataset, our sparse hierarchical loss function performs up to 10% (RMSE) better compared to the baseline loss function. We implement our sparse hierarchical loss function within an existing forecasting model at bol, a large European e-commerce platform, res
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#26799;&#24230;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#33021;&#22815;&#20943;&#23569;&#26799;&#24230;&#19981;&#21305;&#37197;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#21512;&#24182;&#30340;&#24615;&#33021;&#24182;&#23545;&#36229;&#21442;&#25968;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12808</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#26799;&#24230;&#21305;&#37197;&#30340;&#27169;&#22411;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
Model Merging by Uncertainty-Based Gradient Matching. (arXiv:2310.12808v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#26799;&#24230;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21512;&#24182;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#33021;&#22815;&#20943;&#23569;&#26799;&#24230;&#19981;&#21305;&#37197;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#21512;&#24182;&#30340;&#24615;&#33021;&#24182;&#23545;&#36229;&#21442;&#25968;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#21442;&#25968;&#30340;&#21152;&#26435;&#24179;&#22343;&#26469;&#21512;&#24182;&#65292;&#20294;&#20026;&#20160;&#20040;&#20250;&#36215;&#20316;&#29992;&#65292;&#20160;&#20040;&#24773;&#20917;&#19979;&#20250;&#22833;&#36133;&#65311;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#21152;&#26435;&#24179;&#22343;&#30340;&#19981;&#20934;&#30830;&#24615;&#19982;&#26799;&#24230;&#19981;&#21305;&#37197;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#20943;&#23569;&#19981;&#21305;&#37197;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#36825;&#31181;&#32852;&#31995;&#36824;&#25581;&#31034;&#20102;&#20854;&#20182;&#26041;&#26696;&#65288;&#22914;&#24179;&#22343;&#20540;&#12289;&#20219;&#21153;&#31639;&#26415;&#21644;Fisher&#21152;&#26435;&#24179;&#22343;&#65289;&#20013;&#30340;&#38544;&#21547;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#26041;&#38754;&#37117;&#22312;&#24615;&#33021;&#21644;&#36229;&#21442;&#25968;&#40065;&#26834;&#24615;&#26041;&#38754;&#24471;&#21040;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models trained on different datasets can be merged by a weighted-averaging of their parameters, but why does it work and when can it fail? Here, we connect the inaccuracy of weighted-averaging to mismatches in the gradients and propose a new uncertainty-based scheme to improve the performance by reducing the mismatch. The connection also reveals implicit assumptions in other schemes such as averaging, task arithmetic, and Fisher-weighted averaging. Our new method gives consistent improvements for large language models and vision transformers, both in terms of performance and robustness to hyperparameters.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#32858;&#31867;&#21487;&#20998;&#31163;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#37327;&#21270;&#31867;&#38388;&#20998;&#31163;&#21644;&#31867;&#20869;&#36830;&#36890;&#24615;&#65292;&#23545;&#20110;&#23494;&#24230;&#32858;&#31867;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.12806</link><description>&lt;p&gt;
DCSI -- &#22522;&#20110;&#20998;&#31163;&#21644;&#36830;&#36890;&#24615;&#30340;&#25913;&#36827;&#30340;&#32858;&#31867;&#21487;&#20998;&#31163;&#24615;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
DCSI -- An improved measure of cluster separability based on separation and connectedness. (arXiv:2310.12806v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12806
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#32858;&#31867;&#21487;&#20998;&#31163;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#37327;&#21270;&#31867;&#38388;&#20998;&#31163;&#21644;&#31867;&#20869;&#36830;&#36890;&#24615;&#65292;&#23545;&#20110;&#23494;&#24230;&#32858;&#31867;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#21035;&#26631;&#31614;&#26159;&#21542;&#23545;&#24212;&#20110;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#23545;&#20110;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#38598;&#35780;&#20272;&#32858;&#31867;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20010;&#29305;&#24615;&#21487;&#20197;&#36890;&#36807;&#21487;&#20998;&#31163;&#24615;&#24230;&#37327;&#26469;&#37327;&#21270;&#12290;&#29616;&#26377;&#25991;&#29486;&#30340;&#32508;&#36848;&#26174;&#31034;&#65292;&#26082;&#26377;&#30340;&#22522;&#20110;&#20998;&#31867;&#30340;&#22797;&#26434;&#24615;&#24230;&#37327;&#26041;&#27861;&#21644;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631; (CVIs) &#37117;&#27809;&#26377;&#20805;&#20998;&#34701;&#20837;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#30340;&#26680;&#24515;&#29305;&#24449;&#65306;&#31867;&#38388;&#20998;&#31163;&#21644;&#31867;&#20869;&#36830;&#36890;&#24615;&#12290;&#19968;&#31181;&#26032;&#24320;&#21457;&#30340;&#24230;&#37327;&#26041;&#27861; (&#23494;&#24230;&#32858;&#31867;&#21487;&#20998;&#31163;&#24615;&#25351;&#25968;, DCSI) &#26088;&#22312;&#37327;&#21270;&#36825;&#20004;&#20010;&#29305;&#24449;&#65292;&#24182;&#19988;&#20063;&#21487;&#29992;&#20316; CVI&#12290;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;DCSI &#19982;&#36890;&#36807;&#35843;&#25972;&#20848;&#24503;&#25351;&#25968; (ARI) &#27979;&#37327;&#30340;DBSCAN&#30340;&#24615;&#33021;&#20043;&#38388;&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#22312;&#23545;&#22810;&#31867;&#25968;&#25454;&#38598;&#36827;&#34892;&#23494;&#24230;&#32858;&#31867;&#19981;&#36866;&#24403;&#30340;&#37325;&#21472;&#31867;&#21035;&#26102;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#23545;&#32463;&#24120;&#20351;&#29992;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#26174;&#31034;&#65292;DCSI &#33021;&#22815;&#26356;&#22909;&#22320;&#21306;&#20998;&#23494;&#24230;&#32858;&#31867;&#30340;&#21487;&#20998;&#31163;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whether class labels in a given data set correspond to meaningful clusters is crucial for the evaluation of clustering algorithms using real-world data sets. This property can be quantified by separability measures. A review of the existing literature shows that neither classification-based complexity measures nor cluster validity indices (CVIs) adequately incorporate the central aspects of separability for density-based clustering: between-class separation and within-class connectedness. A newly developed measure (density cluster separability index, DCSI) aims to quantify these two characteristics and can also be used as a CVI. Extensive experiments on synthetic data indicate that DCSI correlates strongly with the performance of DBSCAN measured via the adjusted rand index (ARI) but lacks robustness when it comes to multi-class data sets with overlapping classes that are ill-suited for density-based hard clustering. Detailed evaluation on frequently used real-world data sets shows that
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26816;&#27979;&#21644;&#35780;&#20272;&#20559;&#20506;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22240;&#26524;&#20998;&#26512;&#26469;&#29702;&#35299;&#31995;&#32479;&#20559;&#20506;&#34892;&#20026;&#30340;&#21407;&#22240;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#28508;&#22312;&#30340;&#20559;&#20506;&#29305;&#24449;&#65292;&#21363;&#20351;&#23427;&#20204;&#26368;&#21021;&#26159;&#26410;&#30693;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.12805</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#20559;&#20506;&#29305;&#24449;&#30340;&#26816;&#27979;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Detection and Evaluation of bias-inducing Features in Machine learning. (arXiv:2310.12805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26816;&#27979;&#21644;&#35780;&#20272;&#20559;&#20506;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22240;&#26524;&#20998;&#26512;&#26469;&#29702;&#35299;&#31995;&#32479;&#20559;&#20506;&#34892;&#20026;&#30340;&#21407;&#22240;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#28508;&#22312;&#30340;&#20559;&#20506;&#29305;&#24449;&#65292;&#21363;&#20351;&#23427;&#20204;&#26368;&#21021;&#26159;&#26410;&#30693;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#20998;&#26512;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#35299;&#26512;&#38382;&#39064;&#30340;&#25152;&#26377;&#21487;&#33021;&#21407;&#22240;&#65292;&#22914;&#19981;&#33391;&#30340;&#21830;&#19994;&#24773;&#20917;&#25110;&#23545;&#20010;&#20154;&#30340;&#24847;&#22806;&#20260;&#23475;&#12290;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#38382;&#39064;&#30340;&#32487;&#25215;&#26041;&#24335;&#65292;&#23545;&#21407;&#22240;&#36827;&#34892;&#25490;&#24207;&#20197;&#24110;&#21161;&#30830;&#23450;&#20462;&#22797;&#20248;&#20808;&#32423;&#65292;&#31616;&#21270;&#22797;&#26434;&#38382;&#39064;&#24182;&#36827;&#34892;&#21487;&#35270;&#21270;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;&#22240;&#26524;&#20998;&#26512;&#26469;&#29702;&#35299;&#31995;&#32479;&#20559;&#20506;&#34892;&#20026;&#30340;&#21407;&#22240;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#26816;&#26597;&#27599;&#20010;&#29305;&#24449;&#26159;&#21542;&#23545;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#26377;&#28508;&#22312;&#24433;&#21709;&#26469;&#26816;&#26597;&#20559;&#35265;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#21487;&#20197;&#23545;&#32473;&#23450;&#30340;&#29305;&#24449;&#25110;&#19968;&#23545;&#29305;&#24449;&#36827;&#34892;&#24494;&#23567;&#20462;&#25913;&#65292;&#24182;&#36981;&#24490;&#19968;&#20123;&#20934;&#21017;&#35266;&#23519;&#23427;&#23545;&#27169;&#22411;&#20915;&#31574;&#65288;&#21363;&#27169;&#22411;&#39044;&#27979;&#65289;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#21363;&#20351;&#36825;&#20123;&#29305;&#24449;&#26368;&#21021;&#26159;&#26410;&#30693;&#30340;&#65292;&#25105;&#20204;&#20063;&#21487;&#20197;&#20351;&#29992;&#22240;&#26524;&#20998;&#26512;&#26469;&#30830;&#23450;&#28508;&#22312;&#30340;&#24341;&#36215;&#20559;&#20506;&#29305;&#24449;&#12290;&#36825;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#26041;&#27861;&#35201;&#27714;&#20107;&#20808;&#30693;&#36947;&#36825;&#20123;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cause-to-effect analysis can help us decompose all the likely causes of a problem, such as an undesirable business situation or unintended harm to the individual(s). This implies that we can identify how the problems are inherited, rank the causes to help prioritize fixes, simplify a complex problem and visualize them. In the context of machine learning (ML), one can use cause-to-effect analysis to understand the reason for the biased behavior of the system. For example, we can examine the root causes of biases by checking each feature for a potential cause of bias in the model. To approach this, one can apply small changes to a given feature or a pair of features in the data, following some guidelines and observing how it impacts the decision made by the model (i.e., model prediction). Therefore, we can use cause-to-effect analysis to identify the potential bias-inducing features, even when these features are originally are unknown. This is important since most current methods req
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#39030;&#28857;&#25311;&#21512;&#31639;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#27425;&#32423;&#39030;&#28857;&#25311;&#21512;&#65292;&#24182;&#19982;&#31070;&#32463;&#32593;&#32476;&#26080;&#32541;&#38598;&#25104;&#65292;&#29992;&#20110;&#21943;&#27880;&#26631;&#31614;&#12290;&#36825;&#19968;&#26041;&#27861;&#23558;&#29289;&#29702;&#30693;&#35782;&#19982;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#25913;&#21892;&#37325;&#21619;&#36947;&#21943;&#27880;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2310.12804</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#30340;&#39030;&#28857;&#25311;&#21512;&#29992;&#20110;&#21943;&#27880;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
Differentiable Vertex Fitting for Jet Flavour Tagging. (arXiv:2310.12804v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#39030;&#28857;&#25311;&#21512;&#31639;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#27425;&#32423;&#39030;&#28857;&#25311;&#21512;&#65292;&#24182;&#19982;&#31070;&#32463;&#32593;&#32476;&#26080;&#32541;&#38598;&#25104;&#65292;&#29992;&#20110;&#21943;&#27880;&#26631;&#31614;&#12290;&#36825;&#19968;&#26041;&#27861;&#23558;&#29289;&#29702;&#30693;&#35782;&#19982;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#25913;&#21892;&#37325;&#21619;&#36947;&#21943;&#27880;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#39030;&#28857;&#25311;&#21512;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#27425;&#32423;&#39030;&#28857;&#25311;&#21512;&#65292;&#24182;&#21487;&#26080;&#32541;&#38598;&#25104;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#29992;&#20110;&#21943;&#27880;&#26631;&#31614;&#12290;&#39030;&#28857;&#25311;&#21512;&#34987;&#21046;&#23450;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#20248;&#21270;&#30340;&#35299;&#39030;&#28857;&#30340;&#26799;&#24230;&#36890;&#36807;&#38544;&#24335;&#24494;&#20998;&#23450;&#20041;&#65292;&#24182;&#21487;&#20197;&#20256;&#36882;&#32473;&#19978;&#28216;&#25110;&#19979;&#28216;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#36827;&#34892;&#32593;&#32476;&#35757;&#32451;&#12290;&#26356;&#24191;&#27867;&#22320;&#35828;&#65292;&#36825;&#26159;&#23558;&#29289;&#29702;&#30693;&#35782;&#38598;&#25104;&#21040;&#39640;&#33021;&#29289;&#29702;&#20013;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#21487;&#24494;&#20998;&#32534;&#31243;&#24212;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#21487;&#24494;&#20998;&#27425;&#32423;&#39030;&#28857;&#25311;&#21512;&#38598;&#25104;&#21040;&#22522;&#20110;Transformer&#30340;&#22823;&#27169;&#22411;&#20013;&#65292;&#20197;&#25913;&#21892;&#37325;&#21619;&#36947;&#21943;&#27880;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a differentiable vertex fitting algorithm that can be used for secondary vertex fitting, and that can be seamlessly integrated into neural networks for jet flavour tagging. Vertex fitting is formulated as an optimization problem where gradients of the optimized solution vertex are defined through implicit differentiation and can be passed to upstream or downstream neural network components for network training. More broadly, this is an application of differentiable programming to integrate physics knowledge into neural network models in high energy physics. We demonstrate how differentiable secondary vertex fitting can be integrated into larger transformer-based models for flavour tagging and improve heavy flavour jet classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#32467;&#26500;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#25928;&#26524;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26631;&#31614;&#19982;&#23646;&#24615;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12803</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#32467;&#26500;&#30340;&#25991;&#26412;&#31163;&#32676;&#20540;&#27867;&#21270;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Causal-structure Driven Augmentations for Text OOD Generalization. (arXiv:2310.12803v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#32467;&#26500;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#25928;&#26524;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26631;&#31614;&#19982;&#23646;&#24615;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#22120;&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#20381;&#36182;&#21487;&#33021;&#23548;&#33268;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#25928;&#26524;&#19981;&#20339;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#20854;&#22312;&#22914;&#21307;&#30103;&#39046;&#22495;&#31561;&#23433;&#20840;&#20851;&#38190;&#34892;&#19994;&#20013;&#20351;&#29992;&#30340;&#25285;&#24551;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22240;&#26524;&#32467;&#26500;&#30693;&#35782;&#25351;&#23548;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#27169;&#25311;&#23545;&#34394;&#20551;&#29305;&#24449;&#36827;&#34892;&#24178;&#39044;&#65292;&#20197;&#23398;&#20064;&#26356;&#21152;&#40065;&#26834;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26631;&#31614;&#19982;&#23646;&#24615;&#20043;&#38388;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#39044;&#27979;&#38382;&#39064;&#20013;&#65292;&#36825;&#31181;&#31574;&#30053;&#26159;&#21512;&#36866;&#30340;&#12290;&#22312;&#36825;&#31181;&#38382;&#39064;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#30456;&#23545;&#20110;&#37325;&#35201;&#24615;&#37325;&#21152;&#26435;&#30340;&#26377;&#21033;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#36741;&#21161;&#25968;&#25454;&#36890;&#36807;&#24046;&#20998;&#22312;&#24046;&#20998;&#30340;&#26041;&#27861;&#26469;&#21305;&#37197;&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#34920;&#31034;&#25991;&#26412;&#30340;&#26465;&#20214;&#27010;&#29575;&#12290;&#36890;&#36807;&#23545;&#20174;&#21307;&#23398;&#21465;&#36848;&#20013;&#23398;&#20064;&#19982;&#30475;&#25252;&#32773;&#26080;&#20851;&#30340;&#20020;&#24202;&#35786;&#26029;&#39044;&#27979;&#22120;&#20197;&#21450;&#21322;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reliance of text classifiers on spurious correlations can lead to poor generalization at deployment, raising concerns about their use in safety-critical domains such as healthcare. In this work, we propose to use counterfactual data augmentation, guided by knowledge of the causal structure of the data, to simulate interventions on spurious features and to learn more robust text classifiers. We show that this strategy is appropriate in prediction problems where the label is spuriously correlated with an attribute. Under the assumptions of such problems, we discuss the favorable sample complexity of counterfactual data augmentation, compared to importance re-weighting. Pragmatically, we match examples using auxiliary data, based on diff-in-diff methodology, and use a large language model (LLM) to represent a conditional probability of text. Through extensive experimentation on learning caregiver-invariant predictors of clinical diagnoses from medical narratives and on semi-synthetic 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31454;&#20105;&#23616;&#37096;&#23398;&#20064;&#21644;&#21333;&#20803;&#20043;&#38388;&#30340;&#25193;&#25955;&#32806;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21270;&#27169;&#22411;&#65292;&#39044;&#27979;&#20102;&#38598;&#20307;&#23398;&#20064;&#20013;&#30340;&#26080;&#24207;-&#26377;&#24207;-&#26080;&#24207;&#30456;&#21464;&#65292;&#24182;&#39564;&#35777;&#20102;&#36825;&#20010;&#29702;&#35770;&#12290;</title><link>http://arxiv.org/abs/2310.12802</link><description>&lt;p&gt;
&#19968;&#31181;&#38598;&#20307;&#28145;&#24230;&#23398;&#20064;&#30340;&#26377;&#25928;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
An effective theory of collective deep learning. (arXiv:2310.12802v1 [physics.soc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12802
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31454;&#20105;&#23616;&#37096;&#23398;&#20064;&#21644;&#21333;&#20803;&#20043;&#38388;&#30340;&#25193;&#25955;&#32806;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21270;&#27169;&#22411;&#65292;&#39044;&#27979;&#20102;&#38598;&#20307;&#23398;&#20064;&#20013;&#30340;&#26080;&#24207;-&#26377;&#24207;-&#26080;&#24207;&#30456;&#21464;&#65292;&#24182;&#39564;&#35777;&#20102;&#36825;&#20010;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#32806;&#21512;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#20013;&#38598;&#20307;&#23398;&#20064;&#30340;&#20986;&#29616;&#26159;&#23545;&#29289;&#29702;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#31038;&#20250;&#23398;&#30340;&#24191;&#27867;&#24433;&#21709;&#30340;&#19968;&#39033;&#21162;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21270;&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#21508;&#20010;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#21442;&#25968;&#30340;&#23616;&#37096;&#23398;&#20064;&#21160;&#24577;&#21644;&#21333;&#20803;&#20043;&#38388;&#30340;&#25193;&#25955;&#32806;&#21512;&#20043;&#38388;&#30340;&#31454;&#20105;&#65292;&#23558;&#20960;&#20010;&#26368;&#36817;&#30340;&#20998;&#25955;&#31639;&#27861;&#36827;&#34892;&#20102;&#21387;&#32553;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#19982;&#20855;&#26377;&#28140;&#28781;&#38543;&#26426;&#24615;&#30340;Ginzburg-Landau&#27169;&#22411;&#31867;&#20284;&#30340;&#32447;&#24615;&#32593;&#32476;&#30340;&#26377;&#25928;&#29702;&#35770;&#65292;&#25512;&#23548;&#20986;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#31895;&#31890;&#21270;&#34892;&#20026;&#12290;&#36825;&#20010;&#26694;&#26550;&#39044;&#27979;&#20102;&#21442;&#25968;&#35299;&#30340;&#65288;&#28145;&#24230;&#20381;&#36182;&#30340;&#65289;&#26080;&#24207;-&#26377;&#24207;-&#26080;&#24207;&#30456;&#21464;&#65292;&#25581;&#31034;&#20102;&#38598;&#20307;&#23398;&#20064;&#30456;&#30340;&#24320;&#22987;&#65292;&#20197;&#21450;&#28145;&#24230;&#24341;&#36215;&#30340;&#20020;&#30028;&#28857;&#24310;&#36831;&#21644;&#24494;&#35266;&#23398;&#20064;&#36335;&#24452;&#30340;&#40065;&#26834;&#24418;&#29366;&#12290;&#25105;&#20204;&#22312;&#29616;&#23454;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unraveling the emergence of collective learning in systems of coupled artificial neural networks is an endeavor with broader implications for physics, machine learning, neuroscience and society. Here we introduce a minimal model that condenses several recent decentralized algorithms by considering a competition between two terms: the local learning dynamics in the parameters of each neural network unit, and a diffusive coupling among units that tends to homogenize the parameters of the ensemble. We derive the coarse-grained behavior of our model via an effective theory for linear networks that we show is analogous to a deformed Ginzburg-Landau model with quenched disorder. This framework predicts (depth-dependent) disorder-order-disorder phase transitions in the parameters' solutions that reveal the onset of a collective learning phase, along with a depth-induced delay of the critical point and a robust shape of the microscopic learning path. We validate our theory in realistic ensembl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#21360;&#24230;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#35782;&#21035;&#21496;&#27861;&#26696;&#20214;&#30340;&#22270;&#32467;&#26500;&#24182;&#36827;&#34892;&#33410;&#28857;&#20998;&#31867;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#21270;&#25512;&#27979;&#26696;&#20214;&#32467;&#26524;&#12290;&#30740;&#31350;&#32771;&#34385;&#20102;&#27169;&#22411;&#29305;&#24449;&#12289;&#20844;&#24179;&#24615;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.12800</link><description>&lt;p&gt;
&#25506;&#32034;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21360;&#24230;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Graph Neural Networks for Indian Legal Judgment Prediction. (arXiv:2310.12800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#21360;&#24230;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#35782;&#21035;&#21496;&#27861;&#26696;&#20214;&#30340;&#22270;&#32467;&#26500;&#24182;&#36827;&#34892;&#33410;&#28857;&#20998;&#31867;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#21270;&#25512;&#27979;&#26696;&#20214;&#32467;&#26524;&#12290;&#30740;&#31350;&#32771;&#34385;&#20102;&#27169;&#22411;&#29305;&#24449;&#12289;&#20844;&#24179;&#24615;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#24179;&#34913;&#30340;&#27861;&#23448;&#19982;&#26696;&#20214;&#27604;&#20363;&#23545;&#21496;&#27861;&#31995;&#32479;&#20135;&#29983;&#20102;&#32321;&#37325;&#30340;&#24433;&#21709;&#65292;&#34920;&#29616;&#20026;&#22823;&#37327;&#31215;&#21387;&#30340;&#26410;&#20915;&#26696;&#20214;&#20197;&#21450;&#25345;&#32493;&#28044;&#20837;&#30340;&#26032;&#26696;&#20214;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#21152;&#24555;&#21496;&#27861;&#31243;&#24207;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#20107;&#23454;&#35777;&#25454;&#21644;&#36807;&#21435;&#26696;&#20214;&#30340;&#20808;&#20363;&#26469;&#25512;&#27979;&#26696;&#20214;&#32467;&#26524;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#24314;&#35758;&#21464;&#24471;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#20391;&#37325;&#20110;&#24320;&#21457;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#65288;LJP&#65289;&#38382;&#39064;&#65292;&#35782;&#21035;&#21496;&#27861;&#26696;&#20214;&#30340;&#20869;&#22312;&#22270;&#32467;&#26500;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#20108;&#36827;&#21046;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#23884;&#20837;&#20316;&#20026;&#27169;&#22411;&#29305;&#24449;&#65292;&#21516;&#26102;&#28155;&#21152;&#21644;&#20462;&#21098;&#20102;&#26102;&#38388;&#33410;&#28857;&#21644;&#21496;&#27861;&#34892;&#20026;&#33410;&#28857;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#36824;&#32771;&#34385;&#20102;&#36825;&#20123;&#39044;&#27979;&#30340;&#20844;&#24179;&#24615;&#30340;&#20262;&#29702;&#32500;&#24230;&#65292;&#32771;&#34385;&#20102;&#24615;&#21035;&#21644;&#22995;&#21517;&#30340;&#20559;&#35265;&#12290;&#36824;&#36827;&#34892;&#20102;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20197;&#35780;&#20272;&#27169;&#22411;&#23545;&#26410;&#26469;&#26696;&#20214;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The burdensome impact of a skewed judges-to-cases ratio on the judicial system manifests in an overwhelming backlog of pending cases alongside an ongoing influx of new ones. To tackle this issue and expedite the judicial process, the proposition of an automated system capable of suggesting case outcomes based on factual evidence and precedent from past cases gains significance. This research paper centres on developing a graph neural network-based model to address the Legal Judgment Prediction (LJP) problem, recognizing the intrinsic graph structure of judicial cases and making it a binary node classification problem. We explored various embeddings as model features, while nodes such as time nodes and judicial acts were added and pruned to evaluate the model's performance. The study is done while considering the ethical dimension of fairness in these predictions, considering gender and name biases. A link prediction task is also conducted to assess the model's proficiency in anticipati
&lt;/p&gt;</description></item><item><title>OODRobustBench&#26159;&#19968;&#20010;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#20998;&#26512;&#22312;&#20998;&#24067;&#36801;&#31227;&#19979;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;&#22823;&#35268;&#27169;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#22312;&#31163;&#32676;&#20998;&#24067;&#27979;&#35797;&#19979;&#23384;&#22312;&#20005;&#37325;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#32780;&#20869;&#20998;&#24067;&#40065;&#26834;&#24615;&#19982;&#31163;&#32676;&#20998;&#24067;&#40065;&#26834;&#24615;&#21576;&#24378;&#27491;&#32447;&#24615;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2310.12793</link><description>&lt;p&gt;
OODRobustBench: &#22312;&#20998;&#24067;&#36801;&#31227;&#19979;&#35780;&#20272;&#21644;&#20998;&#26512;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OODRobustBench: benchmarking and analyzing adversarial robustness under distribution shift. (arXiv:2310.12793v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12793
&lt;/p&gt;
&lt;p&gt;
OODRobustBench&#26159;&#19968;&#20010;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#20998;&#26512;&#22312;&#20998;&#24067;&#36801;&#31227;&#19979;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;&#22823;&#35268;&#27169;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#22312;&#31163;&#32676;&#20998;&#24067;&#27979;&#35797;&#19979;&#23384;&#22312;&#20005;&#37325;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#32780;&#20869;&#20998;&#24067;&#40065;&#26834;&#24615;&#19982;&#31163;&#32676;&#20998;&#24067;&#40065;&#26834;&#24615;&#21576;&#24378;&#27491;&#32447;&#24615;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30740;&#31350;&#22312;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#36890;&#24120;&#21482;&#22312;&#19982;&#35757;&#32451;&#25968;&#25454;&#30456;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#21363;&#20869;&#20998;&#24067;&#65288;ID&#65289;&#27979;&#35797;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#31181;&#40065;&#26834;&#24615;&#22312;&#36755;&#20837;&#20998;&#24067;&#36801;&#31227;&#65292;&#21363;&#31163;&#32676;&#20998;&#24067;&#65288;OOD&#65289;&#27979;&#35797;&#19979;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#23454;&#38469;&#37096;&#32626;&#26102;&#65292;&#30001;&#20110;&#36825;&#31181;&#20998;&#24067;&#36801;&#31227;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#36825;&#19968;&#38382;&#39064;&#21313;&#20998;&#20196;&#20154;&#25285;&#24551;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;OODRobustBench&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#20351;&#29992;23&#20010;&#22522;&#20110;&#25968;&#25454;&#38598;&#30340;&#36801;&#31227;&#65288;&#21363;&#36755;&#20837;&#20998;&#24067;&#30340;&#33258;&#28982;&#36801;&#31227;&#65289;&#21644;6&#20010;&#22522;&#20110;&#23041;&#32961;&#30340;&#36801;&#31227;&#65288;&#21363;&#26410;&#30693;&#30340;&#23545;&#25239;&#24615;&#23041;&#32961;&#27169;&#22411;&#65289;&#26469;&#20840;&#38754;&#35780;&#20272;OOD&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;OODRobustBench&#29992;&#20110;&#35780;&#20272;&#20102;706&#20010;&#40065;&#26834;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;60.7K&#27425;&#23545;&#25239;&#24615;&#35780;&#20272;&#12290;&#36825;&#20010;&#22823;&#35268;&#27169;&#20998;&#26512;&#34920;&#26126;&#65306;1&#65289;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#23384;&#22312;&#20005;&#37325;&#30340;OOD&#27867;&#21270;&#38382;&#39064;&#65307;2&#65289;ID&#40065;&#26834;&#24615;&#19982;OOD&#40065;&#26834;&#24615;&#21576;&#24378;&#27491;&#32447;&#24615;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing works have made great progress in improving adversarial robustness, but typically test their method only on data from the same distribution as the training data, i.e. in-distribution (ID) testing. As a result, it is unclear how such robustness generalizes under input distribution shifts, i.e. out-of-distribution (OOD) testing. This is a concerning omission as such distribution shifts are unavoidable when methods are deployed in the wild. To address this issue we propose a benchmark named OODRobustBench to comprehensively assess OOD adversarial robustness using 23 dataset-wise shifts (i.e. naturalistic shifts in input distribution) and 6 threat-wise shifts (i.e., unforeseen adversarial threat models). OODRobustBench is used to assess 706 robust models using 60.7K adversarial evaluations. This large-scale analysis shows that: 1) adversarial robustness suffers from a severe OOD generalization issue; 2) ID robustness correlates strongly with OOD robustness, in a positive linear wa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#23545;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#24085;&#32047;&#25176;&#21069;&#27839;&#36827;&#34892;&#20102;&#24314;&#27169;&#21644;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#26377;&#35265;&#22320;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.12785</link><description>&lt;p&gt;
&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#26469;&#34920;&#24449;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#24085;&#32047;&#25176;&#21069;&#27839;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Approach to Characterize the Accuracy-Fairness Trade-off Pareto Frontier. (arXiv:2310.12785v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#23545;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#24085;&#32047;&#25176;&#21069;&#27839;&#36827;&#34892;&#20102;&#24314;&#27169;&#21644;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#26377;&#35265;&#22320;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#30340;&#25991;&#29486;&#20013;&#65292;&#25105;&#20204;&#32463;&#24120;&#35266;&#23519;&#21040;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#19968;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#32570;&#20047;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#24085;&#32047;&#25176;&#21069;&#27839;&#36827;&#34892;&#24314;&#27169;&#65292;&#21457;&#23637;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#12290;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#24085;&#32047;&#25176;&#21069;&#27839;&#26159;&#30001;&#19968;&#32452;&#25152;&#26377;&#26368;&#20248;&#24085;&#32047;&#25176;&#20998;&#31867;&#22120;&#32452;&#25104;&#30340;&#65292;&#27809;&#26377;&#20854;&#20182;&#20998;&#31867;&#22120;&#33021;&#22815;&#25903;&#37197;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#35777;&#26126;&#20102;&#35813;&#26435;&#34913;&#30340;&#23384;&#22312;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#22235;&#20010;&#28508;&#22312;&#30340;&#20998;&#31867;&#26469;&#34920;&#24449;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#37325;&#35201;&#29305;&#24615;&#12290;&#23545;&#20110;&#27599;&#20010;&#20998;&#31867;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#23548;&#33268;&#30456;&#24212;&#26435;&#34913;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#30340;&#23454;&#39564;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#24471;&#20986;&#20102;&#26377;&#35265;&#22320;&#30340;&#21457;&#29616;&#65306;&#65288;1&#65289;&#24403;&#25935;&#24863;&#23646;&#24615;&#21487;&#20197;&#23436;&#20840;&#36890;&#36807;&#38750;&#25935;&#24863;&#23646;&#24615;&#36827;&#34892;&#35299;&#37322;&#26102;&#65292;&#26435;&#34913;&#24085;&#32047;&#25176;&#21069;&#27839;&#22823;&#22810;&#26159;&#36830;&#32493;&#30340;&#12290;&#65288;2&#65289;&#20934;&#30830;&#24615;&#21487;&#33021;&#20250;&#21463;&#21040;...
&lt;/p&gt;
&lt;p&gt;
While the accuracy-fairness trade-off has been frequently observed in the literature of fair machine learning, rigorous theoretical analyses have been scarce. To demystify this long-standing challenge, this work seeks to develop a theoretical framework by characterizing the shape of the accuracy-fairness trade-off Pareto frontier (FairFrontier), determined by a set of all optimal Pareto classifiers that no other classifiers can dominate. Specifically, we first demonstrate the existence of the trade-off in real-world scenarios and then propose four potential categories to characterize the important properties of the accuracy-fairness Pareto frontier. For each category, we identify the necessary conditions that lead to corresponding trade-offs. Experimental results on synthetic data suggest insightful findings of the proposed framework: (1) When sensitive attributes can be fully interpreted by non-sensitive attributes, FairFrontier is mostly continuous. (2) Accuracy can suffer a \textit{
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#20013;&#36827;&#34892;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#22120;&#26469;&#36817;&#20284;&#27169;&#22411;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#32479;&#35745;&#20998;&#26512;&#36807;&#31243;&#20013;&#21482;&#33021;&#35775;&#38382;&#31169;&#26377;&#21270;&#25968;&#25454;&#23548;&#33268;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#22686;&#21152;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12781</link><description>&lt;p&gt;
&#20174;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#20013;&#36827;&#34892;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Conditional Density Estimations from Privacy-Protected Data. (arXiv:2310.12781v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#20013;&#36827;&#34892;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#22120;&#26469;&#36817;&#20284;&#27169;&#22411;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#32479;&#35745;&#20998;&#26512;&#36807;&#31243;&#20013;&#21482;&#33021;&#35775;&#38382;&#31169;&#26377;&#21270;&#25968;&#25454;&#23548;&#33268;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#22686;&#21152;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#20195;&#32479;&#35745;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#38656;&#35201;&#22312;&#25935;&#24863;&#29992;&#25143;&#25968;&#25454;&#19978;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#24046;&#20998;&#38544;&#31169;&#25552;&#20379;&#20102;&#19968;&#31181;&#27491;&#24335;&#30340;&#20445;&#35777;&#65292;&#21363;&#20010;&#20307;&#29992;&#25143;&#20449;&#24687;&#19981;&#20250;&#27844;&#38706;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#38543;&#26426;&#31639;&#27861;&#21521;&#20445;&#23494;&#25968;&#25454;&#27880;&#20837;&#26657;&#20934;&#30340;&#22122;&#22768;&#65292;&#20174;&#32780;&#20135;&#29983;&#38544;&#31169;&#20445;&#25252;&#30340;&#25968;&#25454;&#38598;&#25110;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#22312;&#32479;&#35745;&#20998;&#26512;&#36807;&#31243;&#20013;&#21482;&#33021;&#35775;&#38382;&#31169;&#26377;&#21270;&#25968;&#25454;&#20250;&#23548;&#33268;&#35745;&#31639;&#22797;&#26434;&#24230;&#22686;&#21152;&#65292;&#38590;&#20197;&#23545;&#22522;&#30784;&#26426;&#23494;&#25968;&#25454;&#30340;&#21442;&#25968;&#36827;&#34892;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#38598;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#29702;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#22120;&#20316;&#20026;&#19968;&#32452;&#28789;&#27963;&#30340;&#20998;&#24067;&#26469;&#36817;&#20284;&#32473;&#23450;&#35266;&#27979;&#21040;&#30340;&#31169;&#26377;&#26597;&#35810;&#32467;&#26524;&#30340;&#27169;&#22411;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;&#20256;&#26579;&#30149;&#27169;&#22411;&#19979;&#30340;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20197;&#21450;&#26222;&#36890;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#19978;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many modern statistical analysis and machine learning applications require training models on sensitive user data. Differential privacy provides a formal guarantee that individual-level information about users does not leak. In this framework, randomized algorithms inject calibrated noise into the confidential data, resulting in privacy-protected datasets or queries. However, restricting access to only the privatized data during statistical analysis makes it computationally challenging to perform valid inferences on parameters underlying the confidential data. In this work, we propose simulation-based inference methods from privacy-protected datasets. Specifically, we use neural conditional density estimators as a flexible family of distributions to approximate the posterior distribution of model parameters given the observed private query results. We illustrate our methods on discrete time-series data under an infectious disease model and on ordinary linear regression models. Illustra
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#24863;&#30693;&#30340;&#33258;&#21160;&#35821;&#35328;&#34920;&#36798;&#26041;&#24335;&#65288;LAAV&#65289;&#65292;&#36890;&#36807;&#22686;&#21152;&#25163;&#21160;&#26631;&#31614;&#21644;&#36830;&#25509;&#35789;&#8220;&#21644;&#8221;&#26469;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20174;&#32780;&#22312;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.12778</link><description>&lt;p&gt;
&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#30340;&#33258;&#21160;&#35821;&#35328;&#34920;&#36798;&#26041;&#24335;&#25552;&#20379;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Label-Aware Automatic Verbalizer for Few-Shot Text Classification. (arXiv:2310.12778v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12778
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#24863;&#30693;&#30340;&#33258;&#21160;&#35821;&#35328;&#34920;&#36798;&#26041;&#24335;&#65288;LAAV&#65289;&#65292;&#36890;&#36807;&#22686;&#21152;&#25163;&#21160;&#26631;&#31614;&#21644;&#36830;&#25509;&#35789;&#8220;&#21644;&#8221;&#26469;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#65292;&#20174;&#32780;&#22312;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#22312;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#20013;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#12290;&#20854;&#25104;&#21151;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#26159;&#19968;&#31181;&#35821;&#35328;&#34920;&#36798;&#26041;&#24335;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#36716;&#21270;&#20026;&#39044;&#27979;&#31867;&#21035;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26368;&#31616;&#21333;&#21644;&#24191;&#27867;&#35748;&#21487;&#30340;&#35821;&#35328;&#34920;&#36798;&#26041;&#24335;&#20351;&#29992;&#25163;&#21160;&#26631;&#31614;&#26469;&#34920;&#31034;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#36873;&#25321;&#24182;&#19981;&#33021;&#20445;&#35777;&#22312;&#36873;&#25321;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26465;&#20214;&#19979;&#25152;&#36873;&#25321;&#30340;&#21333;&#35789;&#30340;&#26368;&#20248;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#24863;&#30693;&#30340;&#33258;&#21160;&#35821;&#35328;&#34920;&#36798;&#26041;&#24335;&#65288;LAAV&#65289;&#65292;&#36890;&#36807;&#26377;&#25928;&#22686;&#21152;&#25163;&#21160;&#26631;&#31614;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#23569;&#26679;&#26412;&#20998;&#31867;&#32467;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25163;&#21160;&#26631;&#31614;&#20197;&#21450;&#36830;&#25509;&#35789;&#8220;&#21644;&#8221;&#26469;&#35825;&#23548;&#27169;&#22411;&#29983;&#25104;&#26356;&#26377;&#25928;&#30340;&#35821;&#35328;&#34920;&#36798;&#26041;&#24335;&#20013;&#30340;&#21333;&#35789;&#12290;&#22312;&#20116;&#20010;&#25968;&#25454;&#38598;&#36328;&#20116;&#31181;&#35821;&#35328;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LAAV&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#35821;&#35328;&#34920;&#36798;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#21457;&#29616;&#65292;&#19982;&#31867;&#20284;&#26041;&#27861;&#30456;&#27604;&#65292;LAAV&#25552;&#20379;&#26356;&#30456;&#20851;&#30340;&#21333;&#35789;&#65292;&#29305;&#21035;&#26159;&#22312;&#20013;&#21040;
&lt;/p&gt;
&lt;p&gt;
Prompt-based learning has shown its effectiveness in few-shot text classification. One important factor in its success is a verbalizer, which translates output from a language model into a predicted class. Notably, the simplest and widely acknowledged verbalizer employs manual labels to represent the classes. However, manual selection does not guarantee the optimality of the selected words when conditioned on the chosen language model. Therefore, we propose Label-Aware Automatic Verbalizer (LAAV), effectively augmenting the manual labels to achieve better few-shot classification results. Specifically, we use the manual labels along with the conjunction "and" to induce the model to generate more effective words for the verbalizer. The experimental results on five datasets across five languages demonstrate that LAAV significantly outperforms existing verbalizers. Furthermore, our analysis reveals that LAAV suggests more relevant words compared to similar approaches, especially in mid-to-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ClaPS&#30340;&#31616;&#21333;&#40657;&#30418;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#31867;&#21644;&#20462;&#21098;&#25628;&#32034;&#31354;&#38388;&#20013;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#25552;&#31034;&#20196;&#29260;&#65292;&#35299;&#20915;&#20102;&#29616;&#20195;&#40657;&#30418;&#26041;&#27861;&#20013;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12774</link><description>&lt;p&gt;
&#23384;&#27963;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#25552;&#31034;&#65306;&#36890;&#36807;&#32858;&#31867;&#21644;&#20462;&#21098;&#23454;&#29616;&#39640;&#25928;&#30340;&#40657;&#30418;&#25552;&#31034;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Survival of the Most Influential Prompts: Efficient Black-Box Prompt Search via Clustering and Pruning. (arXiv:2310.12774v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ClaPS&#30340;&#31616;&#21333;&#40657;&#30418;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#31867;&#21644;&#20462;&#21098;&#25628;&#32034;&#31354;&#38388;&#20013;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#25552;&#31034;&#20196;&#29260;&#65292;&#35299;&#20915;&#20102;&#29616;&#20195;&#40657;&#30418;&#26041;&#27861;&#20013;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26377;&#25928;&#33539;&#20363;&#65292;&#20351;&#24471;&#23569;&#26679;&#26412;&#29978;&#33267;&#38646;&#26679;&#26412;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#12290;&#26368;&#36817;&#65292;&#40657;&#30418;&#25552;&#31034;&#25628;&#32034;&#22240;&#20854;&#26799;&#24230;-free&#20248;&#21270;&#30340;&#29420;&#29305;&#29305;&#24615;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#34987;&#35777;&#26126;&#22312;&#27169;&#22411;&#21363;&#26381;&#21153;&#30340;&#20351;&#29992;&#20013;&#29305;&#21035;&#26377;&#29992;&#21644;&#24378;&#22823;&#12290;&#28982;&#32780;&#65292;&#32452;&#21512;&#20248;&#21270;&#30340;&#31163;&#25955;&#26412;&#36136;&#21644;&#22797;&#26434;&#24615;&#38459;&#30861;&#20102;&#29616;&#20195;&#40657;&#30418;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;&#23613;&#31649;&#22312;&#25628;&#32034;&#31639;&#27861;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#25628;&#32034;&#31354;&#38388;&#35774;&#35745;&#21644;&#20248;&#21270;&#30340;&#20851;&#38190;&#26041;&#38754;&#21364;&#34987;&#22823;&#37096;&#20998;&#24573;&#35270;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25552;&#31034;LLM&#36827;&#34892;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#25581;&#31034;&#21482;&#26377;&#23569;&#37327;&#30340;&#20196;&#29260;&#23545;LLM&#39044;&#27979;&#20135;&#29983;&#20102;&#19981;&#25104;&#27604;&#20363;&#30340;&#24433;&#21709;&#12290;&#21033;&#29992;&#36825;&#19968;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Clustering and Pruning for Efficient Black-box Prompt Search&#65288;ClaPS&#65289;&#30340;&#31616;&#21333;&#40657;&#30418;&#25628;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#23545;&#25628;&#32034;&#31354;&#38388;&#36827;&#34892;&#32858;&#31867;&#21644;&#20462;&#21098;&#65292;&#21482;&#20851;&#27880;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#25552;&#31034;&#20196;&#29260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based learning has been an effective paradigm for large pretrained language models (LLM), enabling few-shot or even zero-shot learning. Black-box prompt search has received growing interest recently for its distinctive properties of gradient-free optimization, proven particularly useful and powerful for model-as-a-service usage. However, the discrete nature and the complexity of combinatorial optimization hinder the efficiency of modern black-box approaches. Despite extensive research on search algorithms, the crucial aspect of search space design and optimization has been largely overlooked. In this paper, we first conduct a sensitivity analysis by prompting LLM, revealing that only a small number of tokens exert a disproportionate amount of influence on LLM predictions. Leveraging this insight, we propose the Clustering and Pruning for Efficient Black-box Prompt Search (ClaPS), a simple black-box search method that first clusters and prunes the search space to focus exclusivel
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23433;&#20840;RLHF&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#24179;&#34913;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;&#23427;&#36890;&#36807;&#35299;&#32806;&#20154;&#31867;&#20559;&#22909;&#65292;&#24182;&#35757;&#32451;&#20998;&#21035;&#30340;&#22870;&#21169;&#21644;&#25104;&#26412;&#27169;&#22411;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#26377;&#30410;&#21644;&#26080;&#23475;&#30446;&#26631;&#20043;&#38388;&#30340;&#22266;&#26377;&#24352;&#21147;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#24179;&#34913;&#26469;&#20248;&#21270;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.12773</link><description>&lt;p&gt;
&#23433;&#20840;RLHF&#65306;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe RLHF: Safe Reinforcement Learning from Human Feedback. (arXiv:2310.12773v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12773
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23433;&#20840;RLHF&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#24179;&#34913;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;&#23427;&#36890;&#36807;&#35299;&#32806;&#20154;&#31867;&#20559;&#22909;&#65292;&#24182;&#35757;&#32451;&#20998;&#21035;&#30340;&#22870;&#21169;&#21644;&#25104;&#26412;&#27169;&#22411;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#26377;&#30410;&#21644;&#26080;&#23475;&#30446;&#26631;&#20043;&#38388;&#30340;&#22266;&#26377;&#24352;&#21147;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#24179;&#34913;&#26469;&#20248;&#21270;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#65292;&#24179;&#34913;AI&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#21464;&#24471;&#26356;&#21152;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;LLM&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26377;&#30410;&#21644;&#26080;&#23475;&#30446;&#26631;&#20043;&#38388;&#30340;&#22266;&#26377;&#24352;&#21147;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#22686;&#21152;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23433;&#20840;RLHF&#65306;&#19968;&#31181;&#29992;&#20110;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#30340;&#26032;&#39062;&#31639;&#27861;&#12290;&#23433;&#20840;RLHF&#26126;&#30830;&#35299;&#32806;&#20102;&#20851;&#20110;&#26377;&#30410;&#24615;&#21644;&#26080;&#23475;&#24615;&#30340;&#20154;&#31867;&#20559;&#22909;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#20247;&#21253;&#24037;&#20316;&#32773;&#23545;&#24352;&#21147;&#30340;&#22256;&#24785;&#65292;&#24182;&#20801;&#35768;&#25105;&#20204;&#35757;&#32451;&#20998;&#21035;&#30340;&#22870;&#21169;&#21644;&#25104;&#26412;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;LLM&#30340;&#23433;&#20840;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20248;&#21270;&#20219;&#21153;&#65292;&#21363;&#22312;&#28385;&#36275;&#25351;&#23450;&#25104;&#26412;&#32422;&#26463;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#22870;&#21169;&#20989;&#25968;&#12290;&#36890;&#36807;&#21033;&#29992;Lagrangian&#26041;&#27861;&#35299;&#20915;&#36825;&#20010;&#32422;&#26463;&#38382;&#39064;&#65292;&#23433;&#20840;RLHF&#22312;&#31934;&#35843;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#20004;&#20010;&#30446;&#26631;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#36890;&#36807;&#19977;&#36718;&#20351;&#29992;&#23433;&#20840;RLHF&#36827;&#34892;&#31934;&#35843;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#23433;&#20840;&#19988;&#20855;&#26377;&#20248;&#33391;&#24615;&#33021;&#30340;AI&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of large language models (LLMs), striking a balance between the performance and safety of AI systems has never been more critical. However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during LLM training. To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe RLHF), a novel algorithm for human value alignment. Safe RLHF explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowdworkers' confusion about the tension and allowing us to train separate reward and cost models. We formalize the safety concern of LLMs as an optimization task of maximizing the reward function while satisfying specified cost constraints. Leveraging the Lagrangian method to solve this constrained problem, Safe RLHF dynamically adjusts the balance between the two objectives during fine-tuning. Through a three-round fine-tuning using Safe RLHF, we d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;SAG&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20123;&#26631;&#20934;&#20248;&#21270;&#22120;&#65292;&#26088;&#22312;&#23558;&#38543;&#26426;&#26041;&#27861;&#30340;&#25104;&#26412;&#19982;&#30830;&#23450;&#24615;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#32467;&#21512;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2310.12771</link><description>&lt;p&gt;
&#38543;&#26426;&#24179;&#22343;&#26799;&#24230;&#65306;&#19968;&#39033;&#31616;&#21333;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Stochastic Average Gradient : A Simple Empirical Investigation. (arXiv:2310.12771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;SAG&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20123;&#26631;&#20934;&#20248;&#21270;&#22120;&#65292;&#26088;&#22312;&#23558;&#38543;&#26426;&#26041;&#27861;&#30340;&#25104;&#26412;&#19982;&#30830;&#23450;&#24615;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35770;&#30740;&#31350;&#21644;&#23454;&#35777;&#25104;&#21151;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#20173;&#28982;&#26159;&#35757;&#32451;&#36825;&#20123;&#32593;&#32476;&#30340;&#26368;&#24120;&#29992;&#31639;&#27861;&#12290;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#26377;&#30830;&#23450;&#24615;&#25110;&#20840;&#26799;&#24230;&#65288;FG&#65289;&#26041;&#27861;&#65292;&#20854;&#25104;&#26412;&#19982;&#25152;&#20351;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#25104;&#27491;&#27604;&#65292;&#20294;&#25910;&#25947;&#36895;&#24230;&#20026;&#32447;&#24615;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#26377;&#38543;&#26426;&#26799;&#24230;&#65288;SG&#65289;&#26041;&#27861;&#65292;&#20854;&#25104;&#26412;&#19982;&#25968;&#25454;&#38598;&#22823;&#23567;&#26080;&#20851;&#65292;&#20294;&#25910;&#25947;&#36895;&#24230;&#19981;&#22914;&#30830;&#23450;&#24615;&#26041;&#27861;&#29702;&#24819;&#12290;&#20026;&#20102;&#23558;&#38543;&#26426;&#26041;&#27861;&#30340;&#25104;&#26412;&#19982;&#30830;&#23450;&#24615;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#38543;&#26426;&#24179;&#22343;&#26799;&#24230;&#65288;SAG&#65289;&#26041;&#27861;&#12290;SAG&#26159;&#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#26377;&#38480;&#20010;&#24179;&#28369;&#20984;&#20989;&#25968;&#20043;&#21644;&#30340;&#26041;&#27861;&#12290;&#19982;SG&#26041;&#27861;&#19968;&#26679;&#65292;SAG&#26041;&#27861;&#30340;&#36845;&#20195;&#25104;&#26412;&#19982;&#27714;&#21644;&#20013;&#30340;&#39033;&#25968;&#26080;&#20851;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;SAG&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20123;&#26631;&#20934;&#20248;&#21270;&#22120;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent growth of theoretical studies and empirical successes of neural networks, gradient backpropagation is still the most widely used algorithm for training such networks. On the one hand, we have deterministic or full gradient (FG) approaches that have a cost proportional to the amount of training data used but have a linear convergence rate, and on the other hand, stochastic gradient (SG) methods that have a cost independent of the size of the dataset, but have a less optimal convergence rate than the determinist approaches. To combine the cost of the stochastic approach with the convergence rate of the deterministic approach, a stochastic average gradient (SAG) has been proposed. SAG is a method for optimizing the sum of a finite number of smooth convex functions. Like SG methods, the SAG method's iteration cost is independent of the number of terms in the sum. In this work, we propose to compare SAG to some standard optimizers used in machine learning. SAG converges f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#35821;&#20041;&#24178;&#25200;&#28040;&#38500;&#65288;SemantIC&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;6G&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#30340;&#20449;&#24687;&#36136;&#37327;&#12290;&#36890;&#36807;&#22312;&#25509;&#25910;&#22120;&#19978;&#20351;&#29992;&#35821;&#20041;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;SemantIC&#33021;&#22815;&#36845;&#20195;&#28040;&#38500;&#20449;&#21495;&#20013;&#30340;&#22122;&#22768;&#21644;&#35821;&#20041;&#24178;&#25200;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;SemantIC&#33021;&#22815;&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#20449;&#36947;&#36164;&#28304;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.12768</link><description>&lt;p&gt;
SemantIC: &#35821;&#20041;&#24178;&#25200;&#28040;&#38500;&#22312;6G&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SemantIC: Semantic Interference Cancellation Towards 6G Wireless Communications. (arXiv:2310.12768v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#35821;&#20041;&#24178;&#25200;&#28040;&#38500;&#65288;SemantIC&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;6G&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#30340;&#20449;&#24687;&#36136;&#37327;&#12290;&#36890;&#36807;&#22312;&#25509;&#25910;&#22120;&#19978;&#20351;&#29992;&#35821;&#20041;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;SemantIC&#33021;&#22815;&#36845;&#20195;&#28040;&#38500;&#20449;&#21495;&#20013;&#30340;&#22122;&#22768;&#21644;&#35821;&#20041;&#24178;&#25200;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;SemantIC&#33021;&#22815;&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#20449;&#36947;&#36164;&#28304;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25239;&#24178;&#25200;&#25216;&#26415;&#65292;&#21363;&#35821;&#20041;&#24178;&#25200;&#28040;&#38500;&#65288;SemantIC&#65289;&#65292;&#29992;&#20110;&#25552;&#39640;&#31532;&#20845;&#20195;&#65288;6G&#65289;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#36136;&#37327;&#12290;SemantIC&#21482;&#38656;&#35201;&#25509;&#25910;&#22120;&#23558;&#20449;&#36947;&#35299;&#30721;&#22120;&#19982;&#35821;&#20041;&#33258;&#21160;&#32534;&#30721;&#22120;&#36830;&#25509;&#36215;&#26469;&#12290;&#36825;&#26500;&#24314;&#20102;&#19968;&#20010;&#36845;&#20195;&#24490;&#29615;&#65292;&#20132;&#26367;&#28040;&#38500;&#20449;&#21495;&#22495;&#21644;&#35821;&#20041;&#22495;&#20013;&#30340;&#22122;&#22768;&#12290;&#20174;&#32593;&#32476;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#35821;&#20041;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#35757;&#32451;&#23384;&#20648;&#20102;&#36741;&#21161;&#20449;&#24687;&#65292;&#24182;&#22312;&#36845;&#20195;&#35299;&#30721;&#20013;&#25552;&#20379;&#36741;&#21161;&#20449;&#24687;&#65292;&#20316;&#20026;Wyner-Ziv&#23450;&#29702;&#30340;&#19968;&#31181;&#23454;&#29616;&#12290;&#20223;&#30495;&#32467;&#26524;&#39564;&#35777;&#20102;SemantIC&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#20449;&#36947;&#36164;&#28304;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This letter proposes a novel anti-interference technique, semantic interference cancellation (SemantIC), for enhancing information quality towards the sixth-generation (6G) wireless networks. SemantIC only requires the receiver to concatenate the channel decoder with a semantic auto-encoder. This constructs a turbo loop which iteratively and alternately eliminates noise in the signal domain and the semantic domain. From the viewpoint of network information theory, the neural network of the semantic auto-encoder stores side information by training, and provides side information in iterative decoding, as an implementation of the Wyner-Ziv theorem. Simulation results verify the performance improvement by SemantIC without extra channel resource cost.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer-based&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#20307;&#27861;&#24459;&#24418;&#24335;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#27604;&#36739;&#20013;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#24615;&#33021;&#24182;&#24471;&#21040;&#20102;&#31532;&#19977;&#26041;&#35780;&#23457;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2310.12766</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#23454;&#20307;&#27861;&#24459;&#24418;&#24335;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Entity Legal Form Classification. (arXiv:2310.12766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12766
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer-based&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#20307;&#27861;&#24459;&#24418;&#24335;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#27604;&#36739;&#20013;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#24615;&#33021;&#24182;&#24471;&#21040;&#20102;&#31532;&#19977;&#26041;&#35780;&#23457;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#23545;&#21407;&#22987;&#27861;&#24459;&#23454;&#20307;&#21517;&#31216;&#36827;&#34892;&#23454;&#20307;&#27861;&#24459;&#24418;&#24335;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21508;&#31181;BERT&#21464;&#31181;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#24615;&#33021;&#19982;&#22810;&#20010;&#20256;&#32479;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#28085;&#30422;&#20102;&#19968;&#20010;&#24222;&#22823;&#30340;&#33258;&#30001;&#21487;&#29992;&#30340;&#27861;&#24459;&#23454;&#20307;&#26631;&#35782;&#31526;&#65288;LEI&#65289;&#25968;&#25454;&#23376;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;30&#20010;&#19981;&#21516;&#27861;&#24459;&#21496;&#27861;&#36758;&#21306;&#30340;&#36229;&#36807;110&#19975;&#20010;&#27861;&#24459;&#23454;&#20307;&#12290;&#27599;&#20010;&#21496;&#27861;&#36758;&#21306;&#30340;&#20998;&#31867;&#30340;&#30495;&#23454;&#26631;&#31614;&#26469;&#33258;&#23454;&#20307;&#27861;&#24459;&#24418;&#24335;&#65288;ELF&#65289;&#20195;&#30721;&#26631;&#20934;&#65288;ISO 20275&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;BERT&#21464;&#31181;&#22312;F1&#20998;&#25968;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#22312;Macro F1&#20998;&#25968;&#26041;&#38754;&#34920;&#29616;&#30456;&#24403;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25552;&#26696;&#24471;&#21040;&#20102;&#22312;&#21313;&#20010;&#36873;&#25321;&#30340;&#21496;&#27861;&#36758;&#21306;&#36827;&#34892;&#30340;&#31532;&#19977;&#26041;&#19987;&#23478;&#35780;&#23457;&#30340;&#25903;&#25345;&#12290;&#26412;&#30740;&#31350;&#20984;&#26174;&#20102;&#22522;&#20110;Transformer&#27169;&#22411;&#22312;&#25512;&#36827;&#25968;&#25454;&#26631;&#20934;&#21270;&#26041;&#38754;&#30340;&#37325;&#35201;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the application of Transformer-based language models for classifying entity legal forms from raw legal entity names. Specifically, we employ various BERT variants and compare their performance against multiple traditional baselines. Our evaluation encompasses a substantial subset of freely available Legal Entity Identifier (LEI) data, comprising over 1.1 million legal entities from 30 different legal jurisdictions. The ground truth labels for classification per jurisdiction are taken from the Entity Legal Form (ELF) code standard (ISO 20275). Our findings demonstrate that pre-trained BERT variants outperform traditional text classification approaches in terms of F1 score, while also performing comparably well in the Macro F1 Score. Moreover, the validity of our proposal is supported by the outcome of third-party expert reviews conducted in ten selected jurisdictions. This study highlights the significant potential of Transformer-based models in advancing data standardization
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21363;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411; (EBMs)&#65292;&#36890;&#36807;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#36827;&#34892;&#35757;&#32451;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992; Langevin MCMC &#36827;&#34892;&#37319;&#26679;&#65292;&#24182;&#22312;&#35821;&#38899;&#21512;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.12765</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Models For Speech Synthesis. (arXiv:2310.12765v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21363;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411; (EBMs)&#65292;&#36890;&#36807;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#36827;&#34892;&#35757;&#32451;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992; Langevin MCMC &#36827;&#34892;&#37319;&#26679;&#65292;&#24182;&#22312;&#35821;&#38899;&#21512;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#38750;&#33258;&#22238;&#24402; (non-AR) &#27169;&#22411;&#22312;&#35821;&#38899;&#21512;&#25104;&#39046;&#22495;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#27604;&#22914; FastSpeech 2 &#21644;&#25193;&#25955;&#27169;&#22411;&#12290;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#20043;&#38388;&#27809;&#26377;&#33258;&#22238;&#24402;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#20351;&#24471;&#25512;&#26029;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411; (EBMs) &#26469;&#25299;&#23637;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#33539;&#22260;&#12290;&#35770;&#25991;&#25551;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#26469;&#35757;&#32451;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#20381;&#36182;&#20110;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#29983;&#25104;&#26377;&#25928;&#36127;&#26679;&#26412;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#20351;&#29992;&#34920;&#29616;&#20248;&#31168;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#35770;&#25991;&#36824;&#25551;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992; Langevin Markov Chain Monte-Carlo (MCMC) &#20174;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#20351;&#29992; Langevin MCMC &#21487;&#20197;&#24314;&#31435;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#19982;&#24403;&#21069;&#27969;&#34892;&#30340;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#22312; LJSpeech &#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110; Tacotron 2 &#26377;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently there has been a lot of interest in non-autoregressive (non-AR) models for speech synthesis, such as FastSpeech 2 and diffusion models. Unlike AR models, these models do not have autoregressive dependencies among outputs which makes inference efficient. This paper expands the range of available non-AR models with another member called energy-based models (EBMs). The paper describes how noise contrastive estimation, which relies on the comparison between positive and negative samples, can be used to train EBMs. It proposes a number of strategies for generating effective negative samples, including using high-performing AR models. It also describes how sampling from EBMs can be performed using Langevin Markov Chain Monte-Carlo (MCMC). The use of Langevin MCMC enables to draw connections between EBMs and currently popular diffusion models. Experiments on LJSpeech dataset show that the proposed approach offers improvements over Tacotron 2.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38750;&#21551;&#21457;&#24335;&#31639;&#27861;&#23558;&#35889;&#32858;&#31867;&#30340;&#26494;&#24347;&#35299;&#31163;&#25955;&#21270;&#65292;&#20197;&#23547;&#25214;&#26368;&#23567;&#21270;&#21407;&#22987;&#30446;&#26631;&#20989;&#25968;&#30340;&#31163;&#25955;&#35299;&#65292;&#24182;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36830;&#32493;&#26368;&#20248;&#35299;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.12752</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#21551;&#21457;&#24335;&#31639;&#27861;&#23558;&#35889;&#32858;&#31867;&#30340;&#26494;&#24347;&#35299;&#31163;&#25955;&#21270;
&lt;/p&gt;
&lt;p&gt;
Discretize Relaxed Solution of Spectral Clustering via a Non-Heuristic Algorithm. (arXiv:2310.12752v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12752
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38750;&#21551;&#21457;&#24335;&#31639;&#27861;&#23558;&#35889;&#32858;&#31867;&#30340;&#26494;&#24347;&#35299;&#31163;&#25955;&#21270;&#65292;&#20197;&#23547;&#25214;&#26368;&#23567;&#21270;&#21407;&#22987;&#30446;&#26631;&#20989;&#25968;&#30340;&#31163;&#25955;&#35299;&#65292;&#24182;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36830;&#32493;&#26368;&#20248;&#35299;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35889;&#32858;&#31867;&#21450;&#20854;&#25193;&#23637;&#36890;&#24120;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#65288;1&#65289;&#26500;&#24314;&#22270;&#24182;&#35745;&#31639;&#26494;&#24347;&#35299;&#65307;&#65288;2&#65289;&#31163;&#25955;&#21270;&#26494;&#24347;&#35299;&#12290;&#23613;&#31649;&#21069;&#32773;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#31163;&#25955;&#21270;&#25216;&#26415;&#20027;&#35201;&#26159;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#22914;k-means&#65292;&#35889;&#26059;&#36716;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#30446;&#26631;&#24182;&#38750;&#23547;&#25214;&#20351;&#21407;&#22987;&#30446;&#26631;&#20989;&#25968;&#26368;&#23567;&#21270;&#30340;&#31163;&#25955;&#35299;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#20027;&#35201;&#32570;&#28857;&#26159;&#22312;&#35745;&#31639;&#31163;&#25955;&#35299;&#26102;&#24573;&#30053;&#20102;&#21407;&#22987;&#30446;&#26631;&#12290;&#21463;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24320;&#21457;&#31532;&#19968;&#38454;&#39033;&#26469;&#36830;&#25509;&#21407;&#22987;&#38382;&#39064;&#21644;&#31163;&#25955;&#21270;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#36825;&#26159;&#38750;&#21551;&#21457;&#24335;&#26041;&#27861;&#20013;&#30340;&#39318;&#20010;&#12290;&#30001;&#20110;&#38750;&#21551;&#21457;&#24335;&#26041;&#27861;&#24847;&#35782;&#21040;&#20102;&#21407;&#22987;&#22270;&#21106;&#38382;&#39064;&#65292;&#26368;&#32456;&#31163;&#25955;&#35299;&#30340;&#21487;&#38752;&#24615;&#26356;&#39640;&#65292;&#21487;&#20197;&#36798;&#21040;&#36739;&#22909;&#30340;&#25439;&#22833;&#20540;&#12290;&#25105;&#20204;&#36824;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36830;&#32493;&#26368;&#20248;&#35299;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral clustering and its extensions usually consist of two steps: (1) constructing a graph and computing the relaxed solution; (2) discretizing relaxed solutions. Although the former has been extensively investigated, the discretization techniques are mainly heuristic methods, e.g., k-means, spectral rotation. Unfortunately, the goal of the existing methods is not to find a discrete solution that minimizes the original objective. In other words, the primary drawback is the neglect of the original objective when computing the discrete solution. Inspired by the first-order optimization algorithms, we propose to develop a first-order term to bridge the original problem and discretization algorithm, which is the first non-heuristic to the best of our knowledge. Since the non-heuristic method is aware of the original graph cut problem, the final discrete solution is more reliable and achieves the preferable loss value. We also theoretically show that the continuous optimum is beneficial 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#30340;Tabula&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#24037;&#20855;&#65292;&#36890;&#36807;&#30740;&#31350;&#25105;&#20204;&#21457;&#29616;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35774;&#35745;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#26041;&#38754;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.12746</link><description>&lt;p&gt;
TabuLa: &#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
TabuLa: Harnessing Language Models for Tabular Data Synthesis. (arXiv:2310.12746v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#30340;Tabula&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#24037;&#20855;&#65292;&#36890;&#36807;&#30740;&#31350;&#25105;&#20204;&#21457;&#29616;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35774;&#35745;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#26041;&#38754;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#34920;&#26684;&#25968;&#25454;&#22312;&#21508;&#34892;&#19994;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#20197;&#21450;&#23545;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#34920;&#26126;&#65292;&#21487;&#20197;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#29983;&#25104;&#36924;&#30495;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;&#30001;&#20110;LLMs&#23558;&#34920;&#26684;&#25968;&#25454;&#39044;&#22788;&#29702;&#20026;&#20840;&#25991;&#65292;&#23427;&#20204;&#20855;&#26377;&#36991;&#20813;&#39640;&#32500;&#24230;&#25968;&#25454;&#30340;&#29420;&#28909;&#32534;&#30721;&#25152;&#24102;&#26469;&#30340;&#32500;&#24230;&#28798;&#38590;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#35757;&#32451;&#26102;&#38388;&#38271;&#12289;&#22312;&#26032;&#20219;&#21153;&#19978;&#30340;&#21487;&#37325;&#29992;&#24615;&#26377;&#38480;&#65292;&#20351;&#24471;&#23427;&#20204;&#26080;&#27861;&#21462;&#20195;&#29616;&#26377;&#30340;&#34920;&#26684;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#30340;Tabula&#65292;&#19968;&#31181;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#22120;&#12290;&#36890;&#36807;Tabula&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#30340;&#32972;&#26223;&#19979;&#65292;&#37319;&#29992;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#35774;&#35745;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#19987;&#38376;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#23450;&#21046;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the ubiquitous use of tabular data in industries and the growing concerns in data privacy and security, tabular data synthesis emerges as a critical research area. The recent state-of-the-art methods show that large language models (LLMs) can be adopted to generate realistic tabular data. As LLMs pre-process tabular data as full text, they have the advantage of avoiding the curse of dimensionality associated with one-hot encoding high-dimensional data. However, their long training time and limited re-usability on new tasks prevent them from replacing exiting tabular generative models. In this paper, we propose Tabula, a tabular data synthesizer based on the language model structure. Through Tabula, we demonstrate the inherent limitation of employing pre-trained language models designed for natural language processing (NLP) in the context of tabular data synthesis. Our investigation delves into the development of a dedicated foundational model tailored specifically for tabular dat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35268;&#33539;&#21270;&#27491;&#24577;&#27969;&#26041;&#27861;&#65292;&#29992;&#20110;&#27969;&#24418;&#23398;&#20064;&#12290;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#21487;&#36870;&#21464;&#25442;&#23558;&#25968;&#25454;&#23884;&#20837;&#21040;&#39640;&#32500;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#27969;&#24418;&#19978;&#35745;&#31639;&#27010;&#29575;&#23494;&#24230;&#24182;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#21040;&#30340;&#27969;&#24418;&#34920;&#31034;&#20013;&#23384;&#22312;&#30528;&#19982;&#27969;&#24418;&#20851;&#32852;&#19988;&#36864;&#21270;&#30340;&#20869;&#22312;&#22522;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12743</link><description>&lt;p&gt;
&#27969;&#24418;&#23398;&#20064;&#30340;&#35268;&#33539;&#21270;&#27491;&#24577;&#27969;
&lt;/p&gt;
&lt;p&gt;
Canonical normalizing flows for manifold learning. (arXiv:2310.12743v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12743
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35268;&#33539;&#21270;&#27491;&#24577;&#27969;&#26041;&#27861;&#65292;&#29992;&#20110;&#27969;&#24418;&#23398;&#20064;&#12290;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#21487;&#36870;&#21464;&#25442;&#23558;&#25968;&#25454;&#23884;&#20837;&#21040;&#39640;&#32500;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#27969;&#24418;&#19978;&#35745;&#31639;&#27010;&#29575;&#23494;&#24230;&#24182;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#21040;&#30340;&#27969;&#24418;&#34920;&#31034;&#20013;&#23384;&#22312;&#30528;&#19982;&#27969;&#24418;&#20851;&#32852;&#19988;&#36864;&#21270;&#30340;&#20869;&#22312;&#22522;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#23398;&#20064;&#27969;&#26159;&#19968;&#31867;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#65292;&#20551;&#35774;&#25968;&#25454;&#20855;&#26377;&#20302;&#32500;&#27969;&#24418;&#25551;&#36848;&#12290;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#21487;&#36870;&#21464;&#25442;&#23558;&#36825;&#31181;&#27969;&#24418;&#23884;&#20837;&#21040;&#25968;&#25454;&#30340;&#39640;&#32500;&#31354;&#38388;&#20013;&#12290;&#22240;&#27492;&#65292;&#19968;&#26086;&#36890;&#36807;&#37325;&#26500;&#25439;&#22833;&#27491;&#30830;&#23545;&#40784;&#27969;&#24418;&#65292;&#27969;&#24418;&#19978;&#30340;&#27010;&#29575;&#23494;&#24230;&#23601;&#26159;&#21487;&#35745;&#31639;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#26469;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#12290;&#33258;&#28982;&#22320;&#65292;&#25968;&#25454;&#30340;&#20302;&#32500;&#34920;&#31034;&#38656;&#35201;&#26159;&#21333;&#23556;&#26144;&#23556;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#24314;&#27169;&#30340;&#27969;&#24418;&#19978;&#23545;&#23494;&#24230;&#36827;&#34892;&#23545;&#20934;&#65292;&#24182;&#22312;&#23884;&#20837;&#21040;&#39640;&#32500;&#31354;&#38388;&#26102;&#39640;&#25928;&#35745;&#31639;&#23494;&#24230;&#20307;&#31215;&#21464;&#21270;&#39033;&#12290;&#28982;&#32780;&#65292;&#38500;&#38750;&#21333;&#23556;&#26144;&#23556;&#22312;&#35299;&#26512;&#19978;&#39044;&#23450;&#20041;&#65292;&#21542;&#21017;&#23398;&#20064;&#21040;&#30340;&#27969;&#24418;&#19981;&#19968;&#23450;&#26159;&#25968;&#25454;&#30340;&#26377;&#25928;&#34920;&#31034;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#36825;&#31181;&#27169;&#22411;&#30340;&#28508;&#22312;&#32500;&#24230;&#32463;&#24120;&#20250;&#23398;&#20064;&#21040;&#19982;&#27969;&#24418;&#30456;&#20851;&#24182;&#19988;&#36864;&#21270;&#30340;&#20869;&#22312;&#22522;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manifold learning flows are a class of generative modelling techniques that assume a low-dimensional manifold description of the data. The embedding of such manifold into the high-dimensional space of the data is achieved via learnable invertible transformations. Therefore, once the manifold is properly aligned via a reconstruction loss, the probability density is tractable on the manifold and maximum likelihood can be used optimize the network parameters. Naturally, the lower-dimensional representation of the data requires an injective-mapping. Recent approaches were able to enforce that density aligns with the modelled manifold, while efficiently calculating the density volume-change term when embedding to the higher-dimensional space. However, unless the injective-mapping is analytically predefined, the learned manifold is not necessarily an efficient representation of the data. Namely, the latent dimensions of such models frequently learn an entangled intrinsic basis with degenerat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20195;&#29702;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#30446;&#26631;&#27169;&#22411;&#30340;&#21382;&#21490;&#29366;&#24577;&#20316;&#20026;&#20195;&#29702;&#26469;&#22686;&#24378;&#27169;&#22411;&#23545;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12713</link><description>&lt;p&gt;
&#20174;&#36807;&#21435;&#23398;&#20064;&#65306;&#22522;&#20110;&#20195;&#29702;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26694;&#26550;&#26469;&#22686;&#24378;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learn from the Past: A Proxy based Adversarial Defense Framework to Boost Robustness. (arXiv:2310.12713v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20195;&#29702;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#30446;&#26631;&#27169;&#22411;&#30340;&#21382;&#21490;&#29366;&#24577;&#20316;&#20026;&#20195;&#29702;&#26469;&#22686;&#24378;&#27169;&#22411;&#23545;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#23545;&#25239;&#26679;&#26412;&#30340;&#33030;&#24369;&#24615;&#21450;&#20854;&#24102;&#26469;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#21253;&#25324;&#31361;&#20986;&#20195;&#34920;&#24615;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#65292;&#26088;&#22312;&#22686;&#24378;&#27169;&#22411;&#23545;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#26159;&#36890;&#36807;&#26126;&#30830;&#25110;&#38544;&#24615;&#30340;&#35745;&#31639;&#36127;&#25285;&#24110;&#21161;&#30446;&#26631;&#27169;&#22411;&#30340;&#24403;&#21069;&#29366;&#24577;&#26469;&#38450;&#24481;&#38754;&#21521;&#21442;&#25968;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#21516;&#26102;&#30001;&#20110;&#20248;&#21270;&#36712;&#36857;&#19981;&#19968;&#33268;&#32780;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#26412;&#25991;&#37325;&#26032;&#32771;&#34385;&#20102;&#30446;&#26631;&#27169;&#22411;&#30340;&#26356;&#26032;&#35268;&#21017;&#21450;&#20854;&#24403;&#21069;&#29366;&#24577;&#19979;&#30340;&#38450;&#24481;&#19981;&#36275;&#12290;&#36890;&#36807;&#24341;&#20837;&#30446;&#26631;&#27169;&#22411;&#30340;&#21382;&#21490;&#29366;&#24577;&#20316;&#20026;&#20195;&#29702;&#65292;&#24182;&#36171;&#20104;&#20854;&#29992;&#20110;&#38450;&#24481;&#30340;&#20808;&#39564;&#20449;&#24687;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26356;&#26032;&#35268;&#21017;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#36890;&#29992;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26694;&#26550;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;"LAST"&#12290;
&lt;/p&gt;
&lt;p&gt;
In light of the vulnerability of deep learning models to adversarial samples and the ensuing security issues, a range of methods, including Adversarial Training (AT) as a prominent representative, aimed at enhancing model robustness against various adversarial attacks, have seen rapid development. However, existing methods essentially assist the current state of target model to defend against parameter-oriented adversarial attacks with explicit or implicit computation burdens, which also suffers from unstable convergence behavior due to inconsistency of optimization trajectories. Diverging from previous work, this paper reconsiders the update rule of target model and corresponding deficiency to defend based on its current state. By introducing the historical state of the target model as a proxy, which is endowed with much prior information for defense, we formulate a two-stage update rule, resulting in a general adversarial defense framework, which we refer to as `LAST' ({\bf L}earn fr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Cosmos&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#24314;&#27169;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#21270;&#22522;&#30784;&#21644;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#22330;&#26223;&#19978;&#30340;&#39640;&#24615;&#33021;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12690</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#21270;&#22522;&#30784;&#19978;&#30340;&#32452;&#21512;&#24335;&#19990;&#30028;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Neurosymbolic Grounding for Compositional World Models. (arXiv:2310.12690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Cosmos&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#24314;&#27169;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#21270;&#22522;&#30784;&#21644;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#22330;&#26223;&#19978;&#30340;&#39640;&#24615;&#33021;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Cosmos&#65292;&#19968;&#20010;&#38024;&#23545;&#32452;&#21512;&#27867;&#21270;&#65288;CG&#65289;&#35774;&#35745;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#19990;&#30028;&#24314;&#27169;&#26694;&#26550;&#65292;&#21363;&#22312;&#36890;&#36807;&#24050;&#30693;&#30340;&#35270;&#35273;&#8220;&#21407;&#23376;&#8221;&#32452;&#21512;&#33719;&#24471;&#30340;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#22330;&#26223;&#19978;&#20855;&#26377;&#39640;&#24615;&#33021;&#12290;Cosmos&#30340;&#26680;&#24515;&#27934;&#23519;&#21147;&#26159;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#22522;&#30784;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#24037;&#20855;&#65306;&#65288;i&#65289;&#31070;&#32463;&#31526;&#21495;&#21270;&#22330;&#26223;&#32534;&#30721;&#65292;&#20351;&#29992;&#31070;&#32463;&#32534;&#30721;&#22120;&#35745;&#31639;&#27599;&#20010;&#22330;&#26223;&#20013;&#30340;&#23454;&#20307;&#30340;&#23454;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#25551;&#36848;&#23454;&#20307;&#23646;&#24615;&#30340;&#21487;&#32452;&#21512;&#31526;&#21495;&#21521;&#37327;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#31070;&#32463;&#31526;&#21495;&#21270;&#27880;&#24847;&#26426;&#21046;&#65292;&#23558;&#36825;&#20123;&#23454;&#20307;&#19982;&#23398;&#20064;&#21040;&#30340;&#20132;&#20114;&#35268;&#21017;&#32465;&#23450;&#36215;&#26469;&#12290;Cosmos&#26159;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;&#65307;&#27492;&#22806;&#65292;&#19982;&#20256;&#32479;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#38656;&#35201;&#25163;&#21160;&#23558;&#34920;&#31034;&#26144;&#23556;&#20026;&#31526;&#21495;&#19981;&#21516;&#65292;&#23427;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#35745;&#31639;&#23454;&#20307;&#30340;&#31526;&#21495;&#23646;&#24615;&#12290;&#36890;&#36807;&#23545;&#24050;&#24314;&#31435;&#30340;blocks&#22330;&#26223;&#36827;&#34892;&#20004;&#31181;&#19981;&#21516;&#24418;&#24335;&#30340;CG&#35780;&#20272;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;Cosmos&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Cosmos, a framework for object-centric world modeling that is designed for compositional generalization (CG), i.e., high performance on unseen input scenes obtained through the composition of known visual "atoms." The central insight behind Cosmos is the use of a novel form of neurosymbolic grounding. Specifically, the framework introduces two new tools: (i) neurosymbolic scene encodings, which represent each entity in a scene using a real vector computed using a neural encoder, as well as a vector of composable symbols describing attributes of the entity, and (ii) a neurosymbolic attention mechanism that binds these entities to learned rules of interaction. Cosmos is end-to-end differentiable; also, unlike traditional neurosymbolic methods that require representations to be manually mapped to symbols, it computes an entity's symbolic attributes using vision-language foundation models. Through an evaluation that considers two different forms of CG on an established blocks-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Rank-Tuning&#30340;&#35757;&#32451;&#21518;&#31209;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#39640;&#25928;&#21387;&#32553;&#27169;&#22411;&#65292;&#24182;&#22312;&#20960;&#20046;&#27809;&#26377;&#24615;&#33021;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#21387;&#32553;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.12688</link><description>&lt;p&gt;
&#20351;&#29992;&#30697;&#38453;&#22240;&#24335;&#20998;&#35299;&#21387;&#32553;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Compression of Recurrent Neural Networks using Matrix Factorization. (arXiv:2310.12688v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Rank-Tuning&#30340;&#35757;&#32451;&#21518;&#31209;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#39640;&#25928;&#21387;&#32553;&#27169;&#22411;&#65292;&#24182;&#22312;&#20960;&#20046;&#27809;&#26377;&#24615;&#33021;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#21387;&#32553;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#26102;&#25110;&#23884;&#20837;&#24335;&#24212;&#29992;&#20013;&#37096;&#32626;&#27169;&#22411;&#26102;&#65292;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#20351;&#29992;&#20302;&#31209;&#36817;&#20284;&#23545;&#27169;&#22411;&#30340;&#30697;&#38453;&#36827;&#34892;&#20998;&#35299;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#21387;&#32553;&#26041;&#27861;&#12290;&#34429;&#28982;&#22312;&#35757;&#32451;&#20043;&#21069;&#21487;&#20197;&#35774;&#32622;&#31209;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#26082;&#19981;&#28789;&#27963;&#20063;&#19981;&#26368;&#20248;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Rank-Tuning&#30340;&#35757;&#32451;&#21518;&#31209;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#27599;&#20010;&#30697;&#38453;&#36873;&#25321;&#19981;&#21516;&#30340;&#31209;&#12290;&#32467;&#21512;&#35757;&#32451;&#36866;&#24212;&#24615;&#30340;&#20351;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20046;&#27809;&#26377;&#24615;&#33021;&#38477;&#20302;&#25110;&#32773;&#26377;&#24456;&#23569;&#24615;&#33021;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#39640;&#21387;&#32553;&#29575;&#12290;&#25105;&#20204;&#22312;&#20449;&#21495;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#33267;&#26368;&#22810;14&#20493;&#65292;&#19988;&#30456;&#23545;&#24615;&#33021;&#38477;&#20302;&#26368;&#22810;&#20026;1.4%&#12290;
&lt;/p&gt;
&lt;p&gt;
Compressing neural networks is a key step when deploying models for real-time or embedded applications. Factorizing the model's matrices using low-rank approximations is a promising method for achieving compression. While it is possible to set the rank before training, this approach is neither flexible nor optimal. In this work, we propose a post-training rank-selection method called Rank-Tuning that selects a different rank for each matrix. Used in combination with training adaptations, our method achieves high compression rates with no or little performance degradation. Our numerical experiments on signal processing tasks show that we can compress recurrent neural networks up to 14x with at most 1.4% relative performance reduction.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#22312;&#20248;&#21270;&#21644;&#27867;&#21270;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#25512;&#23548;&#20102;&#21333;&#23618;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#25910;&#25947;&#24615;&#21644;&#27867;&#21270;&#20445;&#35777;&#65292;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#35789;&#28151;&#21512;&#27169;&#22411;&#65292;&#21021;&#22987;&#21270;&#26465;&#20214;&#28385;&#36275;&#21487;&#23454;&#29616;&#24615;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2310.12680</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#20248;&#21270;&#19982;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
On the Optimization and Generalization of Multi-head Attention. (arXiv:2310.12680v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#22312;&#20248;&#21270;&#21644;&#27867;&#21270;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#25512;&#23548;&#20102;&#21333;&#23618;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#25910;&#25947;&#24615;&#21644;&#27867;&#21270;&#20445;&#35777;&#65292;&#24182;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#35789;&#28151;&#21512;&#27169;&#22411;&#65292;&#21021;&#22987;&#21270;&#26465;&#20214;&#28385;&#36275;&#21487;&#23454;&#29616;&#24615;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26680;&#24515;&#26426;&#21046;&#8212;&#8212;Attention&#26426;&#21046;&#30340;&#35757;&#32451;&#21644;&#27867;&#21270;&#21160;&#24577;&#20173;&#26410;&#28145;&#20837;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#20998;&#26512;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#22836;&#27880;&#24847;&#21147;&#19978;&#12290;&#21463;&#21040;&#20840;&#36830;&#25509;&#32593;&#32476;&#35757;&#32451;&#26102;&#36807;&#21442;&#25968;&#21270;&#30340;&#30410;&#22788;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#28508;&#22312;&#20248;&#21270;&#21644;&#27867;&#21270;&#20248;&#21183;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#25968;&#25454;&#30340;&#36866;&#24403;&#21487;&#23454;&#29616;&#24615;&#26465;&#20214;&#19979;&#65292;&#25512;&#23548;&#20986;&#21333;&#23618;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#25910;&#25947;&#24615;&#21644;&#27867;&#21270;&#20445;&#35777;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24314;&#31435;&#36215;&#21021;&#22987;&#21270;&#26102;&#30830;&#20445;&#21487;&#23454;&#29616;&#24615;&#24471;&#21040;&#28385;&#36275;&#30340;&#22522;&#26412;&#26465;&#20214;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#26465;&#20214;&#36866;&#29992;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#35789;&#28151;&#21512;&#27169;&#22411;&#12290;&#25105;&#20204;&#26399;&#26395;&#36825;&#20010;&#20998;&#26512;&#21487;&#20197;&#25193;&#23637;&#21040;&#21508;&#31181;&#25968;&#25454;&#27169;&#22411;&#21644;&#26550;&#26500;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training and generalization dynamics of the Transformer's core mechanism, namely the Attention mechanism, remain under-explored. Besides, existing analyses primarily focus on single-head attention. Inspired by the demonstrated benefits of overparameterization when training fully-connected networks, we investigate the potential optimization and generalization advantages of using multiple attention heads. Towards this goal, we derive convergence and generalization guarantees for gradient-descent training of a single-layer multi-head self-attention model, under a suitable realizability condition on the data. We then establish primitive conditions on the initialization that ensure realizability holds. Finally, we demonstrate that these conditions are satisfied for a simple tokenized-mixture model. We expect the analysis can be extended to various data-model and architecture variations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#23545;&#39057;&#29575;-&#20005;&#37325;&#24615;&#20445;&#38505;&#23450;&#20215;&#36827;&#34892;&#20102;&#22522;&#20934;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#31934;&#31639;&#31070;&#32463;&#32593;&#32476;(CANN)&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.12671</link><description>&lt;p&gt;
&#21033;&#29992;&#39057;&#29575;&#21644;&#20005;&#37325;&#24615;&#25968;&#25454;&#36827;&#34892;&#20445;&#38505;&#23450;&#20215;&#30340;&#31070;&#32463;&#32593;&#32476;&#65306;&#20174;&#25968;&#25454;&#39044;&#22788;&#29702;&#21040;&#25216;&#26415;&#23450;&#20215;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neural networks for insurance pricing with frequency and severity data: a benchmark study from data preprocessing to technical tariff. (arXiv:2310.12671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#23545;&#39057;&#29575;-&#20005;&#37325;&#24615;&#20445;&#38505;&#23450;&#20215;&#36827;&#34892;&#20102;&#22522;&#20934;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#31934;&#31639;&#31070;&#32463;&#32593;&#32476;(CANN)&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#38505;&#20844;&#21496;&#36890;&#24120;&#20351;&#29992;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#26469;&#24314;&#27169;&#32034;&#36180;&#30340;&#39057;&#29575;&#21644;&#20005;&#37325;&#24615;&#25968;&#25454;&#12290;&#30001;&#20110;&#20854;&#22312;&#20854;&#20182;&#39046;&#22495;&#30340;&#25104;&#21151;&#65292;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#31934;&#31639;&#24037;&#20855;&#31665;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#26412;&#25991;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#20026;&#39057;&#29575;-&#20005;&#37325;&#24615;&#20445;&#38505;&#23450;&#20215;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#25991;&#29486;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#20445;&#38505;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#30740;&#31350;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#21547;&#26377;&#22810;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#21644;&#39057;&#29575;-&#20005;&#37325;&#24615;&#30446;&#26631;&#12290;&#25105;&#20204;&#35814;&#32454;&#27604;&#36739;&#20102;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#22312;&#20998;&#31665;&#36755;&#20837;&#25968;&#25454;&#12289;&#26799;&#24230;&#25552;&#21319;&#26641;&#27169;&#22411;&#12289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FFNN&#65289;&#21644;&#32852;&#21512;&#31934;&#31639;&#31070;&#32463;&#32593;&#32476;&#65288;CANN&#65289;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;CANN&#23558;&#36890;&#36807;GLM&#21644;GBM&#20998;&#21035;&#24314;&#31435;&#30340;&#22522;&#32447;&#39044;&#27979;&#19982;&#31070;&#32463;&#32593;&#32476;&#26657;&#27491;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#25968;&#25454;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#29305;&#21035;&#20851;&#27880;&#36890;&#24120;&#23384;&#22312;&#20110;&#34920;&#26684;&#20445;&#38505;&#25968;&#25454;&#38598;&#20013;&#30340;&#22810;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#27604;&#22914;&#37038;&#32534;&#21644;&#25968;&#23383;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Insurers usually turn to generalized linear models for modelling claim frequency and severity data. Due to their success in other fields, machine learning techniques are gaining popularity within the actuarial toolbox. Our paper contributes to the literature on frequency-severity insurance pricing with machine learning via deep learning structures. We present a benchmark study on four insurance data sets with frequency and severity targets in the presence of multiple types of input features. We compare in detail the performance of: a generalized linear model on binned input data, a gradient-boosted tree model, a feed-forward neural network (FFNN), and the combined actuarial neural network (CANN). Our CANNs combine a baseline prediction established with a GLM and GBM, respectively, with a neural network correction. We explain the data preprocessing steps with specific focus on the multiple types of input features typically present in tabular insurance data sets, such as postal codes, nu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STANLEY&#30340;&#31639;&#27861;&#29992;&#20110;&#37319;&#26679;&#39640;&#32500;&#25968;&#25454;&#65292;&#25913;&#21892;&#20102;&#33021;&#37327;&#27169;&#22411;&#23398;&#20064;&#31639;&#27861;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.12667</link><description>&lt;p&gt;
STANLEY&#65306;&#29992;&#20110;&#23398;&#20064;&#33021;&#37327;&#27169;&#22411;&#30340;&#38543;&#26426;&#26799;&#24230;&#24322;&#21521;&#25289;&#26684;&#26391;&#26085;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
STANLEY: Stochastic Gradient Anisotropic Langevin Dynamics for Learning Energy-Based Models. (arXiv:2310.12667v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STANLEY&#30340;&#31639;&#27861;&#29992;&#20110;&#37319;&#26679;&#39640;&#32500;&#25968;&#25454;&#65292;&#25913;&#21892;&#20102;&#33021;&#37327;&#27169;&#22411;&#23398;&#20064;&#31639;&#27861;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STANLEY&#30340;&#38543;&#26426;&#26799;&#24230;&#24322;&#21521;&#25289;&#26684;&#26391;&#26085;&#21160;&#21147;&#23398;&#31639;&#27861;&#65292;&#29992;&#20110;&#37319;&#26679;&#39640;&#32500;&#25968;&#25454;&#12290;&#36890;&#36807;&#22686;&#24378;&#33021;&#37327;&#27169;&#22411;&#65288;EBM&#65289;&#30340;&#23398;&#20064;&#31639;&#27861;&#26469;&#25913;&#21892;&#37319;&#26679;&#25968;&#25454;&#28857;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;EBM&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20063;&#34987;&#31216;&#20026;&#38750;&#24402;&#19968;&#21270;&#27010;&#29575;&#24314;&#27169;&#12290;&#30001;&#20110;EBMs&#30340;&#26410;&#30693;&#24402;&#19968;&#21270;&#24120;&#25968;&#23548;&#33268;&#35757;&#32451;&#36807;&#31243;&#38590;&#20197;&#22788;&#29702;&#65292;&#37319;&#29992;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#36890;&#24120;&#26159;&#21487;&#34892;&#30340;&#36873;&#25321;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#32500;&#37319;&#26679;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#32454;&#20998;&#38543;&#26426;&#36807;&#31243;&#30340;&#24322;&#21521;&#27493;&#38271;&#21644;&#26799;&#24230;&#20449;&#24687;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#25105;&#20204;&#36890;&#36807;&#35770;&#35777;&#39532;&#23572;&#31185;&#22827;&#38142;&#20013;&#36127;&#26679;&#26412;&#30340;&#24322;&#21521;&#26356;&#26032;&#30340;&#24517;&#35201;&#24615;&#26469;&#35299;&#37322;&#20102;MCMC&#22312;EBM&#35757;&#32451;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose in this paper, STANLEY, a STochastic gradient ANisotropic LangEvin dYnamics, for sampling high dimensional data. With the growing efficacy and potential of Energy-Based modeling, also known as non-normalized probabilistic modeling, for modeling a generative process of different natures of high dimensional data observations, we present an end-to-end learning algorithm for Energy-Based models (EBM) with the purpose of improving the quality of the resulting sampled data points. While the unknown normalizing constant of EBMs makes the training procedure intractable, resorting to Markov Chain Monte Carlo (MCMC) is in general a viable option. Realizing what MCMC entails for the EBM training, we propose in this paper, a novel high dimensional sampling method, based on an anisotropic stepsize and a gradient-informed covariance matrix, embedded into a discretized Langevin diffusion. We motivate the necessity for an anisotropic update of the negative samples in the Markov Chain by the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;EDL&#65289;&#20013;&#20135;&#29983;&#30340;&#26469;&#33258;&#19981;&#30830;&#23450;&#24615;&#30340;&#35777;&#25454;&#20449;&#21495;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#20449;&#21495;&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21306;&#20998;&#31867;&#21035;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;EDL&#19982;&#20854;&#20182;&#22522;&#20110;Dirichlet&#30340;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#34920;&#26126;EDL&#30340;&#8220;&#35777;&#25454;&#20449;&#21495;&#8221;&#26159;&#30001;&#20110;&#35823;&#20998;&#31867;&#20559;&#24046;&#32780;&#20135;&#29983;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.12663</link><description>&lt;p&gt;
&#20174;&#19981;&#30830;&#23450;&#24615;&#20013;&#33719;&#21462;&#30693;&#35782;&#30340;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Knowledge from Uncertainty in Evidential Deep Learning. (arXiv:2310.12663v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;EDL&#65289;&#20013;&#20135;&#29983;&#30340;&#26469;&#33258;&#19981;&#30830;&#23450;&#24615;&#30340;&#35777;&#25454;&#20449;&#21495;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#20449;&#21495;&#21487;&#20197;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21306;&#20998;&#31867;&#21035;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;EDL&#19982;&#20854;&#20182;&#22522;&#20110;Dirichlet&#30340;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#34920;&#26126;EDL&#30340;&#8220;&#35777;&#25454;&#20449;&#21495;&#8221;&#26159;&#30001;&#20110;&#35823;&#20998;&#31867;&#20559;&#24046;&#32780;&#20135;&#29983;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25581;&#31034;&#20102;&#22312;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;EDL&#65289;&#20013;&#20174;&#19981;&#30830;&#23450;&#24615;&#20540;&#20013;&#20135;&#29983;&#30340;&#35777;&#25454;&#20449;&#21495;&#12290; EDL&#26159;&#19968;&#31867;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#31034;&#20363;&#65292;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#24403;&#21069;&#27979;&#35797;&#26679;&#26412;&#30340;&#32622;&#20449;&#24230;&#65288;&#25110;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65289;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#21452;&#21521;&#32534;&#30721;&#22120;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;EDL&#20013;&#30340;Dirichlet&#24378;&#24230;&#24341;&#21457;&#30340;&#8220;&#35777;&#25454;&#20449;&#21495;&#8221;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#21306;&#20998;&#31867;&#21035;&#65292;&#23588;&#20854;&#26159;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#34920;&#29616;&#24471;&#38750;&#24120;&#24378;&#12290;&#25105;&#20204;&#20551;&#35774;KL&#27491;&#21017;&#21270;&#39033;&#23548;&#33268;EDL&#23558;aleatoric&#19981;&#30830;&#23450;&#24615;&#21644;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#32806;&#21512;&#22312;&#19968;&#36215;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#20102;&#35823;&#20998;&#31867;&#21644;&#35780;&#20272;&#19981;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;EDL&#30340;&#8220;&#35777;&#25454;&#20449;&#21495;&#8221;&#26159;&#30001;&#20110;&#35823;&#20998;&#31867;&#20559;&#24046;&#32780;&#20135;&#29983;&#30340;&#12290;&#25105;&#20204;&#25209;&#21028;&#24615;&#22320;&#35780;&#20272;&#20102;EDL&#19982;&#20854;&#20182;&#22522;&#20110;Dirichlet&#30340;&#26041;&#27861;&#65292;&#21363;&#29983;&#25104;&#24335;&#35777;&#25454;&#31070;&#32463;&#32593;&#32476;&#65288;EDL-GEN&#65289;&#21644;&#20808;&#39564;&#32593;&#32476;&#65292;&#24182;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#35777;&#26126;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work reveals an evidential signal that emerges from the uncertainty value in Evidential Deep Learning (EDL). EDL is one example of a class of uncertainty-aware deep learning approaches designed to provide confidence (or epistemic uncertainty) about the current test sample. In particular for computer vision and bidirectional encoder large language models, the `evidential signal' arising from the Dirichlet strength in EDL can, in some cases, discriminate between classes, which is particularly strong when using large language models. We hypothesise that the KL regularisation term causes EDL to couple aleatoric and epistemic uncertainty. In this paper, we empirically investigate the correlations between misclassification and evaluated uncertainty, and show that EDL's `evidential signal' is due to misclassification bias. We critically evaluate EDL with other Dirichlet-based approaches, namely Generative Evidential Neural Networks (EDL-GEN) and Prior Networks, and show theoretically and
&lt;/p&gt;</description></item><item><title>&#26799;&#24230;&#19979;&#38477;&#26080;&#27861;&#23398;&#20064;&#39640;&#39057;&#20989;&#25968;&#21644;&#27169;&#36816;&#31639;&#65292;&#35813;&#30740;&#31350;&#20026;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#25216;&#26415;&#35757;&#32451;&#39640;&#39057;&#21608;&#26399;&#20989;&#25968;&#21644;&#27169;&#20056;&#27861;&#25552;&#20379;&#20102;&#25968;&#23398;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.12660</link><description>&lt;p&gt;
&#26799;&#24230;&#19979;&#38477;&#26080;&#27861;&#23398;&#20064;&#39640;&#39057;&#20989;&#25968;&#21644;&#27169;&#36816;&#31639;
&lt;/p&gt;
&lt;p&gt;
Gradient Descent Fails to Learn High-frequency Functions and Modular Arithmetic. (arXiv:2310.12660v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12660
&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#19979;&#38477;&#26080;&#27861;&#23398;&#20064;&#39640;&#39057;&#20989;&#25968;&#21644;&#27169;&#36816;&#31639;&#65292;&#35813;&#30740;&#31350;&#20026;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#25216;&#26415;&#35757;&#32451;&#39640;&#39057;&#21608;&#26399;&#20989;&#25968;&#21644;&#27169;&#20056;&#27861;&#25552;&#20379;&#20102;&#25968;&#23398;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#19968;&#20123;&#21253;&#21547;&#22823;&#37327;&#36817;&#20284;&#27491;&#20132;&#20803;&#32032;&#30340;&#30446;&#26631;&#20989;&#25968;&#31867;&#21035;&#38590;&#20197;&#34987;&#32479;&#35745;&#26597;&#35810;&#31639;&#27861;&#23398;&#20064;&#21040;&#12290;&#26368;&#36817;&#65292;&#36825;&#19968;&#32463;&#20856;&#20107;&#23454;&#20877;&#27425;&#20986;&#29616;&#22312;&#31070;&#32463;&#32593;&#32476;&#26799;&#24230;&#20248;&#21270;&#30340;&#29702;&#35770;&#20013;&#12290;&#22312;&#36825;&#20010;&#26032;&#30340;&#26694;&#26550;&#20013;&#65292;&#19968;&#20010;&#31867;&#30340;&#38590;&#24230;&#36890;&#24120;&#30001;&#26799;&#24230;&#23545;&#38543;&#26426;&#36873;&#25321;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#26041;&#24046;&#26469;&#34913;&#37327;&#12290;&#26368;&#36817;&#65292;&#19968;&#20010;&#24418;&#24335;&#20026;$x \to ax \bmod p$&#30340;&#20989;&#25968;&#38598;&#21512;&#65292;&#20854;&#20013;$a$&#21462;&#33258;${\mathbb Z}_p$&#65292;&#24341;&#36215;&#20102;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#23478;&#21644;&#23494;&#30721;&#23398;&#23478;&#30340;&#20851;&#27880;&#12290;&#36825;&#20010;&#31867;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;${\mathbb Z}$&#19978;&#30340;$p$-&#21608;&#26399;&#20989;&#25968;&#30340;&#23376;&#38598;&#65292;&#24182;&#19988;&#19982;&#23454;&#25968;&#32447;&#19978;&#30340;&#39640;&#39057;&#21608;&#26399;&#20989;&#25968;&#31867;&#32039;&#23494;&#30456;&#20851;&#12290;&#25105;&#20204;&#23545;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#25216;&#26415;&#20174;&#31034;&#20363;&#20013;&#35757;&#32451;&#39640;&#39057;&#21608;&#26399;&#20989;&#25968;&#25110;&#27169;&#20056;&#27861;&#36827;&#34892;&#20102;&#25968;&#23398;&#20998;&#26512;&#65292;&#24182;&#24378;&#35843;&#20102;&#30456;&#20851;&#30340;&#38480;&#21046;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classes of target functions containing a large number of approximately orthogonal elements are known to be hard to learn by the Statistical Query algorithms. Recently this classical fact re-emerged in a theory of gradient-based optimization of neural networks. In the novel framework, the hardness of a class is usually quantified by the variance of the gradient with respect to a random choice of a target function.  A set of functions of the form $x\to ax \bmod p$, where $a$ is taken from ${\mathbb Z}_p$, has attracted some attention from deep learning theorists and cryptographers recently. This class can be understood as a subset of $p$-periodic functions on ${\mathbb Z}$ and is tightly connected with a class of high-frequency periodic functions on the real line.  We present a mathematical analysis of limitations and challenges associated with using gradient-based learning techniques to train a high-frequency periodic function or modular multiplication from examples. We highlight that t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27668;&#20307;&#37329;&#23646;&#30005;&#24359;&#28938;&#25509;&#39044;&#27979;&#36136;&#37327;&#31995;&#32479;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#31649;&#29702;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#20197;&#21450;&#23454;&#26102;&#22788;&#29702;&#21644;&#29305;&#24449;&#24037;&#31243;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#35782;&#21035;&#20851;&#31995;&#24182;&#39044;&#27979;&#28938;&#25509;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.12632</link><description>&lt;p&gt;
&#38754;&#21521;&#28938;&#25509;&#36807;&#31243;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22312;&#32447;&#36136;&#37327;&#39044;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards a Deep Learning-based Online Quality Prediction System for Welding Processes. (arXiv:2310.12632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12632
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27668;&#20307;&#37329;&#23646;&#30005;&#24359;&#28938;&#25509;&#39044;&#27979;&#36136;&#37327;&#31995;&#32479;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#31649;&#29702;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#20197;&#21450;&#23454;&#26102;&#22788;&#29702;&#21644;&#29305;&#24449;&#24037;&#31243;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#35782;&#21035;&#20851;&#31995;&#24182;&#39044;&#27979;&#28938;&#25509;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21046;&#36896;&#36807;&#31243;&#30340;&#25968;&#23383;&#21270;&#20026;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#36136;&#37327;&#20445;&#35777;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#24212;&#29992;&#12290;&#19968;&#20010;&#24191;&#27867;&#24212;&#29992;&#30340;&#21046;&#36896;&#36807;&#31243;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#39537;&#21160;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#21463;&#30410;&#21290;&#27973;&#65292;&#26159;&#27668;&#20307;&#37329;&#23646;&#30005;&#24359;&#28938;&#25509;&#65288;GMAW&#65289;&#12290;&#28938;&#25509;&#36807;&#31243;&#20197;&#26448;&#26009;&#24615;&#36136;&#12289;&#24037;&#33402;&#26465;&#20214;&#21644;&#28938;&#25509;&#36136;&#37327;&#20043;&#38388;&#22797;&#26434;&#30340;&#22240;&#26524;&#20851;&#31995;&#20026;&#29305;&#24449;&#12290;&#22312;&#39057;&#32321;&#26356;&#25913;&#24037;&#33402;&#21442;&#25968;&#30340;&#38750;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#30772;&#22351;&#24615;&#27979;&#35797;&#20934;&#30830;&#30830;&#23450;&#28938;&#32541;&#36136;&#37327;&#26159;&#32463;&#27982;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#28145;&#24230;&#23398;&#20064;&#25552;&#20379;&#20102;&#20174;&#24037;&#33402;&#35266;&#23519;&#20013;&#35782;&#21035;&#20851;&#31995;&#24182;&#39044;&#27979;&#28938;&#25509;&#36136;&#37327;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27668;&#20307;&#37329;&#23646;&#30005;&#24359;&#28938;&#25509;&#39044;&#27979;&#36136;&#37327;&#31995;&#32479;&#30340;&#27010;&#24565;&#12290;&#26680;&#24515;&#27010;&#24565;&#21253;&#25324;&#30001;&#22235;&#20010;&#20027;&#35201;&#38454;&#27573;&#32452;&#25104;&#30340;&#31649;&#32447;&#65306;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#65288;&#22914;&#30005;&#27969;&#21644;&#30005;&#21387;&#65289;&#30340;&#25910;&#38598;&#21644;&#31649;&#29702;&#12289;&#26102;&#38388;&#24207;&#21015;&#30340;&#23454;&#26102;&#22788;&#29702;&#21644;&#29305;&#24449;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
The digitization of manufacturing processes enables promising applications for machine learning-assisted quality assurance. A widely used manufacturing process that can strongly benefit from data-driven solutions is \ac{GMAW}. The welding process is characterized by complex cause-effect relationships between material properties, process conditions and weld quality. In non-laboratory environments with frequently changing process parameters, accurate determination of weld quality by destructive testing is economically unfeasible. Deep learning offers the potential to identify the relationships in available process data and predict the weld quality from process observations. In this paper, we present a concept for a deep learning based predictive quality system in \ac{GMAW}. At its core, the concept involves a pipeline consisting of four major phases: collection and management of multi-sensor data (e.g. current and voltage), real-time processing and feature engineering of the time series 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36870;&#37325;&#25972;&#32676;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26500;&#36896;&#20102;&#32553;&#25918;&#30340;&#26230;&#26684;&#37197;&#32622;&#65292;&#29992;&#20110;&#30740;&#31350;&#33258;&#26059;&#29627;&#29827;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#22312;&#19981;&#20351;&#29992;&#36229;&#32423;&#35745;&#31639;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#25506;&#32034;&#19981;&#26029;&#22686;&#22823;&#30340;&#26230;&#26684;&#20307;&#31215;&#19979;&#30340;&#31934;&#30830;&#37197;&#32622;&#12290;</title><link>http://arxiv.org/abs/2310.12631</link><description>&lt;p&gt;
&#19981;&#35268;&#21017;&#31995;&#32479;&#30340;&#36870;&#37325;&#25972;&#32676;
&lt;/p&gt;
&lt;p&gt;
Inverse Renormalization Group of Disordered Systems. (arXiv:2310.12631v1 [cond-mat.stat-mech])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36870;&#37325;&#25972;&#32676;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26500;&#36896;&#20102;&#32553;&#25918;&#30340;&#26230;&#26684;&#37197;&#32622;&#65292;&#29992;&#20110;&#30740;&#31350;&#33258;&#26059;&#29627;&#29827;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#22312;&#19981;&#20351;&#29992;&#36229;&#32423;&#35745;&#31639;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#25506;&#32034;&#19981;&#26029;&#22686;&#22823;&#30340;&#26230;&#26684;&#20307;&#31215;&#19979;&#30340;&#31934;&#30830;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#36870;&#37325;&#25972;&#32676;&#21464;&#25442;&#65292;&#29992;&#20110;&#26500;&#24314;&#23578;&#26410;&#34987;&#36229;&#32423;&#35745;&#31639;&#26426;&#25110;&#22823;&#35268;&#27169;&#27169;&#25311;&#26041;&#27861;&#25152;&#35775;&#38382;&#30340;&#26230;&#26684;&#20307;&#31215;&#30340;&#36817;&#20284;&#37197;&#32622;&#65292;&#20197;&#30740;&#31350;&#33258;&#26059;&#29627;&#29827;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#19977;&#32500;&#29233;&#24503;&#21326;-&#23433;&#24503;&#26862;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#20307;&#31215;&#20026;$V=8^{3}$&#30340;&#26230;&#26684;&#24320;&#22987;&#65292;&#25105;&#20204;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26500;&#36896;&#20102;&#32463;&#36807;&#32553;&#25918;&#30340;&#26230;&#26684;&#65292;&#26368;&#22823;&#21040;$V'=128^{3}$&#65292;&#24182;&#25552;&#21462;&#20102;&#20004;&#20010;&#20020;&#30028;&#25351;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#22312;&#36870;&#37325;&#25972;&#32676;&#26041;&#27861;&#20013;&#34701;&#20837;&#25968;&#20540;&#31934;&#30830;&#24615;&#65292;&#20174;&#32780;&#20026;&#19981;&#26029;&#22686;&#22823;&#30340;&#26230;&#26684;&#20307;&#31215;&#25552;&#20379;&#20102;&#25506;&#32034;&#21487;&#25345;&#32493;&#12289;&#33410;&#33021;&#30340;&#31934;&#30830;&#37197;&#32622;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#19987;&#29992;&#36229;&#32423;&#35745;&#31639;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose inverse renormalization group transformations to construct approximate configurations for lattice volumes that have not yet been accessed by supercomputers or large-scale simulations in the study of spin glasses. Specifically, starting from lattices of volume $V=8^{3}$ in the case of the three-dimensional Edwards-Anderson model we employ machine learning algorithms to construct rescaled lattices up to $V'=128^{3}$, which we utilize to extract two critical exponents. We conclude by discussing how to incorporate numerical exactness within inverse renormalization group approaches of disordered systems, thus opening up the opportunity to explore a sustainable and energy-efficient generation of exact configurations for increasing lattice volumes without the use of dedicated supercomputers.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;Frank-Wolfe&#31639;&#27861;&#65292;&#25913;&#36827;&#20102;Metarounding&#31639;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32447;&#24615;&#20248;&#21270;&#31639;&#27861;&#22312;&#32452;&#21512;&#31867;&#38382;&#39064;&#19978;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.12629</link><description>&lt;p&gt;
&#36890;&#36807;Frank-Wolfe&#31639;&#27861;&#25913;&#36827;&#30340;Metarounding&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Improved Metarounding Algorithm via Frank-Wolfe. (arXiv:2310.12629v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12629
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;Frank-Wolfe&#31639;&#27861;&#65292;&#25913;&#36827;&#20102;Metarounding&#31639;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32447;&#24615;&#20248;&#21270;&#31639;&#27861;&#22312;&#32452;&#21512;&#31867;&#38382;&#39064;&#19978;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Metarounding&#26159;&#19968;&#31181;&#23558;&#32447;&#24615;&#20248;&#21270;&#30340;&#36817;&#20284;&#31639;&#27861;&#36716;&#21270;&#20026;&#21516;&#19968;&#31867;&#30340;&#22312;&#32447;&#32447;&#24615;&#20248;&#21270;&#31639;&#27861;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Metarounding&#31639;&#27861;&#65292;&#22522;&#20110;&#19968;&#20010;&#23545;&#20110;&#32452;&#21512;&#31867;&#23384;&#22312;&#22522;&#20110;&#26494;&#24347;&#30340;&#36817;&#20284;&#31639;&#27861;&#30340;&#33258;&#28982;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#26041;&#38754;&#37117;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metarounding is an approach to convert an approximation algorithm for linear optimization over some combinatorial classes to an online linear optimization algorithm for the same class. We propose a new metarounding algorithm under a natural assumption that a relax-based approximation algorithm exists for the combinatorial class. Our algorithm is much more efficient in both theoretical and practical aspects.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32477;&#32536;&#20307;&#25913;&#21892;&#28909;&#21147;&#25193;&#25955;&#36827;&#34892;&#26080;&#30896;&#25758;&#36816;&#21160;&#35268;&#21010;&#30340;&#21435;&#22122;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21333;&#19968;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#21487;&#36798;&#30446;&#26631;&#24182;&#35268;&#21010;&#36991;&#24320;&#38556;&#30861;&#29289;&#30340;&#36816;&#21160;&#36335;&#24452;&#65292;&#20855;&#26377;&#31283;&#20581;&#24615;&#21644;&#22810;&#27169;&#24577;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12609</link><description>&lt;p&gt;
&#20351;&#29992;&#32477;&#32536;&#20307;&#25913;&#21892;&#28909;&#21147;&#25193;&#25955;&#36827;&#34892;&#26080;&#30896;&#25758;&#36816;&#21160;&#35268;&#21010;&#30340;&#21435;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Denoising Heat-inspired Diffusion with Insulators for Collision Free Motion Planning. (arXiv:2310.12609v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32477;&#32536;&#20307;&#25913;&#21892;&#28909;&#21147;&#25193;&#25955;&#36827;&#34892;&#26080;&#30896;&#25758;&#36816;&#21160;&#35268;&#21010;&#30340;&#21435;&#22122;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21333;&#19968;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#21487;&#36798;&#30446;&#26631;&#24182;&#35268;&#21010;&#36991;&#24320;&#38556;&#30861;&#29289;&#30340;&#36816;&#21160;&#36335;&#24452;&#65292;&#20855;&#26377;&#31283;&#20581;&#24615;&#21644;&#22810;&#27169;&#24577;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#22810;&#27169;&#24577;&#24615;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;&#23613;&#31649;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22797;&#26434;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20005;&#37325;&#20381;&#36182;&#20110;&#25512;&#29702;&#26102;&#30340;&#38556;&#30861;&#29289;&#26816;&#27979;&#24182;&#38656;&#35201;&#39069;&#22806;&#30340;&#35774;&#22791;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#20174;&#21333;&#19968;&#30340;&#35270;&#35273;&#36755;&#20837;&#20013;&#21516;&#26102;&#29983;&#25104;&#21487;&#36798;&#30446;&#26631;&#24182;&#35268;&#21010;&#36991;&#24320;&#38556;&#30861;&#29289;&#30340;&#36816;&#21160;&#36335;&#24452;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26032;&#39062;&#30340;&#30896;&#25758;&#36991;&#20813;&#25193;&#25955;&#26680;&#36827;&#34892;&#20351;&#29992;&#12290;&#36890;&#36807;&#19982;&#34892;&#20026;&#20811;&#38534;&#21644;&#32463;&#20856;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#35777;&#26126;&#20102;&#20854;&#31283;&#20581;&#24615;&#12290;&#29305;&#21035;&#26159;&#22312;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#65292;&#23427;&#33021;&#22815;&#23548;&#33322;&#21040;&#30446;&#26631;&#24182;&#36991;&#24320;&#34987;&#38556;&#30861;&#29289;&#38459;&#25377;&#30340;&#19981;&#21487;&#36798;&#30446;&#26631;&#65292;&#21516;&#26102;&#30830;&#20445;&#36991;&#20813;&#30896;&#25758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have risen as a powerful tool in robotics due to their flexibility and multi-modality. While some of these methods effectively address complex problems, they often depend heavily on inference-time obstacle detection and require additional equipment. Addressing these challenges, we present a method that, during inference time, simultaneously generates only reachable goals and plans motions that avoid obstacles, all from a single visual input. Central to our approach is the novel use of a collision-avoiding diffusion kernel for training. Through evaluations against behavior-cloning and classical diffusion models, our framework has proven its robustness. It is particularly effective in multi-modal environments, navigating toward goals and avoiding unreachable ones blocked by obstacles, while ensuring collision avoidance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#30456;&#20284;&#24615;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22914;&#20309;&#20174;&#20855;&#26377;&#30456;&#20284;&#22240;&#26524;&#26426;&#21046;&#30340;&#35757;&#32451;&#20219;&#21153;&#20013;&#27719;&#38598;&#25968;&#25454;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#26032;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12595</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#30456;&#20284;&#24615;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Causal Similarity-Based Hierarchical Bayesian Models. (arXiv:2310.12595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#30456;&#20284;&#24615;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22914;&#20309;&#20174;&#20855;&#26377;&#30456;&#20284;&#22240;&#26524;&#26426;&#21046;&#30340;&#35757;&#32451;&#20219;&#21153;&#20013;&#27719;&#38598;&#25968;&#25454;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#26032;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#23545;&#26032;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23545;&#30001;&#30456;&#20851;&#20219;&#21153;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#20219;&#21153;&#21487;&#33021;&#22312;&#22240;&#26524;&#26426;&#21046;&#19978;&#23384;&#22312;&#24046;&#24322;&#12290;&#20363;&#22914;&#65292;&#22797;&#26434;&#30142;&#30149;&#30340;&#35266;&#23519;&#24615;&#21307;&#23398;&#25968;&#25454;&#22312;&#19981;&#21516;&#24739;&#32773;&#38388;&#20855;&#26377;&#30142;&#30149;&#22240;&#26524;&#26426;&#21046;&#30340;&#24322;&#36136;&#24615;&#65292;&#36825;&#32473;&#38656;&#35201;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#20043;&#22806;&#30340;&#26032;&#24739;&#32773;&#36827;&#34892;&#27867;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#24120;&#29992;&#30340;&#22788;&#29702;&#24322;&#36136;&#24615;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#21253;&#25324;&#20026;&#25972;&#20010;&#25968;&#25454;&#38598;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#20026;&#27599;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#23398;&#20064;&#26412;&#22320;&#27169;&#22411;&#65292;&#25110;&#32773;&#21033;&#29992;&#20998;&#23618;&#12289;&#20803;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#20174;&#27719;&#38598;&#30340;&#22810;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#27867;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22240;&#26524;&#30456;&#20284;&#24615;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22914;&#20309;&#20174;&#20855;&#26377;&#30456;&#20284;&#22240;&#26524;&#26426;&#21046;&#30340;&#35757;&#32451;&#20219;&#21153;&#20013;&#27719;&#38598;&#25968;&#25454;&#26469;&#25552;&#39640;&#23545;&#26032;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#24212;&#29992;&#36825;&#31181;&#36890;&#29992;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The key challenge underlying machine learning is generalisation to new data. This work studies generalisation for datasets consisting of related tasks that may differ in causal mechanisms. For example, observational medical data for complex diseases suffers from heterogeneity in causal mechanisms of disease across patients, creating challenges for machine learning algorithms that need to generalise to new patients outside of the training dataset. Common approaches for learning supervised models with heterogeneous datasets include learning a global model for the entire dataset, learning local models for each tasks' data, or utilising hierarchical, meta-learning and multi-task learning approaches to learn how to generalise from data pooled across multiple tasks. In this paper we propose causal similarity-based hierarchical Bayesian models to improve generalisation to new tasks by learning how to pool data from training tasks with similar causal mechanisms. We apply this general modelling
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#24577;&#24863;&#30693;&#30340;&#38382;&#39064;&#22238;&#31572;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#19978;&#19979;&#25991;&#30340;&#21306;&#38388;&#25277;&#21462;&#20219;&#21153;&#21644;&#30456;&#24212;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;QA&#27169;&#22411;&#30340;&#26102;&#38388;&#24863;&#30693;&#33021;&#21147;&#65292;&#22312;TimeQA&#25968;&#25454;&#38598;&#20013;&#30340;F1&#20998;&#25968;&#19978;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#36798;8.5&#12290;</title><link>http://arxiv.org/abs/2310.12585</link><description>&lt;p&gt;
&#26102;&#24577;&#25935;&#24863;&#38382;&#39064;&#22238;&#31572;&#30340;&#26102;&#24577;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Time-Aware Representation Learning for Time-Sensitive Question Answering. (arXiv:2310.12585v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12585
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#24577;&#24863;&#30693;&#30340;&#38382;&#39064;&#22238;&#31572;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#19978;&#19979;&#25991;&#30340;&#21306;&#38388;&#25277;&#21462;&#20219;&#21153;&#21644;&#30456;&#24212;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;QA&#27169;&#22411;&#30340;&#26102;&#38388;&#24863;&#30693;&#33021;&#21147;&#65292;&#22312;TimeQA&#25968;&#25454;&#38598;&#20013;&#30340;F1&#20998;&#25968;&#19978;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#36798;8.5&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#26159;&#29616;&#23454;&#19990;&#30028;&#20013;&#38382;&#39064;&#22238;&#31572;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#65292;&#28982;&#32780;&#35821;&#35328;&#27169;&#22411;&#24456;&#38590;&#29702;&#35299;&#26102;&#38388;&#38480;&#23450;&#35789;&#22914;&#8220;&#20043;&#21518;&#8221;&#21644;&#8220;&#20043;&#21069;&#8221;&#19982;&#25968;&#23383;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#22240;&#20026;&#29616;&#26377;&#30340;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#36275;&#22815;&#30340;&#26102;&#38388;&#34920;&#36798;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#38382;&#39064;&#22238;&#31572;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#19978;&#19979;&#25991;&#30340;&#21306;&#38388;&#25277;&#21462;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#20381;&#36182;&#20110;&#26102;&#38388;&#19978;&#19979;&#25991;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;QA&#27169;&#22411;&#30340;&#26102;&#38388;&#24863;&#30693;&#24230;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#21306;&#38388;&#25277;&#21462;&#20219;&#21153;&#21253;&#25324;&#19968;&#20010;&#38382;&#39064;&#21644;&#22235;&#20010;&#21477;&#23376;&#20505;&#36873;&#39033;&#65292;&#26681;&#25454;&#26102;&#38388;&#21644;&#19978;&#19979;&#25991;&#20998;&#31867;&#20026;&#27491;&#30830;&#25110;&#38169;&#35823;&#12290;&#27169;&#22411;&#34987;&#35757;&#32451;&#20026;&#20174;&#22312;&#26102;&#38388;&#21644;&#19978;&#19979;&#25991;&#19978;&#37117;&#27491;&#30830;&#30340;&#21477;&#23376;&#20013;&#25552;&#21462;&#31572;&#26696;&#21306;&#38388;&#12290;&#36890;&#36807;&#20351;&#29992;TCQA&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;TimeQA&#25968;&#25454;&#38598;&#19978;&#30340;F1&#20998;&#25968;&#19978;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#36798;8.5&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time is one of the crucial factors in real-world question answering (QA) problems. However, language models have difficulty understanding the relationships between time specifiers, such as 'after' and 'before', and numbers, since existing QA datasets do not include sufficient time expressions. To address this issue, we propose a Time-Context aware Question Answering (TCQA) framework. We suggest a Time-Context dependent Span Extraction (TCSE) task, and build a time-context dependent data generation framework for model training. Moreover, we present a metric to evaluate the time awareness of the QA model using TCSE. The TCSE task consists of a question and four sentence candidates classified as correct or incorrect based on time and context. The model is trained to extract the answer span from the sentence that is both correct in time and context. The model trained with TCQA outperforms baseline models up to 8.5 of the F1-score in the TimeQA dataset. Our dataset and code are available at
&lt;/p&gt;</description></item><item><title>DA-TransUNet &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;Transformer&#21644;&#21452;&#37325;&#27880;&#24847;&#21147;&#22359;&#12290;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#21644;&#22810;&#26041;&#38754;&#29305;&#24449;&#25552;&#21462;&#65292;&#25552;&#21319;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.12570</link><description>&lt;p&gt;
DA-TransUNet: &#23558;Spatial&#21644;Channel Dual Attention&#19982;Transformer U-Net&#38598;&#25104;&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
DA-TransUNet: Integrating Spatial and Channel Dual Attention with Transformer U-Net for Medical Image Segmentation. (arXiv:2310.12570v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12570
&lt;/p&gt;
&lt;p&gt;
DA-TransUNet &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;Transformer&#21644;&#21452;&#37325;&#27880;&#24847;&#21147;&#22359;&#12290;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#21644;&#22810;&#26041;&#38754;&#29305;&#24449;&#25552;&#21462;&#65292;&#25552;&#21319;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#24378;&#22823;&#30340;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#65292;&#33258;&#21160;&#21270;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#12290;Transformer&#30340;&#24433;&#21709;&#23548;&#33268;&#20102;&#23545;&#20854;&#21464;&#20307;&#30340;&#30740;&#31350;&#65292;&#24182;&#22823;&#35268;&#27169;&#26367;&#20195;&#20256;&#32479;&#30340;CNN&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36235;&#21183;&#32463;&#24120;&#24573;&#35270;&#20102;Transformer&#30340;&#22266;&#26377;&#29305;&#24449;&#25552;&#21462;&#33021;&#21147;&#20197;&#21450;&#36890;&#36807;&#24494;&#23567;&#35843;&#25972;&#23545;&#27169;&#22411;&#21644;Transformer&#27169;&#22359;&#30340;&#28508;&#22312;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26694;&#26550;&#65292;&#31216;&#20026;DA-TransUNet&#65292;&#26088;&#22312;&#23558;Transformer&#21644;&#21452;&#37325;&#27880;&#24847;&#22359;&#24341;&#20837;&#20256;&#32479;U&#24418;&#26550;&#26500;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20013;&#12290;&#19982;&#20043;&#21069;&#22522;&#20110;Transformer&#30340;&#35299;&#20915;&#26041;&#26696;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;DA-TransUNet&#21033;&#29992;&#20102;Transformer&#30340;&#27880;&#24847;&#26426;&#21046;&#21644;DA-Block&#30340;&#22810;&#26041;&#38754;&#29305;&#24449;&#25552;&#21462;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#32467;&#21512;&#20840;&#23616;&#12289;&#23616;&#37096;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#20197;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#21516;&#26102;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#20043;&#21069;&#30340;Transformer U-Net&#30340;&#22522;&#30784;&#19978;&#28155;&#21152;&#20102;&#19968;&#20010;&#21452;&#37325;&#27880;&#24847;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Great progress has been made in automatic medical image segmentation due to powerful deep representation learning. The influence of transformer has led to research into its variants, and large-scale replacement of traditional CNN modules. However, such trend often overlooks the intrinsic feature extraction capabilities of the transformer and potential refinements to both the model and the transformer module through minor adjustments. This study proposes a novel deep medical image segmentation framework, called DA-TransUNet, aiming to introduce the Transformer and dual attention block into the encoder and decoder of the traditional U-shaped architecture. Unlike prior transformer-based solutions, our DA-TransUNet utilizes attention mechanism of transformer and multifaceted feature extraction of DA-Block, which can efficiently combine global, local, and multi-scale features to enhance medical image segmentation. Meanwhile, experimental results show that a dual attention block is added bef
&lt;/p&gt;</description></item><item><title>Julearn&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;Python&#24211;&#65292;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#21644;&#35780;&#20272;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#36991;&#20813;&#27844;&#28431;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12568</link><description>&lt;p&gt;
Julearn: &#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#24211;&#65292;&#29992;&#20110;&#26080;&#27844;&#28431;&#35780;&#20272;&#21644;&#26816;&#26597;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Julearn: an easy-to-use library for leakage-free evaluation and inspection of ML models. (arXiv:2310.12568v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12568
&lt;/p&gt;
&lt;p&gt;
Julearn&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;Python&#24211;&#65292;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#21644;&#35780;&#20272;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#36991;&#20813;&#27844;&#28431;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#21457;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20197;&#21450;&#23427;&#22312;&#30740;&#31350;&#20013;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#24212;&#29992;&#20026;&#27809;&#26377;&#28145;&#20837;&#22521;&#35757;&#30340;&#30740;&#31350;&#20154;&#21592;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#20363;&#22914;&#65292;&#22312;&#31070;&#32463;&#31185;&#23398;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#24110;&#21161;&#29702;&#35299;&#33041;&#19982;&#34892;&#20026;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35786;&#26029;&#30142;&#30149;&#65292;&#24182;&#21033;&#29992;&#30913;&#20849;&#25391;&#25104;&#20687;&#21644;&#33041;&#30005;&#22270;&#31561;&#21508;&#31181;&#25968;&#25454;&#28304;&#24320;&#21457;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#26500;&#24314;&#33021;&#22815;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20351;&#29992;&#20132;&#21449;&#39564;&#35777;&#31561;&#25216;&#26415;&#26469;&#35780;&#20272;&#24615;&#33021;&#65292;&#20174;&#32780;&#35777;&#26126;&#36825;&#26679;&#30340;&#21487;&#25512;&#24191;&#27169;&#22411;&#30340;&#23384;&#22312;&#65292;&#20132;&#21449;&#39564;&#35777;&#20351;&#29992;&#31995;&#32479;&#23376;&#25277;&#26679;&#26469;&#20272;&#35745;&#27867;&#21270;&#24615;&#33021;&#12290;&#36873;&#25321;&#20132;&#21449;&#39564;&#35777;&#26041;&#26696;&#24182;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#24182;&#19988;&#22914;&#26524;&#20351;&#29992;&#19981;&#24403;&#21487;&#33021;&#23548;&#33268;&#36807;&#39640;&#30340;&#32467;&#26524;&#21644;&#38169;&#35823;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;julearn&#30340;&#24320;&#28304;Python&#24211;&#65292;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#21644;&#35780;&#20272;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#32780;&#27809;&#26377;&#36935;&#21040;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fast-paced development of machine learning (ML) methods coupled with its increasing adoption in research poses challenges for researchers without extensive training in ML. In neuroscience, for example, ML can help understand brain-behavior relationships, diagnose diseases, and develop biomarkers using various data sources like magnetic resonance imaging and electroencephalography. The primary objective of ML is to build models that can make accurate predictions on unseen data. Researchers aim to prove the existence of such generalizable models by evaluating performance using techniques such as cross-validation (CV), which uses systematic subsampling to estimate the generalization performance. Choosing a CV scheme and evaluating an ML pipeline can be challenging and, if used improperly, can lead to overestimated results and incorrect interpretations.  We created julearn, an open-source Python library, that allow researchers to design and evaluate complex ML pipelines without encount
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Safety-Gymnasium&#30340;&#29615;&#22659;&#22871;&#20214;&#65292;&#20854;&#20013;&#21253;&#25324;&#21333;&#20010;&#21644;&#22810;&#20010;Agent&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;16&#31181;&#26368;&#20808;&#36827;&#30340;SafeRL&#31639;&#27861;&#30340;&#31639;&#27861;&#24211;&#12290;&#36825;&#20010;&#22522;&#20934;&#26088;&#22312;&#20419;&#36827;&#23545;&#23433;&#20840;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#25512;&#21160;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#24615;&#33021;&#19978;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2310.12567</link><description>&lt;p&gt;
Safety-Gymnasion&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark. (arXiv:2310.12567v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Safety-Gymnasium&#30340;&#29615;&#22659;&#22871;&#20214;&#65292;&#20854;&#20013;&#21253;&#25324;&#21333;&#20010;&#21644;&#22810;&#20010;Agent&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;16&#31181;&#26368;&#20808;&#36827;&#30340;SafeRL&#31639;&#27861;&#30340;&#31639;&#27861;&#24211;&#12290;&#36825;&#20010;&#22522;&#20934;&#26088;&#22312;&#20419;&#36827;&#23545;&#23433;&#20840;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#25512;&#21160;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#24615;&#33021;&#19978;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25317;&#26377;&#25512;&#21160;&#31038;&#20250;&#36827;&#27493;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#32463;&#24120;&#38754;&#20020;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;(SafeRL)&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#21487;&#20197;&#22312;&#21516;&#26102;&#36981;&#23432;&#22810;&#20010;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#31574;&#30053;&#65292;&#20174;&#32780;&#35299;&#20915;&#38598;&#25104;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#20013;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Safety-Gymnasium&#30340;&#29615;&#22659;&#22871;&#20214;&#65292;&#21253;&#25324;&#21333;&#20010;&#21644;&#22810;&#20010;Agent&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#65292;&#24182;&#25509;&#21463;&#21521;&#37327;&#21644;&#20165;&#35270;&#35273;&#36755;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;Safe Policy Optimization&#65288;SafePO&#65289;&#30340;&#31639;&#27861;&#24211;&#65292;&#21253;&#21547;16&#31181;&#26368;&#20808;&#36827;&#30340;SafeRL&#31639;&#27861;&#12290;&#36825;&#20010;&#32508;&#21512;&#24615;&#24211;&#21487;&#20197;&#20316;&#20026;&#30740;&#31350;&#31038;&#21306;&#30340;&#39564;&#35777;&#24037;&#20855;&#12290;&#36890;&#36807;&#24341;&#20837;&#36825;&#20010;&#22522;&#20934;&#65292;&#25105;&#20204;&#26088;&#22312;&#20419;&#36827;&#23545;&#23433;&#20840;&#24615;&#33021;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#20174;&#32780;&#25512;&#21160;&#24378;&#21270;&#23398;&#20064;&#22312;&#23433;&#20840;&#24615;&#33021;&#19978;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) systems possess significant potential to drive societal progress. However, their deployment often faces obstacles due to substantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a solution to optimize policies while simultaneously adhering to multiple constraints, thereby addressing the challenge of integrating reinforcement learning in safety-critical scenarios. In this paper, we present an environment suite called Safety-Gymnasium, which encompasses safety-critical tasks in both single and multi-agent scenarios, accepting vector and vision-only input. Additionally, we offer a library of algorithms named Safe Policy Optimization (SafePO), comprising 16 state-of-the-art SafeRL algorithms. This comprehensive library can serve as a validation tool for the research community. By introducing this benchmark, we aim to facilitate the evaluation and comparison of safety performance, thus fostering the development of reinforcement learning for s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24320;&#25918;&#19990;&#30028;&#22330;&#26223;&#19979;&#30340;&#32456;&#36523;&#22270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#26041;&#27861;&#19982;&#22270;&#24418;&#37051;&#22495;&#20449;&#24687;&#32858;&#21512;&#30456;&#32467;&#21512;&#30340;&#26032;&#31867;&#21035;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#30456;&#20851;&#21453;&#39304;&#65288;Open-WRF&#65289;&#26041;&#27861;&#26469;&#38477;&#20302;OOD&#26816;&#27979;&#20013;&#23545;&#38408;&#20540;&#30340;&#25935;&#24863;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;OOD&#24471;&#20998;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#26356;&#21152;&#40065;&#26834;&#12290;</title><link>http://arxiv.org/abs/2310.12565</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#19979;&#30340;&#32456;&#36523;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Open-World Lifelong Graph Learning. (arXiv:2310.12565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24320;&#25918;&#19990;&#30028;&#22330;&#26223;&#19979;&#30340;&#32456;&#36523;&#22270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#26041;&#27861;&#19982;&#22270;&#24418;&#37051;&#22495;&#20449;&#24687;&#32858;&#21512;&#30456;&#32467;&#21512;&#30340;&#26032;&#31867;&#21035;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#30456;&#20851;&#21453;&#39304;&#65288;Open-WRF&#65289;&#26041;&#27861;&#26469;&#38477;&#20302;OOD&#26816;&#27979;&#20013;&#23545;&#38408;&#20540;&#30340;&#25935;&#24863;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;OOD&#24471;&#20998;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#19988;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#32456;&#36523;&#22270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#27169;&#22411;&#38656;&#35201;&#22788;&#29702;&#26032;&#30340;&#20219;&#21153;&#21644;&#28508;&#22312;&#30340;&#26410;&#30693;&#31867;&#21035;&#12290;&#25105;&#20204;&#21033;&#29992;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#26041;&#27861;&#35782;&#21035;&#26032;&#30340;&#31867;&#21035;&#65292;&#24182;&#23558;&#29616;&#26377;&#30340;&#38750;&#22270;&#24418;OOD&#26816;&#27979;&#26041;&#27861;&#36866;&#24212;&#20110;&#22270;&#24418;&#25968;&#25454;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#23558;OOD&#26816;&#27979;&#26041;&#27861;&#19982;&#20174;&#22270;&#24418;&#37051;&#22495;&#32858;&#21512;&#30340;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#26469;&#36827;&#34892;&#26032;&#31867;&#21035;&#30340;&#26816;&#27979;&#12290;&#22823;&#22810;&#25968;OOD&#26816;&#27979;&#26041;&#27861;&#37117;&#36991;&#20813;&#30830;&#23450;&#19968;&#20010;&#30830;&#23450;&#30340;&#38408;&#20540;&#65292;&#26469;&#20915;&#23450;&#19968;&#20010;&#39030;&#28857;&#26159;&#21542;&#20026;OOD&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#30456;&#20851;&#21453;&#39304;&#65288;Open-WRF&#65289;&#26041;&#27861;&#65292;&#38477;&#20302;&#20102;OOD&#26816;&#27979;&#20013;&#23545;&#38408;&#20540;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#37051;&#22495;&#32858;&#21512;&#26041;&#27861;&#23545;OOD&#24471;&#20998;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#32780;&#19981;&#21463;&#22522;&#30784;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;Open-WRF&#26041;&#27861;&#23545;&#20110;&#38408;&#20540;&#36873;&#25321;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of lifelong graph learning in an open-world scenario, where a model needs to deal with new tasks and potentially unknown classes. We utilize Out-of-Distribution (OOD) detection methods to recognize new classes and adapt existing non-graph OOD detection methods to graph data. Crucially, we suggest performing new class detection by combining OOD detection methods with information aggregated from the graph neighborhood. Most OOD detection methods avoid determining a crisp threshold for deciding whether a vertex is OOD. To tackle this problem, we propose a Weakly-supervised Relevance Feedback (Open-WRF) method, which decreases the sensitivity to thresholds in OOD detection. We evaluate our approach on six benchmark datasets. Our results show that the proposed neighborhood aggregation method for OOD scores outperforms existing methods independent of the underlying graph neural network. Furthermore, we demonstrate that our Open-WRF method is more robust to threshold sele
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#24378;&#30423;&#28216;&#25103;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#20851;&#38190;&#21464;&#37327;&#30340;&#20449;&#24687;&#36817;&#20284;&#20540;&#26469;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#20256;&#32479;&#24378;&#30423;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23545;&#20110;&#20004;&#33218;&#24378;&#30423;&#38382;&#39064;&#30340;&#28176;&#36817;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12563</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#24378;&#30423;&#28216;&#25103;&#30340;&#36817;&#20284;&#20449;&#24687;&#26368;&#22823;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Approximate information maximization for bandit games. (arXiv:2310.12563v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#24378;&#30423;&#28216;&#25103;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#20851;&#38190;&#21464;&#37327;&#30340;&#20449;&#24687;&#36817;&#20284;&#20540;&#26469;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#20256;&#32479;&#24378;&#30423;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23545;&#20110;&#20004;&#33218;&#24378;&#30423;&#38382;&#39064;&#30340;&#28176;&#36817;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29109;&#26368;&#22823;&#21270;&#21644;&#33258;&#30001;&#33021;&#26368;&#23567;&#21270;&#26159;&#29992;&#20110;&#27169;&#25311;&#21508;&#31181;&#29289;&#29702;&#31995;&#32479;&#21160;&#24577;&#30340;&#19968;&#33324;&#29289;&#29702;&#21407;&#29702;&#12290;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;&#33258;&#30001;&#33021;&#21407;&#29702;&#23545;&#22823;&#33041;&#20869;&#30340;&#20915;&#31574;&#36827;&#34892;&#24314;&#27169;&#65292;&#20351;&#29992;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#23545;&#35775;&#38382;&#38544;&#34255;&#21464;&#37327;&#26102;&#20248;&#21270;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#26435;&#34913;&#65292;&#20197;&#21450;&#20351;&#29992;&#20449;&#24687;&#26368;&#22823;&#21270;&#36827;&#34892;&#38543;&#26426;&#29615;&#22659;&#23548;&#33322;&#12290;&#22522;&#20110;&#36825;&#19968;&#21407;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#30423;&#31639;&#27861;&#31867;&#21035;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#31995;&#32479;&#20013;&#19968;&#20010;&#20851;&#38190;&#21464;&#37327;&#30340;&#20449;&#24687;&#36817;&#20284;&#26469;&#36827;&#34892;&#20248;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#30340;&#36817;&#20284;&#20998;&#26512;&#29109;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#39044;&#27979;&#27599;&#20010;&#21160;&#20316;&#30340;&#20449;&#24687;&#22686;&#30410;&#65292;&#24182;&#36138;&#23146;&#22320;&#36873;&#25321;&#20449;&#24687;&#22686;&#30410;&#26368;&#22823;&#30340;&#21160;&#20316;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20256;&#32479;&#24378;&#30423;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;&#21463;&#21040;&#20854;&#32463;&#39564;&#24615;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#23545;&#20110;&#20004;&#33218;&#24378;&#30423;&#38382;&#39064;&#30340;&#28176;&#36817;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entropy maximization and free energy minimization are general physical principles for modeling the dynamics of various physical systems. Notable examples include modeling decision-making within the brain using the free-energy principle, optimizing the accuracy-complexity trade-off when accessing hidden variables with the information bottleneck principle (Tishby et al., 2000), and navigation in random environments using information maximization (Vergassola et al., 2007). Built on this principle, we propose a new class of bandit algorithms that maximize an approximation to the information of a key variable within the system. To this end, we develop an approximated analytical physics-based representation of an entropy to forecast the information gain of each action and greedily choose the one with the largest information gain. This method yields strong performances in classical bandit settings. Motivated by its empirical success, we prove its asymptotic optimality for the two-armed bandit
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#27169;&#22411;&#21435;&#20559;&#32622;&#30340;&#26694;&#26550;&#65288;FMD&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#12289;&#35780;&#20272;&#21644;&#28040;&#38500;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20559;&#35265;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25104;&#26412;&#21644;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2310.12560</link><description>&lt;p&gt;
&#24555;&#36895;&#27169;&#22411;&#21435;&#20559;&#32622;&#19982;&#26426;&#22120;&#21462;&#28040;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fast Model Debias with Machine Unlearning. (arXiv:2310.12560v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12560
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#27169;&#22411;&#21435;&#20559;&#32622;&#30340;&#26694;&#26550;&#65288;FMD&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#12289;&#35780;&#20272;&#21644;&#28040;&#38500;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20559;&#35265;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25104;&#26412;&#21644;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#21487;&#33021;&#34920;&#29616;&#20986;&#20559;&#24046;&#30340;&#34892;&#20026;&#12290;&#20363;&#22914;&#65292;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20154;&#33080;&#35782;&#21035;&#25968;&#25454;&#38598;CelebA&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#32593;&#32476;&#20542;&#21521;&#20110;&#39044;&#27979;&#22899;&#24615;&#30340;&#37329;&#33394;&#22836;&#21457;&#21644;&#30007;&#24615;&#30340;&#40657;&#33394;&#22836;&#21457;&#12290;&#36825;&#20123;&#20559;&#24046;&#19981;&#20165;&#21361;&#23475;&#20102;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#32780;&#19988;&#20250;&#25345;&#32493;&#21644;&#25918;&#22823;&#31038;&#20250;&#20559;&#35265;&#65292;&#36825;&#23545;&#20110;&#21307;&#30103;&#12289;&#25307;&#32856;&#31561;&#33258;&#21160;&#20915;&#31574;&#36807;&#31243;&#23588;&#20854;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#21152;&#21095;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#19981;&#20844;&#24179;&#32463;&#27982;&#21644;&#31038;&#20250;&#19981;&#24179;&#31561;&#12290;&#29616;&#26377;&#30340;&#21435;&#20559;&#32622;&#26041;&#27861;&#22312;&#20559;&#35265;&#26631;&#35760;&#25110;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#26041;&#38754;&#25104;&#26412;&#39640;&#26114;&#65292;&#21516;&#26102;&#20063;&#22312;&#38416;&#26126;&#27169;&#22411;&#20869;&#37096;&#20559;&#35265;&#30340;&#36215;&#28304;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#27169;&#22411;&#21435;&#20559;&#32622;&#26694;&#26550;(FMD)&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#12289;&#35780;&#20272;&#21644;&#28040;&#38500;&#35757;&#32451;&#27169;&#22411;&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#12290;FMD&#36890;&#36807;&#26174;&#24335;&#30340;&#21453;&#20107;&#23454;&#26426;&#21046;&#26469;&#35782;&#21035;&#20559;&#32622;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent discoveries have revealed that deep neural networks might behave in a biased manner in many real-world scenarios. For instance, deep networks trained on a large-scale face recognition dataset CelebA tend to predict blonde hair for females and black hair for males. Such biases not only jeopardize the robustness of models but also perpetuate and amplify social biases, which is especially concerning for automated decision-making processes in healthcare, recruitment, etc., as they could exacerbate unfair economic and social inequalities among different groups. Existing debiasing methods suffer from high costs in bias labeling or model re-training, while also exhibiting a deficiency in terms of elucidating the origins of biases within the model. To this respect, we propose a fast model debiasing framework (FMD) which offers an efficient approach to identify, evaluate and remove biases inherent in trained models. The FMD identifies biased attributes through an explicit counterfactual 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#25554;&#20837;/&#21024;&#38500;&#25351;&#26631;&#24863;&#30693;&#30340;&#22522;&#20110;&#35299;&#37322;&#30340;&#20248;&#21270;(ID-ExpO)&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21487;&#21306;&#20998;&#30340;&#39044;&#27979;&#22120;&#26469;&#25552;&#39640;&#35299;&#37322;&#30340;&#25554;&#20837;&#21644;&#21024;&#38500;&#24471;&#20998;&#65292;&#24182;&#20445;&#25345;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ID-ExpO&#33021;&#22815;&#20351;&#27969;&#34892;&#30340;&#20107;&#21518;&#35299;&#37322;&#22120;&#20135;&#29983;&#26356;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2310.12553</link><description>&lt;p&gt;
&#21033;&#29992;&#21487;&#21306;&#20998;&#25554;&#20837;/&#21024;&#38500;&#25351;&#26631;&#24863;&#30693;&#27491;&#21017;&#21270;&#36827;&#34892;&#35299;&#37322;&#24615;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Explanation-Based Training with Differentiable Insertion/Deletion Metric-Aware Regularizers. (arXiv:2310.12553v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12553
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#25554;&#20837;/&#21024;&#38500;&#25351;&#26631;&#24863;&#30693;&#30340;&#22522;&#20110;&#35299;&#37322;&#30340;&#20248;&#21270;(ID-ExpO)&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21487;&#21306;&#20998;&#30340;&#39044;&#27979;&#22120;&#26469;&#25552;&#39640;&#35299;&#37322;&#30340;&#25554;&#20837;&#21644;&#21024;&#38500;&#24471;&#20998;&#65292;&#24182;&#20445;&#25345;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ID-ExpO&#33021;&#22815;&#20351;&#27969;&#34892;&#30340;&#20107;&#21518;&#35299;&#37322;&#22120;&#20135;&#29983;&#26356;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#22120;&#30340;&#35299;&#37322;&#36136;&#37327;&#36890;&#24120;&#20351;&#29992;&#25554;&#20837;&#21644;&#21024;&#38500;&#25351;&#26631;&#36827;&#34892;&#34913;&#37327;&#65292;&#36825;&#20123;&#25351;&#26631;&#35780;&#20272;&#35299;&#37322;&#30340;&#24544;&#23454;&#24230;&#65292;&#21363;&#35299;&#37322;&#27491;&#30830;&#22320;&#21453;&#26144;&#20102;&#39044;&#27979;&#22120;&#30340;&#34892;&#20026;&#31243;&#24230;&#12290;&#20026;&#20102;&#25552;&#39640;&#24544;&#23454;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25554;&#20837;/&#21024;&#38500;&#25351;&#26631;&#24863;&#30693;&#30340;&#22522;&#20110;&#35299;&#37322;&#30340;&#20248;&#21270;&#65288;ID-ExpO&#65289;&#65292;&#35813;&#20248;&#21270;&#33021;&#22815;&#25913;&#21892;&#35299;&#37322;&#30340;&#25554;&#20837;&#21644;&#21024;&#38500;&#24471;&#20998;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#30001;&#20110;&#21407;&#22987;&#30340;&#25554;&#20837;&#21644;&#21024;&#38500;&#25351;&#26631;&#23545;&#20110;&#35299;&#37322;&#26469;&#35828;&#26159;&#19981;&#21487;&#21306;&#20998;&#30340;&#65292;&#24182;&#19988;&#26080;&#27861;&#30452;&#25509;&#36827;&#34892;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#20123;&#25351;&#26631;&#20197;&#20351;&#20854;&#21487;&#21306;&#20998;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#24418;&#24335;&#21270;&#25554;&#20837;&#21644;&#21024;&#38500;&#25351;&#26631;&#30340;&#27491;&#21017;&#21270;&#12290;&#22312;&#22270;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;ID-ExpO&#36827;&#34892;&#24494;&#35843;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#22120;&#33021;&#22815;&#20351;&#27969;&#34892;&#30340;&#20107;&#21518;&#35299;&#37322;&#22120;&#20135;&#29983;&#26356;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of explanations for the predictions of complex machine learning predictors is often measured using insertion and deletion metrics, which assess the faithfulness of the explanations, i.e., how correctly the explanations reflect the predictor's behavior. To improve the faithfulness, we propose insertion/deletion metric-aware explanation-based optimization (ID-ExpO), which optimizes differentiable predictors to improve both insertion and deletion scores of the explanations while keeping their predictive accuracy. Since the original insertion and deletion metrics are indifferentiable with respect to the explanations and directly unavailable for gradient-based optimization, we extend the metrics to be differentiable and use them to formalize insertion and deletion metric-based regularizers. The experimental results on image and tabular datasets show that the deep neural networks-based predictors fine-tuned using ID-ExpO enable popular post-hoc explainers to produce more faithful
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#20010;&#24615;&#21270;&#25235;&#21462;&#20195;&#29702;&#65288;PGA&#65289;&#65292;&#23427;&#36890;&#36807;&#21333;&#19968;&#20154;&#26426;&#20132;&#20114;&#23398;&#20064;&#24182;&#23450;&#20301;&#21644;&#25235;&#21462;&#20010;&#20154;&#29289;&#20307;&#12290;PGA&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#30340;&#20449;&#24687;&#21644;&#29992;&#25143;&#29615;&#22659;&#20013;&#30340;&#21407;&#22987;&#22270;&#20687;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#29289;&#20307;&#25235;&#21462;&#12290;</title><link>http://arxiv.org/abs/2310.12547</link><description>&lt;p&gt;
PGA: &#20010;&#24615;&#21270;&#25235;&#21462;&#20195;&#29702;&#19982;&#21333;&#19968;&#20154;&#26426;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
PGA: Personalizing Grasping Agents with Single Human-Robot Interaction. (arXiv:2310.12547v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12547
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#20010;&#24615;&#21270;&#25235;&#21462;&#20195;&#29702;&#65288;PGA&#65289;&#65292;&#23427;&#36890;&#36807;&#21333;&#19968;&#20154;&#26426;&#20132;&#20114;&#23398;&#20064;&#24182;&#23450;&#20301;&#21644;&#25235;&#21462;&#20010;&#20154;&#29289;&#20307;&#12290;PGA&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#30340;&#20449;&#24687;&#21644;&#29992;&#25143;&#29615;&#22659;&#20013;&#30340;&#21407;&#22987;&#22270;&#20687;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#29289;&#20307;&#25235;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26465;&#20214;&#21270;&#26426;&#22120;&#20154;&#25235;&#21462;&#65288;LCRG&#65289;&#26088;&#22312;&#24320;&#21457;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#26426;&#22120;&#20154;&#26469;&#36827;&#34892;&#29289;&#20307;&#30340;&#25509;&#22320;&#21644;&#25235;&#21462;&#12290;&#34429;&#28982;&#33021;&#22815;&#35782;&#21035;&#20010;&#20154;&#29289;&#21697;&#22914;&#8220;&#25105;&#30340;&#38065;&#21253;&#8221;&#30340;&#26426;&#22120;&#20154;&#21487;&#20197;&#26356;&#33258;&#28982;&#22320;&#19982;&#38750;&#19987;&#23478;&#29992;&#25143;&#20132;&#20114;&#65292;&#20294;&#24403;&#21069;&#30340;LCRG&#31995;&#32479;&#20027;&#35201;&#38480;&#21046;&#26426;&#22120;&#20154;&#21482;&#33021;&#29702;&#35299;&#19968;&#33324;&#34920;&#36798;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;GraspMine&#30340;&#20219;&#21153;&#22330;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#36890;&#36807;&#20174;&#21333;&#19968;&#20154;&#26426;&#20132;&#20114;&#20013;&#23398;&#20064;&#23450;&#20301;&#21644;&#25235;&#21462;&#20010;&#20154;&#29289;&#20307;&#12290;&#20026;&#20102;&#35299;&#20915;GraspMine&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20010;&#24615;&#21270;&#25235;&#21462;&#20195;&#29702;&#65288;PGA&#65289;&#65292;&#36890;&#36807;&#23558;&#29992;&#25143;&#25552;&#20379;&#30340;&#20449;&#24687;&#20256;&#25773;&#21040;&#20010;&#20154;&#29289;&#20307;&#19978;&#65292;&#36890;&#36807;Reminiscence-&#29992;&#25143;&#29615;&#22659;&#20013;&#30340;&#19968;&#31995;&#21015;&#21407;&#22987;&#22270;&#20687;&#65292;&#33719;&#21462;&#20010;&#20154;&#29289;&#20307;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PGA&#36890;&#36807;&#29992;&#25143;&#23637;&#31034;&#24102;&#26377;&#30456;&#20851;&#25351;&#31034;&#22120;&#30340;&#20010;&#20154;&#29289;&#20307;&#65292;&#24182;&#20197;&#26059;&#36716;&#30340;&#26041;&#24335;&#26816;&#26597;&#29289;&#20307;&#26469;&#33719;&#21462;&#20010;&#20154;&#29289;&#20307;&#20449;&#24687;&#12290;&#26681;&#25454;&#25152;&#33719;&#24471;&#30340;&#20449;&#24687;&#65292;PGA&#20026;&#29289;&#20307;&#36827;&#34892;&#20266;&#26631;&#31614;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language-Conditioned Robotic Grasping (LCRG) aims to develop robots that ground and grasp objects based on natural language instructions. While robots capable of recognizing personal objects like "my wallet" can interact more naturally with non-expert users, current LCRG systems primarily limit robots to understanding only generic expressions. To this end, we introduce a task scenario GraspMine with a novel dataset that aims to locate and grasp personal objects given personal indicators via learning from a single human-robot interaction. To address GraspMine, we propose Personalized Grasping Agent (PGA), that learns personal objects by propagating user-given information through a Reminiscence-a collection of raw images from the user's environment. Specifically, PGA acquires personal object information by a user presenting a personal object with its associated indicator, followed by PGA inspecting the object by rotating it. Based on the acquired information, PGA pseudo-labels objects in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26500;&#24314;&#20102;&#25972;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#31070;&#32463;&#20284;&#28982;&#36817;&#20284;&#26041;&#27861;&#65292;&#20351;&#29992;&#22240;&#26524;&#21367;&#31215;&#24182;&#34892;&#35780;&#20272;&#25972;&#20010;&#26102;&#38388;&#24207;&#21015;&#30340;&#20284;&#28982;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#24577;&#23398;&#21644;&#27969;&#34892;&#30149;&#23398;&#27169;&#22411;&#36827;&#34892;&#20934;&#30830;&#25512;&#26029;&#24182;&#26174;&#33879;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.12544</link><description>&lt;p&gt;
&#25972;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#31070;&#32463;&#20284;&#28982;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Neural Likelihood Approximation for Integer Valued Time Series Data. (arXiv:2310.12544v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26500;&#24314;&#20102;&#25972;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#31070;&#32463;&#20284;&#28982;&#36817;&#20284;&#26041;&#27861;&#65292;&#20351;&#29992;&#22240;&#26524;&#21367;&#31215;&#24182;&#34892;&#35780;&#20272;&#25972;&#20010;&#26102;&#38388;&#24207;&#21015;&#30340;&#20284;&#28982;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#24577;&#23398;&#21644;&#27969;&#34892;&#30149;&#23398;&#27169;&#22411;&#36827;&#34892;&#20934;&#30830;&#25512;&#26029;&#24182;&#26174;&#33879;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#29702;&#21644;&#29983;&#29289;&#31185;&#23398;&#20013;&#65292;&#23450;&#20041;&#22312;&#25972;&#25968;&#20540;&#29366;&#24577;&#31354;&#38388;&#19978;&#30340;&#38543;&#26426;&#36807;&#31243;&#24456;&#24120;&#35265;&#12290;&#36825;&#20123;&#27169;&#22411;&#29992;&#20110;&#25429;&#25417;&#23567;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#65292;&#20854;&#20013;&#20010;&#20307;&#32676;&#20307;&#30340;&#20010;&#20307;&#23646;&#24615;&#19981;&#33021;&#34987;&#24573;&#35270;&#65292;&#38543;&#26426;&#25928;&#24212;&#24456;&#37325;&#35201;&#12290;&#30001;&#20110;&#20284;&#28982;&#30340;&#22797;&#26434;&#24615;&#65292;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25512;&#26029;&#36825;&#20123;&#27169;&#22411;&#30340;&#21442;&#25968;&#26159;&#22256;&#38590;&#30340;&#65307;&#30446;&#21069;&#30340;&#26041;&#27861;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#27169;&#25311;&#65292;&#35745;&#31639;&#25104;&#26412;&#38750;&#24120;&#39640;&#26114;&#65292;&#20197;&#33267;&#20110;&#38590;&#20197;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22240;&#26524;&#21367;&#31215;&#26500;&#24314;&#20102;&#29992;&#20110;&#25972;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#31070;&#32463;&#20284;&#28982;&#36817;&#20284;&#26041;&#27861;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#24182;&#34892;&#35780;&#20272;&#25972;&#20010;&#26102;&#38388;&#24207;&#21015;&#30340;&#20284;&#28982;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19968;&#20123;&#29983;&#24577;&#23398;&#21644;&#27969;&#34892;&#30149;&#23398;&#27169;&#22411;&#36827;&#34892;&#25512;&#26029;&#26469;&#28436;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#22320;&#36817;&#20284;&#30495;&#23454;&#30340;&#21518;&#39564;&#27010;&#29575;&#65292;&#21516;&#26102;&#22312;&#24403;&#21069;&#26041;&#27861;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26174;&#33879;&#30340;&#35745;&#31639;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic processes defined on integer valued state spaces are popular within the physical and biological sciences. These models are necessary for capturing the dynamics of small systems where the individual nature of the populations cannot be ignored and stochastic effects are important. The inference of the parameters of such models, from time series data, is difficult due to intractability of the likelihood; current methods, based on simulations of the underlying model, can be so computationally expensive as to be prohibitive. In this paper we construct a neural likelihood approximation for integer valued time series data using causal convolutions, which allows us to evaluate the likelihood of the whole time series in parallel. We demonstrate our method by performing inference on a number of ecological and epidemiological models, showing that we can accurately approximate the true posterior while achieving significant computational speed ups in situations where current methods stru
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#20026;&#22825;&#25991;&#23398;&#30028;&#30340;&#20316;&#32773;&#12289;&#35780;&#23457;&#20154;&#21592;&#21644;&#32534;&#36753;&#25552;&#20379;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23454;&#26045;&#21644;&#32467;&#26524;&#25253;&#21578;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#20197;&#30830;&#20445;&#32467;&#26524;&#20934;&#30830;&#24615;&#12289;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12528</link><description>&lt;p&gt;
&#20026;&#22825;&#25991;&#23398;&#26500;&#24314;&#26377;&#24433;&#21709;&#21147;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#65306;&#30740;&#31350;&#20154;&#21592;&#21644;&#35780;&#23457;&#20154;&#21592;&#30340;&#26368;&#20339;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Constructing Impactful Machine Learning Research for Astronomy: Best Practices for Researchers and Reviewers. (arXiv:2310.12528v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12528
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#20026;&#22825;&#25991;&#23398;&#30028;&#30340;&#20316;&#32773;&#12289;&#35780;&#23457;&#20154;&#21592;&#21644;&#32534;&#36753;&#25552;&#20379;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23454;&#26045;&#21644;&#32467;&#26524;&#25253;&#21578;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#20197;&#30830;&#20445;&#32467;&#26524;&#20934;&#30830;&#24615;&#12289;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#36805;&#36895;&#25104;&#20026;&#22825;&#25991;&#23398;&#30028;&#30340;&#39318;&#36873;&#24037;&#20855;&#12290;&#23427;&#34987;&#24212;&#29992;&#20110;&#21508;&#31181;&#27874;&#38271;&#21644;&#38382;&#39064;&#65292;&#20174;&#30636;&#21464;&#29289;&#30340;&#20998;&#31867;&#21040;&#23439;&#35266;&#27169;&#25311;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#22120;&#65292;&#24182;&#25913;&#21464;&#20102;&#25105;&#20204;&#29983;&#25104;&#21644;&#25253;&#21578;&#31185;&#23398;&#32467;&#26524;&#30340;&#33539;&#24335;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36825;&#31867;&#26041;&#27861;&#20063;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#26368;&#20339;&#23454;&#36341;&#12289;&#25361;&#25112;&#21644;&#32570;&#28857;&#65292;&#28982;&#32780;&#30446;&#21069;&#22312;&#22825;&#20307;&#29289;&#29702;&#39046;&#22495;&#30340;&#25991;&#29486;&#20013;&#24448;&#24448;&#27809;&#26377;&#23436;&#25972;&#22320;&#25253;&#36947;&#12290;&#36890;&#36807;&#26412;&#25991;&#65292;&#25105;&#20204;&#26088;&#22312;&#21521;&#22825;&#25991;&#23398;&#30028;&#30340;&#20316;&#32773;&#12289;&#35780;&#23457;&#20154;&#21592;&#21644;&#32534;&#36753;&#25552;&#20379;&#19968;&#20221;&#20837;&#38376;&#25351;&#21335;&#65292;&#25945;&#25480;&#22914;&#20309;&#23454;&#26045;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#20197;&#30830;&#20445;&#32467;&#26524;&#20934;&#30830;&#24615;&#12289;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#30340;&#26041;&#24335;&#25253;&#21578;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has rapidly become a tool of choice for the astronomical community. It is being applied across a wide range of wavelengths and problems, from the classification of transients to neural network emulators of cosmological simulations, and is shifting paradigms about how we generate and report scientific results. At the same time, this class of method comes with its own set of best practices, challenges, and drawbacks, which, at present, are often reported on incompletely in the astrophysical literature. With this paper, we aim to provide a primer to the astronomical community, including authors, reviewers, and editors, on how to implement machine learning models and report their results in a way that ensures the accuracy of the results, reproducibility of the findings, and usefulness of the method.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27979;&#35797;&#25253;&#21578;&#30340;&#20108;&#20998;&#31867;&#38382;&#39064;&#24615;&#33021;&#20998;&#25968;&#21644;&#23454;&#39564;&#35774;&#32622;&#19968;&#33268;&#24615;&#30340;&#25968;&#20540;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#32479;&#35745;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2310.12527</link><description>&lt;p&gt;
&#27979;&#35797;&#25253;&#21578;&#30340;&#20108;&#20998;&#31867;&#38382;&#39064;&#24615;&#33021;&#20998;&#25968;&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Testing the Consistency of Performance Scores Reported for Binary Classification Problems. (arXiv:2310.12527v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12527
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27979;&#35797;&#25253;&#21578;&#30340;&#20108;&#20998;&#31867;&#38382;&#39064;&#24615;&#33021;&#20998;&#25968;&#21644;&#23454;&#39564;&#35774;&#32622;&#19968;&#33268;&#24615;&#30340;&#25968;&#20540;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#32479;&#35745;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#20998;&#31867;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#24212;&#29992;&#33539;&#22260;&#28085;&#30422;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#12290;&#31185;&#23398;&#23478;&#22312;&#36827;&#34892;&#22522;&#30784;&#30740;&#31350;&#25110;&#20248;&#21270;&#23454;&#38469;&#24212;&#29992;&#26102;&#65292;&#36890;&#24120;&#20250;&#26681;&#25454;&#20934;&#30830;&#29575;&#12289;&#25935;&#24863;&#24230;&#21644;&#29305;&#24322;&#24230;&#31561;&#24615;&#33021;&#25351;&#26631;&#35780;&#20272;&#21644;&#25490;&#21517;&#20998;&#31867;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#25253;&#21578;&#30340;&#24615;&#33021;&#24471;&#20998;&#24182;&#19981;&#24635;&#26159;&#21487;&#38752;&#30340;&#30740;&#31350;&#25490;&#21517;&#20381;&#25454;&#12290;&#36825;&#21487;&#33021;&#24402;&#22240;&#20110;&#26410;&#20844;&#24320;&#25110;&#38750;&#24120;&#35268;&#30340;&#20132;&#21449;&#39564;&#35777;&#23454;&#36341;&#12289;&#25490;&#29256;&#38169;&#35823;&#21644;&#20854;&#20182;&#22240;&#32032;&#12290;&#22312;&#32473;&#23450;&#30340;&#23454;&#39564;&#35774;&#32622;&#20013;&#65292;&#20855;&#26377;&#29305;&#23450;&#25968;&#37327;&#30340;&#38451;&#24615;&#21644;&#38452;&#24615;&#27979;&#35797;&#39033;&#65292;&#22823;&#22810;&#25968;&#24615;&#33021;&#24471;&#20998;&#21487;&#20197;&#20551;&#35774;&#30340;&#29305;&#23450;&#12289;&#30456;&#20114;&#30456;&#20851;&#30340;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25968;&#20540;&#25216;&#26415;&#26469;&#35780;&#20272;&#25253;&#21578;&#30340;&#24615;&#33021;&#24471;&#20998;&#30340;&#19968;&#33268;&#24615;&#21644;&#20551;&#35774;&#30340;&#23454;&#39564;&#35774;&#32622;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#32479;&#35745;&#25512;&#26029;&#65292;&#32780;&#26159;&#20351;&#29992;&#25968;&#20540;&#26041;&#27861;&#26469;&#35782;&#21035;&#19981;&#19968;&#33268;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Binary classification is a fundamental task in machine learning, with applications spanning various scientific domains. Whether scientists are conducting fundamental research or refining practical applications, they typically assess and rank classification techniques based on performance metrics such as accuracy, sensitivity, and specificity. However, reported performance scores may not always serve as a reliable basis for research ranking. This can be attributed to undisclosed or unconventional practices related to cross-validation, typographical errors, and other factors. In a given experimental setup, with a specific number of positive and negative test items, most performance scores can assume specific, interrelated values. In this paper, we introduce numerical techniques to assess the consistency of reported performance scores and the assumed experimental setup. Importantly, the proposed approach does not rely on statistical inference but uses numerical methods to identify inconsi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28385;&#24847;&#24230;&#27748;&#26222;&#26862;&#25277;&#26679;&#30340;&#24182;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;STS-PBO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#25935;&#40657;&#30418;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#29575;&#22833;&#30495;&#29702;&#35770;&#26500;&#24314;&#20102;&#19968;&#20010;&#24179;&#34913;&#20449;&#24687;&#37327;&#21644;&#27425;&#20248;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#37319;&#29992;Blahut-Arimoto&#31639;&#27861;&#35745;&#31639;&#30446;&#26631;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.12526</link><description>&lt;p&gt;
&#24182;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#26102;&#25935;&#40657;&#30418;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;&#65306;&#22522;&#20110;&#28385;&#24847;&#24230;&#27748;&#26222;&#26862;&#25277;&#26679;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Parallel Bayesian Optimization Using Satisficing Thompson Sampling for Time-Sensitive Black-Box Optimization. (arXiv:2310.12526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28385;&#24847;&#24230;&#27748;&#26222;&#26862;&#25277;&#26679;&#30340;&#24182;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;STS-PBO&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#25935;&#40657;&#30418;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#29575;&#22833;&#30495;&#29702;&#35770;&#26500;&#24314;&#20102;&#19968;&#20010;&#24179;&#34913;&#20449;&#24687;&#37327;&#21644;&#27425;&#20248;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#37319;&#29992;Blahut-Arimoto&#31639;&#27861;&#35745;&#31639;&#30446;&#26631;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#40657;&#30418;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#23454;&#38469;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;BO&#26041;&#27861;&#26088;&#22312;&#23398;&#20064;&#26368;&#20248;&#35299;&#65292;&#20294;&#24403;&#21442;&#25968;&#31354;&#38388;&#26497;&#22823;&#25110;&#38382;&#39064;&#26102;&#25935;&#26102;&#65292;&#36825;&#21487;&#33021;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#22312;&#36825;&#20123;&#32972;&#26223;&#19979;&#65292;&#36716;&#21521;&#38656;&#35201;&#26356;&#23569;&#20449;&#24687;&#30340;&#28385;&#24847;&#35299;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#38024;&#23545;&#26102;&#25935;&#40657;&#30418;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#28385;&#24847;&#24230;&#27748;&#26222;&#26862;&#25277;&#26679;&#30340;&#24182;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;STS-PBO&#65289;&#26041;&#27861;&#65292;&#21253;&#25324;&#21516;&#27493;&#21644;&#24322;&#27493;&#29256;&#26412;&#12290;&#25105;&#20204;&#23558;&#30446;&#26631;&#20174;&#26368;&#20248;&#35299;&#36716;&#31227;&#33267;&#26356;&#26131;&#23398;&#20064;&#30340;&#28385;&#24847;&#35299;&#12290;&#24341;&#20837;&#20102;&#29575;&#22833;&#30495;&#29702;&#35770;&#26500;&#24314;&#19968;&#20010;&#24179;&#34913;&#38656;&#35201;&#23398;&#20064;&#30340;&#20449;&#24687;&#37327;&#21644;&#27425;&#20248;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#37319;&#29992;Blahut-Arimoto&#31639;&#27861;&#35745;&#31639;&#30446;&#26631;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is widely used for black-box optimization problems, and have been shown to perform well in various real-world tasks. However, most of the existing BO methods aim to learn the optimal solution, which may become infeasible when the parameter space is extremely large or the problem is time-sensitive. In these contexts, switching to a satisficing solution that requires less information can result in better performance. In this work, we focus on time-sensitive black-box optimization problems and propose satisficing Thompson sampling-based parallel Bayesian optimization (STS-PBO) approaches, including synchronous and asynchronous versions. We shift the target from an optimal solution to a satisficing solution that is easier to learn. The rate-distortion theory is introduced to construct a loss function that balances the amount of information that needs to be learned with sub-optimality, and the Blahut-Arimoto algorithm is adopted to compute the target solution that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;ChouBERT&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#30417;&#27979;&#26893;&#29289;&#20581;&#24247;&#23041;&#32961;&#12290;&#36890;&#36807;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#65292;&#21487;&#20197;&#26816;&#27979;&#24182;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#20174;&#32780;&#35299;&#20915;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20013;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#21644;&#32454;&#31890;&#24230;&#35821;&#20041;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12522</link><description>&lt;p&gt;
&#29992;&#20110;&#30417;&#27979;&#26893;&#29289;&#20581;&#24247;&#23041;&#32961;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65306;&#19968;&#31181;&#22522;&#20110;ChouBERT&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition for Monitoring Plant Health Threats in Tweets: a ChouBERT Approach. (arXiv:2310.12522v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;ChouBERT&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#30417;&#27979;&#26893;&#29289;&#20581;&#24247;&#23041;&#32961;&#12290;&#36890;&#36807;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#65292;&#21487;&#20197;&#26816;&#27979;&#24182;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#20174;&#32780;&#35299;&#20915;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20013;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#21644;&#32454;&#31890;&#24230;&#35821;&#20041;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#23494;&#20892;&#19994;&#30340;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#22330;&#26223;&#26159;&#21033;&#29992;&#20256;&#24863;&#22120;&#21644;&#25968;&#25454;&#20998;&#26512;&#25216;&#26415;&#26816;&#27979;&#21644;&#27979;&#37327;&#20316;&#29289;&#20581;&#24247;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#21644;&#32454;&#31890;&#24230;&#35821;&#20041;&#36164;&#28304;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#23545;&#25991;&#26412;&#25968;&#25454;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20892;&#27665;&#20043;&#38388;&#26085;&#30410;&#22686;&#38271;&#30340;&#20114;&#32852;&#24615;&#21644;&#22312;&#32447;&#20892;&#19994;&#31038;&#21306;&#30340;&#20986;&#29616;&#20351;&#24471;Twitter&#31561;&#31038;&#20132;&#23186;&#20307;&#25104;&#20026;&#26816;&#27979;&#38476;&#29983;&#30340;&#26893;&#29289;&#20581;&#24247;&#20107;&#20214;&#30340;&#24191;&#27867;&#21442;&#19982;&#24179;&#21488;&#65292;&#21069;&#25552;&#26159;&#25105;&#20204;&#33021;&#22815;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#12290;ChouBERT&#26159;&#19968;&#20010;&#27861;&#35821;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#28041;&#21450;&#26893;&#29289;&#20581;&#24247;&#38382;&#39064;&#35266;&#23519;&#30340;&#25512;&#25991;&#65292;&#24182;&#23545;&#26410;&#35265;&#36807;&#30340;&#33258;&#28982;&#28798;&#23475;&#20855;&#26377;&#19968;&#33324;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#36827;&#19968;&#27493;&#30740;&#31350;ChouBERT&#22312;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#30340;&#20196;&#29260;&#32423;&#27880;&#37322;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important application scenario of precision agriculture is detecting and measuring crop health threats using sensors and data analysis techniques. However, the textual data are still under-explored among the existing solutions due to the lack of labelled data and fine-grained semantic resources. Recent research suggests that the increasing connectivity of farmers and the emergence of online farming communities make social media like Twitter a participatory platform for detecting unfamiliar plant health events if we can extract essential information from unstructured textual data. ChouBERT is a French pre-trained language model that can identify Tweets concerning observations of plant health issues with generalizability on unseen natural hazards. This paper tackles the lack of labelled data by further studying ChouBERT's know-how on token-level annotation tasks over small labeled sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#25915;&#20987;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#21160;&#29983;&#25104;&#35780;&#20272;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;ChatGPT&#21644;Natural Questions&#65288;NQ&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.12516</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#25915;&#20987;&#23454;&#29616;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#24187;&#35273;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks. (arXiv:2310.12516v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#36801;&#31227;&#30340;&#23545;&#25239;&#25915;&#20987;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#21160;&#29983;&#25104;&#35780;&#20272;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;ChatGPT&#21644;Natural Questions&#65288;NQ&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#21644;&#26816;&#32034;&#22686;&#24378;&#25216;&#26415;&#38450;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24187;&#35273;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#34913;&#37327;LLM&#30340;&#21487;&#38752;&#24615;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20154;&#24037;&#35780;&#20272;&#25968;&#25454;&#23545;&#20110;&#35768;&#22810;&#20219;&#21153;&#21644;&#39046;&#22495;&#26469;&#35828;&#24182;&#19981;&#21487;&#29992;&#19988;&#21487;&#33021;&#23384;&#22312;&#25968;&#25454;&#27844;&#28431;&#12290;&#21463;&#21040;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#36890;&#36807;&#36866;&#24403;&#20462;&#25913;LLM&#22312;&#20854;&#20013;&#34920;&#29616;&#24544;&#23454;&#30340;&#29616;&#26377;&#25968;&#25454;&#26469;&#33258;&#21160;&#29983;&#25104;&#35780;&#20272;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26694;&#26550;AutoDebug&#65292;&#20351;&#29992;&#25552;&#31034;&#38142;&#25509;&#26469;&#29983;&#25104;&#20197;&#38382;&#31572;&#31034;&#20363;&#24418;&#24335;&#30340;&#21487;&#36801;&#31227;&#23545;&#25239;&#25915;&#20987;&#12290;&#25105;&#20204;&#24076;&#26395;&#20102;&#35299;&#36825;&#20123;&#31034;&#20363;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#35302;&#21457;&#20102;LLM&#30340;&#24187;&#35273;&#34892;&#20026;&#12290;&#25105;&#20204;&#20351;&#29992;ChatGPT&#23454;&#29616;&#20102;AutoDebug&#65292;&#24182;&#23545;&#19968;&#20010;&#28909;&#38376;&#30340;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#25968;&#25454;&#38598;Natural Questions&#65288;NQ&#65289;&#30340;&#20004;&#20010;&#21464;&#20307;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although remarkable progress has been achieved in preventing large language model (LLM) hallucinations using instruction tuning and retrieval augmentation, it remains challenging to measure the reliability of LLMs using human-crafted evaluation data which is not available for many tasks and domains and could suffer from data leakage. Inspired by adversarial machine learning, this paper aims to develop a method of automatically generating evaluation data by appropriately modifying existing data on which LLMs behave faithfully. Specifically, this paper presents AutoDebug, an LLM-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples. We seek to understand the extent to which these examples trigger the hallucination behaviors of LLMs.  We implement AutoDebug using ChatGPT and evaluate the resulting two variants of a popular open-domain question-answering dataset, Natural Questions (NQ), on a collection of open-sour
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36817;&#20284;&#21452;&#36793;&#21305;&#37197;&#38382;&#39064;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;WeaveNet&#65292;&#36890;&#36807;&#20445;&#30041;&#36793;&#20449;&#24687;&#24182;&#23494;&#38598;&#20256;&#36882;&#28040;&#24687;&#65292;&#36991;&#20813;&#20102;&#33410;&#28857;&#20449;&#24687;&#20002;&#22833;&#30340;&#38382;&#39064;&#12290;&#22312;&#20844;&#24179;&#31283;&#23450;&#21305;&#37197;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;&#19982;&#26368;&#26032;&#31639;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.12515</link><description>&lt;p&gt;
&#29992;&#20110;&#36817;&#20284;&#21452;&#36793;&#21305;&#37197;&#38382;&#39064;&#30340;WeaveNet
&lt;/p&gt;
&lt;p&gt;
WeaveNet for Approximating Two-sided Matching Problems. (arXiv:2310.12515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36817;&#20284;&#21452;&#36793;&#21305;&#37197;&#38382;&#39064;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;WeaveNet&#65292;&#36890;&#36807;&#20445;&#30041;&#36793;&#20449;&#24687;&#24182;&#23494;&#38598;&#20256;&#36882;&#28040;&#24687;&#65292;&#36991;&#20813;&#20102;&#33410;&#28857;&#20449;&#24687;&#20002;&#22833;&#30340;&#38382;&#39064;&#12290;&#22312;&#20844;&#24179;&#31283;&#23450;&#21305;&#37197;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;&#19982;&#26368;&#26032;&#31639;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21305;&#37197;&#26159;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#20026;&#26377;&#38480;&#36164;&#28304;&#36827;&#34892;&#26368;&#20248;&#20998;&#37197;&#30340;&#19968;&#39033;&#22522;&#26412;&#25216;&#26415;&#65292;&#35813;&#20219;&#21153;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#30446;&#26631;&#12289;&#26465;&#20214;&#21644;&#32422;&#26463;&#65292;&#28982;&#32780;&#65292;&#29992;&#20110;&#21305;&#37197;&#30340;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#23578;&#26410;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;WeaveNet&#65292;&#19987;&#38376;&#29992;&#20110;&#22788;&#29702;&#20108;&#20998;&#22270;&#12290;&#30001;&#20110;&#20108;&#20998;&#22270;&#36890;&#24120;&#26159;&#23494;&#38598;&#30340;&#65292;&#24120;&#35268;&#30340;GNN&#26550;&#26500;&#22312;&#28145;&#24230;&#22534;&#21472;&#26102;&#20250;&#36807;&#24230;&#24179;&#28369;&#20174;&#32780;&#20002;&#22833;&#33410;&#28857;&#20449;&#24687;&#65292;&#36825;&#23545;&#20110;&#35299;&#20915;&#21305;&#37197;&#38382;&#39064;&#26159;&#19981;&#29702;&#24819;&#30340;&#12290;WeaveNet&#36890;&#36807;&#20445;&#30041;&#36793;&#20449;&#24687;&#24182;&#23494;&#38598;&#20256;&#36882;&#28040;&#24687;&#20197;&#36798;&#21040;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#36825;&#31181;&#29616;&#35937;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#65292;&#25105;&#20204;&#36817;&#20284;&#35299;&#20915;&#20102;&#19968;&#20010;&#24378;NP&#22256;&#38590;&#38382;&#39064;&#8212;&#8212;&#20844;&#24179;&#31283;&#23450;&#21305;&#37197;&#12290;&#23613;&#31649;&#36825;&#20010;&#38382;&#39064;&#22266;&#26377;&#22256;&#38590;&#65292;&#19988;&#32593;&#32476;&#26159;&#36890;&#29992;&#35774;&#35745;&#30340;&#65292;&#20294;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#19982;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#31283;&#23450;&#21305;&#37197;&#30340;&#26368;&#26032;&#31639;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Matching, a task to optimally assign limited resources under constraints, is a fundamental technology for society. The task potentially has various objectives, conditions, and constraints; however, the efficient neural network architecture for matching is underexplored. This paper proposes a novel graph neural network (GNN), \textit{WeaveNet}, designed for bipartite graphs. Since a bipartite graph is generally dense, general GNN architectures lose node-wise information by over-smoothing when deeply stacked. Such a phenomenon is undesirable for solving matching problems. WeaveNet avoids it by preserving edge-wise information while passing messages densely to reach a better solution. To evaluate the model, we approximated one of the \textit{strongly NP-hard} problems, \textit{fair stable matching}. Despite its inherent difficulties and the network's general purpose design, our model reached a comparative performance with state-of-the-art algorithms specially designed for stable matching 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SalUn&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;"&#26435;&#37325;&#26174;&#33879;&#24615;"&#30340;&#27010;&#24565;&#65292;&#23558;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#36951;&#24536;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.12508</link><description>&lt;p&gt;
SalUn&#65306;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#26435;&#37325;&#26174;&#33879;&#24615;&#22686;&#24378;&#26426;&#22120;&#36951;&#24536;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20013;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation. (arXiv:2310.12508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12508
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SalUn&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;"&#26435;&#37325;&#26174;&#33879;&#24615;"&#30340;&#27010;&#24565;&#65292;&#23558;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#36951;&#24536;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#27861;&#35268;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#24050;&#25104;&#20026;&#22686;&#24378;&#24403;&#21069;AI&#27169;&#22411;&#30340;&#20449;&#20219;&#21644;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MU&#26041;&#27861;&#36890;&#24120;&#22312;&#36951;&#24536;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#36328;&#39046;&#22495;&#36866;&#29992;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MU&#20013;&#30340;&#8220;&#26435;&#37325;&#26174;&#33879;&#24615;&#8221;&#27010;&#24565;&#65292;&#20511;&#37492;&#20102;&#27169;&#22411;&#35299;&#37322;&#20013;&#30340;&#36755;&#20837;&#26174;&#33879;&#24615;&#12290;&#36825;&#19968;&#21019;&#26032;&#23558;MU&#30340;&#20851;&#27880;&#28857;&#20174;&#25972;&#20010;&#27169;&#22411;&#24341;&#23548;&#21040;&#20102;&#20855;&#20307;&#30340;&#27169;&#22411;&#26435;&#37325;&#19978;&#65292;&#25552;&#39640;&#20102;&#20854;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#26174;&#33879;&#24615;&#36951;&#24536;&#65288;SalUn&#65289;&#30340;&#26041;&#27861;&#23558;&#20854;&#19982;&#8220;&#31934;&#30830;&#8221;&#36951;&#24536;&#65288;&#22312;&#21024;&#38500;&#36951;&#24536;&#25968;&#25454;&#38598;&#21518;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65289;&#30340;&#24615;&#33021;&#24046;&#36317;&#32553;&#23567;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SalUn&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#20013;&#26377;&#25928;&#28040;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#24433;&#21709;&#30340;&#26377;&#21407;&#21017;&#30340;MU&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;SalUn&#21487;&#22312;&#22270;&#29255;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#25830;&#38500;&#36951;&#24536;&#25968;&#25454;&#12289;&#31867;&#21035;&#25110;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often grapple with limitations in unlearning accuracy, stability, and cross-domain applicability. To address these challenges, we introduce the concept of 'weight saliency' in MU, drawing parallels with input saliency in model explanation. This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning (SalUn) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting dataset). To the best of our knowledge, SalUn is the first principled MU approach adaptable enough to effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation. For example, Sa
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#26469;&#32463;&#27982;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25915;&#20987;&#25552;&#31034;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27169;&#20223;&#20154;&#31867;&#29983;&#25104;&#30340;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20132;&#20114;&#26469;&#21152;&#24378;&#21463;&#25915;&#20987;&#30340;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;LLMs&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12505</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32418;&#38431;&#25915;&#20987;&#25552;&#31034;&#29983;&#25104;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Attack Prompt Generation for Red Teaming and Defending Large Language Models. (arXiv:2310.12505v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12505
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#26469;&#32463;&#27982;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25915;&#20987;&#25552;&#31034;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27169;&#20223;&#20154;&#31867;&#29983;&#25104;&#30340;&#25552;&#31034;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20132;&#20114;&#26469;&#21152;&#24378;&#21463;&#25915;&#20987;&#30340;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;LLMs&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#21463;&#21040;&#32418;&#38431;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;LLMs&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#25163;&#21160;&#25110;&#33258;&#21160;&#26041;&#27861;&#26500;&#24314;&#25915;&#20987;&#25552;&#31034;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#26500;&#24314;&#25104;&#26412;&#21644;&#36136;&#37327;&#19978;&#37117;&#23384;&#22312;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#25163;&#21160;&#21644;&#33258;&#21160;&#26041;&#27861;&#65292;&#32463;&#27982;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25915;&#20987;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32771;&#34385;&#21040;&#26032;&#20852;LLMs&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25915;&#20987;&#26694;&#26550;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25351;&#23548;LLMs&#27169;&#20223;&#20154;&#31867;&#29983;&#25104;&#30340;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38450;&#24481;&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#25915;&#20987;&#26694;&#26550;&#30340;&#36845;&#20195;&#20132;&#20114;&#26469;&#23545;&#21463;&#25915;&#20987;&#30340;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#22686;&#24378;&#23427;&#20204;&#23545;&#32418;&#38431;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#12290;&#23545;&#19981;&#21516;LLMs&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#31995;&#21015;&#25915;&#20987;&#25552;&#31034;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;SAP&#65292;&#22823;&#23567;&#19981;&#21516;&#65292;&#20415;&#20110;&#30740;&#31350;&#32773;&#36827;&#34892;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are susceptible to red teaming attacks, which can induce LLMs to generate harmful content. Previous research constructs attack prompts via manual or automatic methods, which have their own limitations on construction cost and quality. To address these issues, we propose an integrated approach that combines manual and automatic methods to economically generate high-quality attack prompts. Specifically, considering the impressive capabilities of newly emerged LLMs, we propose an attack framework to instruct LLMs to mimic human-generated prompts through in-context learning. Furthermore, we propose a defense framework that fine-tunes victim LLMs through iterative interactions with the attack framework to enhance their safety against red teaming attacks. Extensive experiments on different LLMs validate the effectiveness of our proposed attack and defense frameworks. Additionally, we release a series of attack prompts datasets named SAP with varying sizes, facili
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;GRU&#21644;Shapley&#20540;&#35299;&#37322;&#30340;&#32654;&#24335;&#26399;&#26435;&#23450;&#20215;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21106;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#26399;&#26435;&#30340;&#20215;&#26684;&#12290;</title><link>http://arxiv.org/abs/2310.12500</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;GRU&#21644;Shapley&#20540;&#35299;&#37322;&#30340;&#32654;&#24335;&#26399;&#26435;&#23450;&#20215;
&lt;/p&gt;
&lt;p&gt;
American Option Pricing using Self-Attention GRU and Shapley Value Interpretation. (arXiv:2310.12500v1 [q-fin.PR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;GRU&#21644;Shapley&#20540;&#35299;&#37322;&#30340;&#32654;&#24335;&#26399;&#26435;&#23450;&#20215;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21106;&#25968;&#25454;&#38598;&#21644;&#24314;&#31435;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#26399;&#26435;&#30340;&#20215;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26399;&#26435;&#20316;&#20026;&#19968;&#31181;&#20851;&#38190;&#30340;&#37329;&#34701;&#24037;&#20855;&#65292;&#22312;&#35777;&#21048;&#24066;&#22330;&#20013;&#34987;&#25237;&#36164;&#32773;&#29992;&#26469;&#31649;&#29702;&#21644;&#20943;&#36731;&#25237;&#36164;&#39118;&#38505;&#12290;&#31934;&#30830;&#39044;&#27979;&#26399;&#26435;&#30340;&#29616;&#20215;&#20351;&#25237;&#36164;&#32773;&#33021;&#22815;&#20570;&#20986;&#26126;&#26234;&#21644;&#39640;&#25928;&#30340;&#20915;&#31574;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;SPY&#65288;ETF&#65289;&#26399;&#26435;&#30340;&#20215;&#26684;&#12290;&#25105;&#20204;&#39318;&#20808;&#26681;&#25454;&#36135;&#24065;&#20215;&#20540;&#21644;&#21040;&#26399;&#26085;&#30340;&#26631;&#20934;&#23558;&#21407;&#22987;&#25968;&#25454;&#38598;&#20998;&#25104;15&#20010;&#23376;&#38598;&#12290;&#23545;&#20110;&#27599;&#20010;&#23376;&#38598;&#65292;&#25105;&#20204;&#21305;&#37197;&#30456;&#24212;&#30340;&#32654;&#22269;&#25919;&#24220;&#20538;&#21048;&#21033;&#29575;&#21644;&#38544;&#21547;&#27874;&#21160;&#29575;&#25351;&#25968;&#12290;&#36825;&#31181;&#20998;&#21106;&#26041;&#24335;&#21487;&#20197;&#26356;&#28145;&#20837;&#22320;&#25506;&#32034;&#26080;&#39118;&#38505;&#21033;&#29575;&#21644;&#22522;&#30784;&#27874;&#21160;&#29575;&#23545;&#26399;&#26435;&#23450;&#20215;&#30340;&#24433;&#21709;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#12289;&#33258;&#27880;&#24847;LSTM&#21644;&#33258;&#27880;&#24847;GRU&#65292;&#19982;&#20256;&#32479;&#30340;&#20108;&#39033;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Options, serving as a crucial financial instrument, are used by investors to manage and mitigate their investment risks within the securities market. Precisely predicting the present price of an option enables investors to make informed and efficient decisions. In this paper, we propose a machine learning method for forecasting the prices of SPY (ETF) option based on gated recurrent unit (GRU) and self-attention mechanism. We first partitioned the raw dataset into 15 subsets according to moneyness and days to maturity criteria. For each subset, we matched the corresponding U.S. government bond rates and Implied Volatility Indices. This segmentation allows for a more insightful exploration of the impacts of risk-free rates and underlying volatility on option pricing. Next, we built four different machine learning models, including multilayer perceptron (MLP), long short-term memory (LSTM), self-attention LSTM, and self-attention GRU in comparison to the traditional binomial model. The e
&lt;/p&gt;</description></item><item><title>&#31867;&#26364;&#21704;&#39039;&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#65288;QMWD&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#24230;&#37327;&#20004;&#20010;&#30697;&#38453;&#20043;&#38388;&#30456;&#24322;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#25110;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2310.12498</link><description>&lt;p&gt;
&#31867;&#26364;&#21704;&#39039;&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Quasi Manhattan Wasserstein Distance. (arXiv:2310.12498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12498
&lt;/p&gt;
&lt;p&gt;
&#31867;&#26364;&#21704;&#39039;&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#65288;QMWD&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#24230;&#37327;&#20004;&#20010;&#30697;&#38453;&#20043;&#38388;&#30456;&#24322;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#25110;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#26364;&#21704;&#39039;&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#65288;QMWD&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#24230;&#37327;&#20004;&#20010;&#30697;&#38453;&#20043;&#38388;&#30456;&#24322;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#23427;&#23558;&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#30340;&#20803;&#32032;&#19982;&#29305;&#23450;&#30340;&#21464;&#25442;&#30456;&#32467;&#21512;&#12290;&#19982;&#26364;&#21704;&#39039;&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#65288;MWD&#65289;&#30456;&#27604;&#65292;QMWD&#20855;&#26377;&#26356;&#22909;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;QMWD&#29305;&#21035;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#25110;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#35814;&#32454;&#38416;&#36848;&#20102;QMWD&#30340;&#35745;&#31639;&#12289;&#22797;&#26434;&#24615;&#20998;&#26512;&#20197;&#21450;&#19982;WD&#21644;MWD&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Quasi Manhattan Wasserstein Distance (QMWD) is a metric designed to quantify the dissimilarity between two matrices by combining elements of the Wasserstein Distance with specific transformations. It offers improved time and space complexity compared to the Manhattan Wasserstein Distance (MWD) while maintaining accuracy. QMWD is particularly advantageous for large datasets or situations with limited computational resources. This article provides a detailed explanation of QMWD, its computation, complexity analysis, and comparisons with WD and MWD.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;SDGym&#65292;&#19968;&#20010;&#22522;&#20110;&#31995;&#32479;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#20302;&#20195;&#30721;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#24211;&#65292;&#36890;&#36807;&#29983;&#25104;&#23450;&#21046;&#30340;RL&#29615;&#22659;&#26469;&#35299;&#20915;&#29616;&#23454;&#29615;&#22659;&#35774;&#35745;&#30340;&#22256;&#38590;&#65292;&#20174;&#32780;&#26500;&#24314;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#24378;&#22823;&#26234;&#33021;&#20307;&#12290;</title><link>http://arxiv.org/abs/2310.12494</link><description>&lt;p&gt;
SDGym: &#20351;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#20302;&#20195;&#30721;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
SDGym: Low-Code Reinforcement Learning Environments using System Dynamics Models. (arXiv:2310.12494v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12494
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;SDGym&#65292;&#19968;&#20010;&#22522;&#20110;&#31995;&#32479;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#20302;&#20195;&#30721;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#24211;&#65292;&#36890;&#36807;&#29983;&#25104;&#23450;&#21046;&#30340;RL&#29615;&#22659;&#26469;&#35299;&#20915;&#29616;&#23454;&#29615;&#22659;&#35774;&#35745;&#30340;&#22256;&#38590;&#65292;&#20174;&#32780;&#26500;&#24314;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#24378;&#22823;&#26234;&#33021;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31639;&#27861;&#24178;&#39044;&#23545;&#31038;&#20250;&#30340;&#38271;&#26399;&#24433;&#21709;&#23545;&#20110;&#23454;&#29616;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#35780;&#20272;&#31574;&#30053;&#36890;&#24120;&#38590;&#20197;&#24212;&#23545;&#31038;&#20250;&#30340;&#22797;&#26434;&#12289;&#36866;&#24212;&#24615;&#21644;&#21160;&#24577;&#24615;&#12290;&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#26159;&#20248;&#21270;&#21160;&#24577;&#29615;&#22659;&#19979;&#20915;&#31574;&#30340;&#24378;&#22823;&#26041;&#27861;&#65292;&#20294;&#29616;&#23454;&#29615;&#22659;&#35774;&#35745;&#30340;&#22256;&#38590;&#20173;&#28982;&#26159;&#26500;&#24314;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#24378;&#22823;&#26234;&#33021;&#20307;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#31995;&#32479;&#21160;&#21147;&#23398;&#65288;SD&#65289;&#39046;&#22495;&#20316;&#20026;&#19968;&#31181;&#34917;&#20805;&#26041;&#27861;&#65292;&#32435;&#20837;&#21327;&#20316;&#20223;&#30495;&#27169;&#22411;&#35268;&#33539;&#23454;&#36341;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SDGym&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;OpenAI Gym&#26694;&#26550;&#26500;&#24314;&#30340;&#20302;&#20195;&#30721;&#24211;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;SD&#27169;&#25311;&#27169;&#22411;&#29983;&#25104;&#23450;&#21046;&#30340;RL&#29615;&#22659;&#12290;&#36890;&#36807;&#19968;&#39033;&#21487;&#34892;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#21487;&#20197;&#20174;&#29616;&#26377;SD&#27169;&#22411;&#21644;&#23569;&#37327;&#37197;&#32622;&#20195;&#30721;&#20013;&#29983;&#25104;&#20855;&#26377;&#35268;&#33539;&#33391;&#22909;&#12289;&#20016;&#23500;&#30340;RL&#29615;&#22659;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;SDGym&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the long-term impact of algorithmic interventions on society is vital to achieving responsible AI. Traditional evaluation strategies often fall short due to the complex, adaptive and dynamic nature of society. While reinforcement learning (RL) can be a powerful approach for optimizing decisions in dynamic settings, the difficulty of realistic environment design remains a barrier to building robust agents that perform well in practical settings. To address this issue we tap into the field of system dynamics (SD) as a complementary method that incorporates collaborative simulation model specification practices. We introduce SDGym, a low-code library built on the OpenAI Gym framework which enables the generation of custom RL environments based on SD simulation models. Through a feasibility study we validate that well specified, rich RL environments can be generated from preexisting SD models and a few lines of configuration code. We demonstrate the capabilities of the SDGym 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#20132;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#36816;&#31639;&#31526;&#65292;&#36890;&#36807;&#26680;&#31215;&#20998;&#31639;&#23376;&#30340;&#29305;&#24449;&#20998;&#35299;&#21644;&#31070;&#32463;&#36817;&#20284;&#30340;&#29305;&#24449;&#20989;&#25968;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#19978;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20845;&#20010;&#26631;&#20934;&#31070;&#32463;&#36816;&#31639;&#31526;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.12487</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#20132;&#27880;&#24847;&#21147;&#25552;&#21319;&#36816;&#31639;&#31526;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improved Operator Learning by Orthogonal Attention. (arXiv:2310.12487v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#20132;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#36816;&#31639;&#31526;&#65292;&#36890;&#36807;&#26680;&#31215;&#20998;&#31639;&#23376;&#30340;&#29305;&#24449;&#20998;&#35299;&#21644;&#31070;&#32463;&#36817;&#20284;&#30340;&#29305;&#24449;&#20989;&#25968;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#19978;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20845;&#20010;&#26631;&#20934;&#31070;&#32463;&#36816;&#31639;&#31526;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36816;&#31639;&#31526;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#65292;&#21463;&#21040;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#36816;&#31639;&#31526;&#24050;&#25104;&#20026;&#30456;&#20851;&#30740;&#31350;&#30340;&#20027;&#27969;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27880;&#24847;&#26426;&#21046;&#20013;&#21442;&#25968;&#25968;&#37327;&#24040;&#22823;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#36807;&#25311;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#26680;&#31215;&#20998;&#31639;&#23376;&#30340;&#29305;&#24449;&#20998;&#35299;&#21644;&#31070;&#32463;&#36817;&#20284;&#30340;&#29305;&#24449;&#20989;&#25968;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#27491;&#20132;&#27880;&#24847;&#21147;&#12290;&#27491;&#20132;&#21270;&#33258;&#28982;&#22320;&#23545;&#32467;&#26524;&#31070;&#32463;&#36816;&#31639;&#31526;&#26045;&#21152;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#65292;&#26377;&#21161;&#20110;&#25269;&#25239;&#36807;&#25311;&#21512;&#21644;&#25552;&#21319;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#21253;&#25324;&#27491;&#24120;&#21644;&#38750;&#27491;&#24120;&#20960;&#20309;&#24418;&#29366;&#30340;&#20845;&#20010;&#26631;&#20934;&#31070;&#32463;&#36816;&#31639;&#31526;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#31454;&#20105;&#23545;&#25163;&#65292;&#24182;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operators, as an efficient surrogate model for learning the solutions of PDEs, have received extensive attention in the field of scientific machine learning. Among them, attention-based neural operators have become one of the mainstreams in related research. However, existing approaches overfit the limited training data due to the considerable number of parameters in the attention mechanism. To address this, we develop an orthogonal attention based on the eigendecomposition of the kernel integral operator and the neural approximation of eigenfunctions. The orthogonalization naturally poses a proper regularization effect on the resulting neural operator, which aids in resisting overfitting and boosting generalization. Experiments on six standard neural operator benchmark datasets comprising both regular and irregular geometries show that our method can outperform competing baselines with decent margins.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26435;&#37325;&#21644;&#36755;&#20986;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#24674;&#22797;Transformer&#27169;&#22411;&#20013;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#30740;&#31350;&#32467;&#26524;&#26263;&#31034;&#27169;&#22411;&#35774;&#35745;&#23384;&#22312;&#28508;&#22312;&#30340;&#28431;&#27934;&#12290;</title><link>http://arxiv.org/abs/2310.12462</link><description>&lt;p&gt;
&#25581;&#31034;Transformer&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65306;&#22522;&#20110;&#27880;&#24847;&#21147;&#26435;&#37325;&#30340;&#25968;&#25454;&#24674;&#22797;&#30340;&#29702;&#35770;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unmasking Transformers: A Theoretical Approach to Data Recovery via Attention Weights. (arXiv:2310.12462v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26435;&#37325;&#21644;&#36755;&#20986;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#24674;&#22797;Transformer&#27169;&#22411;&#20013;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;&#30740;&#31350;&#32467;&#26524;&#26263;&#31034;&#27169;&#22411;&#35774;&#35745;&#23384;&#22312;&#28508;&#22312;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;Transformer&#24050;&#32463;&#25104;&#20026;&#20102;&#19968;&#31181;&#20027;&#23548;&#30340;&#26550;&#26500;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#26377;&#20851;&#36825;&#20123;&#27169;&#22411;&#22788;&#29702;&#25968;&#25454;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#30340;&#38382;&#39064;&#24050;&#32463;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#26412;&#25991;&#38024;&#23545;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65306;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;Transformer&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#21644;&#36755;&#20986;&#26469;&#24674;&#22797;&#36755;&#20837;&#25968;&#25454;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;$L(X)$&#20174;&#32473;&#23450;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;$W = QK^\top$&#21644;&#36755;&#20986;$B$&#20013;&#24674;&#22797;&#36755;&#20837;&#25968;&#25454;$X$&#65292;&#20854;&#20013;$X \in \mathbb{R}^{d \times n}$&#65292;$W \in \mathbb{R}^{d \times d}$&#65292;$B \in \mathbb{R}^{n \times n}$&#12290;&#36825;&#20010;&#25439;&#22833;&#20989;&#25968;&#25429;&#25417;&#20102;&#39044;&#26399;&#36755;&#20986;&#19982;&#23454;&#38469;&#36755;&#20986;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#23616;&#37096;&#21270;&#20998;&#23618;&#26426;&#21046;&#65288;Localized Layer-wise Mechanism&#65292;LLM&#65289;&#20855;&#26377;&#37325;&#35201;&#30340;&#24433;&#21709;&#65292;&#34920;&#26126;&#27169;&#22411;&#35774;&#35745;&#23384;&#22312;&#28508;&#22312;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of deep learning, transformers have emerged as a dominant architecture, particularly in natural language processing tasks. However, with their widespread adoption, concerns regarding the security and privacy of the data processed by these models have arisen. In this paper, we address a pivotal question: Can the data fed into transformers be recovered using their attention weights and outputs? We introduce a theoretical framework to tackle this problem. Specifically, we present an algorithm that aims to recover the input data $X \in \mathbb{R}^{d \times n}$ from given attention weights $W = QK^\top \in \mathbb{R}^{d \times d}$ and output $B \in \mathbb{R}^{n \times n}$ by minimizing the loss function $L(X)$. This loss function captures the discrepancy between the expected output and the actual output of the transformer. Our findings have significant implications for the Localized Layer-wise Mechanism (LLM), suggesting potential vulnerabilities in the model's design from a s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#32452;&#21367;&#31215;&#30456;&#23545;&#20110;&#26631;&#20934;&#21367;&#31215;&#30340;&#36817;&#20284;&#31243;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24179;&#34913;&#32452;&#21367;&#31215;&#30340;&#26032;&#21464;&#20307;&#65292;&#23427;&#22312;&#22686;&#21152;&#36739;&#23567;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#23637;&#29616;&#20986;&#26356;&#39640;&#30340;&#36817;&#20284;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.12461</link><description>&lt;p&gt;
&#24179;&#34913;&#32452;&#21367;&#31215;&#65306;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#24615;&#35780;&#20272;&#30340;&#25913;&#36827;&#32452;&#21367;&#31215;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Balanced Group Convolution: An Improved Group Convolution Based on Approximability Estimates. (arXiv:2310.12461v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#32452;&#21367;&#31215;&#30456;&#23545;&#20110;&#26631;&#20934;&#21367;&#31215;&#30340;&#36817;&#20284;&#31243;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24179;&#34913;&#32452;&#21367;&#31215;&#30340;&#26032;&#21464;&#20307;&#65292;&#23427;&#22312;&#22686;&#21152;&#36739;&#23567;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#23637;&#29616;&#20986;&#26356;&#39640;&#30340;&#36817;&#20284;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#21367;&#31215;&#23618;&#20013;&#30340;&#36890;&#36947;&#25968;&#37327;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24615;&#33021;&#25552;&#21319;&#20276;&#38543;&#30528;&#26356;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#23548;&#33268;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#20943;&#23569;&#23427;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#32452;&#21367;&#31215;&#65292;&#36890;&#36807;&#20998;&#32452;&#36890;&#36947;&#26377;&#25928;&#22320;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20851;&#20110;&#32452;&#21367;&#31215;&#22914;&#20309;&#36817;&#20284;&#26631;&#20934;&#21367;&#31215;&#36824;&#27809;&#26377;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#12290;&#26412;&#25991;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#32452;&#21367;&#31215;&#30456;&#23545;&#20110;&#32452;&#25968;&#30340;&#26631;&#20934;&#21367;&#31215;&#30340;&#36817;&#20284;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24179;&#34913;&#32452;&#21367;&#31215;&#30340;&#26032;&#21464;&#20307;&#65292;&#23427;&#22312;&#20165;&#22686;&#21152;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#26356;&#39640;&#30340;&#36817;&#20284;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#24179;&#34913;&#32452;&#21367;&#31215;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of neural networks has been significantly improved by increasing the number of channels in convolutional layers. However, this increase in performance comes with a higher computational cost, resulting in numerous studies focused on reducing it. One promising approach to address this issue is group convolution, which effectively reduces the computational cost by grouping channels. However, to the best of our knowledge, there has been no theoretical analysis on how well the group convolution approximates the standard convolution. In this paper, we mathematically analyze the approximation of the group convolution to the standard convolution with respect to the number of groups. Furthermore, we propose a novel variant of the group convolution called balanced group convolution, which shows a higher approximation with a small additional computational cost. We provide experimental results that validate our theoretical findings and demonstrate the superior performance of the ba
&lt;/p&gt;</description></item><item><title>MuseGNN&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#21644;&#21487;&#25910;&#25947;&#30340;&#22823;&#35268;&#27169;&#22270;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#20943;&#23569;&#22522;&#20110;&#37319;&#26679;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#21516;&#26102;&#20316;&#20026;&#39044;&#27979;&#29305;&#24449;&#21644;&#33021;&#37327;&#20989;&#25968;&#26368;&#23567;&#21270;&#32773;&#65292;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12457</link><description>&lt;p&gt;
MuseGNN: &#21487;&#35299;&#37322;&#21644;&#21487;&#25910;&#25947;&#30340;&#22823;&#35268;&#27169;&#22270;&#31070;&#32463;&#32593;&#32476;&#23618;
&lt;/p&gt;
&lt;p&gt;
MuseGNN: Interpretable and Convergent Graph Neural Network Layers at Scale. (arXiv:2310.12457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12457
&lt;/p&gt;
&lt;p&gt;
MuseGNN&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#21644;&#21487;&#25910;&#25947;&#30340;&#22823;&#35268;&#27169;&#22270;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#20943;&#23569;&#22522;&#20110;&#37319;&#26679;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#21516;&#26102;&#20316;&#20026;&#39044;&#27979;&#29305;&#24449;&#21644;&#33021;&#37327;&#20989;&#25968;&#26368;&#23567;&#21270;&#32773;&#65292;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33021;&#22815;&#24314;&#27169;&#20855;&#26377;&#36328;&#23454;&#20363;&#20851;&#31995;&#30340;&#25968;&#25454;&#30340;&#35768;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#20013;&#65292;&#19968;&#31867;&#37325;&#35201;&#30340;&#23376;&#31867;&#28041;&#21450;&#35774;&#35745;&#23618;&#65292;&#20854;&#27491;&#21521;&#20256;&#36882;&#36845;&#20195;&#22320;&#20943;&#23569;&#24863;&#20852;&#36259;&#30340;&#22270;&#27491;&#21017;&#21270;&#33021;&#37327;&#20989;&#25968;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#36755;&#20986;&#23618;&#20135;&#29983;&#30340;&#33410;&#28857;&#23884;&#20837;&#26082;&#21487;&#20316;&#20026;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#33410;&#28857;&#20998;&#31867;&#65289;&#30340;&#39044;&#27979;&#29305;&#24449;&#65292;&#21448;&#21487;&#20316;&#20026;&#33021;&#37327;&#20989;&#25968;&#26368;&#23567;&#21270;&#32773;&#65292;&#32487;&#25215;&#20102;&#21487;&#38752;&#30340;&#24402;&#32435;&#20559;&#32622;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#20197;&#36825;&#31181;&#26041;&#24335;&#26500;&#24314;&#30340;GNN&#26550;&#26500;&#30340;&#25193;&#23637;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#27491;&#21521;&#20256;&#36882;&#30340;&#25910;&#25947;&#21487;&#33021;&#28041;&#21450;&#20855;&#26377;&#30456;&#24403;&#28145;&#24230;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#33021;&#37327;&#20989;&#25968;&#21644;&#21487;&#25193;&#23637;&#30340;GNN&#23618;&#65292;&#36890;&#36807;&#22312;&#26576;&#20123;&#35774;&#32622;&#20013;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#25351;&#23548;&#65292;&#36845;&#20195;&#22320;&#20943;&#23569;&#23427;&#12290;&#25105;&#20204;&#36824;&#22522;&#20110;&#36825;&#20123;&#35774;&#35745;&#23454;&#20363;&#21270;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;GNN&#26550;&#26500;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#22343;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among the many variants of graph neural network (GNN) architectures capable of modeling data with cross-instance relations, an important subclass involves layers designed such that the forward pass iteratively reduces a graph-regularized energy function of interest. In this way, node embeddings produced at the output layer dually serve as both predictive features for solving downstream tasks (e.g., node classification) and energy function minimizers that inherit desirable inductive biases and interpretability. However, scaling GNN architectures constructed in this way remains challenging, in part because the convergence of the forward pass may involve models with considerable depth. To tackle this limitation, we propose a sampling-based energy function and scalable GNN layers that iteratively reduce it, guided by convergence guarantees in certain settings. We also instantiate a full GNN architecture based on these designs, and the model achieves competitive accuracy and scalability whe
&lt;/p&gt;</description></item><item><title>MTS-LOF&#26159;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#36974;&#25377;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#30340;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#20026;&#21307;&#30103;&#24212;&#29992;&#25552;&#20379;&#26356;&#22797;&#26434;&#12289;&#26356;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#36974;&#25377;&#31574;&#30053;&#23454;&#29616;&#20102;&#36974;&#25377;&#19981;&#21464;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.12451</link><description>&lt;p&gt;
MTS-LOF: &#21033;&#29992;&#36974;&#25377;&#19981;&#21464;&#29305;&#24449;&#36827;&#34892;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MTS-LOF: Medical Time-Series Representation Learning via Occlusion-Invariant Features. (arXiv:2310.12451v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12451
&lt;/p&gt;
&lt;p&gt;
MTS-LOF&#26159;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#36974;&#25377;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#30340;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#20026;&#21307;&#30103;&#24212;&#29992;&#25552;&#20379;&#26356;&#22797;&#26434;&#12289;&#26356;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#36974;&#25377;&#31574;&#30053;&#23454;&#29616;&#20102;&#36974;&#25377;&#19981;&#21464;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#19981;&#21487;&#25110;&#32570;&#65292;&#20026;&#30142;&#30149;&#35786;&#26029;&#12289;&#27835;&#30103;&#35745;&#21010;&#21644;&#24739;&#32773;&#31649;&#29702;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#27934;&#23519;&#21147;&#12290;&#20808;&#36827;&#30340;&#20256;&#24863;&#22120;&#25216;&#26415;&#24102;&#26469;&#30340;&#25968;&#25454;&#22797;&#26434;&#24615;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#20351;&#25968;&#25454;&#26631;&#27880;&#38754;&#20020;&#25361;&#25112;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#19968;&#31181;&#21464;&#38761;&#24615;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#23545;&#24191;&#27867;&#20154;&#24037;&#26631;&#27880;&#30340;&#38656;&#27714;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;MTS-LOF&#12290;MTS-LOF&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#36974;&#25377;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29420;&#29305;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#36825;&#20123;&#25216;&#26415;&#65292;MTS-LOF&#36890;&#36807;&#25552;&#20379;&#26356;&#22797;&#26434;&#12289;&#26356;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#22686;&#24378;&#20102;&#21307;&#30103;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;MTS-LOF&#37319;&#29992;&#22810;&#36974;&#25377;&#31574;&#30053;&#65292;&#20419;&#36827;&#20102;&#36974;&#25377;&#19981;&#21464;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical time series data are indispensable in healthcare, providing critical insights for disease diagnosis, treatment planning, and patient management. The exponential growth in data complexity, driven by advanced sensor technologies, has presented challenges related to data labeling. Self-supervised learning (SSL) has emerged as a transformative approach to address these challenges, eliminating the need for extensive human annotation. In this study, we introduce a novel framework for Medical Time Series Representation Learning, known as MTS-LOF. MTS-LOF leverages the strengths of contrastive learning and Masked Autoencoder (MAE) methods, offering a unique approach to representation learning for medical time series data. By combining these techniques, MTS-LOF enhances the potential of healthcare applications by providing more sophisticated, context-rich representations. Additionally, MTS-LOF employs a multi-masking strategy to facilitate occlusion-invariant feature learning. This appr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38750;&#21442;&#25968;&#21270;&#30340;&#20998;&#24067;&#32422;&#26463;&#26435;&#37325;&#65292;&#24182;&#21033;&#29992;&#26368;&#22823;&#29109;&#21407;&#29702;&#21644;&#26368;&#20248;&#20256;&#36755;&#24037;&#20855;&#35774;&#35745;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#23545;&#35266;&#27979;&#25968;&#25454;&#30340;&#26368;&#20248;&#26435;&#37325;&#35843;&#25972;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#23637;&#29616;&#20102;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12447</link><description>&lt;p&gt;
&#32422;&#26463;&#37325;&#21152;&#26435;&#20998;&#24067;&#65306;&#19968;&#31181;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Constrained Reweighting of Distributions: an Optimal Transport Approach. (arXiv:2310.12447v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#20256;&#36755;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38750;&#21442;&#25968;&#21270;&#30340;&#20998;&#24067;&#32422;&#26463;&#26435;&#37325;&#65292;&#24182;&#21033;&#29992;&#26368;&#22823;&#29109;&#21407;&#29702;&#21644;&#26368;&#20248;&#20256;&#36755;&#24037;&#20855;&#35774;&#35745;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#23545;&#35266;&#27979;&#25968;&#25454;&#30340;&#26368;&#20248;&#26435;&#37325;&#35843;&#25972;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#23637;&#29616;&#20102;&#28789;&#27963;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32463;&#24120;&#36935;&#21040;&#30340;&#38382;&#39064;&#26159;&#35201;&#35782;&#21035;&#20986;&#31526;&#21512;&#39044;&#23450;&#20041;&#30340;&#26435;&#37325;&#32422;&#26463;&#26465;&#20214;&#30340;&#35266;&#27979;&#25968;&#25454;&#30340;&#32463;&#39564;&#20998;&#24067;&#30340;&#26368;&#20248;&#35843;&#25972;&#29256;&#26412;&#12290;&#36825;&#20123;&#32422;&#26463;&#36890;&#24120;&#34920;&#29616;&#20026;&#23545;&#26435;&#37325;&#30340;&#30697;&#12289;&#23614;&#37096;&#34892;&#20026;&#12289;&#24418;&#29366;&#12289;&#27169;&#24335;&#25968;&#37327;&#31561;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#38750;&#21442;&#25968;&#21270;&#30340;&#20998;&#24067;&#32422;&#26463;&#26435;&#37325;&#24182;&#21033;&#29992;&#26368;&#22823;&#29109;&#21407;&#29702;&#21644;&#26368;&#20248;&#20256;&#36755;&#24037;&#20855;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#30830;&#20445;&#35266;&#27979;&#25968;&#25454;&#30340;&#26368;&#22823;&#29109;&#26435;&#37325;&#35843;&#25972;&#32463;&#39564;&#20998;&#24067;&#19982;&#39044;&#23450;&#30340;&#27010;&#29575;&#20998;&#24067;&#22312;&#26368;&#20248;&#20256;&#36755;&#24230;&#37327;&#19979;&#25509;&#36817;&#65292;&#24182;&#20801;&#35768;&#32454;&#24494;&#30340;&#20559;&#24046;&#12290;&#35813;&#26694;&#26550;&#30340;&#22810;&#21151;&#33021;&#24615;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#20854;&#20013;&#25968;&#25454;&#37325;&#21152;&#26435;&#26159;&#21512;&#29702;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We commonly encounter the problem of identifying an optimally weight adjusted version of the empirical distribution of observed data, adhering to predefined constraints on the weights. Such constraints often manifest as restrictions on the moments, tail behaviour, shapes, number of modes, etc., of the resulting weight adjusted empirical distribution. In this article, we substantially enhance the flexibility of such methodology by introducing a nonparametrically imbued distributional constraints on the weights, and developing a general framework leveraging the maximum entropy principle and tools from optimal transport. The key idea is to ensure that the maximum entropy weight adjusted empirical distribution of the observed data is close to a pre-specified probability distribution in terms of the optimal transport metric while allowing for subtle departures. The versatility of the framework is demonstrated in the context of three disparate applications where data re-weighting is warrante
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38271;&#31243;Transformer&#27169;&#22411;MASFormer&#65292;&#36890;&#36807;&#22312;&#23569;&#25968;&#23618;&#20351;&#29992;&#20840;&#23616;&#27880;&#24847;&#21147;&#21644;&#22312;&#20854;&#20182;&#23618;&#20351;&#29992;&#31232;&#30095;&#27880;&#24847;&#21147;&#65292;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#38271;&#24207;&#21015;&#30340;&#20219;&#21153;&#20013;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#24314;&#27169;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12442</link><description>&lt;p&gt;
&#39640;&#25928;&#38271;&#31243;Transformer&#65306;&#38656;&#35201;&#26356;&#22810;&#20851;&#27880;&#65292;&#20294;&#19981;&#19968;&#23450;&#22312;&#27599;&#19968;&#23618;&#37117;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer. (arXiv:2310.12442v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12442
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38271;&#31243;Transformer&#27169;&#22411;MASFormer&#65292;&#36890;&#36807;&#22312;&#23569;&#25968;&#23618;&#20351;&#29992;&#20840;&#23616;&#27880;&#24847;&#21147;&#21644;&#22312;&#20854;&#20182;&#23618;&#20351;&#29992;&#31232;&#30095;&#27880;&#24847;&#21147;&#65292;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#38271;&#24207;&#21015;&#30340;&#20219;&#21153;&#20013;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#24207;&#21015;&#20013;&#30340;&#38271;&#31243;&#21644;&#30701;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#20840;&#23616;&#27880;&#24847;&#26426;&#21046;&#30340;&#35745;&#31639;&#25104;&#26412;&#19982;&#24207;&#21015;&#38271;&#24230;&#21576;&#20108;&#27425;&#20851;&#31995;&#65292;&#22312;&#20855;&#26377;&#38271;&#24207;&#21015;&#30340;&#20219;&#21153;&#20013;&#65288;&#20363;&#22914;8k&#20010;&#26631;&#35760;&#30340;&#36755;&#20837;&#65289;&#26159;&#19981;&#21487;&#25215;&#21463;&#30340;&#12290;&#23613;&#31649;&#29616;&#26377;&#24037;&#20316;&#20013;&#24314;&#35758;&#20351;&#29992;&#31232;&#30095;&#27880;&#24847;&#21147;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#20294;&#23427;&#30340;&#24314;&#27169;&#33021;&#21147;&#26377;&#38480;&#65292;&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#38271;&#24207;&#21015;&#20013;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MASFormer&#65292;&#19968;&#31181;&#26131;&#20110;&#23454;&#29616;&#30340;&#21464;&#31181;Transformer&#65292;&#20855;&#26377;&#28151;&#21512;&#27880;&#24847;&#33539;&#22260;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MASFormer&#37197;&#22791;&#20102;&#20840;&#23616;&#27880;&#24847;&#21147;&#26469;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#20294;&#21482;&#22312;&#23569;&#25968;&#20960;&#23618;&#20351;&#29992;&#12290;&#23545;&#20110;&#21097;&#20313;&#23618;&#65292;MASFormer&#21482;&#37319;&#29992;&#31232;&#30095;&#27880;&#24847;&#21147;&#26469;&#25429;&#25417;&#30701;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;n&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MASFormer&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#19978;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#24314;&#27169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained transformer models have demonstrated remarkable performance across various natural language processing tasks. These models leverage the attention mechanism to capture long- and short-range dependencies in the sequence. However, the (full) attention mechanism incurs high computational cost quadratic in the sequence length, which is not affordable in tasks with long sequences, e.g., inputs with 8k tokens. Although sparse attention can be used to improve computational efficiency, as suggested in existing work, it has limited modeling capacity and often fails to capture complicated dependencies in long sequences. To tackle this challenge, we propose MASFormer, an easy-to-implement transformer variant with Mixed Attention Spans. Specifically, MASFormer is equipped with full attention to capture long-range dependencies, but only at a small number of layers. For the remaining layers, MASformer only employs sparse attention to capture short-range dependencies. Our experiments on n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38381;&#29615;&#23545;&#25239;&#35757;&#32451;&#65288;CAT&#65289;&#26694;&#26550;&#29992;&#20110;&#23454;&#29616;&#23433;&#20840;&#30340;&#31471;&#21040;&#31471;&#39550;&#39542;&#12290;CAT&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#22312;&#21160;&#24577;&#29983;&#25104;&#30340;&#23433;&#20840;&#20851;&#38190;&#24773;&#26223;&#19978;&#26469;&#19981;&#26029;&#25552;&#39640;&#39550;&#39542;&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#29983;&#25104;&#26041;&#27861;&#30456;&#27604;&#65292;CAT&#21487;&#20197;&#21457;&#36215;&#26356;&#39640;&#25928;&#30340;&#29289;&#29702;&#25915;&#20987;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.12432</link><description>&lt;p&gt;
CAT: &#29992;&#20110;&#23433;&#20840;&#31471;&#21040;&#31471;&#39550;&#39542;&#30340;&#38381;&#29615;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CAT: Closed-loop Adversarial Training for Safe End-to-End Driving. (arXiv:2310.12432v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38381;&#29615;&#23545;&#25239;&#35757;&#32451;&#65288;CAT&#65289;&#26694;&#26550;&#29992;&#20110;&#23454;&#29616;&#23433;&#20840;&#30340;&#31471;&#21040;&#31471;&#39550;&#39542;&#12290;CAT&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#22312;&#21160;&#24577;&#29983;&#25104;&#30340;&#23433;&#20840;&#20851;&#38190;&#24773;&#26223;&#19978;&#26469;&#19981;&#26029;&#25552;&#39640;&#39550;&#39542;&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#29983;&#25104;&#26041;&#27861;&#30456;&#27604;&#65292;CAT&#21487;&#20197;&#21457;&#36215;&#26356;&#39640;&#25928;&#30340;&#29289;&#29702;&#25915;&#20987;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#26469;&#35828;&#65292;&#39550;&#39542;&#23433;&#20840;&#26159;&#26368;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#29615;&#22659;&#22686;&#24378;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#38381;&#29615;&#23545;&#25239;&#35757;&#32451;&#65288;CAT&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#23433;&#20840;&#30340;&#31471;&#21040;&#31471;&#39550;&#39542;&#12290;CAT&#26088;&#22312;&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#22312;&#21160;&#24577;&#29983;&#25104;&#30340;&#23433;&#20840;&#20851;&#38190;&#24773;&#26223;&#19978;&#65292;&#25345;&#32493;&#25552;&#39640;&#39550;&#39542;&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37325;&#37319;&#26679;&#25216;&#26415;&#65292;&#36890;&#36807;&#27010;&#29575;&#20998;&#35299;&#23558;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#36716;&#21270;&#20026;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#65292;&#20854;&#20013;&#23545;&#25239;&#24615;&#20132;&#36890;&#29983;&#25104;&#34987;&#24314;&#27169;&#20026;&#26631;&#20934;&#36816;&#21160;&#39044;&#27979;&#23376;&#38382;&#39064;&#30340;&#20056;&#27861;&#12290;&#22240;&#27492;&#65292;CAT&#30456;&#27604;&#29616;&#26377;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#21457;&#36215;&#26356;&#39640;&#25928;&#30340;&#29289;&#29702;&#25915;&#20987;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#36845;&#20195;&#23398;&#20064;&#27969;&#31243;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#23558;CAT&#24212;&#29992;&#20110;MetaDrive&#27169;&#25311;&#22120;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driving safety is a top priority for autonomous vehicles. Orthogonal to prior work handling accident-prone traffic events by algorithm designs at the policy level, we investigate a Closed-loop Adversarial Training (CAT) framework for safe end-to-end driving in this paper through the lens of environment augmentation. CAT aims to continuously improve the safety of driving agents by training the agent on safety-critical scenarios that are dynamically generated over time. A novel resampling technique is developed to turn log-replay real-world driving scenarios into safety-critical ones via probabilistic factorization, where the adversarial traffic generation is modeled as the multiplication of standard motion prediction sub-problems. Consequently, CAT can launch more efficient physical attacks compared to existing safety-critical scenario generation methods and yields a significantly less computational cost in the iterative learning pipeline. We incorporate CAT into the MetaDrive simulator
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#37051;&#36817;&#24615;&#26469;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#39044;&#27979;&#25552;&#20379;&#20102;&#23616;&#37096;&#30340;&#35299;&#37322;&#24615;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#36741;&#30456;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20538;&#21048;&#23450;&#20215;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12428</link><description>&lt;p&gt;
&#23454;&#29616;&#38543;&#26426;&#26862;&#26519;&#30340;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#22686;&#24378;&#65306;&#22522;&#20110;&#37051;&#36817;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Enhanced Local Explainability of Random Forests: a Proximity-Based Approach. (arXiv:2310.12428v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12428
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#37051;&#36817;&#24615;&#26469;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20026;&#27169;&#22411;&#39044;&#27979;&#25552;&#20379;&#20102;&#23616;&#37096;&#30340;&#35299;&#37322;&#24615;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#36741;&#30456;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20538;&#21048;&#23450;&#20215;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#27169;&#22411;&#30340;&#26679;&#26412;&#22806;&#24615;&#33021;&#65292;&#21033;&#29992;&#20102;&#20219;&#20309;RF&#37117;&#21487;&#20197;&#34987;&#34920;&#36848;&#20026;&#33258;&#36866;&#24212;&#21152;&#26435;K&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#27169;&#22411;&#30340;&#20107;&#23454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;RF&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23398;&#21040;&#30340;&#28857;&#20043;&#38388;&#30340;&#37051;&#36817;&#24615;&#65292;&#23558;&#38543;&#26426;&#26862;&#26519;&#30340;&#39044;&#27979;&#37325;&#20889;&#20026;&#35757;&#32451;&#25968;&#25454;&#28857;&#30446;&#26631;&#26631;&#31614;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#12290;&#36825;&#31181;&#32447;&#24615;&#24615;&#36136;&#26377;&#21161;&#20110;&#22312;&#35757;&#32451;&#38598;&#35266;&#27979;&#20013;&#20026;&#20219;&#20309;&#27169;&#22411;&#39044;&#27979;&#29983;&#25104;&#23646;&#24615;&#65292;&#20174;&#32780;&#20026;RF&#39044;&#27979;&#25552;&#20379;&#20102;&#23616;&#37096;&#30340;&#35299;&#37322;&#24615;&#65292;&#34917;&#20805;&#20102;SHAP&#31561;&#24050;&#26377;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21017;&#20026;&#29305;&#24449;&#31354;&#38388;&#32500;&#24230;&#19978;&#30340;&#27169;&#22411;&#39044;&#27979;&#29983;&#25104;&#23646;&#24615;&#12290;&#25105;&#20204;&#22312;&#35757;&#32451;&#20110;&#32654;&#22269;&#20844;&#21496;&#20538;&#21048;&#20132;&#26131;&#25968;&#25454;&#30340;&#20538;&#21048;&#23450;&#20215;&#27169;&#22411;&#20013;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#21508;&#31181;&#29616;&#26377;&#30340;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We initiate a novel approach to explain the out of sample performance of random forest (RF) models by exploiting the fact that any RF can be formulated as an adaptive weighted K nearest-neighbors model. Specifically, we use the proximity between points in the feature space learned by the RF to re-write random forest predictions exactly as a weighted average of the target labels of training data points. This linearity facilitates a local notion of explainability of RF predictions that generates attributions for any model prediction across observations in the training set, and thereby complements established methods like SHAP, which instead generates attributions for a model prediction across dimensions of the feature space. We demonstrate this approach in the context of a bond pricing model trained on US corporate bond trades, and compare our approach to various existing approaches to model explainability.
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#19979;&#65292;&#33258;&#21160;&#20462;&#22797;&#22768;&#26126;&#24335;&#36719;&#20214;&#35268;&#33539;&#30340;&#26377;&#25928;&#25216;&#26415;&#38656;&#27714;&#24840;&#21457;&#31361;&#20986;&#12290;&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#21033;&#29992;ChatGPT&#20462;&#22797;Alloy&#22768;&#26126;&#24335;&#35821;&#35328;&#35268;&#33539;&#30340;&#25928;&#26524;&#65292;&#24182;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#33258;&#21160;&#20462;&#22797;&#36807;&#31243;&#20013;&#30340;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2310.12425</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#19979;&#30340;&#22768;&#26126;&#24335;&#36719;&#20214;&#35268;&#33539;&#33258;&#21160;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Automated Repair of Declarative Software Specifications in the Era of Large Language Models. (arXiv:2310.12425v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12425
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#19979;&#65292;&#33258;&#21160;&#20462;&#22797;&#22768;&#26126;&#24335;&#36719;&#20214;&#35268;&#33539;&#30340;&#26377;&#25928;&#25216;&#26415;&#38656;&#27714;&#24840;&#21457;&#31361;&#20986;&#12290;&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#21033;&#29992;ChatGPT&#20462;&#22797;Alloy&#22768;&#26126;&#24335;&#35821;&#35328;&#35268;&#33539;&#30340;&#25928;&#26524;&#65292;&#24182;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#33258;&#21160;&#20462;&#22797;&#36807;&#31243;&#20013;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#26126;&#24335;&#36719;&#20214;&#35268;&#33539;&#35821;&#35328;&#30340;&#24191;&#27867;&#37319;&#29992;&#20197;&#21450;&#20854;&#22312;&#35843;&#35797;&#26041;&#38754;&#30340;&#22256;&#38590;&#24615;&#65292;&#20984;&#26174;&#20102;&#23545;&#36866;&#29992;&#20110;&#27492;&#31867;&#35821;&#35328;&#30340;&#26377;&#25928;&#33258;&#21160;&#21270;&#20462;&#22797;&#25216;&#26415;&#30340;&#38656;&#27714;&#12290;&#30740;&#31350;&#20154;&#21592;&#26368;&#36817;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#20462;&#22797;&#22768;&#26126;&#24335;&#36719;&#20214;&#35268;&#33539;&#65292;&#22914;&#22522;&#20110;&#27169;&#26495;&#30340;&#20462;&#22797;&#12289;&#21453;&#39304;&#39537;&#21160;&#30340;&#36845;&#20195;&#20462;&#22797;&#21644;&#26377;&#30028;&#31351;&#20030;&#26041;&#27861;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#20026;&#33258;&#21160;&#20462;&#22797;&#22768;&#26126;&#24335;&#35268;&#33539;&#25552;&#20379;&#20102;&#26032;&#26426;&#20250;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21033;&#29992;OpenAI&#30340;ChatGPT&#20462;&#22797;&#29992;Alloy&#22768;&#26126;&#24335;&#35821;&#35328;&#32534;&#20889;&#30340;&#36719;&#20214;&#35268;&#33539;&#30340;&#25928;&#26524;&#12290;&#19982;&#21629;&#20196;&#24335;&#35821;&#35328;&#19981;&#21516;&#65292;Alloy&#20013;&#30340;&#35268;&#33539;&#19981;&#20250;&#34987;&#25191;&#34892;&#65292;&#32780;&#26159;&#34987;&#36716;&#25442;&#20026;&#36923;&#36753;&#20844;&#24335;&#65292;&#24182;&#20351;&#29992;&#21518;&#31471;&#32422;&#26463;&#27714;&#35299;&#22120;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#35782;&#21035;&#35268;&#33539;&#23454;&#20363;&#21644;&#26029;&#35328;&#30340;&#21453;&#20363;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#37325;&#28857;&#26159;ChatGPT&#22312;&#25913;&#36827;&#22768;&#26126;&#24335;&#35268;&#33539;&#20462;&#22797;&#33021;&#21147;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing adoption of declarative software specification languages, coupled with their inherent difficulty in debugging, has underscored the need for effective and automated repair techniques applicable to such languages. Researchers have recently explored various methods to automatically repair declarative software specifications, such as template-based repair, feedback-driven iterative repair, and bounded exhaustive approaches. The latest developments in large language models provide new opportunities for the automatic repair of declarative specifications. In this study, we assess the effectiveness of utilizing OpenAI's ChatGPT to repair software specifications written in the Alloy declarative language. Unlike imperative languages, specifications in Alloy are not executed but rather translated into logical formulas and evaluated using backend constraint solvers to identify specification instances and counterexamples to assertions. Our evaluation focuses on ChatGPT's ability to impr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#31639;&#27861;&#20559;&#35265;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#24615;&#21035;&#20559;&#35265;&#21644;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#23454;&#39564;&#35777;&#26126;&#20102;&#22240;&#26524;&#27169;&#22411;&#22312;&#20943;&#36731;&#24615;&#21035;&#20559;&#35265;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36731;&#24494;&#25552;&#39640;&#20102;&#25972;&#20307;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.12421</link><description>&lt;p&gt;
&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#26816;&#27979;&#21644;&#20943;&#36731;&#20108;&#20803;&#20998;&#31867;&#20013;&#30340;&#31639;&#27861;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Detecting and Mitigating Algorithmic Bias in Binary Classification using Causal Modeling. (arXiv:2310.12421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#31639;&#27861;&#20559;&#35265;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#22312;&#24615;&#21035;&#20559;&#35265;&#21644;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#23454;&#39564;&#35777;&#26126;&#20102;&#22240;&#26524;&#27169;&#22411;&#22312;&#20943;&#36731;&#24615;&#21035;&#20559;&#35265;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36731;&#24494;&#25552;&#39640;&#20102;&#25972;&#20307;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#31639;&#27861;&#20559;&#35265;&#12290;&#25105;&#20204;&#31616;&#35201;&#20171;&#32461;&#20102;&#22240;&#26524;&#24314;&#27169;&#30340;&#27010;&#24565;&#24182;&#27010;&#36848;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;UC Irvine&#26426;&#22120;&#23398;&#20064;&#24211;&#20013;&#21487;&#19979;&#36733;&#30340;&#25104;&#24180;&#20154;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#24320;&#21457;&#20102;&#65288;1&#65289;&#19968;&#20010;&#34987;&#35270;&#20026;&#40657;&#31665;&#30340;&#39044;&#27979;&#27169;&#22411;&#21644;&#65288;2&#65289;&#19968;&#20010;&#29992;&#20110;&#20943;&#36731;&#20559;&#35265;&#30340;&#22240;&#26524;&#27169;&#22411;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#24615;&#21035;&#20559;&#35265;&#21644;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#39044;&#27979;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#32479;&#35745;&#26174;&#33879;&#24615;&#65288;p&lt;0.05&#65289;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#23637;&#31034;&#20102;&#22240;&#26524;&#27169;&#22411;&#22312;&#20943;&#36731;&#24615;&#21035;&#20559;&#35265;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25972;&#20307;&#20998;&#31867;&#20934;&#30830;&#29575;&#30340;&#36731;&#24494;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#26041;&#27861;&#30452;&#35266;&#26131;&#25026;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#29616;&#26377;&#32479;&#35745;&#36719;&#20214;&#24037;&#20855;&#65288;&#22914;R&#20013;&#30340;&#8220;lavaan&#8221;&#65289;&#23454;&#29616;&#12290;&#22240;&#27492;&#65292;&#23427;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#24182;&#20419;&#36827;&#20102;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes the use of causal modeling to detect and mitigate algorithmic bias. We provide a brief description of causal modeling and a general overview of our approach. We then use the Adult dataset, which is available for download from the UC Irvine Machine Learning Repository, to develop (1) a prediction model, which is treated as a black box, and (2) a causal model for bias mitigation. In this paper, we focus on gender bias and the problem of binary classification. We show that gender bias in the prediction model is statistically significant at the 0.05 level. We demonstrate the effectiveness of the causal model in mitigating gender bias by cross-validation. Furthermore, we show that the overall classification accuracy is improved slightly. Our novel approach is intuitive, easy-to-use, and can be implemented using existing statistical software tools such as "lavaan" in R. Hence, it enhances explainability and promotes trust.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#26799;&#24230;&#29305;&#24449;&#23398;&#20064;&#30340;&#32479;&#19968;&#20998;&#26512;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#20856;&#22411;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#26377;&#36259;&#30340;&#23398;&#20064;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2310.12408</link><description>&lt;p&gt;
&#36890;&#36807;&#26799;&#24230;&#29305;&#24449;&#23398;&#20064;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Provable Guarantees for Neural Networks via Gradient Feature Learning. (arXiv:2310.12408v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#26799;&#24230;&#29305;&#24449;&#23398;&#20064;&#30340;&#32479;&#19968;&#20998;&#26512;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#20856;&#22411;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#26377;&#36259;&#30340;&#23398;&#20064;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#65292;&#20294;&#30446;&#21069;&#30340;&#29702;&#35770;&#20998;&#26512;&#19981;&#36275;&#20197;&#29702;&#35299;&#20854;&#25104;&#21151;&#65292;&#20363;&#22914;&#31070;&#32463;&#20999;&#32447;&#26680;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#20854;&#20851;&#38190;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#65292;&#32780;&#26368;&#36817;&#23545;&#29305;&#24449;&#23398;&#20064;&#30340;&#20998;&#26512;&#36890;&#24120;&#26159;&#38382;&#39064;&#29305;&#23450;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#38024;&#23545;&#30001;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#21452;&#23618;&#32593;&#32476;&#12290;&#35813;&#26694;&#26550;&#20197;&#26799;&#24230;&#29305;&#24449;&#23398;&#20064;&#21407;&#29702;&#20026;&#26680;&#24515;&#65292;&#24182;&#36890;&#36807;&#22312;&#20960;&#20010;&#20856;&#22411;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#26469;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#20363;&#22914;&#39640;&#26031;&#28151;&#21512;&#21644;&#22855;&#20598;&#20989;&#25968;&#12290;&#35813;&#26694;&#26550;&#36824;&#25581;&#31034;&#20102;&#26377;&#36259;&#30340;&#32593;&#32476;&#23398;&#20064;&#29616;&#35937;&#65292;&#22914;&#36229;&#36234;&#26680;&#30340;&#29305;&#24449;&#23398;&#20064;&#21644;&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have achieved remarkable empirical performance, while the current theoretical analysis is not adequate for understanding their success, e.g., the Neural Tangent Kernel approach fails to capture their key feature learning ability, while recent analyses on feature learning are typically problem-specific. This work proposes a unified analysis framework for two-layer networks trained by gradient descent. The framework is centered around the principle of feature learning from gradients, and its effectiveness is demonstrated by applications in several prototypical problems, such as mixtures of Gaussians and parity functions. The framework also sheds light on interesting network learning phenomena such as feature learning beyond kernels and the lottery ticket hypothesis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#22686;&#24378;&#30340;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#36741;&#21161;&#30340;&#40065;&#26834;&#22810;&#30446;&#26631;&#36319;&#36394;&#65292;&#22312;&#24378;&#26434;&#27874;&#29615;&#22659;&#20013;&#21033;&#29992;&#38647;&#36798;&#20256;&#24863;&#22120;&#30340;&#27979;&#37327;&#20449;&#24687;&#26469;&#22686;&#24378;&#26434;&#27874;&#21076;&#38500;&#21644;&#25968;&#25454;&#20851;&#32852;&#65292;&#20174;&#32780;&#25552;&#39640;&#30446;&#26631;&#36319;&#36394;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12407</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#22686;&#24378;&#30340;&#28040;&#24687;&#20256;&#36882;&#23454;&#29616;&#20998;&#31867;&#36741;&#21161;&#30340;&#40065;&#26834;&#22810;&#30446;&#26631;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Classification-Aided Robust Multiple Target Tracking Using Neural Enhanced Message Passing. (arXiv:2310.12407v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#22686;&#24378;&#30340;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#36741;&#21161;&#30340;&#40065;&#26834;&#22810;&#30446;&#26631;&#36319;&#36394;&#65292;&#22312;&#24378;&#26434;&#27874;&#29615;&#22659;&#20013;&#21033;&#29992;&#38647;&#36798;&#20256;&#24863;&#22120;&#30340;&#27979;&#37327;&#20449;&#24687;&#26469;&#22686;&#24378;&#26434;&#27874;&#21076;&#38500;&#21644;&#25968;&#25454;&#20851;&#32852;&#65292;&#20174;&#32780;&#25552;&#39640;&#30446;&#26631;&#36319;&#36394;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#24378;&#26434;&#27874;&#29615;&#22659;&#20013;&#21033;&#29992;&#38647;&#36798;&#20256;&#24863;&#22120;&#27979;&#37327;&#23545;&#26410;&#30693;&#25968;&#37327;&#30446;&#26631;&#30340;&#36319;&#36394;&#25361;&#25112;&#12290;&#36890;&#36807;&#21033;&#29992;&#36317;&#31163;-&#22810;&#26222;&#21202;&#35889;&#20449;&#24687;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#27979;&#37327;&#31867;&#21035;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#39069;&#22806;&#20449;&#24687;&#22686;&#24378;&#26434;&#27874;&#21076;&#38500;&#21644;&#25968;&#25454;&#20851;&#32852;&#65292;&#20174;&#32780;&#22686;&#24378;&#30446;&#26631;&#36319;&#36394;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#22686;&#24378;&#30340;&#28040;&#24687;&#20256;&#36882;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#32479;&#19968;&#30340;&#28040;&#24687;&#20256;&#36882;&#33719;&#24471;&#30340;&#20449;&#24565;&#34987;&#36755;&#20837;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#20316;&#20026;&#39069;&#22806;&#20449;&#24687;&#12290;&#28982;&#21518;&#21033;&#29992;&#36755;&#20986;&#30340;&#20449;&#24565;&#26469;&#20248;&#21270;&#21407;&#22987;&#20449;&#24565;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#36741;&#21161;&#30340;&#40065;&#26834;&#22810;&#30446;&#26631;&#36319;&#36394;&#31639;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#22686;&#24378;&#30340;&#28040;&#24687;&#20256;&#36882;&#25216;&#26415;&#12290;&#35813;&#31639;&#27861;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#28040;&#24687;&#20256;&#36882;&#27169;&#22359;&#65292;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;&#21644;Dempster-Shafer&#27169;&#22359;&#12290;&#28040;&#24687;&#20256;&#36882;&#27169;&#22359;&#29992;&#20110;&#36890;&#36807;&#22240;&#23376;&#22270;&#34920;&#31034;&#32479;&#35745;&#27169;&#22411;&#65292;&#24182;&#25512;&#26029;&#30446;&#26631;&#36816;&#21160;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the challenge of tracking an unknown number of targets in strong clutter environments using measurements from a radar sensor. Leveraging the range-Doppler spectra information, we identify the measurement classes, which serve as additional information to enhance clutter rejection and data association, thus bolstering the robustness of target tracking. We first introduce a novel neural enhanced message passing approach, where the beliefs obtained by the unified message passing are fed into the neural network as additional information. The output beliefs are then utilized to refine the original beliefs. Then, we propose a classification-aided robust multiple target tracking algorithm, employing the neural enhanced message passing technique. This algorithm is comprised of three modules: a message-passing module, a neural network module, and a Dempster-Shafer module. The message-passing module is used to represent the statistical model by the factor graph and infers target kinema
&lt;/p&gt;</description></item><item><title>Loop Copilot&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;AI&#38899;&#20048;&#21512;&#22863;&#31995;&#32479;&#65292;&#33021;&#22815;&#36890;&#36807;&#20132;&#20114;&#24335;&#22810;&#36718;&#23545;&#35805;&#30028;&#38754;&#29983;&#25104;&#21644;&#36845;&#20195;&#25913;&#36827;&#38899;&#20048;&#65292;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;AI&#27169;&#22411;&#25191;&#34892;&#20219;&#21153;&#65292;&#24182;&#22312;&#19968;&#20010;&#38598;&#20013;&#30340;&#34920;&#20013;&#20445;&#25345;&#20851;&#38190;&#23646;&#24615;&#20197;&#30830;&#20445;&#38899;&#20048;&#30340;&#36830;&#36143;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12404</link><description>&lt;p&gt;
Loop Copilot: &#29992;&#20110;&#38899;&#20048;&#29983;&#25104;&#21644;&#36845;&#20195;&#32534;&#36753;&#30340;AI&#21512;&#22863;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative Editing. (arXiv:2310.12404v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12404
&lt;/p&gt;
&lt;p&gt;
Loop Copilot&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;AI&#38899;&#20048;&#21512;&#22863;&#31995;&#32479;&#65292;&#33021;&#22815;&#36890;&#36807;&#20132;&#20114;&#24335;&#22810;&#36718;&#23545;&#35805;&#30028;&#38754;&#29983;&#25104;&#21644;&#36845;&#20195;&#25913;&#36827;&#38899;&#20048;&#65292;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;AI&#27169;&#22411;&#25191;&#34892;&#20219;&#21153;&#65292;&#24182;&#22312;&#19968;&#20010;&#38598;&#20013;&#30340;&#34920;&#20013;&#20445;&#25345;&#20851;&#38190;&#23646;&#24615;&#20197;&#30830;&#20445;&#38899;&#20048;&#30340;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#38899;&#20048;&#26159;&#19968;&#20010;&#36845;&#20195;&#36807;&#31243;&#65292;&#27599;&#20010;&#38454;&#27573;&#37117;&#38656;&#35201;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;AI&#38899;&#20048;&#31995;&#32479;&#22312;&#32452;&#32455;&#22810;&#20010;&#23376;&#31995;&#32479;&#20197;&#28385;&#36275;&#19981;&#21516;&#38656;&#27714;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Loop Copilot&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#36890;&#36807;&#20132;&#20114;&#24335;&#12289;&#22810;&#36718;&#23545;&#35805;&#30028;&#38754;&#29983;&#25104;&#21644;&#36845;&#20195;&#25913;&#36827;&#38899;&#20048;&#30340;&#26032;&#22411;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#37322;&#29992;&#25143;&#24847;&#22270;&#65292;&#24182;&#36873;&#25321;&#36866;&#24403;&#30340;AI&#27169;&#22411;&#36827;&#34892;&#20219;&#21153;&#25191;&#34892;&#12290;&#27599;&#20010;&#21518;&#31471;&#27169;&#22411;&#37117;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#36755;&#20986;&#32858;&#21512;&#36215;&#26469;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#35201;&#27714;&#12290;&#20026;&#20102;&#30830;&#20445;&#38899;&#20048;&#30340;&#36830;&#36143;&#24615;&#65292;&#20851;&#38190;&#23646;&#24615;&#34987;&#20445;&#30041;&#22312;&#19968;&#20010;&#38598;&#20013;&#30340;&#34920;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#21322;&#32467;&#26500;&#21270;&#30340;&#35775;&#35848;&#21644;&#38382;&#21367;&#35843;&#26597;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#20986;&#20102;&#23427;&#22312;&#20419;&#36827;&#38899;&#20048;&#21019;&#20316;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#65292;&#20197;&#21450;&#23427;&#22312;&#26356;&#24191;&#27867;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating music is iterative, requiring varied methods at each stage. However, existing AI music systems fall short in orchestrating multiple subsystems for diverse needs. To address this gap, we introduce Loop Copilot, a novel system that enables users to generate and iteratively refine music through an interactive, multi-round dialogue interface. The system uses a large language model to interpret user intentions and select appropriate AI models for task execution. Each backend model is specialized for a specific task, and their outputs are aggregated to meet the user's requirements. To ensure musical coherence, essential attributes are maintained in a centralized table. We evaluate the effectiveness of the proposed system through semi-structured interviews and questionnaires, highlighting its utility not only in facilitating music creation but also its potential for broader applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#23567;&#25209;&#22788;&#29702;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#37051;&#22495;&#29190;&#28856;&#29616;&#35937;&#65288;NEP&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#37319;&#26679;&#23376;&#22270;&#30340;&#22823;&#23567;&#19982;&#25209;&#22788;&#29702;&#22823;&#23567;&#30340;&#20851;&#31995;&#26469;&#20943;&#23569;&#27599;&#20010;&#31181;&#23376;&#39030;&#28857;&#30340;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.12403</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21327;&#20316;&#23567;&#25209;&#27425;
&lt;/p&gt;
&lt;p&gt;
Cooperative Minibatching in Graph Neural Networks. (arXiv:2310.12403v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#20316;&#23567;&#25209;&#22788;&#29702;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#37051;&#22495;&#29190;&#28856;&#29616;&#35937;&#65288;NEP&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#37319;&#26679;&#23376;&#22270;&#30340;&#22823;&#23567;&#19982;&#25209;&#22788;&#29702;&#22823;&#23567;&#30340;&#20851;&#31995;&#26469;&#20943;&#23569;&#27599;&#20010;&#31181;&#23376;&#39030;&#28857;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26102;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#20010;&#36807;&#31243;&#38750;&#24120;&#23494;&#38598;&#12290;&#20943;&#23569;&#36164;&#28304;&#38656;&#27714;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#26159;&#23558;&#23567;&#25209;&#37327;&#35757;&#32451;&#19982;&#22270;&#37319;&#26679;&#30456;&#32467;&#21512;&#12290;GNN&#20855;&#26377;&#19968;&#20010;&#29420;&#29305;&#30340;&#29305;&#24615;&#65292;&#21363;&#23567;&#25209;&#37327;&#20013;&#30340;&#39033;&#20855;&#26377;&#37325;&#21472;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#29420;&#31435;&#23567;&#25209;&#37327;&#26041;&#27861;&#23558;&#27599;&#20010;&#22788;&#29702;&#21333;&#20803;&#65288;PE&#65289;&#20998;&#37197;&#32473;&#33258;&#24049;&#30340;&#23567;&#25209;&#37327;&#36827;&#34892;&#22788;&#29702;&#65292;&#23548;&#33268;&#37325;&#22797;&#35745;&#31639;&#21644;&#36328;PE&#30340;&#36755;&#20837;&#25968;&#25454;&#35775;&#38382;&#12290;&#36825;&#25918;&#22823;&#20102;&#37051;&#22495;&#29190;&#28856;&#29616;&#35937;&#65288;NEP&#65289;&#65292;&#36825;&#26159;&#38480;&#21046;&#25193;&#23637;&#24615;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#20026;&#20102;&#20943;&#23569;&#22810;PE&#29615;&#22659;&#20013;NEP&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21327;&#20316;&#23567;&#25209;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#37319;&#26679;&#23376;&#22270;&#30340;&#22823;&#23567;&#26159;&#25209;&#22788;&#29702;&#22823;&#23567;&#30340;&#20985;&#20989;&#25968;&#36825;&#19968;&#29305;&#24615;&#65292;&#21487;&#20197;&#26126;&#26174;&#20943;&#23569;&#27599;&#20010;&#31181;&#23376;&#39030;&#28857;&#30340;&#24037;&#20316;&#37327;&#65292;&#21516;&#26102;&#22686;&#21152;&#25209;&#22788;&#29702;&#22823;&#23567;&#12290;&#22240;&#27492;&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#21033;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Significant computational resources are required to train Graph Neural Networks (GNNs) at a large scale, and the process is highly data-intensive. One of the most effective ways to reduce resource requirements is minibatch training coupled with graph sampling. GNNs have the unique property that items in a minibatch have overlapping data. However, the commonly implemented Independent Minibatching approach assigns each Processing Element (PE) its own minibatch to process, leading to duplicated computations and input data access across PEs. This amplifies the Neighborhood Explosion Phenomenon (NEP), which is the main bottleneck limiting scaling. To reduce the effects of NEP in the multi-PE setting, we propose a new approach called Cooperative Minibatching. Our approach capitalizes on the fact that the size of the sampled subgraph is a concave function of the batch size, leading to significant reductions in the amount of work per seed vertex as batch sizes increase. Hence, it is favorable 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#24335;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26174;&#24335;&#24179;&#28369;&#30340;&#38381;&#24335;&#24471;&#20998;&#20989;&#25968;&#26469;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#26080;&#38656;&#35757;&#32451;&#65292;&#19988;&#22312;&#28040;&#36153;&#32423;CPU&#19978;&#33021;&#22815;&#23454;&#29616;&#19982;&#31070;&#32463;SGMs&#30456;&#31454;&#20105;&#30340;&#37319;&#26679;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.12395</link><description>&lt;p&gt;
&#38381;&#24335;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Closed-Form Diffusion Models. (arXiv:2310.12395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#24335;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26174;&#24335;&#24179;&#28369;&#30340;&#38381;&#24335;&#24471;&#20998;&#20989;&#25968;&#26469;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#26080;&#38656;&#35757;&#32451;&#65292;&#19988;&#22312;&#28040;&#36153;&#32423;CPU&#19978;&#33021;&#22815;&#23454;&#29616;&#19982;&#31070;&#32463;SGMs&#30456;&#31454;&#20105;&#30340;&#37319;&#26679;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;(SGMs)&#36890;&#36807;&#36845;&#20195;&#22320;&#20351;&#29992;&#25200;&#21160;&#30446;&#26631;&#20989;&#25968;&#30340;&#24471;&#20998;&#20989;&#25968;&#26469;&#20174;&#30446;&#26631;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#23545;&#20110;&#20219;&#20309;&#26377;&#38480;&#30340;&#35757;&#32451;&#38598;&#65292;&#21487;&#20197;&#38381;&#24335;&#22320;&#35780;&#20272;&#36825;&#20010;&#24471;&#20998;&#20989;&#25968;&#65292;&#20294;&#30001;&#27492;&#24471;&#21040;&#30340;SGMs&#20250;&#35760;&#24518;&#20854;&#35757;&#32451;&#25968;&#25454;&#65292;&#19981;&#33021;&#29983;&#25104;&#26032;&#26679;&#26412;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#24471;&#20998;&#20989;&#25968;&#65292;&#20294;&#36825;&#31181;&#36817;&#20284;&#30340;&#35823;&#24046;&#26377;&#21161;&#20110;&#25512;&#24191;&#65292;&#28982;&#32780;&#31070;&#32463;SGMs&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#20195;&#20215;&#39640;&#65292;&#32780;&#19988;&#23545;&#20110;&#36825;&#31181;&#35823;&#24046;&#25552;&#20379;&#30340;&#26377;&#25928;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#23578;&#19981;&#28165;&#26970;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#26174;&#24335;&#24179;&#28369;&#30340;&#38381;&#24335;&#24471;&#20998;&#26469;&#33719;&#24471;&#19968;&#20010;&#29983;&#25104;&#26032;&#26679;&#26412;&#30340;SGMs&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#39640;&#25928;&#24471;&#20998;&#20989;&#25968;&#20272;&#35745;&#22120;&#12290;&#21033;&#29992;&#36825;&#20010;&#20272;&#35745;&#22120;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#28040;&#36153;&#32423;CPU&#19978;&#36816;&#34892;&#26102;&#33021;&#22815;&#36798;&#21040;&#19982;&#31070;&#32463;SGMs&#30456;&#31454;&#20105;&#30340;&#37319;&#26679;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based generative models (SGMs) sample from a target distribution by iteratively transforming noise using the score function of the perturbed target. For any finite training set, this score function can be evaluated in closed form, but the resulting SGM memorizes its training data and does not generate novel samples. In practice, one approximates the score by training a neural network via score-matching. The error in this approximation promotes generalization, but neural SGMs are costly to train and sample, and the effective regularization this error provides is not well-understood theoretically. In this work, we instead explicitly smooth the closed-form score to obtain an SGM that generates novel samples without training. We analyze our model and propose an efficient nearest-neighbor-based estimator of its score function. Using this estimator, our method achieves sampling times competitive with neural SGMs while running on consumer-grade CPUs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#25913;&#36827;&#20256;&#24863;&#22120;&#24067;&#25918;&#31574;&#30053;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#26041;&#27861;&#30340;&#23545;&#27604;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#20135;&#29983;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12387</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#23398;&#20064;&#35299;&#20915;&#27668;&#20505;&#20256;&#24863;&#22120;&#24067;&#25918;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning to Solve Climate Sensor Placement Problems with a Transformer. (arXiv:2310.12387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#25913;&#36827;&#20256;&#24863;&#22120;&#24067;&#25918;&#31574;&#30053;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#26041;&#27861;&#30340;&#23545;&#27604;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#20135;&#29983;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;NP&#38590;&#24615;&#36136;&#65292;&#29615;&#22659;&#30417;&#27979;&#21644;&#28798;&#23475;&#31649;&#29702;&#20013;&#30340;&#20256;&#24863;&#22120;&#24067;&#25918;&#20248;&#21270;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20256;&#24863;&#22120;&#24067;&#25918;&#26041;&#27861;&#21253;&#25324;&#31934;&#30830;&#12289;&#36817;&#20284;&#25110;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20854;&#20013;&#21551;&#21457;&#24335;&#26041;&#27861;&#26159;&#26368;&#24120;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#21551;&#21457;&#24335;&#26041;&#27861;&#21463;&#21040;&#19987;&#23478;&#30452;&#35273;&#21644;&#32463;&#39564;&#30340;&#38480;&#21046;&#12290;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#24050;&#32463;&#25104;&#20026;&#33258;&#21160;&#29983;&#25104;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20256;&#24863;&#22120;&#24067;&#25918;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#23398;&#20064;&#25913;&#36827;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#20844;&#24335;&#26469;&#23398;&#20064;&#25913;&#36827;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36890;&#36807;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#35757;&#32451;&#31574;&#30053;&#32593;&#32476;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#20840;&#38754;&#30340;&#23454;&#39564;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20135;&#29983;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#27668;&#20505;&#20256;&#24863;&#22120;&#24067;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimal placement of sensors for environmental monitoring and disaster management is a challenging problem due to its NP-hard nature. Traditional methods for sensor placement involve exact, approximation, or heuristic approaches, with the latter being the most widely used. However, heuristic methods are limited by expert intuition and experience. Deep learning (DL) has emerged as a promising approach for generating heuristic algorithms automatically. In this paper, we introduce a novel sensor placement approach focused on learning improvement heuristics using deep reinforcement learning (RL) methods. Our approach leverages an RL formulation for learning improvement heuristics, driven by an actor-critic algorithm for training the policy network. We compare our method with several state-of-the-art approaches by conducting comprehensive experiments, demonstrating the effectiveness and superiority of our proposed approach in producing high-quality solutions. Our work presents a promisi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20840;&#23616;&#39044;&#31639;&#24179;&#34913;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#38024;&#23545;&#20855;&#26377;&#23545;&#25239;&#36755;&#20837;&#30340;&#21452;&#36793;&#36152;&#26131;&#30340;&#26080;&#24724;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#21453;&#39304;&#27169;&#22411;&#19979;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.12370</link><description>&lt;p&gt;
&#21452;&#36793;&#36152;&#26131;&#20013;&#22522;&#20110;&#20840;&#23616;&#39044;&#31639;&#24179;&#34913;&#30340;&#26080;&#24724;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
No-Regret Learning in Bilateral Trade via Global Budget Balance. (arXiv:2310.12370v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20840;&#23616;&#39044;&#31639;&#24179;&#34913;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#38024;&#23545;&#20855;&#26377;&#23545;&#25239;&#36755;&#20837;&#30340;&#21452;&#36793;&#36152;&#26131;&#30340;&#26080;&#24724;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#21453;&#39304;&#27169;&#22411;&#19979;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#36793;&#36152;&#26131;&#28041;&#21450;&#22312;&#20004;&#20010;&#25112;&#30053;&#20195;&#29702;&#20154;&#20043;&#38388;&#20419;&#36827;&#20132;&#26131;&#30340;&#25361;&#25112;&#65292;&#19968;&#20010;&#26159;&#21334;&#23478;&#65292;&#19968;&#20010;&#26159;&#20080;&#23478;&#65292;&#20004;&#32773;&#37117;&#23545;&#29289;&#21697;&#26377;&#31169;&#20154;&#20272;&#20540;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35813;&#38382;&#39064;&#30340;&#22312;&#32447;&#29256;&#26412;&#65292;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#38271;&#65292;&#37117;&#20250;&#20986;&#29616;&#19968;&#20010;&#26032;&#30340;&#21334;&#23478;&#21644;&#20080;&#23478;&#12290;&#23398;&#20064;&#32773;&#30340;&#20219;&#21153;&#26159;&#22312;&#19981;&#20102;&#35299;&#20182;&#20204;&#20272;&#20540;&#30340;&#24773;&#20917;&#19979;&#20026;&#27599;&#20010;&#20195;&#29702;&#20154;&#35774;&#32622;&#20215;&#26684;&#12290;&#21334;&#23478;&#21644;&#20080;&#23478;&#30340;&#24207;&#21015;&#30001;&#19968;&#20010;&#36951;&#24536;&#24615;&#23545;&#25163;&#36873;&#25321;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#19979;&#65292;&#24050;&#30693;&#30340;&#36127;&#38754;&#32467;&#26524;&#25490;&#38500;&#20102;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#23398;&#20064;&#32773;&#24517;&#39035;&#20445;&#35777;&#39044;&#31639;&#24179;&#34913;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20840;&#23616;&#39044;&#31639;&#24179;&#34913;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#20165;&#35201;&#27714;&#20195;&#29702;&#20154;&#22312;&#25972;&#20010;&#26102;&#38388;&#33539;&#22260;&#20869;&#20445;&#25345;&#39044;&#31639;&#24179;&#34913;&#12290;&#36890;&#36807;&#35201;&#27714;&#20840;&#23616;&#39044;&#31639;&#24179;&#34913;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#39318;&#20010;&#38024;&#23545;&#20855;&#26377;&#23545;&#25239;&#36755;&#20837;&#30340;&#21452;&#36793;&#36152;&#26131;&#30340;&#26080;&#24724;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#21453;&#39304;&#27169;&#22411;&#19979;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#20840;&#21453;&#39304;&#27169;&#22411;&#20013;&#65292;&#23398;&#20064;&#32773;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilateral trade revolves around the challenge of facilitating transactions between two strategic agents -- a seller and a buyer -- both of whom have a private valuations for the item. We study the online version of the problem, in which at each time step a new seller and buyer arrive. The learner's task is to set a price for each agent, without any knowledge about their valuations. The sequence of sellers and buyers is chosen by an oblivious adversary. In this setting, known negative results rule out the possibility of designing algorithms with sublinear regret when the learner has to guarantee budget balance for each iteration. In this paper, we introduce the notion of global budget balance, which requires the agent to be budget balance only over the entire time horizon. By requiring global budget balance, we provide the first no-regret algorithms for bilateral trade with adversarial inputs under various feedback models. First, we show that in the full-feedback model the learner can g
&lt;/p&gt;</description></item><item><title>MARVEL&#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#21033;&#29992;&#24120;&#35265;&#25968;&#25454;&#22312;&#39640;&#36895;&#20844;&#36335;&#36208;&#24266;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#21487;&#21464;&#36895;&#38480;&#25511;&#21046;&#12290;&#23427;&#36890;&#36807;&#22870;&#21169;&#32467;&#26500;&#21644;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#35843;&#65292;&#25552;&#39640;&#20132;&#36890;&#23433;&#20840;&#24615;&#21644;&#27969;&#21160;&#24615;&#12290;&#19982;&#26080;&#25511;&#21046;&#24773;&#20917;&#30456;&#27604;&#65292;MARVEL&#21487;&#20197;&#25552;&#39640;&#20132;&#36890;&#23433;&#20840;&#24615;63.4%&#65292;&#25552;&#39640;&#20132;&#36890;&#27969;&#21160;&#24615;14.6%&#12290;</title><link>http://arxiv.org/abs/2310.12359</link><description>&lt;p&gt;
MARVEL: &#29992;&#20110;&#22823;&#35268;&#27169;&#21487;&#21464;&#36895;&#38480;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MARVEL: Multi-Agent Reinforcement-Learning for Large-Scale Variable Speed Limits. (arXiv:2310.12359v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12359
&lt;/p&gt;
&lt;p&gt;
MARVEL&#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#21033;&#29992;&#24120;&#35265;&#25968;&#25454;&#22312;&#39640;&#36895;&#20844;&#36335;&#36208;&#24266;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#21487;&#21464;&#36895;&#38480;&#25511;&#21046;&#12290;&#23427;&#36890;&#36807;&#22870;&#21169;&#32467;&#26500;&#21644;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#35843;&#65292;&#25552;&#39640;&#20132;&#36890;&#23433;&#20840;&#24615;&#21644;&#27969;&#21160;&#24615;&#12290;&#19982;&#26080;&#25511;&#21046;&#24773;&#20917;&#30456;&#27604;&#65292;MARVEL&#21487;&#20197;&#25552;&#39640;&#20132;&#36890;&#23433;&#20840;&#24615;63.4%&#65292;&#25552;&#39640;&#20132;&#36890;&#27969;&#21160;&#24615;14.6%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#21464;&#36895;&#38480;&#65288;VSL&#65289;&#25511;&#21046;&#26159;&#19968;&#31181;&#25552;&#39640;&#23433;&#20840;&#24615;&#21644;&#27969;&#21160;&#24615;&#30340;&#26377;&#21069;&#36884;&#30340;&#20132;&#36890;&#31649;&#29702;&#31574;&#30053;&#12290;&#26412;&#24037;&#20316;&#20171;&#32461;&#20102;MARVEL&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#20165;&#26377;&#24120;&#35265;&#21487;&#29992;&#25968;&#25454;&#23454;&#29616;&#39640;&#36895;&#20844;&#36335;&#36208;&#24266;&#22823;&#35268;&#27169;VSL&#25511;&#21046;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#12290;&#26234;&#33021;&#20307;&#36890;&#36807;&#21253;&#25324;&#23545;&#20132;&#36890;&#29366;&#20917;&#30340;&#36866;&#24212;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#27969;&#21160;&#24615;&#22312;&#20869;&#30340;&#22870;&#21169;&#32467;&#26500;&#36827;&#34892;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#35843;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#25152;&#26377;VSL&#26234;&#33021;&#20307;&#20043;&#38388;&#20849;&#20139;&#21442;&#25968;&#65292;&#21487;&#20197;&#25193;&#23637;&#21040;&#21253;&#25324;&#35768;&#22810;&#31435;&#26609;&#30340;&#36208;&#24266;&#12290;&#26234;&#33021;&#20307;&#22312;&#19968;&#20010;&#22522;&#20110;&#19968;&#20010;&#30701;&#30340;&#39640;&#36895;&#20844;&#36335;&#36335;&#27573;&#30340;&#24494;&#20223;&#30495;&#29615;&#22659;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#36335;&#27573;&#26377;8&#20010;&#31435;&#26609;&#65292;&#36328;&#36234;7&#33521;&#37324;&#65292;&#24182;&#22312;&#32435;&#20160;&#32500;&#23572;&#38468;&#36817;&#30340;I-24&#19978;&#26377;34&#20010;&#31435;&#26609;&#65292;&#36328;&#36234;17&#33521;&#37324;&#36827;&#34892;&#27979;&#35797;&#12290;&#19982;&#26080;&#25511;&#21046;&#24773;&#20917;&#30456;&#27604;&#65292;MARVEL&#23558;&#20132;&#36890;&#23433;&#20840;&#24615;&#25552;&#39640;&#20102;63.4%&#65292;&#24182;&#23558;&#20132;&#36890;&#27969;&#21160;&#24615;&#25552;&#39640;&#20102;14.6%&#65292;&#19982;&#24050;&#22312;I-24&#19978;&#37096;&#32626;&#30340;&#26368;&#26032;&#31639;&#27861;&#30456;&#27604;&#12290;&#36827;&#34892;&#20102;&#19968;&#39033;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variable speed limit (VSL) control is a promising traffic management strategy for enhancing safety and mobility. This work introduces MARVEL, a multi-agent reinforcement learning (MARL) framework for implementing large-scale VSL control on freeway corridors using only commonly available data. The agents learn through a reward structure that incorporates adaptability to traffic conditions, safety, and mobility; enabling coordination among the agents. The proposed framework scales to cover corridors with many gantries thanks to a parameter sharing among all VSL agents. The agents are trained in a microsimulation environment based on a short freeway stretch with 8 gantries spanning 7 miles and tested with 34 gantries spanning 17 miles of I-24 near Nashville, TN. MARVEL improves traffic safety by 63.4% compared to the no control scenario and enhances traffic mobility by 14.6% compared to a state-of-the-practice algorithm that has been deployed on I-24. An explainability analysis is underta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#32500;&#22270;&#31070;&#32463;&#32593;&#32476;&#27880;&#24847;&#21147;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#36807;&#21435;&#35266;&#27979;&#21040;&#30340;&#36895;&#24230;&#12289;&#36710;&#36947;&#23553;&#38381;&#20107;&#20214;&#12289;&#28201;&#24230;&#21644;&#21487;&#35270;&#24615;&#26469;&#39044;&#27979;&#20132;&#36890;&#24773;&#20917;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20132;&#36890;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#23398;&#20064;&#65292;&#23637;&#31034;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.12353</link><description>&lt;p&gt;
&#20351;&#29992;&#22806;&#37096;&#20449;&#24687;&#30340;&#32593;&#32476;&#27969;&#37327;&#29366;&#24577;&#39044;&#27979;&#65306;&#19968;&#31181;&#22522;&#20110;&#22810;&#32500;&#22270;&#31070;&#32463;&#32593;&#32476;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Networkwide Traffic State Forecasting Using Exogenous Information: A Multi-Dimensional Graph Attention-Based Approach. (arXiv:2310.12353v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#32500;&#22270;&#31070;&#32463;&#32593;&#32476;&#27880;&#24847;&#21147;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#36807;&#21435;&#35266;&#27979;&#21040;&#30340;&#36895;&#24230;&#12289;&#36710;&#36947;&#23553;&#38381;&#20107;&#20214;&#12289;&#28201;&#24230;&#21644;&#21487;&#35270;&#24615;&#26469;&#39044;&#27979;&#20132;&#36890;&#24773;&#20917;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20132;&#36890;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#23398;&#20064;&#65292;&#23637;&#31034;&#20102;&#36739;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#29366;&#24577;&#39044;&#27979;&#23545;&#20132;&#36890;&#31649;&#29702;&#21644;&#25511;&#21046;&#31574;&#30053;&#65292;&#20197;&#21450;&#20132;&#36890;&#32593;&#32476;&#30340;&#29992;&#25143;&#21644;&#31995;&#32479;&#32423;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#37324;&#65292;&#20132;&#36890;&#39044;&#27979;&#24050;&#32463;&#29992;&#22810;&#31181;&#25216;&#26415;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#20165;&#20381;&#36182;&#20869;&#29983;&#20132;&#36890;&#21464;&#37327;&#36827;&#34892;&#29366;&#24577;&#39044;&#27979;&#65292;&#23613;&#31649;&#26377;&#35777;&#25454;&#34920;&#26126;&#22806;&#29983;&#22240;&#32032;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#20132;&#36890;&#29366;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#32500;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#27880;&#24847;&#21147;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65288;M-STGAT&#65289;&#65292;&#23427;&#26681;&#25454;&#36807;&#21435;&#35266;&#27979;&#21040;&#30340;&#36895;&#24230;&#12289;&#36710;&#36947;&#23553;&#38381;&#20107;&#20214;&#12289;&#28201;&#24230;&#21644;&#21487;&#35270;&#24615;&#26469;&#39044;&#27979;&#20132;&#36890;&#24773;&#20917;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#27880;&#24847;&#21147;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#36824;&#26681;&#25454;&#35266;&#23519;&#21040;&#36825;&#20123;&#21464;&#37327;&#30340;&#20132;&#36890;&#32593;&#32476;&#30340;&#32467;&#26500;&#36827;&#34892;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#21152;&#21033;&#31119;&#23612;&#20122;&#24030;&#20132;&#36890;&#37096;&#30340;&#20132;&#36890;&#36895;&#24230;&#21644;&#36710;&#36947;&#23553;&#38381;&#25968;&#25454;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic state forecasting is crucial for traffic management and control strategies, as well as user- and system-level decision making in the transportation network. While traffic forecasting has been approached with a variety of techniques over the last couple of decades, most approaches simply rely on endogenous traffic variables for state prediction, despite the evidence that exogenous factors can significantly impact traffic conditions. This paper proposes a multi-dimensional spatio-temporal graph attention-based traffic prediction approach (M-STGAT), which predicts traffic based on past observations of speed, along with lane closure events, temperature, and visibility across the transportation network. The approach is based on a graph attention network architecture, which also learns based on the structure of the transportation network on which these variables are observed. Numerical experiments are performed using traffic speed and lane closure data from the California Department 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;F2GNN&#30340;&#26041;&#27861;&#65292;&#23427;&#26088;&#22312;&#22686;&#24378;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20943;&#36731;&#20559;&#35265;&#30340;&#26032;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.12350</link><description>&lt;p&gt;
&#20026;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#32467;&#26500;&#24863;&#30693;&#32676;&#20307;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Equipping Federated Graph Neural Networks with Structure-aware Group Fairness. (arXiv:2310.12350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;F2GNN&#30340;&#26041;&#27861;&#65292;&#23427;&#26088;&#22312;&#22686;&#24378;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20943;&#36731;&#20559;&#35265;&#30340;&#26032;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#30340;&#21508;&#31181;&#22270;&#25968;&#25454;&#22788;&#29702;&#21644;&#20998;&#26512;&#20219;&#21153;&#12290;&#30001;&#20110;&#38544;&#31169;&#21644;&#30417;&#31649;&#38480;&#21046;&#65292;&#23545;&#38598;&#20013;&#24335;&#22270;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#36235;&#21183;&#24615;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;GNN&#21487;&#33021;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#32487;&#25215;&#21382;&#21490;&#20559;&#35265;&#24182;&#23548;&#33268;&#27495;&#35270;&#24615;&#39044;&#27979;&#65292;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#65292;&#23616;&#37096;&#27169;&#22411;&#30340;&#20559;&#35265;&#24456;&#23481;&#26131;&#20256;&#25773;&#21040;&#20840;&#23616;&#27169;&#22411;&#65292;&#36825;&#32473;&#22312;&#32852;&#37030;GNN&#20013;&#20943;&#36731;&#20559;&#35265;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;F2GNN&#65292;&#19968;&#31181;&#22686;&#24378;&#32852;&#37030;GNN&#32676;&#20307;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#12290;&#30001;&#20110;&#20559;&#35265;&#21487;&#33021;&#26469;&#33258;&#25968;&#25454;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;F2GNN&#26088;&#22312;&#22312;&#32852;&#37030;&#29615;&#22659;&#19979;&#20943;&#23569;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have been widely used for various types of graph data processing and analytical tasks in different domains. Training GNNs over centralized graph data can be infeasible due to privacy concerns and regulatory restrictions. Thus, federated learning (FL) becomes a trending solution to address this challenge in a distributed learning paradigm. However, as GNNs may inherit historical bias from training data and lead to discriminatory predictions, the bias of local models can be easily propagated to the global model in distributed settings. This poses a new challenge in mitigating bias in federated GNNs. To address this challenge, we propose $\text{F}^2$GNN, a Fair Federated Graph Neural Network, that enhances group fairness of federated GNNs. As bias can be sourced from both data and learning algorithms, $\text{F}^2$GNN aims to mitigate both types of bias under federated settings. First, we provide theoretical insights on the connection between data bias in a tra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22812;&#38388;&#28783;&#20809;&#25968;&#25454;&#21644;&#31038;&#20132;&#23186;&#20307;&#30417;&#27979;&#20572;&#30005;&#21450;&#20854;&#24863;&#30693;&#21407;&#22240;&#65292;&#22312;&#22996;&#20869;&#29790;&#25289;&#30340;&#26696;&#20363;&#20013;&#26174;&#31034;&#20986;&#22812;&#38388;&#28783;&#20809;&#24378;&#24230;&#19982;&#20572;&#30005;&#37327;&#20043;&#38388;&#30340;&#21453;&#21521;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#25552;&#21040;&#22996;&#20869;&#29790;&#25289;&#24635;&#32479;&#30340;&#25512;&#25991;&#26174;&#31034;&#20986;&#26356;&#39640;&#30340;&#36127;&#38754;&#24773;&#32490;&#21644;&#32511;&#33394;&#35789;&#27719;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.12346</link><description>&lt;p&gt;
&#20351;&#29992;&#22812;&#38388;&#28783;&#20809;&#21644;&#31038;&#20132;&#23186;&#20307;&#36861;&#36394;&#30005;&#21147;&#25439;&#22833;&#21450;&#20854;&#24863;&#30693;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Tracking electricity losses and their perceived causes using nighttime light and social media. (arXiv:2310.12346v1 [physics.soc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22812;&#38388;&#28783;&#20809;&#25968;&#25454;&#21644;&#31038;&#20132;&#23186;&#20307;&#30417;&#27979;&#20572;&#30005;&#21450;&#20854;&#24863;&#30693;&#21407;&#22240;&#65292;&#22312;&#22996;&#20869;&#29790;&#25289;&#30340;&#26696;&#20363;&#20013;&#26174;&#31034;&#20986;&#22812;&#38388;&#28783;&#20809;&#24378;&#24230;&#19982;&#20572;&#30005;&#37327;&#20043;&#38388;&#30340;&#21453;&#21521;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#25552;&#21040;&#22996;&#20869;&#29790;&#25289;&#24635;&#32479;&#30340;&#25512;&#25991;&#26174;&#31034;&#20986;&#26356;&#39640;&#30340;&#36127;&#38754;&#24773;&#32490;&#21644;&#32511;&#33394;&#35789;&#27719;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#29615;&#22659;&#26159;&#22797;&#26434;&#30340;&#31995;&#32479;&#65292;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#30340;&#25925;&#38556;&#21487;&#33021;&#23545;&#31038;&#21306;&#30340;&#32463;&#27982;&#21644;&#31038;&#20250;&#31119;&#31049;&#20135;&#29983;&#24433;&#21709;&#12290;&#30005;&#21147;&#31995;&#32479;&#20855;&#26377;&#29305;&#27530;&#24847;&#20041;&#65292;&#22240;&#20026;&#23427;&#20204;&#23545;&#20854;&#20182;&#22522;&#30784;&#35774;&#26045;&#33267;&#20851;&#37325;&#35201;&#65292;&#20013;&#26029;&#21487;&#33021;&#24341;&#21457;&#24191;&#27867;&#21518;&#26524;&#12290;&#36890;&#24120;&#65292;&#35780;&#20272;&#30005;&#21147;&#20379;&#24212;&#38656;&#35201;&#22320;&#38754;&#25968;&#25454;&#65292;&#32780;&#22312;&#20914;&#31361;&#21306;&#22495;&#21644;&#36164;&#28304;&#26377;&#38480;&#30340;&#22320;&#21306;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#21355;&#26143;&#22270;&#20687;&#12289;&#31038;&#20132;&#23186;&#20307;&#21644;&#20449;&#24687;&#25552;&#21462;&#26469;&#30417;&#27979;&#20572;&#30005;&#21450;&#20854;&#24863;&#30693;&#21407;&#22240;&#12290;&#21033;&#29992;2019&#24180;3&#26376;&#30340;&#22812;&#38388;&#28783;&#20809;&#25968;&#25454;&#65288;&#22996;&#20869;&#29790;&#25289;&#21345;&#25289;&#21345;&#26031;&#65289;&#26469;&#25351;&#31034;&#20572;&#30005;&#21306;&#22495;&#12290;&#21033;&#29992;Twitter&#25968;&#25454;&#26469;&#30830;&#23450;&#24773;&#24863;&#21644;&#20027;&#39064;&#36235;&#21183;&#65292;&#21516;&#26102;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#21644;&#20027;&#39064;&#24314;&#27169;&#20197;&#28145;&#20837;&#20102;&#35299;&#20844;&#20247;&#23545;&#20572;&#30005;&#21407;&#22240;&#30340;&#30475;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#22812;&#38388;&#28783;&#20809;&#24378;&#24230;&#19982;&#20572;&#30005;&#37327;&#20043;&#38388;&#23384;&#22312;&#21453;&#21521;&#20851;&#31995;&#12290;&#25552;&#21040;&#22996;&#20869;&#29790;&#25289;&#24635;&#32479;&#30340;&#25512;&#25991;&#26174;&#31034;&#20986;&#26356;&#39640;&#30340;&#36127;&#38754;&#24773;&#32490;&#21644;&#32511;&#33394;&#35789;&#27719;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Urban environments are intricate systems where the breakdown of critical infrastructure can impact both the economic and social well-being of communities. Electricity systems hold particular significance, as they are essential for other infrastructure, and disruptions can trigger widespread consequences. Typically, assessing electricity availability requires ground-level data, a challenge in conflict zones and regions with limited access. This study shows how satellite imagery, social media, and information extraction can monitor blackouts and their perceived causes. Night-time light data (in March 2019 for Caracas, Venezuela) is used to indicate blackout regions. Twitter data is used to determine sentiment and topic trends, while statistical analysis and topic modeling delved into public perceptions regarding blackout causes. The findings show an inverse relationship between nighttime light intensity. Tweets mentioning the Venezuelan President displayed heightened negativity and a gre
&lt;/p&gt;</description></item><item><title>ClusT3&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#22810;&#23610;&#24230;&#29305;&#24449;&#22270;&#21644;&#31163;&#25955;&#28508;&#22312;&#34920;&#31034;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#22522;&#20934;&#19978;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.12345</link><description>&lt;p&gt;
ClusT3:&#20449;&#24687;&#19981;&#21464;&#30340;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
ClusT3: Information Invariant Test-Time Training. (arXiv:2310.12345v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12345
&lt;/p&gt;
&lt;p&gt;
ClusT3&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#22810;&#23610;&#24230;&#29305;&#24449;&#22270;&#21644;&#31163;&#25955;&#28508;&#22312;&#34920;&#31034;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#22522;&#20934;&#19978;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#27979;&#35797;&#26102;&#38388;&#32463;&#24120;&#21463;&#21040;&#22495;&#20559;&#31227;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20123;&#28431;&#27934;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#65288;TTT&#65289;&#26041;&#27861;&#65292;&#20854;&#20013;&#22312;&#35757;&#32451;&#26102;&#21516;&#26102;&#35299;&#20915;&#20102;&#19968;&#20010;&#27425;&#35201;&#20219;&#21153;&#21644;&#20027;&#20219;&#21153;&#65292;&#24182;&#22312;&#27979;&#35797;&#26102;&#20316;&#20026;&#33258;&#30417;&#30563;&#30340;&#20195;&#29702;&#20219;&#21153;&#20351;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230;&#29305;&#24449;&#22270;&#21644;&#31163;&#25955;&#28508;&#22312;&#34920;&#31034;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#26032;&#39062;&#26080;&#30417;&#30563;TTT&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#20316;&#20026;&#36741;&#21161;&#32858;&#31867;&#20219;&#21153;&#38598;&#25104;&#21040;&#26631;&#20934;&#35757;&#32451;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#30340;&#24120;&#35265;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#22522;&#20934;&#19978;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning models have shown remarkable performance in a broad range of vision tasks. However, they are often vulnerable against domain shifts at test-time. Test-time training (TTT) methods have been developed in an attempt to mitigate these vulnerabilities, where a secondary task is solved at training time simultaneously with the main task, to be later used as an self-supervised proxy task at test-time. In this work, we propose a novel unsupervised TTT technique based on the maximization of Mutual Information between multi-scale feature maps and a discrete latent representation, which can be integrated to the standard training as an auxiliary clustering task. Experimental results demonstrate competitive classification performance on different popular test-time adaptation benchmarks.
&lt;/p&gt;</description></item><item><title>&#36866;&#24212;&#24615;&#23454;&#39564;&#20026;&#25345;&#32493;&#35838;&#31243;&#25913;&#36827;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#36890;&#36807;&#21160;&#24577;&#37096;&#32626;&#26368;&#26377;&#25928;&#30340;&#26465;&#20214;&#20197;&#28385;&#36275;&#23398;&#29983;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.12324</link><description>&lt;p&gt;
&#36890;&#36807;&#24179;&#34913;&#25945;&#24072;&#21644;&#30740;&#31350;&#32773;&#30340;&#28608;&#21169;&#65292;&#25506;&#32034;&#36866;&#24212;&#24615;&#23454;&#39564;&#20419;&#36827;&#25345;&#32493;&#25913;&#36827;&#30340;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Opportunities for Adaptive Experiments to Enable Continuous Improvement that Trades-off Instructor and Researcher Incentives. (arXiv:2310.12324v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12324
&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#24615;&#23454;&#39564;&#20026;&#25345;&#32493;&#35838;&#31243;&#25913;&#36827;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#36890;&#36807;&#21160;&#24577;&#37096;&#32626;&#26368;&#26377;&#25928;&#30340;&#26465;&#20214;&#20197;&#28385;&#36275;&#23398;&#29983;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23454;&#39564;&#27604;&#36739;&#19981;&#21516;&#25945;&#23398;&#31574;&#30053;&#30340;&#26426;&#20250;&#21487;&#20197;&#20026;&#25945;&#24072;&#30340;&#20915;&#31574;&#25552;&#20379;&#26377;&#29992;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#23454;&#39564;&#32570;&#20047;&#28165;&#26224;&#31616;&#26126;&#30340;&#20351;&#29992;&#25968;&#25454;&#24555;&#36895;&#22686;&#21152;&#23454;&#39564;&#23398;&#29983;&#33719;&#24471;&#26368;&#20339;&#26465;&#20214;&#26426;&#20250;&#30340;&#36884;&#24452;&#12290;&#21463;&#39046;&#20808;&#31185;&#25216;&#20844;&#21496;&#22312;&#20135;&#21697;&#24320;&#21457;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#23454;&#39564;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#36866;&#24212;&#24615;&#23454;&#39564;&#26469;&#25345;&#32493;&#25913;&#36827;&#35838;&#31243;&#12290;&#22312;&#36866;&#24212;&#24615;&#23454;&#39564;&#20013;&#65292;&#19981;&#21516;&#30340;&#26465;&#20214;&#23558;&#34987;&#24212;&#29992;&#20110;&#23398;&#29983;&#36523;&#19978;&#65292;&#25968;&#25454;&#23558;&#34987;&#20998;&#26512;&#24182;&#29992;&#20110;&#25913;&#21464;&#26410;&#26469;&#23398;&#29983;&#30340;&#23398;&#20064;&#20307;&#39564;&#12290;&#21487;&#20197;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#21738;&#20123;&#34892;&#21160;&#21487;&#20197;&#26356;&#26377;&#24076;&#26395;&#25913;&#21892;&#23398;&#29983;&#30340;&#20307;&#39564;&#25110;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#21160;&#24577;&#22320;&#23558;&#26368;&#26377;&#25928;&#30340;&#26465;&#20214;&#24212;&#29992;&#20110;&#26410;&#26469;&#30340;&#23398;&#29983;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#28385;&#36275;&#23398;&#29983;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#20363;&#35828;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized experimental comparisons of alternative pedagogical strategies could provide useful empirical evidence in instructors' decision-making. However, traditional experiments do not have a clear and simple pathway to using data rapidly to try to increase the chances that students in an experiment get the best conditions. Drawing inspiration from the use of machine learning and experimentation in product development at leading technology companies, we explore how adaptive experimentation might help in continuous course improvement. In adaptive experiments, as different arms/conditions are deployed to students, data is analyzed and used to change the experience for future students. This can be done using machine learning algorithms to identify which actions are more promising for improving student experience or outcomes. This algorithm can then dynamically deploy the most effective conditions to future students, resulting in better support for students' needs. We illustrate the appr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#23398;&#20064;&#35770;&#35777;&#35821;&#20041;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#35770;&#35777;&#27714;&#35299;&#22120;&#65292;&#24182;&#22312;&#24418;&#24335;&#35770;&#35777;&#21644;&#20154;&#26426;&#23545;&#35805;&#39046;&#22495;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.12309</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#23398;&#20064;&#35770;&#35777;&#35821;&#20041;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unifying Framework for Learning Argumentation Semantics. (arXiv:2310.12309v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#23398;&#20064;&#35770;&#35777;&#35821;&#20041;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#35770;&#35777;&#27714;&#35299;&#22120;&#65292;&#24182;&#22312;&#24418;&#24335;&#35770;&#35777;&#21644;&#20154;&#26426;&#23545;&#35805;&#39046;&#22495;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#35777;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19968;&#20010;&#38750;&#24120;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#28041;&#21450;&#21040;&#22312;&#20154;&#19982;&#20154;&#25110;&#20154;&#19982;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20043;&#38388;&#30340;&#23545;&#35805;&#20013;&#25152;&#20351;&#29992;&#30340;&#35770;&#35777;&#30340;&#34920;&#31034;&#21644;&#35780;&#20272;&#12290;&#27491;&#24335;&#35770;&#35777;&#31995;&#32479;&#30340;&#21487;&#25509;&#21463;&#24615;&#35821;&#20041;&#23450;&#20041;&#20102;&#35770;&#35777;&#30340;&#25509;&#21463;&#25110;&#25298;&#32477;&#30340;&#26631;&#20934;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#19968;&#20123;&#31216;&#20026;&#35770;&#35777;&#27714;&#35299;&#22120;&#30340;&#36719;&#20214;&#31995;&#32479;&#65292;&#29992;&#20110;&#20351;&#29992;&#36825;&#20123;&#26631;&#20934;&#35745;&#31639;&#34987;&#25509;&#21463;/&#34987;&#25298;&#32477;&#30340;&#35770;&#35777;&#12290;&#20854;&#20013;&#19968;&#20123;&#31995;&#32479;&#36890;&#36807;&#20351;&#29992;&#19981;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#35782;&#21035;&#25509;&#21463;&#30340;&#35770;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#26041;&#27861;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#26469;&#23398;&#20064;&#22810;&#20010;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#35770;&#35777;&#26694;&#26550;&#30340;&#21487;&#25509;&#21463;&#24615;&#35821;&#20041;&#12290;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#20248;&#20110;&#29616;&#26377;&#30340;&#35770;&#35777;&#27714;&#35299;&#22120;&#65292;&#20174;&#32780;&#24320;&#36767;&#20102;&#24418;&#24335;&#35770;&#35777;&#21644;&#20154;&#26426;&#23545;&#35805;&#39046;&#22495;&#30340;&#26032;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Argumentation is a very active research field of Artificial Intelligence concerned with the representation and evaluation of arguments used in dialogues between humans and/or artificial agents. Acceptability semantics of formal argumentation systems define the criteria for the acceptance or rejection of arguments. Several software systems, known as argumentation solvers, have been developed to compute the accepted/rejected arguments using such criteria. These include systems that learn to identify the accepted arguments using non-interpretable methods. In this paper we present a novel framework, which uses an Inductive Logic Programming approach to learn the acceptability semantics for several abstract and structured argumentation frameworks in an interpretable way. Through an empirical evaluation we show that our framework outperforms existing argumentation solvers, thus opening up new future research directions in the area of formal argumentation and human-machine dialogues.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#19982;&#21270;&#23398;&#23478;&#30340;&#20559;&#22909;&#23545;&#40784;&#29983;&#25104;&#30340;&#20998;&#23376;&#12290;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#12289;&#39640;&#25928;&#65292;&#19988;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.12304</link><description>&lt;p&gt;
&#20998;&#23376;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Preference Optimization for Molecular Language Models. (arXiv:2310.12304v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#19982;&#21270;&#23398;&#23478;&#30340;&#20559;&#22909;&#23545;&#40784;&#29983;&#25104;&#30340;&#20998;&#23376;&#12290;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#12289;&#39640;&#25928;&#65292;&#19988;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#35821;&#35328;&#24314;&#27169;&#26159;&#19968;&#31181;&#29983;&#25104;&#26032;&#39062;&#21270;&#23398;&#32467;&#26500;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#19981;&#20250;\emph{&#20808;&#39564;&#22320;}&#32534;&#30721;&#21270;&#23398;&#23478;&#21487;&#33021;&#26399;&#26395;&#30340;&#26576;&#20123;&#20559;&#22909;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#19982;&#21270;&#23398;&#23478;&#30340;&#20559;&#22909;&#23545;&#40784;&#29983;&#25104;&#30340;&#20998;&#23376;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#12289;&#39640;&#25928;&#65292;&#19988;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular language modeling is an effective approach to generating novel chemical structures. However, these models do not \emph{a priori} encode certain preferences a chemist may desire. We investigate the use of fine-tuning using Direct Preference Optimization to better align generated molecules with chemist preferences. Our findings suggest that this approach is simple, efficient, and highly effective.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25991;&#26723;&#32423;&#21035;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#32763;&#35793;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#20219;&#20309;&#29616;&#26377;&#30340;&#21477;&#32423;&#21035;&#32763;&#35793;&#27169;&#22411;&#19982;&#25991;&#26723;&#32423;&#21035;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20511;&#37492;&#27169;&#22411;&#32452;&#21512;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#26435;&#37325;&#25216;&#26415;&#30340;&#25552;&#20986;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25991;&#26723;&#32423;&#21035;&#25351;&#26631;&#24182;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2310.12303</link><description>&lt;p&gt;
&#25991;&#26723;&#32423;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Document-Level Language Models for Machine Translation. (arXiv:2310.12303v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12303
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25991;&#26723;&#32423;&#21035;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#32763;&#35793;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#20219;&#20309;&#29616;&#26377;&#30340;&#21477;&#32423;&#21035;&#32763;&#35793;&#27169;&#22411;&#19982;&#25991;&#26723;&#32423;&#21035;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20511;&#37492;&#27169;&#22411;&#32452;&#21512;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#26435;&#37325;&#25216;&#26415;&#30340;&#25552;&#20986;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25991;&#26723;&#32423;&#21035;&#25351;&#26631;&#24182;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#30693;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#20294;&#22823;&#22810;&#25968;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20173;&#28982;&#22312;&#21477;&#32423;&#21035;&#19978;&#36816;&#34892;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#65292;&#22823;&#22810;&#25968;&#24179;&#34892;&#35757;&#32451;&#25968;&#25454;&#21482;&#26377;&#21477;&#32423;&#21035;&#30340;&#23545;&#40784;&#65292;&#27809;&#26377;&#25991;&#26723;&#32423;&#21035;&#30340;&#20803;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#25991;&#26723;&#32423;&#21035;&#30340;&#21333;&#35821;&#25968;&#25454;&#26500;&#24314;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#32763;&#35793;&#31995;&#32479;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20219;&#20309;&#29616;&#26377;&#30340;&#21477;&#32423;&#21035;&#32763;&#35793;&#27169;&#22411;&#19982;&#25991;&#26723;&#32423;&#21035;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#32452;&#21512;&#30340;&#26368;&#26032;&#36827;&#23637;&#26469;&#25913;&#36827;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33021;&#22815;&#20351;&#31995;&#32479;&#32452;&#21512;&#26356;&#28789;&#27963;&#12289;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#26435;&#37325;&#25216;&#26415;&#12290;&#36890;&#36807;&#23545;&#22235;&#20010;&#19981;&#21516;&#30340;&#32763;&#35793;&#20219;&#21153;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25193;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#25991;&#26723;&#32423;&#21035;&#25351;&#26631;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#20063;&#26356;&#20248;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21453;&#21521;&#32763;&#35793;&#30340;&#32467;&#26524;&#26356;&#22909;&#65292;
&lt;/p&gt;
&lt;p&gt;
Despite the known limitations, most machine translation systems today still operate on the sentence-level. One reason for this is, that most parallel training data is only sentence-level aligned, without document-level meta information available. In this work, we set out to build context-aware translation systems utilizing document-level monolingual data instead. This can be achieved by combining any existing sentence-level translation model with a document-level language model. We improve existing approaches by leveraging recent advancements in model combination. Additionally, we propose novel weighting techniques that make the system combination more flexible and significantly reduce computational overhead. In a comprehensive evaluation on four diverse translation tasks, we show that our extensions improve document-targeted scores substantially and are also computationally more efficient. However, we also find that in most scenarios, back-translation gives even better results, at the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Jorge&#65292;&#19968;&#31181;GPU&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#39044;&#22788;&#29702;&#26041;&#27861;&#26367;&#20195;&#30697;&#38453;&#27714;&#36870;&#35745;&#31639;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#21516;&#26102;&#20860;&#20855;&#20108;&#38454;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;Jorge&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12298</link><description>&lt;p&gt;
Jorge: GPU&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#30340;&#36817;&#20284;&#39044;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Jorge: Approximate Preconditioning for GPU-efficient Second-order Optimization. (arXiv:2310.12298v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Jorge&#65292;&#19968;&#31181;GPU&#39640;&#25928;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#36817;&#20284;&#39044;&#22788;&#29702;&#26041;&#27861;&#26367;&#20195;&#30697;&#38453;&#27714;&#36870;&#35745;&#31639;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#21516;&#26102;&#20860;&#20855;&#20108;&#38454;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;Jorge&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#19982;&#19968;&#38454;&#20248;&#21270;&#22120;&#30456;&#27604;&#65292;&#20108;&#38454;&#20248;&#21270;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#35745;&#31639;&#25104;&#26412;&#36739;&#22823;&#65292;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#19968;&#30452;&#19981;&#22826;&#21463;&#27426;&#36814;&#12290;&#36825;&#31181;&#20248;&#21270;&#22120;&#20013;&#30340;&#20027;&#35201;&#25928;&#29575;&#29942;&#39048;&#26159;&#39044;&#22788;&#29702;&#27493;&#39588;&#20013;&#30340;&#30697;&#38453;&#27714;&#36870;&#35745;&#31639;&#65292;&#22312;GPU&#19978;&#35745;&#31639;&#26114;&#36149;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Jorge&#65292;&#19968;&#31181;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#23427;&#20860;&#20855;&#20108;&#38454;&#26041;&#27861;&#30340;&#24555;&#36895;&#25910;&#25947;&#29305;&#24615;&#21644;&#19968;&#38454;&#26041;&#27861;&#30340;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#23436;&#20840;&#28040;&#38500;&#30697;&#38453;&#27714;&#36870;&#35745;&#31639;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#35745;&#31639;&#29942;&#39048;&#65292;&#29992;&#36817;&#20284;&#30340;&#39044;&#22788;&#29702;&#22120;&#35745;&#31639;&#26367;&#20195;&#12290;&#36825;&#20351;&#24471;Jorge&#22312;&#22681;&#38047;&#26102;&#38388;&#19978;&#22312;GPU&#19978;&#38750;&#24120;&#39640;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#35843;&#25972;&#33391;&#22909;&#30340;SGD&#22522;&#20934;&#20013;&#30830;&#23450;Jorge&#36229;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#35843;&#21442;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;Jorge&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their better convergence properties compared to first-order optimizers, second-order optimizers for deep learning have been less popular due to their significant computational costs. The primary efficiency bottleneck in such optimizers is matrix inverse calculations in the preconditioning step, which are expensive to compute on GPUs. In this paper, we introduce Jorge, a second-order optimizer that promises the best of both worlds -- rapid convergence benefits of second-order methods, and high computational efficiency typical of first-order methods. We address the primary computational bottleneck of computing matrix inverses by completely eliminating them using an approximation of the preconditioner computation. This makes Jorge extremely efficient on GPUs in terms of wall-clock time. Further, we describe an approach to determine Jorge's hyperparameters directly from a well-tuned SGD baseline, thereby significantly minimizing tuning efforts. Our empirical evaluations demonstrate
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#25918;&#24335;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#35757;&#32451;&#38454;&#27573;&#35782;&#21035;&#26377;&#38480;&#24322;&#24120;&#31867;&#21035;&#30340;&#23569;&#37327;&#26631;&#35760;&#24322;&#24120;&#65292;&#24182;&#22312;&#27979;&#35797;&#38454;&#27573;&#26816;&#27979;&#21040;&#35265;&#36807;&#21644;&#26410;&#35265;&#36807;&#30340;&#24322;&#24120;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2310.12294</link><description>&lt;p&gt;
&#24320;&#25918;&#24335;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Open-Set Multivariate Time-Series Anomaly Detection. (arXiv:2310.12294v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#25918;&#24335;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#35757;&#32451;&#38454;&#27573;&#35782;&#21035;&#26377;&#38480;&#24322;&#24120;&#31867;&#21035;&#30340;&#23569;&#37327;&#26631;&#35760;&#24322;&#24120;&#65292;&#24182;&#22312;&#27979;&#35797;&#38454;&#27573;&#26816;&#27979;&#21040;&#35265;&#36807;&#21644;&#26410;&#35265;&#36807;&#30340;&#24322;&#24120;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#26159;&#26080;&#30417;&#30563;&#30340;&#65292;&#20165;&#20551;&#35774;&#26377;&#27491;&#24120;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#32780;&#19968;&#20123;&#30417;&#30563;&#26041;&#27861;&#36890;&#36807;&#22312;&#35757;&#32451;&#38454;&#27573;&#21152;&#20837;&#26631;&#35760;&#30340;&#24322;&#24120;&#26679;&#26412;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;&#24322;&#24120;&#31867;&#22411;&#23545;&#26080;&#30417;&#30563;&#26041;&#27861;&#26469;&#35828;&#22312;&#21306;&#20998;&#27491;&#24120;&#25968;&#25454;&#26102;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#32780;&#30417;&#30563;&#26041;&#27861;&#20165;&#33021;&#26816;&#27979;&#31867;&#20284;&#20110;&#35757;&#32451;&#26399;&#38388;&#23384;&#22312;&#30340;&#24322;&#24120;&#65292;&#26080;&#27861;&#25512;&#24191;&#21040;&#26410;&#35265;&#24322;&#24120;&#31867;&#21035;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24320;&#25918;&#24335;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#21487;&#20197;&#30475;&#21040;&#26469;&#33258;&#26377;&#38480;&#24322;&#24120;&#31867;&#21035;&#30340;&#23569;&#37327;&#26631;&#35760;&#24322;&#24120;&#65292;&#24182;&#26088;&#22312;&#22312;&#27979;&#35797;&#38454;&#27573;&#26816;&#27979;&#21040;&#35265;&#36807;&#21644;&#26410;&#35265;&#36807;&#30340;&#24322;&#24120;&#31867;&#21035;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#22810;&#21464;&#37327;&#24320;&#25918;&#24335;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65288;MOSAD&#65289;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous methods for time series anomaly detection (TSAD) methods have emerged in recent years. Most existing methods are unsupervised and assume the availability of normal training samples only, while few supervised methods have shown superior performance by incorporating labeled anomalous samples in the training phase. However, certain anomaly types are inherently challenging for unsupervised methods to differentiate from normal data, while supervised methods are constrained to detecting anomalies resembling those present during training, failing to generalize to unseen anomaly classes. This paper is the first attempt in providing a novel approach for the open-set TSAD problem, in which a small number of labeled anomalies from a limited class of anomalies are visible in the training phase, with the objective of detecting both seen and unseen anomaly classes in the test phase. The proposed method, called Multivariate Open-Set timeseries Anomaly Detection (MOSAD) consists of three prim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22270;&#34920;&#31034;&#23398;&#20064;&#26500;&#24314;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#25552;&#39640;MOOC&#20013;&#33258;&#21160;&#21270;&#25104;&#32489;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.12281</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#34920;&#31034;&#23398;&#20064;&#25552;&#39640; MOOC &#33258;&#21160;&#21270;&#25104;&#32489;&#39044;&#27979;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Enhancing the Performance of Automated Grade Prediction in MOOC using Graph Representation Learning. (arXiv:2310.12281v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22270;&#34920;&#31034;&#23398;&#20064;&#26500;&#24314;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#25552;&#39640;MOOC&#20013;&#33258;&#21160;&#21270;&#25104;&#32489;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#65288;MOOCs&#65289;&#20316;&#20026;&#19968;&#31181;&#24555;&#36895;&#22686;&#38271;&#30340;&#22312;&#32447;&#23398;&#20064;&#29616;&#35937;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#20851;&#27880;&#12290;&#19982;&#20256;&#32479;&#30340;&#25945;&#23460;&#19981;&#21516;&#65292;MOOCs&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#26426;&#20250;&#65292;&#20197;&#28385;&#36275;&#26469;&#33258;&#19981;&#21516;&#32972;&#26223;&#21644;&#22320;&#29702;&#20301;&#32622;&#30340;&#21508;&#31181;&#21463;&#20247;&#12290;&#33879;&#21517;&#22823;&#23398;&#21644;&#19987;&#38376;&#25552;&#20379;MOOCs&#30340;&#20379;&#24212;&#21830;&#65292;&#22914;Coursera&#65292;&#22312;&#21508;&#31181;&#20027;&#39064;&#19978;&#25552;&#20379;MOOC&#35838;&#31243;&#12290;&#30001;&#20110;&#39640;&#20837;&#23398;&#29575;&#21644;&#25945;&#24072;&#19982;&#23398;&#20064;&#32773;&#20043;&#38388;&#30340;&#26377;&#38480;&#30452;&#25509;&#20114;&#21160;&#65292;&#33258;&#21160;&#35780;&#20272;&#20219;&#21153;&#22914;&#25104;&#32489;&#21644;&#26089;&#26399;&#36864;&#23398;&#39044;&#27979;&#21464;&#24471;&#24517;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#24573;&#35270;&#20102;&#19979;&#28216;&#20219;&#21153;&#20013;&#28041;&#21450;&#19981;&#21516;&#23454;&#20307;&#20043;&#38388;&#30340;&#32467;&#26500;&#38142;&#25509;&#65292;&#20363;&#22914;&#23398;&#29983;&#21644;&#35838;&#31243;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#36890;&#36807;&#20132;&#20114;&#22270;&#34920;&#29616;&#30340;&#36825;&#20123;&#32467;&#26500;&#20851;&#31995;&#21253;&#21547;&#21487;&#20197;&#25552;&#39640;&#25152;&#38656;&#20219;&#21153;&#24615;&#33021;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20026;&#19968;&#20010;&#22823;&#35268;&#27169;MOOC&#26500;&#24314;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Massive Open Online Courses (MOOCs) have gained significant traction as a rapidly growing phenomenon in online learning. Unlike traditional classrooms, MOOCs offer a unique opportunity to cater to a diverse audience from different backgrounds and geographical locations. Renowned universities and MOOC-specific providers, such as Coursera, offer MOOC courses on various subjects. Automated assessment tasks like grade and early dropout predictions are necessary due to the high enrollment and limited direct interaction between teachers and learners. However, current automated assessment approaches overlook the structural links between different entities involved in the downstream tasks, such as the students and courses. Our hypothesis suggests that these structural relationships, manifested through an interaction graph, contain valuable information that can enhance the performance of the task at hand. To validate this, we construct a unique knowledge graph for a large MOOC 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27010;&#24565;&#25552;&#31034;&#23398;&#20064;&#65288;MCPL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#26032;&#30340;&#8220;&#35789;&#8221;&#26469;&#35299;&#20915;&#22312;&#21333;&#20010;&#22330;&#26223;&#20013;&#35782;&#21035;&#21644;&#25972;&#21512;&#22810;&#20010;&#23545;&#35937;&#32423;&#27010;&#24565;&#30340;&#25361;&#25112;&#12290;&#38024;&#23545;&#35789;&#27010;&#24565;&#30456;&#20851;&#24615;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27880;&#24847;&#21147;&#25513;&#30721;&#12289;&#25552;&#31034;&#23545;&#27604;&#25439;&#22833;&#21644;&#32465;&#23450;&#24418;&#23481;&#35789;&#31561;&#19977;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#36890;&#36807;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#21644;&#21512;&#25104;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2310.12274</link><description>&lt;p&gt;
&#19968;&#22270;&#25269;&#21315;&#35328;&#65306;&#20351;&#29992;&#22810;&#27010;&#24565;&#25552;&#31034;&#23398;&#20064;&#26469;&#23398;&#20064;&#23545;&#35937;&#32423;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
An Image is Worth Multiple Words: Learning Object Level Concepts using Multi-Concept Prompt Learning. (arXiv:2310.12274v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12274
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27010;&#24565;&#25552;&#31034;&#23398;&#20064;&#65288;MCPL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#26032;&#30340;&#8220;&#35789;&#8221;&#26469;&#35299;&#20915;&#22312;&#21333;&#20010;&#22330;&#26223;&#20013;&#35782;&#21035;&#21644;&#25972;&#21512;&#22810;&#20010;&#23545;&#35937;&#32423;&#27010;&#24565;&#30340;&#25361;&#25112;&#12290;&#38024;&#23545;&#35789;&#27010;&#24565;&#30456;&#20851;&#24615;&#20934;&#30830;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#27880;&#24847;&#21147;&#25513;&#30721;&#12289;&#25552;&#31034;&#23545;&#27604;&#25439;&#22833;&#21644;&#32465;&#23450;&#24418;&#23481;&#35789;&#31561;&#19977;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#36890;&#36807;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#33021;&#22815;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#21644;&#21512;&#25104;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#21453;&#36716;&#26159;&#19968;&#31181;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#19968;&#31181;&#26032;&#30340;&#8220;&#21333;&#35789;&#8221;&#30340;&#23884;&#20837;&#34920;&#31034;&#22270;&#20687;&#39118;&#26684;&#21644;&#22806;&#35266;&#65292;&#20351;&#20854;&#33021;&#22815;&#25972;&#21512;&#21040;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#20013;&#29983;&#25104;&#26032;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23545;&#20110;&#21487;&#33719;&#24471;&#20010;&#21035;&#27010;&#24565;&#30340;&#23884;&#20837;&#65292;&#35782;&#21035;&#21644;&#25972;&#21512;&#19968;&#20010;&#22330;&#26223;&#20013;&#30340;&#22810;&#20010;&#23545;&#35937;&#32423;&#27010;&#24565;&#20173;&#28982;&#38754;&#20020;&#30528;&#26174;&#33879;&#30340;&#25361;&#25112;&#65292;&#36825;&#20063;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;&#23454;&#35777;&#27979;&#35797;&#30340;&#36827;&#19968;&#27493;&#35777;&#23454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#27010;&#24565;&#25552;&#31034;&#23398;&#20064;&#65288;MCPL&#65289;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#19968;&#20010;&#21477;&#23376;-&#22270;&#20687;&#23545;&#20013;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#26032;&#30340;&#8220;&#35789;&#8221;&#12290;&#20026;&#20102;&#22686;&#24378;&#35789;&#27010;&#24565;&#30456;&#20851;&#24615;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#65306;&#27880;&#24847;&#21147;&#25513;&#30721;&#65288;AttnMask&#65289;&#23558;&#23398;&#20064;&#38598;&#20013;&#22312;&#30456;&#20851;&#21306;&#22495;&#65307;&#25552;&#31034;&#23545;&#27604;&#25439;&#22833;&#65288;PromptCL&#65289;&#23558;&#19981;&#21516;&#27010;&#24565;&#30340;&#23884;&#20837;&#20998;&#31163;&#24320;&#26469;&#65307;&#20197;&#21450;&#32465;&#23450;&#24418;&#23481;&#35789;&#65288;Bind adj.&#65289;&#23558;&#26032;&#30340;&#8220;&#35789;&#8221;&#19982;&#24050;&#30693;&#35789;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#36890;&#36807;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Textural Inversion, a prompt learning method, learns a singular embedding for a new "word" to represent image style and appearance, allowing it to be integrated into natural language sentences to generate novel synthesised images. However, identifying and integrating multiple object-level concepts within one scene poses significant challenges even when embeddings for individual concepts are attainable. This is further confirmed by our empirical tests. To address this challenge, we introduce a framework for Multi-Concept Prompt Learning (MCPL), where multiple new "words" are simultaneously learned from a single sentence-image pair. To enhance the accuracy of word-concept correlation, we propose three regularisation techniques: Attention Masking (AttnMask) to concentrate learning on relevant areas; Prompts Contrastive Loss (PromptCL) to separate the embeddings of different concepts; and Bind adjective (Bind adj.) to associate new "words" with known words. We evaluate via image generation
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25913;&#36827;&#20102;SCGAN&#27169;&#22411;&#20013;&#30340;&#30456;&#20284;&#24615;&#32422;&#26463;&#65292;&#20351;&#29992;SSIM&#24230;&#37327;&#22270;&#20687;&#30456;&#20284;&#24615;&#24182;&#24212;&#29992;&#23545;&#27604;&#25439;&#22833;&#21407;&#21017;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12262</link><description>&lt;p&gt;
&#25913;&#36827;SCGAN&#30340;&#30456;&#20284;&#24615;&#32422;&#26463;&#24182;&#23398;&#20064;&#26356;&#22909;&#30340;&#35299;&#32806;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Improving SCGAN's Similarity Constraint and Learning a Better Disentangled Representation. (arXiv:2310.12262v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25913;&#36827;&#20102;SCGAN&#27169;&#22411;&#20013;&#30340;&#30456;&#20284;&#24615;&#32422;&#26463;&#65292;&#20351;&#29992;SSIM&#24230;&#37327;&#22270;&#20687;&#30456;&#20284;&#24615;&#24182;&#24212;&#29992;&#23545;&#27604;&#25439;&#22833;&#21407;&#21017;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SCGAN&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#30456;&#20284;&#24615;&#32422;&#26463;&#65292;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#19982;&#26465;&#20214;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#12290;&#30456;&#20284;&#24615;&#32422;&#26463;&#20316;&#20026;&#23548;&#24072;&#65292;&#25351;&#23548;&#29983;&#25104;&#22120;&#32593;&#32476;&#29702;&#35299;&#22522;&#20110;&#26465;&#20214;&#30340;&#34920;&#31034;&#24046;&#24322;&#12290;&#25105;&#20204;&#28145;&#20837;&#29702;&#35299;&#20102;SCGAN&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;&#36825;&#31181;&#29702;&#35299;&#20351;&#25105;&#20204;&#24847;&#35782;&#21040;&#30456;&#20284;&#24615;&#32422;&#26463;&#30340;&#21151;&#33021;&#31867;&#20284;&#20110;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#20855;&#26377;&#39640;&#24230;&#29702;&#35299;&#21644;&#26234;&#33021;&#30340;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#22270;&#20687;&#30340;&#32467;&#26500;&#21644;&#39640;&#32423;&#29305;&#24449;&#26469;&#24230;&#37327;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#23601;&#20687;&#20154;&#31867;&#19968;&#26679;&#12290;&#25105;&#20204;&#23545;SCGAN&#36827;&#34892;&#20102;&#20004;&#20010;&#20027;&#35201;&#25913;&#21464;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#25913;&#36827;&#30340;&#27169;&#22411;&#65306;&#20351;&#29992;SSIM&#26469;&#24230;&#37327;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#23558;&#23545;&#27604;&#25439;&#22833;&#21407;&#21017;&#24212;&#29992;&#20110;&#30456;&#20284;&#24615;&#32422;&#26463;&#12290;&#25913;&#36827;&#30340;&#27169;&#22411;&#22312;FID&#21644;FactorVAE&#25351;&#26631;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#25913;&#36827;&#30340;&#27169;&#22411;&#36824;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
SCGAN adds a similarity constraint between generated images and conditions as a regularization term on generative adversarial networks. Similarity constraint works as a tutor to instruct the generator network to comprehend the difference of representations based on conditions. We understand how SCGAN works on a deeper level. This understanding makes us realize that the similarity constraint functions like the contrastive loss function. We believe that a model with high understanding and intelligence measures the similarity between images based on their structure and high level features, just like humans do. Two major changes we applied to SCGAN in order to make a modified model are using SSIM to measure similarity between images and applying contrastive loss principles to the similarity constraint. The modified model performs better using FID and FactorVAE metrics. The modified model also has better generalisability compared to other models. Keywords Generative Adversarial Nets, Unsupe
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;PAC&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;MDP&#20013;&#23398;&#20064;&#969;-regular&#30446;&#26631;&#65292;&#19981;&#38656;&#35201;&#31995;&#32479;&#25299;&#25169;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.12248</link><description>&lt;p&gt;
MDP&#20013;LTL&#21644;&#969;-regular&#30446;&#26631;&#30340;PAC&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A PAC Learning Algorithm for LTL and Omega-regular Objectives in MDPs. (arXiv:2310.12248v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12248
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;PAC&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;MDP&#20013;&#23398;&#20064;&#969;-regular&#30446;&#26631;&#65292;&#19981;&#38656;&#35201;&#31995;&#32479;&#25299;&#25169;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#26102;&#24207;&#36923;&#36753;&#65288;LTL&#65289;&#21644;&#969;-regular&#30446;&#26631;&#26159;&#36817;&#26399;&#29992;&#20110;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#36798;&#38750;&#39532;&#23572;&#21487;&#22827;&#30446;&#26631;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#65288;PAC&#65289;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;MDP&#20013;&#30340;&#969;-regular&#30446;&#26631;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20174;&#31995;&#32479;&#30340;&#37319;&#26679;&#36712;&#36857;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#31995;&#32479;&#25299;&#25169;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear temporal logic (LTL) and omega-regular objectives -- a superset of LTL -- have seen recent use as a way to express non-Markovian objectives in reinforcement learning. We introduce a model-based probably approximately correct (PAC) learning algorithm for omega-regular objectives in Markov decision processes. Unlike prior approaches, our algorithm learns from sampled trajectories of the system and does not require prior knowledge of the system's topology.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#24102;&#26377;&#35760;&#24518;&#30340;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65288;UDIL&#65289;&#65292;&#36890;&#36807;&#32479;&#19968;&#19981;&#21516;&#30340;&#29616;&#26377;&#26041;&#27861;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#31995;&#25968;&#65292;&#23454;&#29616;&#20102;&#26356;&#32039;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.12244</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#24102;&#26377;&#35760;&#24518;&#30340;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;: &#29702;&#35770;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Unified Approach to Domain Incremental Learning with Memory: Theory and Algorithm. (arXiv:2310.12244v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12244
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#24102;&#26377;&#35760;&#24518;&#30340;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65288;UDIL&#65289;&#65292;&#36890;&#36807;&#32479;&#19968;&#19981;&#21516;&#30340;&#29616;&#26377;&#26041;&#27861;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#31995;&#25968;&#65292;&#23454;&#29616;&#20102;&#26356;&#32039;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#26088;&#22312;&#36866;&#24212;&#19968;&#31995;&#21015;&#39046;&#22495;&#65292;&#20165;&#33021;&#35775;&#38382;&#20808;&#21069;&#39046;&#22495;&#30340;&#19968;&#23567;&#37096;&#20998;&#25968;&#25454;&#65288;&#21363;&#35760;&#24518;&#65289;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#20174;&#23454;&#36341;&#32773;&#35282;&#24230;&#20309;&#26102;&#36873;&#25321;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#32479;&#19968;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#65288;UDIL&#65289;&#65292;&#29992;&#20110;&#24102;&#26377;&#35760;&#24518;&#30340;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;UDIL&#23558;&#21508;&#31181;&#29616;&#26377;&#26041;&#27861;&#32479;&#19968;&#36215;&#26469;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#19982;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#65292;UDIL&#22987;&#32456;&#23454;&#29616;&#26356;&#32039;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;&#20851;&#38190;&#35266;&#28857;&#26159;&#19981;&#21516;&#30340;&#29616;&#26377;&#26041;&#27861;&#23545;&#24212;&#20110;&#25105;&#20204;&#30340;&#36793;&#30028;&#20855;&#26377;&#19981;&#21516;&#30340;&#22266;&#23450;&#31995;&#25968;&#65307;&#22522;&#20110;&#36825;&#31181;&#32479;&#19968;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#30340;UDIL&#20801;&#35768;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#31995;&#25968;&#65292;&#20174;&#32780;&#22987;&#32456;&#23454;&#29616;&#26368;&#32039;&#30340;&#30028;&#38480;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;UDIL&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain incremental learning aims to adapt to a sequence of domains with access to only a small subset of data (i.e., memory) from previous domains. Various methods have been proposed for this problem, but it is still unclear how they are related and when practitioners should choose one method over another. In response, we propose a unified framework, dubbed Unified Domain Incremental Learning (UDIL), for domain incremental learning with memory. Our UDIL **unifies** various existing methods, and our theoretical analysis shows that UDIL always achieves a tighter generalization error bound compared to these methods. The key insight is that different existing methods correspond to our bound with different **fixed** coefficients; based on insights from this unification, our UDIL allows **adaptive** coefficients during training, thereby always achieving the tightest bound. Empirical results show that our UDIL outperforms the state-of-the-art domain incremental learning methods on both synthe
&lt;/p&gt;</description></item><item><title>REVAMP&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;Python&#24211;&#65292;&#21487;&#29992;&#20110;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#36827;&#34892;&#23545;&#20219;&#24847;&#23545;&#35937;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#33258;&#21160;&#21270;&#27169;&#25311;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#24494;&#28210;&#26579;&#21644;&#27169;&#25311;&#30495;&#23454;&#29615;&#22659;&#22240;&#32032;&#65292;REVAMP&#20351;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#33021;&#22815;&#24555;&#36895;&#25506;&#32034;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2310.12243</link><description>&lt;p&gt;
REVAMP&#65306;&#33258;&#21160;&#21270;&#27169;&#25311;&#29616;&#23454;&#22330;&#26223;&#20013;&#23545;&#20219;&#24847;&#23545;&#35937;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
REVAMP: Automated Simulations of Adversarial Attacks on Arbitrary Objects in Realistic Scenes. (arXiv:2310.12243v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12243
&lt;/p&gt;
&lt;p&gt;
REVAMP&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;Python&#24211;&#65292;&#21487;&#29992;&#20110;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#36827;&#34892;&#23545;&#20219;&#24847;&#23545;&#35937;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#33258;&#21160;&#21270;&#27169;&#25311;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#24494;&#28210;&#26579;&#21644;&#27169;&#25311;&#30495;&#23454;&#29615;&#22659;&#22240;&#32032;&#65292;REVAMP&#20351;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#33021;&#22815;&#24555;&#36895;&#25506;&#32034;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#23545;&#20110;&#23545;&#25239;&#25915;&#20987;&#26159;&#33030;&#24369;&#30340;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#29615;&#22659;&#20013;&#25918;&#32622;&#23545;&#25239;&#24615;&#29289;&#20307;&#65292;&#20174;&#32780;&#23548;&#33268;&#35823;&#20998;&#31867;&#12290;&#34429;&#28982;&#22312;&#25968;&#23383;&#31354;&#38388;&#20013;&#29983;&#25104;&#36825;&#20123;&#23545;&#25239;&#24615;&#29289;&#20307;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#25104;&#21151;&#22320;&#23558;&#36825;&#20123;&#25915;&#20987;&#20174;&#25968;&#23383;&#39046;&#22495;&#36716;&#31227;&#21040;&#29616;&#23454;&#39046;&#22495;&#22312;&#25511;&#21046;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#22240;&#32032;&#26041;&#38754;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#37492;&#20110;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;REVAMP&#65292;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;Python&#24211;&#65292;&#23427;&#26159;&#39318;&#20010;&#20026;&#21019;&#24314;&#20855;&#26377;&#20219;&#24847;&#23545;&#35937;&#30340;&#25915;&#20987;&#22330;&#26223;&#24182;&#27169;&#25311;&#30495;&#23454;&#29615;&#22659;&#22240;&#32032;&#12289;&#20809;&#29031;&#12289;&#21453;&#23556;&#21644;&#25240;&#23556;&#30340;&#24037;&#20855;&#12290;REVAMP&#20351;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#33021;&#22815;&#36890;&#36807;&#25552;&#20379;&#24191;&#27867;&#21487;&#37197;&#32622;&#30340;&#36873;&#39033;&#26469;&#35774;&#35745;&#23454;&#39564;&#24182;&#20351;&#29992;&#21487;&#24494;&#28210;&#26579;&#26469;&#22797;&#29616;&#29289;&#29702;&#19978;&#21487;&#20449;&#30340;&#23545;&#25239;&#24615;&#29289;&#20307;&#65292;&#20174;&#32780;&#36805;&#36895;&#25506;&#32034;&#25968;&#23383;&#39046;&#22495;&#20013;&#30340;&#21508;&#31181;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning models, such as those used in an autonomous vehicle are vulnerable to adversarial attacks where an attacker could place an adversarial object in the environment, leading to mis-classification. Generating these adversarial objects in the digital space has been extensively studied, however successfully transferring these attacks from the digital realm to the physical realm has proven challenging when controlling for real-world environmental factors. In response to these limitations, we introduce REVAMP, an easy-to-use Python library that is the first-of-its-kind tool for creating attack scenarios with arbitrary objects and simulating realistic environmental factors, lighting, reflection, and refraction. REVAMP enables researchers and practitioners to swiftly explore various scenarios within the digital realm by offering a wide range of configurable options for designing experiments and using differentiable rendering to reproduce physically plausible adversarial objects. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#32972;&#26223;&#19979;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27169;&#20223;&#23398;&#20064;&#35270;&#20026;&#29289;&#20307;&#30340;&#22270;&#34920;&#31034;&#20043;&#38388;&#30340;&#26465;&#20214;&#23545;&#40784;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#25110;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#26032;&#30340;&#29289;&#20307;&#38598;&#19978;&#25191;&#34892;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.12238</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#22270;&#23545;&#40784;&#36827;&#34892;&#23569;&#26679;&#26412;&#32972;&#26223;&#19979;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-Shot In-Context Imitation Learning via Implicit Graph Alignment. (arXiv:2310.12238v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#32972;&#26223;&#19979;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27169;&#20223;&#23398;&#20064;&#35270;&#20026;&#29289;&#20307;&#30340;&#22270;&#34920;&#31034;&#20043;&#38388;&#30340;&#26465;&#20214;&#23545;&#40784;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#25110;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#26032;&#30340;&#29289;&#20307;&#38598;&#19978;&#25191;&#34892;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#20197;&#19979;&#38382;&#39064;&#65306;&#32473;&#23450;&#22312;&#20960;&#20010;&#19981;&#21516;&#29289;&#20307;&#19978;&#36827;&#34892;&#30340;&#20219;&#21153;&#30340;&#23569;&#37327;&#28436;&#31034;&#65292;&#26426;&#22120;&#20154;&#22914;&#20309;&#23398;&#20250;&#22312;&#26032;&#30340;&#12289;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#19978;&#25191;&#34892;&#30456;&#21516;&#30340;&#20219;&#21153;&#65311;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#31867;&#21035;&#20869;&#22810;&#26679;&#30340;&#29289;&#20307;&#20351;&#24471;&#25512;&#26029;&#26032;&#29289;&#20307;&#19982;&#28436;&#31034;&#20013;&#30340;&#29289;&#20307;&#20043;&#38388;&#30340;&#20219;&#21153;&#30456;&#20851;&#20851;&#31995;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#27169;&#20223;&#23398;&#20064;&#35270;&#20026;&#29289;&#20307;&#30340;&#22270;&#34920;&#31034;&#20043;&#38388;&#30340;&#26465;&#20214;&#23545;&#40784;&#38382;&#39064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26465;&#20214;&#20801;&#35768;&#32972;&#26223;&#19979;&#30340;&#23398;&#20064;&#65292;&#22312;&#28436;&#31034;&#20043;&#21518;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#31435;&#21363;&#22312;&#19968;&#32452;&#26032;&#29289;&#20307;&#19978;&#25191;&#34892;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#20851;&#20110;&#29289;&#20307;&#31867;&#21035;&#30340;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#25110;&#20219;&#20309;&#36827;&#19968;&#27493;&#30340;&#35757;&#32451;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#21644;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#26085;&#24120;&#20219;&#21153;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#38750;&#24120;&#26377;&#25928;&#65292;&#21516;&#26102;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;&#21487;&#20197;&#22312;&#25105;&#20204;&#30340;&#39033;&#30446;&#32593;&#39029;&#19978;&#35266;&#30475;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider the following problem: given a few demonstrations of a task across a few different objects, how can a robot learn to perform that same task on new, previously unseen objects? This is challenging because the large variety of objects within a class makes it difficult to infer the task-relevant relationship between the new objects and the objects in the demonstrations. We address this by formulating imitation learning as a conditional alignment problem between graph representations of objects. Consequently, we show that this conditioning allows for in-context learning, where a robot can perform a task on a set of new objects immediately after the demonstrations, without any prior knowledge about the object class or any further training. In our experiments, we explore and validate our design choices, and we show that our method is highly effective for few-shot learning of several real-world, everyday tasks, whilst outperforming baselines. Videos are available on our project webpag
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#21152;&#36895;&#19988;&#20934;&#30830;&#22320;&#20272;&#35745;&#20102;&#33033;&#20914;&#26143;&#26102;&#38388;&#38453;&#21015;&#20013;&#38543;&#26426;&#24341;&#21147;&#27874;&#32972;&#26223;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#30456;&#27604;&#20256;&#32479;MCMC&#26041;&#27861;&#65292;&#23558;&#37319;&#26679;&#26102;&#38388;&#20174;&#20960;&#21608;&#32553;&#30701;&#21040;&#20960;&#31186;&#38047;&#12290;</title><link>http://arxiv.org/abs/2310.12209</link><description>&lt;p&gt;
&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#22312;&#33033;&#20914;&#26143;&#26102;&#38388;&#38453;&#21015;&#19978;&#36827;&#34892;&#24555;&#36895;&#21442;&#25968;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Fast Parameter Inference on Pulsar Timing Arrays with Normalizing Flows. (arXiv:2310.12209v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#21152;&#36895;&#19988;&#20934;&#30830;&#22320;&#20272;&#35745;&#20102;&#33033;&#20914;&#26143;&#26102;&#38388;&#38453;&#21015;&#20013;&#38543;&#26426;&#24341;&#21147;&#27874;&#32972;&#26223;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#30456;&#27604;&#20256;&#32479;MCMC&#26041;&#27861;&#65292;&#23558;&#37319;&#26679;&#26102;&#38388;&#20174;&#20960;&#21608;&#32553;&#30701;&#21040;&#20960;&#31186;&#38047;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#26143;&#26102;&#38388;&#38453;&#21015;&#65288;PTAs&#65289;&#20351;&#29992;&#26114;&#36149;&#30340;MCMC&#26041;&#27861;&#36827;&#34892;&#36125;&#21494;&#26031;&#21518;&#39564;&#25512;&#26029;&#12290;&#32473;&#23450;&#19968;&#20010;&#21253;&#21547;&#32422;10-100&#39063;&#33033;&#20914;&#26143;&#21644;&#27599;&#39063;&#26143;&#30340;O&#65288;10^3&#65289;&#20010;&#26102;&#38388;&#27531;&#24046;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#38543;&#26426;&#24341;&#21147;&#27874;&#32972;&#26223;&#65288;SGWB&#65289;&#29983;&#25104;&#21518;&#39564;&#20998;&#24067;&#21487;&#33021;&#38656;&#35201;&#20960;&#22825;&#21040;&#19968;&#21608;&#30340;&#26102;&#38388;&#12290;&#35745;&#31639;&#29942;&#39048;&#20135;&#29983;&#30340;&#21407;&#22240;&#26159;&#22312;&#32771;&#34385;&#25628;&#32034;&#31354;&#38388;&#30340;&#32500;&#24230;&#26102;&#65292;MCMC&#25152;&#38656;&#30340;&#20284;&#28982;&#20989;&#25968;&#35780;&#20272;&#38750;&#24120;&#26114;&#36149;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#29983;&#25104;&#27169;&#25311;&#25968;&#25454;&#26159;&#24555;&#36895;&#30340;&#65292;&#25152;&#20197;&#21487;&#20197;&#20351;&#29992;&#29616;&#20195;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#26029;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#22312;&#27169;&#25311;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#26469;&#23545;SGWB&#21518;&#39564;&#36827;&#34892;&#26497;&#24555;&#19988;&#20934;&#30830;&#30340;&#20272;&#35745;&#65292;&#23558;&#37319;&#26679;&#26102;&#38388;&#20174;&#20960;&#21608;&#20943;&#23569;&#21040;&#20960;&#31186;&#38047;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pulsar timing arrays (PTAs) perform Bayesian posterior inference with expensive MCMC methods. Given a dataset of ~10-100 pulsars and O(10^3) timing residuals each, producing a posterior distribution for the stochastic gravitational wave background (SGWB) can take days to a week. The computational bottleneck arises because the likelihood evaluation required for MCMC is extremely costly when considering the dimensionality of the search space. Fortunately, generating simulated data is fast, so modern simulation-based inference techniques can be brought to bear on the problem. In this paper, we demonstrate how conditional normalizing flows trained on simulated data can be used for extremely fast and accurate estimation of the SGWB posteriors, reducing the sampling time from weeks to a matter of seconds.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#29616;&#26377;GNN&#32858;&#21512;&#32534;&#31243;&#25277;&#35937;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#22312;&#26368;&#20808;&#36827;&#30340;GNN&#24211;&#19978;&#36827;&#34892;&#29305;&#24449;&#30740;&#31350;&#21644;&#24615;&#33021;&#27604;&#36739;&#65292;&#25552;&#20379;&#20102;&#26410;&#26469;GNN&#21152;&#36895;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.12184</link><description>&lt;p&gt;
GNN&#32858;&#21512;&#32534;&#31243;&#25277;&#35937;&#30340;&#26550;&#26500;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Architectural Implications of GNN Aggregation Programming Abstractions. (arXiv:2310.12184v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#29616;&#26377;GNN&#32858;&#21512;&#32534;&#31243;&#25277;&#35937;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#22312;&#26368;&#20808;&#36827;&#30340;GNN&#24211;&#19978;&#36827;&#34892;&#29305;&#24449;&#30740;&#31350;&#21644;&#24615;&#33021;&#27604;&#36739;&#65292;&#25552;&#20379;&#20102;&#26410;&#26469;GNN&#21152;&#36895;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30001;&#20110;&#20174;&#22270;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#29992;&#34920;&#31034;&#30340;&#24378;&#22823;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#38543;&#30528;&#23545;&#39640;&#25928;GNN&#35745;&#31639;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#20026;&#20248;&#21270;GNN&#32858;&#21512;&#32780;&#35774;&#35745;&#30340;&#21508;&#31181;&#32534;&#31243;&#25277;&#35937;&#24212;&#36816;&#32780;&#29983;&#65292;&#20197;&#20419;&#36827;&#21152;&#36895;&#12290;&#28982;&#32780;&#65292;&#23545;&#29616;&#26377;&#25277;&#35937;&#27809;&#26377;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#20998;&#26512;&#65292;&#22240;&#27492;&#23545;&#21738;&#31181;&#26041;&#27861;&#26356;&#22909;&#27809;&#26377;&#26126;&#30830;&#30340;&#20849;&#35782;&#12290;&#22312;&#36825;&#23553;&#20449;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#32452;&#32455;&#21644;&#20256;&#25773;&#26041;&#27861;&#30340;&#32500;&#24230;&#23545;&#29616;&#26377;&#30340;GNN&#32858;&#21512;&#32534;&#31243;&#25277;&#35937;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#22312;&#26368;&#20808;&#36827;&#30340;GNN&#24211;&#19978;&#26500;&#24314;&#36825;&#20123;&#25277;&#35937;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24443;&#24213;&#21644;&#35814;&#32454;&#30340;&#29305;&#24449;&#30740;&#31350;&#65292;&#20197;&#27604;&#36739;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#24182;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#20123;&#20851;&#20110;&#26410;&#26469;GNN&#21152;&#36895;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have gained significant popularity due to the powerful capability to extract useful representations from graph data. As the need for efficient GNN computation intensifies, a variety of programming abstractions designed for optimizing GNN Aggregation have emerged to facilitate acceleration. However, there is no comprehensive evaluation and analysis upon existing abstractions, thus no clear consensus on which approach is better. In this letter, we classify existing programming abstractions for GNN Aggregation by the dimension of data organization and propagation method. By constructing these abstractions on a state-of-the-art GNN library, we perform a thorough and detailed characterization study to compare their performance and efficiency, and provide several insights on future GNN acceleration based on our analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ESGEA&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#32570;&#20047;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#24182;&#35774;&#35745;&#33410;&#28857;&#29305;&#24449;&#65292;&#36890;&#36807;&#21033;&#29992;&#23616;&#37096;&#23376;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#26469;&#21019;&#24314;&#25299;&#25169;&#24863;&#30693;&#30340;&#33410;&#28857;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#39640;&#25928;&#30340;&#35889;&#22270;&#23884;&#20837;&#25216;&#26415;&#29983;&#25104;&#23376;&#22270;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#25429;&#25417;&#32593;&#32476;&#30340;&#23616;&#37096;&#25299;&#25169;&#32452;&#32455;&#12290;</title><link>http://arxiv.org/abs/2310.12169</link><description>&lt;p&gt;
&#22686;&#24378;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#20197;&#8220;&#33258;&#25105;&#20026;&#20013;&#24515;&#8221;&#30340;&#35889;&#23376;&#22270;&#23884;&#20837;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Enhanced Graph Neural Networks with Ego-Centric Spectral Subgraph Embeddings Augmentation. (arXiv:2310.12169v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ESGEA&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#32570;&#20047;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#24182;&#35774;&#35745;&#33410;&#28857;&#29305;&#24449;&#65292;&#36890;&#36807;&#21033;&#29992;&#23616;&#37096;&#23376;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#26469;&#21019;&#24314;&#25299;&#25169;&#24863;&#30693;&#30340;&#33410;&#28857;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#39640;&#25928;&#30340;&#35889;&#22270;&#23884;&#20837;&#25216;&#26415;&#29983;&#25104;&#23376;&#22270;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#25429;&#25417;&#32593;&#32476;&#30340;&#23616;&#37096;&#25299;&#25169;&#32452;&#32455;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22797;&#26434;&#32593;&#32476;&#20013;&#25191;&#34892;&#21508;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20219;&#21153;&#26102;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#20248;&#28857;&#12290;GNN&#30340;&#20248;&#36234;&#24615;&#33021;&#24448;&#24448;&#19982;&#36755;&#20837;&#32593;&#32476;&#20013;&#33410;&#28857;&#32423;&#29305;&#24449;&#30340;&#21487;&#29992;&#24615;&#21644;&#36136;&#37327;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#32593;&#32476;&#24212;&#29992;&#26469;&#35828;&#65292;&#36825;&#31181;&#33410;&#28857;&#32423;&#20449;&#24687;&#21487;&#33021;&#26159;&#32570;&#22833;&#25110;&#19981;&#21487;&#38752;&#30340;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;GNN&#30340;&#36866;&#29992;&#24615;&#21644;&#21151;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;&#35889;&#23376;&#22270;&#23884;&#20837;&#22686;&#24378;&#65288;ESGEA&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#21644;&#35774;&#35745;&#33410;&#28857;&#29305;&#24449;&#65292;&#29305;&#21035;&#26159;&#22312;&#20449;&#24687;&#32570;&#22833;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#23616;&#37096;&#23376;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#26469;&#21019;&#24314;&#25299;&#25169;&#24863;&#30693;&#30340;&#33410;&#28857;&#29305;&#24449;&#12290;&#23376;&#22270;&#29305;&#24449;&#26159;&#20351;&#29992;&#39640;&#25928;&#30340;&#35889;&#22270;&#23884;&#20837;&#25216;&#26415;&#29983;&#25104;&#30340;&#65292;&#23427;&#20204;&#20316;&#20026;&#33410;&#28857;&#29305;&#24449;&#25429;&#25417;&#20102;&#32593;&#32476;&#30340;&#23616;&#37096;&#25299;&#25169;&#32452;&#32455;&#12290;&#28982;&#21518;&#65292;&#22914;&#26524;&#23384;&#22312;&#26126;&#30830;&#30340;&#33410;&#28857;&#29305;&#24449;&#65292;&#21017;&#23545;&#20854;&#36827;&#34892;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have shown remarkable merit in performing various learning-based tasks in complex networks. The superior performance of GNNs often correlates with the availability and quality of node-level features in the input networks. However, for many network applications, such node-level information may be missing or unreliable, thereby limiting the applicability and efficacy of GNNs. To address this limitation, we present a novel approach denoted as Ego-centric Spectral subGraph Embedding Augmentation (ESGEA), which aims to enhance and design node features, particularly in scenarios where information is lacking. Our method leverages the topological structure of the local subgraph to create topology-aware node features. The subgraph features are generated using an efficient spectral graph embedding technique, and they serve as node features that capture the local topological organization of the network. The explicit node features, if present, are then enhanced with th
&lt;/p&gt;</description></item><item><title>RK-core&#26041;&#27861;&#25506;&#32034;&#25968;&#25454;&#38598;&#20013;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#36890;&#36807;&#20998;&#26512;&#26679;&#26412;&#30340;&#26680;&#24515;&#20540;&#65292;&#21457;&#29616;&#26680;&#24515;&#20540;&#39640;&#30340;&#26679;&#26412;&#22312;&#24615;&#33021;&#19978;&#20855;&#26377;&#26356;&#22823;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.12168</link><description>&lt;p&gt;
RK-core: &#19968;&#31181;&#21487;&#29992;&#20110;&#25506;&#32034;&#25968;&#25454;&#38598;&#20013;&#23618;&#27425;&#32467;&#26500;&#30340;&#24050;&#24314;&#31435;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RK-core: An Established Methodology for Exploring the Hierarchical Structure within Datasets. (arXiv:2310.12168v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12168
&lt;/p&gt;
&lt;p&gt;
RK-core&#26041;&#27861;&#25506;&#32034;&#25968;&#25454;&#38598;&#20013;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#36890;&#36807;&#20998;&#26512;&#26679;&#26412;&#30340;&#26680;&#24515;&#20540;&#65292;&#21457;&#29616;&#26680;&#24515;&#20540;&#39640;&#30340;&#26679;&#26412;&#22312;&#24615;&#33021;&#19978;&#20855;&#26377;&#26356;&#22823;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#24050;&#32463;&#20174;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#36716;&#21521;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#12290;&#23545;&#21508;&#31181;&#23398;&#20064;&#20219;&#21153;&#30340;&#36827;&#23637;&#26159;&#30001;&#20110;&#26356;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#30340;&#31215;&#32047;&#25152;&#25512;&#21160;&#30340;&#65292;&#38543;&#20043;&#32780;&#26469;&#30340;&#26159;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26356;&#22823;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20173;&#28982;&#30456;&#23545;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;RK-core&#65292;&#20197;&#24110;&#21161;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#25968;&#25454;&#38598;&#20869;&#22797;&#26434;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#20855;&#26377;&#36739;&#20302;&#26680;&#24515;&#20540;&#30340;&#26679;&#26412;&#22312;&#20854;&#30456;&#24212;&#31867;&#21035;&#20013;&#30340;&#20195;&#34920;&#24615;&#36739;&#20302;&#65292;&#30456;&#21453;&#65292;&#20855;&#26377;&#36739;&#39640;&#26680;&#24515;&#20540;&#30340;&#26679;&#26412;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#20195;&#34920;&#24615;&#12290;&#30456;&#24212;&#22320;&#65292;&#20855;&#26377;&#36739;&#39640;&#26680;&#24515;&#20540;&#30340;&#26679;&#26412;&#30456;&#23545;&#20110;&#20855;&#26377;&#36739;&#20302;&#26680;&#24515;&#20540;&#30340;&#26679;&#26412;&#23545;&#24615;&#33021;&#36129;&#29486;&#26356;&#22823;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;RK-core&#20998;&#26512;&#20855;&#26377;&#19981;&#21516;coreness&#20540;&#30340;&#26679;&#26412;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the field of machine learning has undergone a transition from model-centric to data-centric. The advancements in diverse learning tasks have been propelled by the accumulation of more extensive datasets, subsequently facilitating the training of larger models on these datasets. However, these datasets remain relatively under-explored. To this end, we introduce a pioneering approach known as RK-core, to empower gaining a deeper understanding of the intricate hierarchical structure within datasets. Across several benchmark datasets, we find that samples with low coreness values appear less representative of their respective categories, and conversely, those with high coreness values exhibit greater representativeness. Correspondingly, samples with high coreness values make a more substantial contribution to the performance in comparison to those with low coreness values. Building upon this, we further employ RK-core to analyze the hierarchical structure of samples with differen
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#31435;&#22330;&#25991;&#20214;&#36890;&#36807;&#25506;&#35752;&#20154;&#24037;&#26234;&#33021;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#28508;&#21147;&#65292;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#19987;&#23478;&#30340;&#21327;&#20316;&#30340;&#37325;&#35201;&#24615;&#12290;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#27169;&#24335;&#35782;&#21035;&#21644;&#39044;&#27979;&#24314;&#27169;&#20027;&#21160;&#21457;&#29616;&#28431;&#27934;&#24182;&#26816;&#27979;&#24322;&#24120;&#24773;&#20917;&#65292;&#32780;&#20154;&#31867;&#19987;&#23478;&#21487;&#20197;&#34917;&#20805;&#21644;&#36741;&#21161;AI&#30340;&#36755;&#20986;&#32467;&#26524;&#65292;&#25552;&#39640;&#25972;&#20307;&#30340;&#32593;&#32476;&#23433;&#20840;&#38450;&#25252;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12162</link><description>&lt;p&gt;
AI&#28508;&#21147;&#19982;&#35748;&#30693;&#65306;&#20154;&#24037;&#26234;&#33021;&#19982;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#20154;&#26426;&#21327;&#20316;&#30340;&#31435;&#22330;&#25991;&#20214;
&lt;/p&gt;
&lt;p&gt;
AI Potentiality and Awareness: A Position Paper from the Perspective of Human-AI Teaming in Cybersecurity. (arXiv:2310.12162v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12162
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31435;&#22330;&#25991;&#20214;&#36890;&#36807;&#25506;&#35752;&#20154;&#24037;&#26234;&#33021;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#28508;&#21147;&#65292;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#19987;&#23478;&#30340;&#21327;&#20316;&#30340;&#37325;&#35201;&#24615;&#12290;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#27169;&#24335;&#35782;&#21035;&#21644;&#39044;&#27979;&#24314;&#27169;&#20027;&#21160;&#21457;&#29616;&#28431;&#27934;&#24182;&#26816;&#27979;&#24322;&#24120;&#24773;&#20917;&#65292;&#32780;&#20154;&#31867;&#19987;&#23478;&#21487;&#20197;&#34917;&#20805;&#21644;&#36741;&#21161;AI&#30340;&#36755;&#20986;&#32467;&#26524;&#65292;&#25552;&#39640;&#25972;&#20307;&#30340;&#32593;&#32476;&#23433;&#20840;&#38450;&#25252;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31435;&#22330;&#25991;&#20214;&#22312;&#32593;&#32476;&#23433;&#20840;&#29615;&#22659;&#19979;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#21147;&#65292;&#24182;&#29305;&#21035;&#24378;&#35843;&#20102;&#19982;&#24847;&#35782;&#30456;&#20851;&#30340;&#21487;&#33021;&#39118;&#38505;&#22240;&#32032;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#23558;&#20154;&#31867;&#19987;&#23478;&#32435;&#20837;&#8220;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#8221;&#21327;&#20316;&#20013;&#36827;&#34892;&#31649;&#29702;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#23427;&#20204;&#23558;&#20026;&#25915;&#20987;&#35782;&#21035;&#12289;&#20107;&#20214;&#21709;&#24212;&#21644;&#24674;&#22797;&#25552;&#20379;&#26080;&#19982;&#20262;&#27604;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#25104;&#21151;&#23558;&#20154;&#24037;&#26234;&#33021;&#37096;&#32626;&#21040;&#32593;&#32476;&#23433;&#20840;&#25514;&#26045;&#20013;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#20854;&#33021;&#21147;&#12289;&#25361;&#25112;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;&#39118;&#38505;&#22240;&#32032;&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#24433;&#21709;&#65292;&#20197;&#24212;&#23545;&#23454;&#38469;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19968;&#31181;&#24179;&#34913;&#30340;&#26041;&#27861;&#65292;&#23558;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#33021;&#21147;&#19982;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#32467;&#21512;&#36215;&#26469;&#12290;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#27169;&#24335;&#35782;&#21035;&#21644;&#39044;&#27979;&#24314;&#27169;&#20027;&#21160;&#21457;&#29616;&#28431;&#27934;&#24182;&#26816;&#27979;&#24322;&#24120;&#24773;&#20917;&#65292;&#26497;&#22823;&#25552;&#39640;&#35782;&#21035;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#20154;&#31867;&#19987;&#23478;&#21487;&#20197;&#35299;&#37322;AI&#30340;&#36755;&#20986;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#30340;&#34917;&#20805;&#21644;&#36741;&#21161;&#65292;&#20174;&#32780;&#22686;&#24378;&#25972;&#20307;&#30340;&#32593;&#32476;&#23433;&#20840;&#38450;&#25252;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This position paper explores the broad landscape of AI potentiality in the context of cybersecurity, with a particular emphasis on its possible risk factors with awareness, which can be managed by incorporating human experts in the loop, i.e., "Human-AI" teaming. As artificial intelligence (AI) technologies advance, they will provide unparalleled opportunities for attack identification, incident response, and recovery. However, the successful deployment of AI into cybersecurity measures necessitates an in-depth understanding of its capabilities, challenges, and ethical and legal implications to handle associated risk factors in real-world application areas. Towards this, we emphasize the importance of a balanced approach that incorporates AI's computational power with human expertise. AI systems may proactively discover vulnerabilities and detect anomalies through pattern recognition, and predictive modeling, significantly enhancing speed and accuracy. Human experts can explain AI-gene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31639;&#23376;&#30340;&#26041;&#27861;&#29992;&#20110;&#20998;&#26512;&#28151;&#27788;&#31995;&#32479;&#20013;&#30340;&#38750;&#31283;&#23450;&#21608;&#26399;&#36712;&#36947;&#12290;&#25105;&#20204;&#20351;&#29992;&#26680;&#31215;&#20998;&#31639;&#23376;&#26816;&#27979;&#38750;&#31283;&#23450;&#21608;&#26399;&#36712;&#36947;&#65292;&#21033;&#29992;Koopman&#31639;&#23376;&#35782;&#21035;&#21160;&#24577;&#34892;&#20026;&#65292;&#24182;&#25193;&#23637;&#20102;&#36825;&#31181;&#26041;&#27861;&#29992;&#20110;&#31283;&#23450;&#22855;&#24618;&#21560;&#24341;&#23376;&#12290;&#20197;Lorenz&#21560;&#24341;&#23376;&#20026;&#20363;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12156</link><description>&lt;p&gt;
&#22522;&#20110;&#31639;&#23376;&#30340;&#28151;&#27788;&#31995;&#32479;&#38750;&#31283;&#23450;&#21608;&#26399;&#36712;&#36947;&#30340;&#26816;&#27979;&#12289;&#23398;&#20064;&#21644;&#31283;&#23450;
&lt;/p&gt;
&lt;p&gt;
Operator-Based Detecting, Learning, and Stabilizing Unstable Periodic Orbits of Chaotic Attractors. (arXiv:2310.12156v1 [nlin.AO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31639;&#23376;&#30340;&#26041;&#27861;&#29992;&#20110;&#20998;&#26512;&#28151;&#27788;&#31995;&#32479;&#20013;&#30340;&#38750;&#31283;&#23450;&#21608;&#26399;&#36712;&#36947;&#12290;&#25105;&#20204;&#20351;&#29992;&#26680;&#31215;&#20998;&#31639;&#23376;&#26816;&#27979;&#38750;&#31283;&#23450;&#21608;&#26399;&#36712;&#36947;&#65292;&#21033;&#29992;Koopman&#31639;&#23376;&#35782;&#21035;&#21160;&#24577;&#34892;&#20026;&#65292;&#24182;&#25193;&#23637;&#20102;&#36825;&#31181;&#26041;&#27861;&#29992;&#20110;&#31283;&#23450;&#22855;&#24618;&#21560;&#24341;&#23376;&#12290;&#20197;Lorenz&#21560;&#24341;&#23376;&#20026;&#20363;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36816;&#31639;&#31526;&#29702;&#35770;&#26041;&#27861;&#30740;&#31350;&#20102;&#28151;&#27788;&#31995;&#32479;&#20013;&#38750;&#31283;&#23450;&#21608;&#26399;&#36712;&#36947;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#26816;&#27979;&#12289;&#35782;&#21035;&#21644;&#31283;&#23450;&#38750;&#31283;&#23450;&#21608;&#26399;&#36712;&#36947;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#24310;&#36831;&#22352;&#26631;&#20013;&#20351;&#29992;&#26680;&#31215;&#20998;&#31639;&#23376;&#20316;&#20026;&#38750;&#31283;&#23450;&#21608;&#26399;&#36712;&#36947;&#26816;&#27979;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#20026;&#20102;&#35782;&#21035;&#19982;&#27599;&#20010;&#38750;&#31283;&#23450;&#21608;&#26399;&#36712;&#36947;&#30456;&#20851;&#30340;&#21160;&#24577;&#34892;&#20026;&#65292;&#25105;&#20204;&#21033;&#29992;Koopman&#31639;&#23376;&#23558;&#21160;&#24577;&#34920;&#31034;&#20026;Koopman&#29305;&#24449;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#26041;&#31243;&#12290;&#36825;&#26679;&#21487;&#20197;&#36890;&#36807;&#30740;&#31350;&#22312;&#19981;&#21516;&#38750;&#31283;&#23450;&#21608;&#26399;&#36712;&#36947;&#19978;&#30340;&#20027;&#35201;&#21160;&#24577;&#27169;&#24335;&#26469;&#34920;&#24449;&#28151;&#27788;&#21560;&#24341;&#23376;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#20026;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#31283;&#23450;&#22855;&#24618;&#21560;&#24341;&#23376;&#19978;&#30340;&#38750;&#31283;&#23450;&#21608;&#26399;&#36712;&#36947;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20197;Lorenz&#21560;&#24341;&#23376;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines the use of operator-theoretic approaches to the analysis of chaotic systems through the lens of their unstable periodic orbits (UPOs). Our approach involves three data-driven steps for detecting, identifying, and stabilizing UPOs. We demonstrate the use of kernel integral operators within delay coordinates as an innovative method for UPO detection. For identifying the dynamic behavior associated with each individual UPO, we utilize the Koopman operator to present the dynamics as linear equations in the space of Koopman eigenfunctions. This allows for characterizing the chaotic attractor by investigating its principal dynamical modes across varying UPOs. We extend this methodology into an interpretable machine learning framework aimed at stabilizing strange attractors on their UPOs. To illustrate the efficacy of our approach, we apply it to the Lorenz attractor as a case study.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#26088;&#22312;&#21521;&#31185;&#23398;&#23478;&#20171;&#32461;Transformer&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#22312;&#22825;&#25991;&#23398;&#20013;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#21644;&#25104;&#20687;&#25968;&#25454;&#30340;&#20855;&#20307;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2310.12069</link><description>&lt;p&gt;
&#29992;&#20110;&#31185;&#23398;&#25968;&#25454;&#30340;Transformer&#65306;&#22825;&#25991;&#23398;&#23478;&#30340;&#25945;&#23398;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Transformers for scientific data: a pedagogical review for astronomers. (arXiv:2310.12069v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#26088;&#22312;&#21521;&#31185;&#23398;&#23478;&#20171;&#32461;Transformer&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#22312;&#22825;&#25991;&#23398;&#20013;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#21644;&#25104;&#20687;&#25968;&#25454;&#30340;&#20855;&#20307;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;ChatGPT&#21644;&#30456;&#20851;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#30456;&#20851;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#34987;&#31216;&#20026;Transformer&#12290;&#26368;&#21021;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;Transformer&#21644;&#23427;&#20204;&#21033;&#29992;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#22312;&#33258;&#28982;&#31185;&#23398;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25945;&#23398;&#21644;&#38750;&#27491;&#24335;&#32508;&#36848;&#30340;&#30446;&#26631;&#26159;&#21521;&#31185;&#23398;&#23478;&#20171;&#32461;Transformer&#12290;&#25105;&#20204;&#30340;&#25945;&#23398;&#21644;&#38750;&#27491;&#24335;&#32508;&#36848;&#21253;&#25324;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#23545;&#21407;&#22987;Transformer&#26550;&#26500;&#30340;&#25551;&#36848;&#65292;&#20197;&#21450;&#22312;&#22825;&#25991;&#23398;&#20013;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#21644;&#25104;&#20687;&#25968;&#25454;&#30340;&#19968;&#33410;&#12290;&#25105;&#20204;&#36824;&#21253;&#25324;&#20102;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#35299;&#31572;&#37096;&#20998;&#65292;&#20379;&#37027;&#20123;&#23545;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#24863;&#20852;&#36259;&#24182;&#24076;&#26395;&#24320;&#22987;&#20351;&#29992;Transformer&#36827;&#34892;&#30740;&#31350;&#30340;&#35835;&#32773;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deep learning architecture associated with ChatGPT and related generative AI products is known as transformers. Initially applied to Natural Language Processing, transformers and the self-attention mechanism they exploit have gained widespread interest across the natural sciences. The goal of this pedagogical and informal review is to introduce transformers to scientists. Our pedagogical and informal review includes the mathematics underlying the attention mechanism, a description of the original transformer architecture, and a section on applications to time series and imaging data in astronomy. We include with a Frequently Asked Questions section for readers who are curious about generative AI and interested in getting started with transformers for their research problem.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#22312;&#19981;&#21516;&#25968;&#25454;&#32452;&#25110;&#39046;&#22495;&#20013;&#23398;&#20064;&#19968;&#33268;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;AI&#21161;&#25163;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.11971</link><description>&lt;p&gt;
&#36890;&#36807;&#32676;&#20307;&#19981;&#21464;&#24615;&#23398;&#20064;&#25552;&#39640;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Generalization of Alignment with Human Preferences through Group Invariant Learning. (arXiv:2310.11971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11971
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#22312;&#19981;&#21516;&#25968;&#25454;&#32452;&#25110;&#39046;&#22495;&#20013;&#23398;&#20064;&#19968;&#33268;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;AI&#21161;&#25163;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;AI&#21161;&#25163;&#30340;&#25104;&#21151;&#22312;&#20110;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;, &#20351;&#29983;&#25104;&#30340;&#22238;&#31572;&#26356;&#21152;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;. &#20316;&#20026;&#36890;&#29992;AI&#21161;&#25163;, &#20154;&#20204;&#36234;&#26469;&#36234;&#26399;&#26395;&#23427;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#34920;&#29616;&#19968;&#33268;. &#28982;&#32780;, &#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;,&#24378;&#21270;&#23398;&#20064;(RL)&#32463;&#24120;&#21033;&#29992;&#25463;&#24452;&#20197;&#33719;&#24471;&#36739;&#39640;&#30340;&#22870;&#21169;, &#24573;&#30053;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;. &#36825;&#31181;&#23545;&#24555;&#36895;&#22870;&#21169;&#25910;&#30410;&#30340;&#20851;&#27880;&#19981;&#20165;&#21066;&#24369;&#20102;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;, &#20063;&#21066;&#24369;&#20102;&#27169;&#22411;&#23545;&#26032;&#30340;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;. &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;, &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;, &#21487;&#20197;&#36890;&#36807;RL&#22312;&#19981;&#21516;&#25968;&#25454;&#32452;&#25110;&#39046;&#22495;&#20013;&#23398;&#20064;&#19968;&#33268;&#30340;&#31574;&#30053;. &#37492;&#20110;&#33719;&#24471;&#32676;&#20307;&#26631;&#27880;&#30340;&#25361;&#25112;, &#25105;&#20204;&#30340;&#26041;&#27861;&#20250;&#33258;&#21160;&#23558;&#25968;&#25454;&#20998;&#31867;&#21040;&#19981;&#21516;&#30340;&#32452;&#20013;, &#26377;&#24847;&#22320;&#26368;&#22823;&#21270;&#24615;&#33021;&#24046;&#24322;. &#28982;&#21518;, &#25105;&#20204;&#20248;&#21270;&#31574;&#30053;&#20197;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32452;&#20013;&#34920;&#29616;&#33391;&#22909;. &#26368;&#21518;, &#21033;&#29992;&#24050;&#24314;&#31435;&#30340;
&lt;/p&gt;
&lt;p&gt;
The success of AI assistants based on language models (LLMs) hinges crucially on Reinforcement Learning from Human Feedback (RLHF), which enables the generation of responses more aligned with human preferences. As universal AI assistants, there's a growing expectation for them to perform consistently across various domains. However, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples. This focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data. In this work, we propose a novel approach that can learn a consistent policy via RL across various data groups or domains. Given the challenges associated with acquiring group annotations, our method automatically classifies data into different groups, deliberately maximizing performance variance. Then, we optimize the policy to perform well on challenging groups. Lastly, leveraging the estab
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#19978;&#30340;&#26368;&#20248;&#20256;&#36755;&#26469;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#28040;&#38500;&#20102;&#29616;&#26377;&#25439;&#22833;&#20989;&#25968;&#22312;&#33410;&#28857;&#32423;&#21035;&#39044;&#27979;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11762</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
A Quasi-Wasserstein Loss for Learning Graph Neural Networks. (arXiv:2310.11762v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11762
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#19978;&#30340;&#26368;&#20248;&#20256;&#36755;&#26469;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#28040;&#38500;&#20102;&#29616;&#26377;&#25439;&#22833;&#20989;&#25968;&#22312;&#33410;&#28857;&#32423;&#21035;&#39044;&#27979;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22312;&#33410;&#28857;&#32423;&#21035;&#39044;&#27979;&#20219;&#21153;&#20013;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26102;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25439;&#22833;&#20989;&#25968;&#26159;&#29420;&#31435;&#22320;&#24212;&#29992;&#20110;&#27599;&#20010;&#33410;&#28857;&#30340;&#65292;&#21363;&#20351;&#33410;&#28857;&#23884;&#20837;&#21644;&#23427;&#20204;&#30340;&#26631;&#31614;&#30001;&#20110;&#22270;&#32467;&#26500;&#30340;&#23384;&#22312;&#32780;&#19981;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#12290;&#20026;&#20102;&#28040;&#38500;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#65288;QW&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#20511;&#21161;&#20110;&#22312;&#22270;&#19978;&#23450;&#20041;&#30340;&#26368;&#20248;&#20256;&#36755;&#65292;&#20174;&#32780;&#24341;&#23548;GNN&#30340;&#26032;&#23398;&#20064;&#21644;&#39044;&#27979;&#33539;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#8220;&#20934;&#29926;&#29380;&#26031;&#22374;&#8221;&#36317;&#31163;&#65292;&#29992;&#20110;&#35266;&#27979;&#21040;&#30340;&#22810;&#32500;&#33410;&#28857;&#26631;&#31614;&#21644;&#23427;&#20204;&#30340;&#20272;&#35745;&#20043;&#38388;&#65292;&#36890;&#36807;&#20248;&#21270;&#22312;&#22270;&#36793;&#19978;&#23450;&#20041;&#30340;&#26631;&#31614;&#20256;&#36755;&#12290;&#36825;&#20123;&#20272;&#35745;&#26159;&#30001;&#19968;&#20010;GNN&#21442;&#25968;&#21270;&#30340;&#65292;&#20854;&#20013;&#26368;&#20248;&#26631;&#31614;&#20256;&#36755;&#21487;&#20197;&#36873;&#25321;&#24615;&#22320;&#30830;&#23450;&#22270;&#36793;&#30340;&#26435;&#37325;&#12290;&#36890;&#36807;&#23558;&#26631;&#31614;&#20256;&#36755;&#30340;&#20005;&#26684;&#32422;&#26463;&#37325;&#26032;&#34920;&#36798;&#20026;&#22522;&#20110;Bregman&#25955;&#24230;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#25152;&#25552;&#20986;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#25439;&#22833;&#65292;&#20851;&#32852;&#20004;&#20010;&#39640;&#25928;&#27714;&#35299;&#22120;&#26469;&#23398;&#20064;GNN&#20197;&#21450;&#26368;&#20248;&#26631;&#31614;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;
When learning graph neural networks (GNNs) in node-level prediction tasks, most existing loss functions are applied for each node independently, even if node embeddings and their labels are non-i.i.d. because of their graph structures. To eliminate such inconsistency, in this study we propose a novel Quasi-Wasserstein (QW) loss with the help of the optimal transport defined on graphs, leading to new learning and prediction paradigms of GNNs. In particular, we design a "Quasi-Wasserstein" distance between the observed multi-dimensional node labels and their estimations, optimizing the label transport defined on graph edges. The estimations are parameterized by a GNN in which the optimal label transport may determine the graph edge weights optionally. By reformulating the strict constraint of the label transport to a Bregman divergence-based regularizer, we obtain the proposed Quasi-Wasserstein loss associated with two efficient solvers learning the GNN together with optimal label transp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#29575;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#24314;&#27169;&#21644;&#39044;&#27979;&#20855;&#26377;&#23618;&#27425;&#21270;&#20851;&#31995;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#32771;&#34385;&#28857;&#39044;&#27979;&#65292;&#36824;&#33021;&#25552;&#20379;&#32463;&#36807;&#33391;&#22909;&#26657;&#20934;&#30340;&#27010;&#29575;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#24314;&#27169;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#39044;&#27979;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11569</link><description>&lt;p&gt;
&#24403;&#21018;&#24615;&#25104;&#20026;&#38382;&#39064;&#65306;&#36719;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#29992;&#20110;&#27010;&#29575;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
When Rigidity Hurts: Soft Consistency Regularization for Probabilistic Hierarchical Time Series Forecasting. (arXiv:2310.11569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11569
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#29575;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#24314;&#27169;&#21644;&#39044;&#27979;&#20855;&#26377;&#23618;&#27425;&#21270;&#20851;&#31995;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#32771;&#34385;&#28857;&#39044;&#27979;&#65292;&#36824;&#33021;&#25552;&#20379;&#32463;&#36807;&#33391;&#22909;&#26657;&#20934;&#30340;&#27010;&#29575;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#19988;&#22312;&#24314;&#27169;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;&#39044;&#27979;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#37325;&#35201;&#21464;&#20307;&#65292;&#20854;&#30446;&#26631;&#26159;&#23545;&#20855;&#26377;&#23618;&#27425;&#21270;&#20851;&#31995;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#24314;&#27169;&#21644;&#39044;&#27979;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#20851;&#27880;&#28857;&#39044;&#27979;&#65292;&#24182;&#26410;&#25552;&#20379;&#32463;&#36807;&#33391;&#22909;&#26657;&#20934;&#30340;&#27010;&#29575;&#39044;&#27979;&#20998;&#24067;&#12290;&#26368;&#36817;&#30340;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#20063;&#22312;&#28857;&#39044;&#27979;&#21644;&#20998;&#24067;&#26679;&#26412;&#20013;&#26045;&#21152;&#23618;&#27425;&#20851;&#31995;&#65292;&#20294;&#26410;&#32771;&#34385;&#39044;&#27979;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20063;&#40664;&#35748;&#25968;&#25454;&#38598;&#24635;&#26159;&#19982;&#32473;&#23450;&#30340;&#23618;&#27425;&#20851;&#31995;&#20445;&#25345;&#19968;&#33268;&#65292;&#24182;&#26410;&#36866;&#24212;&#26174;&#31034;&#20986;&#20559;&#31163;&#27492;&#20551;&#35774;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#20004;&#20010;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#20102;PROFHiT&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#20010;&#23436;&#20840;&#27010;&#29575;&#30340;&#20998;&#23618;&#39044;&#27979;&#27169;&#22411;&#65292;&#21516;&#26102;&#23545;&#25972;&#20010;&#23618;&#27425;&#30340;&#39044;&#27979;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;PROFHiT&#20351;&#29992;&#28789;&#27963;&#30340;&#27010;&#29575;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic hierarchical time-series forecasting is an important variant of time-series forecasting, where the goal is to model and forecast multivariate time-series that have underlying hierarchical relations. Most methods focus on point predictions and do not provide well-calibrated probabilistic forecasts distributions. Recent state-of-art probabilistic forecasting methods also impose hierarchical relations on point predictions and samples of distribution which does not account for coherency of forecast distributions. Previous works also silently assume that datasets are always consistent with given hierarchical relations and do not adapt to real-world datasets that show deviation from this assumption. We close both these gap and propose PROFHiT, which is a fully probabilistic hierarchical forecasting model that jointly models forecast distribution of entire hierarchy. PROFHiT uses a flexible probabilistic Bayesian approach and introduces a novel Distributional Coherency regulariz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#34507;&#30333;&#36136;&#22522;&#20110;&#32467;&#26500;&#30340;&#24615;&#36136;&#39044;&#27979;&#20013;&#20351;&#29992;&#39044;&#27979;&#32467;&#26500;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#21407;&#22240;&#65292;&#24182;&#23558;&#20854;&#24402;&#22240;&#20026;&#32467;&#26500;&#23884;&#20837;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.11466</link><description>&lt;p&gt;
&#34507;&#30333;&#36136;&#19977;&#32500;&#22270;&#32467;&#26500;&#23398;&#20064;&#29992;&#20110;&#31283;&#20581;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#34507;&#30333;&#36136;&#24615;&#36136;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Protein 3D Graph Structure Learning for Robust Structure-based Protein Property Prediction. (arXiv:2310.11466v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#34507;&#30333;&#36136;&#22522;&#20110;&#32467;&#26500;&#30340;&#24615;&#36136;&#39044;&#27979;&#20013;&#20351;&#29992;&#39044;&#27979;&#32467;&#26500;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#21407;&#22240;&#65292;&#24182;&#23558;&#20854;&#24402;&#22240;&#20026;&#32467;&#26500;&#23884;&#20837;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#22522;&#20110;&#32467;&#26500;&#30340;&#24615;&#36136;&#39044;&#27979;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#29983;&#29289;&#23398;&#20219;&#21153;&#65288;&#22914;&#34507;&#30333;&#36136;&#21151;&#33021;&#39044;&#27979;&#21644;&#20122;&#32454;&#32990;&#23450;&#20301;&#20272;&#35745;&#65289;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#39640;&#24230;&#20381;&#36182;&#23454;&#39564;&#34507;&#30333;&#36136;&#32467;&#26500;&#25968;&#25454;&#65292;&#22312;&#36825;&#20123;&#25968;&#25454;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#22833;&#36133;&#12290;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65288;&#22914;AlphaFold2&#65289;&#39044;&#27979;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#30446;&#21069;&#30340;&#20570;&#27861;&#65292;&#21363;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20165;&#20351;&#29992;&#20934;&#30830;&#39044;&#27979;&#30340;&#32467;&#26500;&#65292;&#20250;&#23548;&#33268;&#39044;&#27979;&#20934;&#30830;&#24615;&#26126;&#26174;&#19979;&#38477;&#12290;&#34429;&#28982;&#31867;&#20284;&#29616;&#35937;&#24050;&#32463;&#22312;&#19968;&#33324;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#65289;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#20316;&#20026;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#23427;&#20204;&#23545;&#34507;&#30333;&#36136;&#24615;&#36136;&#39044;&#27979;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#32467;&#26500;&#34920;&#31034;&#23398;&#20064;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#21033;&#29992;&#39044;&#27979;&#30340;&#32467;&#26500;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#21407;&#22240;&#65292;&#23558;&#20854;&#24402;&#22240;&#20026;&#32467;&#26500;&#23884;&#20837;&#20559;&#24046;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Protein structure-based property prediction has emerged as a promising approach for various biological tasks, such as protein function prediction and sub-cellular location estimation. The existing methods highly rely on experimental protein structure data and fail in scenarios where these data are unavailable. Predicted protein structures from AI tools (e.g., AlphaFold2) were utilized as alternatives. However, we observed that current practices, which simply employ accurately predicted structures during inference, suffer from notable degradation in prediction accuracy. While similar phenomena have been extensively studied in general fields (e.g., Computer Vision) as model robustness, their impact on protein property prediction remains unexplored. In this paper, we first investigate the reason behind the performance decrease when utilizing predicted structures, attributing it to the structure embedding bias from the perspective of structure representation learning. To study this problem
&lt;/p&gt;</description></item><item><title>HGCVAE&#26159;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#26469;&#35299;&#20915;&#24322;&#26500;&#22270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.11102</link><description>&lt;p&gt;
HGCVAE: &#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HGCVAE: Integrating Generative and Contrastive Learning for Heterogeneous Graph Learning. (arXiv:2310.11102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11102
&lt;/p&gt;
&lt;p&gt;
HGCVAE&#26159;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#24322;&#26500;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#26469;&#35299;&#20915;&#24322;&#26500;&#22270;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#22270;&#23398;&#20064;&#20013;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#21644;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#29983;&#25104;&#24335;SSL&#22312;&#24322;&#26500;&#22270;&#23398;&#20064;&#65288;HGL&#65289;&#20013;&#30340;&#38382;&#39064;&#12290;&#20197;&#24448;&#20851;&#20110;&#24322;&#26500;&#22270;&#30340;SSL&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#23545;&#27604;&#23398;&#20064;&#65292;&#38656;&#35201;&#35774;&#35745;&#22797;&#26434;&#30340;&#35270;&#22270;&#26469;&#25429;&#25417;&#24322;&#36136;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29983;&#25104;&#24335;SSL&#26041;&#27861;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#35299;&#20915;HGL&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HGCVAE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#65292;&#20351;HGL&#25670;&#33073;&#20102;&#22797;&#26434;&#24322;&#36136;&#24615;&#30340;&#36127;&#25285;&#12290;HGCVAE&#19981;&#20877;&#19987;&#27880;&#20110;&#22797;&#26434;&#30340;&#24322;&#36136;&#24615;&#65292;&#32780;&#26159;&#20805;&#20998;&#21033;&#29992;&#20102;&#29983;&#25104;&#24335;SSL&#30340;&#28508;&#21147;&#12290;HGCVAE&#21019;&#26032;&#22320;&#23558;&#23545;&#27604;&#23398;&#20064;&#19982;&#29983;&#25104;&#24335;SSL&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20102;&#20960;&#20010;&#20851;&#38190;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37319;&#29992;&#28176;&#36827;&#26426;&#21046;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;hard&#26679;&#26412;&#65292;
&lt;/p&gt;
&lt;p&gt;
Generative self-supervised learning (SSL) has exhibited significant potential and garnered increasing interest in graph learning. In this study, we aim to explore the problem of generative SSL in the context of heterogeneous graph learning (HGL). The previous SSL approaches for heterogeneous graphs have primarily relied on contrastive learning, necessitating the design of complex views to capture heterogeneity. However, existing generative SSL methods have not fully leveraged the capabilities of generative models to address the challenges of HGL. In this paper, we present HGCVAE, a novel contrastive variational graph auto-encoder that liberates HGL from the burden of intricate heterogeneity capturing. Instead of focusing on complicated heterogeneity, HGCVAE harnesses the full potential of generative SSL. HGCVAE innovatively consolidates contrastive learning with generative SSL, introducing several key innovations. Firstly, we employ a progressive mechanism to generate high-quality hard
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#23545;&#21521;&#32534;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38142;&#36335;&#39044;&#27979;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#24402;&#32435;&#20559;&#24046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#20998;&#31867;&#21508;&#31181;&#19981;&#21516;&#22240;&#32032;&#24418;&#25104;&#30340;&#38142;&#36335;&#12290;</title><link>http://arxiv.org/abs/2310.11009</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#30340;&#23545;&#21521;&#32534;&#30721;&#29992;&#20110;&#38142;&#36335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adaptive Pairwise Encodings for Link Prediction. (arXiv:2310.11009v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11009
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#23545;&#21521;&#32534;&#30721;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38142;&#36335;&#39044;&#27979;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#24402;&#32435;&#20559;&#24046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#20998;&#31867;&#21508;&#31181;&#19981;&#21516;&#22240;&#32032;&#24418;&#25104;&#30340;&#38142;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#36335;&#39044;&#27979;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#22522;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#20219;&#21153;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#26377;&#24212;&#29992;&#12290;&#32463;&#20856;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#25163;&#24037;&#35774;&#35745;&#30340;&#21551;&#21457;&#24335;&#31574;&#30053;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#21551;&#21457;&#24335;&#24230;&#37327;&#34987;&#36873;&#25321;&#20026;&#22312;&#19982;&#38142;&#36335;&#24418;&#25104;&#30456;&#20851;&#30340;&#22522;&#26412;&#22240;&#32032;&#19978;&#19982;&#20043;&#30456;&#20851;&#33391;&#22909;&#12290;&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#19968;&#31867;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#30340;&#20248;&#21183;&#19982;&#21551;&#21457;&#24335;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;MPNN&#30340;&#36755;&#20986;&#20197;&#21450;&#25429;&#25417;&#20505;&#36873;&#38142;&#36335;&#20013;&#33410;&#28857;&#20043;&#38388;&#20851;&#31995;&#30340;&#8220;&#23545;&#21521;&#32534;&#30721;&#8221;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#23427;&#20204;&#24050;&#32463;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#23545;&#21521;&#32534;&#30721;&#24448;&#24448;&#20855;&#26377;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#20351;&#29992;&#30456;&#21516;&#30340;&#22522;&#26412;&#22240;&#32032;&#26469;&#20998;&#31867;&#25152;&#26377;&#38142;&#36335;&#12290;&#36825;&#38480;&#21046;&#20102;&#29616;&#26377;&#26041;&#27861;&#23398;&#20064;&#22914;&#20309;&#27491;&#30830;&#20998;&#31867;&#21487;&#33021;&#30001;&#19981;&#21516;&#22240;&#32032;&#24418;&#25104;&#30340;&#21508;&#31181;&#19981;&#21516;&#38142;&#36335;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;&#23545;&#21521;&#32534;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction is a common task on graph-structured data that has seen applications in a variety of domains. Classically, hand-crafted heuristics were used for this task. Heuristic measures are chosen such that they correlate well with the underlying factors related to link formation. In recent years, a new class of methods has emerged that combines the advantages of message-passing neural networks (MPNN) and heuristics methods. These methods perform predictions by using the output of an MPNN in conjunction with a "pairwise encoding" that captures the relationship between nodes in the candidate link. They have been shown to achieve strong performance on numerous datasets. However, current pairwise encodings often contain a strong inductive bias, using the same underlying factors to classify all links. This limits the ability of existing methods to learn how to properly classify a variety of different links that may form from different factors. To address this limitation, we propose a 
&lt;/p&gt;</description></item><item><title>ACES&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#25551;&#36848;&#31526;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#32534;&#31243;&#38590;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20248;&#21270;&#26377;&#36259;&#30340;&#22810;&#26679;&#24615;&#21644;&#23569;&#26679;&#26412;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.10692</link><description>&lt;p&gt;
ACES: &#20351;&#29992;&#33258;&#25105;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#25551;&#36848;&#31526;&#29983;&#25104;&#22810;&#26679;&#30340;&#32534;&#31243;&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;
ACES: generating diverse programming puzzles with autotelic language models and semantic descriptors. (arXiv:2310.10692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10692
&lt;/p&gt;
&lt;p&gt;
ACES&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#25105;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#25551;&#36848;&#31526;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#32534;&#31243;&#38590;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20248;&#21270;&#26377;&#36259;&#30340;&#22810;&#26679;&#24615;&#21644;&#23569;&#26679;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#21644;&#36873;&#25321;&#26032;&#39062;&#26377;&#36259;&#30340;&#38382;&#39064;&#26159;&#22909;&#22855;&#24515;&#12289;&#31185;&#23398;&#21644;&#21019;&#26032;&#30340;&#26680;&#24515;&#12290;&#22312;Python&#32534;&#31243;&#38590;&#39064;&#30340;&#26080;&#38480;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#21160;&#38382;&#39064;&#29983;&#25104;&#12290;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#26088;&#22312;&#24314;&#27169;&#21442;&#32771;&#20998;&#24067;&#65292;&#27809;&#26377;&#26126;&#30830;&#30340;&#22810;&#26679;&#24615;&#20248;&#21270;&#12290;&#20854;&#20182;&#26041;&#27861;&#22312;&#26377;&#38480;&#30340;&#25163;&#24037;&#32534;&#30721;&#34920;&#31034;&#31354;&#38388;&#25110;&#19981;&#21487;&#35299;&#37322;&#30340;&#23398;&#20064;&#23884;&#20837;&#31354;&#38388;&#20013;&#26126;&#30830;&#20248;&#21270;&#22810;&#26679;&#24615;&#65292;&#36825;&#20123;&#23884;&#20837;&#31354;&#38388;&#21487;&#33021;&#19982;&#20154;&#31867;&#23545;&#26377;&#36259;&#21464;&#21270;&#30340;&#24863;&#30693;&#19981;&#31526;&#12290;&#36890;&#36807;ACES&#65288;&#33258;&#25105;&#30446;&#26631;&#20195;&#30721;&#25506;&#32034;&#19982;&#35821;&#20041;&#25551;&#36848;&#31526;&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#30446;&#26631;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#35821;&#20041;&#25551;&#36848;&#31526;&#65292;&#30452;&#25509;&#20248;&#21270;&#26377;&#36259;&#30340;&#22810;&#26679;&#24615;&#65292;&#20197;&#21450;&#23569;&#26679;&#26412;&#29983;&#25104;&#12290;&#27599;&#20010;&#38590;&#39064;&#37117;&#26631;&#35760;&#26377;10&#20010;&#32500;&#24230;&#65292;&#27599;&#20010;&#32500;&#24230;&#25429;&#25417;&#20102;&#35299;&#20915;&#23427;&#25152;&#38656;&#30340;&#32534;&#31243;&#25216;&#33021;&#12290;ACES&#29983;&#25104;&#24182;&#36861;&#27714;&#26032;&#39062;&#21487;&#34892;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding and selecting new and interesting problems to solve is at the heart of curiosity, science and innovation. We here study automated problem generation in the context of the open-ended space of python programming puzzles. Existing generative models often aim at modeling a reference distribution without any explicit diversity optimization. Other methods explicitly optimizing for diversity do so either in limited hand-coded representation spaces or in uninterpretable learned embedding spaces that may not align with human perceptions of interesting variations. With ACES (Autotelic Code Exploration via Semantic descriptors), we introduce a new autotelic generation method that leverages semantic descriptors produced by a large language model (LLM) to directly optimize for interesting diversity, as well as few-shot-based generation. Each puzzle is labeled along 10 dimensions, each capturing a programming skill required to solve it. ACES generates and pursues novel and feasible goals to 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#36328;&#25991;&#26723;&#30340;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.10638</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#65306;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
In-Context Pretraining: Language Modeling Beyond Document Boundaries. (arXiv:2310.10638v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#36328;&#25991;&#26723;&#30340;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36890;&#36807;&#39044;&#27979;&#32473;&#23450;&#25991;&#26723;&#21069;&#32512;&#30340;&#26631;&#35760;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#33021;&#22815;&#30452;&#25509;&#36827;&#34892;&#38271;&#31687;&#29983;&#25104;&#21644;&#25552;&#31034;&#24335;&#20219;&#21153;&#65292;&#36825;&#21487;&#20197;&#31616;&#21270;&#20026;&#25991;&#26723;&#23436;&#25104;&#12290;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#36890;&#36807;&#36830;&#25509;&#38543;&#26426;&#32452;&#21512;&#30340;&#30701;&#25991;&#26723;&#26469;&#35757;&#32451;LMs&#65292;&#20197;&#21019;&#24314;&#36755;&#20837;&#19978;&#19979;&#25991;&#65292;&#20294;&#21069;&#19968;&#20010;&#25991;&#26723;&#23545;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#25991;&#26723;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#20449;&#21495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#65292;&#21363;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26126;&#30830;&#40723;&#21169;&#23427;&#20204;&#36328;&#36234;&#25991;&#26723;&#36793;&#30028;&#36827;&#34892;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#65292;&#20351;&#27599;&#20010;&#19978;&#19979;&#25991;&#21253;&#21547;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25991;&#26723;&#25490;&#24207;&#38382;&#39064;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26377;&#25968;&#21313;&#20159;&#20010;&#25991;&#26723;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#27599;&#20010;&#25991;&#26723;&#20013;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;&#30456;&#20284;&#24615;&#32780;&#19981;&#37325;&#22797;&#20219;&#20309;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) are currently trained to predict tokens given document prefixes, enabling them to directly perform long-form generation and prompting-style tasks which can be reduced to document completion. Existing pretraining pipelines train LMs by concatenating random sets of short documents to create input contexts but the prior documents provide no signal for predicting the next document. We instead present In-Context Pretraining, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. We can do In-Context Pretraining by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#24179;&#28369;&#39640;&#36136;&#37327;&#30340;&#19987;&#23478;&#36712;&#36857;&#23545;&#40784;&#65292;&#23454;&#29616;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#26367;&#20195;&#65292;&#24182;&#25552;&#20986;&#20102;&#21098;&#36753;&#25439;&#22833;&#21644;&#26799;&#24230;&#24809;&#32602;&#30340;&#38598;&#25104;&#26469;&#35843;&#33410;&#23398;&#29983;&#21644;&#19987;&#23478;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;</title><link>http://arxiv.org/abs/2310.10541</link><description>&lt;p&gt;
&#36890;&#36807;&#19982;&#24179;&#28369;&#39640;&#36136;&#37327;&#19987;&#23478;&#36712;&#36857;&#23545;&#40784;&#23454;&#29616;&#39640;&#25928;&#25968;&#25454;&#38598;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
Efficient Dataset Distillation through Alignment with Smooth and High-Quality Expert Trajectories. (arXiv:2310.10541v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#24179;&#28369;&#39640;&#36136;&#37327;&#30340;&#19987;&#23478;&#36712;&#36857;&#23545;&#40784;&#65292;&#23454;&#29616;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#26367;&#20195;&#65292;&#24182;&#25552;&#20986;&#20102;&#21098;&#36753;&#25439;&#22833;&#21644;&#26799;&#24230;&#24809;&#32602;&#30340;&#38598;&#25104;&#26469;&#35843;&#33410;&#23398;&#29983;&#21644;&#19987;&#23478;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#19968;&#22823;&#22411;&#30340;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36825;&#20351;&#24471;&#35757;&#32451;&#21644;&#21442;&#25968;&#35843;&#25972;&#36807;&#31243;&#21464;&#24471;&#26114;&#36149;&#19988;&#32791;&#26102;&#12290;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#36873;&#25321;&#23558;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;&#20449;&#24687;&#31934;&#28860;&#20026;&#23567;&#22411;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#35757;&#32451;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;DD&#65289;&#30340;&#25968;&#25454;&#39640;&#25928;&#26041;&#27861;&#12290;&#23613;&#31649;&#35813;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#34920;&#29616;&#19981;&#20339;&#65292;&#19981;&#33021;&#26377;&#25928;&#26367;&#20195;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#19982;&#20165;&#20851;&#27880;&#25913;&#36827;&#23398;&#29983;&#25104;&#32489;&#30340;&#20808;&#21069;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#39318;&#27425;&#35748;&#35782;&#21040;&#19987;&#23478;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#37325;&#35201;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#35748;&#20026;&#22312;&#21518;&#32493;&#25968;&#25454;&#38598;&#31934;&#28860;&#20013;&#65292;&#37319;&#29992;&#26356;&#24378;&#22823;&#30340;&#19987;&#23478;&#36712;&#36857;&#26102;&#65292;&#19987;&#23478;&#30340;&#24179;&#28369;&#24615;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21098;&#36753;&#25439;&#22833;&#21644;&#26799;&#24230;&#24809;&#32602;&#30340;&#38598;&#25104;&#65292;&#20197;&#35843;&#33410;&#23398;&#29983;&#21644;&#19987;&#23478;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training a large and state-of-the-art machine learning model typically necessitates the use of large-scale datasets, which, in turn, makes the training and parameter-tuning process expensive and time-consuming. Some researchers opt to distil information from real-world datasets into tiny and compact synthetic datasets while maintaining their ability to train a well-performing model, hence proposing a data-efficient method known as Dataset Distillation (DD). Despite recent progress in this field, existing methods still underperform and cannot effectively replace large datasets. In this paper, unlike previous methods that focus solely on improving the efficacy of student distillation, we are the first to recognize the important interplay between expert and student. We argue the significant impact of expert smoothness when employing more potent expert trajectories in subsequent dataset distillation. Based on this, we introduce the integration of clipping loss and gradient penalty to regul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;Microscaling&#65288;MX&#65289;&#25968;&#25454;&#26684;&#24335;&#22312;&#38477;&#20302;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;MX&#25968;&#25454;&#26684;&#24335;&#21487;&#20197;&#20316;&#20026;&#22522;&#32447;FP32&#30340;&#26367;&#20195;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#29992;&#25143;&#25705;&#25830;&#65292;&#24182;&#19988;&#25104;&#21151;&#22312;&#36229;&#36807;&#20004;&#25171;&#22522;&#20934;&#27979;&#35797;&#20013;&#20197;&#23567;&#20110;8&#20301;&#30340;&#25968;&#25454;&#26684;&#24335;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.10537</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#24494;&#25193;&#23637;&#25968;&#25454;&#26684;&#24335;
&lt;/p&gt;
&lt;p&gt;
Microscaling Data Formats for Deep Learning. (arXiv:2310.10537v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;Microscaling&#65288;MX&#65289;&#25968;&#25454;&#26684;&#24335;&#22312;&#38477;&#20302;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;MX&#25968;&#25454;&#26684;&#24335;&#21487;&#20197;&#20316;&#20026;&#22522;&#32447;FP32&#30340;&#26367;&#20195;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#29992;&#25143;&#25705;&#25830;&#65292;&#24182;&#19988;&#25104;&#21151;&#22312;&#36229;&#36807;&#20004;&#25171;&#22522;&#20934;&#27979;&#35797;&#20013;&#20197;&#23567;&#20110;8&#20301;&#30340;&#25968;&#25454;&#26684;&#24335;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31364;&#20301;&#23485;&#25968;&#25454;&#26684;&#24335;&#23545;&#20110;&#38477;&#20302;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#23558;&#27599;&#20010;&#22359;&#30340;&#32553;&#25918;&#22240;&#23376;&#19982;&#31364;&#28014;&#28857;&#21644;&#25972;&#25968;&#31867;&#22411;&#30456;&#32467;&#21512;&#30340;&#24494;&#25193;&#23637;&#65288;MX&#65289;&#25968;&#25454;&#26684;&#24335;&#65292;&#20197;&#28385;&#36275;&#30828;&#20214;&#25928;&#29575;&#12289;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#29992;&#25143;&#25705;&#25830;&#20043;&#38388;&#30340;&#31454;&#20105;&#38656;&#27714;&#12290;&#23545;&#20110;AI&#25512;&#29702;&#21644;&#35757;&#32451;&#65292;MX&#25968;&#25454;&#26684;&#24335;&#22312;&#36229;&#36807;&#20004;&#25171;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#20316;&#20026;&#22522;&#32447;FP32&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#19988;&#20351;&#29992;&#26102;&#29992;&#25143;&#25705;&#25830;&#23567;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#26368;&#23567;&#30340;&#20934;&#30830;&#24615;&#25439;&#22833;&#21644;&#26080;&#38656;&#20462;&#25913;&#35757;&#32451;&#37197;&#26041;&#30340;&#24773;&#20917;&#19979;&#65292;&#39318;&#27425;&#35757;&#32451;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#23567;&#20110;8&#20301;&#30340;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;&#28176;&#21464;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Narrow bit-width data formats are key to reducing the computational and storage costs of modern deep learning applications. This paper evaluates Microscaling (MX) data formats that combine a per-block scaling factor with narrow floating-point and integer types for individual elements. MX formats balance the competing needs of hardware efficiency, model accuracy, and user friction. Empirical results on over two dozen benchmarks demonstrate practicality of MX data formats as a drop-in replacement for baseline FP32 for AI inference and training with low user friction. We also show the first instance of training generative language models at sub-8-bit weights, activations, and gradients with minimal accuracy loss and no modifications to the training recipe.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#21644;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;60&#22810;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;TSC&#30340;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.10060</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#25968;&#25454;&#22686;&#24378;&#65306;&#19968;&#39033;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#21644;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation for Time-Series Classification: An Extensive Empirical Study and Comprehensive Survey. (arXiv:2310.10060v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#21644;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;60&#22810;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;TSC&#30340;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#24050;&#25104;&#20026;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;TSC&#65289;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#31574;&#30053;&#65292;&#20027;&#35201;&#22240;&#20026;&#23427;&#21487;&#20197;&#22686;&#21152;&#35757;&#32451;&#26679;&#26412;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#20351;&#25968;&#25454;&#38598;&#22810;&#26679;&#21270;&#65292;&#24182;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;TSC&#20013;&#30340;DA&#30740;&#31350;&#23384;&#22312;&#30528;&#25991;&#29486;&#35780;&#23457;&#30340;&#29255;&#27573;&#21270;&#65292;&#26041;&#27861;&#23398;&#20998;&#31867;&#19981;&#28165;&#26224;&#65292;&#35780;&#20272;&#25351;&#26631;&#19981;&#36275;&#20197;&#21450;&#32570;&#20047;&#29992;&#25143;&#21451;&#22909;&#30340;&#24037;&#20855;&#31561;&#38382;&#39064;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#23545;TSC&#39046;&#22495;&#20013;&#30340;DA&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#25345;&#32493;&#21313;&#24180;&#30340;&#24191;&#27867;&#25991;&#29486;&#22238;&#39038;&#65292;&#21457;&#29616;&#24403;&#20195;&#32508;&#36848;&#25991;&#31456;&#24456;&#23569;&#33021;&#22815;&#28085;&#30422;DA&#22312;TSC&#19978;&#30340;&#20840;&#37096;&#36827;&#23637;&#65292;&#22240;&#27492;&#25105;&#20204;&#20180;&#32454;&#20998;&#26512;&#20102;100&#22810;&#31687;&#23398;&#26415;&#25991;&#31456;&#65292;&#24635;&#32467;&#20986;&#20102;60&#22810;&#31181;&#29420;&#29305;&#30340;DA&#25216;&#26415;&#12290;&#36825;&#39033;&#20005;&#26684;&#30340;&#20998;&#26512;&#24418;&#25104;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#19987;&#38376;&#38024;&#23545;TSC&#20013;&#30340;DA&#32454;&#33410;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation (DA) has emerged as an indispensable strategy in Time Series Classification (TSC), primarily due to its capacity to amplify training samples, thereby bolstering model robustness, diversifying datasets, and curtailing overfitting. However, the current landscape of DA in TSC is plagued with fragmented literature reviews, nebulous methodological taxonomies, inadequate evaluative measures, and a dearth of accessible, user-oriented tools. In light of these challenges, this study embarks on an exhaustive dissection of DA methodologies within the TSC realm. Our initial approach involved an extensive literature review spanning a decade, revealing that contemporary surveys scarcely capture the breadth of advancements in DA for TSC, prompting us to meticulously analyze over 100 scholarly articles to distill more than 60 unique DA techniques. This rigorous analysis precipitated the formulation of a novel taxonomy, purpose-built for the intricacies of DA in TSC, categorizing tech
&lt;/p&gt;</description></item><item><title>Rank-DETR&#26159;&#19968;&#31181;&#39640;&#36136;&#37327;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25490;&#21517;&#23548;&#21521;&#30340;&#35774;&#35745;&#65292;&#21253;&#25324;&#26550;&#26500;&#35774;&#35745;&#21644;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#21331;&#36234;&#30340;&#22522;&#20110;DETR&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2310.08854</link><description>&lt;p&gt;
&#39640;&#36136;&#37327;&#30446;&#26631;&#26816;&#27979;&#30340;Rank-DETR&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rank-DETR for High Quality Object Detection. (arXiv:2310.08854v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08854
&lt;/p&gt;
&lt;p&gt;
Rank-DETR&#26159;&#19968;&#31181;&#39640;&#36136;&#37327;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25490;&#21517;&#23548;&#21521;&#30340;&#35774;&#35745;&#65292;&#21253;&#25324;&#26550;&#26500;&#35774;&#35745;&#21644;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#21331;&#36234;&#30340;&#22522;&#20110;DETR&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26816;&#27979;&#21464;&#25442;&#22120;&#65288;DETR&#65289;&#20351;&#29992;&#19968;&#32452;&#23545;&#35937;&#26597;&#35810;&#26469;&#39044;&#27979;&#36793;&#30028;&#26694;&#21015;&#34920;&#65292;&#36890;&#36807;&#23558;&#20854;&#20998;&#31867;&#32622;&#20449;&#24230;&#24471;&#20998;&#36827;&#34892;&#25490;&#24207;&#65292;&#24182;&#36873;&#25321;&#25490;&#21517;&#38752;&#21069;&#30340;&#39044;&#27979;&#32467;&#26524;&#20316;&#20026;&#32473;&#23450;&#36755;&#20837;&#22270;&#20687;&#30340;&#26368;&#32456;&#26816;&#27979;&#32467;&#26524;&#12290;&#24615;&#33021;&#21331;&#36234;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#38656;&#35201;&#23545;&#36793;&#30028;&#26694;&#39044;&#27979;&#36827;&#34892;&#20934;&#30830;&#30340;&#25490;&#24207;&#12290;&#23545;&#20110;&#22522;&#20110;DETR&#30340;&#26816;&#27979;&#22120;&#65292;&#25490;&#21517;&#38752;&#21069;&#30340;&#36793;&#30028;&#26694;&#30001;&#20110;&#20998;&#31867;&#24471;&#20998;&#19982;&#23450;&#20301;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#32780;&#23548;&#33268;&#23450;&#20301;&#36136;&#37327;&#36739;&#24046;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#39640;&#36136;&#37327;&#26816;&#27979;&#22120;&#30340;&#26500;&#24314;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31995;&#21015;&#38754;&#21521;&#25490;&#21517;&#30340;&#35774;&#35745;&#65292;&#20849;&#21516;&#31216;&#20026;Rank-DETR&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#19988;&#24615;&#33021;&#21331;&#36234;&#30340;&#22522;&#20110;DETR&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#65306;&#65288;i&#65289;&#19968;&#20010;&#38754;&#21521;&#25490;&#21517;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#21487;&#20197;&#20419;&#36827;&#27491;&#38754;&#39044;&#27979;&#24182;&#25233;&#21046;&#36127;&#38754;&#39044;&#27979;&#65292;&#20197;&#30830;&#20445;&#26356;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#19968;&#20010;&#38754;&#21521;&#25490;&#21517;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#21305;&#37197;&#25104;&#26412;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern detection transformers (DETRs) use a set of object queries to predict a list of bounding boxes, sort them by their classification confidence scores, and select the top-ranked predictions as the final detection results for the given input image. A highly performant object detector requires accurate ranking for the bounding box predictions. For DETR-based detectors, the top-ranked bounding boxes suffer from less accurate localization quality due to the misalignment between classification scores and localization accuracy, thus impeding the construction of high-quality detectors. In this work, we introduce a simple and highly performant DETR-based object detector by proposing a series of rank-oriented designs, combinedly called Rank-DETR. Our key contributions include: (i) a rank-oriented architecture design that can prompt positive predictions and suppress the negative ones to ensure lower false positive rates, as well as (ii) a rank-oriented loss function and matching cost design 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#21327;&#21464;&#37327;&#28418;&#31227;&#19979;&#23545;&#19968;&#33324;&#38750;&#21442;&#25968;&#26041;&#27861;&#36827;&#34892;&#32479;&#19968;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25910;&#25947;&#36895;&#24230;&#20026;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.08237</link><description>&lt;p&gt;
&#22312;&#21327;&#21464;&#37327;&#28418;&#31227;&#19979;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#32479;&#19968;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Towards a Unified Analysis of Kernel-based Methods Under Covariate Shift. (arXiv:2310.08237v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08237
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#21327;&#21464;&#37327;&#28418;&#31227;&#19979;&#23545;&#19968;&#33324;&#38750;&#21442;&#25968;&#26041;&#27861;&#36827;&#34892;&#32479;&#19968;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#25910;&#25947;&#36895;&#24230;&#20026;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#21327;&#21464;&#37327;&#28418;&#31227;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#21363;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#36755;&#20837;&#20998;&#24067;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#23613;&#31649;&#22312;&#21508;&#31181;&#23398;&#20064;&#38382;&#39064;&#20013;&#20855;&#26377;&#23454;&#38469;&#37325;&#35201;&#24615;&#65292;&#20294;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#21482;&#20851;&#27880;&#20110;&#19968;&#20123;&#29305;&#23450;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#27809;&#26377;&#22312;&#29702;&#35770;&#19978;&#21644;&#25968;&#20540;&#19978;&#24471;&#21040;&#24456;&#22909;&#30340;&#39564;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#21327;&#21464;&#37327;&#28418;&#31227;&#19979;&#23545;&#19968;&#33324;&#38750;&#21442;&#25968;&#26041;&#27861;&#36827;&#34892;&#32479;&#19968;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36866;&#29992;&#20110;&#23646;&#20110;&#19968;&#20010;&#20016;&#23500;&#30340;&#25439;&#22833;&#20989;&#25968;&#23478;&#26063;&#30340;&#19968;&#33324;&#25439;&#22833;&#65292;&#20854;&#20013;&#21253;&#25324;&#35768;&#22810;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#22914;&#22343;&#20540;&#22238;&#24402;&#12289;&#20998;&#20301;&#25968;&#22238;&#24402;&#12289;&#22522;&#20110;&#20284;&#28982;&#30340;&#20998;&#31867;&#21644;&#22522;&#20110;&#36793;&#32536;&#30340;&#20998;&#31867;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#20004;&#31867;&#21327;&#21464;&#37327;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#20026;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#24314;&#31435;&#20102;&#23574;&#38160;&#30340;&#25910;&#25947;&#36895;&#24230;&#20197;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#35813;&#32467;&#26524;&#19982;&#25991;&#29486;&#20013;&#30340;&#26368;&#20248;&#32467;&#26524;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Covariate shift occurs prevalently in practice, where the input distributions of the source and target data are substantially different. Despite its practical importance in various learning problems, most of the existing methods only focus on some specific learning tasks and are not well validated theoretically and numerically. To tackle this problem, we propose a unified analysis of general nonparametric methods in a reproducing kernel Hilbert space (RKHS) under covariate shift. Our theoretical results are established for a general loss belonging to a rich loss function family, which includes many commonly used methods as special cases, such as mean regression, quantile regression, likelihood-based classification, and margin-based classification. Two types of covariate shift problems are the focus of this paper and the sharp convergence rates are established for a general loss function to provide a unified theoretical analysis, which concurs with the optimal results in literature wher
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#22312;&#29289;&#32852;&#32593;&#35774;&#22791;&#20013;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#30340;&#30828;&#20214;&#36719;&#20214;&#21327;&#21516;&#20248;&#21270;&#65292;&#37325;&#28857;&#32771;&#34385;&#20102;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#29992;&#25143;&#20307;&#39564;&#65292;&#24182;&#30740;&#31350;&#20102;&#37327;&#21270;&#12289;&#27169;&#22411;&#32553;&#25918;&#21644;&#22810;&#27169;&#24577;&#31561;&#26041;&#27861;&#19982;&#31995;&#32479;&#32452;&#20214;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.07940</link><description>&lt;p&gt;
&#25104;&#26412;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#30828;&#20214;&#36719;&#20214;&#21327;&#21516;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Cost-Driven Hardware-Software Co-Optimization of Machine Learning Pipelines. (arXiv:2310.07940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07940
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#22312;&#29289;&#32852;&#32593;&#35774;&#22791;&#20013;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#30340;&#30828;&#20214;&#36719;&#20214;&#21327;&#21516;&#20248;&#21270;&#65292;&#37325;&#28857;&#32771;&#34385;&#20102;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#29992;&#25143;&#20307;&#39564;&#65292;&#24182;&#30740;&#31350;&#20102;&#37327;&#21270;&#12289;&#27169;&#22411;&#32553;&#25918;&#21644;&#22810;&#27169;&#24577;&#31561;&#26041;&#27861;&#19982;&#31995;&#32479;&#32452;&#20214;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#23459;&#25196;&#30528;&#30001;&#29289;&#32852;&#32593;&#35774;&#22791;&#65288;&#21253;&#25324;&#26234;&#33021;&#20256;&#24863;&#22120;&#65292;&#23478;&#23621;&#21644;&#22478;&#24066;&#65289;&#25512;&#21160;&#30340;&#26410;&#26469;&#24895;&#26223;&#12290;&#36234;&#26469;&#36234;&#22810;&#22320;&#65292;&#23558;&#26234;&#33021;&#23884;&#20837;&#36825;&#20123;&#35774;&#22791;&#20013;&#28041;&#21450;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#23384;&#20648;&#21644;&#22788;&#29702;&#38656;&#27714;&#20351;&#23427;&#20204;&#23545;&#20110;&#24265;&#20215;&#30340;&#29616;&#25104;&#24179;&#21488;&#26469;&#35828;&#26159;&#31105;&#27490;&#30340;&#12290;&#20811;&#26381;&#36825;&#20123;&#35201;&#27714;&#23545;&#20110;&#23454;&#29616;&#24191;&#27867;&#36866;&#29992;&#30340;&#26234;&#33021;&#35774;&#22791;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#35768;&#22810;&#20351;&#27169;&#22411;&#21464;&#24471;&#26356;&#23567;&#26356;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#29305;&#23450;&#22330;&#26223;&#26368;&#36866;&#21512;&#30340;&#26041;&#27861;&#32570;&#20047;&#29702;&#35299;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#23545;&#20110;&#36793;&#32536;&#24179;&#21488;&#65292;&#36825;&#20123;&#36873;&#25321;&#19981;&#33021;&#19982;&#25104;&#26412;&#21644;&#29992;&#25143;&#20307;&#39564;&#30456;&#21106;&#31163;&#22320;&#36827;&#34892;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#29992;&#25143;&#20307;&#39564;&#30340;&#35282;&#24230;&#20840;&#38754;&#25506;&#32034;&#20102;&#37327;&#21270;&#12289;&#27169;&#22411;&#32553;&#25918;&#21644;&#22810;&#27169;&#24577;&#19982;&#23384;&#20648;&#12289;&#20256;&#24863;&#22120;&#21644;&#22788;&#29702;&#22120;&#31561;&#31995;&#32479;&#32452;&#20214;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#20174;&#30828;&#20214;/&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#30340;&#35282;&#24230;&#36827;&#34892;&#65292;&#32771;&#34385;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#29992;&#25143;&#20307;&#39564;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have long touted a vision of the future enabled by a proliferation of internet-of-things devices, including smart sensors, homes, and cities. Increasingly, embedding intelligence in such devices involves the use of deep neural networks. However, their storage and processing requirements make them prohibitive for cheap, off-the-shelf platforms. Overcoming those requirements is necessary for enabling widely-applicable smart devices. While many ways of making models smaller and more efficient have been developed, there is a lack of understanding of which ones are best suited for particular scenarios. More importantly for edge platforms, those choices cannot be analyzed in isolation from cost and user experience. In this work, we holistically explore how quantization, model scaling, and multi-modality interact with system components such as memory, sensors, and processors. We perform this hardware/software co-design from the cost, latency, and user-experience perspective, and d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fed-GraB&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#26799;&#24230;&#24179;&#34913;&#22120;&#26469;&#35299;&#20915;&#32852;&#37030;&#24335;&#38271;&#23614;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#21051;&#30011;&#20840;&#23616;&#38271;&#23614;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#26412;&#22320;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#22836;&#37096;-&#23614;&#37096;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07587</link><description>&lt;p&gt;
Fed-GraB&#65306;&#20855;&#26377;&#33258;&#36866;&#24212;&#26799;&#24230;&#24179;&#34913;&#22120;&#30340;&#32852;&#37030;&#24335;&#38271;&#23614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fed-GraB: Federated Long-tailed Learning with Self-Adjusting Gradient Balancer. (arXiv:2310.07587v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fed-GraB&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#26799;&#24230;&#24179;&#34913;&#22120;&#26469;&#35299;&#20915;&#32852;&#37030;&#24335;&#38271;&#23614;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#21051;&#30011;&#20840;&#23616;&#38271;&#23614;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#26412;&#22320;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#22836;&#37096;-&#23614;&#37096;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38544;&#31169;&#21644;&#38271;&#23614;&#20998;&#24067;&#22312;&#35768;&#22810;&#29616;&#23454;&#20219;&#21153;&#20013;&#26159;&#24120;&#24577;&#32780;&#38750;&#20363;&#22806;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#32852;&#37030;&#24335;&#38271;&#23614;&#23398;&#20064;&#65288;Fed-LT&#65289;&#20219;&#21153;&#65292;&#22312;&#35813;&#20219;&#21153;&#20013;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#25345;&#26377;&#19968;&#20010;&#26412;&#22320;&#24322;&#26500;&#25968;&#25454;&#38598;&#65307;&#22914;&#26524;&#21487;&#20197;&#20840;&#23616;&#32858;&#21512;&#25968;&#25454;&#38598;&#65292;&#21017;&#23427;&#20204;&#20849;&#21516;&#23637;&#29616;&#20986;&#38271;&#23614;&#20998;&#24067;&#12290;&#22312;&#36825;&#26679;&#30340;&#35774;&#32622;&#19979;&#65292;&#29616;&#26377;&#30340;&#32852;&#37030;&#20248;&#21270;&#21644;/&#25110;&#38598;&#20013;&#24335;&#38271;&#23614;&#23398;&#20064;&#26041;&#27861;&#24456;&#38590;&#24212;&#29992;&#65292;&#22240;&#20026;&#23384;&#22312;&#20197;&#19979;&#25361;&#25112;&#65306;&#65288;a&#65289;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#21051;&#30011;&#20840;&#23616;&#38271;&#23614;&#20998;&#24067;&#65292;&#20197;&#21450;&#65288;b&#65289;&#35843;&#25972;&#26412;&#22320;&#23398;&#20064;&#31574;&#30053;&#20197;&#24212;&#23545;&#22836;&#37096;-&#23614;&#37096;&#19981;&#24179;&#34913;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;$\texttt{Fed-GraB}$&#65292;&#23427;&#21253;&#25324;&#19968;&#20010;&#33258;&#36866;&#24212;&#26799;&#24230;&#24179;&#34913;&#22120;&#65288;SGB&#65289;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#20197;&#38381;&#29615;&#26041;&#24335;&#26681;&#25454;&#20840;&#23616;&#38271;&#23614;&#20998;&#24067;&#30340;&#21453;&#39304;&#23545;&#23458;&#25143;&#31471;&#30340;&#26799;&#24230;&#36827;&#34892;&#37325;&#26032;&#21152;&#26435;&#65292;&#35780;&#20272;&#26041;&#27861;&#20026;&#30452;&#25509;&#20808;&#39564;&#20998;&#26512;&#22120;&#65288;DPA&#65289;&#27169;&#22359;&#12290;&#20351;&#29992;$\texttt{Fed-GraB}$&#65292;&#23458;&#25143;&#31471;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#22343;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data privacy and long-tailed distribution are the norms rather than the exception in many real-world tasks. This paper investigates a federated long-tailed learning (Fed-LT) task in which each client holds a locally heterogeneous dataset; if the datasets can be globally aggregated, they jointly exhibit a long-tailed distribution. Under such a setting, existing federated optimization and/or centralized long-tailed learning methods hardly apply due to challenges in (a) characterizing the global long-tailed distribution under privacy constraints and (b) adjusting the local learning strategy to cope with the head-tail imbalance. In response, we propose a method termed $\texttt{Fed-GraB}$, comprised of a Self-adjusting Gradient Balancer (SGB) module that re-weights clients' gradients in a closed-loop manner, based on the feedback of global long-tailed distribution evaluated by a Direct Prior Analyzer (DPA) module. Using $\texttt{Fed-GraB}$, clients can effectively alleviate the distribution
&lt;/p&gt;</description></item><item><title>ROMO&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#20248;&#21270;&#24179;&#24248;&#35774;&#35745;&#65292;&#24182;&#20445;&#25345;&#32473;&#23450;&#30340;&#32422;&#26463;&#26469;&#35299;&#20915;&#32422;&#26463;MBO&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07560</link><description>&lt;p&gt;
ROMO: &#26816;&#32034;&#22686;&#24378;&#30340;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
ROMO: Retrieval-enhanced Offline Model-based Optimization. (arXiv:2310.07560v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07560
&lt;/p&gt;
&lt;p&gt;
ROMO&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#20248;&#21270;&#24179;&#24248;&#35774;&#35745;&#65292;&#24182;&#20445;&#25345;&#32473;&#23450;&#30340;&#32422;&#26463;&#26469;&#35299;&#20915;&#32422;&#26463;MBO&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#40657;&#30418;&#27169;&#22411;&#20248;&#21270;&#65288;MBO&#65289;&#38382;&#39064;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#20986;&#29616;&#65292;&#20854;&#30446;&#26631;&#26159;&#22522;&#20110;&#38745;&#24577;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#23547;&#25214;&#20351;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#26368;&#22823;&#21270;&#30340;&#25972;&#20010;&#31354;&#38388;&#19978;&#30340;&#35774;&#35745;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#26356;&#19968;&#33324;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;MBO&#35774;&#32622;&#65292;&#31216;&#20026;&#32422;&#26463;MBO&#65288;CoMBO&#65289;&#65292;&#20854;&#20013;&#21482;&#26377;&#37096;&#20998;&#35774;&#35745;&#31354;&#38388;&#21487;&#20197;&#20248;&#21270;&#65292;&#32780;&#20854;&#20313;&#37096;&#20998;&#21463;&#29615;&#22659;&#32422;&#26463;&#12290;CoMBO&#24102;&#26469;&#30340;&#26032;&#25361;&#25112;&#26159;&#22823;&#22810;&#25968;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#30340;&#35266;&#23519;&#35774;&#35745;&#22312;&#35780;&#20272;&#20013;&#26159;&#24179;&#24248;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#20248;&#21270;&#36825;&#20123;&#24179;&#24248;&#30340;&#35774;&#35745;&#65292;&#21516;&#26102;&#20445;&#25345;&#32473;&#23450;&#30340;&#32422;&#26463;&#65292;&#32780;&#19981;&#26159;&#22312;&#20256;&#32479;&#30340;MBO&#35774;&#32622;&#20013;&#36827;&#19968;&#27493;&#22686;&#24378;&#26368;&#20339;&#35266;&#23519;&#35774;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;&#65288;ROMO&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#21487;&#23548;&#24615;&#21069;&#21521;&#26041;&#27861;&#65292;&#23427;&#26816;&#32034;&#31163;&#32447;&#25968;&#25454;&#38598;&#24182;&#32858;&#21512;&#30456;&#20851;&#26679;&#26412;&#20197;&#25552;&#20379;&#21487;&#38752;&#30340;&#39044;&#27979;&#65292;&#28982;&#21518;&#23558;&#20854;&#24212;&#29992;&#20110;CoMBO&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven black-box model-based optimization (MBO) problems arise in a great number of practical application scenarios, where the goal is to find a design over the whole space maximizing a black-box target function based on a static offline dataset. In this work, we consider a more general but challenging MBO setting, named constrained MBO (CoMBO), where only part of the design space can be optimized while the rest is constrained by the environment. A new challenge arising from CoMBO is that most observed designs that satisfy the constraints are mediocre in evaluation. Therefore, we focus on optimizing these mediocre designs in the offline dataset while maintaining the given constraints rather than further boosting the best observed design in the traditional MBO setting. We propose retrieval-enhanced offline model-based optimization (ROMO), a new derivable forward approach that retrieves the offline dataset and aggregates relevant samples to provide a trusted prediction, and use it f
&lt;/p&gt;</description></item><item><title>KwaiYiiMath&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#30417;&#30563;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#33521;&#35821;&#21644;&#20013;&#25991;&#25968;&#23398;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#35299;&#20915;&#29983;&#25104;&#30340;&#38382;&#39064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.07488</link><description>&lt;p&gt;
KwaiYiiMath: &#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
KwaiYiiMath: Technical Report. (arXiv:2310.07488v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07488
&lt;/p&gt;
&lt;p&gt;
KwaiYiiMath&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#30417;&#30563;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#33521;&#35821;&#21644;&#20013;&#25991;&#25968;&#23398;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#35299;&#20915;&#29983;&#25104;&#30340;&#38382;&#39064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#19979;&#28216;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#29978;&#33267;&#21487;&#20197;&#22788;&#29702;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#25968;&#23398;&#20219;&#21153;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;KwaiYiiMath&#65292;&#36890;&#36807;&#24212;&#29992;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22686;&#24378;&#20102;KwaiYiiBase1&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#21253;&#25324;&#33521;&#35821;&#21644;&#20013;&#25991;&#30340;&#25968;&#23398;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#23567;&#35268;&#27169;&#30340;&#20013;&#23567;&#23398;&#25968;&#23398;&#27979;&#35797;&#38598;&#65288;&#21629;&#21517;&#20026;KMath&#65289;&#65292;&#21253;&#21547;188&#20010;&#20363;&#23376;&#65292;&#29992;&#26469;&#35780;&#20272;&#27169;&#22411;&#29983;&#25104;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#30340;&#27491;&#30830;&#24615;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#31867;&#20284;&#35268;&#27169;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;KwaiYiiMath&#22312;GSM8k&#12289;CMath&#21644;KMath&#19978;&#22343;&#33021;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65288;SOTA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have demonstrated remarkable abilities in handling a variety of natural language processing (NLP) downstream tasks, even on mathematical tasks requiring multi-step reasoning. In this report, we introduce the KwaiYiiMath which enhances the mathematical reasoning abilities of KwaiYiiBase1, by applying Supervised Fine-Tuning (SFT) and Reinforced Learning from Human Feedback (RLHF), including on both English and Chinese mathematical tasks. Meanwhile, we also constructed a small-scale Chinese primary school mathematics test set (named KMath), consisting of 188 examples to evaluate the correctness of the problem-solving process generated by the models. Empirical studies demonstrate that KwaiYiiMath can achieve state-of-the-art (SOTA) performance on GSM8k, CMath, and KMath compared with the similar size models, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#65288;RNN LMs&#65289;&#20316;&#20026;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#21482;&#33021;&#34920;&#31034;&#26377;&#38480;&#29366;&#24577;&#27169;&#22411;&#25152;&#33021;&#34920;&#36798;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#19968;&#20010;&#20005;&#26684;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.05161</link><description>&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Recurrent Neural Language Models as Probabilistic Finite-state Automata. (arXiv:2310.05161v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#65288;RNN LMs&#65289;&#20316;&#20026;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#21482;&#33021;&#34920;&#31034;&#26377;&#38480;&#29366;&#24577;&#27169;&#22411;&#25152;&#33021;&#34920;&#36798;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#19968;&#20010;&#20005;&#26684;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20197;&#23481;&#26131;&#29702;&#35299;&#30340;&#24418;&#24335;&#26469;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21487;&#20197;&#20351;&#25105;&#20204;&#31934;&#30830;&#22320;&#25551;&#36848;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#32771;&#23519;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#26080;&#26435;&#37325;&#24418;&#24335;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LMs&#24182;&#19981;&#25551;&#36848;&#26080;&#26435;&#37325;&#24418;&#24335;&#35821;&#35328;&#65292;&#32780;&#26159;&#23450;&#20041;&#20102;&#23545;&#23383;&#31526;&#20018;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;RNN LMs&#21487;&#20197;&#34920;&#31034;&#21738;&#20123;&#31867;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#26356;&#30452;&#25509;&#22320;&#38472;&#36848;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;RNN&#31561;&#20215;&#20110;&#27010;&#29575;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#19968;&#20010;&#23376;&#31867;&#65292;&#22240;&#27492;&#21482;&#33021;&#27169;&#25311;&#26377;&#38480;&#29366;&#24577;&#27169;&#22411;&#25152;&#33021;&#34920;&#36798;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#19968;&#20010;&#20005;&#26684;&#23376;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;RNNs&#34920;&#31034;&#26377;&#38480;&#29366;&#24577;LMs&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#20026;&#20102;&#34920;&#31034;&#19968;&#20010;&#20219;&#24847;&#30830;&#23450;&#30340;&#26377;&#38480;&#29366;&#24577;LMs&#65292;&#20854;&#20013;&#26377;$N$&#20010;&#29366;&#24577;&#19988;&#23383;&#31526;&#38598;&#20026;$\Sigma$&#30340;RNN requir
&lt;/p&gt;
&lt;p&gt;
Studying language models (LMs) in terms of well-understood formalisms allows us to precisely characterize their abilities and limitations. Previous work has investigated the representational capacity of recurrent neural network (RNN) LMs in terms of their capacity to recognize unweighted formal languages. However, LMs do not describe unweighted formal languages -- rather, they define probability distributions over strings. In this work, we study what classes of such probability distributions RNN LMs can represent, which allows us to make more direct statements about their capabilities. We show that simple RNNs are equivalent to a subclass of probabilistic finite-state automata, and can thus model a strict subset of probability distributions expressible by finite-state models. Furthermore, we study the space complexity of representing finite-state LMs with RNNs. We show that, to represent an arbitrary deterministic finite-state LM with $N$ states over an alphabet $\Sigma$, an RNN requir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20316;&#20026;&#23458;&#25143;&#31471;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35774;&#22791;&#24341;&#21457;&#30340;&#25968;&#23383;&#20262;&#29702;&#38382;&#39064;&#12290;&#30637;&#35299;&#20102;&#20854;&#20013;&#28041;&#21450;&#30340;&#28216;&#25103;&#21160;&#24577;&#12289;&#20844;&#24179;&#24615;&#12289;&#22870;&#21169;&#26426;&#21046;&#21644;&#36830;&#36143;&#24615;&#31561;&#25361;&#25112;&#65292;&#24182;&#25506;&#35752;&#20102;&#35299;&#20915;&#26041;&#26696;&#21644;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#29289;&#32852;&#32593;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26426;&#36935;&#12290;</title><link>http://arxiv.org/abs/2310.03178</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#23383;&#20262;&#29702;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Digital Ethics in Federated Learning. (arXiv:2310.03178v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20316;&#20026;&#23458;&#25143;&#31471;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35774;&#22791;&#24341;&#21457;&#30340;&#25968;&#23383;&#20262;&#29702;&#38382;&#39064;&#12290;&#30637;&#35299;&#20102;&#20854;&#20013;&#28041;&#21450;&#30340;&#28216;&#25103;&#21160;&#24577;&#12289;&#20844;&#24179;&#24615;&#12289;&#22870;&#21169;&#26426;&#21046;&#21644;&#36830;&#36143;&#24615;&#31561;&#25361;&#25112;&#65292;&#24182;&#25506;&#35752;&#20102;&#35299;&#20915;&#26041;&#26696;&#21644;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#29289;&#32852;&#32593;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#19981;&#26029;&#20135;&#29983;&#22823;&#37327;&#25968;&#25454;&#65292;&#24341;&#21457;&#20102;&#23545;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#21644;&#25968;&#25454;&#28389;&#29992;&#38480;&#21046;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#20849;&#20139;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#21442;&#25968;&#32780;&#19981;&#26159;&#21407;&#22987;&#29992;&#25143;&#25968;&#25454;&#65292;&#20419;&#36827;&#20102;&#22810;&#26041;&#20043;&#38388;&#30340;&#21327;&#20316;&#33021;&#21147;&#65292;&#24182;&#22240;&#20854;&#22312;&#38544;&#31169;&#20445;&#25252;&#21644;&#23398;&#20064;&#25928;&#29575;&#25552;&#21319;&#26041;&#38754;&#30340;&#28508;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#37325;&#28857;&#35752;&#35770;&#20102;&#22312;FL&#20013;&#20316;&#20026;&#23458;&#25143;&#31471;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35774;&#22791;&#24341;&#21457;&#30340;&#25968;&#23383;&#20262;&#29702;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#30340;&#35266;&#28857;&#21644;&#30446;&#26631;&#30340;&#24773;&#20917;&#19979;&#65292;FL&#38754;&#20020;&#30528;&#28216;&#25103;&#21160;&#24577;&#12289;&#20844;&#24179;&#24615;&#12289;&#22870;&#21169;&#26426;&#21046;&#21644;&#36830;&#36143;&#24615;&#31561;&#25361;&#25112;&#12290;&#25105;&#20204;&#20174;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#30340;&#35282;&#24230;&#65292;&#20197;&#21450;&#38598;&#20013;&#24335;&#21644;&#20998;&#25955;&#24335;FL&#30340;&#35270;&#35282;&#20998;&#26512;&#20102;&#36825;&#20123;&#25361;&#25112;&#21450;&#20854;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#29289;&#32852;&#32593;&#22312;FL&#20013;&#30340;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Internet of Things (IoT) consistently generates vast amounts of data, sparking increasing concern over the protection of data privacy and the limitation of data misuse. Federated learning (FL) facilitates collaborative capabilities among multiple parties by sharing machine learning (ML) model parameters instead of raw user data, and it has recently gained significant attention for its potential in privacy preservation and learning efficiency enhancement. In this paper, we highlight the digital ethics concerns that arise when human-centric devices serve as clients in FL. More specifically, challenges of game dynamics, fairness, incentive, and continuity arise in FL due to differences in perspectives and objectives between clients and the server. We analyze these challenges and their solutions from the perspectives of both the client and the server, and through the viewpoints of centralized and decentralized FL. Finally, we explore the opportunities in FL for human-centric IoT as dir
&lt;/p&gt;</description></item><item><title>zkFL&#26159;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27599;&#36718;&#30340;&#35777;&#26126;&#26469;&#35299;&#20915;&#21327;&#35843;&#32773;&#24694;&#24847;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02554</link><description>&lt;p&gt;
zkFL: &#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning. (arXiv:2310.02554v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02554
&lt;/p&gt;
&lt;p&gt;
zkFL&#26159;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27599;&#36718;&#30340;&#35777;&#26126;&#26469;&#35299;&#20915;&#21327;&#35843;&#32773;&#24694;&#24847;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20351;&#22810;&#20010;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#22312;&#20013;&#22830;&#21327;&#35843;&#32773;&#30340;&#32452;&#32455;&#19979;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#23545;&#20013;&#22830;&#21327;&#35843;&#32773;&#30340;&#20449;&#20219;&#65292;&#23427;&#20197;&#20844;&#24179;&#35802;&#23454;&#30340;&#26041;&#24335;&#24418;&#25104;&#23458;&#25143;&#31471;&#30340;&#32676;&#20307;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#24694;&#24847;&#30340;&#21327;&#35843;&#32773;&#21487;&#33021;&#20250;&#25918;&#24323;&#24182;&#26367;&#25442;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#25110;&#32773;&#21457;&#21160;&#34394;&#20551;&#23458;&#25143;&#31471;&#30340;&#32902;&#24847;&#25915;&#20987;&#12290;&#36825;&#31181;&#24694;&#24847;&#34892;&#20026;&#35753;&#21327;&#35843;&#32773;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#25317;&#26377;&#26356;&#22810;&#25511;&#21046;&#23458;&#25143;&#31471;&#21644;&#20915;&#23450;&#26368;&#32456;&#35757;&#32451;&#32467;&#26524;&#30340;&#26435;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;zkFL&#65292;&#23427;&#21033;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;(ZKPs)&#26469;&#35299;&#20915;&#35757;&#32451;&#27169;&#22411;&#32858;&#21512;&#36807;&#31243;&#20013;&#30340;&#24694;&#24847;&#21327;&#35843;&#32773;&#38382;&#39064;&#12290;&#20026;&#20102;&#20445;&#35777;&#27491;&#30830;&#30340;&#32858;&#21512;&#32467;&#26524;&#65292;&#21327;&#35843;&#32773;&#38656;&#35201;&#27599;&#36718;&#25552;&#20379;&#19968;&#20010;&#35777;&#26126;&#12290;&#36825;&#20010;&#35777;&#26126;&#21487;&#20197;&#21521;&#23458;&#25143;&#31471;&#35777;&#26126;&#21327;&#35843;&#32773;&#24544;&#23454;&#25191;&#34892;&#39044;&#26399;&#34892;&#20026;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20445;&#25252;&#23458;&#25143;&#31471;&#38544;&#31169;&#21644;&#25968;&#25454;&#23433;&#20840;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#24182;&#23545;zkFL&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning paradigm, which enables multiple and decentralized clients to collaboratively train a model under the orchestration of a central aggregator. Traditional FL solutions rely on the trust assumption of the centralized aggregator, which forms cohorts of clients in a fair and honest manner. However, a malicious aggregator, in reality, could abandon and replace the client's training models, or launch Sybil attacks to insert fake clients. Such malicious behaviors give the aggregator more power to control clients in the FL setting and determine the final training results. In this work, we introduce zkFL, which leverages zero-knowledge proofs (ZKPs) to tackle the issue of a malicious aggregator during the training model aggregation process. To guarantee the correct aggregation results, the aggregator needs to provide a proof per round. The proof can demonstrate to the clients that the aggregator executes the intended behavior faithfully. To further r
&lt;/p&gt;</description></item><item><title>SNIP&#24341;&#20837;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#23545;&#27604;&#23398;&#20064;&#21152;&#24378;&#20102;&#31526;&#21495;&#21644;&#25968;&#20540;&#39046;&#22495;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#36328;&#39046;&#22495;&#30340;&#34920;&#31034;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02227</link><description>&lt;p&gt;
SNIP: &#29992;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#36830;&#25509;&#25968;&#23398;&#31526;&#21495;&#21644;&#25968;&#20540;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training. (arXiv:2310.02227v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02227
&lt;/p&gt;
&lt;p&gt;
SNIP&#24341;&#20837;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#23545;&#27604;&#23398;&#20064;&#21152;&#24378;&#20102;&#31526;&#21495;&#21644;&#25968;&#20540;&#39046;&#22495;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#36328;&#39046;&#22495;&#30340;&#34920;&#31034;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#26080;&#27861;&#32570;&#23569;&#31526;&#21495;&#25968;&#23398;&#26041;&#31243;&#26469;&#24314;&#27169;&#22797;&#26434;&#33258;&#28982;&#29616;&#35937;&#30340;&#26102;&#20195;&#65292;&#31185;&#23398;&#25506;&#31350;&#24448;&#24448;&#28041;&#21450;&#21040;&#25910;&#38598;&#35266;&#23519;&#25968;&#25454;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#25968;&#23398;&#34920;&#36798;&#24335;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#27934;&#23519;&#21147;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#29305;&#21270;&#20110;&#25968;&#20540;&#39046;&#22495;&#25110;&#31526;&#21495;&#39046;&#22495;&#65292;&#24182;&#19988;&#36890;&#24120;&#22312;&#20026;&#29305;&#23450;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#30417;&#30563;&#24335;&#35757;&#32451;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#24573;&#35270;&#20102;&#31526;&#21495;&#26041;&#31243;&#21644;&#20854;&#25968;&#20540;&#23545;&#24212;&#29289;&#20043;&#38388;&#21487;&#33021;&#20135;&#29983;&#30340;&#37325;&#22823;&#22909;&#22788;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#31181;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SNIP&#65292;&#19968;&#31181;&#31526;&#21495;-&#25968;&#20540;&#38598;&#25104;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#31526;&#21495;&#21644;&#25968;&#20540;&#39046;&#22495;&#20043;&#38388;&#36827;&#34892;&#32852;&#21512;&#23545;&#27604;&#23398;&#20064;&#65292;&#22686;&#24378;&#20102;&#23427;&#20204;&#22312;&#39044;&#35757;&#32451;&#23884;&#20837;&#20013;&#30340;&#30456;&#20114;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#36827;&#34892;&#28508;&#31354;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;SNIP&#25552;&#20379;&#20102;&#36328;&#39046;&#22495;&#30340;&#34920;&#31034;&#27934;&#23519;&#21147;&#65292;&#25581;&#31034;&#20102;&#31526;&#21495;&#21644;&#25968;&#20540;&#20043;&#38388;&#30340;&#20851;&#32852;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an era where symbolic mathematical equations are indispensable for modeling complex natural phenomena, scientific inquiry often involves collecting observations and translating them into mathematical expressions. Recently, deep learning has emerged as a powerful tool for extracting insights from data. However, existing models typically specialize in either numeric or symbolic domains, and are usually trained in a supervised manner tailored to specific tasks. This approach neglects the substantial benefits that could arise from a task-agnostic unified understanding between symbolic equations and their numeric counterparts. To bridge the gap, we introduce SNIP, a Symbolic-Numeric Integrated Pre-training, which employs joint contrastive learning between symbolic and numeric domains, enhancing their mutual similarities in the pre-trained embeddings. By performing latent space analysis, we observe that SNIP provides cross-domain insights into the representations, revealing that symbolic 
&lt;/p&gt;</description></item><item><title>OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02031</link><description>&lt;p&gt;
OceanGPT&#65306;&#29992;&#20110;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02031
&lt;/p&gt;
&lt;p&gt;
OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#31185;&#23398;&#26159;&#25506;&#32034;&#20805;&#28385;&#29983;&#21629;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#30340;&#28023;&#27915;&#30340;&#31185;&#23398;&#65292;&#32771;&#34385;&#21040;&#28023;&#27915;&#35206;&#30422;&#20102;&#22320;&#29699;&#34920;&#38754;&#30340;70&#65285;&#20197;&#19978;&#65292;&#36825;&#19968;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#25913;&#21464;&#20102;&#31185;&#23398;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;&#22312;&#20854;&#20182;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;LLM&#36890;&#24120;&#26080;&#27861;&#28385;&#36275;&#28023;&#27915;&#23398;&#23478;&#31561;&#39046;&#22495;&#19987;&#23478;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#23545;LLM&#22312;&#28023;&#27915;&#31185;&#23398;&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#36825;&#20854;&#20013;&#30340;&#26681;&#26412;&#21407;&#22240;&#21487;&#33021;&#26159;&#28023;&#27915;&#25968;&#25454;&#30340;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#24615;&#36136;&#65292;&#20197;&#21450;&#23545;&#26356;&#39640;&#30340;&#31890;&#24230;&#21644;&#20016;&#23500;&#30340;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#39318;&#20010;&#28023;&#27915;&#39046;&#22495;&#30340;LLM&#8212;&#8212;OceanGPT&#65292;&#35813;&#27169;&#22411;&#25797;&#38271;&#21508;&#31181;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;DoInstruct&#65292;&#29992;&#20110;&#33258;&#21160;&#33719;&#21462;&#22823;&#37327;&#30340;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#65292;&#23427;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#29983;&#25104;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36866;&#29992;&#20110;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#65292;&#21487;&#20197;&#21253;&#25324;&#20855;&#26377;&#20559;&#24046;&#12289;&#36339;&#36291;&#36830;&#25509;&#21644;&#26368;&#22823;&#27744;&#21270;&#30340;&#36890;&#29992;DAG ReLU&#32593;&#32476;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#24674;&#22797;&#25110;&#36229;&#36234;&#20102;&#24050;&#30693;&#30340;&#36335;&#24452;&#33539;&#25968;&#30028;&#38480;&#65292;&#24182;&#25361;&#25112;&#20102;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#19968;&#20123;&#20855;&#20307;&#25215;&#35834;&#12290;</title><link>http://arxiv.org/abs/2310.01225</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#20195;&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#65306;&#24433;&#21709;&#12289;&#21069;&#26223;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A path-norm toolkit for modern networks: consequences, promises and challenges. (arXiv:2310.01225v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36866;&#29992;&#20110;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#65292;&#21487;&#20197;&#21253;&#25324;&#20855;&#26377;&#20559;&#24046;&#12289;&#36339;&#36291;&#36830;&#25509;&#21644;&#26368;&#22823;&#27744;&#21270;&#30340;&#36890;&#29992;DAG ReLU&#32593;&#32476;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#24674;&#22797;&#25110;&#36229;&#36234;&#20102;&#24050;&#30693;&#30340;&#36335;&#24452;&#33539;&#25968;&#30028;&#38480;&#65292;&#24182;&#25361;&#25112;&#20102;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#19968;&#20123;&#20855;&#20307;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#23436;&#20840;&#33021;&#22815;&#21253;&#25324;&#20855;&#26377;&#20559;&#24046;&#12289;&#36339;&#36291;&#36830;&#25509;&#21644;&#26368;&#22823;&#27744;&#21270;&#30340;&#36890;&#29992;DAG ReLU&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#19981;&#20165;&#36866;&#29992;&#20110;&#26368;&#24191;&#27867;&#30340;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#65292;&#36824;&#21487;&#20197;&#24674;&#22797;&#25110;&#36229;&#36234;&#24050;&#30693;&#30340;&#27492;&#31867;&#33539;&#25968;&#30340;&#26368;&#23574;&#38160;&#30028;&#38480;&#12290;&#36825;&#20123;&#25193;&#23637;&#30340;&#36335;&#24452;&#33539;&#25968;&#36824;&#20139;&#26377;&#36335;&#24452;&#33539;&#25968;&#30340;&#24120;&#35268;&#20248;&#28857;&#65306;&#35745;&#31639;&#31616;&#20415;&#12289;&#23545;&#32593;&#32476;&#30340;&#23545;&#31216;&#24615;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#22312;&#21069;&#39304;&#32593;&#32476;&#19978;&#27604;&#25805;&#20316;&#31526;&#33539;&#25968;&#30340;&#20056;&#31215;&#65288;&#21478;&#19968;&#31181;&#24120;&#29992;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65289;&#20855;&#26377;&#26356;&#22909;&#30340;&#38160;&#24230;&#12290;&#24037;&#20855;&#21253;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#26131;&#20110;&#23454;&#26045;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#25968;&#20540;&#35780;&#20272;&#22312;ImageNet&#19978;&#23545;ResNet&#30340;&#26368;&#23574;&#38160;&#30028;&#38480;&#26469;&#25361;&#25112;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#20855;&#20307;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces the first toolkit around path-norms that is fully able to encompass general DAG ReLU networks with biases, skip connections and max pooling. This toolkit notably allows us to establish generalization bounds for real modern neural networks that are not only the most widely applicable path-norm based ones, but also recover or beat the sharpest known bounds of this type. These extended path-norms further enjoy the usual benefits of path-norms: ease of computation, invariance under the symmetries of the network, and improved sharpness on feedforward networks compared to the product of operators' norms, another complexity measure most commonly used.  The versatility of the toolkit and its ease of implementation allow us to challenge the concrete promises of path-norm-based generalization bounds, by numerically evaluating the sharpest known bounds for ResNets on ImageNet.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12871</link><description>&lt;p&gt;
&#35282;&#24230;&#20248;&#21270;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#20110;&#25552;&#21319;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#21448;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#26222;&#36941;&#25361;&#25112;&#26159;&#28176;&#21464;&#28040;&#22833;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#22312;&#20248;&#21270;&#30446;&#26631;&#20013;&#20381;&#36182;&#20313;&#24358;&#20989;&#25968;&#65292;&#32780;&#20313;&#24358;&#20989;&#25968;&#20855;&#26377;&#39281;&#21644;&#21306;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AnglE&#30340;&#26032;&#22411;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#12290;AnglE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#20135;&#29983;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#38459;&#30861;&#26799;&#24230;&#24182;&#38459;&#30861;&#20248;&#21270;&#36807;&#31243;&#12290;&#20026;&#20102;&#24314;&#31435;&#20840;&#38754;&#30340;STS&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#30701;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#21644;&#20174;GitHub Issues&#20013;&#26032;&#25910;&#38598;&#30340;&#38271;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;AnglE&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;&#20316;&#20026;&#29305;&#24449;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26367;&#20195;&#32447;&#24615;&#26368;&#23567;&#26368;&#22823;&#32553;&#25918;&#21644;&#20998;&#20301;&#25968;&#36716;&#25442;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#36229;&#21442;&#25968;&#20248;&#21270;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32479;&#35745;&#25968;&#25454;&#20998;&#26512;&#20013;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.10194</link><description>&lt;p&gt;
&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
The Kernel Density Integral Transformation. (arXiv:2309.10194v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;&#20316;&#20026;&#29305;&#24449;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26367;&#20195;&#32447;&#24615;&#26368;&#23567;&#26368;&#22823;&#32553;&#25918;&#21644;&#20998;&#20301;&#25968;&#36716;&#25442;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#36229;&#21442;&#25968;&#20248;&#21270;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#32479;&#35745;&#25968;&#25454;&#20998;&#26512;&#20013;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#26041;&#27861;&#20110;&#34920;&#26684;&#25968;&#25454;&#26102;&#65292;&#29305;&#24449;&#39044;&#22788;&#29702;&#32487;&#32493;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;&#20316;&#20026;&#29305;&#24449;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32508;&#21512;&#20102;&#20004;&#31181;&#20027;&#35201;&#30340;&#29305;&#24449;&#39044;&#22788;&#29702;&#26041;&#27861;&#20316;&#20026;&#26497;&#38480;&#24773;&#20917;&#65306;&#32447;&#24615;&#26368;&#23567;&#26368;&#22823;&#32553;&#25918;&#21644;&#20998;&#20301;&#25968;&#36716;&#25442;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#22312;&#19981;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#26680;&#23494;&#24230;&#31215;&#20998;&#36716;&#25442;&#21487;&#20197;&#20316;&#20026;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#31616;&#21333;&#26367;&#20195;&#26041;&#27861;&#65292;&#23545;&#27599;&#31181;&#26041;&#27861;&#30340;&#24369;&#28857;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#21478;&#22806;&#65292;&#36890;&#36807;&#35843;&#25972;&#19968;&#20010;&#36830;&#32493;&#36229;&#21442;&#25968;&#65292;&#25105;&#20204;&#32463;&#24120;&#20248;&#20110;&#36825;&#20004;&#31181;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#26680;&#23494;&#24230;&#36716;&#25442;&#21487;&#20197;&#26377;&#30410;&#22320;&#24212;&#29992;&#20110;&#32479;&#35745;&#25968;&#25454;&#20998;&#26512;&#65292;&#29305;&#21035;&#26159;&#22312;&#30456;&#20851;&#24615;&#20998;&#26512;&#21644;&#21333;&#21464;&#37327;&#32858;&#31867;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature preprocessing continues to play a critical role when applying machine learning and statistical methods to tabular data. In this paper, we propose the use of the kernel density integral transformation as a feature preprocessing step. Our approach subsumes the two leading feature preprocessing methods as limiting cases: linear min-max scaling and quantile transformation. We demonstrate that, without hyperparameter tuning, the kernel density integral transformation can be used as a simple drop-in replacement for either method, offering robustness to the weaknesses of each. Alternatively, with tuning of a single continuous hyperparameter, we frequently outperform both of these methods. Finally, we show that the kernel density transformation can be profitably applied to statistical data analysis, particularly in correlation analysis and univariate clustering.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#35774;&#35745;&#20855;&#26377;&#22797;&#26434;&#30446;&#26631;&#26426;&#26800;&#24615;&#33021;&#32452;&#21512;&#30340;&#26032;&#22411;&#34584;&#34523;&#19997;&#34507;&#30333;&#24207;&#21015;&#12290;&#36890;&#36807;BLAST&#25628;&#32034;&#12289;&#24615;&#33021;&#35780;&#20272;&#12289;&#20998;&#23376;&#32467;&#26500;&#27604;&#36739;&#21644;&#24207;&#21015;&#22522;&#24207;&#20998;&#26512;&#31561;&#26041;&#24335;&#23545;&#27169;&#22411;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.10170</link><description>&lt;p&gt;
&#29983;&#25104;&#24314;&#27169;&#65292;&#35774;&#35745;&#21644;&#20998;&#26512;&#34584;&#34523;&#19997;&#34507;&#30333;&#24207;&#21015;&#20197;&#25552;&#39640;&#26426;&#26800;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Generative modeling, design and analysis of spider silk protein sequences for enhanced mechanical properties. (arXiv:2309.10170v1 [cond-mat.mtrl-sci] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10170
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#35774;&#35745;&#20855;&#26377;&#22797;&#26434;&#30446;&#26631;&#26426;&#26800;&#24615;&#33021;&#32452;&#21512;&#30340;&#26032;&#22411;&#34584;&#34523;&#19997;&#34507;&#30333;&#24207;&#21015;&#12290;&#36890;&#36807;BLAST&#25628;&#32034;&#12289;&#24615;&#33021;&#35780;&#20272;&#12289;&#20998;&#23376;&#32467;&#26500;&#27604;&#36739;&#21644;&#24207;&#21015;&#22522;&#24207;&#20998;&#26512;&#31561;&#26041;&#24335;&#23545;&#27169;&#22411;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34584;&#34523;&#19997;&#26159;&#19968;&#31181;&#25317;&#26377;&#20986;&#33394;&#30340;&#26426;&#26800;&#24615;&#33021;&#65288;&#22914;&#24378;&#24230;&#65292;&#24310;&#23637;&#24615;&#21644;&#36731;&#37327;&#21270;&#65289;&#30340;&#26448;&#26009;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20165;&#26377;&#26377;&#38480;&#30340;&#27169;&#22411;&#21487;&#29992;&#20110;&#23436;&#20840;&#25506;&#32034;&#24207;&#21015;-&#24615;&#33021;&#20851;&#31995;&#20197;&#36827;&#34892;&#20998;&#26512;&#21644;&#35774;&#35745;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#35774;&#35745;&#26032;&#30340;&#34584;&#34523;&#19997;&#34507;&#30333;&#24207;&#21015;&#20197;&#28385;&#36275;&#22797;&#26434;&#30340;&#30446;&#26631;&#26426;&#26800;&#24615;&#33021;&#32452;&#21512;&#12290;&#35813;&#27169;&#22411;&#22312;&#22823;&#37327;&#34507;&#30333;&#24207;&#21015;&#30340;&#39044;&#35757;&#32451;&#22522;&#30784;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#38024;&#23545;&#32422;1,000&#20010;&#20027;&#35201;&#27873;&#33146;&#19997;&#34507;&#30333;&#65288;MaSp&#65289;&#24207;&#21015;&#36827;&#34892;&#20102;&#27491;&#21521;&#21644;&#36870;&#21521;&#29983;&#25104;&#31574;&#30053;&#30340;&#31471;&#21040;&#31471;&#35774;&#35745;&#12290;&#36890;&#36807;&#20197;&#19979;&#35780;&#20272;&#24615;&#33021;&#65306;&#65288;1&#65289;&#36890;&#36807;BLAST&#25628;&#32034;&#23545;&#29983;&#25104;&#30340;&#34584;&#34523;&#19997;&#34507;&#30333;&#24207;&#21015;&#36827;&#34892;&#26032;&#39062;&#24615;&#20998;&#26512;&#21644;&#34507;&#30333;&#31867;&#22411;&#20998;&#31867;&#65292;&#65288;2&#65289;&#23545;&#27604;&#31867;&#20284;&#24207;&#21015;&#30340;&#24615;&#33021;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#65288;3&#65289;&#20998;&#23376;&#32467;&#26500;&#27604;&#36739;&#65292;&#20197;&#21450;&#65288;4&#65289;&#35814;&#32454;&#30340;&#24207;&#21015;&#22522;&#24207;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spider silks are remarkable materials characterized by superb mechanical properties such as strength, extensibility and lightweightedness. Yet, to date, limited models are available to fully explore sequence-property relationships for analysis and design. Here we propose a custom generative large-language model to enable design of novel spider silk protein sequences to meet complex combinations of target mechanical properties. The model, pretrained on a large set of protein sequences, is fine-tuned on ~1,000 major ampullate spidroin (MaSp) sequences for which associated fiber-level mechanical properties exist, to yield an end-to-end forward and inverse generative strategy. Performance is assessed through: (1), a novelty analysis and protein type classification for generated spidroin sequences through BLAST searches, (2) property evaluation and comparison with similar sequences, (3) comparison of molecular structures, as well as, and (4) a detailed sequence motif analyses. We generate s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CONFLATOR&#65306;&#19968;&#31181;&#38024;&#23545;&#20195;&#30721;&#28151;&#21512;&#35821;&#35328;&#30340;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26059;&#36716;&#20301;&#32622;&#32534;&#30721;&#21644;&#20999;&#25442;&#28857;&#20449;&#24687;&#65292;&#22312;&#28151;&#21512;&#35821;&#35328;&#24314;&#27169;&#20013;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.05270</link><description>&lt;p&gt;
CONFLATOR:&#23558;&#22522;&#20110;&#20999;&#25442;&#28857;&#30340;&#26059;&#36716;&#20301;&#32622;&#32534;&#30721;&#32435;&#20837;&#28151;&#21512;&#35821;&#35328;&#24314;&#27169;&#20013;
&lt;/p&gt;
&lt;p&gt;
CONFLATOR: Incorporating Switching Point based Rotatory Positional Encodings for Code-Mixed Language Modeling. (arXiv:2309.05270v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CONFLATOR&#65306;&#19968;&#31181;&#38024;&#23545;&#20195;&#30721;&#28151;&#21512;&#35821;&#35328;&#30340;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26059;&#36716;&#20301;&#32622;&#32534;&#30721;&#21644;&#20999;&#25442;&#28857;&#20449;&#24687;&#65292;&#22312;&#28151;&#21512;&#35821;&#35328;&#24314;&#27169;&#20013;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#31181;&#25110;&#22810;&#31181;&#35821;&#35328;&#30340;&#28151;&#21512;&#31216;&#20026;&#20195;&#30721;&#28151;&#21512;&#65288;CM&#65289;&#12290; CM&#26159;&#22810;&#35821;&#35328;&#31038;&#20250;&#30340;&#31038;&#20250;&#35268;&#33539;&#12290;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65288;NLMs&#65289;&#65288;&#22914;&#21464;&#21387;&#22120;&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;CM&#30340;NLM&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#23613;&#31649;&#21464;&#21387;&#22120;&#20855;&#26377;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#26159;&#38750;&#36882;&#24402;&#30340;&#65292;&#23427;&#20204;&#19981;&#33021;&#22987;&#32456;&#32534;&#30721;&#20301;&#32622;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20016;&#23500;&#35789;&#30340;&#20449;&#24687;&#24182;&#32435;&#20837;&#20301;&#32622;&#20449;&#24687;&#65292;&#23450;&#20041;&#20102;&#20301;&#32622;&#32534;&#30721;&#12290;&#25105;&#20204;&#20551;&#35774;&#36716;&#25442;&#28857;&#65288;SPs&#65289;&#65292;&#21363;&#35821;&#35328;&#20999;&#25442;&#30340;&#25991;&#26412;&#20013;&#30340;&#20132;&#27719;&#28857;&#65288;L1-&gt; L2&#25110;L2-&gt; L1&#65289;&#65292;&#23545;CM&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26500;&#25104;&#25361;&#25112;&#65292;&#24182;&#23545;&#24314;&#27169;&#36807;&#31243;&#20013;SPs&#32473;&#20104;&#29305;&#21035;&#37325;&#35270;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#20960;&#31181;&#20301;&#32622;&#32534;&#30721;&#26426;&#21046;&#65292;&#24182;&#34920;&#26126;&#26059;&#36716;&#20301;&#32622;&#32534;&#30721;&#20197;&#21450;&#20999;&#25442;&#28857;&#20449;&#24687;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#25105;&#20204;&#24341;&#20837;CONFLATOR&#65306;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#30340;&#31070;&#32463;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mixing of two or more languages is called Code-Mixing (CM). CM is a social norm in multilingual societies. Neural Language Models (NLMs) like transformers have been effective on many NLP tasks. However, NLM for CM is an under-explored area. Though transformers are capable and powerful, they cannot always encode positional information since they are non-recurrent. Therefore, to enrich word information and incorporate positional information, positional encoding is defined. We hypothesize that Switching Points (SPs), i.e., junctions in the text where the language switches (L1 -&gt; L2 or L2 -&gt; L1), pose a challenge for CM Language Models (LMs), and hence give special emphasis to SPs in the modeling process. We experiment with several positional encoding mechanisms and show that rotatory positional encodings along with switching point information yield the best results.  We introduce CONFLATOR: a neural language modeling approach for code-mixed languages. CONFLATOR tries to learn to empha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;OTFS&#19982;OFDM&#36866;&#24212;&#24615;&#35843;&#21046;&#35299;&#35843;&#22120;&#65292;&#36890;&#36807;&#35266;&#23519;&#20449;&#36947;&#29366;&#24577;&#12289;&#25509;&#25910;&#20449;&#22122;&#27604;&#21644;&#35843;&#21046;&#26684;&#24335;&#65292;&#22312;&#21457;&#36865;&#31471;&#21644;&#25509;&#25910;&#31471;&#20043;&#38388;&#20999;&#25442;OTFS&#25110;OFDM&#20449;&#21495;&#22788;&#29702;&#38142;&#20197;&#33719;&#24471;&#26368;&#20248;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.01319</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;OTFS&#19982;OFDM&#36866;&#24212;&#24615;&#35843;&#21046;&#35299;&#35843;&#22120;
&lt;/p&gt;
&lt;p&gt;
An ML-assisted OTFS vs. OFDM adaptable modem. (arXiv:2309.01319v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;OTFS&#19982;OFDM&#36866;&#24212;&#24615;&#35843;&#21046;&#35299;&#35843;&#22120;&#65292;&#36890;&#36807;&#35266;&#23519;&#20449;&#36947;&#29366;&#24577;&#12289;&#25509;&#25910;&#20449;&#22122;&#27604;&#21644;&#35843;&#21046;&#26684;&#24335;&#65292;&#22312;&#21457;&#36865;&#31471;&#21644;&#25509;&#25910;&#31471;&#20043;&#38388;&#20999;&#25442;OTFS&#25110;OFDM&#20449;&#21495;&#22788;&#29702;&#38142;&#20197;&#33719;&#24471;&#26368;&#20248;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#20132;&#26102;&#39057;&#31354;&#65288;OTFS&#65289;&#20449;&#21495;&#20855;&#26377;&#24378;&#22823;&#30340;&#25239;&#21452;&#37325;&#23637;&#23485;&#20449;&#36947;&#30340;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#39640;&#31227;&#21160;&#24773;&#26223;&#12290;&#21516;&#26102;&#65292;&#27491;&#20132;&#39057;&#20998;&#22797;&#29992;&#65288;OFDM&#65289;&#27874;&#24418;&#21487;&#37325;&#22797;&#20351;&#29992;&#20256;&#32479;&#32467;&#26500;&#65292;&#25509;&#25910;&#26426;&#35774;&#35745;&#31616;&#21333;&#65292;&#24182;&#20855;&#26377;&#20302;&#22797;&#26434;&#24230;&#26816;&#27979;&#30340;&#20248;&#21183;&#12290;&#35768;&#22810;&#27604;&#36739;OTFS&#21644;OFDM&#24615;&#33021;&#30340;&#30740;&#31350;&#22240;&#39640;&#31227;&#21160;&#26465;&#20214;&#19979;&#28041;&#21450;&#22823;&#37327;&#31995;&#32479;&#21442;&#25968;&#32780;&#20135;&#29983;&#20102;&#28151;&#21512;&#32467;&#26524;&#12290;&#26412;&#25991;&#36890;&#36807;&#27169;&#25311;&#39564;&#35777;&#20102;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#33258;&#36866;&#24212;&#26041;&#26696;&#65292;&#20197;&#22312;&#21457;&#36865;&#31471;&#21644;&#25509;&#25910;&#31471;&#20043;&#38388;&#20999;&#25442;OTFS&#25110;OFDM&#20449;&#21495;&#22788;&#29702;&#38142;&#20197;&#33719;&#24471;&#26368;&#20248;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#24615;&#33021;&#12290;DNN&#20998;&#31867;&#22120;&#36890;&#36807;&#35266;&#23519;&#20449;&#36947;&#29366;&#24577;&#12289;&#25509;&#25910;&#20449;&#22122;&#27604;&#21644;&#35843;&#21046;&#26684;&#24335;&#26469;&#36827;&#34892;&#26041;&#26696;&#20999;&#25442;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;OTFS&#12289;OFDM&#21644;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Orthogonal-Time-Frequency-Space (OTFS) signaling is known to be resilient to doubly-dispersive channels, which impacts high mobility scenarios. On the other hand, the Orthogonal-Frequency-Division-Multiplexing (OFDM) waveforms enjoy the benefits of the reuse of legacy architectures, simplicity of receiver design, and low-complexity detection. Several studies that compare the performance of OFDM and OTFS have indicated mixed outcomes due to the plethora of system parameters at play beyond high-mobility conditions. In this work, we exemplify this observation using simulations and propose a deep neural network (DNN)-based adaptation scheme to switch between using either an OTFS or OFDM signal processing chain at the transmitter and receiver for optimal mean-squared-error (MSE) performance. The DNN classifier is trained to switch between the two schemes by observing the channel condition, received SNR, and modulation format. We compare the performance of the OTFS, OFDM, and the propose
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#21033;&#29992;&#23436;&#25104;&#24341;&#25806;&#26469;&#36827;&#19968;&#27493;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;&#20013;&#21512;&#25104;&#26356;&#22810;&#26377;&#25928;&#30340;&#20462;&#34917;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2309.00608</link><description>&lt;p&gt;
Copiloting the Copilots: &#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#23436;&#25104;&#24341;&#25806;&#34701;&#21512;&#29992;&#20110;&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair. (arXiv:2309.00608v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00608
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#21033;&#29992;&#23436;&#25104;&#24341;&#25806;&#26469;&#36827;&#19968;&#27493;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;&#20013;&#21512;&#25104;&#26356;&#22810;&#26377;&#25928;&#30340;&#20462;&#34917;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;&#20013;&#65292;&#23545;&#20110;&#36890;&#29992;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#23454;&#38469;&#31995;&#32479;&#21512;&#25104;&#27491;&#30830;&#30340;&#20462;&#34917;&#31243;&#24207;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#35777;&#26126;&#23545;&#24320;&#21457;&#20154;&#21592;&#22312;&#21508;&#31181;&#32534;&#30721;&#20219;&#21153;&#20013;&#20855;&#26377;&#24110;&#21161;&#65292;&#24182;&#19988;&#24050;&#30452;&#25509;&#24212;&#29992;&#20110;&#20462;&#34917;&#31243;&#24207;&#30340;&#21512;&#25104;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;LLMs&#23558;&#31243;&#24207;&#35270;&#20026;&#20196;&#29260;&#24207;&#21015;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#23545;&#30446;&#26631;&#32534;&#31243;&#35821;&#35328;&#30340;&#24213;&#23618;&#35821;&#20041;&#32422;&#26463;&#19968;&#26080;&#25152;&#30693;&#12290;&#36825;&#23548;&#33268;&#29983;&#25104;&#20102;&#22823;&#37327;&#38745;&#24577;&#26080;&#25928;&#30340;&#20462;&#34917;&#31243;&#24207;&#65292;&#38459;&#30861;&#20102;&#35813;&#25216;&#26415;&#30340;&#23454;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Repilot&#65292;&#19968;&#31181;&#22312;&#20462;&#22797;&#36807;&#31243;&#20013;&#36890;&#36807;&#21512;&#25104;&#26356;&#22810;&#26377;&#25928;&#20462;&#34917;&#31243;&#24207;&#20174;&#32780;&#36827;&#19968;&#27493;&#25903;&#25345;AI&#8220;&#21103;&#39550;&#39542;&#21592;&#8221;&#65288;&#21363;LLMs&#65289;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#35768;&#22810;LLMs&#20197;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#36755;&#20986;&#65288;&#21363;&#36880;&#20010;&#20196;&#29260;&#29983;&#25104;&#65289;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#32534;&#20889;&#31243;&#24207;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#23436;&#25104;&#24341;&#25806;&#26174;&#33879;&#25552;&#21319;&#21644;&#24341;&#23548;&#12290;Repilot&#21327;&#21516;&#21512;&#25104;&#20102;&#20462;&#34917;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
During Automated Program Repair (APR), it can be challenging to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models (LLMs) have been shown to be helpful "copilots" in assisting developers with various coding tasks, and have also been directly applied for patch synthesis. However, most LLMs treat programs as sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This results in plenty of statically invalid generated patches, impeding the practicality of the technique. Therefore, we propose Repilot, a framework to further copilot the AI "copilots" (i.e., LLMs) by synthesizing more valid patches during the repair process. Our key insight is that many LLMs produce outputs autoregressively (i.e., token by token), resembling human writing programs, which can be significantly boosted and guided through a Completion Engine. Repilot synergistically synthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#20027;&#39064;&#30340;&#36125;&#21494;&#26031;&#24778;&#21916;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#24847;&#22806;&#24615;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#36807;&#28388;&#27873;&#38382;&#39064;&#65292;&#36890;&#36807;&#35782;&#21035;&#30456;&#20284;&#29992;&#25143;&#21644;&#27979;&#37327;&#29992;&#25143;&#23545;&#29289;&#21697;&#30340;&#24847;&#22806;&#24615;&#26469;&#25512;&#33616;&#20855;&#26377;&#39640;&#28508;&#21147;&#30340;&#24847;&#22806;&#24615;&#29289;&#21697;&#12290;</title><link>http://arxiv.org/abs/2308.06368</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#39064;&#30340;&#36125;&#21494;&#26031;&#24778;&#21916;&#21644;&#24847;&#22806;&#24615;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Topic-Level Bayesian Surprise and Serendipity for Recommender Systems. (arXiv:2308.06368v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#20027;&#39064;&#30340;&#36125;&#21494;&#26031;&#24778;&#21916;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#30340;&#24847;&#22806;&#24615;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#36807;&#28388;&#27873;&#38382;&#39064;&#65292;&#36890;&#36807;&#35782;&#21035;&#30456;&#20284;&#29992;&#25143;&#21644;&#27979;&#37327;&#29992;&#25143;&#23545;&#29289;&#21697;&#30340;&#24847;&#22806;&#24615;&#26469;&#25512;&#33616;&#20855;&#26377;&#39640;&#28508;&#21147;&#30340;&#24847;&#22806;&#24615;&#29289;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20248;&#21270;&#20854;&#25512;&#33616;&#20165;&#36866;&#21512;&#29992;&#25143;&#23545;&#24050;&#28040;&#36153;&#29289;&#21697;&#30340;&#35780;&#32423;&#21382;&#21490;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#36807;&#28388;&#27873;&#65292;&#29992;&#25143;&#26080;&#27861;&#20174;&#26032;&#39062;&#12289;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#20013;&#20307;&#39564;&#29289;&#21697;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23481;&#30340;&#24847;&#22806;&#24615;&#24418;&#24335;&#65292;&#20197;&#36125;&#21494;&#26031;&#24778;&#21916;&#20026;&#22522;&#30784;&#65292;&#29992;&#20110;&#27979;&#37327;&#29992;&#25143;&#28040;&#36153;&#24182;&#35780;&#32423;&#21518;&#29289;&#21697;&#30340;&#24847;&#22806;&#24615;&#12290;&#32467;&#21512;&#35782;&#21035;&#30456;&#20284;&#29992;&#25143;&#30340;&#21327;&#21516;&#36807;&#28388;&#32452;&#20214;&#65292;&#21487;&#20197;&#25512;&#33616;&#20855;&#26377;&#39640;&#28508;&#21147;&#24847;&#22806;&#24615;&#30340;&#29289;&#21697;&#12290;&#20026;&#20102;&#20415;&#20110;&#35780;&#20272;&#20027;&#39064;&#32423;&#21035;&#30340;&#24778;&#21916;&#21644;&#24847;&#22806;&#24615;&#27169;&#22411;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20174;Goodreads&#20013;&#25552;&#21462;&#30340;&#22270;&#20070;&#38405;&#35835;&#21382;&#21490;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;26&#21315;&#20010;&#29992;&#25143;&#21644;&#36817;130&#19975;&#26412;&#20070;&#65292;&#24182;&#23545;&#20854;&#20013;&#30340;449&#31687;&#20070;&#36827;&#34892;&#20102;&#25163;&#21160;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recommender system that optimizes its recommendations solely to fit a user's history of ratings for consumed items can create a filter bubble, wherein the user does not get to experience items from novel, unseen categories. One approach to mitigate this undesired behavior is to recommend items with high potential for serendipity, namely surprising items that are likely to be highly rated. In this paper, we propose a content-based formulation of serendipity that is rooted in Bayesian surprise and use it to measure the serendipity of items after they are consumed and rated by the user. When coupled with a collaborative-filtering component that identifies similar users, this enables recommending items with high potential for serendipity. To facilitate the evaluation of topic-level models for surprise and serendipity, we introduce a dataset of book reading histories extracted from Goodreads, containing over 26 thousand users and close to 1.3 million books, where we manually annotate 449 
&lt;/p&gt;</description></item><item><title>Amazon-M2&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#22810;&#21306;&#22495;&#36141;&#29289;&#20250;&#35805;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#22686;&#24378;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#29702;&#35299;&#29992;&#25143;&#20559;&#22909;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.09688</link><description>&lt;p&gt;
Amazon-M2: &#19968;&#20010;&#29992;&#20110;&#25512;&#33616;&#21644;&#25991;&#26412;&#29983;&#25104;&#30340;&#22810;&#35821;&#35328;&#22810;&#21306;&#22495;&#36141;&#29289;&#20250;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation. (arXiv:2307.09688v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09688
&lt;/p&gt;
&lt;p&gt;
Amazon-M2&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#22810;&#21306;&#22495;&#36141;&#29289;&#20250;&#35805;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#22686;&#24378;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#29702;&#35299;&#29992;&#25143;&#20559;&#22909;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30005;&#23376;&#21830;&#21153;&#26469;&#35828;&#65292;&#24314;&#27169;&#23458;&#25143;&#36141;&#29289;&#24847;&#22270;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#30452;&#25509;&#24433;&#21709;&#29992;&#25143;&#20307;&#39564;&#21644;&#21442;&#19982;&#24230;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#29702;&#35299;&#23458;&#25143;&#30340;&#20559;&#22909;&#23545;&#20110;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#25216;&#26415;&#21033;&#29992;&#23458;&#25143;&#20250;&#35805;&#25968;&#25454;&#26469;&#39044;&#27979;&#20182;&#20204;&#30340;&#19979;&#19968;&#27425;&#20114;&#21160;&#65292;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#21040;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20250;&#35805;&#25968;&#25454;&#38598;&#22312;&#39033;&#30446;&#23646;&#24615;&#12289;&#29992;&#25143;&#22810;&#26679;&#24615;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#19981;&#33021;&#20840;&#38754;&#22320;&#25429;&#25417;&#29992;&#25143;&#34892;&#20026;&#21644;&#20559;&#22909;&#30340;&#35889;&#31995;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Amazon Multilingual Multi-locale Shopping Session Dataset&#65292;&#21363;Amazon-M2&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#30001;&#26469;&#33258;&#20845;&#20010;&#19981;&#21516;&#21306;&#22495;&#30340;&#25968;&#30334;&#19975;&#29992;&#25143;&#20250;&#35805;&#32452;&#25104;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#20135;&#21697;&#30340;&#20027;&#35201;&#35821;&#35328;&#26159;&#33521;&#35821;&#12289;&#24503;&#35821;&#12289;&#26085;&#35821;&#12289;&#27861;&#35821;&#12289;&#24847;&#22823;&#21033;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#22686;&#24378;&#20010;&#24615;&#21270;&#21644;&#29702;&#35299;&#29992;&#25143;&#20559;&#22909;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling customer shopping intentions is a crucial task for e-commerce, as it directly impacts user experience and engagement. Thus, accurately understanding customer preferences is essential for providing personalized recommendations. Session-based recommendation, which utilizes customer session data to predict their next interaction, has become increasingly popular. However, existing session datasets have limitations in terms of item attributes, user diversity, and dataset scale. As a result, they cannot comprehensively capture the spectrum of user behaviors and preferences. To bridge this gap, we present the Amazon Multilingual Multi-locale Shopping Session Dataset, namely Amazon-M2. It is the first multilingual dataset consisting of millions of user sessions from six different locales, where the major languages of products are English, German, Japanese, French, Italian, and Spanish. Remarkably, the dataset can help us enhance personalization and understanding of user preferences, w
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#38656;&#35201;&#21516;&#26102;&#36827;&#34892;&#25805;&#25511;&#21644;&#23548;&#33322;&#30340;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#36801;&#31227;&#65292;&#24182;&#23545;&#26410;&#35265;&#36807;&#30340;&#23376;&#20219;&#21153;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06125</link><description>&lt;p&gt;
&#23398;&#20064;&#23618;&#27425;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20197;&#36827;&#34892;&#31227;&#21160;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation. (arXiv:2307.06125v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06125
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#38656;&#35201;&#21516;&#26102;&#36827;&#34892;&#25805;&#25511;&#21644;&#23548;&#33322;&#30340;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#38646;&#26679;&#26412;&#36801;&#31227;&#65292;&#24182;&#23545;&#26410;&#35265;&#36807;&#30340;&#23376;&#20219;&#21153;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30446;&#26631;&#25628;&#32034;&#26041;&#27861;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#33258;&#30001;&#36335;&#24452;&#19978;&#36827;&#34892;&#25628;&#32034;&#65292;&#28982;&#32780;&#65292;&#22312;&#38750;&#32467;&#26500;&#21270;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#29615;&#22659;&#20013;&#25805;&#20316;&#30340;&#26426;&#22120;&#20154;&#32463;&#24120;&#38656;&#35201;&#25805;&#25511;&#29615;&#22659;&#20197;&#28385;&#36275;&#20182;&#20204;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#24335;&#22810;&#30446;&#26631;&#25628;&#32034;&#20219;&#21153;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#25171;&#24320;&#38376;&#20197;&#27983;&#35272;&#25151;&#38388;&#65292;&#24182;&#22312;&#27249;&#26588;&#21644;&#25277;&#23625;&#20869;&#25628;&#32034;&#30446;&#26631;&#29289;&#21697;&#12290;&#36825;&#20123;&#26032;&#25361;&#25112;&#38656;&#35201;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#32467;&#21512;&#25805;&#25511;&#21644;&#23548;&#33322;&#25216;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HIMOS&#65292;&#19968;&#31181;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23398;&#20064;&#32452;&#21512;&#25506;&#32034;&#12289;&#23548;&#33322;&#21644;&#25805;&#25511;&#25216;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22260;&#32469;&#35821;&#20041;&#22320;&#22270;&#35760;&#24518;&#30340;&#25277;&#35937;&#39640;&#32423;&#21160;&#20316;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#25506;&#32034;&#36807;&#30340;&#29615;&#22659;&#20316;&#20026;&#23454;&#20363;&#23548;&#33322;&#28857;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;HIMOS&#21487;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#26377;&#25928;&#22320;&#36801;&#31227;&#21040;&#26032;&#30340;&#29615;&#22659;&#20013;&#65292;&#24182;&#19988;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#23376;&#20219;&#21153;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing object-search approaches enable robots to search through free pathways, however, robots operating in unstructured human-centered environments frequently also have to manipulate the environment to their needs. In this work, we introduce a novel interactive multi-object search task in which a robot has to open doors to navigate rooms and search inside cabinets and drawers to find target objects. These new challenges require combining manipulation and navigation skills in unexplored environments. We present HIMOS, a hierarchical reinforcement learning approach that learns to compose exploration, navigation, and manipulation skills. To achieve this, we design an abstract high-level action space around a semantic map memory and leverage the explored environment as instance navigation points. We perform extensive experiments in simulation and the real-world that demonstrate that HIMOS effectively transfers to new environments in a zero-shot manner. It shows robustness to unseen subp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#36816;&#21160;&#21407;&#29702;&#27169;&#22411;&#65292;&#20855;&#22791;&#22810;&#31181;&#25805;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.05141</link><description>&lt;p&gt;
&#28145;&#24230;&#27010;&#29575;&#36816;&#21160;&#21407;&#29702;&#19982;&#36125;&#21494;&#26031;&#32858;&#21512;&#22120;
&lt;/p&gt;
&lt;p&gt;
Deep Probabilistic Movement Primitives with a Bayesian Aggregator. (arXiv:2307.05141v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05141
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#28145;&#24230;&#36816;&#21160;&#21407;&#29702;&#27169;&#22411;&#65292;&#20855;&#22791;&#22810;&#31181;&#25805;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#21407;&#29702;&#26159;&#21487;&#35757;&#32451;&#30340;&#21442;&#25968;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#26377;&#38480;&#30340;&#28436;&#31034;&#38598;&#21512;&#20013;&#22797;&#21046;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#32447;&#24615;&#27169;&#22411;&#65292;&#36890;&#36807;&#20801;&#35768;&#26102;&#38388;&#35843;&#21046;&#36816;&#21160;&#65288;&#21152;&#36895;&#25110;&#20943;&#36895;&#22797;&#21046;&#36816;&#21160;&#65289;&#12289;&#28151;&#21512;&#65288;&#23558;&#20004;&#20010;&#36816;&#21160;&#21512;&#24182;&#20026;&#19968;&#20010;&#65289;&#12289;&#36890;&#36807;&#28857;&#35843;&#33410;&#65288;&#23558;&#36816;&#21160;&#32422;&#26463;&#21040;&#29305;&#23450;&#30340;&#36890;&#36807;&#28857;&#65289;&#21644;&#19978;&#19979;&#25991;&#35843;&#33410;&#65288;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#29983;&#25104;&#36816;&#21160;&#65292;&#20363;&#22914;&#29289;&#20307;&#30340;&#20301;&#32622;&#65289;&#23637;&#31034;&#20986;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36816;&#21160;&#21407;&#29702;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19968;&#20123;&#24418;&#24335;&#30340;&#36755;&#20837;&#35843;&#33410;&#25110;&#26102;&#38388;&#35843;&#21046;&#34920;&#36798;&#20013;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#36824;&#27809;&#26377;&#25552;&#20986;&#19968;&#20010;&#21333;&#19968;&#32479;&#19968;&#30340;&#28145;&#24230;&#36816;&#21160;&#21407;&#29702;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#20855;&#22791;&#25152;&#26377;&#20808;&#21069;&#30340;&#25805;&#20316;&#65292;&#36825;&#38480;&#21046;&#20102;&#31070;&#32463;&#36816;&#21160;&#21407;&#29702;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#36816;&#21160;&#21407;&#29702;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Movement primitives are trainable parametric models that reproduce robotic movements starting from a limited set of demonstrations. Previous works proposed simple linear models that exhibited high sample efficiency and generalization power by allowing temporal modulation of movements (reproducing movements faster or slower), blending (merging two movements into one), via-point conditioning (constraining a movement to meet some particular via-points) and context conditioning (generation of movements based on an observed variable, e.g., position of an object). Previous works have proposed neural network-based motor primitive models, having demonstrated their capacity to perform tasks with some forms of input conditioning or time-modulation representations. However, there has not been a single unified deep motor primitive's model proposed that is capable of all previous operations, limiting neural motor primitive's potential applications. This paper proposes a deep movement primitive arch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#31216;&#22810;&#39033;&#24335;&#20195;&#25968;&#30340;&#24037;&#20855;&#35777;&#26126;&#20102;&#23545;&#20110;&#20855;&#26377;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#19988;&#20307;&#31995;&#32467;&#26500;&#22823;&#23567;&#19981;&#21464;&#30340;GNNs&#65292;&#23384;&#22312;&#19968;&#23545;&#38750;&#21516;&#26500;&#26681;&#26641;&#22312;&#20219;&#24847;&#36845;&#20195;&#27425;&#25968;&#20869;&#26080;&#27861;&#34987;&#21306;&#20998;&#65292;&#19982;&#27492;&#21516;&#26102;&#65292;&#20855;&#26377;&#19981;&#21516;&#22823;&#23567;&#30340;GNNs&#21482;&#38656;&#20004;&#27425;&#36845;&#20195;&#21363;&#21487;&#21306;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22914;&#26524;&#20801;&#35768;&#38750;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#65292;&#21017;&#22312;&#20004;&#27425;&#36845;&#20195;&#20869;&#65292;&#21333;&#20010;&#31070;&#32463;&#20803;&#24863;&#30693;&#22120;&#21487;&#20197;&#21306;&#20998;&#20219;&#24847;&#19968;&#23545;&#38750;&#21516;&#26500;&#26641;&#30340;&#26681;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.04661</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#21644;&#28608;&#27963;&#20989;&#25968;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the power of graph neural networks and the role of the activation function. (arXiv:2307.04661v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#31216;&#22810;&#39033;&#24335;&#20195;&#25968;&#30340;&#24037;&#20855;&#35777;&#26126;&#20102;&#23545;&#20110;&#20855;&#26377;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#19988;&#20307;&#31995;&#32467;&#26500;&#22823;&#23567;&#19981;&#21464;&#30340;GNNs&#65292;&#23384;&#22312;&#19968;&#23545;&#38750;&#21516;&#26500;&#26681;&#26641;&#22312;&#20219;&#24847;&#36845;&#20195;&#27425;&#25968;&#20869;&#26080;&#27861;&#34987;&#21306;&#20998;&#65292;&#19982;&#27492;&#21516;&#26102;&#65292;&#20855;&#26377;&#19981;&#21516;&#22823;&#23567;&#30340;GNNs&#21482;&#38656;&#20004;&#27425;&#36845;&#20195;&#21363;&#21487;&#21306;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22914;&#26524;&#20801;&#35768;&#38750;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#65292;&#21017;&#22312;&#20004;&#27425;&#36845;&#20195;&#20869;&#65292;&#21333;&#20010;&#31070;&#32463;&#20803;&#24863;&#30693;&#22120;&#21487;&#20197;&#21306;&#20998;&#20219;&#24847;&#19968;&#23545;&#38750;&#21516;&#26500;&#26641;&#30340;&#26681;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34920;&#36798;&#33021;&#21147;&#30340;&#26032;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#20309;&#20855;&#26377;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#12289;&#20854;&#20307;&#31995;&#32467;&#26500;&#22823;&#23567;&#19981;&#38543;&#22270;&#36755;&#20837;&#22823;&#23567;&#22686;&#38271;&#30340;GNNs&#65292;&#23384;&#22312;&#19968;&#23545;&#28145;&#24230;&#20026;&#20108;&#30340;&#38750;&#21516;&#26500;&#26681;&#26641;&#65292;&#20351;&#24471;GNNs&#22312;&#20219;&#24847;&#36845;&#20195;&#27425;&#25968;&#20869;&#26080;&#27861;&#21306;&#20998;&#23427;&#20204;&#30340;&#26681;&#33410;&#28857;&#12290;&#35777;&#26126;&#20381;&#36182;&#20110;&#23545;&#31216;&#22810;&#39033;&#24335;&#20195;&#25968;&#30340;&#24037;&#20855;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24050;&#32463;&#30693;&#36947;&#20855;&#26377;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#26080;&#30028;GNNs&#65288;&#20854;&#22823;&#23567;&#20801;&#35768;&#38543;&#22270;&#22823;&#23567;&#25913;&#21464;&#65289;&#21482;&#38656;&#20004;&#27425;&#36845;&#20195;&#21363;&#21487;&#21306;&#20998;&#36825;&#20123;&#39030;&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#20110;&#26377;&#30028;&#22823;&#23567;&#21644;&#26080;&#30028;&#22823;&#23567;&#30340;GNNs&#20043;&#38388;&#23384;&#22312;&#20005;&#26684;&#30340;&#20998;&#31163;&#65292;&#22238;&#31572;&#20102; [Grohe, 2021] &#25552;&#20986;&#30340;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35777;&#26126;&#22914;&#26524;&#20801;&#35768;&#38750;&#20998;&#27573;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#65292;&#21017;&#22312;&#20004;&#27425;&#36845;&#20195;&#20013;&#65292;&#21333;&#20010;&#31070;&#32463;&#20803;&#24863;&#30693;&#22120;&#21487;&#20197;&#21306;&#20998;&#20219;&#24847;&#19968;&#23545;&#38750;&#21516;&#26500;&#26641;&#30340;&#26681;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article we present new results about the expressivity of Graph Neural Networks (GNNs). We prove that for any GNN with piecewise polynomial activations, whose architecture size does not grow with the graph input sizes, there exists a pair of non-isomorphic rooted trees of depth two such that the GNN cannot distinguish their root vertex up to an arbitrary number of iterations. The proof relies on tools from the algebra of symmetric polynomials. In contrast, it was already known that unbounded GNNs (those whose size is allowed to change with the graph sizes) with piecewise polynomial activations can distinguish these vertices in only two iterations. Our results imply a strict separation between bounded and unbounded size GNNs, answering an open question formulated by [Grohe, 2021]. We next prove that if one allows activations that are not piecewise polynomial, then in two iterations a single neuron perceptron can distinguish the root vertices of any pair of nonisomorphic trees of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#34892;&#31243;&#26102;&#38388;&#23618;&#26512;&#25104;&#20687;&#26041;&#27861;&#65292;&#21033;&#29992;&#25935;&#24863;&#24615;&#20449;&#24687;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#21644;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#65292;&#20197;&#22788;&#29702;&#22320;&#36136;&#22797;&#26434;&#24615;&#20808;&#39564;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.04228</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#34892;&#31243;&#26102;&#38388;&#23618;&#26512;&#25104;&#20687;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#25935;&#24863;&#24615;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#21644;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#30340;&#22320;&#36136;&#22797;&#26434;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Efficient Bayesian travel-time tomography with geologically-complex priors using sensitivity-informed polynomial chaos expansion and deep generative networks. (arXiv:2307.04228v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#34892;&#31243;&#26102;&#38388;&#23618;&#26512;&#25104;&#20687;&#26041;&#27861;&#65292;&#21033;&#29992;&#25935;&#24863;&#24615;&#20449;&#24687;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#21644;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#65292;&#20197;&#22788;&#29702;&#22320;&#36136;&#22797;&#26434;&#24615;&#20808;&#39564;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33945;&#29305;&#21345;&#27931;&#39532;&#23572;&#21487;&#22827;&#38142;&#65288;MCMC&#65289;&#26041;&#27861;&#24120;&#24120;&#38754;&#20020;&#20004;&#20010;&#26681;&#26412;&#24615;&#25361;&#25112;&#65306;&#20808;&#39564;&#20998;&#24067;&#30340;&#20934;&#30830;&#21051;&#30011;&#21644;&#20284;&#28982;&#20989;&#25968;&#30340;&#39640;&#25928;&#35780;&#20272;&#12290;&#22312;&#23618;&#26512;&#25104;&#20687;&#30340;&#36125;&#21494;&#26031;&#30740;&#31350;&#20013;&#65292;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#26041;&#20415;&#22320;&#23450;&#20041;&#20808;&#39564;&#20998;&#24067;&#65292;&#24182;&#21516;&#26102;&#20511;&#21161;&#22522;&#20110;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#65288;PCE&#65289;&#30340;&#20934;&#30830;&#20195;&#29702;&#27169;&#22411;&#26469;&#26367;&#20195;&#35745;&#31639;&#23494;&#38598;&#30340;&#20840;&#29289;&#29702;&#27491;&#21521;&#27714;&#35299;&#22120;&#12290;&#24403;PCA&#26080;&#27861;&#30452;&#25509;&#25552;&#20379;&#23450;&#20041;&#20808;&#39564;&#20998;&#24067;&#30340;&#26041;&#24335;&#26102;&#65292;&#21487;&#20197;&#37319;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#65289;&#31561;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#20135;&#29983;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;VAE&#30340;&#28508;&#22312;&#21442;&#25968;&#19982;&#27491;&#21521;&#24314;&#27169;&#36755;&#20986;&#20043;&#38388;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#20195;&#29702;&#27169;&#22411;&#26159;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monte Carlo Markov Chain (MCMC) methods commonly confront two fundamental challenges: the accurate characterization of the prior distribution and the efficient evaluation of the likelihood. In the context of Bayesian studies on tomography, principal component analysis (PCA) can in some cases facilitate the straightforward definition of the prior distribution, while simultaneously enabling the implementation of accurate surrogate models based on polynomial chaos expansion (PCE) to replace computationally intensive full-physics forward solvers. When faced with scenarios where PCA does not offer a direct means of easily defining the prior distribution alternative methods like deep generative models (e.g., variational autoencoders (VAEs)), can be employed as viable options. However, accurately producing a surrogate capable of capturing the intricate non-linear relationship between the latent parameters of a VAE and the outputs of forward modeling presents a notable challenge. Indeed, while
&lt;/p&gt;</description></item><item><title>URL&#22522;&#20934;&#26159;&#19968;&#20010;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#36716;&#31227;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#21457;&#29616;&#19987;&#27880;&#20110;&#34920;&#31034;&#26412;&#36523;&#19981;&#30830;&#23450;&#24615;&#25110;&#30452;&#25509;&#20272;&#35745;&#39044;&#27979;&#39118;&#38505;&#30340;&#26041;&#27861;&#25928;&#26524;&#20248;&#20110;&#22522;&#20110;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03810</link><description>&lt;p&gt;
URL&#65306;&#19968;&#31181;&#21487;&#36716;&#31227;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#34920;&#31034;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates. (arXiv:2307.03810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03810
&lt;/p&gt;
&lt;p&gt;
URL&#22522;&#20934;&#26159;&#19968;&#20010;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#36716;&#31227;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#21457;&#29616;&#19987;&#27880;&#20110;&#34920;&#31034;&#26412;&#36523;&#19981;&#30830;&#23450;&#24615;&#25110;&#30452;&#25509;&#20272;&#35745;&#39044;&#27979;&#39118;&#38505;&#30340;&#26041;&#27861;&#25928;&#26524;&#20248;&#20110;&#22522;&#20110;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#23398;&#20064;&#26174;&#33879;&#25512;&#21160;&#20102;&#35813;&#39046;&#22495;&#21457;&#23637;&#20986;&#33021;&#22815;&#20316;&#20026;&#20174;&#38646;&#24320;&#22987;&#36801;&#31227;&#21040;&#26032;&#25968;&#25454;&#38598;&#26102;&#30340;&#26377;&#20215;&#20540;&#36215;&#28857;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#38543;&#30528;&#23545;&#21487;&#38752;&#26426;&#22120;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#38656;&#35201;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19981;&#20165;&#33021;&#25552;&#20379;&#23884;&#20837;&#21521;&#37327;&#65292;&#36824;&#33021;&#25552;&#20379;&#21487;&#36716;&#31227;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#20026;&#20102;&#24341;&#23548;&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;URL&#65288;Uncertainty-aware Representation Learning&#65289;&#22522;&#20934;&#12290;&#38500;&#20102;&#34920;&#31034;&#30340;&#21487;&#36716;&#31227;&#24615;&#20043;&#22806;&#65292;&#23427;&#36824;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#38646;&#26679;&#26412;&#21487;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#24212;&#29992;URL&#26469;&#35780;&#20272;11&#31181;&#22312;ImageNet&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#24182;&#36716;&#31227;&#21040;8&#20010;&#19979;&#28216;&#25968;&#25454;&#38598;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22120;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30528;&#37325;&#20110;&#34920;&#31034;&#26412;&#36523;&#30340;&#19981;&#30830;&#23450;&#24615;&#25110;&#30452;&#25509;&#20272;&#35745;&#39044;&#27979;&#39118;&#38505;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#20110;&#19978;&#28216;&#31867;&#21035;&#30340;&#27010;&#29575;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#21487;&#36716;&#31227;&#30340;&#19981;&#30830;&#23450;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning has significantly driven the field to develop pretrained models that can act as a valuable starting point when transferring to new datasets. With the rising demand for reliable machine learning and uncertainty quantification, there is a need for pretrained models that not only provide embeddings but also transferable uncertainty estimates. To guide the development of such models, we propose the Uncertainty-aware Representation Learning (URL) benchmark. Besides the transferability of the representations, it also measures the zero-shot transferability of the uncertainty estimate using a novel metric. We apply URL to evaluate eleven uncertainty quantifiers that are pretrained on ImageNet and transferred to eight downstream datasets. We find that approaches that focus on the uncertainty of the representation itself or estimate the prediction risk directly outperform those that are based on the probabilities of upstream classes. Yet, achieving transferable uncertaint
&lt;/p&gt;</description></item><item><title>Voicebox&#26159;&#19968;&#31181;&#22823;&#35268;&#27169;&#30340;&#22810;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#33258;&#22238;&#24402;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#65292;&#22312;&#25991;&#26412;&#21644;&#38899;&#39057;&#19978;&#19979;&#25991;&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65292;&#22122;&#22768;&#21435;&#38500;&#65292;&#20869;&#23481;&#32534;&#36753;&#65292;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#26679;&#21270;&#30340;&#26679;&#26412;&#29983;&#25104;&#31561;&#22810;&#31181;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.15687</link><description>&lt;p&gt;
Voicebox&#65306;&#22823;&#35268;&#27169;&#30340;&#22810;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale. (arXiv:2306.15687v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15687
&lt;/p&gt;
&lt;p&gt;
Voicebox&#26159;&#19968;&#31181;&#22823;&#35268;&#27169;&#30340;&#22810;&#35821;&#35328;&#36890;&#29992;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#33258;&#22238;&#24402;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#65292;&#22312;&#25991;&#26412;&#21644;&#38899;&#39057;&#19978;&#19979;&#25991;&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65292;&#22122;&#22768;&#21435;&#38500;&#65292;&#20869;&#23481;&#32534;&#36753;&#65292;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#26679;&#21270;&#30340;&#26679;&#26412;&#29983;&#25104;&#31561;&#22810;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;GPT&#21644;DALL-E&#24050;&#32463;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#30340;&#26041;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#25110;&#22270;&#20687;&#36755;&#20986;&#65292;&#32780;&#19988;&#36824;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#35299;&#20915;&#26410;&#34987;&#26126;&#30830;&#25945;&#25480;&#30340;&#20219;&#21153;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#22312;&#35268;&#27169;&#21644;&#20219;&#21153;&#36890;&#29992;&#21270;&#26041;&#38754;&#20173;&#28982;&#27604;&#36739;&#21407;&#22987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Voicebox&#65292;&#36825;&#26159;&#26368;&#22810;&#21151;&#33021;&#30340;&#38754;&#21521;&#35268;&#27169;&#30340;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#27169;&#22411;&#12290;Voicebox&#26159;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#30340;&#27969;&#21305;&#37197;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#38899;&#39057;&#19978;&#19979;&#25991;&#21644;&#25991;&#26412;&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#29992;50,000&#23567;&#26102;&#30340;&#26410;&#32463;&#36807;&#28388;&#25110;&#22686;&#24378;&#30340;&#35821;&#38899;&#36827;&#34892;&#22635;&#20805;&#12290;&#19982;GPT&#31867;&#20284;&#65292;Voicebox&#21487;&#20197;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25191;&#34892;&#22810;&#31181;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#20294;&#26356;&#21152;&#28789;&#27963;&#65292;&#22240;&#20026;&#23427;&#36824;&#21487;&#20197;&#23545;&#26410;&#26469;&#30340;&#19978;&#19979;&#25991;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#12290;Voicebox&#21487;&#20197;&#29992;&#20110;&#21333;&#35821;&#25110;&#36328;&#35821;&#35328;&#38646;&#26679;&#26412;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65292;&#22122;&#22768;&#21435;&#38500;&#65292;&#20869;&#23481;&#32534;&#36753;&#65292;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#26679;&#21270;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#29305;&#21035;&#26159;&#65292;Voicebox
&lt;/p&gt;
&lt;p&gt;
Large-scale generative models such as GPT and DALL-E have revolutionized natural language processing and computer vision research. These models not only generate high fidelity text or image outputs, but are also generalists which can solve tasks not explicitly taught. In contrast, speech generative models are still primitive in terms of scale and task generalization. In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are neither filtered nor enhanced. Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context. Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation. In particular, Voicebox 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38024;&#32455;&#21147;&#20256;&#24863;&#22120;&#19981;&#19968;&#33268;&#24615;&#34917;&#20607;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25351;&#25968;&#24179;&#28369;&#28388;&#27874;&#22120;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#24182;&#20351;&#29992;&#26368;&#23567;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#39640;&#20256;&#24863;&#22120;&#35835;&#25968;&#21644;&#25191;&#34892;&#21147;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2306.12129</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38024;&#32455;&#21147;&#20256;&#24863;&#22120;&#19981;&#19968;&#33268;&#24615;&#34917;&#20607;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Based Compensation for Inconsistencies in Knitted Force Sensors. (arXiv:2306.12129v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38024;&#32455;&#21147;&#20256;&#24863;&#22120;&#19981;&#19968;&#33268;&#24615;&#34917;&#20607;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25351;&#25968;&#24179;&#28369;&#28388;&#27874;&#22120;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#24182;&#20351;&#29992;&#26368;&#23567;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#39640;&#20256;&#24863;&#22120;&#35835;&#25968;&#21644;&#25191;&#34892;&#21147;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#32455;&#20256;&#24863;&#22120;&#32463;&#24120;&#21463;&#21040;&#22266;&#26377;&#25928;&#24212;&#65288;&#22914;&#20559;&#31227;&#12289;&#26494;&#24347;&#21644;&#28418;&#31227;&#65289;&#30340;&#24433;&#21709;&#32780;&#20135;&#29983;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#20123;&#23646;&#24615;&#30340;&#32467;&#21512;&#20351;&#24471;&#20174;&#20256;&#24863;&#22120;&#25968;&#25454;&#21040;&#29289;&#29702;&#25191;&#34892;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#23567;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#32467;&#21512;&#31616;&#21333;&#39044;&#22788;&#29702;&#26041;&#27861;&#26469;&#36827;&#34892;&#22788;&#29702;&#30340;&#26041;&#27861;&#26469;&#23545;&#25239;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#22312;&#37325;&#26032;&#37319;&#26679;&#36807;&#30340;&#20256;&#24863;&#22120;&#20449;&#21495;&#19978;&#24212;&#29992;&#20102;&#22810;&#20010;&#25351;&#25968;&#24179;&#28369;&#28388;&#27874;&#22120;&#65292;&#20197;&#20135;&#29983;&#20445;&#30041;&#19981;&#21516;&#21382;&#21490;&#20256;&#24863;&#22120;&#25968;&#25454;&#27700;&#24179;&#30340;&#29305;&#24449;&#65292;&#24182;&#32467;&#21512;&#34920;&#31034;&#20197;&#21069;&#20256;&#24863;&#22120;&#25191;&#34892;&#20805;&#20998;&#29366;&#24577;&#30340;&#29305;&#24449;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#19977;&#23618;ANN&#65292;&#24635;&#20849;&#26377;8&#20010;&#31070;&#32463;&#20803;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#20256;&#24863;&#22120;&#35835;&#25968;&#21644;&#25191;&#34892;&#21147;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#23545;&#20110;&#26448;&#26009;&#21644;&#32467;&#26500;&#30456;&#24403;&#19981;&#21516;&#30340;&#20256;&#24863;&#22120;&#20063;&#26159;&#36866;&#29992;&#30340;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#30456;&#20851;&#30340;&#29289;&#29702;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knitted sensors frequently suffer from inconsistencies due to innate effects such as offset, relaxation, and drift. These properties, in combination, make it challenging to reliably map from sensor data to physical actuation. In this paper, we demonstrate a method for counteracting this by applying processing using a minimal artificial neural network (ANN) in combination with straightforward pre-processing. We apply a number of exponential smoothing filters on a re-sampled sensor signal, to produce features that preserve different levels of historical sensor data and, in combination, represent an adequate state of previous sensor actuation. By training a three-layer ANN with a total of 8 neurons, we manage to significantly improve the mapping between sensor reading and actuation force. Our findings also show that our technique translates to sensors of reasonably different composition in terms of material and structure, and it can furthermore be applied to related physical features such
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#32452;&#31616;&#21333;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;&#30340;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36716;&#21270;&#20026;&#21487;&#35777;&#26126;&#24378;&#22823;&#30340;&#26377;&#21521;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#26816;&#27979;&#20219;&#20309;&#26377;&#21521;&#23376;&#22270;&#27169;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#20123;&#25913;&#36827;&#26041;&#27861;&#22312;&#21512;&#25104;&#23376;&#22270;&#26816;&#27979;&#20219;&#21153;&#21644;&#37329;&#34701;&#29359;&#32618;&#20998;&#26512;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11586</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#24378;&#22823;&#30340;&#26377;&#21521;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Provably Powerful Graph Neural Networks for Directed Multigraphs. (arXiv:2306.11586v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#32452;&#31616;&#21333;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;&#30340;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36716;&#21270;&#20026;&#21487;&#35777;&#26126;&#24378;&#22823;&#30340;&#26377;&#21521;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#26816;&#27979;&#20219;&#20309;&#26377;&#21521;&#23376;&#22270;&#27169;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#20123;&#25913;&#36827;&#26041;&#27861;&#22312;&#21512;&#25104;&#23376;&#22270;&#26816;&#27979;&#20219;&#21153;&#21644;&#37329;&#34701;&#29359;&#32618;&#20998;&#26512;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#32452;&#31616;&#21333;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;&#30340;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36716;&#21270;&#20026;&#21487;&#35777;&#26126;&#24378;&#22823;&#30340;&#26377;&#21521;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25913;&#36827;&#26041;&#27861;&#21253;&#25324;&#22810;&#22270;&#31471;&#21475;&#32534;&#21495;&#12289;&#20010;&#20307;ID&#21644;&#21453;&#21521;&#28040;&#24687;&#20256;&#36882;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#30340;&#32452;&#21512;&#22312;&#29702;&#35770;&#19978;&#33021;&#22815;&#26816;&#27979;&#20219;&#20309;&#26377;&#21521;&#23376;&#22270;&#27169;&#24335;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;&#25913;&#36827;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#23376;&#22270;&#26816;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20960;&#20046;&#21487;&#20197;&#24471;&#21040;&#23436;&#32654;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#25913;&#36827;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;&#37329;&#34701;&#29359;&#32618;&#20998;&#26512;&#20219;&#21153;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#26816;&#27979;&#27927;&#38065;&#20132;&#26131;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#25913;&#21892;&#65292;&#23558;&#26631;&#20934;&#30340;&#28040;&#24687;&#20256;&#36882;GNN&#30340;&#23569;&#25968;&#31867;F1&#20998;&#25968;&#25552;&#39640;&#20102;&#39640;&#36798;30%&#65292;&#24182;&#19988;&#19982;&#22522;&#20110;&#26641;&#21644;GNN&#30340;&#22522;&#20934;&#30456;&#23218;&#32654;&#25110;&#36229;&#36234;&#12290;&#22312;&#19968;&#20010;&#23454;&#38469;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20063;&#35266;&#23519;&#21040;&#20102;&#31867;&#20284;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#25552;&#21319;&#20102;&#19977;&#20010;&#26631;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper analyses a set of simple adaptations that transform standard message-passing Graph Neural Networks (GNN) into provably powerful directed multigraph neural networks. The adaptations include multigraph port numbering, ego IDs, and reverse message passing. We prove that the combination of these theoretically enables the detection of any directed subgraph pattern. To validate the effectiveness of our proposed adaptations in practice, we conduct experiments on synthetic subgraph detection tasks, which demonstrate outstanding performance with almost perfect results. Moreover, we apply our proposed adaptations to two financial crime analysis tasks. We observe dramatic improvements in detecting money laundering transactions, improving the minority-class F1 score of a standard message-passing GNN by up to 30%, and closely matching or outperforming tree-based and GNN baselines. Similarly impressive results are observed on a real-world phishing detection dataset, boosting three standar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#36229;&#20154;&#27169;&#22411;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#35780;&#20272;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2306.09983</link><description>&lt;p&gt;
&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Superhuman Models with Consistency Checks. (arXiv:2306.09983v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#36229;&#20154;&#27169;&#22411;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#35780;&#20272;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#25110;&#20915;&#31574;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#36229;&#20154;&#33021;&#21147;&#65292;&#37027;&#20040;&#25105;&#20204;&#35813;&#22914;&#20309;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#65292;&#32771;&#34385;&#21040;&#20154;&#31867;&#20195;&#29702;&#20250;&#20135;&#29983;&#20559;&#24046;? &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#21069;&#25552;&#26159;&#65292;&#34429;&#28982;&#35780;&#20272;&#36229;&#20154;&#20915;&#31574;&#30340;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#20294;&#26159;&#22914;&#26524;&#27169;&#22411;&#30340;&#20915;&#31574;&#26410;&#33021;&#28385;&#36275;&#26576;&#20123;&#36923;&#36753;&#19978;&#12289;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#21457;&#29616;&#38169;&#35823;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#65292;&#36825;&#20123;&#20219;&#21153;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#30001;&#20110;&#36229;&#20154;&#27169;&#22411;&#33021;&#21147;&#25110;&#20854;&#20182;&#32570;&#20047;&#22522;&#26412;&#20107;&#23454;&#32780;&#38590;&#20197;&#35780;&#20272;&#65306;&#35780;&#20272;&#22269;&#38469;&#35937;&#26827;&#23616;&#38754;&#12289;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#21644;&#20316;&#20986;&#27861;&#24459;&#21028;&#26029;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#26080;&#35770;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;(&#21487;&#33021;&#26159;&#36229;&#20154;&#30340;)&#65292;&#25105;&#20204;&#37117;&#33021;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#12290;&#20363;&#22914;&#65306;&#22269;&#38469;&#35937;&#26827;&#24341;&#25806;&#32473;&#20986;&#23545;&#23616;&#20013;&#26827;&#23376;&#30456;&#23545;&#20272;&#20540;&#30340;&#19981;&#21516;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
If machine learning models were to achieve superhuman abilities at various reasoning or decision-making tasks, how would we go about evaluating such models, given that humans would necessarily be poor proxies for ground truth? In this paper, we propose a framework for evaluating superhuman models via consistency checks. Our premise is that while the correctness of superhuman decisions may be impossible to evaluate, we can still surface mistakes if the model's decisions fail to satisfy certain logical, human-interpretable rules. We instantiate our framework on three tasks where correctness of decisions is hard to evaluate due to either superhuman model abilities, or to otherwise missing ground truth: evaluating chess positions, forecasting future events, and making legal judgments. We show that regardless of a model's (possibly superhuman) performance on these tasks, we can discover logical inconsistencies in decision making. For example: a chess engine assigning opposing valuations to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#23398;&#20064;&#21160;&#21147;&#23398;&#20013;&#20010;&#20307;&#32676;&#20307;&#30340;&#21147;&#37327;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#35774;&#32622;&#65292;&#24182;&#20998;&#26512;&#20102;&#20960;&#20010;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#20998;&#25955;&#24335;&#21160;&#21147;&#23398;&#23478;&#26063;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#21160;&#21147;&#23398;&#19982;&#19968;&#31867;&#8220;&#38646;&#21644;&#8221;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#30340;&#32852;&#31995;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#20998;&#26512;&#36825;&#20123;&#21327;&#35758;&#30340;&#32676;&#20307;&#32423;&#36951;&#25022;&#12290;&#22312;&#24191;&#27867;&#30340;&#21442;&#25968;&#33539;&#22260;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2306.08670</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;&#23398;&#20064;&#21160;&#21147;&#23398;&#20013;&#20010;&#20307;&#32676;&#20307;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
The Power of Populations in Decentralized Learning Dynamics. (arXiv:2306.08670v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#23398;&#20064;&#21160;&#21147;&#23398;&#20013;&#20010;&#20307;&#32676;&#20307;&#30340;&#21147;&#37327;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#35774;&#32622;&#65292;&#24182;&#20998;&#26512;&#20102;&#20960;&#20010;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#20998;&#25955;&#24335;&#21160;&#21147;&#23398;&#23478;&#26063;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#21160;&#21147;&#23398;&#19982;&#19968;&#31867;&#8220;&#38646;&#21644;&#8221;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#30340;&#32852;&#31995;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#20998;&#26512;&#36825;&#20123;&#21327;&#35758;&#30340;&#32676;&#20307;&#32423;&#36951;&#25022;&#12290;&#22312;&#24191;&#27867;&#30340;&#21442;&#25968;&#33539;&#22260;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#27425;&#32447;&#24615;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#22810;&#33218;&#36172;&#21338;&#26426;&#35774;&#32622;&#65292;&#22312;&#19968;&#20010;&#30001;$n$&#20010;&#21463;&#20869;&#23384;&#38480;&#21046;&#30340;&#33410;&#28857;&#32452;&#25104;&#30340;&#31181;&#32676;&#20013;&#65292;&#37319;&#29992;&#20102;&#35875;&#35328;&#27169;&#22411;&#65306;&#27599;&#36718;&#65292;&#27599;&#20010;&#33410;&#28857;&#26412;&#22320;&#37319;&#29992;$m$&#20010;&#33218;&#20043;&#19968;&#65292;&#35266;&#23519;&#20174;&#33218;&#30340;&#65288;&#23545;&#25239;&#36873;&#25321;&#30340;&#65289;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#22870;&#21169;&#65292;&#28982;&#21518;&#19982;&#38543;&#26426;&#25277;&#21462;&#30340;&#37051;&#23621;&#36827;&#34892;&#36890;&#20449;&#65292;&#20132;&#25442;&#20449;&#24687;&#65292;&#20197;&#30830;&#23450;&#19979;&#19968;&#36718;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#20171;&#32461;&#24182;&#20998;&#26512;&#20102;&#20960;&#20010;&#38024;&#23545;&#27492;&#20219;&#21153;&#30340;&#20998;&#25955;&#24335;&#21160;&#21147;&#23398;&#23478;&#26063;&#65306;&#27599;&#20010;&#33410;&#28857;&#30340;&#20915;&#31574;&#23436;&#20840;&#26159;&#23616;&#37096;&#30340;&#65292;&#21482;&#20381;&#36182;&#20110;&#20854;&#26368;&#26032;&#33719;&#24471;&#30340;&#22870;&#21169;&#20197;&#21450;&#23427;&#25277;&#26679;&#30340;&#37051;&#23621;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20998;&#25955;&#24335;&#21160;&#21147;&#23398;&#30340;&#20840;&#23616;&#28436;&#21270;&#19982;&#29305;&#23450;&#31867;&#22411;&#30340;&#8220;&#38646;&#21644;&#8221;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#31639;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#19988;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#26512;&#36825;&#20123;&#33258;&#28982;&#21327;&#35758;&#30340;&#32676;&#20307;&#32423;&#36951;&#25022;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#21442;&#25968;&#33539;&#22260;&#65288;&#21363;&#65292;&#31181;&#32676;&#30340;&#22823;&#23567;&#21644;nu&#30340;&#22823;&#23567;&#65289;&#19979;&#25512;&#23548;&#20102;&#27425;&#32447;&#24615;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a distributed multi-armed bandit setting among a population of $n$ memory-constrained nodes in the gossip model: at each round, every node locally adopts one of $m$ arms, observes a reward drawn from the arm's (adversarially chosen) distribution, and then communicates with a randomly sampled neighbor, exchanging information to determine its policy in the next round. We introduce and analyze several families of dynamics for this task that are decentralized: each node's decision is entirely local and depends only on its most recently obtained reward and that of the neighbor it sampled. We show a connection between the global evolution of these decentralized dynamics with a certain class of "zero-sum" multiplicative weights update algorithms, and we develop a general framework for analyzing the population-level regret of these natural protocols. Using this framework, we derive sublinear regret bounds under a wide range of parameter regimes (i.e., the size of the population and nu
&lt;/p&gt;</description></item><item><title>&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;ArtWhisperer&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#20154;&#20204;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#26469;&#29983;&#25104;&#21644;&#30446;&#26631;&#22270;&#20687;&#31867;&#20284;&#30340;&#22270;&#20687;&#65292;&#24182;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20132;&#20114;&#35760;&#24405;&#12290;&#22312;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.08141</link><description>&lt;p&gt;
ArtWhisperer&#65306;&#19968;&#20010;&#29992;&#20110;&#25551;&#36848;&#33402;&#26415;&#21019;&#20316;&#20013;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations. (arXiv:2306.08141v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08141
&lt;/p&gt;
&lt;p&gt;
&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;ArtWhisperer&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#20154;&#20204;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#26469;&#29983;&#25104;&#21644;&#30446;&#26631;&#22270;&#20687;&#31867;&#20284;&#30340;&#22270;&#20687;&#65292;&#24182;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20132;&#20114;&#35760;&#24405;&#12290;&#22312;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#36234;&#26469;&#36234;&#26222;&#21450;&#65292;&#30740;&#31350;&#20154;&#31867;&#29992;&#25143;&#22914;&#20309;&#19982;&#36825;&#20123;&#27169;&#22411;&#20132;&#20114;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20154;&#20204;&#22914;&#20309;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#27169;&#22411;&#29983;&#25104;&#25152;&#38656;&#30340;&#30446;&#26631;&#22270;&#20687;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#20132;&#20114;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;ArtWhisperer&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#29992;&#25143;&#20250;&#24471;&#21040;&#19968;&#20010;&#30446;&#26631;&#22270;&#20687;&#65292;&#24182;&#38656;&#35201;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#20197;&#20415;&#29983;&#25104;&#31867;&#20284;&#30446;&#26631;&#22270;&#20687;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#36825;&#20010;&#28216;&#25103;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#20132;&#20114;&#30340;&#35760;&#24405;&#65307;&#27599;&#20010;&#20132;&#20114;&#37117;&#23545;&#24212;&#30528;&#29992;&#25143;&#21019;&#24314;&#30340;&#19968;&#20010;&#25552;&#31034;&#35789;&#21644;&#30456;&#24212;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#22823;&#22810;&#25968;&#35760;&#24405;&#37117;&#26159;&#37325;&#22797;&#30340;&#20132;&#20114;&#65292;&#29992;&#25143;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#25214;&#21040;&#26368;&#20339;&#30340;&#25552;&#31034;&#35789;&#20197;&#29983;&#25104;&#30446;&#26631;&#22270;&#20687;&#65292;&#36825;&#20351;&#24471;&#36825;&#20010;&#25968;&#25454;&#38598;&#25104;&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#29420;&#29305;&#36830;&#32493;&#25968;&#25454;&#38598;&#12290;&#22312;&#23545;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#25552;&#31034;&#35789;&#20132;&#20114;&#21644;&#29992;&#25143;&#31574;&#30053;&#30340;&#29305;&#24449;&#12290;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
As generative AI becomes more prevalent, it is important to study how human users interact with such models. In this work, we investigate how people use text-to-image models to generate desired target images. To study this interaction, we created ArtWhisperer, an online game where users are given a target image and are tasked with iteratively finding a prompt that creates a similar-looking image as the target. Through this game, we recorded over 50,000 human-AI interactions; each interaction corresponds to one text prompt created by a user and the corresponding generated image. The majority of these are repeated interactions where a user iterates to find the best prompt for their target image, making this a unique sequential dataset for studying human-AI collaborations. In an initial analysis of this dataset, we identify several characteristics of prompt interactions and user strategies. People submit diverse prompts and are able to discover a variety of text descriptions that generate
&lt;/p&gt;</description></item><item><title>Kepler&#26159;&#19968;&#31181;&#24555;&#36895;&#21442;&#25968;&#26597;&#35810;&#20248;&#21270;&#30340;&#40065;&#26834;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#23454;&#38469;&#25191;&#34892;&#25968;&#25454;&#30340;&#35780;&#20272;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26368;&#24555;&#35745;&#21010;&#65292;Kepler&#33021;&#22815;&#22312;&#26597;&#35810;&#24310;&#36831;&#19978;&#26174;&#33879;&#21152;&#36895;&#65292;&#36991;&#20813;&#26597;&#35810;&#24615;&#33021;&#22238;&#24402;&#12290;</title><link>http://arxiv.org/abs/2306.06798</link><description>&lt;p&gt;
Kepler: &#24555;&#36895;&#21442;&#25968;&#26597;&#35810;&#20248;&#21270;&#30340;&#40065;&#26834;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Kepler: Robust Learning for Faster Parametric Query Optimization. (arXiv:2306.06798v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06798
&lt;/p&gt;
&lt;p&gt;
Kepler&#26159;&#19968;&#31181;&#24555;&#36895;&#21442;&#25968;&#26597;&#35810;&#20248;&#21270;&#30340;&#40065;&#26834;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#23454;&#38469;&#25191;&#34892;&#25968;&#25454;&#30340;&#35780;&#20272;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#26368;&#24555;&#35745;&#21010;&#65292;Kepler&#33021;&#22815;&#22312;&#26597;&#35810;&#24310;&#36831;&#19978;&#26174;&#33879;&#21152;&#36895;&#65292;&#36991;&#20813;&#26597;&#35810;&#24615;&#33021;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21442;&#25968;&#26597;&#35810;&#20248;&#21270;&#65288;PQO&#65289;&#25216;&#26415;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;&#26597;&#35810;&#20248;&#21270;&#22120;&#25104;&#26412;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#19981;&#20934;&#30830;&#65292;&#23548;&#33268;&#26597;&#35810;&#24615;&#33021;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Kepler&#65292;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;PQO&#26041;&#27861;&#65292;&#22312;&#26597;&#35810;&#24310;&#36831;&#26041;&#38754;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#21152;&#36895;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#34892;&#35745;&#25968;&#28436;&#36827;&#65288;RCE&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#23376;&#35745;&#21010;&#22522;&#25968;&#31354;&#38388;&#25200;&#21160;&#30340;&#26032;&#22411;&#35745;&#21010;&#29983;&#25104;&#31639;&#27861;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#20934;&#30830;&#30340;&#25104;&#26412;&#27169;&#22411;&#65292;&#32780;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#23454;&#38469;&#25191;&#34892;&#25968;&#25454;&#23545;&#20505;&#36873;&#35745;&#21010;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#32473;&#23450;&#21442;&#25968;&#32465;&#23450;&#20540;&#30340;&#26368;&#24555;&#35745;&#21010;&#65292;&#32469;&#36807;&#20102;&#36825;&#19968;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20197;&#40065;&#26834;&#22320;&#39044;&#27979;&#26356;&#24555;&#30340;&#35745;&#21010;&#65292;&#36991;&#20813;&#26597;&#35810;&#24615;&#33021;&#22238;&#24402;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#65292;Kepler &#22312;&#22810;&#20010; PostgreSQL &#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26597;&#35810;&#36816;&#34892;&#26102;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing parametric query optimization (PQO) techniques rely on traditional query optimizer cost models, which are often inaccurate and result in suboptimal query performance. We propose Kepler, an end-to-end learning-based approach to PQO that demonstrates significant speedups in query latency over a traditional query optimizer. Central to our method is Row Count Evolution (RCE), a novel plan generation algorithm based on perturbations in the sub-plan cardinality space. While previous approaches require accurate cost models, we bypass this requirement by evaluating candidate plans via actual execution data and training an ML model to predict the fastest plan given parameter binding values. Our models leverage recent advances in neural network uncertainty in order to robustly predict faster plans while avoiding regressions in query performance. Experimentally, we show that Kepler achieves significant improvements in query runtime on multiple datasets on PostgreSQL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;Probabilistic Reweighting&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#33258;&#28982;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.06599</link><description>&lt;p&gt;
&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;(Variational Imbalanced Regression)
&lt;/p&gt;
&lt;p&gt;
Variational Imbalanced Regression. (arXiv:2306.06599v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;Probabilistic Reweighting&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#33258;&#28982;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#26102;&#65292;&#29616;&#26377;&#30340;&#22238;&#24402;&#27169;&#22411;&#24448;&#24448;&#22312;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#8212;&#8212;&#21464;&#20998;&#19981;&#24179;&#34913;&#22238;&#24402;&#65288;VIR&#65289;&#65292;&#23427;&#19981;&#20165;&#22312;&#19981;&#24179;&#34913;&#22238;&#24402;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#33258;&#28982;&#22320;&#20135;&#29983;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#19982;&#20856;&#22411;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20551;&#35774;I.I.D.&#34920;&#31034;&#65288;&#25968;&#25454;&#28857;&#30340;&#34920;&#31034;&#19981;&#30452;&#25509;&#21463;&#20854;&#20182;&#25968;&#25454;&#28857;&#30340;&#24433;&#21709;&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;VIR&#20511;&#29992;&#20855;&#26377;&#31867;&#20284;&#22238;&#24402;&#26631;&#31614;&#30340;&#25968;&#25454;&#26469;&#35745;&#31639;&#28508;&#22312;&#34920;&#31034;&#30340;&#21464;&#20998;&#20998;&#24067;&#65307;&#27492;&#22806;&#65292;&#19981;&#21516;&#20110;&#20135;&#29983;&#28857;&#20272;&#35745;&#30340;&#30830;&#23450;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292; VIR&#39044;&#27979;&#25972;&#20010;&#27491;&#24577;&#21453;-&#20285;&#29595;&#20998;&#24067;&#24182;&#35843;&#33410;&#30456;&#20851;&#32852;&#30340;&#20849;&#36717;&#20998;&#24067;&#65292;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#26045;&#21152;&#27010;&#29575;&#37325;&#26032;&#21152;&#26435;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#22312;&#20960;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing regression models tend to fall short in both accuracy and uncertainty estimation when the label distribution is imbalanced. In this paper, we propose a probabilistic deep learning model, dubbed variational imbalanced regression (VIR), which not only performs well in imbalanced regression but naturally produces reasonable uncertainty estimation as a byproduct. Different from typical variational autoencoders assuming I.I.D. representations (a data point's representation is not directly affected by other data points), our VIR borrows data with similar regression labels to compute the latent representation's variational distribution; furthermore, different from deterministic regression models producing point estimates, VIR predicts the entire normal-inverse-gamma distributions and modulates the associated conjugate distributions to impose probabilistic reweighting on the imbalanced data, thereby providing better uncertainty estimation. Experiments in several real-world datasets sh
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21463;&#21040;&#35821;&#35328;&#25351;&#23548;&#30340;&#22330;&#26223;&#32423;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#30495;&#23454;&#19988;&#21487;&#25511;&#30340;&#20132;&#36890;&#65292;&#24182;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#29992;&#25143;&#30340;&#26597;&#35810;&#36716;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#26597;&#35810;&#26465;&#20214;&#30340;&#20223;&#30495;&#20132;&#36890;&#12290;</title><link>http://arxiv.org/abs/2306.06344</link><description>&lt;p&gt;
&#35821;&#35328;&#25351;&#23548;&#19979;&#30340;&#22330;&#26223;&#32423;&#20132;&#36890;&#20223;&#30495;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Language-Guided Traffic Simulation via Scene-Level Diffusion. (arXiv:2306.06344v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06344
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21463;&#21040;&#35821;&#35328;&#25351;&#23548;&#30340;&#22330;&#26223;&#32423;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#30495;&#23454;&#19988;&#21487;&#25511;&#30340;&#20132;&#36890;&#65292;&#24182;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#29992;&#25143;&#30340;&#26597;&#35810;&#36716;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#26597;&#35810;&#26465;&#20214;&#30340;&#20223;&#30495;&#20132;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#30495;&#23454;&#21644;&#21487;&#25511;&#30340;&#20132;&#36890;&#20223;&#30495;&#26159;&#21152;&#36895;&#33258;&#20027;&#39550;&#39542;&#27773;&#36710;&#65288;AV&#65289;&#21457;&#23637;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#29992;&#20110;&#25511;&#21046;&#22522;&#20110;&#23398;&#20064;&#30340;&#20132;&#36890;&#27169;&#22411;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#23545;&#20110;&#20174;&#19994;&#32773;&#26469;&#35828;&#24456;&#38590;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CTG++&#65292;&#19968;&#31181;&#21487;&#20197;&#21463;&#21040;&#35821;&#35328;&#25351;&#23548;&#30340;&#22330;&#26223;&#32423;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#38656;&#35201;&#35299;&#20915;&#20004;&#20010;&#38382;&#39064;&#65306;&#38656;&#35201;&#19968;&#20010;&#30495;&#23454;&#21644;&#21487;&#25511;&#30340;&#20132;&#36890;&#27169;&#22411;&#39592;&#24178;&#32467;&#26500;&#65292;&#24182;&#19988;&#35201;&#26377;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#20351;&#29992;&#35821;&#35328;&#19982;&#20132;&#36890;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#26102;&#31354;&#36716;&#25442;&#22120;&#39592;&#24178;&#32467;&#26500;&#30340;&#22330;&#26223;&#32423;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#29983;&#25104;&#20102;&#30495;&#23454;&#21644;&#21487;&#25511;&#30340;&#20132;&#36890;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23558;&#29992;&#25143;&#30340;&#26597;&#35810;&#36716;&#25442;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#26397;&#30528;&#26597;&#35810;&#21512;&#35268;&#30340;&#29983;&#25104;&#26041;&#21521;&#21069;&#36827;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Realistic and controllable traffic simulation is a core capability that is necessary to accelerate autonomous vehicle (AV) development. However, current approaches for controlling learning-based traffic models require significant domain expertise and are difficult for practitioners to use. To remedy this, we present CTG++, a scene-level conditional diffusion model that can be guided by language instructions. Developing this requires tackling two challenges: the need for a realistic and controllable traffic model backbone, and an effective method to interface with a traffic model using language. To address these challenges, we first propose a scene-level diffusion model equipped with a spatio-temporal transformer backbone, which generates realistic and controllable traffic. We then harness a large language model (LLM) to convert a user's query into a loss function, guiding the diffusion model towards query-compliant generation. Through comprehensive evaluation, we demonstrate the effect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#22522;&#30784;&#65292;&#21363;&#20854;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#27491;&#21521;&#36807;&#31243;&#12289;&#36870;&#21521;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#32454;&#31890;&#24230;&#36879;&#35270;&#12290;</title><link>http://arxiv.org/abs/2306.04542</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#22522;&#30784;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
On the Design Fundamentals of Diffusion Models: A Survey. (arXiv:2306.04542v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#35774;&#35745;&#22522;&#30784;&#65292;&#21363;&#20854;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#27491;&#21521;&#36807;&#31243;&#12289;&#36870;&#21521;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#32454;&#31890;&#24230;&#36879;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#21644;&#21024;&#38500;&#22122;&#22768;&#26469;&#23398;&#20064;&#35757;&#32451;&#25968;&#25454;&#30340;&#28508;&#22312;&#20998;&#24067;&#20197;&#29983;&#25104;&#25968;&#25454;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#32452;&#25104;&#37096;&#20998;&#24050;&#32463;&#21463;&#21040;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#35768;&#22810;&#35774;&#35745;&#36873;&#25321;&#34987;&#25552;&#20986;&#12290;&#29616;&#26377;&#30340;&#35780;&#35770;&#20027;&#35201;&#20851;&#27880;&#39640;&#23618;&#27425;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23545;&#32452;&#20214;&#30340;&#35774;&#35745;&#22522;&#30784;&#35206;&#30422;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#20840;&#38754;&#32780;&#36830;&#36143;&#30340;&#32508;&#36848;&#65292;&#38024;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#32452;&#20214;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20998;&#26512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#32508;&#36848;&#25353;&#29031;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#36827;&#34892;&#32452;&#32455;&#65292;&#21363;&#27491;&#21521;&#36807;&#31243;&#12289;&#36870;&#21521;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#25552;&#20379;&#25193;&#25955;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#36879;&#35270;&#65292;&#26377;&#21161;&#20110;&#26410;&#26469;&#30740;&#31350;&#20998;&#26512;&#20010;&#20307;&#32452;&#20214;&#12289;&#35774;&#35745;&#36873;&#25321;&#30340;&#36866;&#29992;&#24615;&#20197;&#21450;&#25193;&#25955;&#27169;&#22411;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are generative models, which gradually add and remove noise to learn the underlying distribution of training data for data generation. The components of diffusion models have gained significant attention with many design choices proposed. Existing reviews have primarily focused on higher-level solutions, thereby covering less on the design fundamentals of components. This study seeks to address this gap by providing a comprehensive and coherent review on component-wise design choices in diffusion models. Specifically, we organize this review according to their three key components, namely the forward process, the reverse process, and the sampling procedure. This allows us to provide a fine-grained perspective of diffusion models, benefiting future studies in the analysis of individual components, the applicability of design choices, and the implementation of diffusion models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807; MASCHInE &#25429;&#25417;&#35821;&#20041;&#23398;&#20064;&#30693;&#35782;&#22270;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#29983;&#25104;&#21407;&#22411;&#22270;&#24182;&#21033;&#29992;&#20854;&#35821;&#20041;&#65292;&#36827;&#32780;&#35757;&#32451;&#20986;&#26356;&#22909;&#22320;&#25429;&#25417;&#35821;&#20041;&#30340; KGEs&#12290;</title><link>http://arxiv.org/abs/2306.03659</link><description>&lt;p&gt;
Schema First&#65281;&#36890;&#36807;MASCHInE&#25429;&#25417;&#35821;&#20041;&#23398;&#20064;&#36890;&#29992;&#30693;&#35782;&#22270;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Schema First! Learn Versatile Knowledge Graph Embeddings by Capturing Semantics with MASCHInE. (arXiv:2306.03659v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807; MASCHInE &#25429;&#25417;&#35821;&#20041;&#23398;&#20064;&#30693;&#35782;&#22270;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#29983;&#25104;&#21407;&#22411;&#22270;&#24182;&#21033;&#29992;&#20854;&#35821;&#20041;&#65292;&#36827;&#32780;&#35757;&#32451;&#20986;&#26356;&#22909;&#22320;&#25429;&#25417;&#35821;&#20041;&#30340; KGEs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#65288;KGEMs&#65289;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#20102;&#30693;&#35782;&#22270;&#20013;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#21363;&#30693;&#35782;&#22270;&#23884;&#20837;&#65288;KGEs&#65289;&#12290;&#23398;&#20064;&#22810;&#21151;&#33021;&#30340;KGEs&#38750;&#24120;&#26377;&#24847;&#20041;&#65292;&#22240;&#20026;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#19978;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;KGEMs&#36890;&#24120;&#26159;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#30340;&#23884;&#20837;&#26159;&#20219;&#21153;&#30456;&#20851;&#30340;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20851;&#20110;KGEMs&#23454;&#38469;&#19978;&#26159;&#21542;&#21019;&#24314;&#20102;&#24213;&#23618;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#35821;&#20041;&#34920;&#31034;&#65288;&#20363;&#22914;&#65292;&#23558;&#30456;&#20284;&#30340;&#23454;&#20307;&#25918;&#22312;&#19968;&#36215;&#65292;&#23558;&#19981;&#30456;&#20284;&#30340;&#23454;&#20307;&#25918;&#22312;&#19968;&#36215;&#65289;&#30340;&#26222;&#36941;&#20551;&#35774;&#21463;&#21040;&#20102;&#36136;&#30097;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#29983;&#25104;&#21407;&#22411;&#22270;-&#19968;&#20010;&#23567;&#22411;&#12289;&#20462;&#25913;&#36807;&#30340;KG&#29256;&#26412;&#65292;&#21033;&#29992;&#20102;RDF/S&#20449;&#24687;&#12290;&#25152;&#23398;&#20064;&#30340;&#22522;&#20110;&#21407;&#22411;&#22270;&#30340;&#23884;&#20837;&#26088;&#22312;&#23553;&#35013;KG&#30340;&#35821;&#20041;&#65292;&#24182;&#21487;&#20197;&#22312;&#23398;&#20064;KGEs&#26102;&#21152;&#20197;&#21033;&#29992;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25429;&#25417;&#35821;&#20041;&#12290;&#23545;&#21508;&#31181;&#35780;&#20272;&#22522;&#20934;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding models (KGEMs) have gained considerable traction in recent years. These models learn a vector representation of knowledge graph entities and relations, a.k.a. knowledge graph embeddings (KGEs). Learning versatile KGEs is desirable as it makes them useful for a broad range of tasks. However, KGEMs are usually trained for a specific task, which makes their embeddings task-dependent. In parallel, the widespread assumption that KGEMs actually create a semantic representation of the underlying entities and relations (e.g., project similar entities closer than dissimilar ones) has been challenged. In this work, we design heuristics for generating protographs -small, modified versions of a KG that leverage RDF/S information. The learnt protograph-based embeddings are meant to encapsulate the semantics of a KG, and can be leveraged in learning KGEs that, in turn, also better capture semantics. Extensive experiments on various evaluation benchmarks demonstrate the so
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.00477</link><description>&lt;p&gt;
&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#21487;&#36870;&#24615;&#65306;&#20174;&#21442;&#25968;&#21040;&#20869;&#23384;&#39640;&#25928;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38750;&#24120;&#25104;&#21151;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35757;&#32451;&#23569;&#37327;&#21442;&#25968;&#32780;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#65292;&#24182;&#38543;&#30528;PLM&#36234;&#26469;&#36234;&#22823;&#32780;&#25104;&#20026;&#20107;&#23454;&#19978;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PEFT&#26041;&#27861;&#19981;&#20855;&#22791;&#20869;&#23384;&#25928;&#29575;&#65292;&#22240;&#20026;&#23427;&#20204;&#20173;&#38656;&#35201;&#23384;&#20648;&#22823;&#37096;&#20998;&#20013;&#38388;&#28608;&#27963;&#20540;&#20197;&#20415;&#35745;&#31639;&#26799;&#24230;&#65292;&#31867;&#20284;&#20110;&#24494;&#35843;&#12290;&#19968;&#20010;&#20943;&#23569;&#28608;&#27963;&#20869;&#23384;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#24212;&#29992;&#21487;&#36870;&#27169;&#22411;&#65292;&#36825;&#26679;&#20013;&#38388;&#28608;&#27963;&#20540;&#23601;&#26080;&#38656;&#32531;&#23384;&#65292;&#21487;&#20197;&#37325;&#26032;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#23558;PLM&#20462;&#25913;&#20026;&#23427;&#30340;&#21487;&#36870;&#21464;&#20307;&#24182;&#36827;&#34892;PEFT&#24182;&#19981;&#26159;&#19968;&#20214;&#23481;&#26131;&#30340;&#20107;&#65292;&#22240;&#20026;&#21487;&#36870;&#27169;&#22411;&#20855;&#26377;&#19982;&#24403;&#21069;&#21457;&#24067;&#30340;PLM&#19981;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#25991;&#39318;&#20808;&#35843;&#26597;&#29616;&#26377;PEFT&#26041;&#27861;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#35748;&#35782;&#21040;&#22312;&#21021;&#22987;&#21270;PEFT&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant with PEFT is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT 
&lt;/p&gt;</description></item><item><title>AdANNS&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#35821;&#20041;&#25628;&#32034;&#26694;&#26550;&#65292;&#21033;&#29992;&#19981;&#21516;&#23481;&#37327;&#30340;&#33258;&#36866;&#24212;&#34920;&#31034;&#24418;&#24335;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#31934;&#24230;-&#35745;&#31639;&#25240;&#34935;&#26435;&#34913;&#65292;&#30456;&#20284;&#24230;&#35745;&#31639;&#36234;&#25509;&#36817;&#30340;&#25968;&#25454;&#28857;&#23558;&#20351;&#29992;&#26356;&#20302;&#23481;&#37327;&#30340;&#34920;&#31034;&#24418;&#24335;&#36827;&#34892;&#35745;&#31639;&#65292;&#28436;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;-&#35745;&#31639;&#25240;&#34935;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.19435</link><description>&lt;p&gt;
AdANNS: &#19968;&#31181;&#33258;&#36866;&#24212;&#35821;&#20041;&#25628;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AdANNS: A Framework for Adaptive Semantic Search. (arXiv:2305.19435v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19435
&lt;/p&gt;
&lt;p&gt;
AdANNS&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#35821;&#20041;&#25628;&#32034;&#26694;&#26550;&#65292;&#21033;&#29992;&#19981;&#21516;&#23481;&#37327;&#30340;&#33258;&#36866;&#24212;&#34920;&#31034;&#24418;&#24335;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#31934;&#24230;-&#35745;&#31639;&#25240;&#34935;&#26435;&#34913;&#65292;&#30456;&#20284;&#24230;&#35745;&#31639;&#36234;&#25509;&#36817;&#30340;&#25968;&#25454;&#28857;&#23558;&#20351;&#29992;&#26356;&#20302;&#23481;&#37327;&#30340;&#34920;&#31034;&#24418;&#24335;&#36827;&#34892;&#35745;&#31639;&#65292;&#28436;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;-&#35745;&#31639;&#25240;&#34935;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#35268;&#27169;&#30340;&#25628;&#32034;&#31995;&#32479;&#23398;&#20064;&#19968;&#20010;&#32534;&#30721;&#22120;&#26469;&#23884;&#20837;&#19968;&#20010;&#32473;&#23450;&#30340;&#26597;&#35810;&#65292;&#28982;&#21518;&#23558;&#20854;&#36830;&#25509;&#21040;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;(ANNS)&#31649;&#36947;&#20013;&#26469;&#26816;&#32034;&#30456;&#20284;&#30340;&#25968;&#25454;&#28857;&#12290;&#20026;&#20102;&#20934;&#30830;&#22320;&#25429;&#25417;&#23614;&#37096;&#26597;&#35810;&#21644;&#25968;&#25454;&#28857;&#65292;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#36890;&#24120;&#26159;&#21018;&#24615;&#30340;&#12289;&#39640;&#32500;&#30340;&#21521;&#37327;&#65292;&#36890;&#24120;&#22312;&#25972;&#20010;ANNS&#31649;&#36947;&#20013;&#19968;&#25104;&#19981;&#21464;&#65292;&#24182;&#19988;&#21487;&#33021;&#23548;&#33268;&#35745;&#31639;&#19978;&#26114;&#36149;&#30340;&#26816;&#32034;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#19982;&#20854;&#20351;&#29992;&#21018;&#24615;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;ANNS&#30340;&#19981;&#21516;&#38454;&#27573;&#21487;&#20197;&#21033;&#29992;&#19981;&#21516;&#23481;&#37327;&#30340;&#33258;&#36866;&#24212;&#34920;&#31034;&#24418;&#24335;&#20197;&#33719;&#24471;&#26174;&#33879;&#30340;&#31934;&#24230;-&#35745;&#31639;&#25240;&#34935;&#26435;&#34913;&#65292;&#21363;&#21487;&#20197;&#36827;&#34892;&#26356;&#21152;&#36817;&#20284;&#35745;&#31639;&#30340;ANNS&#38454;&#27573;&#24212;&#35813;&#20351;&#29992;&#30456;&#21516;&#25968;&#25454;&#28857;&#30340;&#20302;&#23481;&#37327;&#34920;&#31034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AdANNS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;ANNS&#35774;&#35745;&#26694;&#26550;&#65292;&#26126;&#30830;&#21033;&#29992;Matryoshka&#34920;&#31034;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;AdANNS&#30340;&#26032;&#22411;&#20851;&#38190;ANNS&#26500;&#24314;&#28436;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;-&#35745;&#31639;&#25240;&#34935;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Web-scale search systems learn an encoder to embed a given query which is then hooked into an approximate nearest neighbor search (ANNS) pipeline to retrieve similar data points. To accurately capture tail queries and data points, learned representations typically are rigid, high-dimensional vectors that are generally used as-is in the entire ANNS pipeline and can lead to computationally expensive retrieval. In this paper, we argue that instead of rigid representations, different stages of ANNS can leverage adaptive representations of varying capacities to achieve significantly better accuracy-compute trade-offs, i.e., stages of ANNS that can get away with more approximate computation should use a lower-capacity representation of the same data point. To this end, we introduce AdANNS, a novel ANNS design framework that explicitly leverages the flexibility of Matryoshka Representations. We demonstrate state-of-the-art accuracy-compute trade-offs using novel AdANNS-based key ANNS building
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;LSTM&#21644;BLSTM&#20004;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#30005;&#21147;&#28040;&#32791;&#30701;&#26399;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22235;&#20010;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;BLSTM&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.16546</link><description>&lt;p&gt;
&#27604;&#36739;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#21452;&#21521;LSTM&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#30005;&#21147;&#28040;&#32791;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Comparing Long Short-Term Memory (LSTM) and Bidirectional LSTM Deep Neural Networks for power consumption prediction. (arXiv:2305.16546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;LSTM&#21644;BLSTM&#20004;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#30005;&#21147;&#28040;&#32791;&#30701;&#26399;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#22235;&#20010;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;BLSTM&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#28040;&#32791;&#39044;&#27979;&#26041;&#27861;&#26159;&#20026;&#20102;&#20915;&#31574;&#33410;&#33021;&#20197;&#21450;&#22312;&#33021;&#28304;&#24066;&#22330;&#20013;&#39044;&#27979;&#38656;&#27714;&#31561;&#22810;&#31181;&#21407;&#22240;&#32780;&#36827;&#34892;&#30740;&#31350;&#30340;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#27604;&#36739;&#20004;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#21452;&#21521;LSTM&#65288;BLSTM&#65289;&#65292;&#29992;&#20110;&#21333;&#21464;&#37327;&#30005;&#24230;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#30701;&#26399;&#39044;&#27979;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36873;&#25321;&#20102;&#22235;&#20010;&#19981;&#21516;&#19978;&#19979;&#25991;&#21644;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#22235;&#20010;&#25968;&#25454;&#38598;&#20998;&#21035;&#26159;&#65306;&#65288;a&#65289;&#27861;&#22269;&#23478;&#24237;&#30340;&#29992;&#30005;&#37327;&#65307;&#65288;b&#65289;&#24052;&#35199;Santa&#233;m&#30340;&#19968;&#24231;&#22823;&#23398;&#24314;&#31569;&#30340;&#29992;&#30005;&#37327;&#65307;&#65288;c&#65289;&#25705;&#27931;&#21733;T&#233;touan&#24066;&#30340;&#29992;&#30005;&#38656;&#27714;&#65307;&#65288;d&#65289;&#26032;&#21152;&#22369;&#32858;&#21512;&#30005;&#21147;&#38656;&#27714;&#12290;&#37319;&#29992;&#26102;&#38388;&#24207;&#21015;&#20132;&#21449;&#39564;&#35777;&#26041;&#26696;&#35745;&#31639;&#20102;RMSE&#12289;MAE&#12289;MAPE&#21644;R2&#31561;&#25351;&#26631;&#12290;&#23545;&#24402;&#19968;&#21270;RMSE&#65288;NRMSE&#65289;&#30340;&#32467;&#26524;&#24212;&#29992;&#20102;Friedman&#26816;&#39564;&#65292;&#34920;&#26126;BLSTM&#27604;LSTM&#34920;&#29616;&#26356;&#22909;&#24182;&#19988;&#20855;&#26377;&#32479;&#35745;&#26174;&#33879;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electric consumption prediction methods are investigated for many reasons such as decision-making related to energy efficiency as well as for anticipating demand in the energy market dynamics. The objective of the present work is the comparison between two Deep Learning models, namely the Long Short-Term Memory (LSTM) and Bi-directional LSTM (BLSTM) for univariate electric consumption Time Series (TS) short-term forecast. The Data Sets (DSs) were selected for their different contexts and scales, aiming the assessment of the models' robustness. Four DSs were used, related to the power consumption of: (a) a household in France; (b) a university building in Santar\'em, Brazil; (c) the T\'etouan city zones, in Morocco; and (c) the Singapore aggregated electric demand. The metrics RMSE, MAE, MAPE and R2 were calculated in a TS cross-validation scheme. The Friedman's test was applied to normalized RMSE (NRMSE) results, showing that BLSTM outperforms LSTM with statistically significant differ
&lt;/p&gt;</description></item><item><title>Voyager&#26159;&#19968;&#20010;&#22312;Minecraft&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#24335;&#25506;&#32034;&#21644;&#23398;&#20064;&#30340;&#20195;&#29702;&#65292;&#23427;&#36890;&#36807;&#33258;&#21160;&#35838;&#31243;&#35774;&#32622;&#12289;&#21487;&#25191;&#34892;&#20195;&#30721;&#25216;&#33021;&#24211;&#21644;&#36845;&#20195;&#25552;&#31034;&#26426;&#21046;&#19981;&#26029;&#25552;&#21319;&#33258;&#24049;&#30340;&#33021;&#21147;&#65292;&#24182;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#32456;&#36523;&#23398;&#20064;&#33021;&#21147;&#21644;&#22312;&#29609;Minecraft&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.16291</link><description>&lt;p&gt;
Voyager:&#19968;&#20010;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#24335;&#26426;&#22120;&#20307;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Voyager: An Open-Ended Embodied Agent with Large Language Models. (arXiv:2305.16291v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16291
&lt;/p&gt;
&lt;p&gt;
Voyager&#26159;&#19968;&#20010;&#22312;Minecraft&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#24335;&#25506;&#32034;&#21644;&#23398;&#20064;&#30340;&#20195;&#29702;&#65292;&#23427;&#36890;&#36807;&#33258;&#21160;&#35838;&#31243;&#35774;&#32622;&#12289;&#21487;&#25191;&#34892;&#20195;&#30721;&#25216;&#33021;&#24211;&#21644;&#36845;&#20195;&#25552;&#31034;&#26426;&#21046;&#19981;&#26029;&#25552;&#21319;&#33258;&#24049;&#30340;&#33021;&#21147;&#65292;&#24182;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#32456;&#36523;&#23398;&#20064;&#33021;&#21147;&#21644;&#22312;&#29609;Minecraft&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Voyager&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;Minecraft&#20013;&#25345;&#32493;&#25506;&#32034;&#19990;&#30028;&#12289;&#33719;&#24471;&#22810;&#31181;&#25216;&#33021;&#21644;&#36827;&#34892;&#26032;&#30340;&#21457;&#29616;&#32780;&#26080;&#38656;&#20154;&#31867;&#24178;&#39044;&#30340;&#31532;&#19968;&#20010;LLM&#39537;&#21160;&#30340;&#20840;&#33021;&#20195;&#29702;&#12290;Voyager&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;1)&#26368;&#22823;&#21270;&#25506;&#32034;&#30340;&#33258;&#21160;&#35838;&#31243;&#35774;&#32622;&#65292;2)&#29992;&#20110;&#23384;&#20648;&#21644;&#26816;&#32034;&#22797;&#26434;&#34892;&#20026;&#30340;&#19981;&#26029;&#22686;&#38271;&#30340;&#21487;&#25191;&#34892;&#20195;&#30721;&#25216;&#33021;&#24211;&#65292;&#21644;3)&#19968;&#31181;&#36890;&#36807;&#29615;&#22659;&#21453;&#39304;&#12289;&#25191;&#34892;&#38169;&#35823;&#21644;&#33258;&#25105;&#39564;&#35777;&#36827;&#34892;&#31243;&#24207;&#25913;&#36827;&#30340;&#26032;&#30340;&#36845;&#20195;&#25552;&#31034;&#26426;&#21046;&#12290;Voyager&#36890;&#36807;&#40657;&#30418;&#26597;&#35810;&#19982;GPT-4&#20114;&#21160;&#65292;&#36991;&#20813;&#20102;&#27169;&#22411;&#21442;&#25968;&#24494;&#35843;&#30340;&#38656;&#35201;&#12290;Voyager&#24320;&#21457;&#30340;&#25216;&#33021;&#20855;&#26377;&#26102;&#38388;&#24310;&#38271;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#32452;&#21512;&#24615;&#65292;&#36825;&#21152;&#24555;&#20102;&#20195;&#29702;&#30340;&#33021;&#21147;&#22686;&#38271;&#24182;&#20943;&#36731;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20174;&#32463;&#39564;&#19978;&#30475;&#65292;Voyager&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#19978;&#19979;&#25991;&#22411;&#32456;&#36523;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#22312;&#29609;Minecraft&#26041;&#38754;&#23637;&#29616;&#20102;&#24322;&#24120;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique it
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#33021;&#22815;&#36890;&#36807;&#37325;&#26032;&#25277;&#26679;&#65292;&#36807;&#28388;&#25481;&#19981;&#31526;&#21512;&#26368;&#32456;&#29992;&#25143;&#36873;&#25321;&#30340;&#24230;&#37327;&#25351;&#26631;&#30340;&#21512;&#25104;&#25968;&#25454;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25928;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#21644;&#25968;&#25454;&#38598;&#36136;&#37327;</title><link>http://arxiv.org/abs/2305.15538</link><description>&lt;p&gt;
&#25913;&#21892;&#36873;&#25321;&#24615;&#24230;&#37327;&#30340;&#31169;&#26377;&#21512;&#25104;&#25968;&#25454;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Post-processing Private Synthetic Data for Improving Utility on Selected Measures. (arXiv:2305.15538v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#33021;&#22815;&#36890;&#36807;&#37325;&#26032;&#25277;&#26679;&#65292;&#36807;&#28388;&#25481;&#19981;&#31526;&#21512;&#26368;&#32456;&#29992;&#25143;&#36873;&#25321;&#30340;&#24230;&#37327;&#25351;&#26631;&#30340;&#21512;&#25104;&#25968;&#25454;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25928;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#21644;&#25968;&#25454;&#38598;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31169;&#26377;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#24573;&#30053;&#19979;&#28216;&#20219;&#21153;&#65292;&#20294;&#26159;&#26368;&#32456;&#29992;&#25143;&#21487;&#33021;&#26377;&#29305;&#23450;&#30340;&#38656;&#27714;&#65292;&#21512;&#25104;&#25968;&#25454;&#24517;&#39035;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#65292;&#21542;&#21017;&#21487;&#33021;&#20250;&#26174;&#33879;&#38477;&#20302;&#25968;&#25454;&#30340;&#19979;&#28216;&#29992;&#36884;&#25928;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#38024;&#23545;&#26368;&#32456;&#29992;&#25143;&#36873;&#25321;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#25552;&#39640;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#25928;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#35777;&#21644;&#25968;&#25454;&#38598;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#28041;&#21450;&#20174;&#21512;&#25104;&#25968;&#25454;&#20013;&#37325;&#26032;&#25277;&#26679;&#65292;&#36807;&#28388;&#25481;&#19981;&#28385;&#36275;&#25152;&#36873;&#25928;&#29992;&#24230;&#37327;&#30340;&#26679;&#26412;&#65292;&#20351;&#29992;&#26377;&#25928;&#30340;&#38543;&#26426;&#19968;&#38454;&#31639;&#27861;&#23547;&#25214;&#26368;&#20248;&#30340;&#37325;&#26032;&#25277;&#26679;&#26435;&#37325;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22987;&#32456;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#26368;&#20808;&#36827;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#31639;&#27861;&#20013;&#25552;&#39640;&#21512;&#25104;&#25968;&#25454;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing private synthetic data generation algorithms are agnostic to downstream tasks. However, end users may have specific requirements that the synthetic data must satisfy. Failure to meet these requirements could significantly reduce the utility of the data for downstream use. We introduce a post-processing technique that improves the utility of the synthetic data with respect to measures selected by the end user, while preserving strong privacy guarantees and dataset quality. Our technique involves resampling from the synthetic data to filter out samples that do not meet the selected utility measures, using an efficient stochastic first-order algorithm to find optimal resampling weights. Through comprehensive numerical experiments, we demonstrate that our approach consistently improves the utility of synthetic data across multiple benchmark datasets and state-of-the-art synthetic data generation algorithms.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#19968;&#20010;Matroid&#32422;&#26463;&#19979;&#27969;&#24335;&#23376;&#27169;&#26368;&#22823;&#21270;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#27969;&#24335;&#31639;&#27861;&#21644;&#19981;&#21487;&#33021;&#30340;&#32467;&#26524;&#26469;&#26435;&#34913;&#25928;&#29575;&#12289;&#36136;&#37327;&#21644;&#20844;&#24179;&#24615;&#65292;&#24182;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.15118</link><description>&lt;p&gt;
&#22312;&#19968;&#20010;Matroid&#32422;&#26463;&#19979;&#27969;&#24335;&#23376;&#27169;&#26368;&#22823;&#21270;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fairness in Streaming Submodular Maximization over a Matroid Constraint. (arXiv:2305.15118v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15118
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#19968;&#20010;Matroid&#32422;&#26463;&#19979;&#27969;&#24335;&#23376;&#27169;&#26368;&#22823;&#21270;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#27969;&#24335;&#31639;&#27861;&#21644;&#19981;&#21487;&#33021;&#30340;&#32467;&#26524;&#26469;&#26435;&#34913;&#25928;&#29575;&#12289;&#36136;&#37327;&#21644;&#20844;&#24179;&#24615;&#65292;&#24182;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24335;&#23376;&#27169;&#26368;&#22823;&#21270;&#26159;&#20174;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#19968;&#20010;&#20195;&#34920;&#24615;&#23376;&#38598;&#30340;&#33258;&#28982;&#27169;&#22411;&#12290;&#22914;&#26524;&#25968;&#25454;&#28857;&#20855;&#26377;&#25935;&#24863;&#23646;&#24615;&#65292;&#22914;&#24615;&#21035;&#25110;&#31181;&#26063;&#65292;&#24378;&#21046;&#20844;&#24179;&#24615;&#20197;&#36991;&#20813;&#20559;&#35265;&#21644;&#27495;&#35270;&#21464;&#24471;&#37325;&#35201;&#12290;&#36825;&#24341;&#36215;&#20102;&#23545;&#24320;&#21457;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;&#26368;&#36817;&#65292;&#36825;&#26679;&#30340;&#31639;&#27861;&#24050;&#32463;&#34987;&#24320;&#21457;&#29992;&#20110;&#22522;&#20110;&#22522;&#25968;&#32422;&#26463;&#30340;&#21333;&#35843;&#23376;&#27169;&#26368;&#22823;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#33258;&#28982;&#25512;&#24191;&#21040;&#19968;&#20010;Matroid&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27969;&#24335;&#31639;&#27861;&#20197;&#21450;&#19981;&#21487;&#33021;&#30340;&#32467;&#26524;&#65292;&#36825;&#20123;&#32467;&#26524;&#22312;&#25928;&#29575;&#12289;&#36136;&#37327;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#25552;&#20379;&#20102;&#26435;&#34913;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#30693;&#21517;&#30340;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#23545;&#25105;&#20204;&#30340;&#21457;&#29616;&#36827;&#34892;&#20102;&#32463;&#39564;&#35777;&#23454;&#65306;&#22522;&#20110;&#31034;&#20363;&#30340;&#32858;&#31867;&#12289;&#30005;&#24433;&#25512;&#33616;&#21644;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#26368;&#22823;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;
Streaming submodular maximization is a natural model for the task of selecting a representative subset from a large-scale dataset. If datapoints have sensitive attributes such as gender or race, it becomes important to enforce fairness to avoid bias and discrimination. This has spurred significant interest in developing fair machine learning algorithms. Recently, such algorithms have been developed for monotone submodular maximization under a cardinality constraint.  In this paper, we study the natural generalization of this problem to a matroid constraint. We give streaming algorithms as well as impossibility results that provide trade-offs between efficiency, quality and fairness. We validate our findings empirically on a range of well-known real-world applications: exemplar-based clustering, movie recommendation, and maximum coverage in social networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#37197;&#23545;&#25968;&#25454;&#23398;&#20064;MCR&#30340;&#26041;&#27861;&#65292;&#21483;&#20570;C-MCR&#65292;&#24182;&#19988;&#22312;&#26032;&#31354;&#38388;&#20013;&#20351;&#29992;&#37325;&#21472;&#27169;&#24577;B&#30340;&#25968;&#25454;&#26469;&#23545;&#40784;&#20004;&#20010;MCR&#12290;&#36890;&#36807;&#36825;&#20010;&#26041;&#27861;&#65292;&#38750;&#37325;&#21472;&#27169;&#24577;&#23545;&#65288;A&#65292;C&#65289;&#20063;&#21487;&#20197;&#20351;&#29992;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2305.14381</link><description>&lt;p&gt;
&#36830;&#25509;&#22810;&#27169;&#24577;&#23545;&#27604;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Connecting Multi-modal Contrastive Representations. (arXiv:2305.14381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#37197;&#23545;&#25968;&#25454;&#23398;&#20064;MCR&#30340;&#26041;&#27861;&#65292;&#21483;&#20570;C-MCR&#65292;&#24182;&#19988;&#22312;&#26032;&#31354;&#38388;&#20013;&#20351;&#29992;&#37325;&#21472;&#27169;&#24577;B&#30340;&#25968;&#25454;&#26469;&#23545;&#40784;&#20004;&#20010;MCR&#12290;&#36890;&#36807;&#36825;&#20010;&#26041;&#27861;&#65292;&#38750;&#37325;&#21472;&#27169;&#24577;&#23545;&#65288;A&#65292;C&#65289;&#20063;&#21487;&#20197;&#20351;&#29992;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23545;&#27604;&#34920;&#31034;&#65288;MCR&#65289;&#23398;&#20064;&#26088;&#22312;&#23558;&#19981;&#21516;&#30340;&#27169;&#24577;&#32534;&#30721;&#21040;&#19968;&#20010;&#35821;&#20041;&#23545;&#40784;&#30340;&#20849;&#20139;&#31354;&#38388;&#20013;&#12290;&#35813;&#33539;&#20363;&#22312;&#21508;&#31181;&#27169;&#24335;&#19979;&#30340;&#22823;&#37327;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25968;&#25454;&#23545;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#20854;&#22312;&#26356;&#22810;&#27169;&#24577;&#19978;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;&#37197;&#23545;&#25968;&#25454;&#23398;&#20064;MCR&#30340;&#35757;&#32451;&#39640;&#25928;&#26041;&#27861;&#65292;&#31216;&#20026;&#36830;&#25509;&#22810;&#27169;&#24577;&#23545;&#27604;&#34920;&#31034;&#65288;C-MCR&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#65288;A&#65292;B&#65289;&#21644;&#65288;B&#65292;C&#65289;&#27169;&#24577;&#23545;&#19978;&#39044;&#35757;&#32451;&#20004;&#20010;&#29616;&#26377;&#30340;MCR&#20043;&#21518;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#25237;&#24433;&#21040;&#19968;&#20010;&#26032;&#30340;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#37325;&#21472;&#27169;&#24577;B&#30340;&#25968;&#25454;&#26469;&#22312;&#26032;&#31354;&#38388;&#20013;&#23545;&#40784;&#20004;&#20010;MCR&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#27169;&#24577;&#23545;&#65288;A&#65292;B&#65289;&#21644;&#65288;B&#65292;C&#65289;&#22312;&#27599;&#20010;MCR&#20869;&#24050;&#32463;&#23545;&#40784;&#65292;&#22240;&#27492;&#36890;&#36807;&#37325;&#21472;&#27169;&#24577;&#23398;&#20064;&#21040;&#30340;&#36830;&#25509;&#20063;&#21487;&#20197;&#36716;&#31227;&#21040;&#38750;&#37325;&#21472;&#27169;&#24577;&#23545;&#65288;A&#65292;C&#65289;&#12290;&#20026;&#20102;&#21457;&#25381;C-MCR&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#35821;&#20041;&#22686;&#24378;&#30340;int
&lt;/p&gt;
&lt;p&gt;
Multi-modal Contrastive Representation (MCR) learning aims to encode different modalities into a semantically aligned shared space. This paradigm shows remarkable generalization ability on numerous downstream tasks across various modalities. However, the reliance on massive high-quality data pairs limits its further development on more modalities. This paper proposes a novel training-efficient method for learning MCR without paired data called Connecting Multi-modal Contrastive Representations (C-MCR). Specifically, given two existing MCRs pre-trained on (A, B) and (B, C) modality pairs, we project them to a new space and use the data from the overlapping modality B to aligning the two MCRs in the new space. Meanwhile, since the modality pairs (A, B) and (B, C) are already aligned within each MCR, the connection learned by overlapping modality can also be transferred to non-overlapping modality pair (A, C). To unleash the potential of C-MCR, we further introduce a semantic-enhanced int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;ReLU&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;Gradient Flow&#35757;&#32451;&#30340;&#20004;&#23618;&#27169;&#22411;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23436;&#25972;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22235;&#20010;&#19981;&#21516;&#38454;&#27573;&#65292;&#26174;&#31034;&#20986;&#19968;&#20010;&#20174;&#31616;&#21270;&#21040;&#22797;&#26434;&#30340;&#23398;&#20064;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.12467</link><description>&lt;p&gt;
&#29702;&#35299;ReLU&#32593;&#32476;&#30340;&#22810;&#38454;&#27573;&#20248;&#21270;&#21160;&#24577;&#21644;&#20016;&#23500;&#30340;&#38750;&#32447;&#24615;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks. (arXiv:2305.12467v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;ReLU&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;Gradient Flow&#35757;&#32451;&#30340;&#20004;&#23618;&#27169;&#22411;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23436;&#25972;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22235;&#20010;&#19981;&#21516;&#38454;&#27573;&#65292;&#26174;&#31034;&#20986;&#19968;&#20010;&#20174;&#31616;&#21270;&#21040;&#22797;&#26434;&#30340;&#23398;&#20064;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#32463;&#24120;&#34920;&#29616;&#20986;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#29616;&#35937;&#12290;&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#21644;&#25439;&#22833;&#30340;&#38750;&#20984;&#24615;&#20026;&#29702;&#35770;&#20998;&#26512;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#36890;&#36807;Gradient Flow&#35757;&#32451;&#30340;&#20108;&#23618;ReLU&#32593;&#32476;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23436;&#25972;&#30340;&#29702;&#35770;&#25551;&#36848;&#12290;&#22312;&#36825;&#31181;&#29305;&#23450;&#30340;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25429;&#33719;&#20102;&#20174;&#38543;&#26426;&#21021;&#22987;&#21270;&#21040;&#26368;&#32456;&#25910;&#25947;&#30340;&#25972;&#20010;&#20248;&#21270;&#36807;&#31243;&#12290;&#23613;&#31649;&#25105;&#20204;&#30740;&#31350;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#30456;&#23545;&#31616;&#21333;&#65292;&#20294;&#25105;&#20204;&#25581;&#31034;&#20102;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22235;&#20010;&#19981;&#21516;&#38454;&#27573;&#65292;&#26174;&#31034;&#20986;&#19968;&#20010;&#20174;&#31616;&#21270;&#21040;&#22797;&#26434;&#30340;&#23398;&#20064;&#36235;&#21183;&#12290;&#29305;&#23450;&#30340;&#38750;&#32447;&#24615;&#34892;&#20026;&#20063;&#21487;&#20197;&#34987;&#31934;&#30830;&#22320;&#35782;&#21035;&#21644;&#29702;&#35770;&#19978;&#25429;&#33719;&#65292;&#20363;&#22914;...
&lt;/p&gt;
&lt;p&gt;
The training process of ReLU neural networks often exhibits complicated nonlinear phenomena. The nonlinearity of models and non-convexity of loss pose significant challenges for theoretical analysis. Therefore, most previous theoretical works on the optimization dynamics of neural networks focus either on local analysis (like the end of training) or approximate linear models (like Neural Tangent Kernel). In this work, we conduct a complete theoretical characterization of the training process of a two-layer ReLU network trained by Gradient Flow on a linearly separable data. In this specific setting, our analysis captures the whole optimization process starting from random initialization to final convergence. Despite the relatively simple model and data that we studied, we reveal four different phases from the whole training process showing a general simplifying-to-complicating learning trend. Specific nonlinear behaviors can also be precisely identified and captured theoretically, such 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#38271;&#26399;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#21095;&#38598;&#24335;MDP&#27169;&#22411;&#65292;&#25552;&#20986;&#22312;&#38754;&#20020;&#36716;&#25442;&#21644;&#22870;&#21169;&#29305;&#24615;&#19981;&#30830;&#23450;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#26696;&#22522;&#20110;&#21344;&#26377;&#24230;&#37327;&#25552;&#20379;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65292;&#20854;&#26399;&#26395;&#36951;&#25022;&#21463;&#21040;&#30028;&#38480;&#32422;&#26463; $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ .</title><link>http://arxiv.org/abs/2305.10744</link><description>&lt;p&gt;
&#38754;&#21521;&#21095;&#38598;&#24335;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Online Resource Allocation in Episodic Markov Decision Processes. (arXiv:2305.10744v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#38271;&#26399;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#21095;&#38598;&#24335;MDP&#27169;&#22411;&#65292;&#25552;&#20986;&#22312;&#38754;&#20020;&#36716;&#25442;&#21644;&#22870;&#21169;&#29305;&#24615;&#19981;&#30830;&#23450;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#26696;&#22522;&#20110;&#21344;&#26377;&#24230;&#37327;&#25552;&#20379;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65292;&#20854;&#26399;&#26395;&#36951;&#25022;&#21463;&#21040;&#30028;&#38480;&#32422;&#26463; $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ .
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#38271;&#26399;&#30340;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#23427;&#38656;&#35201;&#22312;&#22810;&#20010;&#26102;&#38388;&#27573;&#20869;&#36827;&#34892;&#22810;&#38454;&#27573;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#21095;&#38598;&#24335;&#26377;&#38480;&#26102;&#38388;&#27573;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#20854;&#20013;&#36716;&#25442;&#21644;&#22870;&#21169;&#20197;&#21450;&#27599;&#19968;&#27425;&#30340;&#36164;&#28304;&#28040;&#32791;&#20989;&#25968;&#37117;&#26159;&#38750;&#23450;&#24577;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31561;&#25928;&#30340;&#22312;&#32447;&#32447;&#24615;&#35268;&#21010;&#37325;&#26500;&#26041;&#27861;&#65292;&#22522;&#20110;&#21344;&#26377;&#24230;&#37327;&#65292;&#20026;&#27492;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#36164;&#28304;&#20998;&#37197;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#22788;&#29702;&#20102;&#22312;&#20272;&#31639;&#30495;&#23454;&#21487;&#34892;&#38598;&#26102;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#35823;&#24046;&#65292;&#36825;&#26159;&#30456;&#23545;&#29420;&#31435;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#38543;&#26426;&#22870;&#21169;&#21644;&#36164;&#28304;&#28040;&#32791;&#20989;&#25968;&#65292;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#30340;&#26399;&#26395;&#36951;&#25022;&#21463;&#21040;&#30028;&#38480;&#32422;&#26463;&#65292;&#20854;&#30028;&#38480;&#21463;&#21040; $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ &#30340;&#32422;&#26463;&#65292;&#20854;&#20013; $\rho\in(0,1)$ &#26159;&#39044;&#31639;&#21442;&#25968;&#65292;$H$ &#26159;&#22320;&#24179;&#32447;&#38271;&#24230;&#65292;$S$ &#21644; $A$ &#26159;. . .
&lt;/p&gt;
&lt;p&gt;
This paper studies a long-term resource allocation problem over multiple periods where each period requires a multi-stage decision-making process. We formulate the problem as an online resource allocation problem in an episodic finite-horizon Markov decision process with unknown non-stationary transitions and stochastic non-stationary reward and resource consumption functions for each episode. We provide an equivalent online linear programming reformulation based on occupancy measures, for which we develop an online mirror descent algorithm. Our online dual mirror descent algorithm for resource allocation deals with uncertainties and errors in estimating the true feasible set, which is of independent interest. We prove that under stochastic reward and resource consumption functions, the expected regret of the online mirror descent algorithm is bounded by $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ where $\rho\in(0,1)$ is the budget parameter, $H$ is the length of the horizon, $S$ and $A$ are the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20648;&#22791;&#35745;&#31639;&#26426;&#30340;&#22810;&#21151;&#33021;&#24615;&#33021;&#21147;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#21560;&#24341;&#23376;&#20043;&#38388;&#30340;&#20851;&#31995;&#22914;&#20309;&#24433;&#21709;&#20854;&#23454;&#29616;&#22810;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35201;&#23454;&#29616;&#22810;&#21151;&#33021;&#24615;&#65292;&#38656;&#35201;RC&#20869;&#37096;&#32593;&#32476;c&#30340;&#35889;&#21322;&#24452;&#21512;&#36866;&#36873;&#25321;&#30340;&#20020;&#30028;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05799</link><description>&lt;p&gt;
&#22810;&#21151;&#33021;&#20648;&#22791;&#35745;&#31639;&#26426;&#30340;&#21452;&#37325;&#35270;&#35273;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Seeing double with a multifunctional reservoir computer. (arXiv:2305.05799v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20648;&#22791;&#35745;&#31639;&#26426;&#30340;&#22810;&#21151;&#33021;&#24615;&#33021;&#21147;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#21560;&#24341;&#23376;&#20043;&#38388;&#30340;&#20851;&#31995;&#22914;&#20309;&#24433;&#21709;&#20854;&#23454;&#29616;&#22810;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35201;&#23454;&#29616;&#22810;&#21151;&#33021;&#24615;&#65292;&#38656;&#35201;RC&#20869;&#37096;&#32593;&#32476;c&#30340;&#35889;&#21322;&#24452;&#21512;&#36866;&#36873;&#25321;&#30340;&#20020;&#30028;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21151;&#33021;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#22810;&#37325;&#31283;&#23450;&#24615;&#20197;&#25191;&#34892;&#22810;&#31181;&#20219;&#21153;&#32780;&#19981;&#25913;&#21464;&#20219;&#20309;&#32593;&#32476;&#23646;&#24615;&#12290;&#20351;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#33719;&#24471;&#26576;&#20123;&#22810;&#31283;&#23450;&#24615;&#20197;&#25191;&#34892;&#22810;&#20010;&#20219;&#21153;&#65292;&#20854;&#20013;&#27599;&#20010;&#20219;&#21153;&#19982;&#32593;&#32476;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#29305;&#23450;&#21560;&#24341;&#23376;&#30456;&#20851;&#32852;&#65292;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#33258;&#28982;&#20855;&#26377;&#35768;&#22810;&#20248;&#28857;&#12290;&#22240;&#20026;&#19982;&#22810;&#31283;&#23450;&#24615;&#26377;&#20851;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#19981;&#21516;&#21560;&#24341;&#23376;&#20043;&#38388;&#30340;&#20851;&#31995;&#22914;&#20309;&#24433;&#21709;&#20648;&#22791;&#35745;&#31639;&#26426;&#65288;RC&#65289;&#30340;&#22810;&#21151;&#33021;&#24615;&#33021;&#21147;&#65292;&#20648;&#22791;&#35745;&#31639;&#26426;&#26159;&#19968;&#31181;&#20197;ANN&#24418;&#24335;&#21576;&#29616;&#30340;&#21160;&#24577;&#31995;&#32479;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#8220;&#21452;&#37325;&#35270;&#35273;&#25928;&#24212;&#8221;&#38382;&#39064;&#26469;&#31995;&#32479;&#22320;&#30740;&#31350;&#24403;&#20004;&#20010;&#21560;&#24341;&#23376;&#20043;&#38388;&#23384;&#22312;&#37325;&#21472;&#26102;RC&#22914;&#20309;&#37325;&#26500;&#21560;&#24341;&#23376;&#30340;&#20849;&#23384;&#12290;&#38543;&#30528;&#37325;&#21472;&#37327;&#30340;&#22686;&#21152;&#65292;&#25105;&#20204;&#21457;&#29616;&#35201;&#23454;&#29616;&#22810;&#21151;&#33021;&#24615;&#65292;&#38656;&#35201;RC&#20869;&#37096;&#32593;&#32476;c&#30340;&#35889;&#21322;&#24452;&#21512;&#36866;&#36873;&#25321;&#30340;&#20020;&#30028;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multifunctional biological neural networks exploit multistability in order to perform multiple tasks without changing any network properties. Enabling artificial neural networks (ANNs) to obtain certain multistabilities in order to perform several tasks, where each task is related to a particular attractor in the network's state space, naturally has many benefits from a machine learning perspective. Given the association to multistability, in this paper we explore how the relationship between different attractors influences the ability of a reservoir computer (RC), which is a dynamical system in the form of an ANN, to achieve multifunctionality. We construct the `seeing double' problem to systematically study how a RC reconstructs a coexistence of attractors when there is an overlap between them. As the amount of overlap increases, we discover that for multifunctionality to occur, there is a critical dependence on a suitable choice of the spectral radius for the RC's internal network c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#34507;&#30333;&#36136;&#24314;&#27169;&#20013;&#24212;&#29992;transformer&#21644;&#22270;&#21367;&#31215;&#30340;&#32467;&#26500;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#35757;&#32451;&#21518;&#33021;&#22815;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#24615;&#36136;&#30340;&#34507;&#30333;&#36136;&#65292;&#26696;&#20363;&#39564;&#35777;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#29983;&#25104;&#29702;&#24819;&#30446;&#26631;&#24615;&#36136;&#30340;&#34507;&#30333;&#36136;&#12290;</title><link>http://arxiv.org/abs/2305.04934</link><description>&lt;p&gt;
&#24212;&#29992;&#22522;&#20110;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#33258;&#22238;&#24402;Transformer&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#20998;&#26512;&#21644;&#21457;&#29616;&#26032;&#22411;&#34507;&#30333;&#36136;
&lt;/p&gt;
&lt;p&gt;
Generative Pretrained Autoregressive Transformer Graph Neural Network applied to the Analysis and Discovery of Novel Proteins. (arXiv:2305.04934v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#34507;&#30333;&#36136;&#24314;&#27169;&#20013;&#24212;&#29992;transformer&#21644;&#22270;&#21367;&#31215;&#30340;&#32467;&#26500;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#35757;&#32451;&#21518;&#33021;&#22815;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#24615;&#36136;&#30340;&#34507;&#30333;&#36136;&#65292;&#26696;&#20363;&#39564;&#35777;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#29983;&#25104;&#29702;&#24819;&#30446;&#26631;&#24615;&#36136;&#30340;&#34507;&#30333;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#36947;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#24212;&#29992;&#20110;&#35299;&#20915;&#34507;&#30333;&#36136;&#24314;&#27169;&#20013;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#65292;&#20351;&#29992;&#19968;&#20010;&#25972;&#21512;&#20102;transformer&#21644;&#22270;&#21367;&#31215;&#30340;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#22312;&#22240;&#26524;&#22810;&#22836;&#22270;&#26426;&#21046;&#20013;&#23454;&#29616;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#34987;&#29992;&#20110;&#39044;&#27979;&#20108;&#32423;&#32467;&#26500;&#20869;&#23481;&#65288;&#27599;&#20010;&#27531;&#22522;&#30340;&#27700;&#24179;&#21644;&#24635;&#20307;&#20869;&#23481;&#65289;&#12289;&#34507;&#30333;&#36136;&#21487;&#28342;&#24615;&#21644;&#27979;&#24207;&#20219;&#21153;&#12290;&#36827;&#19968;&#27493;&#22312;&#21453;&#21521;&#20219;&#21153;&#19978;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#35774;&#35745;&#20855;&#26377;&#36825;&#20123;&#24615;&#36136;&#20316;&#20026;&#30446;&#26631;&#29305;&#24449;&#30340;&#34507;&#30333;&#36136;&#12290;&#35813;&#27169;&#22411;&#34987;&#21046;&#23450;&#20026;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23436;&#20840;&#22522;&#20110;&#25552;&#31034;&#65292;&#21487;&#20197;&#20026;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#36866;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;&#28155;&#21152;&#39069;&#22806;&#20219;&#21153;&#20250;&#20135;&#29983;&#30456;&#20114;&#21327;&#21516;&#20316;&#29992;&#65292;&#20351;&#27169;&#22411;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#24471;&#21040;&#25552;&#39640;&#65292;&#36229;&#36807;&#20165;&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;&#26696;&#20363;&#30740;&#31350;&#29992;&#20110;&#39564;&#35777;&#35813;&#26041;&#27861;&#65292;&#29983;&#25104;&#20855;&#26377;&#29702;&#24819;&#30446;&#26631;&#24615;&#36136;&#65292;&#21253;&#25324;&#31283;&#23450;&#24615;&#21644;&#21487;&#28342;&#24615;&#30340;&#34507;&#30333;&#36136;&#65292;&#24182;&#36827;&#34892;&#23454;&#39564;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report a flexible language-model based deep learning strategy, applied here to solve complex forward and inverse problems in protein modeling, based on an attention neural network that integrates transformer and graph convolutional architectures in a causal multi-headed graph mechanism, to realize a generative pretrained model. The model is applied to predict secondary structure content (per-residue level and overall content), protein solubility, and sequencing tasks. Further trained on inverse tasks, the model is rendered capable of designing proteins with these properties as target features. The model is formulated as a general framework, completely prompt-based, and can be adapted for a variety of downstream tasks. We find that adding additional tasks yields emergent synergies that the model exploits in improving overall performance, beyond what would be possible by training a model on each dataset alone. Case studies are presented to validate the method, yielding protein designs
&lt;/p&gt;</description></item><item><title>&#22312;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#21644; beam search &#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#25913;&#36827;&#25552;&#31034;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03495</link><description>&lt;p&gt;
&#22522;&#20110;&#8220;&#26799;&#24230;&#19979;&#38477;&#8221;&#19982; beam search &#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Automatic Prompt Optimization with "Gradient Descent" and Beam Search. (arXiv:2305.03495v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03495
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#21644; beam search &#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#25913;&#36827;&#25552;&#31034;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36890;&#29992;&#26234;&#33021;&#26041;&#38754;&#23637;&#29616;&#20102;&#20986;&#33394;&#24615;&#33021;&#65292;&#20294;&#20854;&#33021;&#21147;&#20173;&#39640;&#24230;&#20381;&#36182;&#20110;&#25163;&#20889;&#30340;&#25552;&#31034;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35797;&#38169;&#23581;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#38750;&#21442;&#25968;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#8212;&#8212;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#65288;APO&#65289;&#65292;&#20854;&#28789;&#24863;&#26469;&#33258;&#20110;&#20351;&#29992;&#25968;&#20540;&#26799;&#24230;&#19979;&#38477;&#33258;&#21160;&#25913;&#36827;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Automatic Prompt Optimization (APO), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language ``gradients'' that criticize the current prompt. The gradients are then ``propagated'' into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing 
&lt;/p&gt;</description></item><item><title>Patch Diffusion &#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22522;&#20110;&#22359;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#23558;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#32553;&#30701;&#33267;&#23569;&#19968;&#20493;&#65292;&#24182;&#22312;&#26356;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.12526</link><description>&lt;p&gt;
Patch Diffusion: &#26356;&#24555;&#26356;&#39640;&#25928;&#30340;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models. (arXiv:2304.12526v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12526
&lt;/p&gt;
&lt;p&gt;
Patch Diffusion &#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22522;&#20110;&#22359;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#23558;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#32553;&#30701;&#33267;&#23569;&#19968;&#20493;&#65292;&#24182;&#22312;&#26356;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#25968;&#25454;&#26469;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22522;&#20110;&#22359;&#30340;&#35757;&#32451;&#26694;&#26550; Patch Diffusion&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35757;&#32451;&#26102;&#38388;&#25104;&#26412;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#29992;&#25143;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#26680;&#24515;&#26159;&#26032;&#30340;&#26465;&#20214;&#35780;&#20998;&#20989;&#25968;&#65292;&#23427;&#22312;&#22359;&#32423;&#21035;&#19978;&#36827;&#34892;&#25805;&#20316;&#65292;&#24182;&#23558;&#21407;&#22987;&#22270;&#20687;&#20013;&#30340;&#22359;&#20301;&#32622;&#20316;&#20026;&#38468;&#21152;&#22352;&#26631;&#36890;&#36947;&#65292;&#21516;&#26102;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38543;&#26426;&#21270;&#21644;&#22810;&#26679;&#21270;&#22359;&#22823;&#23567;&#26469;&#32534;&#30721;&#22810;&#23610;&#24230;&#30340;&#36328;&#21306;&#22495;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#37319;&#26679;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#21407;&#22987;&#25193;&#25955;&#27169;&#22411;&#19968;&#26679;&#31616;&#21333;&#26131;&#29992;&#12290;&#36890;&#36807; Patch Diffusion&#65292;&#25105;&#20204;&#33021;&#22815;&#23454;&#29616; $\mathbf{\ge 2\times}$ &#26356;&#24555;&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#27604;&#36739;&#25110;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;Patch Diffusion &#25552;&#39640;&#20102;&#22312;&#30456;&#23545;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#65292;&#20351;&#29992;&#20165; 5,000 &#24352;&#22270;&#20687;&#36827;&#34892;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are powerful, but they require a lot of time and data to train. We propose Patch Diffusion, a generic patch-wise training framework, to significantly reduce the training time costs while improving data efficiency, which thus helps democratize diffusion model training to broader users. At the core of our innovations is a new conditional score function at the patch level, where the patch location in the original image is included as additional coordinate channels, while the patch size is randomized and diversified throughout training to encode the cross-region dependency at multiple scales. Sampling with our method is as easy as in the original diffusion model. Through Patch Diffusion, we could achieve $\mathbf{\ge 2\times}$ faster training, while maintaining comparable or better generation quality. Patch Diffusion meanwhile improves the performance of diffusion models trained on relatively small datasets, $e.g.$, as few as 5,000 images to train from scratch. We achieve 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PEFT-Ref&#21442;&#32771;&#26550;&#26500;&#65292;&#26631;&#20934;&#21270;&#20102;&#19981;&#21516;PEFT&#25216;&#26415;&#20849;&#20139;&#30340;&#26041;&#38754;&#65292;&#38548;&#31163;&#20102;&#24046;&#24322;&#21040;&#29305;&#23450;&#20301;&#32622;&#21644;&#20132;&#20114;&#20013;&#65292;&#27169;&#22359;&#21270;&#30340;&#35270;&#22270;&#26377;&#21161;&#20110;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#21450;&#20854;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;PEFT&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.12410</link><description>&lt;p&gt;
PEFT-Ref: &#19968;&#31181;&#27169;&#22359;&#21270;&#30340;&#21442;&#32771;&#26550;&#26500;&#21644;&#31867;&#22411;&#65292;&#29992;&#20110;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PEFT-Ref: A Modular Reference Architecture and Typology for Parameter-Efficient Finetuning Techniques. (arXiv:2304.12410v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PEFT-Ref&#21442;&#32771;&#26550;&#26500;&#65292;&#26631;&#20934;&#21270;&#20102;&#19981;&#21516;PEFT&#25216;&#26415;&#20849;&#20139;&#30340;&#26041;&#38754;&#65292;&#38548;&#31163;&#20102;&#24046;&#24322;&#21040;&#29305;&#23450;&#20301;&#32622;&#21644;&#20132;&#20114;&#20013;&#65292;&#27169;&#22359;&#21270;&#30340;&#35270;&#22270;&#26377;&#21161;&#20110;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#21450;&#20854;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;PEFT&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#21442;&#25968;&#25928;&#29575;&#24494;&#35843;(PEFT)&#25216;&#26415;&#26088;&#22312;&#25913;&#21892;&#23436;&#20840;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLM)&#30340;&#39640;&#26114;&#25104;&#26412;&#12290;&#38543;&#30528;&#19981;&#21516;&#30340;PEFT&#25216;&#26415;&#19981;&#26029;&#20986;&#29616;&#65292;&#23545;&#23427;&#20204;&#36827;&#34892;&#27604;&#36739;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#20197;&#19979;&#26041;&#38754;&#65306;(i)&#23427;&#20204;&#28155;&#21152;&#21040;PLM&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#65292;(ii)&#19981;&#21516;&#31867;&#22411;&#21644;&#31243;&#24230;&#30340;&#25928;&#29575;&#25913;&#36827;&#65292;(iii)&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;(iv)&#32467;&#26500;&#21644;&#21151;&#33021;&#24046;&#24322;&#22914;&#20309;&#19982;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#30456;&#20851;&#32852;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#26679;&#30340;&#27604;&#36739;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21442;&#32771;&#26694;&#26550;&#65292;&#26631;&#20934;&#21270;&#20102;&#19981;&#21516;PEFT&#25216;&#26415;&#20849;&#20139;&#30340;&#26041;&#38754;&#65292;&#21516;&#26102;&#23558;&#24046;&#24322;&#38548;&#31163;&#21040;&#19982;&#26631;&#20934;&#32452;&#20214;&#30340;&#29305;&#23450;&#20301;&#32622;&#21644;&#20132;&#20114;&#20013;&#12290;&#36890;&#36807;&#36825;&#20010;&#26631;&#20934;&#21270;&#21644;&#38548;&#31163;&#24046;&#24322;&#30340;&#36807;&#31243;&#65292;&#20986;&#29616;&#20102;PEFT&#25216;&#26415;&#30340;&#27169;&#22359;&#21270;&#35270;&#22270;&#65292;&#19981;&#20165;&#25903;&#25345;&#30452;&#25509;&#27604;&#36739;&#19981;&#21516;&#25216;&#26415;&#21450;&#20854;&#25928;&#29575;&#21644;&#20219;&#21153;&#24615;&#33021;&#65292;&#32780;&#19988;&#36824;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;PEFT&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#21442;&#32771;&#26550;&#26500;&#31216;&#20026;PEFT-Ref&#65292;&#21253;&#25324;&#19971;&#20010;&#26680;&#24515;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#37117;&#22788;&#29702;PEFT&#30340;&#29305;&#23450;&#26041;&#38754;&#65292;&#24182;&#21487;&#29992;&#20316;&#24320;&#21457;&#26032;PEFT&#25216;&#26415;&#21644;&#27604;&#36739;&#29616;&#26377;&#25216;&#26415;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent parameter-efficient finetuning (PEFT) techniques aim to improve over the considerable cost of fully finetuning large pretrained language models (PLM). As different PEFT techniques proliferate, it is becoming difficult to compare them, in particular in terms of (i) the structure and functionality they add to the PLM, (ii) the different types and degrees of efficiency improvements achieved, (iii) performance at different downstream tasks, and (iv) how differences in structure and functionality relate to efficiency and task performance. To facilitate such comparisons, this paper presents a reference framework which standardises aspects shared by different PEFT techniques, while isolating differences to specific locations and interactions with the standard components. Through this process of standardising and isolating differences, a modular view of PEFT techniques emerges, supporting not only direct comparison of different techniques and their efficiency and task performance, but a
&lt;/p&gt;</description></item><item><title>Transformer&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2304.10557</link><description>&lt;p&gt;
Transformer&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
An Introduction to Transformers. (arXiv:2304.10557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10557
&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#12290;Transformer&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;Transformer&#30340;&#20171;&#32461;&#65292;&#20294;&#22823;&#22810;&#25968;&#37117;&#32570;&#23569;&#23545;&#20854;&#26550;&#26500;&#30340;&#31934;&#30830;&#25968;&#23398;&#25551;&#36848;&#65292;&#20854;&#35774;&#35745;&#36873;&#25321;&#30340;&#30452;&#35273;&#20063;&#24120;&#24120;&#32570;&#22833;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#30740;&#31350;&#36335;&#24452;&#30340;&#26354;&#25240;&#65292;Transformer&#37096;&#20214;&#30340;&#35299;&#37322;&#21487;&#33021;&#26159;&#24322;&#36136;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer is a neural network component that can be used to learn useful representations of sequences or sets of datapoints. The transformer has driven recent advances in natural language processing, computer vision, and spatio-temporal modelling. There are many introductions to transformers, but most do not contain precise mathematical descriptions of the architecture and the intuitions behind the design choices are often also missing. Moreover, as research takes a winding path, the explanations for the components of the transformer can be idiosyncratic. In this note we aim for a mathematically precise, intuitive, and clean description of the transformer architecture.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500; MLGCN&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#20102;&#22810;&#26631;&#31614;&#22330;&#26223;&#20013;&#21516;&#31867;&#20559;&#22909;&#30340;&#35821;&#20041;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;MLGCN&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.10398</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-label Node Classification On Graph-Structured Data. (arXiv:2304.10398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10398
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500; MLGCN&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#30740;&#31350;&#20102;&#22810;&#26631;&#31614;&#22330;&#26223;&#20013;&#21516;&#31867;&#20559;&#22909;&#30340;&#35821;&#20041;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;MLGCN&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22270;&#20013;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#25913;&#36827;&#12290;&#34429;&#28982;&#36825;&#20123;&#36827;&#23637;&#22312;&#22810;&#31867;&#20998;&#31867;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#23637;&#31034;&#65292;&#20294;&#19968;&#20010;&#26356;&#26222;&#36941;&#21644;&#29616;&#23454;&#30340;&#24773;&#20917;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#33410;&#28857;&#21487;&#33021;&#26377;&#22810;&#20010;&#26631;&#31614;&#65292;&#19968;&#30452;&#20197;&#26469;&#21463;&#21040;&#20102;&#24456;&#23569;&#20851;&#27880;&#12290;&#22312;&#36827;&#34892;&#20851;&#20110;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;&#30340;&#37325;&#28857;&#30740;&#31350;&#30340;&#39318;&#35201;&#25361;&#25112;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#22810;&#26631;&#31614;&#22270;&#25968;&#25454;&#38598;&#25968;&#37327;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#20316;&#20026;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#65292;&#25105;&#20204;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#19977;&#20010;&#30495;&#23454;&#30340;&#29983;&#29289;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#26631;&#31614;&#22270;&#29983;&#25104;&#22120;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#21487;&#35843;&#23646;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;&#34429;&#28982;&#39640;&#26631;&#31614;&#30456;&#20284;&#24615;&#65288;&#39640;&#21516;&#31867;&#20559;&#22909;&#65289;&#36890;&#24120;&#34987;&#24402;&#22240;&#20110;GNN&#30340;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#22810;&#26631;&#31614;&#22330;&#26223;&#24182;&#19981;&#36981;&#24490;&#30446;&#21069;&#20026;&#22810;&#31867;&#22330;&#26223;&#23450;&#20041;&#30340;&#21516;&#31867;&#20559;&#22909;&#21644;&#24322;&#31867;&#20559;&#22909;&#30340;&#24120;&#35268;&#35821;&#20041;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#65292;&#25105;&#20204;&#38500;&#20102;&#20026;&#22810;&#26631;&#31614;&#22330;&#26223;&#23450;&#20041;&#21516;&#31867;&#20559;&#22909;&#22806;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GNN&#20307;&#31995;&#32467;&#26500;&#65292;&#21363;MLGCN&#65288;&#22810;&#26631;&#31614;&#22270;&#21367;&#31215;&#32593;&#32476;&#65289;&#65292;&#26469;&#22788;&#29702;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;MLGCN&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have shown state-of-the-art improvements in node classification tasks on graphs. While these improvements have been largely demonstrated in a multi-class classification scenario, a more general and realistic scenario in which each node could have multiple labels has so far received little attention. The first challenge in conducting focused studies on multi-label node classification is the limited number of publicly available multi-label graph datasets. Therefore, as our first contribution, we collect and release three real-world biological datasets and develop a multi-label graph generator to generate datasets with tunable properties. While high label similarity (high homophily) is usually attributed to the success of GNNs, we argue that a multi-label scenario does not follow the usual semantics of homophily and heterophily so far defined for a multi-class scenario. As our second contribution, besides defining homophily for the multi-label scenario, we dev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#40065;&#26834;&#30340;&#33258;&#36866;&#24212; $\tau$-Lasso &#20272;&#35745;&#22120;&#65292;&#21516;&#26102;&#37319;&#29992;&#33258;&#36866;&#24212; $\ell_1$-&#33539;&#25968;&#24809;&#32602;&#39033;&#20197;&#38477;&#20302;&#30495;&#23454;&#22238;&#24402;&#31995;&#25968;&#30340;&#20559;&#24046;&#12290;&#23427;&#20855;&#26377;&#21464;&#37327;&#36873;&#25321;&#19968;&#33268;&#24615;&#21644;&#30495;&#23454;&#25903;&#25345;&#19979;&#22238;&#24402;&#21521;&#37327;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#26368;&#20248;&#24615;&#36136;&#65292;&#20551;&#23450;&#24050;&#30693;&#30495;&#23454;&#22238;&#24402;&#21521;&#37327;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2304.09310</link><description>&lt;p&gt;
&#33258;&#36866;&#24212; $\tau$-Lasso&#65306;&#20854;&#20581;&#22766;&#24615;&#21644;&#26368;&#20248;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Adaptive $\tau$-Lasso: Its Robustness and Oracle Properties. (arXiv:2304.09310v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#40065;&#26834;&#30340;&#33258;&#36866;&#24212; $\tau$-Lasso &#20272;&#35745;&#22120;&#65292;&#21516;&#26102;&#37319;&#29992;&#33258;&#36866;&#24212; $\ell_1$-&#33539;&#25968;&#24809;&#32602;&#39033;&#20197;&#38477;&#20302;&#30495;&#23454;&#22238;&#24402;&#31995;&#25968;&#30340;&#20559;&#24046;&#12290;&#23427;&#20855;&#26377;&#21464;&#37327;&#36873;&#25321;&#19968;&#33268;&#24615;&#21644;&#30495;&#23454;&#25903;&#25345;&#19979;&#22238;&#24402;&#21521;&#37327;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#26368;&#20248;&#24615;&#36136;&#65292;&#20551;&#23450;&#24050;&#30693;&#30495;&#23454;&#22238;&#24402;&#21521;&#37327;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#40065;&#26834; $\tau$-&#22238;&#24402;&#20272;&#35745;&#22120;&#65292;&#20197;&#24212;&#23545;&#21709;&#24212;&#21464;&#37327;&#21644;&#21327;&#21464;&#37327;&#30340;&#20005;&#37325;&#27745;&#26579;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#20272;&#35745;&#22120;&#20026;&#33258;&#36866;&#24212; $\tau$-Lasso&#65292;&#23427;&#23545;&#24322;&#24120;&#20540;&#21644;&#39640;&#26464;&#26438;&#28857;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#37319;&#29992;&#33258;&#36866;&#24212; $\ell_1$-&#33539;&#25968;&#24809;&#32602;&#39033;&#26469;&#20943;&#23569;&#30495;&#23454;&#22238;&#24402;&#31995;&#25968;&#30340;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#33258;&#36866;&#24212; $\ell_1$-&#33539;&#25968;&#24809;&#32602;&#39033;&#20026;&#27599;&#20010;&#22238;&#24402;&#31995;&#25968;&#20998;&#37197;&#19968;&#20010;&#26435;&#37325;&#12290;&#23545;&#20110;&#22266;&#23450;&#25968;&#37327;&#30340;&#39044;&#27979;&#21464;&#37327; $p$&#65292;&#25105;&#20204;&#26174;&#31034;&#20986;&#33258;&#36866;&#24212; $\tau$-Lasso &#20855;&#26377;&#21464;&#37327;&#36873;&#25321;&#19968;&#33268;&#24615;&#21644;&#30495;&#23454;&#25903;&#25345;&#19979;&#22238;&#24402;&#21521;&#37327;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#26368;&#20248;&#24615;&#36136;&#65292;&#20551;&#23450;&#24050;&#30693;&#30495;&#23454;&#22238;&#24402;&#21521;&#37327;&#30340;&#25903;&#25345;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#26029;&#28857;&#21644;&#24433;&#21709;&#20989;&#25968;&#26469;&#34920;&#24449;&#20854;&#20581;&#22766;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27169;&#25311;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#20272;&#35745;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new regularized version of the robust $\tau$-regression estimator for analyzing high-dimensional data sets subject to gross contamination in the response variables and covariates. We call the resulting estimator adaptive $\tau$-Lasso that is robust to outliers and high-leverage points and simultaneously employs adaptive $\ell_1$-norm penalty term to reduce the bias associated with large true regression coefficients. More specifically, this adaptive $\ell_1$-norm penalty term assigns a weight to each regression coefficient. For a fixed number of predictors $p$, we show that the adaptive $\tau$-Lasso has the oracle property with respect to variable-selection consistency and asymptotic normality for the regression vector corresponding to the true support, assuming knowledge of the true regression vector support. We then characterize its robustness via the finite-sample breakdown point and the influence function. We carry-out extensive simulations to compare the per
&lt;/p&gt;</description></item><item><title>STO&#20013;&#24050;&#26377;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#19981;&#23436;&#21892;&#65292;&#38590;&#20197;&#20195;&#34920;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#65292;&#38480;&#21046;&#20102;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.08503</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Scalable Test Problem Generator for Sequential Transfer Optimization. (arXiv:2304.08503v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08503
&lt;/p&gt;
&lt;p&gt;
STO&#20013;&#24050;&#26377;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#19981;&#23436;&#21892;&#65292;&#38590;&#20197;&#20195;&#34920;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#65292;&#38480;&#21046;&#20102;&#31639;&#27861;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24207;&#21015;&#36716;&#31227;&#20248;&#21270;(STO)&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#26088;&#22312;&#21033;&#29992;&#20648;&#23384;&#22312;&#25968;&#25454;&#24211;&#20013;&#20197;&#21069;&#27714;&#35299;&#30340;&#20248;&#21270;&#20219;&#21153;&#30340;&#30693;&#35782;&#26469;&#25552;&#39640;&#20248;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#31639;&#27861;&#35774;&#35745;&#24050;&#26377;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;STO&#20013;&#30340;&#27979;&#35797;&#38382;&#39064;&#35774;&#35745;&#24182;&#19981;&#23436;&#21892;&#12290;&#23427;&#20204;&#24448;&#24448;&#26159;&#30001;&#20854;&#20182;&#22522;&#20934;&#20989;&#25968;&#38543;&#26426;&#32452;&#21512;&#32780;&#25104;&#65292;&#36825;&#20123;&#22522;&#20934;&#20989;&#25968;&#20855;&#26377;&#30456;&#21516;&#30340;&#26368;&#20339;&#20540;&#65292;&#25110;&#32773;&#29983;&#25104;&#33258;&#34920;&#29616;&#20986;&#26377;&#38480;&#21464;&#21270;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#20013;&#28304;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#30340;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#25163;&#21160;&#37197;&#32622;&#30340;&#65292;&#22240;&#27492;&#21333;&#35843;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#34920;&#24449;&#30495;&#23454;&#38382;&#39064;&#22810;&#26679;&#21270;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#31639;&#27861;&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#21462;&#24471;&#30340;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#20855;&#26377;&#39640;&#24230;&#30340;&#20559;&#35265;&#65292;&#24182;&#19988;&#38590;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#38382;&#39064;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20123;&#34920;&#24449;STO&#38382;&#39064;&#30340;&#22522;&#26412;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential transfer optimization (STO), which aims to improve optimization performance by exploiting knowledge captured from previously-solved optimization tasks stored in a database, has been gaining increasing research attention in recent years. However, despite significant advancements in algorithm design, the test problems in STO are not well designed. Oftentimes, they are either randomly assembled by other benchmark functions that have identical optima or are generated from practical problems that exhibit limited variations. The relationships between the optimal solutions of source and target tasks in these problems are manually configured and thus monotonous, limiting their ability to represent the diverse relationships of real-world problems. Consequently, the promising results achieved by many algorithms on these problems are highly biased and difficult to be generalized to other problems. In light of this, we first introduce a few rudimentary concepts for characterizing STO pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38416;&#36848;&#20102;&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#24615;&#19968;&#38454;&#26597;&#35810;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26469;&#33258;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#30340;&#26032;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#26032;&#20844;&#24335;&#65292;&#24182;&#22312;&#29616;&#26377;&#20844;&#24335;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.07063</link><description>&lt;p&gt;
&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#24615;&#19968;&#38454;&#26597;&#35810;&#25512;&#29702;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Existential First Order Queries Inference on Knowledge Graphs. (arXiv:2304.07063v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38416;&#36848;&#20102;&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#24615;&#19968;&#38454;&#26597;&#35810;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26469;&#33258;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#30340;&#26032;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#26032;&#20844;&#24335;&#65292;&#24182;&#22312;&#29616;&#26377;&#20844;&#24335;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#21033;&#29992;&#35266;&#23519;&#21040;&#30340;&#20449;&#24687;&#26469;&#39044;&#27979;&#32570;&#22833;&#30340;&#20449;&#24687;&#12290;&#29305;&#21035;&#22320;&#65292;&#22238;&#31572;&#19968;&#38454;&#36923;&#36753;&#20844;&#24335;&#26159;&#29305;&#21035;&#24863;&#20852;&#36259;&#30340;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#28165;&#26224;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#20102;&#19968;&#32452;&#23454;&#20307;&#30340;&#23884;&#20837;&#65292;&#24182;&#23558;&#36923;&#36753;&#36816;&#31639;&#35270;&#20026;&#38598;&#21512;&#36816;&#31639;&#12290;&#23613;&#31649;&#26377;&#24456;&#22810;&#30740;&#31350;&#36981;&#24490;&#30456;&#21516;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#32570;&#20047;&#20174;&#36923;&#36753;&#35282;&#24230;&#36827;&#34892;&#31995;&#32479;&#26816;&#26597;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20808;&#21069;&#30740;&#31350;&#35843;&#26597;&#30340;&#26597;&#35810;&#33539;&#22260;&#65292;&#24182;&#20934;&#30830;&#22320;&#30830;&#23450;&#20102;&#23427;&#19982;&#25972;&#20010;&#23384;&#22312;&#24615;&#20844;&#24335;&#23478;&#26063;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#26032;&#20844;&#24335;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#35752;&#35770;&#20102;&#21516;&#26102;&#20986;&#29616;&#30340;&#26032;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26469;&#33258;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#30340;&#26032;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#26032;&#20844;&#24335;&#65292;&#24182;&#22312;&#29616;&#26377;&#20844;&#24335;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning on knowledge graphs is a challenging task because it utilizes observed information to predict the missing one. Specifically, answering first-order logic formulas is of particular interest because of its clear syntax and semantics. Recently, the query embedding method has been proposed which learns the embedding of a set of entities and treats logic operations as set operations. Though there has been much research following the same methodology, it lacks a systematic inspection from the standpoint of logic. In this paper, we characterize the scope of queries investigated previously and precisely identify the gap between it and the whole family of existential formulas. Moreover, we develop a new dataset containing ten new formulas and discuss the new challenges coming simultaneously. Finally, we propose a new search algorithm from fuzzy logic theory which is capable of solving new formulas and outperforming the previous methods in existing formulas.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#37325;&#24314;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#20013;&#65292;&#36890;&#36807;&#23558;&#29702;&#35770;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#25439;&#22833;&#20989;&#25968;&#24182;&#32467;&#21512;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#37325;&#23376;&#25955;&#23556;&#30340;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2303.14090</link><description>&lt;p&gt;
&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#21033;&#29992;&#26263;&#29289;&#36136;&#37325;&#24314;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks in the recreation of hydrodynamic simulations from dark matter. (arXiv:2303.14090v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#37325;&#24314;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#20013;&#65292;&#36890;&#36807;&#23558;&#29702;&#35770;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#25439;&#22833;&#20989;&#25968;&#24182;&#32467;&#21512;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#37325;&#23376;&#25955;&#23556;&#30340;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#21512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#23558;&#32479;&#35745;&#27169;&#24335;&#19982;&#39046;&#22495;&#30693;&#35782;&#30456;&#32467;&#21512;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#20854;&#22522;&#26412;&#29702;&#24565;&#26159;&#36890;&#36807;&#24050;&#30693;&#20851;&#31995;&#26469;&#20016;&#23500;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#20197;&#38480;&#21046;&#21487;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#31354;&#38388;&#12290;&#27700;&#21160;&#21147;&#23398;&#27169;&#25311;&#26159;&#29616;&#20195;&#23431;&#23449;&#23398;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65292;&#32780;&#25152;&#38656;&#30340;&#35745;&#31639;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24555;&#36895;&#27169;&#25311;&#26263;&#29289;&#36136;&#38656;&#35201;&#26356;&#23569;&#30340;&#36164;&#28304;&#65292;&#36825;&#23548;&#33268;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25104;&#20026;&#30740;&#31350;&#30340;&#19968;&#20010;&#27963;&#36291;&#39046;&#22495;;&#22312;&#36825;&#37324;&#65292;&#37325;&#24314;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#20013;&#21457;&#29616;&#30340;&#25955;&#23556;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#37325;&#24314;&#27969;&#20307;&#21147;&#23398;&#27169;&#25311;&#20013;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#36827;&#27493;&#21644;&#29289;&#29702;&#32422;&#26463;&#65292;&#23558;&#20851;&#20110;&#37325;&#23376;&#36716;&#21270;&#25928;&#29575;&#30340;&#29702;&#35770;&#27880;&#20837;&#27169;&#22411;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#65292;&#22522;&#20110;&#32467;&#26524;&#22270;&#20687;&#20013;&#21160;&#21147;&#23398;&#21151;&#29575;&#35889;&#20013;&#30340;&#35823;&#24046;&#65292;&#36825;&#20351;&#24471;&#21487;&#20197;&#37327;&#21270;&#32593;&#32476;&#23545;&#23431;&#23449;&#23398;&#21442;&#25968;&#25512;&#26029;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks have emerged as a coherent framework for building predictive models that combine statistical patterns with domain knowledge. The underlying notion is to enrich the optimization loss function with known relationships to constrain the space of possible solutions. Hydrodynamic simulations are a core constituent of modern cosmology, while the required computations are both expensive and time-consuming. At the same time, the comparatively fast simulation of dark matter requires fewer resources, which has led to the emergence of machine learning algorithms for baryon inpainting as an active area of research; here, recreating the scatter found in hydrodynamic simulations is an ongoing challenge. This paper presents the first application of physics-informed neural networks to baryon inpainting by combining advances in neural network architectures with physical constraints, injecting theory on baryon conversion efficiency into the model loss function. We also in
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26032;&#22411;&#21160;&#24577;&#22270;&#23398;&#20064;&#26550;&#26500;DyGFormer&#65292;&#24182;&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#24211;DyGLib&#65292;&#20197;&#20419;&#36827;&#21487;&#37325;&#22797;&#12289;&#21487;&#25193;&#23637;&#21644;&#21487;&#20449;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#30740;&#31350;&#12290;DyGFormer&#36890;&#36807;&#37051;&#23621;&#20849;&#29616;&#32534;&#30721;&#26041;&#26696;&#21644;&#20998;&#22359;&#25216;&#26415;&#23454;&#29616;&#26356;&#38271;&#26399;&#21382;&#21490;&#30340;&#39640;&#25928;&#25512;&#29702;&#65292;&#22312;13&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13047</link><description>&lt;p&gt;
&#21521;&#26356;&#22909;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#36808;&#36827;&#65306;&#26032;&#30340;&#26550;&#26500;&#21644;&#32479;&#19968;&#24211;
&lt;/p&gt;
&lt;p&gt;
Towards Better Dynamic Graph Learning: New Architecture and Unified Library. (arXiv:2303.13047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13047
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26032;&#22411;&#21160;&#24577;&#22270;&#23398;&#20064;&#26550;&#26500;DyGFormer&#65292;&#24182;&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#24211;DyGLib&#65292;&#20197;&#20419;&#36827;&#21487;&#37325;&#22797;&#12289;&#21487;&#25193;&#23637;&#21644;&#21487;&#20449;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#30740;&#31350;&#12290;DyGFormer&#36890;&#36807;&#37051;&#23621;&#20849;&#29616;&#32534;&#30721;&#26041;&#26696;&#21644;&#20998;&#22359;&#25216;&#26415;&#23454;&#29616;&#26356;&#38271;&#26399;&#21382;&#21490;&#30340;&#39640;&#25928;&#25512;&#29702;&#65292;&#22312;13&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DyGFormer&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26032;&#22411;&#21160;&#24577;&#22270;&#23398;&#20064;&#26550;&#26500;&#65292;&#20165;&#20174;&#33410;&#28857;&#21382;&#21490;&#30340;&#31532;&#19968;&#36339;&#20132;&#20114;&#24207;&#21015;&#20013;&#23398;&#20064;&#12290;DyGFormer&#32467;&#21512;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#35774;&#35745;&#65306;&#19968;&#31181;&#37051;&#23621;&#20849;&#29616;&#32534;&#30721;&#26041;&#26696;&#65292;&#25506;&#32034;&#28304;&#33410;&#28857;&#21644;&#30446;&#26631;&#33410;&#28857;&#22522;&#20110;&#23427;&#20204;&#30340;&#24207;&#21015;&#30340;&#30456;&#20851;&#24615;&#65307;&#19968;&#31181;&#20998;&#22359;&#25216;&#26415;&#65292;&#23558;&#27599;&#20010;&#24207;&#21015;&#20998;&#25104;&#22810;&#20010;&#22359;&#24182;&#23558;&#20854;&#39304;&#36865;&#32473;Transformer&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#21463;&#30410;&#20110;&#26356;&#38271;&#26399;&#30340;&#21382;&#21490;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;DyGLib&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#24211;&#65292;&#20855;&#26377;&#26631;&#20934;&#30340;&#35757;&#32451;&#31649;&#36947;&#12289;&#21487;&#25193;&#23637;&#30340;&#32534;&#30721;&#25509;&#21475;&#21644;&#32508;&#21512;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#20197;&#20419;&#36827;&#21487;&#37325;&#22797;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#20449;&#30340;&#21160;&#24577;&#22270;&#23398;&#20064;&#30740;&#31350;&#12290;&#36890;&#36807;&#22312;&#26469;&#33258;&#21508;&#20010;&#39046;&#22495;&#30340;13&#20010;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#36827;&#34892;&#25512;&#23548;/&#24402;&#32435;&#21160;&#24577;&#38142;&#25509;&#39044;&#27979;&#21644;&#21160;&#24577;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65306;DyGFormer&#22312;mo&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
We propose DyGFormer, a new Transformer-based architecture for dynamic graph learning that solely learns from the sequences of nodes' historical first-hop interactions. DyGFormer incorporates two distinct designs: a neighbor co-occurrence encoding scheme that explores the correlations of the source node and destination node based on their sequences; a patching technique that divides each sequence into multiple patches and feeds them to Transformer, allowing the model to effectively and efficiently benefit from longer histories. We also introduce DyGLib, a unified library with standard training pipelines, extensible coding interfaces, and comprehensive evaluating protocols to promote reproducible, scalable, and credible dynamic graph learning research. By performing extensive experiments on thirteen datasets from various domains for transductive/inductive dynamic link prediction and dynamic node classification tasks, we observe that: DyGFormer achieves state-of-the-art performance on mo
&lt;/p&gt;</description></item><item><title>EDGI&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#31639;&#27861;&#65292;&#36890;&#36807;&#31561;&#21464;&#25193;&#25955;&#22788;&#29702;&#20869;&#22312;&#23545;&#31216;&#24615;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#37319;&#26679;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#20869;&#22312;&#23545;&#31216;&#24615;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.12410</link><description>&lt;p&gt;
EDGI: &#20869;&#22312;&#23545;&#31216;&#24615;&#35268;&#21010;&#30340;&#31561;&#21464;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
EDGI: Equivariant Diffusion for Planning with Embodied Agents. (arXiv:2303.12410v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12410
&lt;/p&gt;
&lt;p&gt;
EDGI&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#31639;&#27861;&#65292;&#36890;&#36807;&#31561;&#21464;&#25193;&#25955;&#22788;&#29702;&#20869;&#22312;&#23545;&#31216;&#24615;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#37319;&#26679;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#20869;&#22312;&#23545;&#31216;&#24615;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#22312;&#23545;&#31216;&#24615;&#26159;&#26102;&#31354;&#21644;&#25490;&#21015;&#19978;&#30340;&#65292;&#22823;&#22810;&#25968;&#35745;&#21010;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#27809;&#26377;&#32771;&#34385;&#36825;&#31181;&#20016;&#23500;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#23548;&#33268;&#37319;&#26679;&#25928;&#29575;&#20302;&#21644;&#27867;&#21270;&#33021;&#21147;&#24369;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#22312;&#23545;&#31216;&#24615;&#35268;&#21010;&#30340;&#31561;&#21464;&#25193;&#25955;&#31639;&#27861;(EDGI), &#21487;&#29992;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#25903;&#25345;&#22810;&#31181;&#34920;&#31034;&#24418;&#24335;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied agents operate in a structured world, often solving tasks with spatial, temporal, and permutation symmetries. Most algorithms for planning and model-based reinforcement learning (MBRL) do not take this rich geometric structure into account, leading to sample inefficiency and poor generalization. We introduce the Equivariant Diffuser for Generating Interactions (EDGI), an algorithm for MBRL and planning that is equivariant with respect to the product of the spatial symmetry group $\mathrm{SE(3)}$, the discrete-time translation group $\mathbb{Z}$, and the object permutation group $\mathrm{S}_n$. EDGI follows the Diffuser framework (Janner et al. 2022) in treating both learning a world model and planning in it as a conditional generative modeling problem, training a diffusion model on an offline trajectory dataset. We introduce a new $\mathrm{SE(3)} \times \mathbb{Z} \times \mathrm{S}_n$-equivariant diffusion model that supports multiple representations. We integrate this model i
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#23545;&#27604;&#23398;&#20064;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#20998;&#24067;&#24402;&#19968;&#21270;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24120;&#35265;&#30340;&#28857;&#20056;&#25805;&#20316;&#23548;&#33268;&#27979;&#35797;&#26102;&#20449;&#24687;&#20002;&#22833;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#27979;&#35797;&#38454;&#27573;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.11084</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#20998;&#24067;&#24402;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Test-Time Distribution Normalization for Contrastively Learned Vision-language Models. (arXiv:2302.11084v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11084
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#23545;&#27604;&#23398;&#20064;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#27979;&#35797;&#26102;&#20998;&#24067;&#24402;&#19968;&#21270;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24120;&#35265;&#30340;&#28857;&#20056;&#25805;&#20316;&#23548;&#33268;&#27979;&#35797;&#26102;&#20449;&#24687;&#20002;&#22833;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#27979;&#35797;&#38454;&#27573;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#23545;&#27604;&#23398;&#20064;&#30340;&#36827;&#23637;&#20351;&#24471;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#34920;&#31034;&#36827;&#34892;&#28857;&#20056;&#26469;&#39640;&#25928;&#20934;&#30830;&#22320;&#36827;&#34892;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#20195;&#34920;&#24615;&#26041;&#27861;&#20043;&#19968;&#26159;CLIP&#65292;&#30001;&#20110;&#20854;&#26377;&#25928;&#24615;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37319;&#29992;&#12290;CLIP&#20351;&#29992;InfoNCE&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#25439;&#22833;&#21516;&#26102;&#32771;&#34385;&#20102;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#65292;&#20197;&#24110;&#21161;&#23398;&#20064;&#26356;&#21152;&#31283;&#20581;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#24120;&#35265;&#30340;&#19979;&#28216;&#23454;&#36341;&#8212;&#8212;&#36827;&#34892;&#28857;&#20056;&#20165;&#20165;&#26159;&#23545;&#20248;&#21270;&#30446;&#26631;&#30340;&#38646;&#38454;&#36817;&#20284;&#65292;&#23548;&#33268;&#20102;&#27979;&#35797;&#26102;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#30452;&#35266;&#19978;&#65292;&#30001;&#20110;&#27169;&#22411;&#26159;&#22522;&#20110;InfoNCE&#25439;&#22833;&#36827;&#34892;&#20248;&#21270;&#30340;&#65292;&#27979;&#35797;&#26102;&#30340;&#36807;&#31243;&#20063;&#24212;&#35813;&#20445;&#25345;&#19968;&#33268;&#12290;&#38382;&#39064;&#22312;&#20110;&#22914;&#20309;&#20197;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#24335;&#26816;&#32034;&#21040;&#20219;&#20309;&#36127;&#26679;&#26412;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#20998;&#24067;&#24402;&#19968;&#21270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Advances in the field of vision-language contrastive learning have made it possible for many downstream applications to be carried out efficiently and accurately by simply taking the dot product between image and text representations. One of the most representative approaches proposed recently known as CLIP has garnered widespread adoption due to its effectiveness. CLIP is trained with an InfoNCE loss that takes into account both positive and negative samples to help learn a much more robust representation space. This paper reveals that the common downstream practice of taking a dot product is only a zeroth-order approximation of the optimization goal, resulting in a loss of information during test-time. Intuitively, since the model has been optimized based on the InfoNCE loss, test-time procedures should also be in alignment. The question lies in how one can retrieve any semblance of negative samples information during inference in a computationally efficient way. To this end, we prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20998;&#27573;&#30830;&#23450;&#24615;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#33258;&#36866;&#24212;&#31232;&#30095;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#22256;&#38590;&#37319;&#26679;&#38382;&#39064;&#30340;&#21152;&#36895;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#21487;&#34892;&#65292;&#24182;&#33021;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;MCMC&#28151;&#21512;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#12290;</title><link>http://arxiv.org/abs/2302.08724</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#27573;&#30830;&#23450;&#24615;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Piecewise Deterministic Markov Processes for Bayesian Neural Networks. (arXiv:2302.08724v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20998;&#27573;&#30830;&#23450;&#24615;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#33258;&#36866;&#24212;&#31232;&#30095;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#22256;&#38590;&#37319;&#26679;&#38382;&#39064;&#30340;&#21152;&#36895;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#21487;&#34892;&#65292;&#24182;&#33021;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;MCMC&#28151;&#21512;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#25512;&#29702;&#36890;&#24120;&#20381;&#36182;&#20110;&#21464;&#20998;&#25512;&#26029;&#22788;&#29702;&#65292;&#36825;&#35201;&#27714;&#36829;&#21453;&#20102;&#29420;&#31435;&#24615;&#21644;&#21518;&#39564;&#24418;&#24335;&#30340;&#20551;&#35774;&#12290;&#20256;&#32479;&#30340;MCMC&#26041;&#27861;&#36991;&#20813;&#20102;&#36825;&#20123;&#20551;&#35774;&#65292;&#20294;&#30001;&#20110;&#26080;&#27861;&#36866;&#24212;&#20284;&#28982;&#30340;&#23376;&#37319;&#26679;&#65292;&#23548;&#33268;&#35745;&#31639;&#37327;&#22686;&#21152;&#12290;&#26032;&#30340;&#20998;&#27573;&#30830;&#23450;&#24615;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65288;PDMP&#65289;&#37319;&#26679;&#22120;&#20801;&#35768;&#23376;&#37319;&#26679;&#65292;&#20294;&#24341;&#20837;&#20102;&#27169;&#22411;&#29305;&#23450;&#30340;&#19981;&#22343;&#21248;&#27850;&#26494;&#36807;&#31243;&#65288;IPPs&#65289;&#65292;&#20174;&#20013;&#37319;&#26679;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#33258;&#36866;&#24212;&#31232;&#30095;&#26041;&#26696;&#65292;&#29992;&#20110;&#20174;&#36825;&#20123;IPPs&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21152;&#36895;&#23558;PDMPs&#24212;&#29992;&#20110;BNNs&#25512;&#29702;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#25512;&#29702;&#22312;&#35745;&#31639;&#19978;&#26159;&#21487;&#34892;&#30340;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;MCMC&#28151;&#21512;&#24615;&#33021;&#65292;&#24182;&#19982;&#20854;&#20182;&#36817;&#20284;&#25512;&#29702;&#26041;&#26696;&#30456;&#27604;&#65292;&#25552;&#20379;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inference on modern Bayesian Neural Networks (BNNs) often relies on a variational inference treatment, imposing violated assumptions of independence and the form of the posterior. Traditional MCMC approaches avoid these assumptions at the cost of increased computation due to its incompatibility to subsampling of the likelihood. New Piecewise Deterministic Markov Process (PDMP) samplers permit subsampling, though introduce a model specific inhomogenous Poisson Process (IPPs) which is difficult to sample from. This work introduces a new generic and adaptive thinning scheme for sampling from these IPPs, and demonstrates how this approach can accelerate the application of PDMPs for inference in BNNs. Experimentation illustrates how inference with these methods is computationally feasible, can improve predictive accuracy, MCMC mixing performance, and provide informative uncertainty measurements when compared against other approximate inference schemes.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21333;&#27425;&#32463;&#39564;&#38544;&#31169;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#36827;&#34892;&#38544;&#31169;&#25439;&#22833;&#23457;&#35745;&#65292;&#19988;&#26080;&#38656;&#20107;&#20808;&#20102;&#35299;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#25110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#36866;&#29992;&#20110;&#22312;&#23454;&#36341;&#20013;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;</title><link>http://arxiv.org/abs/2302.03098</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21333;&#27425;&#32463;&#39564;&#38544;&#31169;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One-shot Empirical Privacy Estimation for Federated Learning. (arXiv:2302.03098v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21333;&#27425;&#32463;&#39564;&#38544;&#31169;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#36827;&#34892;&#38544;&#31169;&#25439;&#22833;&#23457;&#35745;&#65292;&#19988;&#26080;&#38656;&#20107;&#20808;&#20102;&#35299;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#25110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#36866;&#29992;&#20110;&#22312;&#23454;&#36341;&#20013;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;ially private&#65288;DP&#65289;&#31639;&#27861;&#30340;&#38544;&#31169;&#20272;&#35745;&#25216;&#26415;&#21487;&#29992;&#20110;&#19982;&#20998;&#26512;&#19978;&#30028;&#36827;&#34892;&#27604;&#36739;&#65292;&#25110;&#22312;&#24050;&#30693;&#20998;&#26512;&#19978;&#30028;&#19981;&#32039;&#30340;&#24773;&#20917;&#19979;&#23454;&#39564;&#27979;&#37327;&#38544;&#31169;&#25439;&#22833;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#38544;&#31169;&#23457;&#35745;&#25216;&#26415;&#36890;&#24120;&#23545;&#23545;&#25163;&#20570;&#20986;&#24378;&#28872;&#20551;&#35774;&#65288;&#20363;&#22914;&#65292;&#20102;&#35299;&#20013;&#38388;&#27169;&#22411;&#36845;&#20195;&#25110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65289;&#65292;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#21644;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#35843;&#25972;&#65292;&#24182;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#22810;&#27425;&#65288;&#36890;&#24120;&#25968;&#37327;&#32423;&#20026;&#25968;&#21315;&#65289;&#12290;&#36825;&#20123;&#32570;&#28857;&#20351;&#24471;&#22312;&#23454;&#36341;&#20013;&#38590;&#20197;&#22823;&#35268;&#27169;&#37096;&#32626;&#27492;&#31867;&#25216;&#26415;&#65292;&#23588;&#20854;&#26159;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#65292;&#27169;&#22411;&#35757;&#32451;&#21487;&#33021;&#38656;&#35201;&#25968;&#22825;&#25110;&#25968;&#21608;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#21333;&#27425;&#8221;&#26041;&#27861;&#65292;&#21487;&#20197;&#31995;&#32479;&#22320;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#21333;&#20010;&#35757;&#32451;&#36816;&#34892;&#26399;&#38388;&#39640;&#25928;&#22320;&#23457;&#35745;&#25110;&#20272;&#35745;&#27169;&#22411;&#30340;&#38544;&#31169;&#25439;&#22833;&#65292;&#32780;&#19981;&#38656;&#35201;&#20107;&#20808;&#20102;&#35299;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#25110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#31561;&#35774;&#32622;&#20013;&#20351;&#29992;&#30340;&#19968;&#33324;DP&#31639;&#27861;&#65292;&#24182;&#30001;&#23454;&#39564;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20854;&#25552;&#20379;&#30340;&#20934;&#30830;&#38544;&#31169;&#25439;&#22833;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy estimation techniques for differentially private (DP) algorithms are useful for comparing against analytical bounds, or to empirically measure privacy loss in settings where known analytical bounds are not tight. However, existing privacy auditing techniques usually make strong assumptions on the adversary (e.g., knowledge of intermediate model iterates or the training data distribution), are tailored to specific tasks and model architectures, and require retraining the model many times (typically on the order of thousands). These shortcomings make deploying such techniques at scale difficult in practice, especially in federated settings where model training can take days or weeks. In this work, we present a novel "one-shot" approach that can systematically address these challenges, allowing efficient auditing or estimation of the privacy loss of a model during the same, single training run used to fit model parameters, and without requiring any a priori knowledge about the mod
&lt;/p&gt;</description></item><item><title>"IC3: Image Captioning by Committee Consensus"&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#22996;&#21592;&#20250;&#20849;&#35782;&#29983;&#25104;&#22270;&#20687;&#23383;&#24149;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#22810;&#20010;&#27880;&#37322;&#32773;&#30340;&#35270;&#35282;&#25429;&#25417;&#39640;&#23618;&#32454;&#33410;&#65292;&#20248;&#20110;&#21333;&#20010;&#20154;&#29983;&#25104;&#30340;&#21442;&#32771;&#23383;&#24149;&#65292;&#24182;&#22312;&#35270;&#35273;&#25551;&#36848;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2302.01328</link><description>&lt;p&gt;
IC3&#65306;&#36890;&#36807;&#22996;&#21592;&#20250;&#20849;&#35782;&#36827;&#34892;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
IC3: Image Captioning by Committee Consensus. (arXiv:2302.01328v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01328
&lt;/p&gt;
&lt;p&gt;
"IC3: Image Captioning by Committee Consensus"&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#22996;&#21592;&#20250;&#20849;&#35782;&#29983;&#25104;&#22270;&#20687;&#23383;&#24149;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#22810;&#20010;&#27880;&#37322;&#32773;&#30340;&#35270;&#35282;&#25429;&#25417;&#39640;&#23618;&#32454;&#33410;&#65292;&#20248;&#20110;&#21333;&#20010;&#20154;&#29983;&#25104;&#30340;&#21442;&#32771;&#23383;&#24149;&#65292;&#24182;&#22312;&#35270;&#35273;&#25551;&#36848;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#20320;&#35831;&#19968;&#20010;&#20154;&#25551;&#36848;&#19968;&#24133;&#22270;&#20687;&#65292;&#20182;&#20204;&#21487;&#33021;&#20250;&#29992;&#19968;&#21315;&#31181;&#19981;&#21516;&#30340;&#26041;&#24335;&#26469;&#25551;&#36848;&#12290;&#20256;&#32479;&#19978;&#65292;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#34987;&#35757;&#32451;&#25104;&#29983;&#25104;&#19968;&#20010;&#8220;&#26368;&#20339;&#8221;&#65288;&#19982;&#21442;&#32771;&#26368;&#30456;&#20284;&#65289;&#30340;&#22270;&#20687;&#23383;&#24149;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#20570;&#20250;&#40723;&#21169;&#29983;&#25104;&#8220;&#20449;&#24687;&#36139;&#20047;&#8221;&#30340;&#23383;&#24149;&#65292;&#24182;&#19988;&#21482;&#20851;&#27880;&#21487;&#33021;&#32454;&#33410;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#32780;&#24573;&#30053;&#20102;&#22330;&#26223;&#20013;&#20854;&#20182;&#21487;&#33021;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26032;&#39062;&#30340;&#26041;&#27861;&#65306;"&#36890;&#36807;&#22996;&#21592;&#20250;&#20849;&#35782;&#36827;&#34892;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;"&#65288;IC3&#65289;&#65292;&#26088;&#22312;&#29983;&#25104;&#19968;&#20010;&#33021;&#22815;&#20174;&#22810;&#20010;&#27880;&#37322;&#32773;&#30340;&#35270;&#35282;&#25429;&#25417;&#21040;&#39640;&#23618;&#32454;&#33410;&#30340;&#21333;&#20010;&#23383;&#24149;&#12290;&#20154;&#31867;&#35780;&#20215;IC3&#29983;&#25104;&#30340;&#23383;&#24149;&#33267;&#23569;&#19982;&#22522;&#20934;SOTA&#27169;&#22411;&#19968;&#26679;&#26377;&#24110;&#21161;&#30340;&#24773;&#20917;&#21344;&#20102;&#19977;&#20998;&#20043;&#20108;&#20197;&#19978;&#65292;&#24182;&#19988;IC3&#21487;&#20197;&#23558;SOTA&#33258;&#21160;&#21484;&#22238;&#31995;&#32479;&#30340;&#24615;&#33021;&#25552;&#21319;&#39640;&#36798;84%&#65292;&#32988;&#36807;&#21333;&#20010;&#20154;&#29983;&#25104;&#30340;&#21442;&#32771;&#23383;&#24149;&#65292;&#24182;&#26174;&#31034;&#20986;&#22312;&#35270;&#35273;&#25551;&#36848;&#26041;&#38754;&#30456;&#27604;&#20110;SOTA&#26041;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#20195;&#30721;&#21487;&#36890;&#36807;https://davidmchan&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
If you ask a human to describe an image, they might do so in a thousand different ways. Traditionally, image captioning models are trained to generate a single "best" (most like a reference) image caption. Unfortunately, doing so encourages captions that are "informationally impoverished," and focus on only a subset of the possible details, while ignoring other potentially useful information in the scene. In this work, we introduce a simple, yet novel, method: "Image Captioning by Committee Consensus" (IC3), designed to generate a single caption that captures high-level details from several annotator viewpoints. Humans rate captions produced by IC3 at least as helpful as baseline SOTA models more than two thirds of the time, and IC3 can improve the performance of SOTA automated recall systems by up to 84%, outperforming single human-generated reference captions, and indicating significant improvements over SOTA approaches for visual description. Code is available at https://davidmchan.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#25454;&#30340;&#20851;&#31995;&#32467;&#26500;&#35782;&#21035;&#26631;&#31614;&#38169;&#35823;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#35821;&#35328;&#39046;&#22495;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2301.12321</link><description>&lt;p&gt;
&#31070;&#32463;&#20851;&#31995;&#22270;&#65306;&#35782;&#21035;&#26631;&#31614;&#22122;&#38899;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Neural Relation Graph: A Unified Framework for Identifying Label Noise and Outlier Data. (arXiv:2301.12321v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25968;&#25454;&#30340;&#20851;&#31995;&#32467;&#26500;&#35782;&#21035;&#26631;&#31614;&#38169;&#35823;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#35821;&#35328;&#39046;&#22495;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35786;&#26029;&#21644;&#28165;&#29702;&#25968;&#25454;&#26159;&#26500;&#24314;&#20581;&#22766;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#23384;&#22312;&#22797;&#26434;&#38382;&#39064;&#65292;&#22914;&#26631;&#31614;&#38169;&#35823;&#12289;&#27424;&#34920;&#31034;&#21644;&#24322;&#24120;&#20540;&#65292;&#22240;&#27492;&#22312;&#20855;&#26377;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29305;&#24449;&#23884;&#20837;&#31354;&#38388;&#20013;&#25968;&#25454;&#30340;&#20851;&#31995;&#32467;&#26500;&#36825;&#19968;&#34987;&#24573;&#35270;&#30340;&#20449;&#24687;&#26469;&#28304;&#65292;&#26469;&#35782;&#21035;&#26377;&#38382;&#39064;&#30340;&#25968;&#25454;&#30340;&#32479;&#19968;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#20851;&#31995;&#22270;&#32467;&#26500;&#26469;&#26816;&#27979;&#26631;&#31614;&#38169;&#35823;&#21644;&#24322;&#24120;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#21644;&#26377;&#25928;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#25552;&#20379;&#29305;&#24449;&#23884;&#20837;&#31354;&#38388;&#20013;&#25968;&#25454;&#28857;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20316;&#20026;&#20132;&#20114;&#24335;&#35786;&#26029;&#25968;&#25454;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#35821;&#35328;&#39046;&#22495;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26631;&#31614;&#38169;&#35823;&#21644;&#31163;&#32676;&#20540;/&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diagnosing and cleaning data is a crucial step for building robust machine learning systems. However, identifying problems within large-scale datasets with real-world distributions is challenging due to the presence of complex issues such as label errors, under-representation, and outliers. In this paper, we propose a unified approach for identifying the problematic data by utilizing a largely ignored source of information: a relational structure of data in the feature-embedded space. To this end, we present scalable and effective algorithms for detecting label errors and outlier data based on the relational graph structure of data. We further introduce a visualization tool that provides contextual information of a data point in the feature-embedded space, serving as an effective tool for interactively diagnosing data. We evaluate the label error and outlier/out-of-distribution (OOD) detection performances of our approach on the large-scale image, speech, and language domain tasks, inc
&lt;/p&gt;</description></item><item><title>DISSC&#26159;&#19968;&#31181;&#26032;&#39062;&#12289;&#36731;&#37327;&#32423;&#30340;&#35821;&#38899;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#20197;&#26080;&#38656;&#25991;&#26412;&#30340;&#26041;&#24335;&#23558;&#24405;&#38899;&#30340;&#33410;&#22863;&#12289;&#38899;&#39640;&#36718;&#24275;&#21644;&#38899;&#33394;&#36716;&#25442;&#20026;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#39118;&#26684;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#33258;&#30417;&#30563;&#27169;&#22411;&#32534;&#30721;&#35821;&#38899;&#20026;&#31163;&#25955;&#21333;&#20803;&#65292;&#20855;&#26377;&#31616;&#21333;&#12289;&#26377;&#25928;&#19988;&#24555;&#36895;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#36866;&#29992;&#20110;&#26080;&#37197;&#23545;&#25968;&#25454;&#30340;&#22810;&#23545;&#22810;&#35821;&#38899;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2212.09730</link><description>&lt;p&gt;
&#22312;&#27874;&#24418;&#22495;&#20013;&#20351;&#29992;&#31163;&#25955;&#33258;&#30417;&#30563;&#21333;&#20803;&#36827;&#34892;&#35821;&#38899;&#39118;&#26684;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Speaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units. (arXiv:2212.09730v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09730
&lt;/p&gt;
&lt;p&gt;
DISSC&#26159;&#19968;&#31181;&#26032;&#39062;&#12289;&#36731;&#37327;&#32423;&#30340;&#35821;&#38899;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#20197;&#26080;&#38656;&#25991;&#26412;&#30340;&#26041;&#24335;&#23558;&#24405;&#38899;&#30340;&#33410;&#22863;&#12289;&#38899;&#39640;&#36718;&#24275;&#21644;&#38899;&#33394;&#36716;&#25442;&#20026;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#39118;&#26684;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#33258;&#30417;&#30563;&#27169;&#22411;&#32534;&#30721;&#35821;&#38899;&#20026;&#31163;&#25955;&#21333;&#20803;&#65292;&#20855;&#26377;&#31616;&#21333;&#12289;&#26377;&#25928;&#19988;&#24555;&#36895;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#36866;&#29992;&#20110;&#26080;&#37197;&#23545;&#25968;&#25454;&#30340;&#22810;&#23545;&#22810;&#35821;&#38899;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DISSC&#30340;&#26032;&#39062;&#12289;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#20197;&#26080;&#38656;&#25991;&#26412;&#30340;&#26041;&#24335;&#23558;&#24405;&#38899;&#30340;&#33410;&#22863;&#12289;&#38899;&#39640;&#36718;&#24275;&#21644;&#38899;&#33394;&#36716;&#25442;&#20026;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#39118;&#26684;&#12290;&#19982;DISSC&#19981;&#21516;&#65292;&#22823;&#22810;&#25968;&#35821;&#38899;&#36716;&#25442;&#65288;VC&#65289;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#38899;&#33394;&#65292;&#24182;&#24573;&#30053;&#20154;&#20204;&#29420;&#29305;&#30340;&#35828;&#35805;&#39118;&#26684;&#65288;&#38901;&#24459;&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#23558;&#35821;&#38899;&#32534;&#30721;&#20026;&#31163;&#25955;&#21333;&#20803;&#65292;&#20351;&#24471;&#35757;&#32451;&#31616;&#21333;&#12289;&#26377;&#25928;&#19988;&#24555;&#36895;&#12290;&#25152;&#26377;&#30340;&#36716;&#25442;&#27169;&#22359;&#20165;&#22312;&#37325;&#24314;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#26080;&#37197;&#23545;&#25968;&#25454;&#30340;&#22810;&#23545;&#22810;&#35821;&#38899;&#36716;&#25442;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#22871;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;DISSC&#22312;&#36825;&#20010;&#35774;&#32622;&#19979;&#26126;&#26174;&#20248;&#20110;&#35780;&#20272;&#22522;&#32447;&#12290;&#20195;&#30721;&#21644;&#26679;&#20363;&#21487;&#22312;https://pages.cs.huji.ac.il/adiyoss-lab/dissc/ &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce DISSC, a novel, lightweight method that converts the rhythm, pitch contour and timbre of a recording to a target speaker in a textless manner. Unlike DISSC, most voice conversion (VC) methods focus primarily on timbre, and ignore people's unique speaking style (prosody). The proposed approach uses a pretrained, self-supervised model for encoding speech to discrete units, which makes it simple, effective, and fast to train. All conversion modules are only trained on reconstruction like tasks, thus suitable for any-to-many VC with no paired data. We introduce a suite of quantitative and qualitative evaluation metrics for this setup, and empirically demonstrate that DISSC significantly outperforms the evaluated baselines. Code and samples are available at https://pages.cs.huji.ac.il/adiyoss-lab/dissc/.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#31616;&#21270;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#22823;&#23398;&#20064;&#29575;&#19979;&#38750;&#20984;&#35757;&#32451;&#21160;&#24577;&#30340;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#23398;&#20064;&#38408;&#20540;&#26679;&#24335;&#31070;&#32463;&#20803;&#30340;&#20020;&#30028;&#27493;&#38271;&#12290;</title><link>http://arxiv.org/abs/2212.07469</link><description>&lt;p&gt;
&#36890;&#36807;"&#31283;&#23450;&#36793;&#32536;"&#23398;&#20064;&#38408;&#20540;&#31070;&#32463;&#20803;
&lt;/p&gt;
&lt;p&gt;
Learning threshold neurons via the "edge of stability". (arXiv:2212.07469v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07469
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;&#31616;&#21270;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#22823;&#23398;&#20064;&#29575;&#19979;&#38750;&#20984;&#35757;&#32451;&#21160;&#24577;&#30340;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#23398;&#20064;&#38408;&#20540;&#26679;&#24335;&#31070;&#32463;&#20803;&#30340;&#20020;&#30028;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20998;&#26512;&#36890;&#24120;&#22522;&#20110;&#26497;&#23567;&#23398;&#20064;&#29575;&#30340;&#19981;&#29616;&#23454;&#20551;&#35774;&#12290;&#19982;&#23454;&#38469;&#26234;&#24935;&#21644;&#32463;&#39564;&#30740;&#31350;&#30456;&#21453;&#65292;&#20363;&#22914;J. Cohen&#31561;&#20154;&#30340;&#24037;&#20316;&#65288;ICLR 2021&#65289;&#65292;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#26032;&#29616;&#35937;&#65288;"&#31283;&#23450;&#36793;&#32536;"&#25110;"&#19981;&#31283;&#23450;&#25910;&#25947;"&#65289;&#65292;&#20197;&#21450;&#22823;&#23398;&#20064;&#29575;&#20307;&#21046;&#19979;&#30340;&#28508;&#22312;&#27867;&#21270;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#26377;&#22823;&#37327;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#30740;&#31350;&#65292;&#20294;&#21518;&#19968;&#31181;&#25928;&#24212;&#20173;&#28982;&#29702;&#35299;&#26377;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#31616;&#21270;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#65292;&#36808;&#20986;&#20102;&#29702;&#35299;&#22823;&#23398;&#20064;&#29575;&#19979;&#30495;&#27491;&#38750;&#20984;&#35757;&#32451;&#21160;&#24577;&#30340;&#19968;&#27493;&#12290;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20010;&#23574;&#38160;&#30340;&#38454;&#36291;&#36716;&#21464;&#65292;&#24403;&#27493;&#38271;&#23567;&#20110;&#27492;&#20540;&#26102;&#65292;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#23398;&#20064;&#21040;"&#38408;&#20540;&#26679;&#24335;"&#31070;&#32463;&#20803;&#65288;&#21363;&#20855;&#26377;&#38750;&#38646;&#31532;&#19968;&#23618;&#20559;&#32622;&#30340;&#31070;&#32463;&#20803;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing analyses of neural network training often operate under the unrealistic assumption of an extremely small learning rate. This lies in stark contrast to practical wisdom and empirical studies, such as the work of J. Cohen et al. (ICLR 2021), which exhibit startling new phenomena (the "edge of stability" or "unstable convergence") and potential benefits for generalization in the large learning rate regime. Despite a flurry of recent works on this topic, however, the latter effect is still poorly understood. In this paper, we take a step towards understanding genuinely non-convex training dynamics with large learning rates by performing a detailed analysis of gradient descent for simplified models of two-layer neural networks. For these models, we provably establish the edge of stability phenomenon and discover a sharp phase transition for the step size below which the neural network fails to learn "threshold-like" neurons (i.e., neurons with a non-zero first-layer bias). This elu
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20851;&#20110;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#26368;&#20248;&#26435;&#34913;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#19978;&#21487;&#34892;&#30340;&#26399;&#26395;&#35823;&#24046;&#26368;&#23567;&#21270;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#35745;&#31639;&#20998;&#37197;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.01365</link><description>&lt;p&gt;
&#35745;&#31639;&#26368;&#20248;&#31070;&#32463;&#32593;&#32476;&#32553;&#25918;&#23450;&#24459;&#30340;&#20449;&#24687;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Information-Theoretic Analysis of Compute-Optimal Neural Scaling Laws. (arXiv:2212.01365v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01365
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20851;&#20110;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#26368;&#20248;&#26435;&#34913;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#19978;&#21487;&#34892;&#30340;&#26399;&#26395;&#35823;&#24046;&#26368;&#23567;&#21270;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#35745;&#31639;&#20998;&#37197;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#20043;&#38388;&#30340;&#35745;&#31639;&#26368;&#20248;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31867;&#20284;&#20110;&#30333;&#40736;&#23454;&#39564;&#25152;&#25903;&#25345;&#30340;&#32447;&#24615;&#20851;&#31995;&#12290;&#23613;&#31649;&#30333;&#40736;&#23454;&#39564;&#30740;&#31350;&#30340;&#26159;&#22522;&#20110;MassiveText&#35821;&#26009;&#24211;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#31616;&#21270;&#30340;&#23398;&#20064;&#27169;&#22411;&#21644;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#27599;&#20010;&#37117;&#22522;&#20110;&#19968;&#20010;&#20855;&#26377;S&#22411;&#36755;&#20986;&#21333;&#20803;&#21644;&#21333;&#38544;&#34255;&#23618;ReLU&#28608;&#27963;&#21333;&#20803;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31867;&#31639;&#27861;&#30340;&#19968;&#33324;&#35823;&#24046;&#19978;&#30028;&#65292;&#36825;&#20123;&#31639;&#27861;&#36880;&#27493;&#26356;&#26032;&#19968;&#20010;&#32479;&#35745;&#37327;&#65288;&#20363;&#22914;&#26799;&#24230;&#19979;&#38477;&#65289;&#12290;&#38024;&#23545;&#21463;&#21040;barron 1993&#21551;&#21457;&#30340;&#29305;&#23450;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#19978;&#30028;&#65292;&#35813;&#19978;&#30028;&#26159;&#20316;&#20026;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#20989;&#25968;&#30340;&#20449;&#24687;&#35770;&#19978;&#21487;&#23454;&#29616;&#30340;&#26399;&#26395;&#35823;&#24046;&#30340;&#26368;&#23567;&#20540;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#26368;&#23567;&#21270;&#36825;&#19968;&#19978;&#30028;&#30340;&#35745;&#31639;&#20998;&#37197;&#26041;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20123;&#23454;&#35777;&#32467;&#26524;&#65292;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;...&#65288;&#24453;&#32493;&#65289;
&lt;/p&gt;
&lt;p&gt;
We study the compute-optimal trade-off between model and training data set sizes for large neural networks. Our result suggests a linear relation similar to that supported by the empirical analysis of chinchilla. While that work studies transformer-based large language models trained on the MassiveText corpus gopher, as a starting point for development of a mathematical theory, we focus on a simpler learning model and data generating process, each based on a neural network with a sigmoidal output unit and single hidden layer of ReLU activation units. We introduce general error upper bounds for a class of algorithms which incrementally update a statistic (for example gradient descent). For a particular learning model inspired by barron 1993, we establish an upper bound on the minimal information-theoretically achievable expected error as a function of model and data set sizes. We then derive allocations of computation that minimize this bound. We present empirical results which suggest 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#38480;&#26102;&#38388;&#38543;&#26426;&#32447;&#24615;&#20108;&#27425;&#25511;&#21046;&#38382;&#39064;&#20013;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;Fisher&#20960;&#20309;&#21644;Bures-Wasserstein&#20960;&#20309;&#30340;&#20960;&#20309;&#24863;&#30693;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20197;&#32447;&#24615;&#36895;&#29575;&#20840;&#23616;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2211.00617</link><description>&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#26377;&#38480;&#26102;&#38388;&#38543;&#26426;&#32447;&#24615;&#20108;&#27425;&#25511;&#21046;&#38382;&#39064;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convergence of policy gradient methods for finite-horizon stochastic linear-quadratic control problems. (arXiv:2211.00617v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#38480;&#26102;&#38388;&#38543;&#26426;&#32447;&#24615;&#20108;&#27425;&#25511;&#21046;&#38382;&#39064;&#20013;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;Fisher&#20960;&#20309;&#21644;Bures-Wasserstein&#20960;&#20309;&#30340;&#20960;&#20309;&#24863;&#30693;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20197;&#32447;&#24615;&#36895;&#29575;&#20840;&#23616;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#38480;&#26102;&#38388;&#36830;&#32493;&#26102;&#38388;&#25506;&#32034;&#24615;&#32447;&#24615;&#20108;&#27425;&#25511;&#21046;&#38382;&#39064;&#20013;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#24615;&#12290;&#35813;&#35774;&#32622;&#21253;&#25324;&#20855;&#26377;&#38750;&#30830;&#23450;&#24615;&#25104;&#26412;&#21644;&#20801;&#35768;&#30446;&#26631;&#20013;&#28155;&#21152;&#39069;&#22806;&#30340;&#29109;&#27491;&#21017;&#21270;&#39033;&#30340;&#38543;&#26426;&#32447;&#24615;&#20108;&#27425;&#25511;&#21046;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#36830;&#32493;&#26102;&#38388;&#30340;&#39640;&#26031;&#31574;&#30053;&#65292;&#20854;&#22343;&#20540;&#26159;&#29366;&#24577;&#21464;&#37327;&#30340;&#32447;&#24615;&#20989;&#25968;&#65292;&#26041;&#24046;&#19982;&#29366;&#24577;&#26080;&#20851;&#12290;&#19982;&#31163;&#25955;&#26102;&#38388;&#38382;&#39064;&#30456;&#21453;&#65292;&#31574;&#30053;&#20013;&#30340;&#25104;&#26412;&#20989;&#25968;&#19981;&#26159;&#20005;&#26684;&#20984;&#20989;&#25968;&#65292;&#24182;&#19988;&#24182;&#38750;&#25152;&#26377;&#30340;&#19979;&#38477;&#26041;&#21521;&#37117;&#23548;&#33268;&#26377;&#30028;&#30340;&#36845;&#20195;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Fisher&#20960;&#20309;&#21644;Bures-Wasserstein&#20960;&#20309;&#30340;&#31574;&#30053;&#22343;&#20540;&#21644;&#21327;&#26041;&#24046;&#30340;&#20960;&#20309;&#24863;&#30693;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#12290;&#31574;&#30053;&#36845;&#20195;&#34987;&#35777;&#26126;&#28385;&#36275;&#20808;&#39564;&#30028;&#65292;&#24182;&#20197;&#32447;&#24615;&#36895;&#29575;&#20840;&#23616;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#31163;&#25955;&#26102;&#38388;&#31574;&#30053;&#30340;PG&#26041;&#27861;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#36830;&#32493;&#26102;&#38388;&#30340;&#20998;&#26512;&#32467;&#26524;&#65292;&#24182;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;&#32447;&#24615;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the global linear convergence of policy gradient (PG) methods for finite-horizon continuous-time exploratory linear-quadratic control (LQC) problems. The setting includes stochastic LQC problems with indefinite costs and allows additional entropy regularisers in the objective. We consider a continuous-time Gaussian policy whose mean is linear in the state variable and whose covariance is state-independent. Contrary to discrete-time problems, the cost is noncoercive in the policy and not all descent directions lead to bounded iterates. We propose geometry-aware gradient descents for the mean and covariance of the policy using the Fisher geometry and the Bures-Wasserstein geometry, respectively. The policy iterates are shown to satisfy an a-priori bound, and converge globally to the optimal policy with a linear rate. We further propose a novel PG method with discrete-time policies. The algorithm leverages the continuous-time analysis, and achieves a robust linear convergence acr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#36172;&#21338;&#26426;&#22312;&#35821;&#38899;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#35752;&#35770;&#20102;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#23427;&#20204;&#35299;&#20915;&#30456;&#20851;&#38382;&#39064;&#65292;&#26500;&#24314;&#36866;&#24212;&#24615;&#12289;&#20132;&#20114;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.13623</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#21644;&#36172;&#21338;&#26426;&#22312;&#35821;&#38899;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;: &#25945;&#31243;&#65292;&#22238;&#39038;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning and Bandits for Speech and Language Processing: Tutorial, Review and Outlook. (arXiv:2210.13623v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13623
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#36172;&#21338;&#26426;&#22312;&#35821;&#38899;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#35752;&#35770;&#20102;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#23427;&#20204;&#35299;&#20915;&#30456;&#20851;&#38382;&#39064;&#65292;&#26500;&#24314;&#36866;&#24212;&#24615;&#12289;&#20132;&#20114;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#24378;&#21270;&#23398;&#20064;&#21644;&#36172;&#21338;&#26426;&#24050;&#32463;&#22312;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#65292;&#37329;&#34701;&#65292;&#25512;&#33616;&#31995;&#32479;&#65292;&#26426;&#22120;&#20154;&#20197;&#21450;&#35821;&#38899;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#21457;&#29983;&#20102;&#36716;&#21464;&#12290;&#34429;&#28982;&#22823;&#37096;&#20998;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#35821;&#38899;&#21644;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#24212;&#29992;&#20027;&#35201;&#38598;&#20013;&#20110;&#21033;&#29992;&#20854;&#28789;&#27963;&#30340;&#20248;&#21270;&#23646;&#24615;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#65292;&#20294;&#20173;&#26377;&#35768;&#22810;&#30740;&#31350;&#31354;&#38388;&#21487;&#20197;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#22909;&#22788;&#65292;&#27604;&#22914;&#22522;&#20110;&#22870;&#21169;&#39537;&#21160;&#30340;&#36866;&#24212;&#24615;&#12289;&#29366;&#24577;&#34920;&#31034;&#12289;&#26102;&#38388;&#32467;&#26500;&#21644;&#36890;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#36172;&#21338;&#26426;&#30340;&#26368;&#26032;&#36827;&#23637;&#32508;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#26377;&#25928;&#22320;&#21033;&#29992;&#23427;&#20204;&#26469;&#35299;&#20915;&#35821;&#38899;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#26500;&#24314;&#36866;&#24212;&#24615;&#12289;&#20132;&#20114;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, reinforcement learning and bandits have transformed a wide range of real-world applications including healthcare, finance, recommendation systems, robotics, and last but not least, the speech and natural language processing. While most speech and language applications of reinforcement learning algorithms are centered around improving the training of deep neural networks with its flexible optimization properties, there are still many grounds to explore to utilize the benefits of reinforcement learning, such as its reward-driven adaptability, state representations, temporal structures and generalizability. In this survey, we present an overview of recent advancements of reinforcement learning and bandits, and discuss how they can be effectively employed to solve speech and natural language processing problems with models that are adaptive, interactive and scalable.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#33041;&#20449;&#21495;&#21644;&#20154;&#31867;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MTAM&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20851;&#31995;&#26816;&#27979;&#31561;&#19979;&#28216;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.06348</link><description>&lt;p&gt;
&#33021;&#21542;&#36890;&#36807;&#33041;&#20449;&#21495;&#25581;&#31034;&#20154;&#31867;&#35821;&#35328;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Brain Signals Reveal Inner Alignment with Human Languages?. (arXiv:2208.06348v4 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#33041;&#20449;&#21495;&#21644;&#20154;&#31867;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MTAM&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20851;&#31995;&#26816;&#27979;&#31561;&#19979;&#28216;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#20449;&#21495;&#65288;&#22914;&#33041;&#30005;&#22270;&#65289;&#21644;&#20154;&#31867;&#35821;&#35328;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20108;&#32773;&#20043;&#38388;&#30340;&#32852;&#31995;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33041;&#30005;&#22270;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#20381;&#36182;&#24615;&#12290;&#22312;&#34920;&#31034;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;MTAM&#65288;Multimodal Transformer Alignment Model&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35266;&#23519;&#36825;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#21327;&#35843;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22810;&#31181;&#20851;&#31995;&#23545;&#40784;&#25216;&#26415;&#65292;&#22914;&#20856;&#22411;&#30456;&#20851;&#20998;&#26512;&#21644;Wasserstein&#36317;&#31163;&#65292;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#26469;&#36716;&#25442;&#29305;&#24449;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#21644;&#20851;&#31995;&#26816;&#27979;&#31561;&#19979;&#28216;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#22312;ZuCo&#21644;K-EmoCon&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#26512;&#26041;&#38754;&#20351;K-EmoCon&#25968;&#25454;&#38598;&#30340;F1&#20998;&#25968;&#25552;&#39640;&#20102;1.7&#65285;&#65292;ZuCo&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;9.3&#65285;&#65292;&#22312;&#20851;&#31995;&#26816;&#27979;&#26041;&#38754;ZuCo&#25968;&#25454;&#38598;&#25552;&#39640;&#20102;7.4&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22269;&#38469;&#19978;&#26368;&#22823;&#30340;&#20154;&#31867;&#31867;&#27604;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#32534;&#30721;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain Signals, such as Electroencephalography (EEG), and human languages have been widely explored independently for many downstream tasks, however, the connection between them has not been well explored. In this study, we explore the relationship and dependency between EEG and language. To study at the representation level, we introduced \textbf{MTAM}, a \textbf{M}ultimodal \textbf{T}ransformer \textbf{A}lignment \textbf{M}odel, to observe coordinated representations between the two modalities. We used various relationship alignment-seeking techniques, such as Canonical Correlation Analysis and Wasserstein Distance, as loss functions to transfigure features. On downstream applications, sentiment analysis and relation detection, we achieved new state-of-the-art results on two datasets, ZuCo and K-EmoCon. Our method achieved an F1-score improvement of 1.7% on K-EmoCon and 9.3% on Zuco datasets for sentiment analysis, and 7.4% on ZuCo for relation detection. In addition, we provide inter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;-&#32858;&#31867;&#65292;&#24182;&#28145;&#20837;&#25551;&#36848;&#20102;&#26368;&#24120;&#29992;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#21442;&#25968;&#36873;&#25321;&#21644;&#21021;&#22987;&#21270;&#30340;&#25351;&#23548;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#27604;&#36739;&#19977;&#20010;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#25928;&#29575;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#31639;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#20248;&#21155;&#12290;</title><link>http://arxiv.org/abs/2207.06949</link><description>&lt;p&gt;
&#23547;&#27714;&#25968;&#25454;&#32972;&#21518;&#30340;&#30495;&#30456;&#65306;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Seeking the Truth Beyond the Data. An Unsupervised Machine Learning Approach. (arXiv:2207.06949v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;-&#32858;&#31867;&#65292;&#24182;&#28145;&#20837;&#25551;&#36848;&#20102;&#26368;&#24120;&#29992;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#21442;&#25968;&#36873;&#25321;&#21644;&#21021;&#22987;&#21270;&#30340;&#25351;&#23548;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#27604;&#36739;&#19977;&#20010;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#25928;&#29575;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#31639;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#20248;&#21155;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#26410;&#26631;&#35760;&#30340;&#20803;&#32032;/&#23545;&#35937;&#20998;&#32452;&#65292;&#26088;&#22312;&#26500;&#24314;&#33391;&#22909;&#24314;&#31435;&#30340;&#32858;&#31867;&#65292;&#20854;&#20013;&#20803;&#32032;&#26681;&#25454;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#36807;&#31243;&#30340;&#30446;&#26631;&#26159;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#29992;&#30340;&#24110;&#21161;&#65292;&#24110;&#21161;&#20182;&#20204;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#12290;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#24211;&#26102;&#65292;&#36825;&#20123;&#27169;&#24335;&#21487;&#33021;&#19981;&#23481;&#26131;&#22312;&#27809;&#26377;&#32858;&#31867;&#31639;&#27861;&#30340;&#36129;&#29486;&#19979;&#26816;&#27979;&#21040;&#12290;&#26412;&#25991;&#28145;&#20837;&#25551;&#36848;&#20102;&#26368;&#24120;&#29992;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20851;&#36866;&#24403;&#21442;&#25968;&#36873;&#25321;&#21644;&#21021;&#22987;&#21270;&#30340;&#26377;&#29992;&#20171;&#32461;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#19981;&#20165;&#26159;&#19968;&#31687;&#35780;&#35770;&#65292;&#31361;&#20986;&#20102;&#25152;&#26816;&#26597;&#30340;&#32858;&#31867;&#25216;&#26415;&#30340;&#20027;&#35201;&#35201;&#32032;&#65292;&#36824;&#36890;&#36807;&#23545;3&#20010;&#25968;&#25454;&#38598;&#30340;&#22522;&#20110;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#32858;&#31867;&#25928;&#29575;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#29616;&#26377;&#24369;&#28857;&#21644;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering is an unsupervised machine learning methodology where unlabeled elements/objects are grouped together aiming to the construction of well-established clusters that their elements are classified according to their similarity. The goal of this process is to provide a useful aid to the researcher that will help her/him to identify patterns among the data. Dealing with large databases, such patterns may not be easily detectable without the contribution of a clustering algorithm. This article provides a deep description of the most widely used clustering methodologies accompanied by useful presentations concerning suitable parameter selection and initializations. Simultaneously, this article not only represents a review highlighting the major elements of examined clustering techniques but emphasizes the comparison of these algorithms' clustering efficiency based on 3 datasets, revealing their existing weaknesses and capabilities through accuracy and complexity, during the confront
&lt;/p&gt;</description></item><item><title>PROFHIT&#26159;&#19968;&#20010;&#27010;&#29575;&#40065;&#26834;&#30340;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#25972;&#20010;&#23618;&#27425;&#32467;&#26500;&#30340;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.07940</link><description>&lt;p&gt;
PROFHIT: &#38754;&#21521;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#40065;&#26834;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PROFHIT: Probabilistic Robust Forecasting for Hierarchical Time-series. (arXiv:2206.07940v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07940
&lt;/p&gt;
&lt;p&gt;
PROFHIT&#26159;&#19968;&#20010;&#27010;&#29575;&#40065;&#26834;&#30340;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#25972;&#20010;&#23618;&#27425;&#32467;&#26500;&#30340;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#24615;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#37325;&#35201;&#21464;&#31181;&#65292;&#20854;&#30446;&#26631;&#26159;&#24314;&#27169;&#21644;&#39044;&#27979;&#20855;&#26377;&#20998;&#23618;&#20851;&#31995;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#20851;&#27880;&#28857;&#39044;&#27979;&#65292;&#24182;&#27809;&#26377;&#25552;&#20379;&#33391;&#22909;&#26657;&#20934;&#30340;&#27010;&#29575;&#39044;&#27979;&#20998;&#24067;&#12290;&#26368;&#36817;&#30340;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#22312;&#28857;&#39044;&#27979;&#21644;&#20998;&#24067;&#26679;&#26412;&#19978;&#20063;&#24341;&#20837;&#20102;&#20998;&#23618;&#20851;&#31995;&#65292;&#20294;&#27809;&#26377;&#32771;&#34385;&#39044;&#27979;&#20998;&#24067;&#30340;&#19968;&#33268;&#24615;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#20063;&#40664;&#40664;&#22320;&#20551;&#35774;&#25968;&#25454;&#38598;&#24635;&#26159;&#19982;&#32473;&#23450;&#30340;&#20998;&#23618;&#20851;&#31995;&#19968;&#33268;&#65292;&#24182;&#19988;&#19981;&#36866;&#24212;&#26174;&#31034;&#19982;&#27492;&#20551;&#35774;&#20559;&#31163;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#20004;&#20010;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20102;PROFHIT&#65292;&#36825;&#26159;&#19968;&#20010;&#23436;&#20840;&#27010;&#29575;&#24615;&#30340;&#20998;&#23618;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#24314;&#27169;&#25972;&#20010;&#23618;&#27425;&#32467;&#26500;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;PROFHIT&#37319;&#29992;&#28789;&#27963;&#30340;&#27010;&#29575;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic hierarchical time-series forecasting is an important variant of time-series forecasting, where the goal is to model and forecast multivariate time-series that have underlying hierarchical relations. Most methods focus on point predictions and do not provide well-calibrated probabilistic forecasts distributions. Recent state-of-art probabilistic forecasting methods also impose hierarchical relations on point predictions and samples of distribution which does not account for coherency of forecast distributions. Previous works also silently assume that datasets are always consistent with given hierarchical relations and do not adapt to real-world datasets that show deviation from this assumption. We close both these gaps and propose PROFHIT, which is a fully probabilistic hierarchical forecasting model that jointly models forecast distribution of entire hierarchy. PROFHIT uses a flexible probabilistic Bayesian approach and introduces a novel Distributional Coherency regulari
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20851;&#31867;&#21035;&#30340;6D&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#36807;&#31243;&#20803;&#23398;&#20064;&#26469;&#25429;&#25417;&#23545;&#35937;&#30340;&#32441;&#29702;&#21644;&#20960;&#20309;&#24418;&#29366;&#65292;&#24182;&#20351;&#29992;&#20960;&#20309;&#24863;&#30693;&#35299;&#30721;&#22120;&#32771;&#34385;&#23545;&#35937;&#30340;&#20960;&#20309;&#32422;&#26463;&#36827;&#34892;&#20851;&#38190;&#28857;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2206.07162</link><description>&lt;p&gt;
&#26080;&#20851;&#31867;&#21035;&#30340;&#26377;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#30340;6D&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Category-Agnostic 6D Pose Estimation with Conditional Neural Processes. (arXiv:2206.07162v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20851;&#31867;&#21035;&#30340;6D&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#36807;&#31243;&#20803;&#23398;&#20064;&#26469;&#25429;&#25417;&#23545;&#35937;&#30340;&#32441;&#29702;&#21644;&#20960;&#20309;&#24418;&#29366;&#65292;&#24182;&#20351;&#29992;&#20960;&#20309;&#24863;&#30693;&#35299;&#30721;&#22120;&#32771;&#34385;&#23545;&#35937;&#30340;&#20960;&#20309;&#32422;&#26463;&#36827;&#34892;&#20851;&#38190;&#28857;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26410;&#30693;&#29289;&#20307;&#30340;6D&#23039;&#24577;&#20272;&#35745;&#12290;&#19982;&#8220;&#23454;&#20363;&#32423;&#8221;&#21644;&#8220;&#31867;&#21035;&#32423;&#8221;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20197;&#26080;&#20851;&#31867;&#21035;&#30340;&#26041;&#24335;&#23398;&#20064;&#23545;&#35937;&#34920;&#31034;&#65292;&#20351;&#20854;&#20855;&#26377;&#24378;&#22823;&#30340;&#36328;&#23545;&#35937;&#31867;&#21035;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#31070;&#32463;&#36807;&#31243;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24456;&#23569;&#30340;RGB-D&#22270;&#20687;&#21644;&#30495;&#23454;&#20851;&#38190;&#28857;&#26469;&#35757;&#32451;&#32534;&#30721;&#22120;&#25429;&#25417;&#23545;&#35937;&#30340;&#32441;&#29702;&#21644;&#20960;&#20309;&#24418;&#29366;&#65292;&#24182;&#24471;&#21040;&#19968;&#20010;&#28508;&#22312;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#28508;&#22312;&#34920;&#31034;&#34987;&#21516;&#26102;&#20803;&#35757;&#32451;&#30340;&#35299;&#30721;&#22120;&#29992;&#20110;&#39044;&#27979;&#26032;&#22270;&#20687;&#20013;&#29289;&#20307;&#30340;6D&#23039;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20851;&#38190;&#28857;&#39044;&#27979;&#30340;&#26032;&#39062;&#20960;&#20309;&#24863;&#30693;&#35299;&#30721;&#22120;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26126;&#30830;&#32771;&#34385;&#21040;&#27599;&#20010;&#23545;&#35937;&#30340;&#20960;&#20309;&#32422;&#26463;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#22312;linemod&#25968;&#25454;&#38598;&#21644;&#25105;&#20204;&#30340;&#26032;&#30340;&#23436;&#20840;&#27880;&#37322;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel meta-learning approach for 6D pose estimation on unknown objects. In contrast to ``instance-level" and ``category-level" pose estimation methods, our algorithm learns object representation in a category-agnostic way, which endows it with strong generalization capabilities across object categories. Specifically, we employ a neural process-based meta-learning approach to train an encoder to capture texture and geometry of an object in a latent representation, based on very few RGB-D images and ground-truth keypoints. The latent representation is then used by a simultaneously meta-trained decoder to predict the 6D pose of the object in new images. Furthermore, we propose a novel geometry-aware decoder for the keypoint prediction using a Graph Neural Network (GNN), which explicitly takes geometric constraints specific to each object into consideration. To evaluate our algorithm, extensive experiments are conducted on the \linemod dataset, and on our new fully-annotated s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#23454;&#20363;&#24863;&#30693;&#38408;&#20540;&#30340;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#65288;INSTA-BNN&#65289;&#30340;&#26032;&#39062;&#35774;&#35745;&#65292;&#23427;&#26681;&#25454;&#36755;&#20837;&#30456;&#20851;&#20449;&#24687;&#21160;&#24577;&#35843;&#25972;&#37327;&#21270;&#38408;&#20540;&#65292;&#24182;&#36890;&#36807;&#31934;&#24515;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#26368;&#23567;&#30340;&#35774;&#22791;&#24320;&#38144;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;ImageNet&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;INSTA-BNN&#27604;&#22522;&#32447;&#26041;&#27861;&#24615;&#33021;&#25552;&#21319;&#20102;3.0%&#21644;2.8%&#12290;</title><link>http://arxiv.org/abs/2204.07439</link><description>&lt;p&gt;
INSTA-BNN: &#20855;&#26377;&#23454;&#20363;&#24863;&#30693;&#38408;&#20540;&#30340;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
INSTA-BNN: Binary Neural Network with INSTAnce-aware Threshold. (arXiv:2204.07439v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#23454;&#20363;&#24863;&#30693;&#38408;&#20540;&#30340;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#65288;INSTA-BNN&#65289;&#30340;&#26032;&#39062;&#35774;&#35745;&#65292;&#23427;&#26681;&#25454;&#36755;&#20837;&#30456;&#20851;&#20449;&#24687;&#21160;&#24577;&#35843;&#25972;&#37327;&#21270;&#38408;&#20540;&#65292;&#24182;&#36890;&#36807;&#31934;&#24515;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#26368;&#23567;&#30340;&#35774;&#22791;&#24320;&#38144;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;ImageNet&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;INSTA-BNN&#27604;&#22522;&#32447;&#26041;&#27861;&#24615;&#33021;&#25552;&#21319;&#20102;3.0%&#21644;2.8%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#24050;&#25104;&#20026;&#38477;&#20302;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20869;&#23384;&#21344;&#29992;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#30001;&#20110;&#28608;&#27963;&#21644;&#26435;&#37325;&#21463;&#38480;&#20110;&#20108;&#36827;&#21046;&#20540;&#65292;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#12290;&#20026;&#20102;&#24357;&#34917;&#20934;&#30830;&#24615;&#19979;&#38477;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;BNN&#35774;&#35745;&#65292;&#31216;&#20026;&#20855;&#26377;&#23454;&#20363;&#24863;&#30693;&#38408;&#20540;&#30340;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#65288;INSTA-BNN&#65289;&#65292;&#23427;&#20197;&#36755;&#20837;&#30456;&#20851;&#25110;&#23454;&#20363;&#24863;&#30693;&#30340;&#26041;&#24335;&#21160;&#24577;&#22320;&#25511;&#21046;&#37327;&#21270;&#38408;&#20540;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#39640;&#38454;&#32479;&#35745;&#37327;&#21487;&#20197;&#20316;&#20026;&#20272;&#35745;&#36755;&#20837;&#20998;&#24067;&#29305;&#24449;&#30340;&#20195;&#34920;&#24615;&#25351;&#26631;&#12290;INSTA-BNN&#34987;&#35774;&#35745;&#20026;&#26681;&#25454;&#21508;&#31181;&#20449;&#24687;&#65288;&#21253;&#25324;&#39640;&#38454;&#32479;&#35745;&#37327;&#65289;&#21160;&#24577;&#35843;&#25972;&#38408;&#20540;&#65292;&#20294;&#20063;&#32463;&#36807;&#31934;&#24515;&#20248;&#21270;&#65292;&#20197;&#23454;&#29616;&#22312;&#23454;&#38469;&#35774;&#22791;&#19978;&#30340;&#26368;&#23567;&#24320;&#38144;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;INSTA-BNN&#22312;ImageNet&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;3.0%&#21644;2.8%&#12290;
&lt;/p&gt;
&lt;p&gt;
Binary Neural Networks (BNNs) have emerged as a promising solution for reducing the memory footprint and compute costs of deep neural networks, but they suffer from quality degradation due to the lack of freedom as activations and weights are constrained to the binary values. To compensate for the accuracy drop, we propose a novel BNN design called Binary Neural Network with INSTAnce-aware threshold (INSTA-BNN), which controls the quantization threshold dynamically in an input-dependent or instance-aware manner. According to our observation, higher-order statistics can be a representative metric to estimate the characteristics of the input distribution. INSTA-BNN is designed to adjust the threshold dynamically considering various information, including higher-order statistics, but it is also optimized judiciously to realize minimal overhead on a real device. Our extensive study shows that INSTA-BNN outperforms the baseline by 3.0% and 2.8% on the ImageNet classification task with compa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31034;&#20363;&#30340;&#36229;&#32593;&#32476;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#20010;&#28304;&#39046;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#26469;&#36827;&#34892;&#39046;&#22495;&#22806;&#27867;&#21270;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#29983;&#25104;&#36755;&#20837;&#31034;&#20363;&#30340;&#21807;&#19968;&#31614;&#21517;&#65292;&#24182;&#23558;&#20854;&#23884;&#20837;&#28304;&#39046;&#22495;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;&#24182;&#21033;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#20219;&#21153;&#20998;&#31867;&#22120;&#30340;&#26435;&#37325;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;29&#20010;&#36866;&#24212;&#22330;&#26223;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#19988;&#22312;&#36755;&#20837;&#31034;&#20363;&#30340;&#34920;&#31034;&#19978;&#20855;&#26377;&#20016;&#23500;&#24615;&#12290;&#21516;&#26102;&#65292;&#19982;&#23569;&#26679;&#26412;GPT-3&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.14276</link><description>&lt;p&gt;
&#22522;&#20110;&#31034;&#20363;&#30340;&#36229;&#32593;&#32476;&#29992;&#20110;&#39046;&#22495;&#22806;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Example-based Hypernetworks for Out-of-Distribution Generalization. (arXiv:2203.14276v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.14276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31034;&#20363;&#30340;&#36229;&#32593;&#32476;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#20010;&#28304;&#39046;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#26469;&#36827;&#34892;&#39046;&#22495;&#22806;&#27867;&#21270;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#29983;&#25104;&#36755;&#20837;&#31034;&#20363;&#30340;&#21807;&#19968;&#31614;&#21517;&#65292;&#24182;&#23558;&#20854;&#23884;&#20837;&#28304;&#39046;&#22495;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#65292;&#24182;&#21033;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#20219;&#21153;&#20998;&#31867;&#22120;&#30340;&#26435;&#37325;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;29&#20010;&#36866;&#24212;&#22330;&#26223;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#19988;&#22312;&#36755;&#20837;&#31034;&#20363;&#30340;&#34920;&#31034;&#19978;&#20855;&#26377;&#20016;&#23500;&#24615;&#12290;&#21516;&#26102;&#65292;&#19982;&#23569;&#26679;&#26412;GPT-3&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#31639;&#27861;&#19981;&#26029;&#31361;&#30772;&#26032;&#30340;&#37324;&#31243;&#30865;&#65292;&#39046;&#22495;&#22806;&#27867;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#23545;&#20110;&#38476;&#29983;&#39046;&#22495;&#30340;&#22810;&#28304;&#36866;&#24212;&#38382;&#39064;&#65306;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#28304;&#39046;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#22312;&#35757;&#32451;&#20013;&#27867;&#21270;&#21040;&#26410;&#30693;&#30446;&#26631;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#24615;&#26694;&#26550;&#37319;&#29992;&#22522;&#20110;&#31034;&#20363;&#30340;&#36229;&#32593;&#32476;&#36866;&#24212;&#65306;&#19968;&#20010;T5&#32534;&#30721;-&#35299;&#30721;&#22120;&#39318;&#20808;&#20174;&#36755;&#20837;&#31034;&#20363;&#20013;&#29983;&#25104;&#19968;&#20010;&#21807;&#19968;&#30340;&#31614;&#21517;&#65292;&#24182;&#23558;&#20854;&#23884;&#20837;&#21040;&#28304;&#39046;&#22495;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#12290;&#28982;&#21518;&#65292;&#36825;&#20010;&#31614;&#21517;&#34987;&#19968;&#20010;&#36229;&#32593;&#32476;&#21033;&#29992;&#26469;&#29983;&#25104;&#20219;&#21153;&#20998;&#31867;&#22120;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#22312;29&#31181;&#36866;&#24212;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#28041;&#21450;&#24773;&#24863;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20004;&#20010;&#20219;&#21153;&#65292;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#24050;&#26377;&#31639;&#27861;&#12290;&#22312;&#39640;&#32423;&#29256;&#26412;&#20013;&#65292;&#31614;&#21517;&#36824;&#20016;&#23500;&#20102;&#36755;&#20837;&#31034;&#20363;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#24494;&#35843;&#26550;&#26500;&#19982;&#23569;&#26679;&#26412;GPT-3&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Natural Language Processing (NLP) algorithms continually achieve new milestones, out-of-distribution generalization remains a significant challenge. This paper addresses the issue of multi-source adaptation for unfamiliar domains: We leverage labeled data from multiple source domains to generalize to unknown target domains at training. Our innovative framework employs example-based Hypernetwork adaptation: a T5 encoder-decoder initially generates a unique signature from an input example, embedding it within the source domains' semantic space. This signature is subsequently utilized by a Hypernetwork to generate the task classifier's weights. We evaluated our method across two tasks - sentiment classification and natural language inference - in 29 adaptation scenarios, where it outpaced established algorithms. In an advanced version, the signature also enriches the input example's representation. We also compare our finetuned architecture to few-shot GPT-3, demonstrating its effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20851;&#31995;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;ReSSL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#27169;&#19981;&#21516;&#23454;&#20363;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#65292;&#20197;&#24357;&#34917;&#24403;&#21069;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;&#23454;&#20363;&#20851;&#31995;&#30340;&#32570;&#20047;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2203.08717</link><description>&lt;p&gt;
&#20851;&#31995;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Relational Self-Supervised Learning. (arXiv:2203.08717v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.08717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20851;&#31995;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;ReSSL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#27169;&#19981;&#21516;&#23454;&#20363;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#65292;&#20197;&#24357;&#34917;&#24403;&#21069;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;&#23454;&#20363;&#20851;&#31995;&#30340;&#32570;&#20047;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#65292;&#21253;&#25324;&#20027;&#27969;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#22312;&#27809;&#26377;&#25968;&#25454;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#65292;&#21487;&#20197;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#23454;&#20363;&#32423;&#20449;&#24687;&#65288;&#21363;&#65292;&#30456;&#21516;&#23454;&#20363;&#30340;&#19981;&#21516;&#22686;&#24378;&#22270;&#20687;&#24212;&#20855;&#26377;&#30456;&#21516;&#30340;&#29305;&#24449;&#25110;&#32858;&#38598;&#21040;&#30456;&#21516;&#30340;&#31867;&#21035;&#65289;&#65292;&#20294;&#23545;&#19981;&#21516;&#23454;&#20363;&#20043;&#38388;&#30340;&#20851;&#31995;&#32570;&#20047;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#65292;&#21363;&#20851;&#31995;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;ReSSL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#24314;&#27169;&#19981;&#21516;&#23454;&#20363;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#23398;&#20064;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#19981;&#21516;&#23454;&#20363;&#20043;&#38388;&#30340;&#25104;&#23545;&#30456;&#20284;&#24615;&#30340;&#38160;&#21270;&#20998;&#24067;&#20316;&#20026;&#8220;&#20851;&#31995;&#8221;&#24230;&#37327;&#65292;&#28982;&#21518;&#21033;&#29992;&#35813;&#24230;&#37327;&#21305;&#37197;&#19981;&#21516;&#22686;&#24378;&#30340;&#29305;&#24449;&#23884;&#20837;&#12290;&#20026;&#20102;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#35748;&#20026;&#24369;&#22686;&#24378;&#23545;&#20110;&#34920;&#31034;&#26356;&#21487;&#38752;&#30340;&#20851;&#31995;&#24456;&#37325;&#35201;&#65292;&#24182;&#21033;&#29992;&#21160;&#37327;&#25216;&#26415;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Learning (SSL) including the mainstream contrastive learning has achieved great success in learning visual representations without data annotations. However, most methods mainly focus on the instance level information (\ie, the different augmented images of the same instance should have the same feature or cluster into the same class), but there is a lack of attention on the relationships between different instances. In this paper, we introduce a novel SSL paradigm, which we term as relational self-supervised learning (ReSSL) framework that learns representations by modeling the relationship between different instances. Specifically, our proposed method employs sharpened distribution of pairwise similarities among different instances as \textit{relation} metric, which is thus utilized to match the feature embeddings of different augmentations. To boost the performance, we argue that weak augmentations matter to represent a more reliable relation, and leverage momentum s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#21028;&#21035;&#32593;&#32476;&#36716;&#25442;&#20026;&#29983;&#25104;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#39640;&#26031;&#26680;&#26367;&#25442;&#22810;&#38754;&#20307;&#20013;&#30340;&#20223;&#23556;&#20989;&#25968;&#26469;&#29983;&#25104;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#25968;&#25454;&#26657;&#20934;&#38382;&#39064;&#65292;&#24182;&#22312; CIFAR-10&#65292;CIFAR-100 &#21644; SVHN &#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.13001</link><description>&lt;p&gt;
&#28145;&#24230;&#21028;&#21035;&#21040;&#26680;&#29983;&#25104;&#32593;&#32476;&#30340;&#23450;&#26631;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Discriminative to Kernel Generative Networks for Calibrated Inference. (arXiv:2201.13001v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.13001
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23558;&#21028;&#21035;&#32593;&#32476;&#36716;&#25442;&#20026;&#29983;&#25104;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29992;&#39640;&#26031;&#26680;&#26367;&#25442;&#22810;&#38754;&#20307;&#20013;&#30340;&#20223;&#23556;&#20989;&#25968;&#26469;&#29983;&#25104;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#25968;&#25454;&#26657;&#20934;&#38382;&#39064;&#65292;&#24182;&#22312; CIFAR-10&#65292;CIFAR-100 &#21644; SVHN &#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21028;&#21035;&#19982;&#29983;&#25104;&#32593;&#32476;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#26234;&#33021;&#30340;&#30740;&#31350;&#20013;&#37117;&#26377;&#20854;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20108;&#32773;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#21028;&#21035;&#32593;&#32476;&#36716;&#25442;&#20026;&#26680;&#29983;&#25104;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;&#28145;&#24230;&#27169;&#22411;&#35270;&#20026;&#24191;&#20041;&#30340;&#21010;&#20998;&#35268;&#21017;&#65292;&#24182;&#20351;&#29992;&#39640;&#26031;&#26680;&#26367;&#25442;&#30001;&#35757;&#32451;&#25968;&#25454;&#26500;&#25104;&#30340;&#22810;&#38754;&#20307;&#20013;&#30340;&#20223;&#23556;&#20989;&#25968;&#65292;&#26469;&#33719;&#24471;&#29983;&#25104;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fight between discriminative versus generative goes deep, in both the study of artificial and natural intelligence. In our view, both camps have complementary values. So, we sought to synergistically combine them. Here, we propose a methodology to convert deep discriminative networks to kernel generative networks. We leveraged the fact that deep models, including both random forests and deep networks, learn internal representations which are unions of polytopes with affine activation functions to conceptualize them both as generalized partitioning rules. We replace the affine function in each polytope populated by the training data with Gaussian kernel that results in a generative model. Theoretically, we derive the conditions under which our generative models are a consistent estimator of the corresponding class conditional density. Moreover, our proposed models obtain well calibrated posteriors for in-distribution, and extrapolate beyond the training data to handle out-of-distrib
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32447;&#24615;&#38408;&#20540;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#31867;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#34920;&#31034;&#30340;&#20989;&#25968;&#30340;&#29305;&#28857;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#20004;&#20010;&#38544;&#34255;&#23618;&#21487;&#20197;&#34920;&#31034;&#35813;&#31867;&#20013;&#30340;&#20219;&#20309;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#34920;&#31034;&#36825;&#31867;&#20989;&#25968;&#25152;&#38656;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#30340;&#30028;&#38480;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#33021;&#22815;&#35299;&#20915;&#20855;&#26377;&#22266;&#23450;&#26550;&#26500;&#30340;&#36825;&#31867;&#31070;&#32463;&#32593;&#32476;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#19982;&#25968;&#25454;&#26679;&#26412;&#22823;&#23567;&#21576;&#22810;&#39033;&#24335;&#20851;&#31995;&#65292;&#32780;&#36755;&#20837;&#32500;&#24230;&#21644;&#32593;&#32476;&#26550;&#26500;&#30340;&#22823;&#23567;&#34987;&#35748;&#20026;&#26159;&#22266;&#23450;&#24120;&#25968;&#12290;</title><link>http://arxiv.org/abs/2111.08117</link><description>&lt;p&gt;
&#20855;&#26377;&#32447;&#24615;&#38408;&#20540;&#28608;&#27963;&#30340;&#31070;&#32463;&#32593;&#32476;&#65306;&#32467;&#26500;&#19982;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Neural networks with linear threshold activations: structure and algorithms. (arXiv:2111.08117v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.08117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32447;&#24615;&#38408;&#20540;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#31867;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#34920;&#31034;&#30340;&#20989;&#25968;&#30340;&#29305;&#28857;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#20004;&#20010;&#38544;&#34255;&#23618;&#21487;&#20197;&#34920;&#31034;&#35813;&#31867;&#20013;&#30340;&#20219;&#20309;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#34920;&#31034;&#36825;&#31867;&#20989;&#25968;&#25152;&#38656;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#30340;&#30028;&#38480;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#33021;&#22815;&#35299;&#20915;&#20855;&#26377;&#22266;&#23450;&#26550;&#26500;&#30340;&#36825;&#31867;&#31070;&#32463;&#32593;&#32476;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#19982;&#25968;&#25454;&#26679;&#26412;&#22823;&#23567;&#21576;&#22810;&#39033;&#24335;&#20851;&#31995;&#65292;&#32780;&#36755;&#20837;&#32500;&#24230;&#21644;&#32593;&#32476;&#26550;&#26500;&#30340;&#22823;&#23567;&#34987;&#35748;&#20026;&#26159;&#22266;&#23450;&#24120;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#25105;&#20204;&#20171;&#32461;&#20102;&#20851;&#20110;&#20855;&#26377;&#32447;&#24615;&#38408;&#20540;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#32467;&#26524;&#12290;&#25105;&#20204;&#31934;&#30830;&#22320;&#25551;&#36848;&#20102;&#21487;&#20197;&#30001;&#36825;&#26679;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#20989;&#25968;&#31867;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#29992;2&#20010;&#38544;&#34255;&#23618;&#34920;&#31034;&#35813;&#31867;&#20013;&#30340;&#20219;&#20309;&#21487;&#34920;&#31034;&#20989;&#25968;&#26082;&#26159;&#24517;&#35201;&#30340;&#20063;&#26159;&#20805;&#20998;&#30340;&#12290;&#36825;&#26159;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65292;&#32771;&#34385;&#21040;&#26368;&#36817;&#20351;&#29992;&#20854;&#20182;&#27969;&#34892;&#28608;&#27963;&#20989;&#25968;&#22914;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#31934;&#30830;&#21487;&#34920;&#31034;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#34920;&#31034;&#35813;&#31867;&#20013;&#20219;&#20309;&#20989;&#25968;&#25152;&#38656;&#30340;&#31070;&#32463;&#32593;&#32476;&#22823;&#23567;&#30340;&#31934;&#30830;&#30028;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#35299;&#20915;&#20855;&#26377;&#22266;&#23450;&#26550;&#26500;&#30340;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#38382;&#39064;&#65292;&#20197;&#20840;&#23616;&#26368;&#20248;&#24615;&#12290;&#22914;&#26524;&#23558;&#36755;&#20837;&#32500;&#24230;&#21644;&#32593;&#32476;&#26550;&#26500;&#30340;&#22823;&#23567;&#35270;&#20026;&#22266;&#23450;&#24120;&#25968;&#65292;&#21017;&#35813;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#26159;&#22810;&#39033;&#24335;&#26102;&#38388;&#12290;&#35813;&#31639;&#27861;&#22312;&#20219;&#20309;&#24773;&#20917;&#19979;&#37117;&#21487;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article we present new results on neural networks with linear threshold activation functions. We precisely characterize the class of functions that are representable by such neural networks and show that 2 hidden layers are necessary and sufficient to represent any function representable in the class. This is a surprising result in the light of recent exact representability investigations for neural networks using other popular activation functions like rectified linear units (ReLU). We also give precise bounds on the sizes of the neural networks required to represent any function in the class. Finally, we design an algorithm to solve the empirical risk minimization (ERM) problem to global optimality for these neural networks with a fixed architecture. The algorithm's running time is polynomial in the size of the data sample, if the input dimension and the size of the network architecture are considered fixed constants. The algorithm is unique in the sense that it works for any
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35774;&#22791;&#19978;&#39640;&#25928;&#36890;&#20449;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#37030;&#33976;&#39311;&#21644;&#22686;&#24378;&#35299;&#20915;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#38750;IID&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#30340;&#21516;&#26102;&#65292;&#20173;&#33021;&#36798;&#21040;&#36739;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/1811.11479</link><description>&lt;p&gt;
&#22312;&#35774;&#22791;&#19978;&#39640;&#25928;&#36890;&#20449;&#30340;&#26426;&#22120;&#23398;&#20064;&#65306;&#38750;IID&#31169;&#26377;&#25968;&#25454;&#19979;&#30340;&#32852;&#37030;&#33976;&#39311;&#19982;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data. (arXiv:1811.11479v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1811.11479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35774;&#22791;&#19978;&#39640;&#25928;&#36890;&#20449;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#37030;&#33976;&#39311;&#21644;&#22686;&#24378;&#35299;&#20915;&#20102;&#27169;&#22411;&#22823;&#23567;&#21644;&#38750;IID&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#30340;&#21516;&#26102;&#65292;&#20173;&#33021;&#36798;&#21040;&#36739;&#39640;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#22791;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20351;&#35757;&#32451;&#36807;&#31243;&#33021;&#22815;&#21033;&#29992;&#22823;&#37327;&#29992;&#25143;&#29983;&#25104;&#30340;&#31169;&#26377;&#25968;&#25454;&#26679;&#26412;&#12290;&#20026;&#20102;&#20139;&#21463;&#36825;&#19968;&#20248;&#21183;&#65292;&#24212;&#35813;&#23613;&#37327;&#20943;&#23569;&#35774;&#22791;&#38388;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#33976;&#39311;&#65288;FD&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#27169;&#22411;&#35757;&#32451;&#31639;&#27861;&#65292;&#20854;&#36890;&#20449;&#36127;&#36733;&#22823;&#23567;&#36828;&#23567;&#20110;&#22522;&#20934;&#26041;&#26696;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#65292;&#29305;&#21035;&#26159;&#24403;&#27169;&#22411;&#22823;&#23567;&#24456;&#22823;&#26102;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#29983;&#25104;&#30340;&#25968;&#25454;&#26679;&#26412;&#22312;&#35774;&#22791;&#20043;&#38388;&#24448;&#24448;&#19981;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;IID&#65289;&#65292;&#36825;&#24120;&#24120;&#38477;&#20302;&#24615;&#33021;&#19982;IID&#25968;&#25454;&#38598;&#30456;&#27604;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#22686;&#24378;&#65288;FAug&#65289;&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#27599;&#20010;&#35774;&#22791;&#37117;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#26412;&#22320;&#25968;&#25454;&#20197;&#20135;&#29983;IID&#25968;&#25454;&#38598;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;FD&#19982;FAug&#30456;&#27604;FL&#65292;&#36890;&#20449;&#24320;&#38144;&#20943;&#23569;&#20102;&#32422;26&#20493;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;95-98%&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
On-device machine learning (ML) enables the training process to exploit a massive amount of user-generated private data samples. To enjoy this benefit, inter-device communication overhead should be minimized. With this end, we propose federated distillation (FD), a distributed model training algorithm whose communication payload size is much smaller than a benchmark scheme, federated learning (FL), particularly when the model size is large. Moreover, user-generated data samples are likely to become non-IID across devices, which commonly degrades the performance compared to the case with an IID dataset. To cope with this, we propose federated augmentation (FAug), where each device collectively trains a generative model, and thereby augments its local data towards yielding an IID dataset. Empirical studies demonstrate that FD with FAug yields around 26x less communication overhead while achieving 95-98% test accuracy compared to FL.
&lt;/p&gt;</description></item></channel></rss>