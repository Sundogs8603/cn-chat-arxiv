<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36739;&#23567;&#30340;&#29983;&#25104;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#26469;&#26500;&#24314;&#22823;&#22411;&#29983;&#25104;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#25968;&#25454;&#37096;&#20998;&#30340;&#27867;&#21270;&#65292;&#24182;&#33021;&#22815;&#32534;&#20889;&#21644;&#26500;&#24314;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01103</link><description>&lt;p&gt;
&#32452;&#21512;&#29983;&#25104;&#24314;&#27169;&#65306;&#21333;&#19968;&#27169;&#22411;&#24182;&#19981;&#26159;&#24744;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;
&lt;/p&gt;
&lt;p&gt;
Compositional Generative Modeling: A Single Model is Not All You Need
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36739;&#23567;&#30340;&#29983;&#25104;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#26469;&#26500;&#24314;&#22823;&#22411;&#29983;&#25104;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#25968;&#25454;&#37096;&#20998;&#30340;&#27867;&#21270;&#65292;&#24182;&#33021;&#22815;&#32534;&#20889;&#21644;&#26500;&#24314;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#35757;&#32451;&#22823;&#35268;&#27169;&#30340;&#24040;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#36234;&#26469;&#36234;&#20027;&#27969;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#24212;&#35813;&#36890;&#36807;&#23558;&#36739;&#23567;&#30340;&#29983;&#25104;&#27169;&#22411;&#32452;&#21512;&#22312;&#19968;&#36215;&#26469;&#26500;&#24314;&#22823;&#22411;&#29983;&#25104;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#32452;&#21512;&#29983;&#25104;&#26041;&#27861;&#22914;&#20309;&#20197;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#23398;&#20064;&#20998;&#24067;&#65292;&#20351;&#24471;&#25105;&#20204;&#22312;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#25968;&#25454;&#20998;&#24067;&#37096;&#20998;&#20063;&#33021;&#36827;&#34892;&#27867;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#35757;&#32451;&#26102;&#23436;&#20840;&#26410;&#35265;&#30340;&#20219;&#21153;&#32534;&#20889;&#21644;&#26500;&#24314;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#29420;&#31435;&#30340;&#32452;&#21512;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large monolithic generative models trained on massive amounts of data have become an increasingly dominant approach in AI research. In this paper, we argue that we should instead construct large generative systems by composing smaller generative models together. We show how such a compositional generative approach enables us to learn distributions in a more data-efficient manner, enabling generalization to parts of the data distribution unseen at training time. We further show how this enables us to program and construct new generative models for tasks completely unseen at training. Finally, we show that in many cases, we can discover separate compositional components from data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#38750;&#20809;&#28369;&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#65292;&#36866;&#24212;&#24615;&#30340;&#20195;&#20215;&#26159;&#26080;&#27861;&#36991;&#20813;&#30340;&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#20851;&#20110;&#19981;&#30830;&#23450;&#24615;&#21442;&#25968;&#30340;&#27425;&#20248;&#24615;&#20056;&#27861;&#22686;&#21152;&#30340;&#19979;&#30028;&#12290;</title><link>https://arxiv.org/abs/2402.10898</link><description>&lt;p&gt;
&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#36866;&#24212;&#24615;&#30340;&#20195;&#20215;
&lt;/p&gt;
&lt;p&gt;
The Price of Adaptivity in Stochastic Convex Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10898
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#38750;&#20809;&#28369;&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#65292;&#36866;&#24212;&#24615;&#30340;&#20195;&#20215;&#26159;&#26080;&#27861;&#36991;&#20813;&#30340;&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#20851;&#20110;&#19981;&#30830;&#23450;&#24615;&#21442;&#25968;&#30340;&#27425;&#20248;&#24615;&#20056;&#27861;&#22686;&#21152;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#38750;&#20809;&#28369;&#38543;&#26426;&#20984;&#20248;&#21270;&#20013;&#36866;&#24212;&#24615;&#30340;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#12290;&#32473;&#23450;&#19968;&#32452;&#25105;&#20204;&#24076;&#26395;&#36866;&#24212;&#30340;&#38382;&#39064;&#21442;&#25968;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#8220;&#36866;&#24212;&#24615;&#30340;&#20195;&#20215;&#8221;&#65288;PoA&#65289;&#65292;&#31895;&#30053;&#22320;&#35828;&#65292;&#23427;&#34913;&#37327;&#20102;&#30001;&#20110;&#36825;&#20123;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#32780;&#23548;&#33268;&#30340;&#27425;&#20248;&#24615;&#30340;&#20056;&#27861;&#22686;&#21152;&#12290;&#24403;&#21021;&#22987;&#36317;&#31163;&#26368;&#20248;&#35299;&#26410;&#30693;&#20294;&#26799;&#24230;&#33539;&#25968;&#26377;&#30028;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;PoA&#33267;&#23569;&#23545;&#20110;&#26399;&#26395;&#27425;&#20248;&#24615;&#26159;&#23545;&#25968;&#32423;&#21035;&#65292;&#23545;&#20110;&#20013;&#20301;&#25968;&#27425;&#20248;&#24615;&#26159;&#21452;&#23545;&#25968;&#32423;&#21035;&#12290;&#24403;&#36317;&#31163;&#21644;&#26799;&#24230;&#33539;&#25968;&#37117;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#25105;&#20204;&#34920;&#26126;PoA&#24517;&#39035;&#26159;&#19982;&#19981;&#30830;&#23450;&#24615;&#27700;&#24179;&#22810;&#39033;&#24335;&#30456;&#20851;&#30340;&#12290;&#25105;&#20204;&#30340;&#19979;&#30028;&#20960;&#20046;&#19982;&#29616;&#26377;&#30340;&#19978;&#30028;&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#30830;&#23450;&#20102;&#27809;&#26377;&#26080;&#21442;&#25968;&#21320;&#39184;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10898v1 Announce Type: cross  Abstract: We prove impossibility results for adaptivity in non-smooth stochastic convex optimization. Given a set of problem parameters we wish to adapt to, we define a "price of adaptivity" (PoA) that, roughly speaking, measures the multiplicative increase in suboptimality due to uncertainty in these parameters. When the initial distance to the optimum is unknown but a gradient norm bound is known, we show that the PoA is at least logarithmic for expected suboptimality, and double-logarithmic for median suboptimality. When there is uncertainty in both distance and gradient norm, we show that the PoA must be polynomial in the level of uncertainty. Our lower bounds nearly match existing upper bounds, and establish that there is no parameter-free lunch.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#34701;&#21512;&#23398;&#20064;&#32593;&#32476;&#65292;&#21033;&#29992;&#25193;&#25955;&#21152;&#26435;MRI&#27169;&#24335;&#21644;&#32467;&#26500;&#21270;&#20581;&#24247;&#27010;&#20917;&#25968;&#25454;&#32852;&#21512;&#39044;&#27979;&#24613;&#24615;&#20013;&#39118;&#21518;&#30340;&#21151;&#33021;&#32467;&#26524;&#65292;&#36890;&#36807;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#21306;&#20998;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10894</link><description>&lt;p&gt;
&#23558;&#25193;&#25955;&#21152;&#26435;MRI&#21644;&#20020;&#24202;&#25968;&#25454;&#34701;&#21512;&#29992;&#20110;&#39044;&#27979;&#24613;&#24615;&#32570;&#34880;&#24615;&#20013;&#39118;&#21518;&#30340;&#21151;&#33021;&#32467;&#26524;&#65306;&#22522;&#20110;&#28145;&#24230;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fusion of Diffusion Weighted MRI and Clinical Data for Predicting Functional Outcome after Acute Ischemic Stroke with Deep Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10894
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#34701;&#21512;&#23398;&#20064;&#32593;&#32476;&#65292;&#21033;&#29992;&#25193;&#25955;&#21152;&#26435;MRI&#27169;&#24335;&#21644;&#32467;&#26500;&#21270;&#20581;&#24247;&#27010;&#20917;&#25968;&#25454;&#32852;&#21512;&#39044;&#27979;&#24613;&#24615;&#20013;&#39118;&#21518;&#30340;&#21151;&#33021;&#32467;&#26524;&#65292;&#36890;&#36807;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#21306;&#20998;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#39118;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#33268;&#27531;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#65292;&#24433;&#21709;25&#23681;&#20197;&#19978;&#25104;&#24180;&#20154;&#21475;&#20013;&#32422;&#22235;&#20998;&#20043;&#19968;&#65307;&#36229;&#36807;&#19968;&#21322;&#30340;&#24739;&#32773;&#22312;&#24613;&#24615;&#20013;&#39118;&#21457;&#20316;&#21518;&#20173;&#28982;&#26377;&#19981;&#33391;&#32467;&#23616;&#65292;&#22914;&#27704;&#20037;&#24615;&#21151;&#33021;&#20381;&#36182;&#29978;&#33267;&#27515;&#20129;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#25193;&#25955;&#21152;&#26435;MRI&#27169;&#24335;&#19982;&#32467;&#26500;&#21270;&#20581;&#24247;&#27010;&#20917;&#30456;&#32467;&#21512;&#23545;&#39044;&#27979;&#21151;&#33021;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#20419;&#36827;&#26089;&#26399;&#24178;&#39044;&#12290; &#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#34701;&#21512;&#23398;&#20064;&#32593;&#32476;&#65292;&#21253;&#25324;&#20004;&#38454;&#27573;&#35757;&#32451;&#65306;&#31532;&#19968;&#38454;&#27573;&#20391;&#37325;&#20110;&#36328;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#65292;&#31532;&#20108;&#38454;&#27573;&#20391;&#37325;&#20110;&#20998;&#31867;&#12290;&#21033;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#21306;&#20998;&#29305;&#24615;&#65292;&#23558;&#20004;&#31867;&#24739;&#32773;&#20174;&#21333;&#20010;&#27169;&#24577;&#30340;&#23884;&#20837;&#21644;&#34701;&#21512;&#30340;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#20998;&#24320;&#12290;&#32593;&#32476;&#20197;DWI&#21644;ADC&#22270;&#20687;&#20197;&#21450;&#32467;&#26500;&#21270;&#20581;&#24247;&#27010;&#20917;&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#12290;&#36755;&#20986;&#26159;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10894v1 Announce Type: cross  Abstract: Stroke is a common disabling neurological condition that affects about one-quarter of the adult population over age 25; more than half of patients still have poor outcomes, such as permanent functional dependence or even death, after the onset of acute stroke. The aim of this study is to investigate the efficacy of diffusion-weighted MRI modalities combining with structured health profile on predicting the functional outcome to facilitate early intervention. A deep fusion learning network is proposed with two-stage training: the first stage focuses on cross-modality representation learning and the second stage on classification. Supervised contrastive learning is exploited to learn discriminative features that separate the two classes of patients from embeddings of individual modalities and from the fused multimodal embedding. The network takes as the input DWI and ADC images, and structured health profile data. The outcome is the pred
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21033;&#29992;&#21475;&#22836;&#21453;&#39304;&#36827;&#34892;&#23450;&#21046;&#21270;&#35843;&#25972;&#32780;&#19981;&#21457;&#29983;&#36807;&#24230;&#27867;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;C3PO&#12290;</title><link>https://arxiv.org/abs/2402.10893</link><description>&lt;p&gt;
RLVF: &#23398;&#20064;&#22914;&#20309;&#22312;&#27809;&#26377;&#27867;&#21270;&#30340;&#24773;&#20917;&#19979;&#20174;&#21475;&#22836;&#21453;&#39304;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RLVF: Learning from Verbal Feedback without Overgeneralization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10893
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21033;&#29992;&#21475;&#22836;&#21453;&#39304;&#36827;&#34892;&#23450;&#21046;&#21270;&#35843;&#25972;&#32780;&#19981;&#21457;&#29983;&#36807;&#24230;&#27867;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;C3PO&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37096;&#32626;&#30340;&#19981;&#21516;&#24773;&#22659;&#30340;&#22810;&#26679;&#24615;&#35201;&#27714;&#33021;&#22815;&#20462;&#25913;&#25110;&#23450;&#21046;&#40664;&#35748;&#27169;&#22411;&#34892;&#20026;&#65292;&#20197;&#28385;&#36275;&#32454;&#24494;&#30340;&#35201;&#27714;&#21644;&#20559;&#22909;&#12290;&#35268;&#23450;&#36825;&#31181;&#27169;&#22411;&#35843;&#25972;&#30340;&#26041;&#20415;&#30028;&#38754;&#26159;&#39640;&#23618;&#27425;&#21475;&#22836;&#21453;&#39304;&#65292;&#27604;&#22914;"&#22312;&#32473;&#32769;&#26495;&#36215;&#33609;&#37038;&#20214;&#26102;&#19981;&#35201;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;"&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#25776;&#20889;&#39640;&#23618;&#21453;&#39304;&#27604;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#25910;&#38598;&#24378;&#21270;&#23398;&#20064;&#27880;&#37322;&#65288;RLHF&#65289;&#31616;&#21333;&#24471;&#22810;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#21482;&#26159;&#29992;&#36825;&#31181;&#21453;&#39304;&#25552;&#31034;&#27169;&#22411;&#20250;&#23548;&#33268;&#21453;&#39304;&#22312;&#19981;&#30456;&#20851;&#30340;&#24773;&#22659;&#20013;&#20135;&#29983;&#27867;&#21270;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#27809;&#26377;&#36825;&#31181;&#27867;&#21270;&#30340;&#24773;&#20917;&#19979;&#25972;&#21512;&#21475;&#22836;&#21453;&#39304;&#30340;&#38382;&#39064;&#65292;&#24182;&#21551;&#21457;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65306;&#24102;&#32422;&#26463;&#20559;&#22909;&#20248;&#21270;&#30340;&#24773;&#22659;&#21270;&#35780;&#35770;&#65288;C3PO&#65289;&#12290;C3PO&#20351;&#29992;&#19968;&#27573;&#39640;&#23618;&#27425;&#21453;&#39304;&#29983;&#25104;&#19968;&#20010;&#23567;&#30340;&#21512;&#25104;&#20559;&#22909;&#25968;&#25454;&#38598;&#65292;&#25351;&#23450;&#20102;&#21453;&#39304;&#24212;&#35813;&#22914;&#20309;&#65288;&#20197;&#21450;&#19981;&#24212;&#35813;&#22914;&#20309;&#65289;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10893v1 Announce Type: cross  Abstract: The diversity of contexts in which large language models (LLMs) are deployed requires the ability to modify or customize default model behaviors to incorporate nuanced requirements and preferences. A convenient interface to specify such model adjustments is high-level verbal feedback, such as "Don't use emojis when drafting emails to my boss." However, while writing high-level feedback is far simpler than collecting annotations for reinforcement learning from human feedback (RLHF), we find that simply prompting a model with such feedback leads to overgeneralization of the feedback to contexts where it is not relevant. We study the problem of incorporating verbal feedback without such overgeneralization, inspiring a new method Contextualized Critiques with Constrained Preference Optimization (C3PO). C3PO uses a piece of high-level feedback to generate a small synthetic preference dataset specifying how the feedback should (and should no
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25968;&#25454;&#27700;&#21360;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#26816;&#27979;&#29256;&#26435;&#25345;&#26377;&#20154;&#20316;&#21697;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36827;&#34892;&#21512;&#29702;&#26816;&#27979;&#19988;&#25552;&#20379;&#35823;&#26816;&#29575;&#20445;&#35777;&#65292;&#30740;&#31350;&#20102;&#27700;&#21360;&#35774;&#35745;&#23545;&#20551;&#35774;&#26816;&#39564;&#33021;&#21147;&#30340;&#24433;&#21709;&#20197;&#21450;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#32553;&#25918;&#19979;&#30340;&#26816;&#27979;&#24378;&#24230;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10892</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#27700;&#21360;&#35777;&#26126;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#21592;&#36164;&#26684;
&lt;/p&gt;
&lt;p&gt;
Proving membership in LLM pretraining data via data watermarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10892
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#27700;&#21360;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#26816;&#27979;&#29256;&#26435;&#25345;&#26377;&#20154;&#20316;&#21697;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36827;&#34892;&#21512;&#29702;&#26816;&#27979;&#19988;&#25552;&#20379;&#35823;&#26816;&#29575;&#20445;&#35777;&#65292;&#30740;&#31350;&#20102;&#27700;&#21360;&#35774;&#35745;&#23545;&#20551;&#35774;&#26816;&#39564;&#33021;&#21147;&#30340;&#24433;&#21709;&#20197;&#21450;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#32553;&#25918;&#19979;&#30340;&#26816;&#27979;&#24378;&#24230;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#29256;&#26435;&#25345;&#26377;&#20154;&#30340;&#20316;&#21697;&#26159;&#21542;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#20351;&#29992;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#25968;&#25454;&#27700;&#21360;&#23454;&#29616;&#22522;&#20110;&#40657;&#30418;&#27169;&#22411;&#35775;&#38382;&#30340;&#21512;&#29702;&#26816;&#27979;&#65292;&#21069;&#25552;&#26159;&#29256;&#26435;&#25345;&#26377;&#20154;&#22312;&#20844;&#24320;&#21457;&#24067;&#20043;&#21069;&#36129;&#29486;&#20102;&#22810;&#20010;&#35757;&#32451;&#25991;&#26723;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#27700;&#21360;&#22788;&#29702;&#12290;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#37319;&#26679;&#30340;&#25968;&#25454;&#27700;&#21360;&#65292;&#26816;&#27979;&#21487;&#20197;&#34987;&#26500;&#36896;&#20026;&#20551;&#35774;&#26816;&#39564;&#65292;&#20174;&#32780;&#25552;&#20379;&#23545;&#35823;&#26816;&#29575;&#30340;&#20445;&#35777;&#12290;&#30740;&#31350;&#20102;&#20004;&#31181;&#27700;&#21360;&#65306;&#19968;&#31181;&#25554;&#20837;&#38543;&#26426;&#24207;&#21015;&#65292;&#21478;&#19968;&#31181;&#38543;&#26426;&#29992;Unicode&#31867;&#20284;&#23383;&#31526;&#26367;&#25442;&#23383;&#31526;&#12290;&#39318;&#20808;&#23637;&#31034;&#20102;&#27700;&#21360;&#35774;&#35745;&#30340;&#19977;&#20010;&#26041;&#38754;--&#27700;&#21360;&#38271;&#24230;&#12289;&#22797;&#21046;&#27425;&#25968;&#21644;&#24178;&#25200;--&#22914;&#20309;&#24433;&#21709;&#20551;&#35774;&#26816;&#39564;&#30340;&#33021;&#21147;&#12290;&#25509;&#30528;&#30740;&#31350;&#20102;&#27700;&#21360;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#32553;&#25918;&#19979;&#30340;&#26816;&#27979;&#24378;&#24230;&#22914;&#20309;&#21464;&#21270;&#65306;&#22686;&#21152;&#25968;&#25454;&#38598;&#22823;&#23567;&#20250;&#38477;&#20302;&#27700;&#21360;&#30340;&#24378;&#24230;&#65292;&#27700;&#21360;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10892v1 Announce Type: cross  Abstract: Detecting whether copyright holders' works were used in LLM pretraining is poised to be an important problem. This work proposes using data watermarks to enable principled detection with only black-box model access, provided that the rightholder contributed multiple training documents and watermarked them before public release. By applying a randomly sampled data watermark, detection can be framed as hypothesis testing, which provides guarantees on the false detection rate. We study two watermarks: one that inserts random sequences, and another that randomly substitutes characters with Unicode lookalikes. We first show how three aspects of watermark design -- watermark length, number of duplications, and interference -- affect the power of the hypothesis test. Next, we study how a watermark's detection strength changes under model and dataset scaling: while increasing the dataset size decreases the strength of the watermark, watermarks
&lt;/p&gt;</description></item><item><title>&#25351;&#23548;&#35843;&#25972;&#36890;&#36807;&#22686;&#21152;&#25351;&#20196;&#38598;&#30340;&#22810;&#26679;&#24615;&#26469;&#25512;&#21160;&#27169;&#22411;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10891</link><description>&lt;p&gt;
&#25351;&#23548;&#22810;&#26679;&#24615;&#25512;&#21160;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Instruction Diversity Drives Generalization To Unseen Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10891
&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#25972;&#36890;&#36807;&#22686;&#21152;&#25351;&#20196;&#38598;&#30340;&#22810;&#26679;&#24615;&#26469;&#25512;&#21160;&#27169;&#22411;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#25972;&#8212;&#8212;&#22312;&#25351;&#20196;&#21644;&#26399;&#26395;&#32467;&#26524;&#20043;&#38388;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#8212;&#8212;&#26159;&#19968;&#31181;&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#24182;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#30340;&#26041;&#27861;&#12290;&#20854;&#23454;&#38469;&#25104;&#21151;&#21462;&#20915;&#20110;&#27169;&#22411;&#23398;&#20064;&#27604;&#20854;&#35757;&#32451;&#26102;&#26356;&#24191;&#27867;&#30340;&#25351;&#20196;&#38598;&#12290;&#28982;&#32780;&#65292;&#20915;&#23450;&#27169;&#22411;&#23545;&#36825;&#31181;&#8220;&#26410;&#35265;&#20219;&#21153;&#8221;&#30340;&#27867;&#21270;&#30340;&#22240;&#32032;&#23578;&#19981;&#21313;&#20998;&#28165;&#26970;&#12290;&#20026;&#20102;&#20102;&#35299;&#27867;&#21270;&#30340;&#39537;&#21160;&#22240;&#32032;&#65292;&#26412;&#25991;&#36890;&#36807;&#23383;&#31526;&#20018;&#37325;&#20889;&#36827;&#34892;&#23454;&#39564;&#65292;&#36825;&#26159;&#19968;&#20010;&#31526;&#21495;&#20219;&#21153;&#65292;&#26159;&#22270;&#28789;&#23436;&#25972;&#39532;&#23572;&#21487;&#22827;&#31639;&#27861;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#21516;&#26102;&#20801;&#35768;&#23454;&#39564;&#23545;&#8220;&#36755;&#20837;&#8221;&#21644;&#8220;&#25351;&#20196;&#8221;&#36827;&#34892;&#25511;&#21046;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#27169;&#22411;&#25509;&#21463;&#30340;&#25351;&#20196;&#25968;&#37327;&#21644;&#20026;&#27599;&#20010;&#25351;&#20196;&#25552;&#20379;&#30340;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#35266;&#23519;&#21040;&#25351;&#20196;&#38598;&#30340;&#22810;&#26679;&#24615;&#30830;&#23450;&#20102;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10891v1 Announce Type: cross  Abstract: Instruction tuning -- fine-tuning a large language model (LLM) on pairs of instructions and desired outcomes -- is an approach that enables pre-trained language models to perform real-world tasks and follow human instructions. Its practical success depends on the model learning a broader set of instructions than those it was trained on. Yet the factors that determine model generalization to such \emph{unseen tasks} are not well understood. %To understand the driving factors of generalization, In this paper, we experiment with string rewrites, a symbolic task that serves as a building block for Turing complete Markov algorithms while allowing experimental control of "inputs" and "instructions". We investigate the trade-off between the number of instructions the model is trained on and the number of training samples provided for each instruction and observe that the diversity of the instruction set determines generalization. Generalizati
&lt;/p&gt;</description></item><item><title>&#24403;&#21069;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#38382;&#39064;&#27714;&#35299;&#20013;&#20351;&#29992;&#26641;&#25628;&#32034;&#30340;&#21487;&#34892;&#24615;&#65292;&#25351;&#20986;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#38656;&#35201;&#37492;&#21035;&#22120;&#33267;&#23569;90%&#20934;&#30830;&#24615;&#25165;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10890</link><description>&lt;p&gt;
LLM&#35268;&#21010;&#20013;&#26641;&#25628;&#32034;&#20309;&#26102;&#26377;&#29992;&#65311;&#21462;&#20915;&#20110;&#37492;&#21035;&#22120;
&lt;/p&gt;
&lt;p&gt;
When is Tree Search Useful for LLM Planning? It Depends on the Discriminator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10890
&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#38382;&#39064;&#27714;&#35299;&#20013;&#20351;&#29992;&#26641;&#25628;&#32034;&#30340;&#21487;&#34892;&#24615;&#65292;&#25351;&#20986;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#38656;&#35201;&#37492;&#21035;&#22120;&#33267;&#23569;90%&#20934;&#30830;&#24615;&#25165;&#33021;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#35821;&#35328;&#20195;&#29702;&#26694;&#26550;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#22312;&#22810;&#27493;&#38382;&#39064;&#19979;&#35299;&#20915;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#29983;&#25104;&#22120;&#12289;&#37492;&#21035;&#22120;&#21644;&#35268;&#21010;&#26041;&#27861;&#19977;&#20010;&#37096;&#20998;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#20808;&#36827;&#35268;&#21010;&#26041;&#27861;&#65292;&#36845;&#20195;&#26657;&#27491;&#21644;&#26641;&#25628;&#32034;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;&#25105;&#20204;&#20840;&#38754;&#20998;&#26512;&#20102;&#37492;&#21035;&#20934;&#30830;&#24615;&#22914;&#20309;&#24433;&#21709;&#20195;&#29702;&#22312;&#20351;&#29992;&#36825;&#20004;&#31181;&#26041;&#27861;&#25110;&#26356;&#31616;&#21333;&#30340;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#26102;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#20004;&#39033;&#20219;&#21153;&#65292;&#25991;&#26412;&#21040;SQL&#35299;&#26512;&#21644;&#25968;&#23398;&#25512;&#29702;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65306;&#65288;1&#65289;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#38656;&#35201;&#33267;&#23569;90%&#20934;&#30830;&#24615;&#30340;&#37492;&#21035;&#22120;&#25165;&#33021;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#65307;&#65288;2&#65289;&#24403;&#21069;LLMs&#30340;&#37492;&#21035;&#33021;&#21147;&#23578;&#26410;&#28385;&#36275;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#23454;&#29616;&#36825;&#31181;&#25913;&#36827;&#30340;&#38656;&#27714;&#65307;&#65288;3&#65289;&#37319;&#29992;&#22522;&#20110;LLM&#30340;&#37492;&#21035;&#22120;&#26102;&#65292;&#39640;&#32423;&#35268;&#21010;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10890v1 Announce Type: cross  Abstract: In this paper, we examine how large language models (LLMs) solve multi-step problems under a language agent framework with three components: a generator, a discriminator, and a planning method. We investigate the practical utility of two advanced planning methods, iterative correction and tree search. We present a comprehensive analysis of how discrimination accuracy affects the overall performance of agents when using these two methods or a simpler method, re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical reasoning, show that: (1) advanced planning methods demand discriminators with at least 90% accuracy to achieve significant improvements over re-ranking; (2) current LLMs' discrimination abilities have not met the needs of advanced planning methods to achieve such improvements; (3) with LLM-based discriminators, advanced planning methods may not adequately balance accuracy and efficiency. For example, compare
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#20026;&#24050;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#23616;&#37096;&#35299;&#37322;&#24182;&#30830;&#20445;&#36825;&#20123;&#35299;&#37322;&#23545;&#29992;&#25143;&#20855;&#26377;&#21487;&#29702;&#35299;&#24615;&#65292;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#24320;&#21457;&#20855;&#26377;&#25968;&#25454;&#36866;&#24212;&#24615;&#21644;&#29992;&#25143;&#24863;&#30693;&#35201;&#27714;&#31449;&#28857;&#35299;&#37322;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10888</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65306;&#20174;&#25968;&#25454;&#36866;&#24212;&#24615;&#21040;&#29992;&#25143;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Explainability for Machine Learning Models: From Data Adaptability to User Perception
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20026;&#24050;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#23616;&#37096;&#35299;&#37322;&#24182;&#30830;&#20445;&#36825;&#20123;&#35299;&#37322;&#23545;&#29992;&#25143;&#20855;&#26377;&#21487;&#29702;&#35299;&#24615;&#65292;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#24320;&#21457;&#20855;&#26377;&#25968;&#25454;&#36866;&#24212;&#24615;&#21644;&#29992;&#25143;&#24863;&#30693;&#35201;&#27714;&#31449;&#28857;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#20026;&#24050;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#23616;&#37096;&#35299;&#37322;&#65292;&#26088;&#22312;&#30830;&#23450;&#20135;&#29983;&#26377;&#24847;&#20041;&#35299;&#37322;&#30340;&#26368;&#20339;&#26465;&#20214;&#65292;&#32771;&#34385;&#21040;&#25968;&#25454;&#21644;&#29992;&#25143;&#38656;&#27714;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#24320;&#21457;&#26041;&#27861;&#65292;&#29983;&#25104;&#20219;&#20309;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#21516;&#26102;&#30830;&#20445;&#36825;&#20123;&#35299;&#37322;&#20445;&#25345;&#24544;&#23454;&#20110;&#22522;&#30784;&#27169;&#22411;&#24182;&#23545;&#29992;&#25143;&#20855;&#26377;&#21487;&#29702;&#35299;&#24615;&#12290;&#35770;&#25991;&#20998;&#20026;&#20004;&#37096;&#20998;&#12290;&#31532;&#19968;&#37096;&#20998;&#22686;&#24378;&#20102;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#28982;&#21518;&#24341;&#20837;&#20102;&#19968;&#20010;&#35780;&#20272;&#32447;&#24615;&#35299;&#37322;&#36924;&#36817;&#27169;&#22411;&#36866;&#23452;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#23545;&#27604;&#20102;&#20004;&#31181;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#26063;&#20197;&#20998;&#26512;&#20854;&#20013;&#19968;&#31181;&#30456;&#23545;&#21478;&#19968;&#31181;&#30340;&#20248;&#21183;&#12290;&#31532;&#20108;&#37096;&#20998;&#20391;&#37325;&#20110;&#29992;&#25143;&#23454;&#39564;&#65292;&#35780;&#20272;&#19977;&#31181;&#35299;&#37322;&#26041;&#27861;&#21644;&#20004;&#31181;&#19981;&#21516;&#34920;&#31034;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#23454;&#39564;&#27979;&#37327;&#20102;&#29992;&#25143;&#29702;&#35299;&#35299;&#37322;&#30340;&#36895;&#24230;&#65292;&#21487;&#20449;&#24230;&#21644;&#20851;&#27880;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10888v1 Announce Type: new  Abstract: This thesis explores the generation of local explanations for already deployed machine learning models, aiming to identify optimal conditions for producing meaningful explanations considering both data and user requirements. The primary goal is to develop methods for generating explanations for any model while ensuring that these explanations remain faithful to the underlying model and comprehensible to the users.   The thesis is divided into two parts. The first enhances a widely used rule-based explanation method. It then introduces a novel approach for evaluating the suitability of linear explanations to approximate a model. Additionally, it conducts a comparative experiment between two families of counterfactual explanation methods to analyze the advantages of one over the other. The second part focuses on user experiments to assess the impact of three explanation methods and two distinct representations. These experiments measure ho
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31574;&#30053;&#25193;&#25955;&#21644;3D&#22330;&#26223;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;3D Diffuser Actor&#65292;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#26550;&#26500;&#65292;&#21487;&#20197;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#26500;&#24314;3D&#35270;&#35273;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#23545;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;3D&#26059;&#36716;&#21644;&#24179;&#31227;&#36827;&#34892;&#36845;&#20195;&#21435;&#22122;&#12290;</title><link>https://arxiv.org/abs/2402.10885</link><description>&lt;p&gt;
&#22522;&#20110;3D&#22330;&#26223;&#34920;&#31034;&#30340;3D&#25193;&#25955;&#22120;Actor&#65306;&#36890;&#36807;&#31574;&#30053;&#25193;&#25955;&#36827;&#34892;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
3D Diffuser Actor: Policy Diffusion with 3D Scene Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10885
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31574;&#30053;&#25193;&#25955;&#21644;3D&#22330;&#26223;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;3D Diffuser Actor&#65292;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#26550;&#26500;&#65292;&#21487;&#20197;&#26681;&#25454;&#35821;&#35328;&#25351;&#20196;&#26500;&#24314;3D&#35270;&#35273;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#23545;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;3D&#26059;&#36716;&#21644;&#24179;&#31227;&#36827;&#34892;&#36845;&#20195;&#21435;&#22122;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#25193;&#25955;&#31574;&#30053;&#21644;3D&#22330;&#26223;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#25193;&#25955;&#31574;&#30053;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#22522;&#20110;&#26426;&#22120;&#20154;&#21644;&#29615;&#22659;&#29366;&#24577;&#30340;&#21160;&#20316;&#20998;&#24067;&#12290;&#26368;&#36817;&#65292;&#23427;&#20204;&#24050;&#32463;&#34920;&#29616;&#20986;&#20248;&#20110;&#30830;&#23450;&#24615;&#21644;&#20854;&#20182;&#22522;&#20110;&#29366;&#24577;&#30340;&#21160;&#20316;&#20998;&#24067;&#23398;&#20064;&#26041;&#27861;&#12290;3D&#26426;&#22120;&#20154;&#31574;&#30053;&#20351;&#29992;&#20174;&#21333;&#20010;&#25110;&#22810;&#20010;&#25668;&#20687;&#22836;&#35270;&#35282;&#33719;&#21462;&#30340;&#24863;&#24212;&#28145;&#24230;&#32858;&#21512;&#30340;3D&#22330;&#26223;&#29305;&#24449;&#34920;&#31034;&#12290;&#23427;&#20204;&#24050;&#32463;&#35777;&#26126;&#27604;&#20854;2D&#23545;&#24212;&#29289;&#22312;&#25668;&#20687;&#26426;&#35270;&#35282;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#32479;&#19968;&#20102;&#36825;&#20004;&#26465;&#32447;&#36335;&#30340;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;3D&#25193;&#25955;&#22120;Actor&#65292;&#36825;&#26159;&#19968;&#20010;&#31070;&#32463;&#31574;&#30053;&#26550;&#26500;&#65292;&#23427;&#22312;&#32473;&#23450;&#35821;&#35328;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;&#26500;&#24314;&#35270;&#35273;&#22330;&#26223;&#30340;3D&#34920;&#31034;&#65292;&#24182;&#22312;&#20854;&#19978;&#36827;&#34892;&#26465;&#20214;&#36845;&#20195;&#21435;&#22122;&#26426;&#22120;&#20154;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;3D&#26059;&#36716;&#21644;&#24179;&#31227;&#12290;&#22312;&#27599;&#20010;&#21435;&#22122;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#26411;&#31471;&#25191;&#34892;&#22120;&#23039;&#24577;&#20272;&#35745;&#34920;&#31034;&#20026;3D&#22330;&#26223;&#20196;&#29260;&#65292;&#24182;&#39044;&#27979;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10885v1 Announce Type: cross  Abstract: We marry diffusion policies and 3D scene representations for robot manipulation. Diffusion policies learn the action distribution conditioned on the robot and environment state using conditional diffusion models. They have recently shown to outperform both deterministic and alternative state-conditioned action distribution learning methods. 3D robot policies use 3D scene feature representations aggregated from a single or multiple camera views using sensed depth. They have shown to generalize better than their 2D counterparts across camera viewpoints. We unify these two lines of work and present 3D Diffuser Actor, a neural policy architecture that, given a language instruction, builds a 3D representation of the visual scene and conditions on it to iteratively denoise 3D rotations and translations for the robot's end-effector. At each denoising iteration, our model represents end-effector pose estimates as 3D scene tokens and predicts t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25910;&#38598;&#36731;&#37327;&#32423;VQA&#20559;&#22909;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;Direct Preference Optimization&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#33021;&#21147;&#19978;&#21462;&#24471;&#26174;&#33879;&#25552;&#21319;&#65292;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#19979;&#27604;&#20854;&#20182;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.10884</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#20559;&#22909;&#23545;&#40784;&#20462;&#22797;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#19978;&#30340;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Multi-modal preference alignment remedies regression of visual instruction tuning on language model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10884
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25910;&#38598;&#36731;&#37327;&#32423;VQA&#20559;&#22909;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;Direct Preference Optimization&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#33021;&#21147;&#19978;&#21462;&#24471;&#26174;&#33879;&#25552;&#21319;&#65292;&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#19979;&#27604;&#20854;&#20182;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#34987;&#26399;&#26395;&#33021;&#22815;&#25903;&#25345;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#20132;&#25442;&#24335;&#22810;&#36718;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20351;&#29992;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#65288;VQA&#65289;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;MLLMs&#21487;&#33021;&#20250;&#20986;&#29616;&#36864;&#21270;&#65292;&#22240;&#20026;VQA&#25968;&#25454;&#38598;&#32570;&#20047;&#21407;&#22987;&#25991;&#26412;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#21518;&#32773;&#26159;&#24213;&#23618;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#65288;6k&#26465;&#35760;&#24405;&#65289;&#30340;VQA&#20559;&#22909;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#31572;&#26696;&#30001;Gemini&#20197;&#32454;&#31890;&#24230;&#26041;&#24335;&#27880;&#37322;&#20102;5&#20010;&#36136;&#37327;&#25351;&#26631;&#65292;&#28982;&#21518;&#30740;&#31350;&#20102;&#26631;&#20934;&#30340;&#30417;&#30563;&#24494;&#35843;&#12289;&#25298;&#32477;&#25277;&#26679;&#12289;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#21644;SteerLM&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;DPO&#65292;&#25105;&#20204;&#33021;&#22815;&#36229;&#36234;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#23548;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;6.73&#30340;MT-Bench&#20998;&#25968;&#65292;&#32780;Vicuna&#30340;6.57&#21644;LLaVA&#30340;5.99&#65292;&#23613;&#31649;&#25968;&#25454;&#35268;&#27169;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10884v1 Announce Type: cross  Abstract: In production, multi-modal large language models (MLLMs) are expected to support multi-turn queries of interchanging image and text modalities. However, the current MLLMs trained with visual-question-answering (VQA) datasets could suffer from degradation, as VQA datasets lack the diversity and complexity of the original text instruction datasets which the underlying language model had been trained with. To address this challenging degradation, we first collect a lightweight (6k entries) VQA preference dataset where answers were annotated by Gemini for 5 quality metrics in a granular fashion, and investigate standard Supervised Fine-tuning, rejection sampling, Direct Preference Optimization (DPO), and SteerLM. Our findings indicate that the with DPO we are able to surpass instruction-following capabilities of the language model, achieving a 6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despite small data scale. This
&lt;/p&gt;</description></item><item><title>&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#22312;&#24191;&#27867;&#30340;&#20998;&#24067;&#36716;&#21464;&#19979;&#36798;&#21040;&#21518;&#24724;&#30028;&#38480;&#65292;&#36825;&#23545;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#31561;&#30740;&#31350;&#39046;&#22495;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.10877</link><description>&lt;p&gt;
&#24378;&#20581;&#30340;&#26234;&#33021;&#20307;&#23398;&#20064;&#22240;&#26524;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust agents learn causal world models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10877
&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#22312;&#24191;&#27867;&#30340;&#20998;&#24067;&#36716;&#21464;&#19979;&#36798;&#21040;&#21518;&#24724;&#30028;&#38480;&#65292;&#36825;&#23545;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#31561;&#30740;&#31350;&#39046;&#22495;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#30452;&#26377;&#20154;&#20551;&#35774;&#22240;&#26524;&#25512;&#29702;&#22312;&#24378;&#20581;&#19988;&#20855;&#26377;&#36890;&#29992;&#26234;&#33021;&#20013;&#36215;&#30528;&#22522;&#30784;&#20316;&#29992;&#65292;&#28982;&#32780;&#19981;&#28165;&#26970;&#26234;&#33021;&#20307;&#26159;&#21542;&#24517;&#39035;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#25165;&#33021;&#25512;&#24191;&#21040;&#26032;&#30340;&#39046;&#22495;&#65292;&#25110;&#32773;&#20854;&#20182;&#24402;&#32435;&#20559;&#24046;&#26159;&#21542;&#36275;&#22815;&#12290;&#25105;&#20204;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#34920;&#26126;&#20219;&#20309;&#33021;&#22815;&#22312;&#22823;&#37327;&#20998;&#24067;&#36716;&#21464;&#19979;&#28385;&#36275;&#21518;&#24724;&#30028;&#38480;&#30340;&#26234;&#33021;&#20307;&#24517;&#39035;&#23398;&#20064;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#36817;&#20284;&#22240;&#26524;&#27169;&#22411;&#65292;&#23545;&#20110;&#20248;&#21270;&#26234;&#33021;&#20307;&#26469;&#35828;&#65292;&#35813;&#36817;&#20284;&#27169;&#22411;&#20250;&#25910;&#25947;&#21040;&#30495;&#23454;&#30340;&#22240;&#26524;&#27169;&#22411;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#19968;&#32467;&#26524;&#23545;&#20110;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#21253;&#25324;&#36801;&#31227;&#23398;&#20064;&#21644;&#22240;&#26524;&#25512;&#26029;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10877v1 Announce Type: new  Abstract: It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound under a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;"&#27169;&#25311;&#21463;&#25511;&#32452;&#35013;"&#26041;&#27861;&#65292;&#25104;&#21151;&#35774;&#35745;&#20986;&#22810;&#31181;&#31283;&#23450;&#30340;2D Skyrm&#31163;&#23376;&#36229;&#26448;&#26009;&#12290;</title><link>https://arxiv.org/abs/2402.10874</link><description>&lt;p&gt;
&#36890;&#36807;&#21463;&#25511;&#32452;&#35013;&#35774;&#35745;2D Skyrm&#31163;&#23376;&#36229;&#26448;&#26009;
&lt;/p&gt;
&lt;p&gt;
Design of 2D Skyrmionic Metamaterial Through Controlled Assembly
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10874
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;"&#27169;&#25311;&#21463;&#25511;&#32452;&#35013;"&#26041;&#27861;&#65292;&#25104;&#21151;&#35774;&#35745;&#20986;&#22810;&#31181;&#31283;&#23450;&#30340;2D Skyrm&#31163;&#23376;&#36229;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#30913;&#24615;Skyrmion&#21644;&#21453;Skyrmion&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#21363;&#22914;&#20309;&#21046;&#36896;&#20855;&#26377;&#19981;&#21516;&#29978;&#33267;&#23450;&#21046;&#25299;&#25169;&#32467;&#26500;&#30340;&#38750;&#24179;&#20961;&#39640;&#38454;Skyrm&#31163;&#23376;&#32441;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#38598;&#20013;&#22312;&#21333;&#23618;&#34180;&#33180;&#20869;Skyrmion&#36229;&#26448;&#26009;&#30340;&#26500;&#24314;&#36884;&#24452;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#31283;&#23450;&#30340;&#32593;&#26684;&#29366;&#12289;&#34180;&#29255;&#29366;&#21644;&#32454;&#32990;&#29366;Skyrm&#31163;&#23376;&#36229;&#26448;&#26009;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#8220;&#27169;&#25311;&#21463;&#25511;&#32452;&#35013;&#8221;&#27010;&#24565;&#65292;&#31616;&#32780;&#35328;&#20043;&#65292;&#36825;&#26159;&#21463;&#8220;&#28857;&#20987;&#21270;&#23398;&#8221;&#21551;&#21457;&#30340;&#19968;&#31181;&#21327;&#35758;&#65292;&#20801;&#35768;&#22312;&#21916;&#27426;&#30340;&#20301;&#32622;&#25918;&#32622;&#25299;&#25169;&#30913;&#32467;&#26500;&#65292;&#28982;&#21518;&#36890;&#36807;&#33021;&#37327;&#26368;&#23567;&#21270;&#26469;&#38416;&#26126;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10874v1 Announce Type: cross  Abstract: Despite extensive research on magnetic skyrmions and antiskyrmions, a significant challenge remains in crafting nontrivial high-order skyrmionic textures with varying, or even tailor-made, topologies. We address this challenge, by focusing on a construction pathway of skyrmionics metamaterial within a monolayer thin film and suggest several promising lattice-like, flakes-like, and cell-like skyrmionic metamaterials that are surprisingly stable. Central to our approach is the concept of 'simulated controlled assembly', in short, a protocol inspired by 'click chemistry' that allows for positioning topological magnetic structures where one likes, and then allowing for energy minimization to elucidate the stability. Utilizing high-throughput atomistic-spin-dynamic (ASD) simulations alongside state-of-the-art AI-driven tools, we have isolated skyrmions (topological charge Q=1), antiskyrmions (Q=-1), and skyrmionium (Q=0). These entities ser
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#20139;&#20102;&#20851;&#20110;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#20351;&#29992;AED&#31995;&#32479;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#36866;&#24403;&#30340;&#30446;&#26631;&#21644;&#31995;&#32479;&#35268;&#26684;&#30340;&#35270;&#35282;&#65292;&#26368;&#32456;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#21453;&#20107;&#23454;&#25512;&#26029;&#30340;AED&#26694;&#26550;&#24182;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2402.10870</link><description>&lt;p&gt;
&#19977;&#30028;&#20043;&#26368;&#65306;&#23454;&#36341;&#20013;&#30340;&#25968;&#23383;&#33829;&#38144;&#33258;&#36866;&#24212;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Best of Three Worlds: Adaptive Experimentation for Digital Marketing in Practice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#20139;&#20102;&#20851;&#20110;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#20351;&#29992;AED&#31995;&#32479;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#36866;&#24403;&#30340;&#30446;&#26631;&#21644;&#31995;&#32479;&#35268;&#26684;&#30340;&#35270;&#35282;&#65292;&#26368;&#32456;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#21453;&#20107;&#23454;&#25512;&#26029;&#30340;AED&#26694;&#26550;&#24182;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#23454;&#39564;&#35774;&#35745;&#65288;AED&#65289;&#26041;&#27861;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#24037;&#19994;&#30028;&#29992;&#20316;&#19968;&#31181;&#24037;&#20855;&#65292;&#20197;&#25552;&#39640;&#27979;&#35797;&#21534;&#21520;&#37327;&#25110;&#20943;&#23569;&#19982;&#20256;&#32479;A/B/N&#27979;&#35797;&#26041;&#27861;&#30456;&#27604;&#30340;&#23454;&#39564;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#34892;&#20026;&#21644;&#20445;&#35777;&#22312;&#29702;&#24819;&#21270;&#30340;&#31283;&#24577;&#35774;&#32622;&#20043;&#22806;&#24182;&#19981;&#20026;&#20154;&#29087;&#30693;&#12290;&#26412;&#25991;&#20998;&#20139;&#20102;&#26377;&#20851;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#22825;&#30495;&#22320;&#20351;&#29992;AED&#31995;&#32479;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#36866;&#24403;&#30340;&#30446;&#26631;&#21644;&#31995;&#32479;&#35268;&#26684;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#26681;&#25454;&#36825;&#20123;&#32463;&#39564;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#21453;&#20107;&#23454;&#25512;&#26029;&#30340;AED&#26694;&#26550;&#65292;&#24182;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10870v1 Announce Type: new  Abstract: Adaptive experimental design (AED) methods are increasingly being used in industry as a tool to boost testing throughput or reduce experimentation cost relative to traditional A/B/N testing methods. However, the behavior and guarantees of such methods are not well-understood beyond idealized stationary settings. This paper shares lessons learned regarding the challenges of naively using AED systems in industrial settings where non-stationarity is prevalent, while also providing perspectives on the proper objectives and system specifications in such settings. We developed an AED framework for counterfactual inference based on these experiences, and tested it in a commercial environment.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24046;&#20998;&#31169;&#26377;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#24046;&#20998;&#38544;&#31169;&#21644;&#36801;&#31227;&#23398;&#20064;&#65292;&#20197;&#25552;&#21319;&#24515;&#29702;&#20581;&#24247;&#30417;&#27979;&#20013;&#30340;&#25968;&#25454;&#38544;&#31169;&#24615;&#21644;&#25968;&#25454;&#20805;&#36275;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10862</link><description>&lt;p&gt;
&#22312;&#26085;&#24120;&#29615;&#22659;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#30417;&#27979;&#65306;&#20197;&#21387;&#21147;&#26816;&#27979;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Differential Private Federated Transfer Learning for Mental Health Monitoring in Everyday Settings: A Case Study on Stress Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10862
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24046;&#20998;&#31169;&#26377;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#24046;&#20998;&#38544;&#31169;&#21644;&#36801;&#31227;&#23398;&#20064;&#65292;&#20197;&#25552;&#21319;&#24515;&#29702;&#20581;&#24247;&#30417;&#27979;&#20013;&#30340;&#25968;&#25454;&#38544;&#31169;&#24615;&#21644;&#25968;&#25454;&#20805;&#36275;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#22312;&#21508;&#20010;&#20154;&#32676;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#38656;&#35201;&#26377;&#25928;&#30340;&#30417;&#27979;&#26469;&#20943;&#36731;&#20854;&#23545;&#29983;&#27963;&#36136;&#37327;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;&#25968;&#25454;&#39537;&#21160;&#30340;&#24515;&#29702;&#20581;&#24247;&#30417;&#27979;&#26041;&#27861;&#30340;&#20852;&#36215;&#24378;&#35843;&#20102;&#22312;&#22788;&#29702;&#25935;&#24863;&#20581;&#24247;&#25968;&#25454;&#26102;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#32852;&#37030;&#23398;&#20064;&#22312;&#24515;&#29702;&#20581;&#24247;&#30417;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#24212;&#23545;&#29305;&#23450;&#32593;&#32476;&#25915;&#20987;&#21644;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#25968;&#25454;&#19981;&#36275;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24046;&#20998;&#31169;&#26377;&#32852;&#37030;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#25968;&#25454;&#38544;&#31169;&#24615;&#24182;&#20016;&#23500;&#25968;&#25454;&#20805;&#36275;&#24615;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#32852;&#37030;&#23398;&#20064;&#19982;&#24046;&#20998;&#38544;&#31169;&#65288;&#36890;&#36807;&#23558;&#22122;&#22768;&#24341;&#20837;&#26356;&#26032;&#65289;&#21644;&#36801;&#31227;&#23398;&#20064;&#65288;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#36890;&#29992;&#27169;&#22411;&#65289;&#20004;&#20010;&#20851;&#38190;&#20803;&#32032;&#30456;&#32467;&#21512;&#65292;&#20197;&#24212;&#23545;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#32570;&#22914;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10862v1 Announce Type: new  Abstract: Mental health conditions, prevalent across various demographics, necessitate efficient monitoring to mitigate their adverse impacts on life quality. The surge in data-driven methodologies for mental health monitoring has underscored the importance of privacy-preserving techniques in handling sensitive health data. Despite strides in federated learning for mental health monitoring, existing approaches struggle with vulnerabilities to certain cyber-attacks and data insufficiency in real-world applications. In this paper, we introduce a differential private federated transfer learning framework for mental health monitoring to enhance data privacy and enrich data sufficiency. To accomplish this, we integrate federated learning with two pivotal elements: (1) differential privacy, achieved by introducing noise into the updates, and (2) transfer learning, employing a pre-trained universal model to adeptly address issues of data imbalance and in
&lt;/p&gt;</description></item><item><title>JetTrain&#26159;&#19968;&#20010;&#23558;&#29305;&#23450;&#20219;&#21153;&#20174;IDE&#22996;&#27966;&#32473;&#36828;&#31243;&#35745;&#31639;&#36164;&#28304;&#30340;IDE&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#23454;&#39564;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.10857</link><description>&lt;p&gt;
JetTrain: IDE&#21407;&#29983;&#30340;&#26426;&#22120;&#23398;&#20064;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
JetTrain: IDE-Native Machine Learning Experiments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10857
&lt;/p&gt;
&lt;p&gt;
JetTrain&#26159;&#19968;&#20010;&#23558;&#29305;&#23450;&#20219;&#21153;&#20174;IDE&#22996;&#27966;&#32473;&#36828;&#31243;&#35745;&#31639;&#36164;&#28304;&#30340;IDE&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#23454;&#39564;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#24320;&#21457;&#29615;&#22659;&#65288;IDE&#65289;&#26159;&#24120;&#35265;&#30340;&#32534;&#20889;&#21644;&#35843;&#35797;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23578;&#26410;&#34987;&#24191;&#27867;&#37319;&#29992;&#20110;&#21551;&#21160;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#23454;&#39564;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;JetTrain&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;JetTrain&#26159;&#19968;&#20010;IDE&#38598;&#25104;&#24037;&#20855;&#65292;&#23558;&#29305;&#23450;&#20219;&#21153;&#20174;IDE&#22996;&#27966;&#32473;&#36828;&#31243;&#35745;&#31639;&#36164;&#28304;&#12290;&#29992;&#25143;&#21487;&#20197;&#22312;&#26412;&#22320;&#32534;&#20889;&#21644;&#35843;&#35797;&#20195;&#30721;&#65292;&#28982;&#21518;&#20351;&#29992;&#25353;&#38656;&#30828;&#20214;&#26080;&#32541;&#22320;&#22312;&#36828;&#31243;&#36816;&#34892;&#23427;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;ML&#35757;&#32451;&#38382;&#39064;&#30340;&#20934;&#20837;&#38376;&#27099;&#24182;&#22686;&#21152;&#23454;&#39564;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10857v1 Announce Type: cross  Abstract: Integrated development environments (IDEs) are prevalent code-writing and debugging tools. However, they have yet to be widely adopted for launching machine learning (ML) experiments. This work aims to fill this gap by introducing JetTrain, an IDE-integrated tool that delegates specific tasks from an IDE to remote computational resources. A user can write and debug code locally and then seamlessly run it remotely using on-demand hardware. We argue that this approach can lower the entry barrier for ML training problems and increase experiment throughput.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#33014;&#22218;&#32593;&#32476;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65292;&#39318;&#27425;&#22312;&#36825;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#33014;&#22218;&#32593;&#32476;&#22312;&#22686;&#24378;&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#20998;&#26512;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10851</link><description>&lt;p&gt;
HistoSegCap: &#33014;&#22218;&#32593;&#32476;&#29992;&#20110;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#20013;&#32452;&#32455;&#31867;&#22411;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
HistoSegCap: Capsules for Weakly-Supervised Semantic Segmentation of Histological Tissue Type in Whole Slide Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#20998;&#26512;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#33014;&#22218;&#32593;&#32476;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65292;&#39318;&#27425;&#22312;&#36825;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#33014;&#22218;&#32593;&#32476;&#22312;&#22686;&#24378;&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#20998;&#26512;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10851v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#36234; &#25688;&#35201;&#65306;&#25968;&#23383;&#30149;&#29702;&#23398;&#28041;&#21450;&#23558;&#29289;&#29702;&#32452;&#32455;&#20999;&#29255;&#36716;&#25442;&#20026;&#39640;&#20998;&#36776;&#29575;&#30340;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#65292;&#30149;&#29702;&#23398;&#23478;&#20998;&#26512;&#20854;&#20013;&#21463;&#30142;&#30149;&#24433;&#21709;&#30340;&#32452;&#32455;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#35768;&#22810;&#26174;&#24494;&#39046;&#22495;&#30340;&#22823;&#22411;&#32452;&#32455;&#23398;&#20999;&#29255;&#23545;&#35270;&#35273;&#25628;&#32034;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#24110;&#21161;&#30149;&#29702;&#23398;&#23478;&#65292;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#65288;CAD&#65289;&#31995;&#32479;&#25552;&#20379;&#35270;&#35273;&#36741;&#21161;&#65292;&#20197;&#26377;&#25928;&#22320;&#26816;&#26597;WSIs&#24182;&#35782;&#21035;&#35786;&#26029;&#30456;&#20851;&#21306;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#20998;&#26512;&#26041;&#27861;&#65292;&#37319;&#29992;&#22522;&#20110;&#33014;&#22218;&#32593;&#32476;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;WSSS&#65289;&#65292;&#36825;&#26159;&#39318;&#27425;&#24212;&#29992;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#20351;&#29992;&#25968;&#23383;&#30149;&#29702;&#23398;&#22270;&#35889;&#65288;ADP&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;&#20854;&#20182;&#32452;&#32455;&#30149;&#29702;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#33014;&#22218;&#32593;&#32476;&#22312;&#22686;&#24378;&#32452;&#32455;&#30149;&#29702;&#22270;&#20687;&#20998;&#26512;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10851v1 Announce Type: cross  Abstract: Digital pathology involves converting physical tissue slides into high-resolution Whole Slide Images (WSIs), which pathologists analyze for disease-affected tissues. However, large histology slides with numerous microscopic fields pose challenges for visual search. To aid pathologists, Computer Aided Diagnosis (CAD) systems offer visual assistance in efficiently examining WSIs and identifying diagnostically relevant regions. This paper presents a novel histopathological image analysis method employing Weakly Supervised Semantic Segmentation (WSSS) based on Capsule Networks, the first such application. The proposed model is evaluated using the Atlas of Digital Pathology (ADP) dataset and its performance is compared with other histopathological semantic segmentation methodologies. The findings underscore the potential of Capsule Networks in enhancing the precision and efficiency of histopathological image analysis. Experimental results s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FedD2S&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#21040;&#27973;&#30340;&#23618;&#20002;&#24323;&#26426;&#21046;&#65292;&#22312;&#26080;&#25968;&#25454;&#32852;&#37030;&#30693;&#35782;&#33976;&#39311;&#20013;&#22686;&#24378;&#20102;&#26412;&#22320;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#65292;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#21644;&#25913;&#21892;&#23458;&#25143;&#38388;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10846</link><description>&lt;p&gt;
FedD2S: &#20010;&#24615;&#21270;&#26080;&#25968;&#25454;&#32852;&#37030;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
FedD2S: Personalized Data-Free Federated Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10846
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FedD2S&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#21040;&#27973;&#30340;&#23618;&#20002;&#24323;&#26426;&#21046;&#65292;&#22312;&#26080;&#25968;&#25454;&#32852;&#37030;&#30693;&#35782;&#33976;&#39311;&#20013;&#22686;&#24378;&#20102;&#26412;&#22320;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#65292;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#21644;&#25913;&#21892;&#23458;&#25143;&#38388;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26694;&#26550;&#20013;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedD2S&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;pFL&#65289;&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#12290;FedD2S&#22312;&#26080;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#20013;&#32467;&#21512;&#20102;&#28145;&#21040;&#27973;&#30340;&#23618;&#20002;&#24323;&#26426;&#21046;&#65292;&#20197;&#22686;&#24378;&#26412;&#22320;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#22270;&#20687;&#25968;&#25454;&#38598;&#65288;FEMNIST&#12289;CIFAR10&#12289;CINIC0&#21644;CIFAR100&#65289;&#19978;&#36827;&#34892;&#22823;&#37327;&#27169;&#25311;&#65292;&#25105;&#20204;&#23558;FedD2S&#19982;&#26368;&#20808;&#36827;&#30340;FL&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#20855;&#26377;&#21152;&#36895;&#25910;&#25947;&#21644;&#25913;&#21892;&#23458;&#25143;&#38388;&#20844;&#24179;&#24615;&#30340;&#29305;&#28857;&#12290;&#24341;&#20837;&#30340;&#23618;&#20002;&#24323;&#25216;&#26415;&#26377;&#25928;&#25429;&#25417;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10846v1 Announce Type: cross  Abstract: This paper addresses the challenge of mitigating data heterogeneity among clients within a Federated Learning (FL) framework. The model-drift issue, arising from the noniid nature of client data, often results in suboptimal personalization of a global model compared to locally trained models for each client. To tackle this challenge, we propose a novel approach named FedD2S for Personalized Federated Learning (pFL), leveraging knowledge distillation. FedD2S incorporates a deep-to-shallow layer-dropping mechanism in the data-free knowledge distillation process to enhance local model personalization. Through extensive simulations on diverse image datasets-FEMNIST, CIFAR10, CINIC0, and CIFAR100-we compare FedD2S with state-of-the-art FL baselines. The proposed approach demonstrates superior performance, characterized by accelerated convergence and improved fairness among clients. The introduced layer-dropping technique effectively capture
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22810;&#39057;&#25955;&#23556;&#30005;&#22330;&#30340;&#25391;&#24133;&#20934;&#30830;&#39640;&#25928;&#22320;&#37325;&#24314;&#38543;&#26426;&#24418;&#29366;&#30340;&#20108;&#32500;&#20171;&#36136;&#29289;&#20307;&#65292;&#36890;&#36807;&#36870;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#35299;&#20915;&#20102;&#38750;&#21807;&#19968;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.10831</link><description>&lt;p&gt;
&#22522;&#20110;GAN&#39537;&#21160;&#30340;&#20108;&#32500;&#20171;&#36136;&#25955;&#23556;&#20307;&#30005;&#30913;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
GAN-driven Electromagnetic Imaging of 2-D Dielectric Scatterers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#22810;&#39057;&#25955;&#23556;&#30005;&#22330;&#30340;&#25391;&#24133;&#20934;&#30830;&#39640;&#25928;&#22320;&#37325;&#24314;&#38543;&#26426;&#24418;&#29366;&#30340;&#20108;&#32500;&#20171;&#36136;&#29289;&#20307;&#65292;&#36890;&#36807;&#36870;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#35299;&#20915;&#20102;&#38750;&#21807;&#19968;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#28459;&#23556;&#38382;&#39064;&#20855;&#26377;&#22266;&#26377;&#30340;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#36870;&#38382;&#39064;&#19988;&#38750;&#32447;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24378;&#22823;&#26041;&#27861;&#65292;&#20381;&#36182;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20174;&#22810;&#39057;&#25955;&#23556;&#30005;&#22330;&#30340;&#25391;&#24133;&#20934;&#30830;&#39640;&#25928;&#22320;&#37325;&#24314;&#20219;&#24847;&#24418;&#29366;&#30340;&#20108;&#32500;&#20171;&#36136;&#29289;&#20307;&#12290;&#36890;&#36807;&#35757;&#32451;&#23545;&#25239;&#33258;&#32534;&#30721;&#22120;&#65288;AAE&#65289;&#26469;&#23398;&#20064;&#29983;&#25104;&#25955;&#23556;&#20307;&#20960;&#20309;&#24418;&#29366;&#65292;&#20854;&#32422;&#26463;&#20026;&#36981;&#24490;&#39640;&#26031;&#20998;&#24067;&#30340;&#20302;&#32500;&#28508;&#22312;&#34920;&#31034;&#12290;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#19968;&#31995;&#21015;&#36866;&#24403;&#35774;&#35745;&#30340;&#23494;&#38598;&#23618;&#12289;&#24050;&#35757;&#32451;&#30340;&#29983;&#25104;&#22120;&#20197;&#21450;&#21333;&#29420;&#35757;&#32451;&#30340;&#27491;&#21521;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#32858;&#36870;&#31070;&#32463;&#32593;&#32476;&#65288;INN&#65289;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;&#36870;&#32593;&#32476;&#36755;&#20986;&#30340;&#22270;&#20687;&#19982;&#27491;&#21521;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#36827;&#34892;&#27604;&#36739;&#39564;&#35777;&#37325;&#24314;&#30340;&#22270;&#20687;&#65292;&#35299;&#20915;&#20102;&#38750;&#21807;&#19968;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10831v1 Announce Type: cross  Abstract: Inverse scattering problems are inherently challenging, given the fact they are ill-posed and nonlinear. This paper presents a powerful deep learning-based approach that relies on generative adversarial networks to accurately and efficiently reconstruct randomly-shaped two-dimensional dielectric objects from amplitudes of multi-frequency scattered electric fields. An adversarial autoencoder (AAE) is trained to learn to generate the scatterer's geometry from a lower-dimensional latent representation constrained to adhere to the Gaussian distribution. A cohesive inverse neural network (INN) framework is set up comprising a sequence of appropriately designed dense layers, the already-trained generator as well as a separately trained forward neural network. The images reconstructed at the output of the inverse network are validated through comparison with outputs from the forward neural network, addressing the non-uniqueness challenge inhe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24230;&#37327;&#23398;&#20064;&#30340;&#30446;&#26631;&#26465;&#20214;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#31232;&#30095;&#22870;&#21169;&#12289;&#23545;&#31216;&#21644;&#30830;&#23450;&#24615;&#21160;&#20316;&#19979;&#30340;&#26368;&#20248;&#20540;&#20989;&#25968;&#36817;&#20284;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#23398;&#20064;&#27425;&#20248;&#31163;&#32447;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#26174;&#30528;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10820</link><description>&lt;p&gt;
&#36890;&#36807;&#24230;&#37327;&#23398;&#20064;&#30340;&#30446;&#26631;&#26465;&#20214;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Goal-Conditioned Offline Reinforcement Learning via Metric Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10820
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24230;&#37327;&#23398;&#20064;&#30340;&#30446;&#26631;&#26465;&#20214;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#31232;&#30095;&#22870;&#21169;&#12289;&#23545;&#31216;&#21644;&#30830;&#23450;&#24615;&#21160;&#20316;&#19979;&#30340;&#26368;&#20248;&#20540;&#20989;&#25968;&#36817;&#20284;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#23398;&#20064;&#27425;&#20248;&#31163;&#32447;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#26174;&#30528;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#30446;&#26631;&#26465;&#20214;&#19979;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#20174;&#27425;&#20248;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26368;&#20248;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#22788;&#29702;&#31232;&#30095;&#22870;&#21169;&#12289;&#23545;&#31216;&#19988;&#30830;&#23450;&#24615;&#21160;&#20316;&#19979;&#30340;&#30446;&#26631;&#26465;&#20214;&#31163;&#32447;RL&#38382;&#39064;&#30340;&#26368;&#20248;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#34920;&#31034;&#24674;&#22797;&#20248;&#21270;&#30340;&#23646;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23548;&#33268;&#35813;&#23646;&#24615;&#30340;&#26032;&#20248;&#21270;&#30446;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#20540;&#20989;&#25968;&#20197;&#28436;&#21592;-&#35780;&#35770;&#32773;&#30340;&#26041;&#24335;&#25351;&#23548;&#31574;&#30053;&#30340;&#23398;&#20064;&#65292;&#36825;&#31181;&#26041;&#27861;&#34987;&#25105;&#20204;&#31216;&#20026;MetricRL&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#31163;&#32447;RL&#22522;&#32447;&#22312;&#20174;&#27425;&#20248;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#32500;&#35266;&#27979;&#21644;&#22810;&#30446;&#26631;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10820v1 Announce Type: new  Abstract: In this work, we address the problem of learning optimal behavior from sub-optimal datasets in the context of goal-conditioned offline reinforcement learning. To do so, we propose a novel way of approximating the optimal value function for goal-conditioned offline RL problems under sparse rewards, symmetric and deterministic actions. We study a property for representations to recover optimality and propose a new optimization objective that leads to such property. We use the learned value function to guide the learning of a policy in an actor-critic fashion, a method we name MetricRL. Experimentally, we show how our method consistently outperforms other offline RL baselines in learning from sub-optimal offline datasets. Moreover, we show the effectiveness of our method in dealing with high-dimensional observations and in multi-goal tasks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#20302;&#32500;&#26367;&#20195;&#31354;&#38388;&#20013;&#30340;&#20984;&#22810;&#38754;&#20307;&#39030;&#28857;&#19978;&#23884;&#20837;&#32467;&#26524;&#65292;&#24182;&#25506;&#31350;&#21333;&#32431;&#24418;&#20013;&#30340;&#19968;&#33268;&#24615;&#21306;&#22495;&#65292;&#26435;&#34913;&#20102;&#26367;&#20195;&#25439;&#22833;&#32500;&#24230;&#12289;&#38382;&#39064;&#23454;&#20363;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.10818</link><description>&lt;p&gt;
&#22312;&#27169;&#22411;&#30340;&#20984;&#26367;&#20195;&#21697;&#30340;&#19968;&#33268;&#24615;&#21644;&#32500;&#24230;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Trading off Consistency and Dimensionality of Convex Surrogates for the Mode
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10818
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20302;&#32500;&#26367;&#20195;&#31354;&#38388;&#20013;&#30340;&#20984;&#22810;&#38754;&#20307;&#39030;&#28857;&#19978;&#23884;&#20837;&#32467;&#26524;&#65292;&#24182;&#25506;&#31350;&#21333;&#32431;&#24418;&#20013;&#30340;&#19968;&#33268;&#24615;&#21306;&#22495;&#65292;&#26435;&#34913;&#20102;&#26367;&#20195;&#25439;&#22833;&#32500;&#24230;&#12289;&#38382;&#39064;&#23454;&#20363;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#31867;&#20998;&#31867;&#20013;&#65292;&#24517;&#39035;&#23558;&#32467;&#26524;&#23884;&#20837;&#21040;&#33267;&#23569;&#26377;$n-1$&#32500;&#30340;&#23454;&#25968;&#31354;&#38388;&#20013;&#65292;&#20197;&#35774;&#35745;&#19968;&#31181;&#19968;&#33268;&#30340;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#65292;&#36825;&#20250;&#23548;&#33268;"&#27491;&#30830;"&#30340;&#20998;&#31867;&#65292;&#32780;&#19981;&#21463;&#25968;&#25454;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#22312;&#20449;&#24687;&#26816;&#32034;&#21644;&#32467;&#26500;&#21270;&#39044;&#27979;&#20219;&#21153;&#31561;&#38656;&#35201;&#22823;&#37327;n&#26102;&#65292;&#20248;&#21270;n-1&#32500;&#26367;&#20195;&#24120;&#24120;&#26159;&#26840;&#25163;&#30340;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22810;&#31867;&#20998;&#31867;&#20013;&#22914;&#20309;&#26435;&#34913;&#26367;&#20195;&#25439;&#22833;&#32500;&#24230;&#12289;&#38382;&#39064;&#23454;&#20363;&#25968;&#37327;&#20197;&#21450;&#22312;&#21333;&#32431;&#24418;&#19978;&#32422;&#26463;&#19968;&#33268;&#24615;&#21306;&#22495;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36319;&#38543;&#36807;&#21435;&#30340;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#23884;&#20837;&#36807;&#31243;&#65292;&#23558;&#32467;&#26524;&#26144;&#23556;&#21040;&#20302;&#32500;&#26367;&#20195;&#31354;&#38388;&#20013;&#30340;&#20984;&#22810;&#38754;&#20307;&#30340;&#39030;&#28857;&#19978;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#27599;&#20010;&#28857;&#36136;&#37327;&#20998;&#24067;&#21608;&#22260;&#23384;&#22312;&#21333;&#32431;&#24418;&#30340;&#20840;&#32500;&#23376;&#38598;&#65292;&#20854;&#20013;&#19968;&#33268;&#24615;&#25104;&#31435;&#65292;&#20294;&#26159;&#65292;&#23569;&#20110;n-1&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#19968;&#20123;&#20998;&#24067;&#65292;&#23545;&#20110;&#36825;&#20123;&#20998;&#24067;&#65292;&#19968;&#31181;&#29616;&#35937;&#24615;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10818v1 Announce Type: new  Abstract: In multiclass classification over $n$ outcomes, the outcomes must be embedded into the reals with dimension at least $n-1$ in order to design a consistent surrogate loss that leads to the "correct" classification, regardless of the data distribution. For large $n$, such as in information retrieval and structured prediction tasks, optimizing a surrogate in $n-1$ dimensions is often intractable. We investigate ways to trade off surrogate loss dimension, the number of problem instances, and restricting the region of consistency in the simplex for multiclass classification. Following past work, we examine an intuitive embedding procedure that maps outcomes into the vertices of convex polytopes in a low-dimensional surrogate space. We show that full-dimensional subsets of the simplex exist around each point mass distribution for which consistency holds, but also, with less than $n-1$ dimensions, there exist distributions for which a phenomeno
&lt;/p&gt;</description></item><item><title>TernaryVote&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#19977;&#20540;&#21387;&#32553;&#22120;&#21644;&#22823;&#22810;&#25968;&#25237;&#31080;&#26426;&#21046;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#26799;&#24230;&#21387;&#32553;&#21644;&#25308;&#21344;&#24237;&#23481;&#24525;&#65292;&#30456;&#36739;&#20110;&#24050;&#26377;&#30340;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#35777;&#21644;&#25910;&#25947;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10816</link><description>&lt;p&gt;
TernaryVote&#65306;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#12289;&#36890;&#20449;&#39640;&#25928;&#24615;&#21644;&#25308;&#21344;&#24237;&#23481;&#24525;&#29305;&#24615;&#30340;&#24322;&#26500;&#25968;&#25454;&#19978;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
TernaryVote: Differentially Private, Communication Efficient, and Byzantine Resilient Distributed Optimization on Heterogeneous Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10816
&lt;/p&gt;
&lt;p&gt;
TernaryVote&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#19977;&#20540;&#21387;&#32553;&#22120;&#21644;&#22823;&#22810;&#25968;&#25237;&#31080;&#26426;&#21046;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#26799;&#24230;&#21387;&#32553;&#21644;&#25308;&#21344;&#24237;&#23481;&#24525;&#65292;&#30456;&#36739;&#20110;&#24050;&#26377;&#30340;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#35777;&#21644;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#38754;&#20020;&#30528;&#38544;&#31169;&#20445;&#25252;&#12289;&#36890;&#20449;&#25928;&#29575;&#21644;&#23545;&#25925;&#38556;&#21644;&#25932;&#23545;&#34892;&#20026;&#30340;&#40065;&#26834;&#24615;&#19977;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#22823;&#37327;&#30740;&#31350;&#33268;&#21147;&#20110;&#20998;&#21035;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#23427;&#20204;&#30340;&#32508;&#21512;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TernaryVote&#65292;&#23427;&#32467;&#21512;&#20102;&#19977;&#20540;&#21387;&#32553;&#22120;&#21644;&#22823;&#22810;&#25968;&#25237;&#31080;&#26426;&#21046;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#26799;&#24230;&#21387;&#32553;&#21644;&#25308;&#21344;&#24237;&#23481;&#24525;&#12290;&#25105;&#20204;&#20174;&#26032;&#20852;&#30340;f-&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21644;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#25308;&#21344;&#24237;&#23481;&#24525;&#35282;&#24230;&#23545;&#38544;&#31169;&#20445;&#35777;&#36827;&#34892;&#20102;&#29702;&#35770;&#37327;&#21270;&#12290;&#29305;&#21035;&#26159;&#22312;&#38544;&#31169;&#20445;&#35777;&#26041;&#38754;&#65292;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#31526;&#21495;&#30340;&#26041;&#27861;StoSign&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#23545;&#26799;&#24230;&#22823;&#23567;&#30340;&#32500;&#24230;&#20381;&#36182;&#65292;&#24182;&#36890;&#36807;&#23567;&#25209;&#37327;&#25277;&#26679;&#23454;&#29616;&#20102;&#38544;&#31169;&#25918;&#22823;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#21487;&#27604;&#36739;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10816v1 Announce Type: new  Abstract: Distributed training of deep neural networks faces three critical challenges: privacy preservation, communication efficiency, and robustness to fault and adversarial behaviors. Although significant research efforts have been devoted to addressing these challenges independently, their synthesis remains less explored. In this paper, we propose TernaryVote, which combines a ternary compressor and the majority vote mechanism to realize differential privacy, gradient compression, and Byzantine resilience simultaneously. We theoretically quantify the privacy guarantee through the lens of the emerging f-differential privacy (DP) and the Byzantine resilience of the proposed algorithm. Particularly, in terms of privacy guarantees, compared to the existing sign-based approach StoSign, the proposed method improves the dimension dependence on the gradient size and enjoys privacy amplification by mini-batch sampling while ensuring a comparable conver
&lt;/p&gt;</description></item><item><title>&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#35745;&#31639;&#30456;&#20284;&#24615;&#20197;&#25552;&#21319;&#32852;&#24819;&#23384;&#20648;&#22120;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#39044;&#35757;&#32451;&#32593;&#32476;&#26469;&#29983;&#25104;&#23884;&#20837;&#21521;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#26816;&#32034;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.10814</link><description>&lt;p&gt;
&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#32852;&#24819;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Associative Memories in the Feature Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10814
&lt;/p&gt;
&lt;p&gt;
&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#35745;&#31639;&#30456;&#20284;&#24615;&#20197;&#25552;&#21319;&#32852;&#24819;&#23384;&#20648;&#22120;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#39044;&#35757;&#32451;&#32593;&#32476;&#26469;&#29983;&#25104;&#23884;&#20837;&#21521;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#26816;&#32034;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#32852;&#24819;&#23384;&#20648;&#22120;&#27169;&#22411;&#26159;&#19968;&#20010;&#20989;&#25968;&#65292;&#32473;&#23450;&#19968;&#32452;&#25968;&#25454;&#28857;&#65292;&#20197;&#20219;&#24847;&#21521;&#37327;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#19982;&#35760;&#24518;&#38598;&#20013;&#26368;&#30456;&#20284;&#30340;&#25968;&#25454;&#28857;&#12290;&#28982;&#32780;&#65292;&#27969;&#34892;&#30340;&#23384;&#20648;&#27169;&#22411;&#22312;&#36731;&#24494;&#25439;&#22351;&#30340;&#24773;&#20917;&#19979;&#29978;&#33267;&#26080;&#27861;&#26816;&#32034;&#22270;&#20687;&#65292;&#23613;&#31649;&#36825;&#31181;&#25439;&#22351;&#23545;&#20154;&#31867;&#35780;&#20272;&#32773;&#26469;&#35828;&#24456;&#23481;&#26131;&#26816;&#27979;&#12290;&#36825;&#26159;&#22240;&#20026;&#30456;&#20284;&#24615;&#26159;&#22312;&#21407;&#22987;&#20687;&#32032;&#31354;&#38388;&#20013;&#35780;&#20272;&#30340;&#65292;&#32780;&#21407;&#22987;&#20687;&#32032;&#31354;&#38388;&#19981;&#21253;&#21547;&#26377;&#20851;&#22270;&#20687;&#30340;&#20219;&#20309;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#22312;&#23884;&#20837;&#31354;&#38388;&#32780;&#19981;&#26159;&#20687;&#32032;&#31354;&#38388;&#20013;&#35745;&#31639;&#30456;&#20284;&#24615;&#26469;&#36731;&#26494;&#35299;&#20915;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#24102;&#26377;&#23545;&#27604;&#25439;&#22833;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#26469;&#35745;&#31639;&#36825;&#20123;&#23884;&#20837;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#30001;&#20110;&#23884;&#20837;&#31354;&#38388;&#30340;&#32500;&#24230;&#36890;&#24120;&#26174;&#30528;&#23567;&#20110;&#20687;&#32032;&#31354;&#38388;&#65292;&#25105;&#20204;&#36824;&#33021;&#26356;&#24555;&#22320;&#35745;&#31639;&#30456;&#20284;&#24230;&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#65288;&#22914;CIFAR10&#21644;STL10&#65289;&#19978;&#27979;&#35797;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;&#24403;&#21069;&#27169;&#22411;&#30340;&#21478;&#19968;&#20010;&#32570;&#28857;&#26159;&#38656;&#35201;&#23384;&#20648;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10814v1 Announce Type: new  Abstract: An autoassociative memory model is a function that, given a set of data points, takes as input an arbitrary vector and outputs the most similar data point from the memorized set. However, popular memory models fail to retrieve images even when the corruption is mild and easy to detect for a human evaluator. This is because similarities are evaluated in the raw pixel space, which does not contain any semantic information about the images. This problem can be easily solved by computing \emph{similarities} in an embedding space instead of the pixel space. We show that an effective way of computing such embeddings is via a network pretrained with a contrastive loss. As the dimension of embedding spaces is often significantly smaller than the pixel space, we also have a faster computation of similarity scores. We test this method on complex datasets such as CIFAR10 and STL10. An additional drawback of current models is the need of storing the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;Variational Primal-Dual Policy Optimization (VPDPO)&#65292;&#36890;&#36807;&#23454;&#29616;Lagrangian&#21644;Fenchel&#23545;&#20598;&#26469;&#23558;&#21407;&#22987;&#21463;&#38480;&#38382;&#39064;&#37325;&#26500;&#20026;&#26080;&#32422;&#26463;&#30340;&#21407;&#22987;-&#23545;&#20598;&#20248;&#21270;&#65292;&#24182;&#19988;&#37319;&#29992;&#20048;&#35266;&#21407;&#21017;&#26356;&#26032;&#21407;&#22987;&#21464;&#37327;&#21644;&#26799;&#24230;&#19978;&#21319;&#26356;&#26032;&#23545;&#20598;&#21464;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.10810</link><description>&lt;p&gt;
&#21452;&#37325;&#23545;&#20598;&#65306;&#29992;&#20110;&#21463;&#38480;&#21046;&#24378;&#21270;&#23398;&#20064;&#30340;&#21464;&#20998;&#21407;&#22987;&#23545;&#20598;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Double Duality: Variational Primal-Dual Policy Optimization for Constrained Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;Variational Primal-Dual Policy Optimization (VPDPO)&#65292;&#36890;&#36807;&#23454;&#29616;Lagrangian&#21644;Fenchel&#23545;&#20598;&#26469;&#23558;&#21407;&#22987;&#21463;&#38480;&#38382;&#39064;&#37325;&#26500;&#20026;&#26080;&#32422;&#26463;&#30340;&#21407;&#22987;-&#23545;&#20598;&#20248;&#21270;&#65292;&#24182;&#19988;&#37319;&#29992;&#20048;&#35266;&#21407;&#21017;&#26356;&#26032;&#21407;&#22987;&#21464;&#37327;&#21644;&#26799;&#24230;&#19978;&#21319;&#26356;&#26032;&#23545;&#20598;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#21463;&#38480;&#20984;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#35775;&#38382;&#24230;&#37327;&#30340;&#20984;&#27867;&#20989;&#65292;&#21463;&#21040;&#20984;&#32422;&#26463;&#30340;&#38480;&#21046;&#12290;&#20026;&#21463;&#38480;&#20984;MDP&#35774;&#35745;&#31639;&#27861;&#38754;&#20020;&#20960;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#65288;1&#65289;&#22788;&#29702;&#22823;&#29366;&#24577;&#31354;&#38388;&#65292;&#65288;2&#65289;&#31649;&#29702;&#25506;&#32034;/&#24320;&#25299;&#30340;&#26435;&#34913;&#65292;&#20197;&#21450;&#65288;3&#65289;&#35299;&#20915;&#32422;&#26463;&#20248;&#21270;&#65292;&#20854;&#20013;&#30446;&#26631;&#21644;&#32422;&#26463;&#37117;&#26159;&#35775;&#38382;&#24230;&#37327;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21464;&#20998;&#21407;&#22987;&#23545;&#20598;&#31574;&#30053;&#20248;&#21270;&#65288;VPDPO&#65289;&#65292;&#20854;&#20013;Lagrangian &#21644; Fenchel &#23545;&#20598;&#34987;&#29992;&#20110;&#23558;&#21407;&#22987;&#21463;&#38480;&#38382;&#39064;&#37325;&#26032;&#20844;&#24335;&#21270;&#20026;&#26080;&#38480;&#21046;&#21407;&#22987;-&#23545;&#20598;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#21407;&#22987;&#21464;&#37327;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#20540;&#36845;&#20195;&#26356;&#26032;&#65292;&#36981;&#24490;&#19981;&#30830;&#23450;&#24615;&#38754;&#21069;&#30340;&#20048;&#35266;&#21407;&#21017;&#65288;OFU&#65289;&#65292;&#32780;&#23545;&#20598;&#21464;&#37327;&#21017;&#36890;&#36807;&#26799;&#24230;&#19978;&#21319;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10810v1 Announce Type: new  Abstract: We study the Constrained Convex Markov Decision Process (MDP), where the goal is to minimize a convex functional of the visitation measure, subject to a convex constraint. Designing algorithms for a constrained convex MDP faces several challenges, including (1) handling the large state space, (2) managing the exploration/exploitation tradeoff, and (3) solving the constrained optimization where the objective and the constraint are both nonlinear functions of the visitation measure. In this work, we present a model-based algorithm, Variational Primal-Dual Policy Optimization (VPDPO), in which Lagrangian and Fenchel duality are implemented to reformulate the original constrained problem into an unconstrained primal-dual optimization. Moreover, the primal variables are updated by model-based value iteration following the principle of Optimism in the Face of Uncertainty (OFU), while the dual variables are updated by gradient ascent. Moreover,
&lt;/p&gt;</description></item><item><title>&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#24037;&#19994;&#32423;&#22522;&#20934;TimeSeriesBench&#22635;&#34917;&#20102;&#24403;&#21069;&#31639;&#27861;&#22312;&#35757;&#32451;&#33539;&#24335;&#12289;&#22312;&#32447;&#26816;&#27979;&#33539;&#24335;&#21644;&#35780;&#20272;&#26631;&#20934;&#26041;&#38754;&#19982;&#23454;&#38469;&#38656;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.10802</link><description>&lt;p&gt;
TimeSeriesBench&#65306;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#24037;&#19994;&#32423;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
TimeSeriesBench: An Industrial-Grade Benchmark for Time Series Anomaly Detection Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10802
&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#24037;&#19994;&#32423;&#22522;&#20934;TimeSeriesBench&#22635;&#34917;&#20102;&#24403;&#21069;&#31639;&#27861;&#22312;&#35757;&#32451;&#33539;&#24335;&#12289;&#22312;&#32447;&#26816;&#27979;&#33539;&#24335;&#21644;&#35780;&#20272;&#26631;&#20934;&#26041;&#38754;&#19982;&#23454;&#38469;&#38656;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#21644;&#35268;&#27169;&#30340;&#34067;&#24310;&#65292;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65288;TSAD&#65289;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#19982;&#23454;&#38469;&#24037;&#19994;&#31995;&#32479;&#30340;&#38656;&#27714;&#30456;&#27604;&#65292;&#29616;&#26377;&#31639;&#27861;&#22312;&#35757;&#32451;&#33539;&#24335;&#12289;&#22312;&#32447;&#26816;&#27979;&#33539;&#24335;&#21644;&#35780;&#20272;&#26631;&#20934;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#12290;&#24403;&#21069;&#31639;&#27861;&#36890;&#24120;&#20026;&#27599;&#20010;&#21333;&#29420;&#30340;&#26102;&#38388;&#24207;&#21015;&#35757;&#32451;&#19968;&#20010;&#29305;&#23450;&#27169;&#22411;&#65292;&#28982;&#32780;&#22312;&#20855;&#26377;&#25968;&#20197;&#19975;&#35745;&#26354;&#32447;&#30340;&#22823;&#35268;&#27169;&#22312;&#32447;&#31995;&#32479;&#20013;&#65292;&#32500;&#25252;&#36825;&#20040;&#22810;&#27169;&#22411;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#20165;&#20351;&#29992;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#26469;&#26816;&#27979;&#24322;&#24120;&#30340;&#24615;&#33021;&#23578;&#19981;&#26126;&#30830;&#12290;&#22823;&#22810;&#25968;TSAD&#27169;&#22411;&#37117;&#26159;&#22312;&#26102;&#38388;&#24207;&#21015;&#30340;&#21382;&#21490;&#37096;&#20998;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#20854;&#26410;&#26469;&#37096;&#20998;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#20013;&#65292;&#32463;&#24120;&#37096;&#32626;&#21644;&#21319;&#32423;&#31995;&#32479;&#65292;&#27599;&#22825;&#37117;&#20250;&#20986;&#29616;&#26032;&#30340;&#12289;&#20197;&#21069;&#27809;&#26377;&#35265;&#36807;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#25152;&#35757;&#32451;&#27169;&#22411;&#30452;&#25509;&#24212;&#29992;&#20110;&#26032;&#26102;&#38388;&#24207;&#21015;&#30340;&#24615;&#33021;&#20063;&#19981;&#26126;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10802v1 Announce Type: new  Abstract: Driven by the proliferation of real-world application scenarios and scales, time series anomaly detection (TSAD) has attracted considerable scholarly and industrial interest. However, existing algorithms exhibit a gap in terms of training paradigm, online detection paradigm, and evaluation criteria when compared to the actual needs of real-world industrial systems. Firstly, current algorithms typically train a specific model for each individual time series. In a large-scale online system with tens of thousands of curves, maintaining such a multitude of models is impractical. The performance of using merely one single unified model to detect anomalies remains unknown. Secondly, most TSAD models are trained on the historical part of a time series and are tested on its future segment. In distributed systems, however, there are frequent system deployments and upgrades, with new, previously unseen time series emerging daily. The performance o
&lt;/p&gt;</description></item><item><title>BlackJAX&#26159;&#19968;&#20010;&#23454;&#29616;&#22312;JAX&#20013;&#32452;&#21512;&#24335;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#24211;&#65292;&#37319;&#29992;&#20989;&#25968;&#24335;&#26041;&#27861;&#25552;&#39640;&#26131;&#29992;&#24615;&#12289;&#36895;&#24230;&#21644;&#27169;&#22359;&#21270;&#65292;&#36866;&#29992;&#20110;&#38656;&#35201;&#23574;&#31471;&#26041;&#27861;&#12289;&#30740;&#31350;&#20154;&#21592;&#21644;&#24819;&#35201;&#20102;&#35299;&#24037;&#20316;&#21407;&#29702;&#30340;&#20154;&#12290;</title><link>https://arxiv.org/abs/2402.10797</link><description>&lt;p&gt;
BlackJAX: JAX&#20013;&#30340;&#32452;&#21512;&#24335;&#36125;&#21494;&#26031;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
BlackJAX: Composable Bayesian inference in JAX
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10797
&lt;/p&gt;
&lt;p&gt;
BlackJAX&#26159;&#19968;&#20010;&#23454;&#29616;&#22312;JAX&#20013;&#32452;&#21512;&#24335;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#24211;&#65292;&#37319;&#29992;&#20989;&#25968;&#24335;&#26041;&#27861;&#25552;&#39640;&#26131;&#29992;&#24615;&#12289;&#36895;&#24230;&#21644;&#27169;&#22359;&#21270;&#65292;&#36866;&#29992;&#20110;&#38656;&#35201;&#23574;&#31471;&#26041;&#27861;&#12289;&#30740;&#31350;&#20154;&#21592;&#21644;&#24819;&#35201;&#20102;&#35299;&#24037;&#20316;&#21407;&#29702;&#30340;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BlackJAX&#26159;&#19968;&#20010;&#24211;&#65292;&#23454;&#29616;&#20102;&#22312;&#36125;&#21494;&#26031;&#35745;&#31639;&#20013;&#24120;&#29992;&#30340;&#25277;&#26679;&#21644;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#12290;&#23427;&#36890;&#36807;&#37319;&#29992;&#20989;&#25968;&#24335;&#26041;&#27861;&#23454;&#29616;&#31639;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#26131;&#29992;&#24615;&#12289;&#36895;&#24230;&#21644;&#27169;&#22359;&#21270;&#12290;BlackJAX&#20351;&#29992;Python&#32534;&#20889;&#65292;&#21033;&#29992;JAX&#22312;CPU&#12289;GPU&#21644;TPU&#19978;&#32534;&#35793;&#21644;&#36816;&#34892;&#31867;&#20284;Numpy&#30340;&#25277;&#26679;&#22120;&#21644;&#21464;&#20998;&#26041;&#27861;&#12290;&#35813;&#24211;&#36890;&#36807;&#30452;&#25509;&#22788;&#29702;&#65288;&#38750;&#27491;&#21017;&#21270;&#65289;&#30446;&#26631;&#23545;&#25968;&#23494;&#24230;&#20989;&#25968;&#65292;&#19982;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#24456;&#22909;&#22320;&#38598;&#25104;&#12290;BlackJAX&#26088;&#22312;&#25104;&#20026;&#22522;&#26412;&#32479;&#35745;&#8220;&#22522;&#20803;&#8221;&#30340;&#20302;&#32423;&#21487;&#32452;&#21512;&#23454;&#29616;&#30340;&#38598;&#21512;&#65292;&#21487;&#32452;&#21512;&#25191;&#34892;&#23450;&#20041;&#33391;&#22909;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#39640;&#32423;&#20363;&#31243;&#20197;&#25552;&#39640;&#26131;&#29992;&#24615;&#12290;&#23427;&#38754;&#21521;&#38656;&#35201;&#23574;&#31471;&#26041;&#27861;&#30340;&#29992;&#25143;&#12289;&#24076;&#26395;&#21019;&#24314;&#22797;&#26434;&#25277;&#26679;&#26041;&#27861;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#20197;&#21450;&#24819;&#35201;&#20102;&#35299;&#36825;&#20123;&#26041;&#27861;&#24037;&#20316;&#21407;&#29702;&#30340;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10797v1 Announce Type: cross  Abstract: BlackJAX is a library implementing sampling and variational inference algorithms commonly used in Bayesian computation. It is designed for ease of use, speed, and modularity by taking a functional approach to the algorithms' implementation. BlackJAX is written in Python, using JAX to compile and run NumpPy-like samplers and variational methods on CPUs, GPUs, and TPUs. The library integrates well with probabilistic programming languages by working directly with the (un-normalized) target log density function. BlackJAX is intended as a collection of low-level, composable implementations of basic statistical 'atoms' that can be combined to perform well-defined Bayesian inference, but also provides high-level routines for ease of use. It is designed for users who need cutting-edge methods, researchers who want to create complex sampling methods, and people who want to learn how these work.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#21442;&#19982;&#32773;&#21327;&#20316;&#35299;&#20915;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#24182;&#33021;&#22788;&#29702;&#23376;&#32452;&#19981;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10795</link><description>&lt;p&gt;
&#22810;&#26679;&#21270;&#38598;&#25104;&#65306;&#20247;&#21253;&#26426;&#22120;&#23398;&#20064;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Diversified Ensembling: An Experiment in Crowdsourced Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10795
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#21442;&#19982;&#32773;&#21327;&#20316;&#35299;&#20915;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#24182;&#33021;&#22788;&#29702;&#23376;&#32452;&#19981;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10795v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#22312;&#35832;&#22914;Kaggle&#31561;&#31454;&#36187;&#24179;&#21488;&#19978;&#30340;&#20247;&#21253;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#24120;&#35265;&#19988;&#36890;&#24120;&#26377;&#25928;&#30340;&#29983;&#25104;&#20934;&#30830;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#24120;&#65292;&#22242;&#38431;&#31454;&#20105;&#33719;&#24471;&#26368;&#20934;&#30830;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#20445;&#30041;&#25968;&#25454;&#38598;&#19978;&#30340;&#25972;&#20307;&#35823;&#24046;&#26469;&#34913;&#37327;&#65292;&#24448;&#24448;&#22312;&#36825;&#26679;&#30340;&#27604;&#36187;&#24555;&#32467;&#26463;&#26102;&#65292;&#25490;&#34892;&#27036;&#21069;&#20960;&#21517;&#30340;&#22242;&#38431;&#20250;&#22312;&#24179;&#21488;&#26426;&#21046;&#20043;&#22806;&#38598;&#25104;&#25110;&#24179;&#22343;&#20182;&#20204;&#30340;&#27169;&#22411;&#65292;&#24471;&#21040;&#26368;&#32456;&#12289;&#26368;&#22909;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#22312;arXiv:2201.10408&#20013;&#65292;&#20316;&#32773;&#20204;&#22312;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#30340;&#20247;&#21253;&#26694;&#26550;&#65292;&#20197;&#20415;&#22312;&#23376;&#32452;&#19981;&#20844;&#24179;&#23384;&#22312;&#19988;&#21487;&#35782;&#21035;&#26102;&#23558;&#31038;&#21306;&#21453;&#39304;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#12290;&#22312;&#37027;&#37324;&#65292;&#19982;&#32463;&#20856;&#30340;&#20247;&#21253;ML&#19981;&#21516;&#65292;&#21442;&#19982;&#32773;&#20204;&#36890;&#36807;&#19987;&#27880;&#20110;&#23376;&#38382;&#39064;&#65292;&#22914;&#26381;&#21153;&#20110;&#20844;&#24179;&#24615;&#30340;&#20154;&#21475;&#23376;&#32676;&#65292;&#26377;&#24847;&#22320;&#19987;&#38376;&#21270;&#20182;&#20204;&#30340;&#21162;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#26356;&#24191;&#27867;&#30340;&#35266;&#28857;&#65306;&#25105;&#20204;&#27880;&#24847;&#21040;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#21442;&#19982;&#32773;&#21487;&#20197;&#20174;&#20107;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#35299;&#20915;&#27963;&#21160;&#65292;&#21327;&#20316;&#20197;&#32452;&#35013;&#23376;&#38382;&#39064;&#30340;&#19987;&#23478;&#36873;&#25321;&#65292;&#24182;&#20026;&#21508;&#31181;&#23376;&#38382;&#39064;&#35774;&#35745;&#29305;&#23450;&#30340;&#25439;&#22833;&#20989;&#25968;&#26159;&#22914;&#20309;&#21327;&#21161;&#35753;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#24471;&#20197;&#35299;&#20915;&#24182;&#24471;&#21040;&#35299;&#20915;&#26041;&#26696;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10795v1 Announce Type: new  Abstract: Crowdsourced machine learning on competition platforms such as Kaggle is a popular and often effective method for generating accurate models. Typically, teams vie for the most accurate model, as measured by overall error on a holdout set, and it is common towards the end of such competitions for teams at the top of the leaderboard to ensemble or average their models outside the platform mechanism to get the final, best global model. In arXiv:2201.10408, the authors developed an alternative crowdsourcing framework in the context of fair machine learning, in order to integrate community feedback into models when subgroup unfairness is present and identifiable. There, unlike in classical crowdsourced ML, participants deliberately specialize their efforts by working on subproblems, such as demographic subgroups in the service of fairness. Here, we take a broader perspective on this work: we note that within this framework, participants may b
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#19978;&#23398;&#20064;&#30340;&#31616;&#21333;&#26367;&#20195;&#26041;&#27861;&#65292;&#31216;&#20026;&#25513;&#30721;&#27880;&#24847;&#21147;&#65288;MAG&#65289;&#65292;&#20854;&#21033;&#29992;&#27880;&#24847;&#21147;&#30697;&#38453;&#26469;&#21019;&#24314;&#23450;&#21046;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#22312;&#38271;&#36317;&#31163;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#24182;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10793</link><description>&lt;p&gt;
&#25513;&#30721;&#27880;&#24847;&#21147;&#26159;&#22270;&#30340;&#20851;&#38190;
&lt;/p&gt;
&lt;p&gt;
Masked Attention is All You Need for Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10793
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#19978;&#23398;&#20064;&#30340;&#31616;&#21333;&#26367;&#20195;&#26041;&#27861;&#65292;&#31216;&#20026;&#25513;&#30721;&#27880;&#24847;&#21147;&#65288;MAG&#65289;&#65292;&#20854;&#21033;&#29992;&#27880;&#24847;&#21147;&#30697;&#38453;&#26469;&#21019;&#24314;&#23450;&#21046;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#22312;&#38271;&#36317;&#31163;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#24182;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#30340;&#21464;&#31181;&#20027;&#35201;&#29992;&#20110;&#22312;&#22270;&#19978;&#23398;&#20064;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#23427;&#20204;&#30340;&#28789;&#27963;&#24615;&#12289;&#36895;&#24230;&#21644;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#24378;&#22823;&#32780;&#36890;&#29992;&#30340;GNNs&#38656;&#35201;&#22823;&#37327;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#31934;&#24515;&#36873;&#25321;&#30340;&#25163;&#24037;&#21046;&#20316;&#30340;&#28040;&#24687;&#20256;&#36882;&#25805;&#20316;&#31526;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#19978;&#23398;&#20064;&#30340;&#38750;&#24120;&#31616;&#21333;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#27880;&#24847;&#21147;&#12290;&#22270;&#34987;&#34920;&#31034;&#20026;&#33410;&#28857;&#25110;&#36793;&#38598;&#65292;&#24182;&#36890;&#36807;&#25513;&#30721;&#27880;&#24847;&#26435;&#37325;&#30697;&#38453;&#26469;&#24378;&#21046;&#23427;&#20204;&#30340;&#36830;&#25509;&#65292;&#26377;&#25928;&#22320;&#20026;&#27599;&#20010;&#22270;&#21019;&#24314;&#23450;&#21046;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#12290;&#23613;&#31649;&#20854;&#31616;&#21333;&#24615;&#65292;&#29992;&#20110;&#22270;&#30340;&#25513;&#30721;&#27880;&#24847;&#21147;&#65288;MAG&#65289;&#22312;&#38271;&#36317;&#31163;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;55&#22810;&#20010;&#33410;&#28857;&#21644;&#22270;&#32423;&#20219;&#21153;&#19978;&#20248;&#20110;&#24378;&#28040;&#24687;&#20256;&#36882;&#22522;&#32447;&#21644;&#26356;&#22797;&#26434;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10793v1 Announce Type: cross  Abstract: Graph neural networks (GNNs) and variations of the message passing algorithm are the predominant means for learning on graphs, largely due to their flexibility, speed, and satisfactory performance. The design of powerful and general purpose GNNs, however, requires significant research efforts and often relies on handcrafted, carefully-chosen message passing operators. Motivated by this, we propose a remarkably simple alternative for learning on graphs that relies exclusively on attention. Graphs are represented as node or edge sets and their connectivity is enforced by masking the attention weight matrix, effectively creating custom attention patterns for each graph. Despite its simplicity, masked attention for graphs (MAG) has state-of-the-art performance on long-range tasks and outperforms strong message passing baselines and much more involved attention-based methods on over 55 node and graph-level tasks. We also show significantly 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#22686;&#24378;&#23545; GPT-2 &#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#38271;&#36798; 1000 &#19975;&#20010;&#20803;&#32032;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#22788;&#29702;&#26368;&#38271;&#36755;&#20837;&#30340;&#24320;&#25918;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#38271;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.10790</link><description>&lt;p&gt;
&#22312;&#19968;&#20010; 1000 &#19975;&#26681;&#33609;&#22427;&#20013;&#23547;&#25214;&#38024;&#65306;&#24490;&#29615;&#35760;&#24518;&#25214;&#21040;&#20102;&#35821;&#35328;&#27169;&#22411;&#19981;&#25797;&#38271;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10790
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#22686;&#24378;&#23545; GPT-2 &#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#38271;&#36798; 1000 &#19975;&#20010;&#20803;&#32032;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#22788;&#29702;&#26368;&#38271;&#36755;&#20837;&#30340;&#24320;&#25918;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#38271;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335; Transformer &#27169;&#22411;&#22788;&#29702;&#38271;&#25991;&#26723;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35780;&#20272;&#19981;&#21516;&#26041;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; BABILong&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22312;&#25552;&#21462;&#21644;&#22788;&#29702;&#24191;&#27867;&#25991;&#26412;&#20013;&#20998;&#24067;&#24335;&#20107;&#23454;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#21253;&#25324; GPT-4 &#21644; RAG &#30340;&#22522;&#20934;&#65292;&#32467;&#26524;&#26174;&#31034;&#24120;&#35265;&#26041;&#27861;&#20165;&#36866;&#29992;&#20110;&#26368;&#22810; $10^4$ &#20010;&#20803;&#32032;&#30340;&#24207;&#21015;&#12290;&#30456;&#21453;&#65292;&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#22686;&#24378;&#23545; GPT-2 &#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#28041;&#21450;&#26368;&#22810; $10^7$ &#20010;&#20803;&#32032;&#30340;&#20219;&#21153;&#12290;&#36825;&#19968;&#25104;&#23601;&#26631;&#24535;&#30528;&#36804;&#20170;&#20026;&#27490;&#20219;&#20309;&#24320;&#28304;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22788;&#29702;&#30340;&#26368;&#38271;&#36755;&#20837;&#65292;&#26174;&#31034;&#20102;&#23545;&#38271;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10790v1 Announce Type: cross  Abstract: This paper addresses the challenge of processing long documents using generative transformer models. To evaluate different approaches, we introduce BABILong, a new benchmark designed to assess model capabilities in extracting and processing distributed facts within extensive texts. Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to $10^4$ elements. In contrast, fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving up to $10^7$ elements. This achievement marks a substantial leap, as it is by far the longest input processed by any open neural network model to date, demonstrating a significant improvement in the processing capabilities for long sequences.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EdgeQAT&#65292;&#20351;&#29992;&#29109;&#21644;&#20998;&#24067;&#24341;&#23548;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#26469;&#20248;&#21270;&#36731;&#37327;&#32423;LLMs&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2402.10787</link><description>&lt;p&gt;
EdgeQAT: &#29109;&#21644;&#20998;&#24067;&#24341;&#23548;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65292;&#29992;&#20110;&#21152;&#36895;&#36731;&#37327;&#32423;LLMs&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EdgeQAT&#65292;&#20351;&#29992;&#29109;&#21644;&#20998;&#24067;&#24341;&#23548;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#26469;&#20248;&#21270;&#36731;&#37327;&#32423;LLMs&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#20854;&#24222;&#22823;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#37327;&#65292;LLMs&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#36890;&#24120;&#37319;&#29992;&#37327;&#21270;&#26041;&#27861;&#29983;&#25104;&#20855;&#26377;&#39640;&#25928;&#35745;&#31639;&#21644;&#24555;&#36895;&#25512;&#29702;&#30340;&#36731;&#37327;&#32423;LLMs&#12290;&#28982;&#32780;&#65292;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#26041;&#27861;&#22312;&#23558;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;KV&#32531;&#23384;&#19968;&#36215;&#37327;&#21270;&#33267;8&#20301;&#20197;&#19979;&#26102;&#65292;&#36136;&#37327;&#20250;&#24613;&#21095;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#24037;&#20316;&#23545;&#27169;&#22411;&#26435;&#37325;&#36827;&#34892;&#37327;&#21270;&#65292;&#32780;&#28608;&#27963;&#26410;&#34987;&#35302;&#21450;&#65292;&#36825;&#19981;&#33021;&#20805;&#20998;&#21457;&#25381;&#37327;&#21270;&#23545;&#36793;&#32536;&#31471;&#25512;&#29702;&#21152;&#36895;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EdgeQAT&#65292;&#21363;&#29109;&#21644;&#20998;&#24067;&#24341;&#23548;&#30340;QAT&#65292;&#29992;&#20110;&#20248;&#21270;&#36731;&#37327;&#32423;LLMs&#20197;&#23454;&#29616;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#25512;&#29702;&#21152;&#36895;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#37327;&#21270;&#24615;&#33021;&#19979;&#38477;&#20027;&#35201;&#28304;&#33258;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10787v1 Announce Type: cross  Abstract: Despite the remarkable strides of Large Language Models (LLMs) in various fields, the wide applications of LLMs on edge devices are limited due to their massive parameters and computations. To address this, quantization is commonly adopted to generate lightweight LLMs with efficient computations and fast inference. However, Post-Training Quantization (PTQ) methods dramatically degrade in quality when quantizing weights, activations, and KV cache together to below 8 bits. Besides, many Quantization-Aware Training (QAT) works quantize model weights, leaving the activations untouched, which do not fully exploit the potential of quantization for inference acceleration on the edge. In this paper, we propose EdgeQAT, the Entropy and Distribution Guided QAT for the optimization of lightweight LLMs to achieve inference acceleration on Edge devices. We first identify that the performance drop of quantization primarily stems from the information
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29616;&#20195;&#24418;&#24335;&#30340;&#38169;&#35823;&#21453;&#39304;EF21&#65292;&#23558;&#20854;&#20381;&#36182;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#20174;&#24179;&#26041;&#24179;&#22343;&#20540;&#25913;&#36827;&#20026;&#26356;&#23567;&#30340;&#31639;&#26415;&#24179;&#22343;&#20540;&#65292;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.10774</link><description>&lt;p&gt;
&#38169;&#35823;&#21453;&#39304;&#37325;&#26032;&#21152;&#36733;&#65306;&#20174;&#24179;&#26041;&#21040;&#24179;&#28369;&#24230;&#24120;&#25968;&#30340;&#31639;&#26415;&#24179;&#22343;&#20540;
&lt;/p&gt;
&lt;p&gt;
Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness Constants
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29616;&#20195;&#24418;&#24335;&#30340;&#38169;&#35823;&#21453;&#39304;EF21&#65292;&#23558;&#20854;&#20381;&#36182;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#20174;&#24179;&#26041;&#24179;&#22343;&#20540;&#25913;&#36827;&#20026;&#26356;&#23567;&#30340;&#31639;&#26415;&#24179;&#22343;&#20540;&#65292;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38169;&#35823;&#21453;&#39304;&#65288;EF&#65289;&#26159;&#19968;&#31181;&#38750;&#24120;&#27969;&#34892;&#19988;&#26497;&#20854;&#26377;&#25928;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#35299;&#20915;&#20998;&#24067;&#24335;&#35757;&#32451;&#26041;&#27861;&#65288;&#22914;&#20998;&#24067;&#24335;GD&#25110;SGD&#65289;&#20013;&#30001;&#20110;&#19982;&#36138;&#23146;&#36890;&#20449;&#21387;&#32553;&#25216;&#26415;&#65288;&#22914;TopK&#65289;&#32467;&#21512;&#32780;&#20135;&#29983;&#30340;&#25910;&#25947;&#38382;&#39064;&#12290;&#23613;&#31649;EF&#25552;&#20986;&#24050;&#26377;&#36817;&#21313;&#24180;&#26102;&#38388;&#65288;Seide&#31561;&#20154;&#65292;2014&#24180;&#65289;&#65292;&#24182;&#19988;&#23613;&#31649;&#31038;&#21306;&#20026;&#25512;&#36827;&#23545;&#35813;&#26426;&#21046;&#30340;&#29702;&#35770;&#29702;&#35299;&#32780;&#38598;&#20013;&#21162;&#21147;&#65292;&#20173;&#26377;&#24456;&#22810;&#23578;&#24453;&#25506;&#32034;&#20043;&#22788;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;EF21&#65288;Richtarik&#31561;&#20154;&#65292;2021&#24180;&#65289;&#30340;&#29616;&#20195;&#24418;&#24335;&#30340;&#38169;&#35823;&#21453;&#39304;&#65292;&#23427;&#25552;&#20379;&#20102;&#30446;&#21069;&#24050;&#30693;&#30340;&#26368;&#20339;&#29702;&#35770;&#20445;&#35777;&#65292;&#22312;&#26368;&#24369;&#30340;&#20551;&#35774;&#19979;&#20063;&#22312;&#23454;&#36341;&#20013;&#36816;&#34892;&#33391;&#22909;&#12290;&#29305;&#21035;&#22320;&#65292;&#34429;&#28982;EF21&#30340;&#29702;&#35770;&#36890;&#20449;&#22797;&#26434;&#24230;&#21462;&#20915;&#20110;&#26576;&#20123;&#24179;&#28369;&#24230;&#21442;&#25968;&#30340;&#24179;&#26041;&#22343;&#20540;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#20381;&#36182;&#24615;&#25913;&#36827;&#20026;&#23427;&#20204;&#30340;&#31639;&#26415;&#24179;&#22343;&#20540;&#65292;&#21518;&#32773;&#22987;&#32456;&#26356;&#23567;&#65292;&#23588;&#20854;&#26159;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10774v1 Announce Type: cross  Abstract: Error Feedback (EF) is a highly popular and immensely effective mechanism for fixing convergence issues which arise in distributed training methods (such as distributed GD or SGD) when these are enhanced with greedy communication compression techniques such as TopK. While EF was proposed almost a decade ago (Seide et al., 2014), and despite concentrated effort by the community to advance the theoretical understanding of this mechanism, there is still a lot to explore. In this work we study a modern form of error feedback called EF21 (Richtarik et al., 2021) which offers the currently best-known theoretical guarantees, under the weakest assumptions, and also works well in practice. In particular, while the theoretical communication complexity of EF21 depends on the quadratic mean of certain smoothness parameters, we improve this dependence to their arithmetic mean, which is always smaller, and can be substantially smaller, especially in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#22312;&#31163;&#32447;&#21160;&#21147;&#23398;&#24378;&#21270;&#23398;&#20064;&#20013;&#22914;&#20309;&#24212;&#23545;&#28304;&#29615;&#22659;&#21644;&#30446;&#26631;&#29615;&#22659;&#20043;&#38388;&#30340;&#21160;&#24577;&#24046;&#24322;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.10765</link><description>&lt;p&gt;
&#25919;&#31574;&#23398;&#20064;&#22312;&#25903;&#25345;&#19981;&#36275;&#30340;&#31163;&#32447;&#21160;&#21147;&#23398;RL&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Policy Learning for Off-Dynamics RL with Deficient Support
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10765
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#22312;&#31163;&#32447;&#21160;&#21147;&#23398;&#24378;&#21270;&#23398;&#20064;&#20013;&#22914;&#20309;&#24212;&#23545;&#28304;&#29615;&#22659;&#21644;&#30446;&#26631;&#29615;&#22659;&#20043;&#38388;&#30340;&#21160;&#24577;&#24046;&#24322;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#26377;&#25928;&#23398;&#20064;&#22797;&#26434;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#36825;&#20123;&#31574;&#30053;&#36890;&#24120;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#22823;&#37327;&#30340;&#35797;&#38169;&#20132;&#20114;&#12290;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#25104;&#26412;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#23454;&#38469;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#31574;&#30053;&#26159;&#23558;&#22312;&#20302;&#25104;&#26412;&#12289;&#24555;&#36895;&#28304;&#27169;&#25311;&#22120;&#20013;&#35757;&#32451;&#30340;&#31574;&#30053;&#36716;&#31227;&#21040;&#30495;&#23454;&#19990;&#30028;&#30340;&#30446;&#26631;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#23384;&#22312;&#25361;&#25112;&#12290;&#26080;&#35770;&#27169;&#25311;&#22120;&#22810;&#20040;&#20808;&#36827;&#65292;&#37117;&#19981;&#33021;&#23436;&#32654;&#22797;&#21046;&#30495;&#23454;&#19990;&#30028;&#30340;&#22797;&#26434;&#24615;&#65292;&#23548;&#33268;&#28304;&#29615;&#22659;&#21644;&#30446;&#26631;&#29615;&#22659;&#20043;&#38388;&#23384;&#22312;&#21160;&#24577;&#24046;&#24322;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#65292;&#28304;&#39046;&#22495;&#24517;&#39035;&#21253;&#21547;&#25152;&#26377;&#21487;&#33021;&#30340;&#30446;&#26631;&#36716;&#25442;&#65292;&#36825;&#31181;&#26465;&#20214;&#25105;&#20204;&#31216;&#20043;&#20026;&#23436;&#20840;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#26399;&#23436;&#20840;&#25903;&#25345;&#24448;&#24448;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#37325;&#22823;&#21160;&#24577;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#36716;&#21521;&#20102;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10765v1 Announce Type: cross  Abstract: Reinforcement Learning (RL) can effectively learn complex policies. However, learning these policies often demands extensive trial-and-error interactions with the environment. In many real-world scenarios, this approach is not practical due to the high costs of data collection and safety concerns. As a result, a common strategy is to transfer a policy trained in a low-cost, rapid source simulator to a real-world target environment. However, this process poses challenges. Simulators, no matter how advanced, cannot perfectly replicate the intricacies of the real world, leading to dynamics discrepancies between the source and target environments. Past research posited that the source domain must encompass all possible target transitions, a condition we term full support. However, expecting full support is often unrealistic, especially in scenarios where significant dynamics discrepancies arise. In this paper, our emphasis shifts to addres
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;RAGIC&#65292;&#24341;&#20837;&#24207;&#21015;&#29983;&#25104;&#29992;&#20110;&#32929;&#31080;&#21306;&#38388;&#39044;&#27979;&#65292;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#26410;&#26469;&#20215;&#26684;&#24207;&#21015;&#65292;&#36890;&#36807;&#39118;&#38505;&#27169;&#22359;&#21644;&#26102;&#38388;&#27169;&#22359;&#21019;&#24314;&#39118;&#38505;&#25935;&#24863;&#21306;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.10760</link><description>&lt;p&gt;
RAGIC: &#38754;&#21521;&#32929;&#31080;&#21306;&#38388;&#26500;&#24314;&#30340;&#39118;&#38505;&#24863;&#30693;&#29983;&#25104;&#23545;&#25239;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RAGIC: Risk-Aware Generative Adversarial Model for Stock Interval Construction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10760
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;RAGIC&#65292;&#24341;&#20837;&#24207;&#21015;&#29983;&#25104;&#29992;&#20110;&#32929;&#31080;&#21306;&#38388;&#39044;&#27979;&#65292;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#26410;&#26469;&#20215;&#26684;&#24207;&#21015;&#65292;&#36890;&#36807;&#39118;&#38505;&#27169;&#22359;&#21644;&#26102;&#38388;&#27169;&#22359;&#21019;&#24314;&#39118;&#38505;&#25935;&#24863;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#32929;&#24066;&#32467;&#26524;&#30340;&#21162;&#21147;&#21462;&#24471;&#20102;&#26377;&#38480;&#30340;&#25104;&#21151;&#65292;&#36825;&#26159;&#30001;&#24066;&#22330;&#22266;&#26377;&#30340;&#38543;&#26426;&#29305;&#24615;&#21463;&#35768;&#22810;&#19981;&#21487;&#39044;&#27979;&#22240;&#32032;&#24433;&#21709;&#36896;&#25104;&#30340;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#39044;&#27979;&#26041;&#27861;&#20391;&#37325;&#20110;&#21333;&#28857;&#39044;&#27979;&#65292;&#32570;&#20047;&#26377;&#25928;&#20915;&#31574;&#25152;&#38656;&#30340;&#28145;&#24230;&#65292;&#32463;&#24120;&#24573;&#35270;&#24066;&#22330;&#39118;&#38505;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;RAGIC&#65292;&#24341;&#20837;&#24207;&#21015;&#29983;&#25104;&#29992;&#20110;&#32929;&#31080;&#21306;&#38388;&#39044;&#27979;&#65292;&#26356;&#26377;&#25928;&#22320;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#29983;&#25104;&#26410;&#26469;&#20215;&#26684;&#24207;&#21015;&#65292;&#27880;&#20837;&#37329;&#34701;&#24066;&#22330;&#22266;&#26377;&#30340;&#38543;&#26426;&#24615;&#12290;RAGIC&#30340;&#29983;&#25104;&#22120;&#21253;&#25324;&#19968;&#20010;&#39118;&#38505;&#27169;&#22359;&#65292;&#25429;&#25417;&#29087;&#24713;&#25237;&#36164;&#32773;&#30340;&#39118;&#38505;&#24863;&#30693;&#65292;&#20197;&#21450;&#19968;&#20010;&#26102;&#38388;&#27169;&#22359;&#65292;&#32771;&#34385;&#21382;&#21490;&#20215;&#26684;&#36235;&#21183;&#21644;&#23395;&#33410;&#24615;&#12290;&#36825;&#20010;&#22810;&#26041;&#38754;&#30340;&#29983;&#25104;&#22120;&#36890;&#36807;&#32479;&#35745;&#25512;&#26029;&#21576;&#29616;&#39118;&#38505;&#25935;&#24863;&#21306;&#38388;&#30340;&#21019;&#24314;&#65292;&#34701;&#20837;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10760v1 Announce Type: cross  Abstract: Efforts to predict stock market outcomes have yielded limited success due to the inherently stochastic nature of the market, influenced by numerous unpredictable factors. Many existing prediction approaches focus on single-point predictions, lacking the depth needed for effective decision-making and often overlooking market risk. To bridge this gap, we propose a novel model, RAGIC, which introduces sequence generation for stock interval prediction to quantify uncertainty more effectively. Our approach leverages a Generative Adversarial Network (GAN) to produce future price sequences infused with randomness inherent in financial markets. RAGIC's generator includes a risk module, capturing the risk perception of informed investors, and a temporal module, accounting for historical price trends and seasonality. This multi-faceted generator informs the creation of risk-sensitive intervals through statistical inference, incorporating horizon
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLIPS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#21518;&#39564;&#25277;&#26679;&#23454;&#29616;&#38543;&#26426;&#23450;&#20301;&#65292;&#22635;&#34917;&#20102;&#20174;&#38750;&#26631;&#20934;&#21270;&#30446;&#26631;&#23494;&#24230;&#20013;&#25277;&#26679;&#30340;&#38382;&#39064;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.10758</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#21518;&#39564;&#25277;&#26679;&#23454;&#29616;&#38543;&#26426;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Stochastic Localization via Iterative Posterior Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLIPS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#21518;&#39564;&#25277;&#26679;&#23454;&#29616;&#38543;&#26426;&#23450;&#20301;&#65292;&#22635;&#34917;&#20102;&#20174;&#38750;&#26631;&#20934;&#21270;&#30446;&#26631;&#23494;&#24230;&#20013;&#25277;&#26679;&#30340;&#38382;&#39064;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#22312;&#22522;&#20110;&#24471;&#20998;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#65292;&#36817;&#26399;&#23545;&#38543;&#26426;&#23450;&#20301;&#25216;&#26415;&#20135;&#29983;&#20102;&#26032;&#30340;&#20852;&#36259;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#20154;&#20204;&#36890;&#36807;&#38543;&#26426;&#36807;&#31243;&#65288;&#31216;&#20026;&#35266;&#27979;&#36807;&#31243;&#65289;&#20026;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#26679;&#26412;&#24341;&#20837;&#22122;&#22768;&#65292;&#24182;&#36880;&#28176;&#23398;&#20064;&#19982;&#35813;&#21160;&#21147;&#23398;&#20851;&#32852;&#30340;&#21435;&#22122;&#22120;&#12290;&#38500;&#20102;&#29305;&#23450;&#24212;&#29992;&#20043;&#22806;&#65292;&#23545;&#20110;&#20174;&#38750;&#26631;&#20934;&#21270;&#30446;&#26631;&#23494;&#24230;&#20013;&#25277;&#26679;&#30340;&#38382;&#39064;&#65292;&#23545;&#38543;&#26426;&#23450;&#20301;&#30340;&#20351;&#29992;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#35752;&#12290;&#26412;&#39033;&#24037;&#20316;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#38543;&#26426;&#23450;&#20301;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31867;&#26126;&#30830;&#30340;&#35266;&#27979;&#36807;&#31243;&#65292;&#19982;&#28789;&#27963;&#30340;&#21435;&#22122;&#26102;&#38388;&#34920;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#23436;&#25972;&#30340;&#26041;&#27861;&#35770;&#65292;&#21363;&#8220;&#36890;&#36807;&#36845;&#20195;&#21518;&#39564;&#25277;&#26679;&#23454;&#29616;&#38543;&#26426;&#23450;&#20301;&#8221;&#65288;SLIPS&#65289;&#65292;&#20197;&#33719;&#24471;&#35813;&#21160;&#21147;&#23398;&#30340;&#36817;&#20284;&#26679;&#26412;&#65292;&#24182;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#26679;&#26412;&#26469;&#33258;&#30446;&#26631;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#26696;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10758v1 Announce Type: cross  Abstract: Building upon score-based learning, new interest in stochastic localization techniques has recently emerged. In these models, one seeks to noise a sample from the data distribution through a stochastic process, called observation process, and progressively learns a denoiser associated to this dynamics. Apart from specific applications, the use of stochastic localization for the problem of sampling from an unnormalized target density has not been explored extensively. This work contributes to fill this gap. We consider a general stochastic localization framework and introduce an explicit class of observation processes, associated with flexible denoising schedules. We provide a complete methodology, $\textit{Stochastic Localization via Iterative Posterior Sampling}$ (SLIPS), to obtain approximate samples of this dynamics, and as a by-product, samples from the target distribution. Our scheme is based on a Markov chain Monte Carlo estimati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#20844;&#24179;&#27491;&#21017;&#21270;&#30340;&#20010;&#20307;&#20844;&#24179;&#38750;&#36127;&#30697;&#38453;&#19977;&#22240;&#23376;&#20998;&#35299;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#24179;&#34913;&#21644;&#20957;&#32858;&#30340;&#31751;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#22312;&#31934;&#24230;&#21644;&#20844;&#24179;&#24230;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.10756</link><description>&lt;p&gt;
&#26397;&#21521;&#20957;&#32858;-&#20844;&#24179;-&#21644;&#35856;&#65306;&#23545;&#27604;&#27491;&#21017;&#21270;&#22312;&#20010;&#20307;&#20844;&#24179;&#22270;&#32858;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Cohesion-Fairness Harmony: Contrastive Regularization in Individual Fair Graph Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10756
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#20844;&#24179;&#27491;&#21017;&#21270;&#30340;&#20010;&#20307;&#20844;&#24179;&#38750;&#36127;&#30697;&#38453;&#19977;&#22240;&#23376;&#20998;&#35299;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#24179;&#34913;&#21644;&#20957;&#32858;&#30340;&#31751;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#22312;&#31934;&#24230;&#21644;&#20844;&#24179;&#24230;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20844;&#24179;&#22270;&#32858;&#31867;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#23427;&#20204;&#36890;&#36807;&#26045;&#21152;&#20005;&#26684;&#30340;&#32422;&#26463;&#20248;&#20808;&#32771;&#34385;&#24179;&#34913;&#30340;&#31751;&#65292;&#32780;&#29306;&#29298;&#20102;&#31751;&#30340;&#20957;&#32858;&#24615;&#65307;&#29616;&#26377;&#30340;&#20010;&#20154;&#21644;&#32676;&#20307;&#32423;&#20844;&#24179;&#26041;&#27861;&#22312;&#22270;&#20998;&#21306;&#20013;&#20027;&#35201;&#20381;&#36182;&#20110;&#29305;&#24449;&#20540;&#20998;&#35299;&#65292;&#22240;&#27492;&#36890;&#24120;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;iFairNMTF&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#23545;&#27604;&#20844;&#24179;&#27491;&#21017;&#21270;&#30340;&#20010;&#20307;&#20844;&#24179;&#38750;&#36127;&#30697;&#38453;&#19977;&#22240;&#23376;&#20998;&#35299;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#24179;&#34913;&#21644;&#20957;&#32858;&#30340;&#31751;&#12290;&#36890;&#36807;&#24341;&#20837;&#20844;&#24179;&#24615;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20801;&#35768;&#23450;&#21046;&#31934;&#24230;-&#20844;&#24179;&#24230;&#30340;&#26435;&#34913;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#29992;&#25143;&#30340;&#33258;&#20027;&#26435;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#38750;&#36127;&#30697;&#38453;&#19977;&#22240;&#23376;&#20998;&#35299;&#25552;&#20379;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;iFairNMTF&#22312;&#23454;&#29616;&#20844;&#24179;&#24615;&#21644;&#32858;&#31867;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10756v1 Announce Type: cross  Abstract: Conventional fair graph clustering methods face two primary challenges: i) They prioritize balanced clusters at the expense of cluster cohesion by imposing rigid constraints, ii) Existing methods of both individual and group-level fairness in graph partitioning mostly rely on eigen decompositions and thus, generally lack interpretability. To address these issues, we propose iFairNMTF, an individual Fairness Nonnegative Matrix Tri-Factorization model with contrastive fairness regularization that achieves balanced and cohesive clusters. By introducing fairness regularization, our model allows for customizable accuracy-fairness trade-offs, thereby enhancing user autonomy without compromising the interpretability provided by nonnegative matrix tri-factorization. Experimental evaluations on real and synthetic datasets demonstrate the superior flexibility of iFairNMTF in achieving fairness and clustering performance.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#25968;&#25454;&#27969;&#20998;&#26512;&#26694;&#26550;&#65292;&#21487;&#20197;&#20998;&#26512;&#20219;&#24847;&#20195;&#30721;&#29255;&#27573;&#65292;&#26080;&#38656;&#32534;&#35793;&#22522;&#30784;&#35774;&#26045;&#65292;&#24182;&#33258;&#21160;&#21512;&#25104;&#19979;&#28216;&#24212;&#29992;&#65292;&#26377;&#25928;&#35299;&#20915;&#25968;&#25454;&#27969;&#30456;&#20851;&#28431;&#27934;&#26816;&#27979;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.10754</link><description>&lt;p&gt;
&#24403;&#25968;&#25454;&#27969;&#20998;&#26512;&#36935;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
When Dataflow Analysis Meets Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10754
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#25968;&#25454;&#27969;&#20998;&#26512;&#26694;&#26550;&#65292;&#21487;&#20197;&#20998;&#26512;&#20219;&#24847;&#20195;&#30721;&#29255;&#27573;&#65292;&#26080;&#38656;&#32534;&#35793;&#22522;&#30784;&#35774;&#26045;&#65292;&#24182;&#33258;&#21160;&#21512;&#25104;&#19979;&#28216;&#24212;&#29992;&#65292;&#26377;&#25928;&#35299;&#20915;&#25968;&#25454;&#27969;&#30456;&#20851;&#28431;&#27934;&#26816;&#27979;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#27969;&#20998;&#26512;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#20195;&#30721;&#20998;&#26512;&#25216;&#26415;&#65292;&#21487;&#20197;&#25512;&#26029;&#31243;&#24207;&#20540;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#25903;&#25345;&#20195;&#30721;&#20248;&#21270;&#12289;&#31243;&#24207;&#29702;&#35299;&#21644;&#38169;&#35823;&#26816;&#27979;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LLMDFA&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#25968;&#25454;&#27969;&#20998;&#26512;&#26694;&#26550;&#65292;&#21487;&#20197;&#20998;&#26512;&#20219;&#24847;&#20195;&#30721;&#29255;&#27573;&#65292;&#26080;&#38656;&#32534;&#35793;&#22522;&#30784;&#35774;&#26045;&#65292;&#24182;&#33258;&#21160;&#21512;&#25104;&#19979;&#28216;&#24212;&#29992;&#12290;LLMDFA&#21463;&#22522;&#20110;&#25688;&#35201;&#30340;&#25968;&#25454;&#27969;&#20998;&#26512;&#21551;&#21457;&#65292;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#19977;&#20010;&#23376;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20960;&#31181;&#20851;&#38190;&#31574;&#30053;&#26377;&#25928;&#35299;&#20915;&#65292;&#21253;&#25324;&#23569;&#26679;&#26412;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#21644;&#24037;&#20855;&#21512;&#25104;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#35774;&#35745;&#21487;&#20197;&#20943;&#36731;&#24187;&#35273;&#24182;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#21462;&#39640;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10754v1 Announce Type: cross  Abstract: Dataflow analysis is a powerful code analysis technique that reasons dependencies between program values, offering support for code optimization, program comprehension, and bug detection. Existing approaches require the successful compilation of the subject program and customizations for downstream applications. This paper introduces LLMDFA, an LLM-powered dataflow analysis framework that analyzes arbitrary code snippets without requiring a compilation infrastructure and automatically synthesizes downstream applications. Inspired by summary-based dataflow analysis, LLMDFA decomposes the problem into three sub-problems, which are effectively resolved by several essential strategies, including few-shot chain-of-thought prompting and tool synthesis. Our evaluation has shown that the design can mitigate the hallucination and improve the reasoning ability, obtaining high precision and recall in detecting dataflow-related bugs upon benchmark
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24494;&#22411;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#20302;&#21151;&#32791;&#24494;&#25511;&#21046;&#22120;&#19978;&#23545;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#20998;&#26512;&#65292;&#20165;&#38656;6k&#20010;&#21442;&#25968;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;98.97%&#65292;&#36866;&#29992;&#20110;&#35782;&#21035;MIT-BIH&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#24211;&#20013;&#30340;5&#20010;&#26368;&#24120;&#35265;&#24515;&#24459;&#22833;&#24120;&#31867;&#21035;</title><link>https://arxiv.org/abs/2402.10748</link><description>&lt;p&gt;
&#20215;&#20540;16&#20010;&#23383;&#30340;&#22122;&#22768;&#33410;&#25293;: &#19968;&#31181;&#29992;&#20110;&#24494;&#25511;&#21046;&#22120;&#20302;&#21151;&#29575;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#30340;&#24494;&#22411;Transformer
&lt;/p&gt;
&lt;p&gt;
A Noisy Beat is Worth 16 Words: a Tiny Transformer for Low-Power Arrhythmia Classification on Microcontrollers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10748
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24494;&#22411;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#20302;&#21151;&#32791;&#24494;&#25511;&#21046;&#22120;&#19978;&#23545;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#20998;&#26512;&#65292;&#20165;&#38656;6k&#20010;&#21442;&#25968;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;98.97%&#65292;&#36866;&#29992;&#20110;&#35782;&#21035;MIT-BIH&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#24211;&#20013;&#30340;5&#20010;&#26368;&#24120;&#35265;&#24515;&#24459;&#22833;&#24120;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#30417;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#21487;&#31359;&#25140;&#31995;&#32479;&#27491;&#22312;&#25104;&#20026;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#24191;&#27867;&#24212;&#29992;&#19988;&#26377;&#20215;&#20540;&#30340;&#36164;&#20135;&#12290;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#20998;&#26512;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20449;&#21495;&#65292;&#20197;&#21450;&#26816;&#27979;&#24515;&#33039;&#29366;&#20917;&#65288;&#22914;&#24515;&#24459;&#22833;&#24120;&#65289;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;Transformer&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#26159;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#24378;&#22823;&#27169;&#22411;&#65292;&#20294;&#22312;&#21487;&#31359;&#25140;&#39046;&#22495;&#30340;&#39640;&#25928;&#23454;&#29616;&#21364;&#38754;&#20020;&#30528;&#37325;&#22823;&#30340;&#35774;&#35745;&#25361;&#25112;&#65292;&#38656;&#35201;&#22312;&#20860;&#39038;&#36275;&#22815;&#31934;&#24230;&#21644;&#36866;&#24403;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;ECG&#20449;&#21495;&#30340;&#24494;&#22411;Transformer&#27169;&#22411;&#65292;&#20165;&#38656;&#35201;6k&#20010;&#21442;&#25968;&#65292;&#22312;MIT-BIH&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#24211;&#20013;&#35782;&#21035;5&#20010;&#26368;&#24120;&#35265;&#24515;&#24459;&#22833;&#24120;&#31867;&#21035;&#26102;&#36798;&#21040;&#20102;98.97%&#30340;&#20934;&#30830;&#29575;&#65292;&#32771;&#34385;&#21040;&#23545;&#20302;&#21151;&#32791;&#24494;&#25511;&#21046;&#22120;&#35774;&#22791;&#36827;&#34892;&#39640;&#25928;&#25191;&#34892;&#25152;&#38656;&#30340;8&#20301;&#25972;&#25968;&#25512;&#29702;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10748v1 Announce Type: cross  Abstract: Wearable systems for the long-term monitoring of cardiovascular diseases are becoming widespread and valuable assets in diagnosis and therapy. A promising approach for real-time analysis of the electrocardiographic (ECG) signal and the detection of heart conditions, such as arrhythmia, is represented by the transformer machine learning model. Transformers are powerful models for the classification of time series, although efficient implementation in the wearable domain raises significant design challenges, to combine adequate accuracy and a suitable complexity. In this work, we present a tiny transformer model for the analysis of the ECG signal, requiring only 6k parameters and reaching 98.97% accuracy in the recognition of the 5 most common arrhythmia classes from the MIT-BIH Arrhythmia database, assessed considering 8-bit integer inference as required for efficient execution on low-power microcontroller-based devices. We explored an 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#21487;&#24494;&#30340;&#25289;&#26684;&#26391;&#26085;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#29289;&#29702;&#20449;&#24687;&#19982;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#22312;&#38477;&#27700;&#39044;&#25253;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#20026;&#20854;&#20182;&#25289;&#26684;&#26391;&#26085;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.10747</link><description>&lt;p&gt;
&#23436;&#20840;&#21487;&#24494;&#30340;&#25289;&#26684;&#26391;&#26085;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36830;&#32493;&#19968;&#33268;&#29289;&#29702;&#20449;&#24687;&#38477;&#27700;&#39044;&#25253;
&lt;/p&gt;
&lt;p&gt;
Fully Differentiable Lagrangian Convolutional Neural Network for Continuity-Consistent Physics-Informed Precipitation Nowcasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10747
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#21487;&#24494;&#30340;&#25289;&#26684;&#26391;&#26085;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#29289;&#29702;&#20449;&#24687;&#19982;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#22312;&#38477;&#27700;&#39044;&#25253;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#20026;&#20854;&#20182;&#25289;&#26684;&#26391;&#26085;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#38477;&#27700;&#39044;&#25253;&#65292;&#32467;&#21512;&#20102;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#21644;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#39046;&#22495;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LUPIN&#65292;&#21363;&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#25289;&#26684;&#26391;&#26085;&#21452;U-Net&#30340;&#29616;&#22312;&#39044;&#25253;&#65292;&#20511;&#37492;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#22806;&#25512;&#30340;&#39044;&#25253;&#26041;&#27861;&#65292;&#24182;&#20197;&#23436;&#20840;&#21487;&#24494;&#19988;GPU&#21152;&#36895;&#30340;&#26041;&#24335;&#23454;&#29616;&#20102;&#25968;&#25454;&#30340;&#25289;&#26684;&#26391;&#26085;&#22352;&#26631;&#31995;&#36716;&#25442;&#65292;&#20197;&#20801;&#35768;&#23454;&#26102;&#31471;&#21040;&#31471;&#35757;&#32451;&#21644;&#25512;&#26029;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;LUPIN&#19982;&#24182;&#36229;&#36807;&#20102;&#25152;&#36873;&#25321;&#22522;&#20934;&#30340;&#24615;&#33021;&#65292;&#20026;&#20854;&#20182;&#25289;&#26684;&#26391;&#26085;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25950;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10747v1 Announce Type: cross  Abstract: This paper presents a convolutional neural network model for precipitation nowcasting that combines data-driven learning with physics-informed domain knowledge. We propose LUPIN, a Lagrangian Double U-Net for Physics-Informed Nowcasting, that draws from existing extrapolation-based nowcasting methods and implements the Lagrangian coordinate system transformation of the data in a fully differentiable and GPU-accelerated manner to allow for real-time end-to-end training and inference. Based on our evaluation, LUPIN matches and exceeds the performance of the chosen benchmark, opening the door for other Lagrangian machine learning models.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#39118;&#38505;&#20998;&#35299;&#21644;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#21516;&#26469;&#28304;&#65292;&#24182;&#28548;&#28165;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.10727</link><description>&lt;p&gt;
&#36890;&#36807;&#39118;&#38505;&#20998;&#35299;&#23454;&#29616;&#20005;&#26684;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Predictive Uncertainty Quantification via Risk Decompositions for Strictly Proper Scoring Rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10727
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#39118;&#38505;&#20998;&#35299;&#21644;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#21516;&#26469;&#28304;&#65292;&#24182;&#28548;&#28165;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#39044;&#27979;&#27169;&#22411;&#24212;&#29992;&#20013;&#65292;&#21306;&#20998;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#35768;&#22810;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#20294;&#24182;&#27809;&#26377;&#20005;&#26684;&#30340;&#23450;&#20041;&#26469;&#35299;&#24320;&#23427;&#20204;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25514;&#26045;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#28982;&#26377;&#20123;&#19981;&#28165;&#26224;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26681;&#26893;&#20110;&#32479;&#35745;&#25512;&#29702;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#19981;&#20165;&#20801;&#35768;&#21019;&#24314;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#36824;&#28548;&#28165;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#32479;&#35745;&#39118;&#38505;&#26469;&#21306;&#20998;aleatoric&#21644;epistemic&#19981;&#30830;&#23450;&#24615;&#25104;&#20998;&#65292;&#24182;&#21033;&#29992;&#36866;&#24403;&#30340;&#35780;&#20998;&#35268;&#21017;&#23545;&#20854;&#36827;&#34892;&#37327;&#21270;&#12290;&#20026;&#20102;&#20351;&#20854;&#22312;&#23454;&#36341;&#20013;&#26131;&#20110;&#22788;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#36825;&#19968;&#26694;&#26550;&#20013;&#25972;&#21512;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#24819;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#25152;&#25552;&#36817;&#20284;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10727v1 Announce Type: cross  Abstract: Distinguishing sources of predictive uncertainty is of crucial importance in the application of forecasting models across various domains. Despite the presence of a great variety of proposed uncertainty measures, there are no strict definitions to disentangle them. Furthermore, the relationship between different measures of uncertainty quantification remains somewhat unclear. In this work, we introduce a general framework, rooted in statistical reasoning, which not only allows the creation of new uncertainty measures but also clarifies their interrelations. Our approach leverages statistical risk to distinguish aleatoric and epistemic uncertainty components and utilizes proper scoring rules to quantify them. To make it practically tractable, we propose an idea to incorporate Bayesian reasoning into this framework and discuss the properties of the proposed approximation.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#39134;&#26426;&#26426;&#36523;&#21160;&#24577;&#27700;&#38477;&#36733;&#33655;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#25110;Koopman&#31639;&#23376;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10724</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27700;&#38477;&#36733;&#33655;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Machine Learning based Prediction of Ditching Loads
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10724
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#39134;&#26426;&#26426;&#36523;&#21160;&#24577;&#27700;&#38477;&#36733;&#33655;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#25110;Koopman&#31639;&#23376;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#39134;&#26426;&#26426;&#36523;&#21160;&#24577;&#27700;&#38477;&#36733;&#33655;&#30340;&#26041;&#27861;&#12290;&#25152;&#37319;&#29992;&#30340;&#23398;&#20064;&#36807;&#31243;&#20998;&#20026;&#20004;&#37096;&#20998;&#65292;&#21363;&#20351;&#29992;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#65288;CAE&#65289;&#37325;&#26500;&#31354;&#38388;&#36733;&#33655;&#65292;&#20197;&#21450;&#22312;&#38543;&#21518;&#30340;&#37096;&#20998;&#20013;&#36825;&#20123;&#36733;&#33655;&#30340;&#30636;&#26102;&#28436;&#21270;&#12290;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;CAE&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#25110;&#22522;&#20110;Koopman&#31639;&#23376;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#39044;&#27979;&#30636;&#26102;&#34892;&#20026;&#12290;&#35757;&#32451;&#25968;&#25454;&#26159;&#36890;&#36807;von-Karman&#21644;Wagner&#30340;&#21160;&#37327;&#26041;&#27861;&#30340;&#25193;&#23637;&#32534;&#21046;&#30340;&#65292;&#35757;&#32451;&#26041;&#27861;&#30340;&#22522;&#26412;&#21407;&#29702;&#34987;&#31616;&#35201;&#24635;&#32467;&#12290;&#25152;&#28041;&#21450;&#30340;&#24212;&#29992;&#26159;&#25351;DLR-D150&#39134;&#26426;&#30340;&#20840;&#23610;&#23544;&#26426;&#36523;&#65292;&#21253;&#25324;&#19968;&#31995;&#21015;&#27700;&#24179;&#21644;&#22402;&#30452;&#36827;&#22330;&#36895;&#24230;&#65292;&#20837;&#23556;&#35282;&#20026;6&#176;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#25152;&#26377;&#22235;&#20010;&#30740;&#31350;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#37319;&#29992;LSTM&#32467;&#21512;...&#65288;&#20869;&#23481;&#32570;&#22833;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10724v1 Announce Type: new  Abstract: We present approaches to predict dynamic ditching loads on aircraft fuselages using machine learning. The employed learning procedure is structured into two parts, the reconstruction of the spatial loads using a convolutional autoencoder (CAE) and the transient evolution of these loads in a subsequent part. Different CAE strategies are assessed and combined with either long short-term memory (LSTM) networks or Koopman-operator based methods to predict the transient behaviour. The training data is compiled by an extension of the momentum method of von-Karman and Wagner and the rationale of the training approach is briefly summarised. The application included refers to a full-scale fuselage of a DLR-D150 aircraft for a range of horizontal and vertical approach velocities at 6{\deg} incidence. Results indicate a satisfactory level of predictive agreement for all four investigated surrogate models examined, with the combination of an LSTM an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19968;&#33268;&#39044;&#27979;&#26041;&#27861;&#26469;&#23398;&#20064;&#21487;&#20449;&#38598;&#21512;&#39044;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#34920;&#31034;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10723</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#21487;&#20449;&#38598;&#21512;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Conformalized Credal Set Predictors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19968;&#33268;&#39044;&#27979;&#26041;&#27861;&#26469;&#23398;&#20064;&#21487;&#20449;&#38598;&#21512;&#39044;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#34920;&#31034;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#38598;&#21512;&#26159;&#34987;&#35270;&#20026;&#19981;&#30830;&#23450;&#24050;&#30693;&#30495;&#23454;&#20998;&#24067;&#30340;&#20505;&#36873;&#27010;&#29575;&#20998;&#24067;&#38598;&#21512;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#30001;&#20110;&#21487;&#20449;&#38598;&#21512;&#33021;&#22815;&#34920;&#31034;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23588;&#20854;&#26159;&#33021;&#22815;&#34920;&#31034;&#39044;&#27979;&#30340;aleatoric&#21644;epistemic&#19981;&#30830;&#23450;&#24615;&#65292;&#22240;&#27492;&#36817;&#26469;&#24050;&#32463;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#29992;&#20110;&#23398;&#20064;&#21487;&#20449;&#38598;&#21512;&#39044;&#27979;&#22120;&#30340;&#26041;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#33268;&#39044;&#27979;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#39044;&#27979;&#21487;&#20449;&#38598;&#21512;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26631;&#35760;&#20026;&#27010;&#29575;&#20998;&#24067;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#32487;&#25215;&#20102;&#19968;&#33268;&#39044;&#27979;&#30340;&#35206;&#30422;&#24615;&#20445;&#35777;&#65292;&#25105;&#20204;&#30340;&#19968;&#33268;&#21487;&#20449;&#38598;&#21512;&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#20445;&#35777;&#26159;&#26377;&#25928;&#30340;&#65288;&#32780;&#26080;&#38656;&#23545;&#27169;&#22411;&#25110;&#20998;&#24067;&#20570;&#20986;&#20219;&#20309;&#20551;&#35774;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10723v1 Announce Type: cross  Abstract: Credal sets are sets of probability distributions that are considered as candidates for an imprecisely known ground-truth distribution. In machine learning, they have recently attracted attention as an appealing formalism for uncertainty representation, in particular due to their ability to represent both the aleatoric and epistemic uncertainty in a prediction. However, the design of methods for learning credal set predictors remains a challenging problem. In this paper, we make use of conformal prediction for this purpose. More specifically, we propose a method for predicting credal sets in the classification task, given training data labeled by probability distributions. Since our method inherits the coverage guarantees of conformal prediction, our conformal credal sets are guaranteed to be valid with high probability (without any assumptions on model or distribution). We demonstrate the applicability of our method to natural languag
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;GNN&#20013;&#36793;&#35299;&#38500;&#36807;&#31243;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#36807;&#24230;&#36951;&#24536;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#26469;&#35299;&#20915;&#25439;&#22833;&#20989;&#25968;&#24341;&#36215;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10695</link><description>&lt;p&gt;
&#19982;&#36951;&#24536;&#21628;&#24212;&#30340;&#35299;&#38500;&#38142;&#25509;&#65306;&#31616;&#21270;GNN&#20013;&#30340;&#36793;&#35299;&#38500;
&lt;/p&gt;
&lt;p&gt;
Unlink to Unlearn: Simplifying Edge Unlearning in GNNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10695
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;GNN&#20013;&#36793;&#35299;&#38500;&#36807;&#31243;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#36807;&#24230;&#36951;&#24536;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#26469;&#35299;&#20915;&#25439;&#22833;&#20989;&#25968;&#24341;&#36215;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#25285;&#24551;&#21152;&#21095;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#30340;&#35299;&#38500;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#23398;&#26415;&#30028;&#19968;&#20010;&#31361;&#20986;&#30340;&#30740;&#31350;&#21069;&#27839;&#12290;&#36825;&#19968;&#27010;&#24565;&#22312;&#24378;&#35843;&#34987;&#36951;&#24536;&#26435;&#21033;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21253;&#25324;&#22312;&#29992;&#25143;&#35831;&#27714;&#26102;&#26377;&#36873;&#25321;&#24615;&#22320;&#20174;&#24050;&#35757;&#32451;&#30340;GNN&#20013;&#21024;&#38500;&#29305;&#23450;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20851;&#27880;&#36793;&#30340;&#35299;&#38500;&#23398;&#20064;&#65292;&#36825;&#19968;&#36807;&#31243;&#23545;&#29616;&#23454;&#24212;&#29992;&#29305;&#21035;&#30456;&#20851;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#22914;GNNDelete&#21487;&#20197;&#28040;&#38500;&#29305;&#23450;&#36793;&#30340;&#24433;&#21709;&#65292;&#28982;&#32780;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#23616;&#38480;&#65292;&#31216;&#20026;&#36807;&#24230;&#36951;&#24536;&#12290;&#24403;&#35299;&#38500;&#23398;&#20064;&#36807;&#31243;&#26080;&#24847;&#20013;&#38500;&#21435;&#36229;&#20986;&#29305;&#23450;&#25968;&#25454;&#30340;&#36807;&#22810;&#20449;&#24687;&#26102;&#65292;&#20250;&#23548;&#33268;&#23545;&#21097;&#20313;&#36793;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;GNNDelete&#30340;&#25439;&#22833;&#20989;&#25968;&#20316;&#20026;&#36807;&#24230;&#36951;&#24536;&#29616;&#35937;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10695v1 Announce Type: cross  Abstract: As concerns over data privacy intensify, unlearning in Graph Neural Networks (GNNs) has emerged as a prominent research frontier in academia. This concept is pivotal in enforcing the right to be forgotten, which entails the selective removal of specific data from trained GNNs upon user request. Our research focuses on edge unlearning, a process of particular relevance to real-world applications, owing to its widespread applicability. Current state-of-the-art approaches like GNNDelete can eliminate the influence of specific edges, yet our research has revealed a critical limitation in these approaches, termed over-forgetting. It occurs when the unlearning process inadvertently removes excessive information beyond specific data, leading to a significant decline in prediction accuracy for the remaining edges. To address this issue, we have identified the loss functions of GNNDelete as the primary source of the over-forgetting phenomenon. 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#23558;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#25351;&#26631;&#20174;&#22270;&#20687;&#29983;&#25104;&#36716;&#21270;&#20026;&#25991;&#26412;&#29983;&#25104;&#65292;&#32454;&#33268;&#35780;&#20272;&#20102;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;LLMs&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#24615;&#33021;&#34920;&#29616;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.10693</link><description>&lt;p&gt;
&#25506;&#32034;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#20197;&#35780;&#20272;LLMs&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring Precision and Recall to assess the quality and diversity of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10693
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#23558;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#25351;&#26631;&#20174;&#22270;&#20687;&#29983;&#25104;&#36716;&#21270;&#20026;&#25991;&#26412;&#29983;&#25104;&#65292;&#32454;&#33268;&#35780;&#20272;&#20102;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;LLMs&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#24615;&#33021;&#34920;&#29616;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;Llama-2&#21644;Mistral&#30340;&#26032;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#37325;&#28857;&#26159;&#23558;&#22270;&#20687;&#29983;&#25104;&#30340;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#25351;&#26631;&#36716;&#21270;&#20026;&#25991;&#26412;&#29983;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#23545;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#36827;&#34892;&#32454;&#33268;&#35780;&#20272;&#65292;&#32780;&#26080;&#38656;&#23545;&#40784;&#30340;&#35821;&#26009;&#24211;&#12290;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#30740;&#31350;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#24320;&#25918;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#36825;&#26159;&#20256;&#32479;&#22522;&#20934;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#30340;&#12290;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#22312;&#27169;&#22411;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#29983;&#25104;&#26679;&#26412;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#22522;&#20110;&#20998;&#24067;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35780;&#20272;&#24037;&#20855;&#21253;&#65292;&#20026;&#24403;&#21069;LLMs&#22312;&#29983;&#25104;&#22810;&#26679;&#24615;&#21644;&#39640;&#36136;&#37327;&#25991;&#26412;&#26041;&#38754;&#38754;&#20020;&#30340;&#23454;&#38469;&#33021;&#21147;&#21644;&#25361;&#25112;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10693v1 Announce Type: new  Abstract: This paper introduces a novel evaluation framework for Large Language Models (LLMs) such as Llama-2 and Mistral, focusing on the adaptation of Precision and Recall metrics from image generation to text generation. This approach allows for a nuanced assessment of the quality and diversity of generated text without the need for aligned corpora. By conducting a comprehensive evaluation of state-of-the-art language models, the study reveals significant insights into their performance on open-ended generation tasks, which are not adequately captured by traditional benchmarks. The findings highlight a trade-off between the quality and diversity of generated samples, particularly when models are fine-tuned with human feedback. This work extends the toolkit for distribution-based NLP evaluation, offering insights into the practical capabilities and challenges faced by current LLMs in generating diverse and high-quality text.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20449;&#24687;&#35770;&#26694;&#26550;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20284;&#28982;&#27604;&#25915;&#20987;&#23545;&#19981;&#30830;&#23450;&#24615;&#12289;&#26657;&#20934;&#27700;&#24179;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#20102;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#20013;&#38544;&#21547;&#30340;&#39118;&#38505;</title><link>https://arxiv.org/abs/2402.10686</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#12289;&#26657;&#20934;&#21644;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65306;&#20449;&#24687;&#35770;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Uncertainty, Calibration, and Membership Inference Attacks: An Information-Theoretic Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10686
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#35770;&#26694;&#26550;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20284;&#28982;&#27604;&#25915;&#20987;&#23545;&#19981;&#30830;&#23450;&#24615;&#12289;&#26657;&#20934;&#27700;&#24179;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#20102;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#20013;&#38544;&#21547;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIA&#65289;&#20013;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#20856;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#20986;&#30340;&#36807;&#24230;&#33258;&#20449;&#26469;&#30830;&#23450;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#30446;&#26631;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#20869;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20284;&#28982;&#27604;&#25915;&#20987;&#65288;LiRA&#65289;&#30340;&#24615;&#33021;&#65292;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#20801;&#35768;&#30740;&#31350;&#30495;&#23454;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#30001;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#38598;&#24341;&#36215;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#30446;&#26631;&#27169;&#22411;&#30340;&#26657;&#20934;&#27700;&#24179;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#20174;&#30446;&#26631;&#27169;&#22411;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#36880;&#28176;&#20943;&#23569;&#65306;&#32622;&#20449;&#21521;&#37327;&#65288;CV&#65289;&#25259;&#38706;&#65292;&#20854;&#20013;&#36755;&#20986;&#27010;&#29575;&#21521;&#37327;&#34987;&#21457;&#24067;&#65307;&#30495;&#23454;&#26631;&#31614;&#32622;&#20449;&#24230;&#65288;TLC&#65289;&#25259;&#38706;&#65292;&#20854;&#20013;&#21482;&#26377;&#27169;&#22411;&#20998;&#37197;&#32473;&#30495;&#23454;&#26631;&#31614;&#30340;&#27010;&#29575;&#26159;&#21487;&#29992;&#30340;&#65307;&#20197;&#21450;&#20915;&#31574;&#38598;&#65288;DS&#65289;&#25259;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10686v1 Announce Type: cross  Abstract: In a membership inference attack (MIA), an attacker exploits the overconfidence exhibited by typical machine learning models to determine whether a specific data point was used to train a target model. In this paper, we analyze the performance of the state-of-the-art likelihood ratio attack (LiRA) within an information-theoretical framework that allows the investigation of the impact of the aleatoric uncertainty in the true data generation process, of the epistemic uncertainty caused by a limited training data set, and of the calibration level of the target model. We compare three different settings, in which the attacker receives decreasingly informative feedback from the target model: confidence vector (CV) disclosure, in which the output probability vector is released; true label confidence (TLC) disclosure, in which only the probability assigned to the true label is made available by the model; and decision set (DS) disclosure, in 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29289;&#29702;&#30456;&#20851;&#30340;MeshGraphNets&#65288;PI-MGNs&#65289;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#32593;&#26684;&#19978;&#36827;&#34892;&#38750;&#23450;&#24577;&#21644;&#38750;&#32447;&#24615;&#20223;&#30495;&#65292;&#21033;&#29992;PINNs&#26469;&#20943;&#23569;&#23545;&#22823;&#37327;&#26114;&#36149;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;</title><link>https://arxiv.org/abs/2402.10681</link><description>&lt;p&gt;
&#29289;&#29702;&#30456;&#20851;&#30340;MeshGraphNets&#65288;PI-MGNs&#65289;&#65306;&#36866;&#29992;&#20110;&#20219;&#24847;&#32593;&#26684;&#19978;&#38750;&#23450;&#24577;&#21644;&#38750;&#32447;&#24615;&#20223;&#30495;&#30340;&#31070;&#32463;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
Physics-informed MeshGraphNets (PI-MGNs): Neural finite element solvers for non-stationary and nonlinear simulations on arbitrary meshes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10681
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29289;&#29702;&#30456;&#20851;&#30340;MeshGraphNets&#65288;PI-MGNs&#65289;&#65292;&#21487;&#20197;&#22312;&#20219;&#24847;&#32593;&#26684;&#19978;&#36827;&#34892;&#38750;&#23450;&#24577;&#21644;&#38750;&#32447;&#24615;&#20223;&#30495;&#65292;&#21033;&#29992;PINNs&#26469;&#20943;&#23569;&#23545;&#22823;&#37327;&#26114;&#36149;&#35757;&#32451;&#25968;&#25454;&#30340;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#31243;&#32452;&#20214;&#24517;&#39035;&#28385;&#36275;&#26085;&#30410;&#22686;&#38271;&#30340;&#25216;&#26415;&#38656;&#27714;&#65292;&#32780;&#19988;&#24320;&#21457;&#21608;&#26399;&#21464;&#24471;&#36234;&#26469;&#36234;&#30701;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#38656;&#35201;&#19968;&#31181;&#25972;&#20307;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#24320;&#21457;&#38646;&#20214;&#35774;&#35745;&#12289;&#26448;&#26009;&#31995;&#32479;&#21644;&#21046;&#36896;&#24037;&#33402;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#25968;&#20540;&#20223;&#30495;&#65292;&#28982;&#32780;&#23545;&#20110;&#36845;&#20195;&#20248;&#21270;&#32780;&#35328;&#24456;&#24555;&#21464;&#24471;&#35745;&#31639;&#23494;&#38598;&#12290;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21487;&#29992;&#20110;&#21462;&#20195;&#32791;&#26102;&#21644;&#36164;&#28304;&#23494;&#38598;&#30340;&#25968;&#20540;&#20223;&#30495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MeshGraphNets&#65288;MGNs&#65289;&#26174;&#31034;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#23427;&#20204;&#21487;&#20197;&#22312;&#26410;&#30693;&#32593;&#26684;&#20960;&#20309;&#19978;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#23545;&#20248;&#21270;&#26159;&#23436;&#20840;&#21487;&#24494;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20381;&#36182;&#20110;&#22823;&#37327;&#26114;&#36149;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20363;&#22914;&#25968;&#20540;&#20223;&#30495;&#12290;&#29289;&#29702;&#30456;&#20851;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26426;&#20250;&#65292;&#20351;&#29992;&#20559;&#24494;&#20998;&#26041;&#31243;&#32780;&#19981;&#26159;&#26631;&#35760;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10681v1 Announce Type: cross  Abstract: Engineering components must meet increasing technological demands in ever shorter development cycles. To face these challenges, a holistic approach is essential that allows for the concurrent development of part design, material system and manufacturing process. Current approaches employ numerical simulations, which however quickly becomes computation-intensive, especially for iterative optimization. Data-driven machine learning methods can be used to replace time- and resource-intensive numerical simulations. In particular, MeshGraphNets (MGNs) have shown promising results. They enable fast and accurate predictions on unseen mesh geometries while being fully differentiable for optimization. However, these models rely on large amounts of expensive training data, such as numerical simulations. Physics-informed neural networks (PINNs) offer an opportunity to train neural networks with partial differential equations instead of labeled dat
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#23884;&#22871;&#30697;&#38453;&#24352;&#37327;&#27169;&#22411;&#20013;&#22810;&#35270;&#22270;&#32858;&#31867;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#37327;&#21270;&#20102;&#24352;&#37327;&#26041;&#27861;&#21644;&#30697;&#38453;&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#21457;&#29616;&#20102;&#23637;&#24320;&#26041;&#27861;&#30340;&#31639;&#27861;&#38408;&#20540;&#65292;&#23637;&#31034;&#20102;&#31867;&#20284;BBP&#36807;&#28193;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.10677</link><description>&lt;p&gt;
&#22810;&#35270;&#22270;&#32858;&#31867;&#22312;&#23884;&#22871;&#30697;&#38453;&#24352;&#37327;&#27169;&#22411;&#19979;&#30340;&#24615;&#33021;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Performance Gaps in Multi-view Clustering under the Nested Matrix-Tensor Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10677
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#23884;&#22871;&#30697;&#38453;&#24352;&#37327;&#27169;&#22411;&#20013;&#22810;&#35270;&#22270;&#32858;&#31867;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#37327;&#21270;&#20102;&#24352;&#37327;&#26041;&#27861;&#21644;&#30697;&#38453;&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#21457;&#29616;&#20102;&#23637;&#24320;&#26041;&#27861;&#30340;&#31639;&#27861;&#38408;&#20540;&#65292;&#23637;&#31034;&#20102;&#31867;&#20284;BBP&#36807;&#28193;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#36817;&#24341;&#20837;&#30340;&#23884;&#22871;&#30697;&#38453;&#24352;&#37327;&#27169;&#22411;&#20013;&#38544;&#34255;&#30340;&#26893;&#20837;&#20449;&#21495;&#30340;&#20272;&#35745;&#65292;&#35813;&#27169;&#22411;&#26159;&#32463;&#20856;&#23574;&#23792;&#31209;&#19968;&#24352;&#37327;&#27169;&#22411;&#30340;&#25193;&#23637;&#65292;&#21463;&#22810;&#35270;&#22270;&#32858;&#31867;&#30340;&#21551;&#21457;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#22522;&#20110;&#24352;&#37327;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#25214;&#21040;&#26368;&#20339;&#31209;&#19968;&#36924;&#36817;&#65292;&#36825;&#26159;&#19968;&#20010;&#35745;&#31639;&#38590;&#39064;&#12290;&#19968;&#20010;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#27861;&#26159;&#35745;&#31639;&#35266;&#27979;&#21040;&#30340;&#24352;&#37327;&#25968;&#25454;&#23637;&#24320;&#30340;&#26368;&#20339;&#31209;&#19968;&#65288;&#30697;&#38453;&#65289;&#36924;&#36817;&#65292;&#20294;&#20854;&#24615;&#33021;&#36804;&#20170;&#20026;&#27490;&#26410;&#30693;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#37327;&#21270;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#25512;&#23548;&#20986;&#23637;&#24320;&#26041;&#27861;&#30340;&#31934;&#30830;&#31639;&#27861;&#38408;&#20540;&#65292;&#24182;&#23637;&#31034;&#23427;&#23637;&#29616;&#20986;&#31867;&#20284;BBP&#36807;&#28193;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#36825;&#39033;&#24037;&#20316;&#19982;&#26368;&#36817;&#30340;&#36129;&#29486;&#19968;&#33268;&#65292;&#36825;&#20123;&#36129;&#29486;&#21152;&#28145;&#20102;&#25105;&#20204;&#23545;&#20026;&#20160;&#20040;&#24352;&#37327;&#26041;&#27861;&#20248;&#20110;&#30697;&#38453;&#26041;&#27861;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10677v1 Announce Type: cross  Abstract: We study the estimation of a planted signal hidden in a recently introduced nested matrix-tensor model, which is an extension of the classical spiked rank-one tensor model, motivated by multi-view clustering. Prior work has theoretically examined the performance of a tensor-based approach, which relies on finding a best rank-one approximation, a problem known to be computationally hard. A tractable alternative approach consists in computing instead the best rank-one (matrix) approximation of an unfolding of the observed tensor data, but its performance was hitherto unknown. We quantify here the performance gap between these two approaches, in particular by deriving the precise algorithmic threshold of the unfolding approach and demonstrating that it exhibits a BBP-type transition behavior. This work is therefore in line with recent contributions which deepen our understanding of why tensor-based methods surpass matrix-based methods in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#35821;&#20041;&#20998;&#21106;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#22270;&#20687;&#32423;&#32622;&#20449;&#24230;&#27979;&#37327;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2402.10665</link><description>&lt;p&gt;
&#20351;&#29992;&#20107;&#21518;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#24615;&#33021;&#21450;&#20854;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Selective Prediction for Semantic Segmentation using Post-Hoc Confidence Estimation and Its Performance under Distribution Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#35821;&#20041;&#20998;&#21106;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#22270;&#20687;&#32423;&#32622;&#20449;&#24230;&#27979;&#37327;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#28982;&#32780;&#20854;&#26377;&#25928;&#24615;&#24120;&#24120;&#21463;&#21040;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#32570;&#20047;&#25152;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#19968;&#20010;&#24120;&#35265;&#31574;&#30053;&#26159;&#21033;&#29992;&#22312;&#19981;&#21516;&#31181;&#32676;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22914;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#22312;&#20852;&#36259;&#31181;&#32676;&#19978;&#34920;&#29616;&#20986;&#38477;&#20302;&#30340;&#24615;&#33021;&#12290;&#22312;&#27169;&#22411;&#38169;&#35823;&#21487;&#33021;&#24102;&#26469;&#37325;&#22823;&#21518;&#26524;&#30340;&#24773;&#20917;&#19979;&#65292;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#20943;&#36731;&#39118;&#38505;&#12289;&#20943;&#23569;&#23545;&#19987;&#23478;&#30417;&#30563;&#20381;&#36182;&#30340;&#25163;&#27573;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#21294;&#20047;&#29615;&#22659;&#19979;&#35821;&#20041;&#20998;&#21106;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#65292;&#30528;&#37325;&#20110;&#24212;&#29992;&#20110;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#36816;&#34892;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20107;&#21518;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#22270;&#20687;&#32423;&#32622;&#20449;&#24230;&#27979;&#37327;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10665v1 Announce Type: new  Abstract: Semantic segmentation plays a crucial role in various computer vision applications, yet its efficacy is often hindered by the lack of high-quality labeled data. To address this challenge, a common strategy is to leverage models trained on data from different populations, such as publicly available datasets. This approach, however, leads to the distribution shift problem, presenting a reduced performance on the population of interest. In scenarios where model errors can have significant consequences, selective prediction methods offer a means to mitigate risks and reduce reliance on expert supervision. This paper investigates selective prediction for semantic segmentation in low-resource settings, thus focusing on post-hoc confidence estimators applied to pre-trained models operating under distribution shift. We propose a novel image-level confidence measure tailored for semantic segmentation and demonstrate its effectiveness through expe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#32447;&#24615;&#21464;&#25442;&#22120;&#19982;&#21463;&#25351;&#25968;&#20989;&#25968;&#30340;Taylor&#23637;&#24320;&#21551;&#21457;&#30340;&#26680;&#20989;&#25968;&#21644;&#21367;&#31215;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#21270;&#20854;In-Context Learning&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10644</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#23398;&#20064;&#26680;&#20989;&#25968;&#30340;&#32447;&#24615;&#21464;&#25442;&#22120;&#22312;&#19978;&#19979;&#25991;&#27169;&#22411;&#20013;&#34920;&#29616;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Linear Transformers with Learnable Kernel Functions are Better In-Context Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#32447;&#24615;&#21464;&#25442;&#22120;&#19982;&#21463;&#25351;&#25968;&#20989;&#25968;&#30340;Taylor&#23637;&#24320;&#21551;&#21457;&#30340;&#26680;&#20989;&#25968;&#21644;&#21367;&#31215;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#21270;&#20854;In-Context Learning&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#27425;&#20108;&#27425;&#20307;&#31995;&#32467;&#26500;&#30340;&#21069;&#27839;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#19981;&#26029;&#21457;&#23637;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;&#30340;&#26680;&#20989;&#25968;&#65292;&#22686;&#24378;&#20102;&#20854;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#22312;Multi-Query Associative Recall&#20219;&#21153;&#21644;&#25972;&#20307;&#35821;&#35328;&#24314;&#27169;&#36807;&#31243;&#20013;&#24471;&#21040;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10644v1 Announce Type: new  Abstract: Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities - a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demon
&lt;/p&gt;</description></item><item><title>&#35843;&#26597;&#20102;&#27169;&#22411;&#38477;&#38454;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#39044;&#27979;&#20914;&#20987;&#23556;&#27969;&#23545;&#20985;&#20984;&#38754;&#20256;&#28909;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;FFT-ANN&#21644;POD-LSTM&#20004;&#31181;&#26041;&#27861;&#26469;&#39044;&#27979;&#24179;&#22343;&#21162;&#22622;&#25968;&#20197;&#21450;&#38543;&#26426;&#39057;&#29575;&#20914;&#20987;&#24773;&#20917;&#19979;&#30340;&#23616;&#37096;&#20256;&#28909;&#36895;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.10641</link><description>&lt;p&gt;
&#38024;&#23545;&#20985;&#20984;&#38754;&#19978;&#23556;&#27969;&#20256;&#28909;&#30340;&#39044;&#27979;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Predictive Surrogate Model for Heat Transfer of an Impinging Jet on a Concave Surface
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10641
&lt;/p&gt;
&lt;p&gt;
&#35843;&#26597;&#20102;&#27169;&#22411;&#38477;&#38454;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#39044;&#27979;&#20914;&#20987;&#23556;&#27969;&#23545;&#20985;&#20984;&#38754;&#20256;&#28909;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;FFT-ANN&#21644;POD-LSTM&#20004;&#31181;&#26041;&#27861;&#26469;&#39044;&#27979;&#24179;&#22343;&#21162;&#22622;&#25968;&#20197;&#21450;&#38543;&#26426;&#39057;&#29575;&#20914;&#20987;&#24773;&#20917;&#19979;&#30340;&#23616;&#37096;&#20256;&#28909;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#35843;&#26597;&#21508;&#31181;&#27169;&#22411;&#38477;&#38454;&#65288;MOR&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#39044;&#27979;&#20914;&#20987;&#23556;&#27969;&#23545;&#20985;&#20984;&#38754;&#30340;&#20256;&#28909;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25193;&#23637;&#20102;&#20808;&#21069;&#28041;&#21450;&#33033;&#20914;&#22278;&#24418;&#23556;&#27969;&#30340;&#23454;&#39564;&#21644;&#25968;&#20540;&#30740;&#31350;&#65292;&#26412;&#30740;&#31350;&#23558;&#35780;&#20272;&#29992;&#20110;&#19981;&#21516;&#23556;&#27969;&#29305;&#24615;&#19979;&#30340;&#20256;&#28909;&#30340;&#39044;&#27979;&#20195;&#29702;&#27169;&#22411;&#65288;PSM&#65289;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#39044;&#27979;&#26041;&#27861;&#65292;&#19968;&#31181;&#37319;&#29992;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#22686;&#24378;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;FFT-ANN&#65289;&#26469;&#39044;&#27979;&#22312;&#24658;&#23450;&#39057;&#29575;&#24773;&#20917;&#19979;&#30340;&#24179;&#22343;&#21162;&#22622;&#25968;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#24341;&#20837;&#20102;Proper Orthogonal Decomposition&#21644;Long Short-Term Memory&#65288;POD-LSTM&#65289;&#26041;&#27861;&#26469;&#22788;&#29702;&#38543;&#26426;&#39057;&#29575;&#30340;&#20914;&#20987;&#23556;&#27969;&#12290;POD-LSTM&#26041;&#27861;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#22312;&#38543;&#26426;&#39057;&#29575;&#20914;&#20987;&#24773;&#20917;&#19979;&#39044;&#27979;&#23616;&#37096;&#20256;&#28909;&#36895;&#29575;&#30340;&#24378;&#22823;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10641v1 Announce Type: cross  Abstract: This paper aims to comprehensively investigate the efficacy of various Model Order Reduction (MOR) and deep learning techniques in predicting heat transfer in a pulsed jet impinging on a concave surface. Expanding on the previous experimental and numerical research involving pulsed circular jets, this investigation extends to evaluate Predictive Surrogate Models (PSM) for heat transfer across various jet characteristics. To this end, this work introduces two predictive approaches, one employing a Fast Fourier Transformation augmented Artificial Neural Network (FFT-ANN) for predicting the average Nusselt number under constant-frequency scenarios. Moreover, the investigation introduces the Proper Orthogonal Decomposition and Long Short-Term Memory (POD-LSTM) approach for random-frequency impingement jets. The POD-LSTM method proves to be a robust solution for predicting the local heat transfer rate under random-frequency impingement scen
&lt;/p&gt;</description></item><item><title>&#23558;vanilla Transformer&#30340;&#20851;&#31995;&#24314;&#27169;&#25193;&#23637;&#21040;&#36830;&#32493;&#26102;&#38388;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;ContiFormer&#12290;</title><link>https://arxiv.org/abs/2402.10635</link><description>&lt;p&gt;
ContiFormer&#65306;&#29992;&#20110;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#30340;&#36830;&#32493;&#26102;&#38388;Transformer
&lt;/p&gt;
&lt;p&gt;
ContiFormer: Continuous-Time Transformer for Irregular Time Series Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10635
&lt;/p&gt;
&lt;p&gt;
&#23558;vanilla Transformer&#30340;&#20851;&#31995;&#24314;&#27169;&#25193;&#23637;&#21040;&#36830;&#32493;&#26102;&#38388;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;ContiFormer&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#19978;&#24314;&#27169;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#23545;&#20110;&#35299;&#37322;&#36830;&#32493;&#21457;&#29983;&#30340;&#25968;&#25454;&#28436;&#21464;&#21644;&#30456;&#20851;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290; &#20256;&#32479;&#26041;&#27861;&#21253;&#25324;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25110;Transformer&#27169;&#22411;&#36890;&#36807;&#24378;&#22823;&#30340;&#31070;&#32463;&#26550;&#26500;&#21033;&#29992;&#24402;&#32435;&#20559;&#24046;&#26469;&#25429;&#33719;&#22797;&#26434;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#31163;&#25955;&#29305;&#24615;&#65292;&#23427;&#20204;&#22312;&#27867;&#21270;&#21040;&#36830;&#32493;&#26102;&#38388;&#25968;&#25454;&#33539;&#24335;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290; &#23613;&#31649;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#21450;&#20854;&#21464;&#20307;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#26080;&#27861;&#25429;&#33719;&#36825;&#20123;&#24207;&#21015;&#20869;&#37096;&#22797;&#26434;&#30340;&#30456;&#20851;&#24615;&#12290; &#21516;&#26102;&#23545;&#36755;&#20837;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#24182;&#25429;&#33719;&#36830;&#32493;&#26102;&#38388;&#31995;&#32479;&#30340;&#21160;&#24577;&#21464;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21448;&#38656;&#27714;&#36843;&#20999;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10635v1 Announce Type: cross  Abstract: Modeling continuous-time dynamics on irregular time series is critical to account for data evolution and correlations that occur continuously. Traditional methods including recurrent neural networks or Transformer models leverage inductive bias via powerful neural architectures to capture complex patterns. However, due to their discrete characteristic, they have limitations in generalizing to continuous-time data paradigms. Though neural ordinary differential equations (Neural ODEs) and their variants have shown promising results in dealing with irregular time series, they often fail to capture the intricate correlations within these sequences. It is challenging yet demanding to concurrently model the relationship between input data points and capture the dynamic changes of the continuous-time system. To tackle this problem, we propose ContiFormer that extends the relation modeling of vanilla Transformer to the continuous-time domain, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807; hierarchical spatiotemporal downsampling &#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#65292;&#32467;&#21512;&#21487;&#35299;&#37322;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#23545;&#26102;&#31354;&#39044;&#27979;&#30340;&#26377;&#25928;&#24314;&#27169;</title><link>https://arxiv.org/abs/2402.10634</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#26102;&#31354;&#38477;&#37319;&#26679;&#32570;&#22833;&#25968;&#25454;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph-based Forecasting with Missing Data through Spatiotemporal Downsampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10634
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807; hierarchical spatiotemporal downsampling &#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#65292;&#32467;&#21512;&#21487;&#35299;&#37322;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#23545;&#26102;&#31354;&#39044;&#27979;&#30340;&#26377;&#25928;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#32452;&#19982;&#31354;&#38388;&#20013;&#20256;&#24863;&#22120;&#28857;&#30456;&#20851;&#32852;&#12289;&#20855;&#26377;&#30456;&#20114;&#20851;&#31995;&#30340;&#21516;&#27493;&#26102;&#38388;&#24207;&#21015;&#65292;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#21253;&#25324;&#20026;&#27599;&#20010;&#28857;&#39044;&#27979;&#26410;&#26469;&#35266;&#27979;&#20540;&#12290;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23558;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#20026;&#22270;&#26469;&#23454;&#29616;&#24341;&#20154;&#27880;&#30446;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#24120;&#24120;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#21363;&#36755;&#20837;&#22987;&#32456;&#21487;&#29992;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#37096;&#20998;&#32570;&#22833;&#26102;&#26080;&#27861;&#25429;&#25417;&#38544;&#34255;&#30340;&#26102;&#31354;&#21160;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#23618;&#26102;&#31354;&#38477;&#37319;&#26679;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#38543;&#30528;&#26102;&#38388;&#21644;&#31354;&#38388;&#30340;&#25512;&#31227;&#36880;&#28176;&#31895;&#21270;&#65292;&#33719;&#24471;&#19968;&#32452;&#25429;&#25417;&#24322;&#36136;&#26102;&#38388;&#21644;&#31354;&#38388;&#21160;&#24577;&#30340;&#34920;&#31034;&#12290;&#22312;&#35266;&#27979;&#20540;&#21644;&#32570;&#22833;&#25968;&#25454;&#27169;&#24335;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#32452;&#21512;&#36825;&#20123;&#34920;&#31034;&#20197;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10634v1 Announce Type: cross  Abstract: Given a set of synchronous time series, each associated with a sensor-point in space and characterized by inter-series relationships, the problem of spatiotemporal forecasting consists of predicting future observations for each point. Spatiotemporal graph neural networks achieve striking results by representing the relationships across time series as a graph. Nonetheless, most existing methods rely on the often unrealistic assumption that inputs are always available and fail to capture hidden spatiotemporal dynamics when part of the data is missing. In this work, we tackle this problem through hierarchical spatiotemporal downsampling. The input time series are progressively coarsened over time and space, obtaining a pool of representations that capture heterogeneous temporal and spatial dynamics. Conditioned on observations and missing data patterns, such representations are combined by an interpretable attention mechanism to generate 
&lt;/p&gt;</description></item><item><title>&#23558;&#36923;&#36753;&#32422;&#26463;&#34701;&#21512;&#21040;&#22810;&#20219;&#21153;&#26680;&#24515;&#23398;&#20064;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#26469;&#36716;&#25442;&#36923;&#36753;&#38472;&#36848;&#20026;&#36830;&#32493;&#23454;&#29616;&#65292;&#20197;&#23454;&#29616;&#26680;&#24515;&#35859;&#35789;&#35745;&#31639;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.10617</link><description>&lt;p&gt;
&#24102;&#26377;&#36923;&#36753;&#32422;&#26463;&#30340;&#22810;&#20219;&#21153;&#22522;&#20110;&#26680;&#24515;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multitask Kernel-based Learning with Logic Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10617
&lt;/p&gt;
&lt;p&gt;
&#23558;&#36923;&#36753;&#32422;&#26463;&#34701;&#21512;&#21040;&#22810;&#20219;&#21153;&#26680;&#24515;&#23398;&#20064;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#26469;&#36716;&#25442;&#36923;&#36753;&#38472;&#36848;&#20026;&#36830;&#32493;&#23454;&#29616;&#65292;&#20197;&#23454;&#29616;&#26680;&#24515;&#35859;&#35789;&#35745;&#31639;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;&#36923;&#36753;&#32422;&#26463;&#24418;&#24335;&#30340;&#20808;&#39564;&#30693;&#35782;&#25972;&#21512;&#21040;&#26680;&#24515;&#26426;&#22120;&#20013;&#30340;&#19968;&#32452;&#20219;&#21153;&#20989;&#25968;&#20013;&#12290;&#36923;&#36753;&#21629;&#39064;&#25552;&#20379;&#20102;&#29615;&#22659;&#30340;&#37096;&#20998;&#34920;&#31034;&#65292;&#23398;&#20064;&#31639;&#27861;&#21033;&#29992;&#23427;&#19982;&#30417;&#30563;&#26679;&#26412;&#20013;&#21487;&#29992;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#26696;&#65292;&#20854;&#20013;&#22810;&#20010;&#29305;&#24449;&#31354;&#38388;&#19978;&#30340;&#19968;&#20803;&#35859;&#35789;&#35201;&#30001;&#26680;&#24515;&#26426;&#22120;&#23398;&#20064;&#65292;&#39640;&#32423;&#25277;&#35937;&#34920;&#31034;&#30001;&#36825;&#20123;&#35859;&#35789;&#30340;&#36923;&#36753;&#23376;&#21477;&#32452;&#25104;&#65292;&#24050;&#30693;&#23545;&#20110;&#20219;&#20309;&#36755;&#20837;&#37117;&#25104;&#31435;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#23558;&#36923;&#36753;&#23376;&#21477;&#36716;&#25442;&#20026;&#36830;&#32493;&#23454;&#29616;&#65292;&#22788;&#29702;&#26680;&#24515;&#35859;&#35789;&#35745;&#31639;&#30340;&#36755;&#20986;&#12290;&#23398;&#20064;&#20219;&#21153;&#34987;&#21046;&#23450;&#20026;&#25439;&#22833;&#20989;&#25968;&#30340;&#21407;&#22987;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#32467;&#21512;&#20102;&#27979;&#37327;&#30417;&#30563;&#26679;&#26412;&#25311;&#21512;&#24230;&#30340;&#39033;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10617v1 Announce Type: cross  Abstract: This paper presents a general framework to integrate prior knowledge in the form of logic constraints among a set of task functions into kernel machines. The logic propositions provide a partial representation of the environment, in which the learner operates, that is exploited by the learning algorithm together with the information available in the supervised examples. In particular, we consider a multi-task learning scheme, where multiple unary predicates on the feature space are to be learned by kernel machines and a higher level abstract representation consists of logic clauses on these predicates, known to hold for any input. A general approach is presented to convert the logic clauses into a continuous implementation, that processes the outputs computed by the kernel-based predicates. The learning task is formulated as a primal optimization problem of a loss function that combines a term measuring the fitting of the supervised ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36777;&#35770;&#35843;&#33410;LLMs&#65292;&#20351;&#20854;&#29983;&#25104;&#21487;&#25511;&#30340;&#25903;&#25345;&#29992;&#25143;&#23450;&#20041;&#35770;&#28857;&#30340;&#22768;&#26126;&#65292;&#25913;&#36827;&#20102;LLMs&#30340;&#21487;&#25511;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;DEBATunE&#27969;&#31243;&#12290;&#36890;&#36807;&#20004;&#20010;LLMs&#20043;&#38388;&#30340;&#22810;&#36718;&#36777;&#35770;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#25903;&#25345;&#29983;&#25104;&#26377;&#26356;&#39640;&#36136;&#37327;&#21644;&#26356;&#31361;&#20986;&#30340;&#22768;&#26126;&#12290;</title><link>https://arxiv.org/abs/2402.10614</link><description>&lt;p&gt;
&#36890;&#36807;&#36777;&#35770;&#35843;&#33410;LLMs&#20197;&#29983;&#25104;&#21487;&#25511;&#30340;&#20855;&#26377;&#20105;&#35758;&#24615;&#30340;&#22768;&#26126;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate Controllable Controversial Statements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36777;&#35770;&#35843;&#33410;LLMs&#65292;&#20351;&#20854;&#29983;&#25104;&#21487;&#25511;&#30340;&#25903;&#25345;&#29992;&#25143;&#23450;&#20041;&#35770;&#28857;&#30340;&#22768;&#26126;&#65292;&#25913;&#36827;&#20102;LLMs&#30340;&#21487;&#25511;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;DEBATunE&#27969;&#31243;&#12290;&#36890;&#36807;&#20004;&#20010;LLMs&#20043;&#38388;&#30340;&#22810;&#36718;&#36777;&#35770;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#25903;&#25345;&#29983;&#25104;&#26377;&#26356;&#39640;&#36136;&#37327;&#21644;&#26356;&#31361;&#20986;&#30340;&#22768;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#20195;&#34920;&#19981;&#21516;&#30340;&#20154;&#32676;&#65292;&#23588;&#20854;&#26159;&#23569;&#25968;&#32676;&#20307;&#65292;&#24182;&#20135;&#29983;&#25903;&#25345;&#20854;&#22810;&#26679;&#21270;&#29978;&#33267;&#26377;&#20105;&#35758;&#35266;&#28857;&#30340;&#22768;&#26126;&#23545;&#20110;&#21019;&#36896;&#19968;&#20010;&#21253;&#23481;&#30340;&#29615;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#32570;&#20047;&#36275;&#22815;&#30340;&#25511;&#21046;&#24615;&#26469;&#25903;&#25345;&#29983;&#25104;&#20869;&#23481;&#30340;&#31435;&#22330;&#65292;&#20854;&#20013;&#24448;&#24448;&#21253;&#21547;&#19981;&#19968;&#33268;&#12289;&#20013;&#31435;&#25110;&#26377;&#20559;&#35265;&#30340;&#22768;&#26126;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;LLMs&#22312;&#29983;&#25104;&#25903;&#25345;&#29992;&#25143;&#22312;&#25552;&#31034;&#20013;&#23450;&#20041;&#30340;&#35770;&#28857;&#30340;&#22768;&#26126;&#26102;&#30340;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#20004;&#20010;&#25345;&#26377;&#30456;&#21453;&#31435;&#22330;&#30340;LLMs&#20043;&#38388;&#30340;&#22810;&#36718;&#36777;&#35770;&#20135;&#29983;&#20102;&#26356;&#39640;&#36136;&#37327;&#21644;&#26356;&#31361;&#20986;&#30340;&#22768;&#26126;&#65292;&#36825;&#20123;&#22768;&#26126;&#23545;&#20110;&#25913;&#21892;LLMs&#30340;&#21487;&#25511;&#24615;&#26159;&#37325;&#35201;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Debate &amp; Tuning&#65288;&#8220;DEBATunE&#8221;&#65289;&#27969;&#31243;&#65292;&#36890;&#36807;&#24494;&#35843;LLMs&#29983;&#25104;&#36890;&#36807;&#36777;&#35770;&#33719;&#24471;&#30340;&#22768;&#26126;&#12290;&#20026;&#20102;&#26816;&#39564;DEBATunE&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#36804;&#20170;&#20026;&#27490;&#28085;&#30422;710&#20010;&#20105;&#35758;&#24615;&#20027;&#39064;&#30340;&#26368;&#22823;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10614v1 Announce Type: cross  Abstract: Making LLMs speak for different, especially minority groups of people, and generate statements supporting their diverse or even controversial perspectives is critical to creating an inclusive environment. However, existing LLMs lack sufficient controllability to the stance of their generated content, which often contains inconsistent, neutral, or biased statements. In this paper, we improve the controllability of LLMs in generating statements supporting an argument the user defined in the prompt. We find that multi-round debates between two LLMs with opposite stances generate higher-quality and more salient statements for each, which are important training data to improve the controllability of LLMs. Motivated by this, we develop a novel debate &amp; tuning ("DEBATunE") pipeline finetuning LLMs to generate the statements obtained via debate. To examine DEBATunE, we curate the largest dataset of debate topics so far, which covers 710 contro
&lt;/p&gt;</description></item><item><title>U$^2$MRPD&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#24341;&#23548;&#65292;&#23454;&#29616;&#20102;&#26080;&#30417;&#30563;&#30340;&#27424;&#37319;&#26679;MRI&#37325;&#24314;&#65292;&#33021;&#22815;&#25903;&#25345;&#22270;&#20687;&#29305;&#23450;&#30340;MRI&#37325;&#24314;&#65292;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;&#30417;&#30563;&#21644;MRI&#25193;&#25955;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10609</link><description>&lt;p&gt;
U$^2$MRPD: &#36890;&#36807;&#22823;&#22411;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#24341;&#23548;&#30340;&#26080;&#30417;&#30563;MRI&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
U$^2$MRPD: Unsupervised undersampled MRI reconstruction by prompting a large latent diffusion model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10609
&lt;/p&gt;
&lt;p&gt;
U$^2$MRPD&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#24341;&#23548;&#65292;&#23454;&#29616;&#20102;&#26080;&#30417;&#30563;&#30340;&#27424;&#37319;&#26679;MRI&#37325;&#24314;&#65292;&#33021;&#22815;&#25903;&#25345;&#22270;&#20687;&#29305;&#23450;&#30340;MRI&#37325;&#24314;&#65292;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;&#30417;&#30563;&#21644;MRI&#25193;&#25955;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10609v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LLDM)&#20013;&#34164;&#21547;&#30528;&#20016;&#23500;&#32780;&#20551;&#35774;&#19978;&#26222;&#36941;&#36866;&#29992;&#20110;&#33258;&#28982;&#21644;&#21307;&#23398;&#22270;&#20687;&#30340;&#38544;&#21547;&#35270;&#35273;&#30693;&#35782;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#31034;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;U$^2$MRPD&#65289;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#27424;&#37319;&#26679;MRI&#37325;&#24314;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#39537;&#21160;&#12289;&#30417;&#30563;&#30340;&#27424;&#37319;&#26679;MRI&#37325;&#24314;&#32593;&#32476;&#36890;&#24120;&#20855;&#26377;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#36866;&#24212;&#24615;&#65292;&#19981;&#36275;&#20197;&#24212;&#23545;&#21508;&#31181;&#25968;&#25454;&#37319;&#38598;&#22330;&#26223;&#65307;&#28982;&#32780;&#65292;U$^2$MRPD&#36890;&#36807;&#20351;&#29992;&#37327;&#36523;&#23450;&#21046;&#30340;MRSampler&#65292;&#25903;&#25345;&#22270;&#20687;&#29305;&#23450;&#30340;MRI&#37325;&#24314;&#65292;&#35813;MRSampler&#36866;&#29992;&#20110;&#22797;&#20540;MRI&#22270;&#20687;&#12290;&#36890;&#36807;&#20219;&#20309;&#21333;&#19968;&#26469;&#28304;&#25110;&#22810;&#28304;MRI&#25968;&#25454;&#38598;&#65292;U$^2$MRPD&#30340;&#24615;&#33021;&#36824;&#21487;&#20197;&#36890;&#36807;MRAdapter&#36827;&#34892;&#36827;&#19968;&#27493;&#25552;&#21319;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#22270;&#20687;&#20808;&#39564;&#19981;&#21464;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;U$^2$MRPD&#23454;&#29616;&#20102;&#19982;&#30417;&#30563;&#21644;MRI&#25193;&#25955;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10609v1 Announce Type: cross  Abstract: Implicit visual knowledge in a large latent diffusion model (LLDM) pre-trained on natural images is rich and hypothetically universal to natural and medical images. To test this hypothesis, we introduce a novel framework for Unsupervised Undersampled MRI Reconstruction by Prompting a pre-trained large latent Diffusion model ( U$^2$MRPD). Existing data-driven, supervised undersampled MRI reconstruction networks are typically of limited generalizability and adaptability toward diverse data acquisition scenarios; yet U$^2$MRPD supports image-specific MRI reconstruction by prompting an LLDM with an MRSampler tailored for complex-valued MRI images. With any single-source or diverse-source MRI dataset, U$^2$MRPD's performance is further boosted by an MRAdapter while keeping the generative image priors intact. Experiments on multiple datasets show that U$^2$MRPD achieves comparable or better performance than supervised and MRI diffusion metho
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#23454;&#39564;&#20869;&#37096;&#24615;&#33021;&#21644;&#23454;&#39564;&#21518;&#32467;&#26524;&#65292;&#22312;&#20248;&#21270;&#22823;&#35268;&#27169;&#20154;&#32676;&#20013;&#30340;&#34920;&#29616;&#26041;&#38754;&#25552;&#20379;&#20102;&#23574;&#38160;&#29702;&#35770;&#65292;&#25581;&#31034;&#20102;&#26032;&#39062;&#30340;&#35265;&#35299;</title><link>https://arxiv.org/abs/2402.10592</link><description>&lt;p&gt;
&#20248;&#21270;&#33258;&#36866;&#24212;&#23454;&#39564;&#65306;&#26368;&#23567;&#21270;&#21518;&#24724;&#21644;&#26368;&#20339;&#33218;&#35782;&#21035;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimizing Adaptive Experiments: A Unified Approach to Regret Minimization and Best-Arm Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10592
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#23454;&#39564;&#20869;&#37096;&#24615;&#33021;&#21644;&#23454;&#39564;&#21518;&#32467;&#26524;&#65292;&#22312;&#20248;&#21270;&#22823;&#35268;&#27169;&#20154;&#32676;&#20013;&#30340;&#34920;&#29616;&#26041;&#38754;&#25552;&#20379;&#20102;&#23574;&#38160;&#29702;&#35770;&#65292;&#25581;&#31034;&#20102;&#26032;&#39062;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#34892;&#33258;&#36866;&#24212;&#23454;&#39564;&#30340;&#20174;&#19994;&#32773;&#36890;&#24120;&#38754;&#20020;&#20004;&#20010;&#31454;&#20105;&#24615;&#20248;&#20808;&#32423;&#65306;&#36890;&#36807;&#22312;&#23454;&#39564;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#20998;&#37197;&#27835;&#30103;&#26469;&#38477;&#20302;&#23454;&#39564;&#25104;&#26412;&#65292;&#20197;&#21450;&#36805;&#36895;&#25910;&#38598;&#20449;&#24687;&#20197;&#32467;&#26463;&#23454;&#39564;&#24182;&#22312;&#25972;&#20010;&#20154;&#32676;&#20013;&#23454;&#26045;&#27835;&#30103;&#12290;&#24403;&#21069;&#65292;&#25991;&#29486;&#24847;&#35265;&#20998;&#27495;&#65292;&#26377;&#20851;&#26368;&#23567;&#21270;&#21518;&#24724;&#30340;&#30740;&#31350;&#29420;&#31435;&#22320;&#22788;&#29702;&#21069;&#32773;&#30340;&#20248;&#20808;&#32423;&#65292;&#32780;&#26377;&#20851;&#26368;&#20339;&#33218;&#35782;&#21035;&#30340;&#30740;&#31350;&#21017;&#19987;&#27880;&#20110;&#21518;&#32773;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#27169;&#22411;&#65292;&#32771;&#34385;&#21040;&#23454;&#39564;&#20869;&#37096;&#24615;&#33021;&#21644;&#23454;&#39564;&#21518;&#32467;&#26524;&#12290;&#25105;&#20204;&#38543;&#21518;&#25552;&#20379;&#20102;&#19968;&#20010;&#38024;&#23545;&#22823;&#35268;&#27169;&#20154;&#32676;&#30340;&#26368;&#20339;&#24615;&#33021;&#30340;&#23574;&#38160;&#29702;&#35770;&#65292;&#23558;&#25991;&#29486;&#20013;&#30340;&#32463;&#20856;&#32467;&#26524;&#32479;&#19968;&#36215;&#26469;&#12290;&#36825;&#31181;&#32479;&#19968;&#36824;&#25581;&#31034;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#20363;&#22914;&#65292;&#29702;&#35770;&#25581;&#31034;&#20102;&#31867;&#20284;&#26368;&#36817;&#25552;&#20986;&#30340;&#39030;&#37096;&#20004;&#20010;Thompson&#25277;&#26679;&#31639;&#27861;&#31561;&#29087;&#24713;&#31639;&#27861;&#21487;&#34987;&#35843;&#25972;&#20197;&#20248;&#21270;&#24191;&#27867;&#31867;&#21035;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10592v1 Announce Type: new  Abstract: Practitioners conducting adaptive experiments often encounter two competing priorities: reducing the cost of experimentation by effectively assigning treatments during the experiment itself, and gathering information swiftly to conclude the experiment and implement a treatment across the population. Currently, the literature is divided, with studies on regret minimization addressing the former priority in isolation, and research on best-arm identification focusing solely on the latter. This paper proposes a unified model that accounts for both within-experiment performance and post-experiment outcomes. We then provide a sharp theory of optimal performance in large populations that unifies canonical results in the literature. This unification also uncovers novel insights. For example, the theory reveals that familiar algorithms, like the recently proposed top-two Thompson sampling algorithm, can be adapted to optimize a broad class of obj
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#19982;&#32852;&#21512;&#35821;&#20041;&#20998;&#21106;&#21644;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30456;&#32467;&#21512;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#29983;-&#25945;&#24072;&#33976;&#39311;&#26041;&#27861; EMUFormer&#65292;&#25581;&#31034;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#23545;&#19981;&#30830;&#23450;&#24615;&#36136;&#37327;&#30340;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.10580</link><description>&lt;p&gt;
&#39640;&#25928;&#22810;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#29992;&#20110;&#32852;&#21512;&#35821;&#20041;&#20998;&#21106;&#21644;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient Multi-task Uncertainties for Joint Semantic Segmentation and Monocular Depth Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#19982;&#32852;&#21512;&#35821;&#20041;&#20998;&#21106;&#21644;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30456;&#32467;&#21512;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#29983;-&#25945;&#24072;&#33976;&#39311;&#26041;&#27861; EMUFormer&#65292;&#25581;&#31034;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#23545;&#19981;&#30830;&#23450;&#24615;&#36136;&#37327;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#25104;&#20026;&#24212;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26222;&#36941;&#25361;&#25112;&#65288;&#22914;&#36807;&#24230;&#33258;&#20449;&#12289;&#32570;&#20047;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#65289;&#30340;&#21487;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#23613;&#31649;&#23427;&#24448;&#24448;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#37117;&#20855;&#26377;&#22810;&#27169;&#24577;&#24615;&#36136;&#65292;&#22240;&#27492;&#21463;&#30410;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#20363;&#22914;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#35821;&#20041;&#20998;&#21106;&#21644;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30340;&#32852;&#21512;&#35299;&#20915;&#26041;&#26696;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#19982;&#32852;&#21512;&#35821;&#20041;&#20998;&#21106;&#21644;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30456;&#32467;&#21512;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#30456;&#20114;&#20043;&#38388;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19982;&#21333;&#29420;&#35299;&#20915;&#36825;&#20004;&#20010;&#20219;&#21153;&#30456;&#27604;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#19981;&#30830;&#23450;&#24615;&#36136;&#37327;&#26041;&#38754;&#30340;&#30410;&#22788;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;EMUFormer&#65292;&#19968;&#31181;&#26032;&#30340;&#23398;&#29983;-&#25945;&#24072;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#32852;&#21512;&#35821;&#20041;&#20998;&#21106;&#21644;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10580v1 Announce Type: cross  Abstract: Quantifying the predictive uncertainty emerged as a possible solution to common challenges like overconfidence or lack of explainability and robustness of deep neural networks, albeit one that is often computationally expensive. Many real-world applications are multi-modal in nature and hence benefit from multi-task learning. In autonomous driving, for example, the joint solution of semantic segmentation and monocular depth estimation has proven to be valuable. In this work, we first combine different uncertainty quantification methods with joint semantic segmentation and monocular depth estimation and evaluate how they perform in comparison to each other. Additionally, we reveal the benefits of multi-task learning with regard to the uncertainty quality compared to solving both tasks separately. Based on these insights, we introduce EMUFormer, a novel student-teacher distillation approach for joint semantic segmentation and monocular d
&lt;/p&gt;</description></item><item><title>&#31526;&#21495;&#33258;&#32534;&#30721;&#65288;$\Sigma$AE&#65289;&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#37325;&#26500;&#25439;&#22833;&#21644;&#24179;&#34892;&#25968;&#25454;&#30340;&#30417;&#30563;&#25439;&#22833;&#26469;&#20248;&#21270;&#36830;&#25509;&#20004;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#36716;&#23548;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10575</link><description>&lt;p&gt;
&#31526;&#21495;&#33258;&#32534;&#30721;&#29992;&#20110;&#33258;&#30417;&#30563;&#24207;&#21015;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Symbolic Autoencoding for Self-Supervised Sequence Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10575
&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#33258;&#32534;&#30721;&#65288;$\Sigma$AE&#65289;&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#37325;&#26500;&#25439;&#22833;&#21644;&#24179;&#34892;&#25968;&#25454;&#30340;&#30417;&#30563;&#25439;&#22833;&#26469;&#20248;&#21270;&#36830;&#25509;&#20004;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#36716;&#23548;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#25797;&#38271;&#39044;&#27979;&#25991;&#26412;&#24207;&#21015;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#65292;&#20294;&#22312;&#19981;&#21516;&#31526;&#21495;&#31995;&#32479;&#20043;&#38388;&#25191;&#34892;&#36716;&#23548;&#20219;&#21153;&#26102;&#36890;&#24120;&#20250;&#36935;&#21040;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#24179;&#34892;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31526;&#21495;&#33258;&#32534;&#30721;&#65288;$\Sigma$AE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#20016;&#23500;&#30340;&#19981;&#24179;&#34892;&#25968;&#25454;&#21644;&#26377;&#38480;&#30340;&#24179;&#34892;&#25968;&#25454;&#12290;$\Sigma$AE&#36890;&#36807;&#19968;&#20010;&#31163;&#25955;&#29942;&#39048;&#23618;&#36830;&#25509;&#20004;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#37325;&#26500;&#25439;&#22833;&#65288;&#19982;&#24179;&#34892;&#25968;&#25454;&#30340;&#30417;&#30563;&#25439;&#22833;&#21516;&#26102;&#36827;&#34892;&#20248;&#21270;&#65289;&#36827;&#34892;&#31471;&#21040;&#31471;&#20248;&#21270;&#65292;&#20351;&#24471;&#31163;&#25955;&#29942;&#39048;&#29983;&#25104;&#30340;&#24207;&#21015;&#21487;&#20197;&#34987;&#35835;&#21462;&#20026;&#36716;&#23548;&#30340;&#36755;&#20837;&#24207;&#21015;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23613;&#31649;&#23384;&#22312;&#29942;&#39048;&#31163;&#25955;&#24615;&#65292;&#20173;&#33021;&#36827;&#34892;&#39640;&#25928;&#30340;&#33258;&#30417;&#30563;&#24207;&#21015;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;$\Sigma$AE&#26174;&#33879;&#25552;&#39640;&#20102;&#36716;&#23548;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#20351;&#29992;&#20102;&#26368;&#23569;&#37327;&#30340;&#24179;&#34892;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10575v1 Announce Type: cross  Abstract: Traditional language models, adept at next-token prediction in text sequences, often struggle with transduction tasks between distinct symbolic systems, particularly when parallel data is scarce. Addressing this issue, we introduce \textit{symbolic autoencoding} ($\Sigma$AE), a self-supervised framework that harnesses the power of abundant unparallel data alongside limited parallel data. $\Sigma$AE connects two generative models via a discrete bottleneck layer and is optimized end-to-end by minimizing reconstruction loss (simultaneously with supervised loss for the parallel data), such that the sequence generated by the discrete bottleneck can be read out as the transduced input sequence. We also develop gradient-based methods allowing for efficient self-supervised sequence learning despite the discreteness of the bottleneck. Our results demonstrate that $\Sigma$AE significantly enhances performance on transduction tasks, even with min
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#65292;&#21363;&#20855;&#26377;&#20559;&#32622;&#30340;DPO&#65288;ODPO&#65289;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#19981;&#21516;&#23545;&#24453;&#27599;&#20010;&#20559;&#22909;&#23545;&#12290;</title><link>https://arxiv.org/abs/2402.10571</link><description>&lt;p&gt;
&#20855;&#26377;&#20559;&#32622;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Direct Preference Optimization with an Offset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10571
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#65292;&#21363;&#20855;&#26377;&#20559;&#32622;&#30340;DPO&#65288;ODPO&#65289;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#19981;&#21516;&#23545;&#24453;&#27599;&#20010;&#20559;&#22909;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26159;&#19968;&#31181;&#25104;&#21151;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#25110;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;DPO&#30340;&#27867;&#21270;&#24418;&#24335;&#65292;&#31216;&#20026;&#20855;&#26377;&#20559;&#32622;&#30340;DPO&#65288;ODPO&#65289;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#19981;&#23558;&#27599;&#20010;&#20559;&#22909;&#23545;&#35270;&#20026;&#30456;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10571v1 Announce Type: cross  Abstract: Direct preference optimization (DPO) is a successful fine-tuning strategy for aligning large language models with human preferences without the need to train a reward model or employ reinforcement learning. DPO, as originally formulated, relies on binary preference data and fine-tunes a language model to increase the likelihood of a preferred response over a dispreferred response. However, not all preference pairs are equal: while in some cases the preferred response is only slightly better than the dispreferred response, there can be a stronger preference for one response when, for example, the other response includes harmful or toxic content. In this paper, we propose a generalization of DPO, termed DPO with an offset (ODPO), that does not treat every preference pair equally during fine-tuning. Intuitively, ODPO requires the difference between the likelihood of the preferred and dispreferred response to be greater than an offset valu
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21019;&#26032;&#24895;&#26223;&#65292;&#35748;&#20026;&#22312;&#24037;&#19994;4.0&#26102;&#20195;&#65292;&#21327;&#20316;&#26426;&#22120;&#20154;&#65288;Cobot&#65289;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#20043;&#38388;&#23558;&#20250;&#26377;&#26356;&#32039;&#23494;&#30340;&#21327;&#20316;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#24037;&#19994;&#33258;&#21160;&#21270;&#30340;&#25928;&#29575;&#21644;&#20915;&#31574;&#21046;&#23450;&#12290;</title><link>https://arxiv.org/abs/2402.10553</link><description>&lt;p&gt;
&#24037;&#19994;4.0&#26102;&#20195;&#20013;&#20855;&#26377;&#23545;&#35805;&#20132;&#20114;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26032;&#22411;&#38598;&#25104;&#24037;&#19994;&#26041;&#27861;&#19982;&#21512;&#20316;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
A novel integrated industrial approach with cobots in the age of industry 4.0 through conversational interaction and computer vision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10553
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21019;&#26032;&#24895;&#26223;&#65292;&#35748;&#20026;&#22312;&#24037;&#19994;4.0&#26102;&#20195;&#65292;&#21327;&#20316;&#26426;&#22120;&#20154;&#65288;Cobot&#65289;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#20043;&#38388;&#23558;&#20250;&#26377;&#26356;&#32039;&#23494;&#30340;&#21327;&#20316;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#24037;&#19994;&#33258;&#21160;&#21270;&#30340;&#25928;&#29575;&#21644;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21462;&#20195;&#24037;&#20154;&#30340;&#26426;&#22120;&#20154;&#21040;&#20316;&#20026;&#26377;&#30410;&#21516;&#20107;&#30340;&#26426;&#22120;&#20154;&#65292;&#26426;&#22120;&#20154;&#33258;&#21160;&#21270;&#39046;&#22495;&#27491;&#22312;&#32463;&#21382;&#19968;&#31181;&#26032;&#30340;&#36235;&#21183;&#65292;&#36825;&#23545;&#38646;&#37096;&#20214;&#21046;&#36896;&#21830;&#26469;&#35828;&#26159;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#12290;&#35813;&#35770;&#25991;&#36129;&#29486;&#20102;&#19968;&#20010;&#21019;&#26032;&#24895;&#26223;&#65292;&#30475;&#21040;&#21327;&#20316;&#26426;&#22120;&#20154;&#65288;Cobot&#65289;&#19982;&#20154;&#24037;&#26234;&#33021;&#19990;&#30028;&#20197;&#21450;&#20154;&#31867;&#20043;&#38388;&#24840;&#21457;&#32039;&#23494;&#30340;&#21512;&#20316;&#65292;Cobot&#33021;&#22815;&#31934;&#30830;&#25191;&#34892;&#29305;&#23450;&#30340;&#20307;&#21147;&#24037;&#20316;&#65292;&#20154;&#24037;&#26234;&#33021;&#33021;&#22815;&#20998;&#26512;&#20449;&#24687;&#24182;&#25903;&#25345;&#20915;&#31574;&#36807;&#31243;&#65292;&#32780;&#20154;&#31867;&#33021;&#22815;&#23545;&#26410;&#26469;&#26377;&#25112;&#30053;&#24615;&#30340;&#24895;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10553v1 Announce Type: cross  Abstract: From robots that replace workers to robots that serve as helpful colleagues, the field of robotic automation is experiencing a new trend that represents a huge challenge for component manufacturers. The contribution starts from an innovative vision that sees an ever closer collaboration between Cobot, able to do a specific physical job with precision, the AI world, able to analyze information and support the decision-making process, and the man able to have a strategic vision of the future.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#21464;&#24322;&#21015;&#34920;&#30340;&#39034;&#24207;&#32467;&#26500;&#24182;&#21033;&#29992;&#36741;&#21161;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#36229;&#36234;&#26368;&#20808;&#36827;DRP&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;</title><link>https://arxiv.org/abs/2402.10551</link><description>&lt;p&gt;
&#20351;&#29992;&#36741;&#21161;&#20449;&#24687;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#20010;&#24615;&#21270;&#30284;&#30151;&#27835;&#30103;&#33647;&#29289;&#35782;&#21035;&#22120;
&lt;/p&gt;
&lt;p&gt;
Personalised Drug Identifier for Cancer Treatment with Transformers using Auxiliary Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10551
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#21464;&#24322;&#21015;&#34920;&#30340;&#39034;&#24207;&#32467;&#26500;&#24182;&#21033;&#29992;&#36741;&#21161;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#36229;&#36234;&#26368;&#20808;&#36827;DRP&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#22240;&#20854;&#19981;&#26029;&#22686;&#38271;&#30340;&#20020;&#24202;&#21644;&#32463;&#27982;&#36127;&#25285;&#32780;&#20173;&#28982;&#26159;&#20840;&#29699;&#24615;&#25361;&#25112;&#12290;&#20854;&#29420;&#29305;&#30340;&#20010;&#24615;&#21270;&#34920;&#29616;&#20351;&#27835;&#30103;&#21464;&#24471;&#22256;&#38590;&#65292;&#25512;&#21160;&#20102;&#20010;&#24615;&#21270;&#27835;&#30103;&#31574;&#30053;&#30340;&#36861;&#27714;&#12290;&#22240;&#27492;&#65292;&#22522;&#22240;&#32452;&#23398;&#20998;&#26512;&#36234;&#26469;&#36234;&#25104;&#20026;&#20020;&#24202;&#35786;&#26029;&#38754;&#26495;&#30340;&#19968;&#37096;&#20998;&#12290;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#38754;&#26495;&#38656;&#35201;&#20934;&#30830;&#30340;&#33647;&#29289;&#21453;&#24212;&#39044;&#27979;&#65288;DRP&#65289;&#27169;&#22411;&#65292;&#32780;&#30001;&#20110;&#26377;&#38480;&#30340;&#26631;&#35760;&#24739;&#32773;&#25968;&#25454;&#65292;&#24314;&#31435;&#36825;&#26679;&#30340;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20197;&#24448;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26041;&#27861;&#24050;&#32463;&#20351;&#29992;&#20102;&#21508;&#31181;&#24418;&#24335;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#27809;&#26377;&#26126;&#30830;&#24314;&#27169;&#36825;&#20123;&#35786;&#26029;&#38754;&#26495;&#20013;&#21464;&#24322;&#21015;&#34920;&#30340;&#21487;&#21464;&#38271;&#24230;&#30340;&#39034;&#24207;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20063;&#27809;&#26377;&#21033;&#29992;&#36741;&#21161;&#20449;&#24687;&#65288;&#22914;&#24739;&#32773;&#29983;&#23384;&#65289;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;DRP&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10551v1 Announce Type: new  Abstract: Cancer remains a global challenge due to its growing clinical and economic burden. Its uniquely personal manifestation, which makes treatment difficult, has fuelled the quest for personalized treatment strategies. Thus, genomic profiling is increasingly becoming part of clinical diagnostic panels. Effective use of such panels requires accurate drug response prediction (DRP) models, which are challenging to build due to limited labelled patient data. Previous methods to address this problem have used various forms of transfer learning. However, they do not explicitly model the variable length sequential structure of the list of mutations in such diagnostic panels. Further, they do not utilize auxiliary information (like patient survival) for model training. We address these limitations through a novel transformer based method, which surpasses the performance of state-of-the-art DRP models on benchmark data. We also present the design of a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;SynTone&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#38899;&#39057;&#34920;&#31034;&#23398;&#20064;&#20013;&#22522;&#20934;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#35299;&#32806;&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10547</link><description>&lt;p&gt;
&#36890;&#36807;&#21463;&#25511;&#21512;&#25104;&#23398;&#20064;&#35299;&#32806;&#30340;&#38899;&#39057;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Disentangled Audio Representations through Controlled Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10547
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;SynTone&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#38899;&#39057;&#34920;&#31034;&#23398;&#20064;&#20013;&#22522;&#20934;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#35299;&#32806;&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#22312;&#35299;&#32806;&#38899;&#39057;&#34920;&#31034;&#23398;&#20064;&#20013;&#22522;&#20934;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SynTone&#65292;&#19968;&#20010;&#20855;&#26377;&#26126;&#30830;&#22320;&#38754;&#23454;&#38469;&#35299;&#37322;&#22240;&#32032;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#35299;&#32806;&#25216;&#26415;&#12290;&#22312;SynTone&#19978;&#23545;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#31361;&#26174;&#20102;&#23427;&#22312;&#26041;&#27861;&#35780;&#20272;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#38899;&#39057;&#35299;&#32806;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#28608;&#21169;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10547v1 Announce Type: cross  Abstract: This paper tackles the scarcity of benchmarking data in disentangled auditory representation learning. We introduce SynTone, a synthetic dataset with explicit ground truth explanatory factors for evaluating disentanglement techniques. Benchmarking state-of-the-art methods on SynTone highlights its utility for method evaluation. Our results underscore strengths and limitations in audio disentanglement, motivating future research.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#37322;&#22312;&#22810;&#39046;&#22495;&#25351;&#23548;&#24494;&#35843;&#25968;&#25454;&#38598;&#19978;&#30340;&#29305;&#24615;&#65292;&#21457;&#29616;&#29983;&#25104;&#30340;&#35299;&#37322;&#34920;&#29616;&#20986;&#36873;&#25321;&#24615;&#21644;&#21253;&#21547;&#35828;&#26126;&#24615;&#20803;&#32032;&#65292;&#20294;&#36739;&#23569;&#26159;&#20027;&#35266;&#25110;&#35823;&#23548;&#24615;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.10532</link><description>&lt;p&gt;
LLM&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#29305;&#24615;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Properties and Challenges of LLM-Generated Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10532
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#37322;&#22312;&#22810;&#39046;&#22495;&#25351;&#23548;&#24494;&#35843;&#25968;&#25454;&#38598;&#19978;&#30340;&#29305;&#24615;&#65292;&#21457;&#29616;&#29983;&#25104;&#30340;&#35299;&#37322;&#34920;&#29616;&#20986;&#36873;&#25321;&#24615;&#21644;&#21253;&#21547;&#35828;&#26126;&#24615;&#20803;&#32032;&#65292;&#20294;&#36739;&#23569;&#26159;&#20027;&#35266;&#25110;&#35823;&#23548;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#25105;&#21512;&#29702;&#21270;&#33021;&#21147;&#22312;&#38480;&#23450;&#29615;&#22659;&#20013;&#24471;&#21040;&#20102;&#25506;&#32034;&#65292;&#20351;&#29992;&#29305;&#23450;&#20219;&#21153;/&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;LLMs&#24182;&#19981;&#65288;&#20165;&#65289;&#20381;&#36182;&#20110;&#29305;&#23450;&#27880;&#37322;&#30340;&#25968;&#25454;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#35299;&#37322;&#23427;&#20204;&#30340;&#36755;&#20986;&#12290;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#29305;&#24615;&#21463;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#21644;&#29992;&#20110;&#25351;&#23548;&#24494;&#35843;&#30340;&#30446;&#26631;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#21253;&#21547;&#22823;&#37327;&#37326;&#22806;&#20154;&#31867;&#32534;&#20889;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#20551;&#35774;LLMs&#37319;&#29992;&#20102;&#20154;&#31867;&#35299;&#37322;&#30340;&#20849;&#21516;&#29305;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#22810;&#22495;&#25351;&#23548;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#36755;&#20986;&#65292;&#25105;&#20204;&#21457;&#29616;&#29983;&#25104;&#30340;&#35299;&#37322;&#34920;&#29616;&#20986;&#36873;&#25321;&#24615;&#24182;&#21253;&#21547;&#35828;&#26126;&#24615;&#20803;&#32032;&#65292;&#20294;&#24456;&#23569;&#26159;&#20027;&#35266;&#25110;&#35823;&#23548;&#24615;&#30340;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23646;&#24615;&#23384;&#22312;&#25110;&#32570;&#22833;&#30340;&#21407;&#22240;&#21644;&#21518;&#26524;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#26681;&#25454;LLMs&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#21644;&#24494;&#35843;&#25968;&#25454;&#30340;&#24615;&#36136;&#65292;&#36825;&#20123;&#23646;&#24615;&#23384;&#22312;&#25110;&#32570;&#22833;&#30340;&#31215;&#26497;&#21644;&#28040;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10532v1 Announce Type: cross  Abstract: The self-rationalising capabilities of large language models (LLMs) have been explored in restricted settings, using task/specific data sets. However, current LLMs do not (only) rely on specifically annotated data; nonetheless, they frequently explain their outputs. The properties of the generated explanations are influenced by the pre-training corpus and by the target data used for instruction fine-tuning. As the pre-training corpus includes a large amount of human-written explanations "in the wild", we hypothesise that LLMs adopt common properties of human explanations. By analysing the outputs for a multi-domain instruction fine-tuning data set, we find that generated explanations show selectivity and contain illustrative elements, but less frequently are subjective or misleading. We discuss reasons and consequences of the properties' presence or absence. In particular, we outline positive and negative implications depending on the 
&lt;/p&gt;</description></item><item><title>LLM Comparator&#26159;&#19968;&#31181;&#29992;&#20110;&#20132;&#20114;&#24335;&#20998;&#26512;&#33258;&#21160;&#24182;&#34892;&#35780;&#20272;&#32467;&#26524;&#30340;&#26032;&#22411;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#25903;&#25345;&#29992;&#25143;&#29702;&#35299;&#27169;&#22411;&#34920;&#29616;&#20248;&#21155;&#21644;&#19981;&#21516;&#20043;&#22788;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.10524</link><description>&lt;p&gt;
LLM&#27604;&#36739;&#22120;&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24182;&#34892;&#35780;&#20272;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10524
&lt;/p&gt;
&lt;p&gt;
LLM Comparator&#26159;&#19968;&#31181;&#29992;&#20110;&#20132;&#20114;&#24335;&#20998;&#26512;&#33258;&#21160;&#24182;&#34892;&#35780;&#20272;&#32467;&#26524;&#30340;&#26032;&#22411;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#25903;&#25345;&#29992;&#25143;&#29702;&#35299;&#27169;&#22411;&#34920;&#29616;&#20248;&#21155;&#21644;&#19981;&#21516;&#20043;&#22788;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24182;&#34892;&#35780;&#20272;&#24050;&#25104;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21709;&#24212;&#36136;&#37327;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#36825;&#31181;&#35780;&#20272;&#26041;&#27861;&#30340;&#32467;&#26524;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LLM&#27604;&#36739;&#22120;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#22320;&#20998;&#26512;&#33258;&#21160;&#24182;&#34892;&#35780;&#20272;&#32467;&#26524;&#12290;&#35813;&#24037;&#20855;&#25903;&#25345;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#24335;&#24037;&#20316;&#27969;&#65292;&#20197;&#20102;&#35299;&#20026;&#20160;&#20040;&#21644;&#20309;&#26102;&#27169;&#22411;&#27604;&#22522;&#20934;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#25110;&#26356;&#24046;&#65292;&#20197;&#21450;&#20004;&#20010;&#27169;&#22411;&#30340;&#21709;&#24212;&#22312;&#36136;&#37327;&#19978;&#26377;&#20309;&#19981;&#21516;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#19968;&#23478;&#22823;&#22411;&#31185;&#25216;&#20844;&#21496;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#31243;&#24072;&#23494;&#20999;&#21512;&#20316;&#65292;&#36845;&#20195;&#35774;&#35745;&#21644;&#24320;&#21457;&#20102;&#35813;&#24037;&#20855;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#35782;&#21035;&#30340;&#29992;&#25143;&#25361;&#25112;&#12289;&#35813;&#24037;&#20855;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#65292;&#20197;&#21450;&#23450;&#26399;&#35780;&#20272;&#20854;&#27169;&#22411;&#30340;&#21442;&#19982;&#32773;&#30340;&#35266;&#23519;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10524v1 Announce Type: cross  Abstract: Automatic side-by-side evaluation has emerged as a promising approach to evaluating the quality of responses from large language models (LLMs). However, analyzing the results from this evaluation approach raises scalability and interpretability challenges. In this paper, we present LLM Comparator, a novel visual analytics tool for interactively analyzing results from automatic side-by-side evaluation. The tool supports interactive workflows for users to understand when and why a model performs better or worse than a baseline model, and how the responses from two models are qualitatively different. We iteratively designed and developed the tool by closely working with researchers and engineers at a large technology company. This paper details the user challenges we identified, the design and development of the tool, and an observational study with participants who regularly evaluate their models.
&lt;/p&gt;</description></item><item><title>&#20219;&#24847;&#31934;&#24230;LLM&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#22823;&#23567;LLMs&#37327;&#21270;&#20026;&#19981;&#21516;&#20301;&#23485;&#65288;&#22914;3&#12289;4&#12289;...&#65292;n&#20301;&#65289;&#24182;&#21472;&#21152;&#21040;&#20869;&#23384;&#20013;&#65292;&#26174;&#30528;&#38477;&#20302;&#20102;&#37096;&#32626;&#22810;&#20010;&#19981;&#21516;&#22823;&#23567;LLMs&#30340;&#39640;&#25104;&#26412;</title><link>https://arxiv.org/abs/2402.10517</link><description>&lt;p&gt;
&#20219;&#24847;&#31934;&#24230;LLM&#65306;&#22810;&#20010;&#19981;&#21516;&#22823;&#23567;LLM&#30340;&#20302;&#25104;&#26412;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10517
&lt;/p&gt;
&lt;p&gt;
&#20219;&#24847;&#31934;&#24230;LLM&#24341;&#20837;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#22823;&#23567;LLMs&#37327;&#21270;&#20026;&#19981;&#21516;&#20301;&#23485;&#65288;&#22914;3&#12289;4&#12289;...&#65292;n&#20301;&#65289;&#24182;&#21472;&#21152;&#21040;&#20869;&#23384;&#20013;&#65292;&#26174;&#30528;&#38477;&#20302;&#20102;&#37096;&#32626;&#22810;&#20010;&#19981;&#21516;&#22823;&#23567;LLMs&#30340;&#39640;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#30456;&#24403;&#22810;&#30340;&#21162;&#21147;&#65292;&#36825;&#20123;LLMs&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#31361;&#30772;&#24615;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#20854;&#24222;&#22823;&#30340;&#20307;&#31215;&#32780;&#23548;&#33268;&#37096;&#32626;&#25104;&#26412;&#39640;&#26114;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23613;&#31649;&#22810;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;LLMs&#37096;&#32626;&#30340;&#25104;&#26412;&#22312;&#23454;&#38469;&#24847;&#20041;&#19978;&#24456;&#37325;&#35201;&#65292;&#20294;&#21364;&#21463;&#21040;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#8220;&#20219;&#24847;&#31934;&#24230;LLM&#8221;&#65292;&#23558;&#20219;&#24847;&#31934;&#24230;DNN&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;LLMs&#12290;&#35299;&#20915;&#20102;&#20219;&#24847;&#31934;&#24230;LLM&#20013;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;LLMs&#20219;&#24847;&#31934;&#24230;&#37327;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#21518;&#35757;&#32451;&#37327;&#21270;&#26694;&#26550;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#36719;&#20214;&#24341;&#25806;&#26469;&#23454;&#29616;&#20854;&#26377;&#25928;&#30340;&#26381;&#21153;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#23558;&#20197;&#19981;&#21516;&#20301;&#23485;&#65288;&#22914;3&#12289;4&#12289;&#8230;&#65292;n&#20301;&#65289;&#37327;&#21270;&#30340;LLMs&#21472;&#21152;&#21040;&#20869;&#23384;&#36275;&#21360;&#20013;&#65292;&#26174;&#30528;&#38477;&#20302;&#20102;&#37096;&#32626;&#22810;&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;LLMs&#30340;&#39640;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10517v1 Announce Type: new  Abstract: Recently, considerable efforts have been directed towards compressing Large Language Models (LLMs), which showcase groundbreaking capabilities across diverse applications but entail significant deployment costs due to their large sizes. Meanwhile, much less attention has been given to mitigating the costs associated with deploying multiple LLMs of varying sizes despite its practical significance. Thus, this paper introduces \emph{any-precision LLM}, extending the concept of any-precision DNN to LLMs. Addressing challenges in any-precision LLM, we propose a lightweight method for any-precision quantization of LLMs, leveraging a post-training quantization framework, and develop a specialized software engine for its efficient serving. As a result, our solution significantly reduces the high costs of deploying multiple, different-sized LLMs by overlaying LLMs quantized to varying bit-widths, such as 3, 4, ..., $n$ bits, into a memory footpri
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#36827;&#27493;&#25512;&#21160;&#20102;&#34507;&#30333;&#36136;&#35774;&#35745;&#39046;&#22495;&#26397;&#30528;&#21069;&#25152;&#26410;&#26377;&#30340;&#38761;&#21629;&#26041;&#21521;&#21457;&#23637;&#65292;&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#29992;&#20110;&#21487;&#25511;&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#30340;&#29983;&#25104;AI&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.10516</link><description>&lt;p&gt;
&#25511;&#21046;&#21487;&#25511;&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#30340;&#29983;&#25104;AI&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Generative AI for Controllable Protein Sequence Design: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10516
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#36827;&#27493;&#25512;&#21160;&#20102;&#34507;&#30333;&#36136;&#35774;&#35745;&#39046;&#22495;&#26397;&#30528;&#21069;&#25152;&#26410;&#26377;&#30340;&#38761;&#21629;&#26041;&#21521;&#21457;&#23637;&#65292;&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#29992;&#20110;&#21487;&#25511;&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#30340;&#29983;&#25104;AI&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20855;&#26377;&#38024;&#23545;&#24615;&#21151;&#33021;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#24207;&#21015;&#26159;&#34507;&#30333;&#36136;&#24037;&#31243;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#20027;&#39064;&#65292;&#24433;&#21709;&#30528;&#33647;&#29289;&#21457;&#29616;&#21644;&#37238;&#24037;&#31243;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26102;&#38388;&#21644;&#37329;&#34701;&#38480;&#21046;&#65292;&#23548;&#33322;&#36825;&#20010;&#24222;&#22823;&#30340;&#32452;&#21512;&#25628;&#32034;&#31354;&#38388;&#20173;&#28982;&#26159;&#19968;&#20010;&#20005;&#23803;&#25361;&#25112;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#38761;&#21629;&#24615;&#36827;&#27493;&#65292;&#29305;&#21035;&#26159;&#29983;&#25104;&#27169;&#22411;&#21644;&#20248;&#21270;&#31639;&#27861;&#30340;&#31361;&#30772;&#24615;&#36827;&#23637;&#65292;&#36825;&#31181;&#24773;&#20917;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#25512;&#21160;&#34507;&#30333;&#36136;&#35774;&#35745;&#39046;&#22495;&#26397;&#30528;&#21069;&#25152;&#26410;&#26377;&#30340;&#38761;&#21629;&#26041;&#21521;&#21457;&#23637;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#29992;&#20110;&#21487;&#25511;&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#30340;&#29983;&#25104;AI&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20026;&#20102;&#22880;&#23450;&#22522;&#30784;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#20013;&#28041;&#21450;&#30340;&#32422;&#26463;&#24615;&#22522;&#26412;&#20219;&#21153;&#65292;&#24182;&#20171;&#32461;&#20102;&#20851;&#38190;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#20248;&#21270;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#23457;&#26597;&#20102;&#27599;&#20010;&#35774;&#35745;&#20219;&#21153;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10516v1 Announce Type: cross  Abstract: The design of novel protein sequences with targeted functionalities underpins a central theme in protein engineering, impacting diverse fields such as drug discovery and enzymatic engineering. However, navigating this vast combinatorial search space remains a severe challenge due to time and financial constraints. This scenario is rapidly evolving as the transformative advancements in AI, particularly in the realm of generative models and optimization algorithms, have been propelling the protein design field towards an unprecedented revolution. In this survey, we systematically review recent advances in generative AI for controllable protein sequence design. To set the stage, we first outline the foundational tasks in protein sequence design in terms of the constraints involved and present key generative models and optimization algorithms. We then offer in-depth reviews of each design task and discuss the pertinent applications. Finall
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;Resoformer&#65292;&#29992;&#20110;&#39044;&#27979;&#30005;&#21160;&#27773;&#36710;&#20256;&#21160;&#36724;&#19978;&#30340;&#25197;&#25391;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#38459;&#23612;&#25216;&#26415;&#21482;&#33021;&#22312;&#20849;&#25391;&#21457;&#29983;&#21518;&#26816;&#27979;&#21040;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.10511</link><description>&lt;p&gt;
&#21487;&#20197;&#21464;&#21387;&#22120;&#39044;&#27979;&#25391;&#21160;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Transformers Predict Vibrations?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10511
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;Resoformer&#65292;&#29992;&#20110;&#39044;&#27979;&#30005;&#21160;&#27773;&#36710;&#20256;&#21160;&#36724;&#19978;&#30340;&#25197;&#25391;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#38459;&#23612;&#25216;&#26415;&#21482;&#33021;&#22312;&#20849;&#25391;&#21457;&#29983;&#21518;&#26816;&#27979;&#21040;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#25391;&#21160;&#26159;&#30005;&#21160;&#27773;&#36710;&#65288;EVs&#65289;&#30340;&#37325;&#35201;&#30740;&#31350;&#38382;&#39064;&#12290;EVs&#22312;&#23822;&#23702;&#22320;&#24418;&#19978;&#34892;&#39542;&#26102;&#32463;&#24120;&#20250;&#20135;&#29983;&#25391;&#21160;&#65292;&#34987;&#31216;&#20026;&#25197;&#25391;&#20849;&#25391;&#12290;&#36825;&#31181;&#30001;&#30005;&#26426;&#21644;&#36718;&#32974;&#25391;&#21160;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#24341;&#36215;&#30340;&#20849;&#25391;&#20250;&#22312;&#36710;&#36742;&#20256;&#21160;&#36724;&#19978;&#26045;&#21152;&#36807;&#22823;&#36127;&#33655;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#38459;&#23612;&#25216;&#26415;&#20165;&#22312;&#20256;&#21160;&#36724;&#25197;&#30697;&#25391;&#21160;&#24133;&#24230;&#36798;&#21040;&#19968;&#23450;&#38408;&#20540;&#21518;&#25165;&#33021;&#26816;&#27979;&#21040;&#20849;&#25391;&#65292;&#23548;&#33268;&#22312;&#26816;&#27979;&#26102;&#20256;&#21160;&#36724;&#19978;&#25215;&#21463;&#37325;&#35201;&#36127;&#33655;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;Resoformer&#65292;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#25197;&#25391;&#30340;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#12290;Resoformer&#21033;&#29992;&#30005;&#26426;&#36716;&#36895;&#30340;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#22312;&#36755;&#20837;&#24207;&#21015;&#20043;&#21518;&#30340;&#29305;&#23450;&#20998;&#20301;&#25968;&#22788;&#39044;&#27979;&#20256;&#21160;&#36724;&#25197;&#25391;&#30340;&#24133;&#24230;&#12290;&#36890;&#36807;&#35745;&#31639;&#36882;&#24402;&#21644;&#21367;&#31215;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10511v1 Announce Type: cross  Abstract: Highly accurate time-series vibration prediction is an important research issue for electric vehicles (EVs). EVs often experience vibrations when driving on rough terrains, known as torsional resonance. This resonance, caused by the interaction between motor and tire vibrations, puts excessive loads on the vehicle's drive shaft. However, current damping technologies only detect resonance after the vibration amplitude of the drive shaft torque reaches a certain threshold, leading to significant loads on the shaft at the time of detection. In this study, we propose a novel approach to address this issue by introducing Resoformer, a transformer-based model for predicting torsional resonance. Resoformer utilizes time-series of the motor rotation speed as input and predicts the amplitude of torsional vibration at a specified quantile occurring in the shaft after the input series. By calculating the attention between recursive and convolutio
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#20108;&#27425;Littlewood-Offord&#38382;&#39064;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#65292;&#20272;&#35745;&#20102;&#23545;&#25239;&#24615;&#22122;&#22768;&#23545;&#20108;&#27425;Radamecher&#28151;&#27788;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20108;&#27425;&#21644;&#21452;&#32447;&#24615;Rademacher&#28151;&#27788;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#30340;&#19979;&#38480;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.10504</link><description>&lt;p&gt;
&#20108;&#27425;Littlewood-Offord&#38382;&#39064;&#30340;&#24377;&#24615;
&lt;/p&gt;
&lt;p&gt;
Resilience of the quadratic Littlewood-Offord problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10504
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#20108;&#27425;Littlewood-Offord&#38382;&#39064;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#65292;&#20272;&#35745;&#20102;&#23545;&#25239;&#24615;&#22122;&#22768;&#23545;&#20108;&#27425;Radamecher&#28151;&#27788;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20108;&#27425;&#21644;&#21452;&#32447;&#24615;Rademacher&#28151;&#27788;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#30340;&#19979;&#38480;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39640;&#32500;&#25968;&#25454;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#20851;&#20110;&#23545;&#25239;&#24615;&#22122;&#22768;&#23545;&#20108;&#27425;Radamecher&#28151;&#27788;$\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi}$&#21453;&#38598;&#20013;&#29305;&#24615;&#30340;&#24433;&#21709;&#30340;&#20272;&#35745;&#65292;&#20854;&#20013;$M$&#26159;&#19968;&#20010;&#22266;&#23450;&#30340;&#65288;&#39640;&#32500;&#65289;&#30697;&#38453;&#65292;$\boldsymbol{\xi}$&#26159;&#19968;&#20010;&#20849;&#24418;Rademacher&#21521;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;$\boldsymbol{\xi}$&#33021;&#22815;&#25215;&#21463;&#22810;&#23569;&#23545;&#25239;&#24615;&#31526;&#21495;&#32763;&#36716;&#32780;&#19981;&#8220;&#33192;&#32960;&#8221;$\sup_{x\in \mathbb{R}} \mathbb{P} \left\{\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi} = x\right\}$&#65292;&#20174;&#32780;&#8220;&#21435;&#38500;&#8221;&#21407;&#22987;&#20998;&#24067;&#23548;&#33268;&#26356;&#8220;&#26377;&#31890;&#24230;&#8221;&#21644;&#23545;&#25239;&#24615;&#20559;&#20506;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#20108;&#27425;&#21644;&#21452;&#32447;&#24615;Rademacher&#28151;&#27788;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#25552;&#20379;&#20102;&#19979;&#38480;&#20272;&#35745;&#65307;&#36825;&#20123;&#32467;&#26524;&#22312;&#20851;&#38190;&#21306;&#22495;&#34987;&#35777;&#26126;&#26159;&#28176;&#36817;&#32039;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10504v1 Announce Type: cross  Abstract: We study the statistical resilience of high-dimensional data. Our results provide estimates as to the effects of adversarial noise over the anti-concentration properties of the quadratic Radamecher chaos $\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi}$, where $M$ is a fixed (high-dimensional) matrix and $\boldsymbol{\xi}$ is a conformal Rademacher vector. Specifically, we pursue the question of how many adversarial sign-flips can $\boldsymbol{\xi}$ sustain without "inflating" $\sup_{x\in \mathbb{R}} \mathbb{P} \left\{\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi} = x\right\}$ and thus "de-smooth" the original distribution resulting in a more "grainy" and adversarially biased distribution. Our results provide lower bound estimations for the statistical resilience of the quadratic and bilinear Rademacher chaos; these are shown to be asymptotically tight across key regimes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23545;&#32477;&#23545;&#26143;&#31561;$M_B$&#36827;&#34892;&#32422;&#26463;&#65292;&#24182;&#21457;&#29616;&#22312;$z\approx 1$&#21306;&#22495;&#23384;&#22312;&#36716;&#25240;&#32418;&#31227;&#36857;&#35937;</title><link>https://arxiv.org/abs/2402.10502</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;$M_B$&#30340;&#26202;&#26399;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
Late-time transition of $M_B$ inferred via neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#23545;&#32477;&#23545;&#26143;&#31561;$M_B$&#36827;&#34892;&#32422;&#26463;&#65292;&#24182;&#21457;&#29616;&#22312;$z\approx 1$&#21306;&#22495;&#23384;&#22312;&#36716;&#25240;&#32418;&#31227;&#36857;&#35937;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23431;&#23449;&#21442;&#25968;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#23548;&#33268;&#23545;&#26631;&#20934;&#23431;&#23449;&#23398;&#22522;&#26412;&#26041;&#38754;&#30340;&#37325;&#26032;&#32771;&#34385;&#12290;&#21704;&#21187;&#24120;&#25968;&#30340;&#32039;&#24352;&#20063;&#21487;&#20197;&#34987;&#35270;&#20026;&#23616;&#37096;&#21644;&#26089;&#26399;&#23431;&#23449;&#23545;Ia&#22411;&#36229;&#26032;&#26143;&#32477;&#23545;&#26143;&#31561;$ M_B $&#30340;&#32422;&#26463;&#20043;&#38388;&#30340;&#32039;&#24352;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20197;&#19968;&#31181;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#24335;&#37325;&#26032;&#32771;&#34385;&#20102;&#35813;&#21442;&#25968;&#30340;&#21464;&#21270;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#26080;&#20559;&#22320;&#38480;&#21046;&#32477;&#23545;&#26143;&#31561;&#20540;&#65292;&#24182;&#35780;&#20272;&#19982;Pantheon+&#27719;&#32534;&#20013;$ M_B $&#38543;&#32418;&#31227;&#21464;&#21270;&#30340;&#24433;&#21709;&#21644;&#32479;&#35745;&#26174;&#33879;&#24615;&#65292;&#20197;&#21450;&#23545;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#24443;&#24213;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#22312;$ z\approx 1 $&#21306;&#22495;&#30340;&#36716;&#25240;&#32418;&#31227;&#30340;&#36857;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10502v1 Announce Type: cross  Abstract: The strengthening of tensions in the cosmological parameters has led to a reconsideration of fundamental aspects of standard cosmology. The tension in the Hubble constant can also be viewed as a tension between local and early Universe constraints on the absolute magnitude $M_B$ of Type Ia supernova. In this work, we reconsider the possibility of a variation of this parameter in a model-independent way. We employ neural networks to agnostically constrain the value of the absolute magnitude as well as assess the impact and statistical significance of a variation in $M_B$ with redshift from the Pantheon+ compilation, together with a thorough analysis of the neural network architecture. We find an indication for a transition redshift at the $z\approx 1$ region.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Active Preference Optimization&#31639;&#27861;&#65292;&#22312;Bradley-Terry-Luce&#20559;&#22909;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;RLHF&#30340;&#26679;&#26412;&#25928;&#29575;&#25552;&#39640;&#65292;&#20248;&#21270;&#20102;&#23545;&#25552;&#31034;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.10500</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#21160;&#20559;&#22909;&#20248;&#21270;&#23454;&#29616;&#32463;&#39564;&#35777;&#30340;&#26679;&#26412;&#25928;&#29575;&#30340;RLHF
&lt;/p&gt;
&lt;p&gt;
Provably Sample Efficient RLHF via Active Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10500
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Active Preference Optimization&#31639;&#27861;&#65292;&#22312;Bradley-Terry-Luce&#20559;&#22909;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;RLHF&#30340;&#26679;&#26412;&#25928;&#29575;&#25552;&#39640;&#65292;&#20248;&#21270;&#20102;&#23545;&#25552;&#31034;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#36825;&#20123;&#23545;&#40784;&#30340;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#20381;&#36182;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#22312;&#23454;&#38469;RLHF&#23454;&#26045;&#20013;&#26500;&#25104;&#20102;&#26114;&#36149;&#30340;&#29942;&#39048;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26356;&#22909;&#21644;&#33258;&#36866;&#24212;&#30340;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;RLHF&#20197;&#19978;&#19979;&#25991;&#20559;&#22909;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#24418;&#24335;&#26694;&#23450;&#65292;&#20854;&#20013;&#25552;&#31034;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#24182;&#34920;&#26126;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#25552;&#31034;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#22825;&#30495;&#26041;&#24335;&#23548;&#33268;&#19968;&#20010;&#22312;&#22870;&#21169;&#26041;&#38754;&#20855;&#26377;$\Omega(1)$&#27425;&#20248;&#24615;&#24046;&#36317;&#30340;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{Active Preference Optimization}$&#65288;$\texttt{APO}$&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#31215;&#26497;&#36873;&#25321;&#25552;&#31034;&#20197;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#12290;&#22312;Bradley-Terry-Luce&#65288;BTL&#65289;&#20559;&#22909;&#27169;&#22411;&#19979;&#65292;\texttt{APO}&#23454;&#29616;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#32780;&#19981;&#20250;&#22949;&#21327;&#20110;polic
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10500v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is pivotal in aligning Large Language Models (LLMs) with human preferences. While these aligned generative models have demonstrated impressive capabilities across various tasks, the dependence on high-quality human preference data poses a costly bottleneck in practical implementation of RLHF. Hence better and adaptive strategies for data collection is needed. To this end, we frame RLHF as a contextual preference bandit problem with prompts as contexts and show that the naive way of collecting preference data by choosing prompts uniformly at random leads to a policy that suffers an $\Omega(1)$ suboptimality gap in rewards. Then we propose $\textit{Active Preference Optimization}$ ($\texttt{APO}$), an algorithm that actively selects prompts to collect preference data. Under the Bradley-Terry-Luce (BTL) preference model, \texttt{APO} achieves sample efficiency without compromising on polic
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27604;&#36739;&#19977;&#31181;&#19981;&#21516;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#36890;&#29992;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;GRNN&#65289;&#22312;&#39044;&#27979;&#23567;&#40614;&#26543;&#21494;&#30149;&#20005;&#37325;&#31243;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#65292;&#24182;&#38656;&#35201;&#36739;&#23569;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.10492</link><description>&lt;p&gt;
&#21457;&#23637;&#19968;&#31181;&#39044;&#27979;&#23567;&#40614;&#26543;&#21494;&#30149;&#20005;&#37325;&#31243;&#24230;&#30340;&#26368;&#20339;&#27169;&#22411;&#65288;&#38463;&#23572;&#35199;&#21644;&#24052;&#21202;&#21306;&#26696;&#20363;&#30740;&#31350;&#65289;
&lt;/p&gt;
&lt;p&gt;
Developing an Optimal Model for Predicting the Severity of Wheat Stem Rust (Case study of Arsi and Bale Zone)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10492
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;&#19977;&#31181;&#19981;&#21516;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#36890;&#29992;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;GRNN&#65289;&#22312;&#39044;&#27979;&#23567;&#40614;&#26543;&#21494;&#30149;&#20005;&#37325;&#31243;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#65292;&#24182;&#38656;&#35201;&#36739;&#23569;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#20102;&#19977;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#26041;&#27861;&#65292;&#20998;&#21035;&#26159;&#20855;&#26377;&#19981;&#21516;&#35757;&#32451;&#12289;&#20256;&#36755;&#12289;&#20998;&#21106;&#21644;&#23398;&#20064;&#21151;&#33021;&#30340;&#21453;&#21521;&#20256;&#25773;&#31070;&#32463;&#32593;&#32476;&#65288;BPNN&#65289;&#65292;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#65288;RBFNN&#65289;&#21644;&#36890;&#29992;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;GRNN&#65289;&#65292;&#26469;&#39044;&#27979;&#26543;&#21494;&#30149;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#32771;&#34385;&#20102;&#21442;&#25968;&#22914;&#24179;&#22343;&#26368;&#39640;&#28201;&#24230;&#12289;&#24179;&#22343;&#26368;&#20302;&#28201;&#24230;&#12289;&#24179;&#22343;&#38477;&#38632;&#37327;&#12289;&#24179;&#22343;&#27668;&#28201;&#12289;&#24179;&#22343;&#30456;&#23545;&#28287;&#24230;&#21644;&#19981;&#21516;&#23567;&#40614;&#21697;&#31181;&#12290;&#32479;&#35745;&#20998;&#26512;&#34920;&#26126;&#65292;GRNN&#34920;&#29616;&#20986;&#26377;&#25928;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#19988;&#30456;&#27604;&#20854;&#20182;&#27169;&#22411;&#38656;&#35201;&#26356;&#23569;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#32467;&#26524;&#34920;&#26126;&#24635;&#23395;&#33410;&#38477;&#38632;&#37327;&#23545;&#23567;&#40614;&#26543;&#21494;&#30149;&#30340;&#21457;&#23637;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10492v1 Announce Type: cross  Abstract: This research utilized three types of artificial neural network (ANN) methodologies, namely Backpropagation Neural Network (BPNN) with varied training, transfer, divide, and learning functions; Radial Basis Function Neural Network (RBFNN); and General Regression Neural Network (GRNN), to forecast the severity of stem rust. It considered parameters such as mean maximum temperature, mean minimum temperature, mean rainfall, mean average temperature, mean relative humidity, and different wheat varieties. The statistical analysis revealed that GRNN demonstrated effective predictive capability and required less training time compared to the other models. Additionally, the results indicated that total seasonal rainfall positively influenced the development of wheat stem rust.   Keywords: Wheat stem rust, Back propagation neural network, Radial Basis Function Neural Network, General Regression Neural Network.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;RPMixer&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#25237;&#24433;&#23618;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#22686;&#21152;&#20102;&#22359;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#25552;&#39640;&#20102;&#25972;&#20307;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.10487</link><description>&lt;p&gt;
&#38024;&#23545;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#38543;&#26426;&#25237;&#24433;&#23618;
&lt;/p&gt;
&lt;p&gt;
Random Projection Layers for Multidimensional Time Sires Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10487
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;RPMixer&#65292;&#36890;&#36807;&#23558;&#38543;&#26426;&#25237;&#24433;&#23618;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#22686;&#21152;&#20102;&#22359;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#25552;&#39640;&#20102;&#25972;&#20307;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#28151;&#21512;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#27492;&#31867;&#27169;&#22411;&#24212;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#65288;&#20363;&#22914;&#31354;&#38388;-&#26102;&#38388;&#25968;&#25454;&#38598;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#65289;&#26102;&#65292;&#30001;&#20110;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#20854;&#24615;&#33021;&#21487;&#33021;&#20250;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26550;&#26500;&#65292;&#31216;&#20026;RPMixer&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38598;&#25104;&#24335;&#34892;&#20026;&#65292;&#20854;&#20013;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#21333;&#29420;&#22359;&#30340;&#20316;&#29992;&#31867;&#20284;&#20110;&#38598;&#25104;&#27169;&#22411;&#20013;&#30340;&#22522;&#26412;&#23398;&#20064;&#22120;&#65292;&#29305;&#21035;&#26159;&#22312;&#24341;&#20837;&#36523;&#20221;&#26144;&#23556;&#27531;&#24046;&#36830;&#25509;&#26102;&#12290;&#36890;&#36807;&#23558;&#38543;&#26426;&#25237;&#24433;&#23618;&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#22686;&#21152;&#20102;&#22359;&#36755;&#20986;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;RPMixer&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#23545;&#22823;&#35268;&#27169;&#31354;&#38388;-&#26102;&#38388;&#39044;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10487v1 Announce Type: cross  Abstract: All-Multi-Layer Perceptron (all-MLP) mixer models have been shown to be effective for time series forecasting problems. However, when such a model is applied to high-dimensional time series (e.g., the time series in a spatial-temporal dataset), its performance is likely to degrade due to overfitting issues. In this paper, we propose an all-MLP time series forecasting architecture, referred to as RPMixer. Our method leverages the ensemble-like behavior of deep neural networks, where each individual block within the network acts like a base learner in an ensemble model, especially when identity mapping residual connections are incorporated. By integrating random projection layers into our model, we increase the diversity among the blocks' outputs, thereby enhancing the overall performance of RPMixer. Extensive experiments conducted on large-scale spatial-temporal forecasting benchmark datasets demonstrate that our proposed method outperf
&lt;/p&gt;</description></item><item><title>&#33258;&#33976;&#39311;&#22312;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#25198;&#28436;&#30528;&#26631;&#31614;&#24179;&#22343;&#21270;&#30340;&#35282;&#33394;&#65292;&#26377;&#21161;&#20110;&#27169;&#22411;&#20851;&#27880;&#19982;&#29305;&#23450;&#23454;&#20363;&#30456;&#20851;&#30340;&#29305;&#24449;&#31751;&#20197;&#39044;&#27979;&#26631;&#31614;&#65292;&#20294;&#38543;&#30528;&#33976;&#39311;&#36718;&#27425;&#22686;&#21152;&#65292;&#24615;&#33021;&#20250;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#22312;&#26631;&#31614;&#22122;&#22768;&#24773;&#26223;&#19979;&#33258;&#33976;&#39311;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#25214;&#21040;&#20102;&#23454;&#29616;100%&#20998;&#31867;&#20934;&#30830;&#29575;&#25152;&#38656;&#30340;&#26368;&#23567;&#33976;&#39311;&#36718;&#27425;&#12290;</title><link>https://arxiv.org/abs/2402.10482</link><description>&lt;p&gt;
&#29702;&#35299;&#24102;&#26377;&#26631;&#31614;&#22122;&#38899;&#30340;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#30340;&#33258;&#33976;&#39311;&#21644;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Understanding Self-Distillation and Partial Label Learning in Multi-Class Classification with Label Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10482
&lt;/p&gt;
&lt;p&gt;
&#33258;&#33976;&#39311;&#22312;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#25198;&#28436;&#30528;&#26631;&#31614;&#24179;&#22343;&#21270;&#30340;&#35282;&#33394;&#65292;&#26377;&#21161;&#20110;&#27169;&#22411;&#20851;&#27880;&#19982;&#29305;&#23450;&#23454;&#20363;&#30456;&#20851;&#30340;&#29305;&#24449;&#31751;&#20197;&#39044;&#27979;&#26631;&#31614;&#65292;&#20294;&#38543;&#30528;&#33976;&#39311;&#36718;&#27425;&#22686;&#21152;&#65292;&#24615;&#33021;&#20250;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#22312;&#26631;&#31614;&#22122;&#22768;&#24773;&#26223;&#19979;&#33258;&#33976;&#39311;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#25214;&#21040;&#20102;&#23454;&#29616;100%&#20998;&#31867;&#20934;&#30830;&#29575;&#25152;&#38656;&#30340;&#26368;&#23567;&#33976;&#39311;&#36718;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#33976;&#39311;&#65288;SD&#65289;&#26159;&#20351;&#29992;&#25945;&#24072;&#27169;&#22411;&#30340;&#36755;&#20986;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#30340;&#36807;&#31243;&#65292;&#20004;&#20010;&#27169;&#22411;&#20849;&#20139;&#30456;&#21516;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#29702;&#35770;&#19978;&#32771;&#23519;&#20102;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#30340;SD&#65292;&#25506;&#32034;&#20102;&#22810;&#36718;SD&#21644;&#20855;&#26377;&#31934;&#28860;&#25945;&#24072;&#36755;&#20986;&#30340;SD&#65292;&#36825;&#20123;&#28789;&#24863;&#26469;&#33258;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#65288;PLL&#65289;&#12290;&#36890;&#36807;&#25512;&#23548;&#23398;&#29983;&#27169;&#22411;&#36755;&#20986;&#30340;&#23553;&#38381;&#24418;&#24335;&#35299;&#65292;&#25105;&#20204;&#21457;&#29616;SD&#26412;&#36136;&#19978;&#26159;&#22312;&#20855;&#26377;&#39640;&#29305;&#24449;&#30456;&#20851;&#24615;&#30340;&#23454;&#20363;&#20043;&#38388;&#36827;&#34892;&#26631;&#31614;&#24179;&#22343;&#12290;&#26368;&#21021;&#26377;&#30410;&#30340;&#24179;&#22343;&#21270;&#26377;&#21161;&#20110;&#27169;&#22411;&#19987;&#27880;&#20110;&#19982;&#32473;&#23450;&#23454;&#20363;&#30456;&#20851;&#32852;&#30340;&#29305;&#24449;&#31751;&#20197;&#39044;&#27979;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#33976;&#39311;&#36718;&#27425;&#30340;&#22686;&#21152;&#65292;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SD&#22312;&#26631;&#31614;&#22122;&#22768;&#24773;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#30830;&#23450;&#23454;&#29616;100%&#20998;&#31867;&#20934;&#30830;&#29575;&#25152;&#38656;&#30340;&#26631;&#31614;&#25439;&#22351;&#26465;&#20214;&#21644;&#26368;&#23567;&#33976;&#39311;&#36718;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10482v1 Announce Type: new  Abstract: Self-distillation (SD) is the process of training a student model using the outputs of a teacher model, with both models sharing the same architecture. Our study theoretically examines SD in multi-class classification with cross-entropy loss, exploring both multi-round SD and SD with refined teacher outputs, inspired by partial label learning (PLL). By deriving a closed-form solution for the student model's outputs, we discover that SD essentially functions as label averaging among instances with high feature correlations. Initially beneficial, this averaging helps the model focus on feature clusters correlated with a given instance for predicting the label. However, it leads to diminishing performance with increasing distillation rounds. Additionally, we demonstrate SD's effectiveness in label noise scenarios and identify the label corruption condition and minimum number of distillation rounds needed to achieve 100% classification accur
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;GPT-4&#21644;BERT&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#21457;&#29616;&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#24773;&#32490;&#30340;&#31574;&#30053;&#21487;&#20197;&#24110;&#21161;&#36991;&#20813;&#24066;&#22330;&#19979;&#25387;&#24182;&#31283;&#23450;&#22238;&#25253;&#12290;</title><link>https://arxiv.org/abs/2402.10481</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#30340;&#21152;&#23494;&#36164;&#20135;&#24066;&#22330;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
Emoji Driven Crypto Assets Market Reactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10481
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;GPT-4&#21644;BERT&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#21457;&#29616;&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#24773;&#32490;&#30340;&#31574;&#30053;&#21487;&#20197;&#24110;&#21161;&#36991;&#20813;&#24066;&#22330;&#19979;&#25387;&#24182;&#31283;&#23450;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21152;&#23494;&#36135;&#24065;&#39046;&#22495;&#65292;&#35832;&#22914;Twitter&#20043;&#31867;&#30340;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#24050;&#32463;&#25104;&#20026;&#24433;&#21709;&#24066;&#22330;&#36235;&#21183;&#21644;&#25237;&#36164;&#32773;&#24773;&#32490;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;GPT-4&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#34920;&#24773;&#31526;&#21495;&#24773;&#32490;&#23545;&#21152;&#23494;&#36135;&#24065;&#24066;&#22330;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23558;&#34920;&#24773;&#31526;&#21495;&#36716;&#21270;&#20026;&#21487;&#37327;&#21270;&#30340;&#24773;&#24863;&#25968;&#25454;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#35265;&#35299;&#19982;BTC&#20215;&#26684;&#21644;VCRIX&#25351;&#25968;&#31561;&#20851;&#38190;&#24066;&#22330;&#25351;&#26631;&#36827;&#34892;&#20102;&#30456;&#20851;&#32852;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#26088;&#22312;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#20803;&#32032;&#35782;&#21035;&#21644;&#39044;&#27979;&#24066;&#22330;&#36235;&#21183;&#30340;&#20132;&#26131;&#31574;&#30053;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#34920;&#24773;&#31526;&#21495;&#24773;&#32490;&#30340;&#31574;&#30053;&#21487;&#20197;&#26377;&#21161;&#20110;&#36991;&#20813;&#37325;&#22823;&#24066;&#22330;&#19979;&#25387;&#65292;&#24182;&#26377;&#21161;&#20110;&#22238;&#25253;&#30340;&#31283;&#23450;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#23558;&#20808;&#36827;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20998;&#26512;&#25972;&#21512;&#21040;&#37329;&#34701;&#31574;&#30053;&#20013;&#30340;&#23454;&#38469;&#30410;&#22788;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#26469;&#30475;&#24453;&#24066;&#22330;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10481v1 Announce Type: cross  Abstract: In the burgeoning realm of cryptocurrency, social media platforms like Twitter have become pivotal in influencing market trends and investor sentiments. In our study, we leverage GPT-4 and a fine-tuned transformer-based BERT model for a multimodal sentiment analysis, focusing on the impact of emoji sentiment on cryptocurrency markets. By translating emojis into quantifiable sentiment data, we correlate these insights with key market indicators like BTC Price and the VCRIX index. This approach may be fed into the development of trading strategies aimed at utilizing social media elements to identify and forecast market trends. Crucially, our findings suggest that strategies based on emoji sentiment can facilitate the avoidance of significant market downturns and contribute to the stabilization of returns. This research underscores the practical benefits of integrating advanced AI-driven analyses into financial strategies, offering a nuan
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CodaMal&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20302;&#25104;&#26412;&#26174;&#24494;&#38236;&#19979;&#30111;&#30142;&#26816;&#27979;&#30340;&#23545;&#27604;&#22495;&#33258;&#36866;&#24212;&#65292;&#35299;&#20915;&#20102;HCM&#21644;LCM&#22270;&#20687;&#20043;&#38388;&#30340;&#22495;&#24046;&#24322;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.10478</link><description>&lt;p&gt;
CodaMal&#65306;&#20302;&#25104;&#26412;&#26174;&#24494;&#38236;&#19979;&#30340;&#30111;&#30142;&#26816;&#27979;&#30340;&#23545;&#27604;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
CodaMal: Contrastive Domain Adaptation for Malaria Detection in Low-Cost Microscopes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10478
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CodaMal&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20302;&#25104;&#26412;&#26174;&#24494;&#38236;&#19979;&#30111;&#30142;&#26816;&#27979;&#30340;&#23545;&#27604;&#22495;&#33258;&#36866;&#24212;&#65292;&#35299;&#20915;&#20102;HCM&#21644;LCM&#22270;&#20687;&#20043;&#38388;&#30340;&#22495;&#24046;&#24322;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30111;&#30142;&#26159;&#20840;&#29699;&#37325;&#22823;&#20581;&#24247;&#38382;&#39064;&#65292;&#20854;&#35786;&#26029;&#38656;&#35201;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#20302;&#25104;&#26412;&#26174;&#24494;&#38236;(LCM)&#19979;&#30340;&#26174;&#24494;&#22270;&#20687;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#20174;&#26174;&#24494;&#22270;&#20687;&#20013;&#36827;&#34892;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#26631;&#27880;&#30340;&#26174;&#29616;&#20986;&#21463;&#30111;&#30142;&#23492;&#29983;&#34411;&#24433;&#21709;&#30340;&#32454;&#32990;&#21450;&#20854;&#29983;&#21629;&#21608;&#26399;&#38454;&#27573;&#30340;&#22270;&#20687;&#12290;&#19982;&#20174;&#39640;&#25104;&#26412;&#26174;&#24494;&#38236;(HCM)&#20013;&#26631;&#27880;&#22270;&#20687;&#30456;&#27604;&#65292;&#20174;LCM&#20013;&#26631;&#27880;&#22270;&#20687;&#26174;&#33879;&#22686;&#21152;&#20102;&#21307;&#23398;&#19987;&#23478;&#30340;&#36127;&#25285;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#23454;&#38469;&#30340;&#35299;&#20915;&#26041;&#26696;&#24212;&#35813;&#22312;HCM&#22270;&#20687;&#19978;&#35757;&#32451;&#65292;&#33021;&#22815;&#22312;LCM&#22270;&#20687;&#19978;&#27979;&#35797;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#20316;&#21697;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CodaMal&#65288;&#23545;&#27604;&#22495;&#33258;&#36866;&#24212;&#29992;&#20110;&#30111;&#30142;&#26816;&#27979;&#65289;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10478v1 Announce Type: cross  Abstract: Malaria is a major health issue worldwide, and its diagnosis requires scalable solutions that can work effectively with low-cost microscopes (LCM). Deep learning-based methods have shown success in computer-aided diagnosis from microscopic images. However, these methods need annotated images that show cells affected by malaria parasites and their life stages. Annotating images from LCM significantly increases the burden on medical experts compared to annotating images from high-cost microscopes (HCM). For this reason, a practical solution would be trained on HCM images which should generalize well on LCM images during testing. While earlier methods adopted a multi-stage learning process, they did not offer an end-to-end approach. In this work, we present an end-to-end learning framework, named CodaMal (Contrastive Domain Adpation for Malaria). In order to bridge the gap between HCM (training) and LCM (testing), we propose a domain adap
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36739;&#31616;&#21333;&#30340;&#22270;&#20687;&#22312;&#27491;&#24577;&#27969;&#27169;&#22411;&#20013;&#24471;&#21040;&#26356;&#39640;&#21487;&#33021;&#24615;&#65292;&#25581;&#31034;&#20102;&#23545;&#35813;&#29616;&#35937;&#30340;&#28508;&#22312;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22270;&#20687;&#22797;&#26434;&#24230;&#20316;&#20026;&#29420;&#31435;&#21464;&#37327;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10477</link><description>&lt;p&gt;
&#36890;&#36807;&#24322;&#24120;&#26816;&#27979;&#25506;&#31350;&#27491;&#24577;&#27969;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#21644;&#22270;&#20687;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Understanding Likelihood of Normalizing Flow and Image Complexity through the Lens of Out-of-Distribution Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10477
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36739;&#31616;&#21333;&#30340;&#22270;&#20687;&#22312;&#27491;&#24577;&#27969;&#27169;&#22411;&#20013;&#24471;&#21040;&#26356;&#39640;&#21487;&#33021;&#24615;&#65292;&#25581;&#31034;&#20102;&#23545;&#35813;&#29616;&#35937;&#30340;&#28508;&#22312;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22270;&#20687;&#22797;&#26434;&#24230;&#20316;&#20026;&#29420;&#31435;&#21464;&#37327;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#26412;&#25991;&#37325;&#28857;&#35299;&#37322;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;DGM&#65289;&#24120;&#24120;&#23558;&#26410;&#30693;&#30340;OOD&#36755;&#20837;&#20998;&#37197;&#26356;&#39640;&#30340;&#21487;&#33021;&#24615;&#65292;&#32780;&#19981;&#26159;&#23427;&#20204;&#24050;&#30693;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#28508;&#22312;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#36739;&#31616;&#21333;&#30340;&#22270;&#20687;&#22312;&#28508;&#22312;&#31354;&#38388;&#30340;&#39640;&#23494;&#24230;&#21306;&#22495;&#38598;&#20013;&#65292;&#23548;&#33268;&#22312;&#27491;&#24577;&#27969;&#65288;NF&#65289;&#20013;&#20998;&#37197;&#26356;&#39640;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#20551;&#35774;&#22312;&#20116;&#31181;NF&#20307;&#31995;&#32467;&#26500;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24471;&#20986;&#23427;&#20204;&#30340;&#21487;&#33021;&#24615;&#26159;&#19981;&#21487;&#20449;&#30340;&#32467;&#35770;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#23558;&#22270;&#20687;&#22797;&#26434;&#24230;&#20316;&#20026;&#19968;&#20010;&#29420;&#31435;&#21464;&#37327;&#26469;&#32531;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10477v1 Announce Type: new  Abstract: Out-of-distribution (OOD) detection is crucial to safety-critical machine learning applications and has been extensively studied. While recent studies have predominantly focused on classifier-based methods, research on deep generative model (DGM)-based methods have lagged relatively. This disparity may be attributed to a perplexing phenomenon: DGMs often assign higher likelihoods to unknown OOD inputs than to their known training data. This paper focuses on explaining the underlying mechanism of this phenomenon. We propose a hypothesis that less complex images concentrate in high-density regions in the latent space, resulting in a higher likelihood assignment in the Normalizing Flow (NF). We experimentally demonstrate its validity for five NF architectures, concluding that their likelihood is untrustworthy. Additionally, we show that this problem can be alleviated by treating image complexity as an independent variable. Finally, we provi
&lt;/p&gt;</description></item><item><title>Alt-GDA&#31639;&#27861;&#34987;&#35777;&#26126;&#26356;&#24555;&#65292;&#25552;&#20986;&#20102;&#20132;&#26367;&#22806;&#25512;GDA&#65288;Alex-GDA&#65289;&#36890;&#29992;&#31639;&#27861;&#26694;&#26550;&#65292;&#20197;&#36718;&#27969;&#20174;&#36845;&#20195;&#30340;&#22806;&#25512;&#20013;&#33719;&#21462;&#26799;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.10475</link><description>&lt;p&gt;
&#26497;&#23567;&#21270;&#20248;&#21270;&#20013;&#20132;&#26367;&#26356;&#26032;&#30340;&#22522;&#26412;&#21033;&#30410;
&lt;/p&gt;
&lt;p&gt;
Fundamental Benefit of Alternating Updates in Minimax Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10475
&lt;/p&gt;
&lt;p&gt;
Alt-GDA&#31639;&#27861;&#34987;&#35777;&#26126;&#26356;&#24555;&#65292;&#25552;&#20986;&#20102;&#20132;&#26367;&#22806;&#25512;GDA&#65288;Alex-GDA&#65289;&#36890;&#29992;&#31639;&#27861;&#26694;&#26550;&#65292;&#20197;&#36718;&#27969;&#20174;&#36845;&#20195;&#30340;&#22806;&#25512;&#20013;&#33719;&#21462;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Gradient Descent-Ascent&#65288;GDA&#65289;&#31639;&#27861;&#26088;&#22312;&#35299;&#20915;&#26497;&#23567;&#21270;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#37319;&#29992;&#19979;&#38477;&#21644;&#19978;&#21319;&#27493;&#39588;&#65292;&#20998;&#20026;&#21516;&#26102;&#36827;&#34892;&#65288;Sim-GDA&#65289;&#25110;&#20132;&#26367;&#36827;&#34892;&#65288;Alt-GDA&#65289;&#12290; Alt-GDA&#36890;&#24120;&#25910;&#25947;&#26356;&#24555;&#65292;&#20294;&#20004;&#32773;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#23578;&#26410;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#24456;&#22909;&#30340;&#29702;&#35299;&#65292;&#23588;&#20854;&#26159;&#22312;&#20840;&#23616;&#25910;&#25947;&#36895;&#29575;&#26041;&#38754;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#38024;&#23545;&#24378;&#20984;&#24378;&#20985;&#21644;Lipschitz&#26799;&#24230;&#30446;&#26631;&#25552;&#20986;&#20102;&#23545;&#20004;&#31181;&#31639;&#27861;&#30340;&#32454;&#31890;&#24230;&#25910;&#25947;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;Alt-GDA&#30340;&#26032;&#36845;&#20195;&#22797;&#26434;&#24615;&#19978;&#30028;&#20005;&#26684;&#23567;&#20110;Sim-GDA&#30340;&#19979;&#30028;&#65307;&#21363;Alt-GDA&#34987;&#35777;&#26126;&#26356;&#24555;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20132;&#26367;&#22806;&#25512;GDA&#65288;Alex-GDA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;Sim-GDA&#21644;Alt-GDA&#30340;&#36890;&#29992;&#31639;&#27861;&#26694;&#26550;&#65292;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#36718;&#27969;&#20174;&#36845;&#20195;&#30340;&#22806;&#25512;&#20013;&#33719;&#21462;&#26799;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Alex-GDA&#28385;&#36275;&#26356;&#23567;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10475v1 Announce Type: cross  Abstract: The Gradient Descent-Ascent (GDA) algorithm, designed to solve minimax optimization problems, takes the descent and ascent steps either simultaneously (Sim-GDA) or alternately (Alt-GDA). While Alt-GDA is commonly observed to converge faster, the performance gap between the two is not yet well understood theoretically, especially in terms of global convergence rates. To address this theory-practice gap, we present fine-grained convergence analyses of both algorithms for strongly-convex-strongly-concave and Lipschitz-gradient objectives. Our new iteration complexity upper bound of Alt-GDA is strictly smaller than the lower bound of Sim-GDA; i.e., Alt-GDA is provably faster. Moreover, we propose Alternating-Extrapolation GDA (Alex-GDA), a general algorithmic framework that subsumes Sim-GDA and Alt-GDA, for which the main idea is to alternately take gradients from extrapolations of the iterates. We show that Alex-GDA satisfies a smaller it
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27491;&#21017;&#21270;&#22238;&#24402;&#65292;&#22312;&#36229;&#21442;&#25968;&#21270;&#33539;&#22260;&#20869;&#65292;&#26681;&#25454;&#29305;&#23450;&#36873;&#25321;&#30340;&#20984;&#20989;&#25968;&#24182;&#36866;&#24403;&#22686;&#21152;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#65292;&#21487;&#20197;&#23454;&#29616;&#31232;&#30095;&#21644;&#19968;&#20301;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#24615;&#33021;&#20960;&#20046;&#19982;&#26368;&#20339;&#20998;&#31867;&#24615;&#33021;&#30456;&#21516;&#12290;</title><link>https://arxiv.org/abs/2402.10474</link><description>&lt;p&gt;
&#19968;&#20301;&#37327;&#21270;&#21644;&#31232;&#30095;&#21270;&#29992;&#20110;&#22810;&#31867;&#32447;&#24615;&#20998;&#31867;&#30340;&#27491;&#21017;&#21270;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
One-Bit Quantization and Sparsification for Multiclass Linear Classification via Regularized Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10474
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27491;&#21017;&#21270;&#22238;&#24402;&#65292;&#22312;&#36229;&#21442;&#25968;&#21270;&#33539;&#22260;&#20869;&#65292;&#26681;&#25454;&#29305;&#23450;&#36873;&#25321;&#30340;&#20984;&#20989;&#25968;&#24182;&#36866;&#24403;&#22686;&#21152;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#65292;&#21487;&#20197;&#23454;&#29616;&#31232;&#30095;&#21644;&#19968;&#20301;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#24615;&#33021;&#20960;&#20046;&#19982;&#26368;&#20339;&#20998;&#31867;&#24615;&#33021;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#24615;&#22238;&#24402;&#20013;&#29992;&#20110;&#22810;&#31867;&#20998;&#31867;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#36229;&#21442;&#25968;&#21270;&#33539;&#22260;&#20869;&#65292;&#35757;&#32451;&#25968;&#25454;&#20013;&#19968;&#20123;&#26631;&#35760;&#38169;&#35823;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20026;&#20102;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#38169;&#35823;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#38656;&#35201;&#28155;&#21152;&#19968;&#20010;&#26174;&#24335;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;$\lambda f(w)$&#65292;&#20854;&#20013;$f(\cdot)$&#26159;&#26576;&#20010;&#20984;&#20989;&#25968;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#25968;&#25454;&#26159;&#20174;&#19968;&#20010;&#20855;&#26377;&#30456;&#31561;&#31867;&#22823;&#23567;&#30340;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#20013;&#37319;&#26679;&#30340;&#65292;&#24182;&#19988;&#27599;&#20010;&#31867;&#21035;&#30340;&#35757;&#32451;&#26631;&#31614;&#20013;&#26377;&#19968;&#37096;&#20998;&#27604;&#20363;&#20026;$c$&#26159;&#38169;&#35823;&#30340;&#12290;&#22312;&#36825;&#20123;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;$f(\cdot) = \|\cdot\|^2_2$&#19988;$\lambda \to \infty$&#26102;&#65292;&#21487;&#20197;&#33719;&#24471;&#26368;&#20339;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#28982;&#21518;&#25105;&#20204;&#32487;&#32493;&#20998;&#26512;&#20102;&#22312;&#22823;$\lambda$&#33539;&#22260;&#20869;$f(\cdot) = \|\cdot\|_1$&#21644;$f(\cdot) = \|\cdot\|_\infty$&#30340;&#20998;&#31867;&#38169;&#35823;&#65292;&#24182;&#19988;&#27880;&#24847;&#21040;&#36890;&#24120;&#21487;&#20197;&#25214;&#21040;&#31232;&#30095;&#21644;&#19968;&#20301;&#35299;&#20915;&#26041;&#26696;&#65292;&#20998;&#21035;&#34920;&#29616;&#20960;&#20046;&#19982;$f(\cdot) = \|\cdot\|^2_2$&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10474v1 Announce Type: new  Abstract: We study the use of linear regression for multiclass classification in the over-parametrized regime where some of the training data is mislabeled. In such scenarios it is necessary to add an explicit regularization term, $\lambda f(w)$, for some convex function $f(\cdot)$, to avoid overfitting the mislabeled data. In our analysis, we assume that the data is sampled from a Gaussian Mixture Model with equal class sizes, and that a proportion $c$ of the training labels is corrupted for each class. Under these assumptions, we prove that the best classification performance is achieved when $f(\cdot) = \|\cdot\|^2_2$ and $\lambda \to \infty$. We then proceed to analyze the classification errors for $f(\cdot) = \|\cdot\|_1$ and $f(\cdot) = \|\cdot\|_\infty$ in the large $\lambda$ regime and notice that it is often possible to find sparse and one-bit solutions, respectively, that perform almost as well as the one corresponding to $f(\cdot) = \|\
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#20449;&#24687;&#29942;&#39048;(IB)&#26041;&#27861;&#21644;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;(LDP)&#30456;&#32467;&#21512;&#30340;&#20449;&#24687;&#28151;&#28102;&#26041;&#27861;&#65292;&#20197;&#22635;&#34917;&#38544;&#31169;&#19982;&#20844;&#24179;&#20043;&#38388;&#20851;&#31995;&#30340;&#29702;&#35770;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.10473</link><description>&lt;p&gt;
&#38544;&#31169;&#19982;&#20844;&#24179;&#65306;&#20449;&#24687;&#28151;&#28102;&#29992;&#20110;&#24102;&#26377;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#30340;&#20844;&#24179;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Privacy for Fairness: Information Obfuscation for Fair Representation Learning with Local Differential Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10473
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#20449;&#24687;&#29942;&#39048;(IB)&#26041;&#27861;&#21644;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;(LDP)&#30456;&#32467;&#21512;&#30340;&#20449;&#24687;&#28151;&#28102;&#26041;&#27861;&#65292;&#20197;&#22635;&#34917;&#38544;&#31169;&#19982;&#20844;&#24179;&#20043;&#38388;&#20851;&#31995;&#30340;&#29702;&#35770;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#22312;&#20154;&#31867;&#20013;&#24515;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#31639;&#27861;&#20844;&#24179;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#26085;&#30410;&#21463;&#21040;&#37325;&#35270;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#22312;&#20844;&#24179;&#24615;&#26426;&#22120;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#20449;&#24687;&#29942;&#39048;&#65288;IB&#65289;&#26041;&#27861;&#21644;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#30456;&#32467;&#21512;&#30340;&#20449;&#24687;&#28151;&#28102;&#26041;&#27861;&#65292;&#20197;&#29702;&#35770;&#26694;&#26550;&#20840;&#38754;&#32771;&#23519;&#38544;&#31169;&#19982;&#20844;&#24179;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10473v1 Announce Type: new  Abstract: As machine learning (ML) becomes more prevalent in human-centric applications, there is a growing emphasis on algorithmic fairness and privacy protection. While previous research has explored these areas as separate objectives, there is a growing recognition of the complex relationship between privacy and fairness. However, previous works have primarily focused on examining the interplay between privacy and fairness through empirical investigations, with limited attention given to theoretical exploration. This study aims to bridge this gap by introducing a theoretical framework that enables a comprehensive examination of their interrelation. We shall develop and analyze an information bottleneck (IB) based information obfuscation method with local differential privacy (LDP) for fair representation learning. In contrast to many empirical studies on fairness in ML, we show that the incorporation of LDP randomizers during the encoding proce
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#34920;&#26126;&#21508;&#31181;&#23545;&#25239;&#24615;&#25200;&#21160;&#65288;&#29978;&#33267;&#26159;&#20960;&#20010;&#20687;&#32032;&#30340;&#25200;&#21160;&#65289;&#21253;&#21547;&#36275;&#22815;&#30340;&#31867;&#29305;&#24449;&#29992;&#20110;&#27867;&#21270;&#65292;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#20174;&#25200;&#21160;&#20013;&#23398;&#20064;&#26102;&#30340;&#20915;&#31574;&#36793;&#30028;</title><link>https://arxiv.org/abs/2402.10470</link><description>&lt;p&gt;
&#23545;&#20174;&#23545;&#25239;&#24615;&#25200;&#21160;&#20013;&#23398;&#20064;&#30340;&#29702;&#35770;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Theoretical Understanding of Learning from Adversarial Perturbations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10470
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#34920;&#26126;&#21508;&#31181;&#23545;&#25239;&#24615;&#25200;&#21160;&#65288;&#29978;&#33267;&#26159;&#20960;&#20010;&#20687;&#32032;&#30340;&#25200;&#21160;&#65289;&#21253;&#21547;&#36275;&#22815;&#30340;&#31867;&#29305;&#24449;&#29992;&#20110;&#27867;&#21270;&#65292;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#20174;&#25200;&#21160;&#20013;&#23398;&#20064;&#26102;&#30340;&#20915;&#31574;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#20026;&#20160;&#20040;&#23545;&#25239;&#24615;&#31034;&#20363;&#21487;&#20197;&#27450;&#39575;&#31070;&#32463;&#32593;&#32476;&#24182;&#22312;&#19981;&#21516;&#32593;&#32476;&#20043;&#38388;&#20256;&#36882;&#12290;&#20026;&#20102;&#38416;&#26126;&#36825;&#19968;&#28857;&#65292;&#20960;&#39033;&#30740;&#31350;&#20551;&#35774;&#65292;&#23613;&#31649;&#23545;&#25239;&#24615;&#25200;&#21160;&#30475;&#20284;&#26159;&#22122;&#38899;&#65292;&#20294;&#23454;&#38469;&#19978;&#21253;&#21547;&#31867;&#29305;&#24449;&#12290;&#36825;&#24471;&#21040;&#20102;&#36890;&#36807;&#23454;&#35777;&#35777;&#25454;&#25903;&#25345;&#65292;&#21363;&#23545;&#20110;&#22312;&#38169;&#35823;&#26631;&#35760;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#19978;&#35757;&#32451;&#30340;&#32593;&#32476;&#20173;&#28982;&#21487;&#20197;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#27491;&#30830;&#26631;&#35760;&#30340;&#27979;&#35797;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25200;&#21160;&#22914;&#20309;&#21253;&#21547;&#31867;&#29305;&#24449;&#24182;&#20419;&#36827;&#27867;&#21270;&#30340;&#29702;&#35770;&#29702;&#35299;&#26159;&#26377;&#38480;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#36890;&#36807;&#22312;&#30456;&#20114;&#27491;&#20132;&#26679;&#26412;&#19978;&#35757;&#32451;&#30340;&#21333;&#38544;&#34255;&#23618;&#32593;&#32476;&#20174;&#25200;&#21160;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#21508;&#31181;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#29978;&#33267;&#26159;&#20960;&#20010;&#20687;&#32032;&#30340;&#25200;&#21160;&#65292;&#22343;&#21253;&#21547;&#36275;&#22815;&#30340;&#31867;&#29305;&#24449;&#29992;&#20110;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20174;&#25200;&#21160;&#23398;&#20064;&#26102;&#30340;&#20915;&#31574;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10470v1 Announce Type: new  Abstract: It is not fully understood why adversarial examples can deceive neural networks and transfer between different networks. To elucidate this, several studies have hypothesized that adversarial perturbations, while appearing as noises, contain class features. This is supported by empirical evidence showing that networks trained on mislabeled adversarial examples can still generalize well to correctly labeled test samples. However, a theoretical understanding of how perturbations include class features and contribute to generalization is limited. In this study, we provide a theoretical framework for understanding learning from perturbations using a one-hidden-layer network trained on mutually orthogonal samples. Our results highlight that various adversarial perturbations, even perturbations of a few pixels, contain sufficient class features for generalization. Moreover, we reveal that the decision boundary when learning from perturbations m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#23545;&#25239;&#35838;&#31243;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;ACGCL&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#25104;&#23545;&#22686;&#24378;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#30456;&#20284;&#24615;&#30340;&#22270;&#32423;&#27491;&#36127;&#26679;&#26412;&#65292;&#21516;&#26102;&#36890;&#36807;&#23376;&#22270;&#23545;&#27604;&#23398;&#20064;&#26469;&#35782;&#21035;&#26377;&#25928;&#30340;&#22270;&#27169;&#24335;</title><link>https://arxiv.org/abs/2402.10468</link><description>&lt;p&gt;
&#20855;&#26377;&#23545;&#25239;&#35838;&#31243;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#25104;&#23545;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Adversarial Curriculum Graph Contrastive Learning with Pair-wise Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10468
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#23545;&#25239;&#35838;&#31243;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;ACGCL&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#25104;&#23545;&#22686;&#24378;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#30456;&#20284;&#24615;&#30340;&#22270;&#32423;&#27491;&#36127;&#26679;&#26412;&#65292;&#21516;&#26102;&#36890;&#36807;&#23376;&#22270;&#23545;&#27604;&#23398;&#20064;&#26469;&#35782;&#21035;&#26377;&#25928;&#30340;&#22270;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#24050;&#25104;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65306;&#23545;&#25239;&#35838;&#31243;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;ACGCL&#65289;&#65292;&#21033;&#29992;&#25104;&#23545;&#22686;&#24378;&#30340;&#20248;&#28857;&#29983;&#25104;&#20855;&#26377;&#21487;&#25511;&#30456;&#20284;&#24615;&#30340;&#22270;&#32423;&#27491;&#36127;&#26679;&#26412;&#65292;&#20197;&#21450;&#23376;&#22270;&#23545;&#27604;&#23398;&#20064;&#26469;&#35782;&#21035;&#20854;&#20013;&#30340;&#26377;&#25928;&#22270;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10468v1 Announce Type: cross  Abstract: Graph contrastive learning (GCL) has emerged as a pivotal technique in the domain of graph representation learning. A crucial aspect of effective GCL is the caliber of generated positive and negative samples, which is intrinsically dictated by their resemblance to the original data. Nevertheless, precise control over similarity during sample generation presents a formidable challenge, often impeding the effective discovery of representative graph patterns. To address this challenge, we propose an innovative framework: Adversarial Curriculum Graph Contrastive Learning (ACGCL), which capitalizes on the merits of pair-wise augmentation to engender graph-level positive and negative samples with controllable similarity, alongside subgraph contrastive learning to discern effective graph patterns therein. Within the ACGCL framework, we have devised a novel adversarial curriculum training methodology that facilitates progressive learning by se
&lt;/p&gt;</description></item><item><title>FedKit&#26159;&#19968;&#20010;&#19987;&#20026;&#23433;&#21331;&#21644;iOS&#35774;&#22791;&#19978;&#30340;&#36328;&#24179;&#21488;&#32852;&#37030;&#23398;&#20064;&#30740;&#31350;&#35774;&#35745;&#30340;&#31995;&#32479;&#65292;&#25903;&#25345;&#27169;&#22411;&#36716;&#25442;&#12289;&#30828;&#20214;&#21152;&#36895;&#35757;&#32451;&#21644;&#36328;&#24179;&#21488;&#27169;&#22411;&#32858;&#21512;&#65292;&#20197;&#20419;&#36827;&#25345;&#32493;&#27169;&#22411;&#20132;&#20184;&#21644;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.10464</link><description>&lt;p&gt;
FedKit&#65306;&#23454;&#29616;&#23433;&#21331;&#21644;iOS&#24179;&#21488;&#19978;&#30340;&#36328;&#24179;&#21488;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedKit: Enabling Cross-Platform Federated Learning for Android and iOS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10464
&lt;/p&gt;
&lt;p&gt;
FedKit&#26159;&#19968;&#20010;&#19987;&#20026;&#23433;&#21331;&#21644;iOS&#35774;&#22791;&#19978;&#30340;&#36328;&#24179;&#21488;&#32852;&#37030;&#23398;&#20064;&#30740;&#31350;&#35774;&#35745;&#30340;&#31995;&#32479;&#65292;&#25903;&#25345;&#27169;&#22411;&#36716;&#25442;&#12289;&#30828;&#20214;&#21152;&#36895;&#35757;&#32451;&#21644;&#36328;&#24179;&#21488;&#27169;&#22411;&#32858;&#21512;&#65292;&#20197;&#20419;&#36827;&#25345;&#32493;&#27169;&#22411;&#20132;&#20184;&#21644;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;FedKit&#65292;&#19968;&#20010;&#19987;&#20026;&#23433;&#21331;&#21644;iOS&#35774;&#22791;&#19978;&#30340;&#36328;&#24179;&#21488;&#32852;&#37030;&#23398;&#20064;(FL)&#30740;&#31350;&#37327;&#36523;&#23450;&#21046;&#30340;FL&#31995;&#32479;&#12290;FedKit&#36890;&#36807;&#23454;&#29616;&#27169;&#22411;&#36716;&#25442;&#12289;&#30828;&#20214;&#21152;&#36895;&#35757;&#32451;&#21644;&#36328;&#24179;&#21488;&#27169;&#22411;&#32858;&#21512;&#65292;&#25512;&#21160;&#20102;&#36328;&#24179;&#21488;FL&#24320;&#21457;&#12290;&#25105;&#20204;&#30340;FL&#24037;&#20316;&#27969;&#25903;&#25345;&#29983;&#20135;&#20013;&#28789;&#27963;&#30340;&#26426;&#22120;&#23398;&#20064;&#25805;&#20316;(MLOps)&#65292;&#20419;&#36827;&#25345;&#32493;&#27169;&#22411;&#20132;&#20184;&#21644;&#35757;&#32451;&#12290;&#25105;&#20204;&#24050;&#22312;&#22823;&#23398;&#26657;&#22253;&#30340;&#20581;&#24247;&#25968;&#25454;&#20998;&#26512;&#23454;&#38469;&#29992;&#20363;&#20013;&#37096;&#32626;&#20102;FedKit&#65292;&#23637;&#31034;&#20102;&#20854;&#25928;&#26524;&#12290;FedKit&#26159;&#24320;&#28304;&#30340;&#65292;&#32593;&#22336;&#20026;https://github.com/FedCampus/FedKit&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10464v1 Announce Type: new  Abstract: We present FedKit, a federated learning (FL) system tailored for cross-platform FL research on Android and iOS devices. FedKit pipelines cross-platform FL development by enabling model conversion, hardware-accelerated training, and cross-platform model aggregation. Our FL workflow supports flexible machine learning operations (MLOps) in production, facilitating continuous model delivery and training. We have deployed FedKit in a real-world use case for health data analysis on university campuses, demonstrating its effectiveness. FedKit is open-source at https://github.com/FedCampus/FedKit.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QDyLoRA&#30340;&#39640;&#25928;&#37327;&#21270;&#21160;&#24577;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#23450;&#20041;&#31209;&#19978;&#23454;&#29616;&#26377;&#25928;&#24494;&#35843;&#65292;&#19982;QLoRA&#30456;&#31454;&#20105;&#65292;&#24182;&#19988;&#22312;&#37319;&#29992;&#20854;&#26368;&#20339;&#31209;&#26102;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.10462</link><description>&lt;p&gt;
QDyLoRA: &#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#20248;&#30340;&#37327;&#21270;&#21160;&#24577;&#20302;&#31209;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QDyLoRA&#30340;&#39640;&#25928;&#37327;&#21270;&#21160;&#24577;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#23450;&#20041;&#31209;&#19978;&#23454;&#29616;&#26377;&#25928;&#24494;&#35843;&#65292;&#19982;QLoRA&#30456;&#31454;&#20105;&#65292;&#24182;&#19988;&#22312;&#37319;&#29992;&#20854;&#26368;&#20339;&#31209;&#26102;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Finetuning&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#24040;&#22823;&#30340;GPU&#20869;&#23384;&#65292;&#38480;&#21046;&#20102;&#33719;&#21462;&#26356;&#22823;&#27169;&#22411;&#30340;&#36873;&#25321;&#12290;&#34429;&#28982;&#21629;&#21517;&#20026;QLoRA&#30340;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#30340;&#37327;&#21270;&#29256;&#26412;&#26174;&#33879;&#32531;&#35299;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#26159;&#25214;&#21040;&#39640;&#25928;&#30340;LoRA&#31209;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;QLoRA&#26159;&#22312;&#39044;&#23450;&#20041;&#30340;&#31209;&#19978;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#65292;&#22312;&#19981;&#38656;&#35201;&#36827;&#19968;&#27493;&#24494;&#35843;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#37325;&#26032;&#37197;&#32622;&#20026;&#20854;&#36739;&#20302;&#30340;&#31209;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;QDyLoRA-Quantized Dynamic Low-Rank Adaptation-&#65292;&#20316;&#20026;&#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#20302;&#31209;&#36866;&#24212;&#30340;&#39640;&#25928;&#37327;&#21270;&#26041;&#27861;&#12290;&#21463;Dynamic LoRA&#30340;&#21551;&#21457;&#65292;QDyLoRA&#33021;&#22815;&#22312;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;LoRA&#31209;&#19978;&#26377;&#25928;&#22320;&#24494;&#35843;LLMs&#12290;&#36890;&#36807;&#19968;&#36718;&#24494;&#35843;&#65292;QDyLoRA&#33021;&#22815;&#22312;&#21333;&#20010;32 GB V100-GPU&#19978;&#20026;1&#21040;64&#20010;&#31209;&#30340;Falcon-40b&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;QDyLoRA&#19982;QLoRA&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#22312;&#20351;&#29992;&#20854;&#26368;&#20339;&#31209;&#26102;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10462v1 Announce Type: cross  Abstract: Finetuning large language models requires huge GPU memory, restricting the choice to acquire Larger models. While the quantized version of the Low-Rank Adaptation technique, named QLoRA, significantly alleviates this issue, finding the efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a pre-defined rank and, therefore, cannot be reconfigured for its lower ranks without requiring further fine-tuning steps. This paper proposes QDyLoRA -Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach for dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to efficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables fine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one round of fine-tuning. Experimental results show that QDyLoRA is competitive to QLoRA and outperforms when employing its optimal rank.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#24314;&#35758;&#19982;&#36339;&#34920;&#35774;&#35745;&#25972;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22686;&#24378;&#22411;&#36339;&#34920;&#65292;&#33021;&#22815;&#23454;&#29616;&#26368;&#20248;&#26399;&#26395;&#25628;&#32034;&#26102;&#38388;&#65292;&#22312;&#22788;&#29702;&#25628;&#32034;&#26597;&#35810;&#26102;&#20855;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.10457</link><description>&lt;p&gt;
&#23398;&#20064;&#22686;&#24378;&#22411;&#36339;&#34920;
&lt;/p&gt;
&lt;p&gt;
Learning-Augmented Skip Lists
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10457
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#24314;&#35758;&#19982;&#36339;&#34920;&#35774;&#35745;&#25972;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22686;&#24378;&#22411;&#36339;&#34920;&#65292;&#33021;&#22815;&#23454;&#29616;&#26368;&#20248;&#26399;&#26395;&#25628;&#32034;&#26102;&#38388;&#65292;&#22312;&#22788;&#29702;&#25628;&#32034;&#26597;&#35810;&#26102;&#20855;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#24314;&#35758;&#25972;&#21512;&#21040;&#36339;&#34920;&#35774;&#35745;&#20013;&#65292;&#20197;&#25913;&#36827;&#20256;&#32479;&#25968;&#25454;&#32467;&#26500;&#35774;&#35745;&#12290;&#36890;&#36807;&#35775;&#38382;&#21487;&#33021;&#26377;&#35823;&#30340;&#39044;&#27979;&#20998;&#25968;&#39057;&#29575;&#30340;&#39044;&#27979;&#20540;&#30340;&#39044;&#35328;&#31070;&#35861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#36339;&#34920;&#65292;&#21487;&#20197;&#35777;&#26126;&#25552;&#20379;&#26368;&#20339;&#30340;&#26399;&#26395;&#25628;&#32034;&#26102;&#38388;&#65292;&#20960;&#20046;&#26377;&#20108;&#20493;&#30340;&#20248;&#21183;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#30340;&#23398;&#20064;&#22686;&#24378;&#22411;&#36339;&#34920;&#20173;&#28982;&#26159;&#26368;&#20339;&#30340;&#65292;&#21363;&#20351;&#31070;&#35861;&#21482;&#22312;&#24120;&#25968;&#22240;&#23376;&#20869;&#20934;&#30830;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;&#25628;&#32034;&#26597;&#35810;&#36981;&#24490;&#26222;&#36941;&#23384;&#22312;&#30340;Zipfian&#20998;&#24067;&#65292;&#37027;&#20040;&#25105;&#20204;&#30340;&#36339;&#34920;&#23545;&#20110;&#19968;&#20010;&#39033;&#30446;&#30340;&#26399;&#26395;&#25628;&#32034;&#26102;&#38388;&#20165;&#20026;&#19968;&#20010;&#24120;&#25968;&#65292;&#19982;&#39033;&#30446;&#24635;&#25968;n&#26080;&#20851;&#65292;&#21363;O(1)&#65292;&#32780;&#20256;&#32479;&#30340;&#36339;&#34920;&#30340;&#26399;&#26395;&#25628;&#32034;&#26102;&#38388;&#20026;O(log n)&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#32467;&#26500;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23637;&#31034;&#25105;&#20204;&#30340;&#25968;&#25454;&#32467;&#26500;&#23454;&#29616;&#20102;&#19968;&#20010;&#26399;&#26395;&#25628;&#32034;&#26102;&#38388;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10457v1 Announce Type: cross  Abstract: We study the integration of machine learning advice into the design of skip lists to improve upon traditional data structure design. Given access to a possibly erroneous oracle that outputs estimated fractional frequencies for search queries on a set of items, we construct a skip list that provably provides the optimal expected search time, within nearly a factor of two. In fact, our learning-augmented skip list is still optimal up to a constant factor, even if the oracle is only accurate within a constant factor. We show that if the search queries follow the ubiquitous Zipfian distribution, then the expected search time for an item by our skip list is only a constant, independent of the total number $n$ of items, i.e., $\mathcal{O}(1)$, whereas a traditional skip list will have an expected search time of $\mathcal{O}(\log n)$. We also demonstrate robustness by showing that our data structure achieves an expected search time that is wi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;POTNet&#30340;&#29983;&#25104;&#24314;&#27169;&#32593;&#32476;&#65292;&#22522;&#20110;&#36793;&#32536;&#24809;&#32602;&#30340;Wasserstein&#25439;&#22833;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#24314;&#27169;&#21516;&#26102;&#21253;&#21547;&#20998;&#31867;&#21644;&#36830;&#32493;&#29305;&#24449;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.10456</link><description>&lt;p&gt;
&#36890;&#36807;&#24809;&#32602;&#26368;&#20248;&#36755;&#36816;&#32593;&#32476;&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling for Tabular Data via Penalized Optimal Transport Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10456
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;POTNet&#30340;&#29983;&#25104;&#24314;&#27169;&#32593;&#32476;&#65292;&#22522;&#20110;&#36793;&#32536;&#24809;&#32602;&#30340;Wasserstein&#25439;&#22833;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#24314;&#27169;&#21516;&#26102;&#21253;&#21547;&#20998;&#31867;&#21644;&#36830;&#32493;&#29305;&#24449;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#23398;&#20064;&#34920;&#26684;&#25968;&#25454;&#20013;&#34892;&#30340;&#27010;&#29575;&#20998;&#24067;&#24182;&#29983;&#25104;&#30495;&#23454;&#30340;&#21512;&#25104;&#26679;&#26412;&#30340;&#20219;&#21153;&#26082;&#20851;&#38190;&#21448;&#38750;&#24179;&#20961;&#12290;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(WGAN)&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#35299;&#20915;&#20102;&#20854;&#21069;&#36523;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#34920;&#26684;&#25968;&#25454;&#20013;&#23384;&#22312;&#28151;&#21512;&#25968;&#25454;&#31867;&#22411;&#21644;&#22810;&#27169;&#24577;&#24615;&#65292;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#20043;&#38388;&#30340;&#24494;&#22937;&#24179;&#34913;&#20197;&#21450;Wasserstein&#36317;&#31163;&#22312;&#39640;&#32500;&#24230;&#20013;&#30340;&#22266;&#26377;&#19981;&#31283;&#23450;&#24615;&#65292;WGAN&#36890;&#24120;&#26080;&#27861;&#29983;&#25104;&#39640;&#20445;&#30495;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;POTNet&#65288;&#24809;&#32602;&#26368;&#20248;&#36755;&#36816;&#32593;&#32476;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#12289;&#24378;&#22823;&#19988;&#21487;&#35299;&#37322;&#30340;&#36793;&#38469;&#24809;&#32602;Wasserstein&#65288;MPW&#65289;&#25439;&#22833;&#30340;&#29983;&#25104;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;POTNet&#33021;&#22815;&#26377;&#25928;&#22320;&#24314;&#27169;&#21253;&#21547;&#20998;&#31867;&#21644;&#36830;&#32493;&#29305;&#24449;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10456v1 Announce Type: cross  Abstract: The task of precisely learning the probability distribution of rows within tabular data and producing authentic synthetic samples is both crucial and non-trivial. Wasserstein generative adversarial network (WGAN) marks a notable improvement in generative modeling, addressing the challenges faced by its predecessor, generative adversarial network. However, due to the mixed data types and multimodalities prevalent in tabular data, the delicate equilibrium between the generator and discriminator, as well as the inherent instability of Wasserstein distance in high dimensions, WGAN often fails to produce high-fidelity samples. To this end, we propose POTNet (Penalized Optimal Transport Network), a generative deep neural network based on a novel, robust, and interpretable marginally-penalized Wasserstein (MPW) loss. POTNet can effectively model tabular data containing both categorical and continuous features. Moreover, it offers the flexibil
&lt;/p&gt;</description></item><item><title>&#23558;&#26102;&#38388;&#21160;&#20316;&#25277;&#35937;&#35270;&#20026;&#24207;&#21015;&#21387;&#32553;&#38382;&#39064;&#65292;&#20351;&#29992;Primitive Sequence Encoding (PRISE)&#26041;&#27861;&#32467;&#21512;&#36830;&#32493;&#21160;&#20316;&#37327;&#21270;&#19982;BPE&#26469;&#23398;&#20064;&#24378;&#22823;&#30340;&#21160;&#20316;&#25277;&#35937;&#65292;&#24182;&#22312;&#22810;&#20219;&#21153;&#27169;&#20223;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;</title><link>https://arxiv.org/abs/2402.10450</link><description>&lt;p&gt;
PRISE&#65306;&#23558;&#26102;&#38388;&#21160;&#20316;&#25277;&#35937;&#35270;&#20026;&#24207;&#21015;&#21387;&#32553;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
PRISE: Learning Temporal Action Abstractions as a Sequence Compression Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10450
&lt;/p&gt;
&lt;p&gt;
&#23558;&#26102;&#38388;&#21160;&#20316;&#25277;&#35937;&#35270;&#20026;&#24207;&#21015;&#21387;&#32553;&#38382;&#39064;&#65292;&#20351;&#29992;Primitive Sequence Encoding (PRISE)&#26041;&#27861;&#32467;&#21512;&#36830;&#32493;&#21160;&#20316;&#37327;&#21270;&#19982;BPE&#26469;&#23398;&#20064;&#24378;&#22823;&#30340;&#21160;&#20316;&#25277;&#35937;&#65292;&#24182;&#22312;&#22810;&#20219;&#21153;&#27169;&#20223;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#21160;&#20316;&#25277;&#35937;&#20197;&#21450;&#20449;&#24565;&#29366;&#24577;&#34920;&#31034;&#26159;&#24207;&#36143;&#20915;&#31574;&#20013;&#30340;&#24378;&#22823;&#30693;&#35782;&#20849;&#20139;&#26426;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35266;&#28857;&#65292;&#23558;&#35825;&#23548;&#26102;&#38388;&#21160;&#20316;&#25277;&#35937;&#35270;&#20026;&#24207;&#21015;&#21387;&#32553;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;LLM&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#19968;&#20010;&#24494;&#22937;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998; -- &#36755;&#20837;&#26631;&#35760;&#21270;&#36890;&#36807;&#23383;&#33410;&#23545;&#32534;&#30721;&#65288;BPE&#65289; -- &#24102;&#21040;&#20102;&#36830;&#32493;&#25511;&#21046;&#39046;&#22495;&#20013;&#23398;&#20064;&#21487;&#21464;&#26102;&#38388;&#36328;&#24230;&#25216;&#33021;&#30340; seemingly distant &#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#31216;&#20026;Primitive Sequence Encoding&#65288;PRISE&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#36830;&#32493;&#21160;&#20316;&#37327;&#21270;&#19982;BPE&#30456;&#32467;&#21512;&#65292;&#23398;&#20064;&#24378;&#22823;&#30340;&#21160;&#20316;&#25277;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;PRISE&#20174;&#19968;&#32452;&#26426;&#22120;&#20154;&#25805;&#20316;&#28436;&#31034;&#20013;&#21457;&#29616;&#30340;&#39640;&#32423;&#25216;&#33021;&#26174;&#33879;&#25552;&#21319;&#20102;&#22810;&#20219;&#21153;&#27169;&#20223;&#23398;&#20064;&#20197;&#21450;&#22312;&#26410;&#35265;&#20219;&#21153;&#19978;&#30340;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312; https: &#25918;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10450v1 Announce Type: new  Abstract: Temporal action abstractions, along with belief state representations, are a powerful knowledge sharing mechanism for sequential decision making. In this work, we propose a novel view that treats inducing temporal action abstractions as a sequence compression problem. To do so, we bring a subtle but critical component of LLM training pipelines -- input tokenization via byte pair encoding (BPE) -- to the seemingly distant task of learning skills of variable time span in continuous control domains. We introduce an approach called Primitive Sequence Encoding (PRISE) that combines continuous action quantization with BPE to learn powerful action abstractions. We empirically show that high-level skills discovered by PRISE from a multitask set of robotic manipulation demonstrations significantly boost the performance of both multitask imitation learning as well as few-shot imitation learning on unseen tasks. Our code will be released at https:/
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IS3&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22686;&#37327;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#20013;&#30340;E2O&#21644;O2E&#20004;&#31181;&#37325;&#35201;&#30340;&#35821;&#20041;&#36716;&#21464;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#32500;&#25345;&#23545;&#26087;&#23454;&#20307;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10447</link><description>&lt;p&gt;
&#22686;&#37327;&#24207;&#21015;&#26631;&#35760;&#65306;&#20004;&#31181;&#36716;&#21464;&#30340;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Incremental Sequence Labeling: A Tale of Two Shifts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10447
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IS3&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22686;&#37327;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#20013;&#30340;E2O&#21644;O2E&#20004;&#31181;&#37325;&#35201;&#30340;&#35821;&#20041;&#36716;&#21464;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#32500;&#25345;&#23545;&#26087;&#23454;&#20307;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#28041;&#21450;&#22312;&#20445;&#30041;&#23545;&#20808;&#21069;&#31867;&#21035;&#30693;&#35782;&#30340;&#21516;&#26102;&#65292;&#38543;&#26102;&#38388;&#19981;&#26029;&#23398;&#20064;&#26032;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30830;&#23450;&#20102;&#20004;&#31181;&#37325;&#35201;&#30340;&#35821;&#20041;&#36716;&#21464;&#65306;E2O&#65288;&#27169;&#22411;&#23558;&#26087;&#23454;&#20307;&#38169;&#35823;&#26631;&#35760;&#20026;&#38750;&#23454;&#20307;&#65289;&#21644;O2E&#65288;&#27169;&#22411;&#23558;&#38750;&#23454;&#20307;&#25110;&#26087;&#23454;&#20307;&#26631;&#35760;&#20026;&#26032;&#23454;&#20307;&#65289;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#20915;E2O&#38382;&#39064;&#19978;&#65292;&#24573;&#35270;&#20102;O2E&#38382;&#39064;&#12290;&#36825;&#31181;&#24573;&#30053;&#23548;&#33268;&#27169;&#22411;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#23545;&#26032;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#26102;&#23384;&#22312;&#20559;&#35265;&#65292;&#35748;&#20026;&#23427;&#20204;&#23646;&#20110;&#26032;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#26080;&#35821;&#20041;&#36716;&#21464;&#30340;&#22686;&#37327;&#39034;&#24207;&#26631;&#35760;&#65288;IS3&#65289;&#12290;&#21463;&#21040;&#24050;&#30830;&#23450;&#30340;&#35821;&#20041;&#36716;&#21464;&#65288;E2O&#21644;O2E&#65289;&#30340;&#21551;&#21457;&#65292;IS3&#26088;&#22312;&#32531;&#35299;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#33267;&#20110;E2O&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#32500;&#25345;&#27169;&#22411;&#23545;&#26087;&#23454;&#20307;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10447v1 Announce Type: new  Abstract: The incremental sequence labeling task involves continuously learning new classes over time while retaining knowledge of the previous ones. Our investigation identifies two significant semantic shifts: E2O (where the model mislabels an old entity as a non-entity) and O2E (where the model labels a non-entity or old entity as a new entity). Previous research has predominantly focused on addressing the E2O problem, neglecting the O2E issue. This negligence results in a model bias towards classifying new data samples as belonging to the new class during the learning process. To address these challenges, we propose a novel framework, Incremental Sequential Labeling without Semantic Shifts (IS3). Motivated by the identified semantic shifts (E2O and O2E), IS3 aims to mitigate catastrophic forgetting in models. As for the E2O problem, we use knowledge distillation to maintain the model's discriminative ability for old entities. Simultaneously, t
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#26631;&#27880;&#20989;&#25968;&#30340;&#21327;&#20316;&#23398;&#20064;&#20013;&#65292;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#22312;&#22686;&#24378;&#20551;&#35774;&#31867;&#19978;&#30340;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10445</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21516;&#26631;&#27880;&#20989;&#25968;&#30340;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Collaborative Learning with Different Labeling Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10445
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#26631;&#27880;&#20989;&#25968;&#30340;&#21327;&#20316;&#23398;&#20064;&#20013;&#65292;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#22312;&#22686;&#24378;&#20551;&#35774;&#31867;&#19978;&#30340;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181; Collaborative PAC Learning &#30340;&#21464;&#20307;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#26088;&#22312;&#23398;&#20064;&#27599;&#20010;$n$&#20010;&#25968;&#25454;&#20998;&#24067;&#30340;&#20934;&#30830;&#20998;&#31867;&#22120;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20174;&#23427;&#20204;&#24635;&#20849;&#25277;&#21462;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#19982;&#36890;&#24120;&#30340;&#21327;&#20316;&#23398;&#20064;&#35774;&#32622;&#19981;&#21516;&#65292;&#19981;&#20551;&#35774;&#23384;&#22312;&#19968;&#20010;&#21516;&#26102;&#23545;&#25152;&#26377;&#20998;&#24067;&#20934;&#30830;&#30340;&#21333;&#19968;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#25968;&#25454;&#20998;&#24067;&#28385;&#36275;&#36739;&#24369;&#30340;&#21487;&#23454;&#29616;&#24615;&#20551;&#35774;&#26102;&#65292;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#20551;&#35774;&#31867;&#30340;&#19968;&#20010;&#33258;&#28982;&#22686;&#24378;&#65292;&#20998;&#26512;&#20381;&#36182;&#20110;&#23545;&#35813;&#22686;&#24378;&#31867;&#30340;VC&#32500;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10445v1 Announce Type: new  Abstract: We study a variant of Collaborative PAC Learning, in which we aim to learn an accurate classifier for each of the $n$ data distributions, while minimizing the number of samples drawn from them in total. Unlike in the usual collaborative learning setup, it is not assumed that there exists a single classifier that is simultaneously accurate for all distributions.   We show that, when the data distributions satisfy a weaker realizability assumption, sample-efficient learning is still feasible. We give a learning algorithm based on Empirical Risk Minimization (ERM) on a natural augmentation of the hypothesis class, and the analysis relies on an upper bound on the VC dimension of this augmented class.   In terms of the computational efficiency, we show that ERM on the augmented hypothesis class is NP-hard, which gives evidence against the existence of computationally efficient learners in general. On the positive side, for two special cases, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20449;&#24687;&#29702;&#35770;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24191;&#65292;&#24635;&#32467;&#26368;&#24120;&#29992;&#30340;&#22686;&#24191;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10434</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#23545;&#27604;&#23398;&#20064;&#30340;&#21442;&#25968;&#22686;&#24191;
&lt;/p&gt;
&lt;p&gt;
Parametric Augmentation for Time Series Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10434
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#29702;&#35770;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24191;&#65292;&#24635;&#32467;&#26368;&#24120;&#29992;&#30340;&#22686;&#24191;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25216;&#26415;&#22914;&#23545;&#27604;&#23398;&#20064;&#24050;&#32463;&#22312;&#35768;&#22810;&#39046;&#22495;&#24471;&#21040;&#26377;&#25928;&#24212;&#29992;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22270;&#32467;&#26500;&#25968;&#25454;&#12290;&#22312;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#21019;&#24314;&#26377;&#21161;&#20110;&#27169;&#22411;&#23398;&#20064;&#20581;&#22766;&#21644;&#20855;&#26377;&#21306;&#20998;&#24615;&#34920;&#31034;&#30340;&#27491;&#20363;&#26159;&#19968;&#20010;&#20851;&#38190;&#38454;&#27573;&#12290;&#36890;&#24120;&#65292;&#39044;&#35774;&#30340;&#20154;&#31867;&#30452;&#35273;&#25351;&#23548;&#30456;&#20851;&#25968;&#25454;&#22686;&#24191;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20154;&#31867;&#23481;&#26131;&#35782;&#21035;&#30340;&#27169;&#24335;&#65292;&#27492;&#32463;&#39564;&#27861;&#21017;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#39046;&#22495;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#35270;&#35273;&#26816;&#26597;&#26102;&#38388;&#32467;&#26500;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#25968;&#25454;&#38598;&#21644;&#23454;&#20363;&#32423;&#21035;&#30340;&#26102;&#38388;&#24207;&#21015;&#22686;&#24191;&#30340;&#22810;&#26679;&#24615;&#20351;&#24471;&#38590;&#20197;&#21363;&#26102;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#22686;&#24191;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20449;&#24687;&#29702;&#35770;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24191;&#65292;&#24182;&#20197;&#32479;&#19968;&#26684;&#24335;&#24635;&#32467;&#26368;&#24120;&#37319;&#29992;&#30340;&#22686;&#24191;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10434v1 Announce Type: new  Abstract: Modern techniques like contrastive learning have been effectively used in many areas, including computer vision, natural language processing, and graph-structured data. Creating positive examples that assist the model in learning robust and discriminative representations is a crucial stage in contrastive learning approaches. Usually, preset human intuition directs the selection of relevant data augmentations. Due to patterns that are easily recognized by humans, this rule of thumb works well in the vision and language domains. However, it is impractical to visually inspect the temporal structures in time series. The diversity of time series augmentations at both the dataset and instance levels makes it difficult to choose meaningful augmentations on the fly. In this study, we address this gap by analyzing time series data augmentation using information theory and summarizing the most commonly adopted augmentations in a unified format. We
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#39044;&#35757;&#32451;&#29983;&#25104;&#37319;&#26679;&#22120;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#32467;&#21512;MD&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#34507;&#30333;&#36136;&#26500;&#35937;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.10433</link><description>&lt;p&gt;
&#23558;&#31070;&#32463;&#19982;&#29289;&#29702;&#34701;&#21512;&#65306;&#29992;&#21487;&#22788;&#29702;&#30340;&#27169;&#25311;&#22686;&#24378;&#34507;&#30333;&#36136;&#26500;&#35937;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Fusing Neural and Physical: Augment Protein Conformation Sampling with Tractable Simulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#39044;&#35757;&#32451;&#29983;&#25104;&#37319;&#26679;&#22120;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#32467;&#21512;MD&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#34507;&#30333;&#36136;&#26500;&#35937;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#21160;&#21147;&#23398;&#23545;&#20110;&#23427;&#20204;&#30340;&#29983;&#29289;&#21151;&#33021;&#21644;&#24615;&#36136;&#38750;&#24120;&#26222;&#36941;&#19988;&#37325;&#35201;&#65292;&#30740;&#31350;&#36890;&#24120;&#28041;&#21450;&#32791;&#26102;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;(MD)&#27169;&#25311;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;&#27169;&#22411;&#34987;&#21033;&#29992;&#20316;&#20026;&#19968;&#20010;&#26367;&#20195;&#37319;&#26679;&#22120;&#65292;&#20197;&#27604;&#20256;&#32479;&#26041;&#27861;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#22320;&#33719;&#24471;&#26500;&#35937;&#38598;&#21512;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#25311;&#25968;&#25454;&#65288;&#8220;&#38646;&#27425;&#25512;&#26029;&#8221;&#65289;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#32771;&#34385;&#24213;&#23618;&#33021;&#37327;&#26223;&#35266;&#65292;&#36825;&#31181;&#29983;&#25104;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#20173;&#28982;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36825;&#31181;&#39044;&#35757;&#32451;&#29983;&#25104;&#37319;&#26679;&#22120;&#30340;&#23569;&#26679;&#26412;&#35774;&#32622;&#65292;&#23427;&#20197;&#19968;&#31181;&#21487;&#22788;&#29702;&#30340;&#26041;&#24335;&#32467;&#21512;&#20102;MD&#27169;&#25311;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#19968;&#20010;&#30446;&#26631;&#34507;&#30333;&#36136;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#39044;&#35757;&#32451;&#37319;&#26679;&#22120;&#20013;&#33719;&#21462;&#19968;&#20123;&#31181;&#23376;&#26500;&#35937;&#65292;&#28982;&#21518;&#20174;&#36825;&#20123;&#31181;&#23376;&#26679;&#26412;&#24320;&#22987;&#36827;&#34892;&#19968;&#31995;&#21015;&#29289;&#29702;&#27169;&#25311;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#36712;&#36857;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10433v1 Announce Type: cross  Abstract: The protein dynamics are common and important for their biological functions and properties, the study of which usually involves time-consuming molecular dynamics (MD) simulations in silico. Recently, generative models has been leveraged as a surrogate sampler to obtain conformation ensembles with orders of magnitude faster and without requiring any simulation data (a "zero-shot" inference). However, being agnostic of the underlying energy landscape, the accuracy of such generative model may still be limited. In this work, we explore the few-shot setting of such pre-trained generative sampler which incorporates MD simulations in a tractable manner. Specifically, given a target protein of interest, we first acquire some seeding conformations from the pre-trained sampler followed by a number of physical simulations in parallel starting from these seeding samples. Then we fine-tuned the generative model using the simulation trajectories a
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22312;&#36125;&#21494;&#26031;&#35774;&#32622;&#20013;&#25506;&#35752;&#20102;&#22266;&#23450;&#32622;&#20449;&#24230;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20256;&#32479;&#39057;&#29575;&#35774;&#23450;&#19979;&#30340;&#31639;&#27861;&#22312;&#27492;&#35774;&#32622;&#19979;&#34920;&#29616;&#27425;&#20248;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#24615;&#33021;&#19982;&#29702;&#35770;&#19979;&#38480;&#30456;&#21305;&#37197;&#30340;&#36830;&#32493;&#25490;&#38500;&#21464;&#31181;&#12290;</title><link>https://arxiv.org/abs/2402.10429</link><description>&lt;p&gt;
&#22312;&#36125;&#21494;&#26031;&#35774;&#32622;&#20013;&#30340;&#22266;&#23450;&#32622;&#20449;&#24230;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Fixed Confidence Best Arm Identification in the Bayesian Setting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10429
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22312;&#36125;&#21494;&#26031;&#35774;&#32622;&#20013;&#25506;&#35752;&#20102;&#22266;&#23450;&#32622;&#20449;&#24230;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20256;&#32479;&#39057;&#29575;&#35774;&#23450;&#19979;&#30340;&#31639;&#27861;&#22312;&#27492;&#35774;&#32622;&#19979;&#34920;&#29616;&#27425;&#20248;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#24615;&#33021;&#19982;&#29702;&#35770;&#19979;&#38480;&#30456;&#21305;&#37197;&#30340;&#36830;&#32493;&#25490;&#38500;&#21464;&#31181;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#36125;&#21494;&#26031;&#35774;&#32622;&#20013;&#30340;&#22266;&#23450;&#32622;&#20449;&#24230;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;FC-BAI&#65289;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#26088;&#22312;&#22312;&#24050;&#30693;&#20808;&#39564;&#37319;&#26679;&#30340;&#24773;&#20917;&#19979;&#20197;&#22266;&#23450;&#32622;&#20449;&#27700;&#24179;&#25214;&#21040;&#22343;&#20540;&#26368;&#22823;&#30340;&#33218;&#12290;&#22823;&#22810;&#25968;&#20851;&#20110;FC-BAI&#38382;&#39064;&#30340;&#30740;&#31350;&#37117;&#26159;&#22312;&#39057;&#29575;&#35774;&#23450;&#20013;&#36827;&#34892;&#30340;&#65292;&#22312;&#35813;&#35774;&#23450;&#19979;&#65292;&#28216;&#25103;&#24320;&#22987;&#21069;&#21363;&#30830;&#23450;&#20102;&#36172;&#21338;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36125;&#21494;&#26031;&#35774;&#32622;&#20013;&#65292;&#20256;&#32479;&#30340;&#22312;&#39057;&#29575;&#35774;&#23450;&#20013;&#30740;&#31350;&#30340;FC-BAI&#31639;&#27861;&#65288;&#22914;track-and-stop&#21644;top-two&#31639;&#27861;&#65289;&#20250;&#23548;&#33268;&#20219;&#24847;&#27425;&#20248;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21516;&#26102;&#35777;&#26126;&#20102;&#22312;&#36125;&#21494;&#26031;&#35774;&#32622;&#19979;&#39044;&#26399;&#26679;&#26412;&#25968;&#30340;&#19979;&#38480;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36830;&#32493;&#25490;&#38500;&#30340;&#21464;&#31181;&#65292;&#20854;&#24615;&#33021;&#19982;&#19979;&#38480;&#30456;&#21305;&#37197;&#65292;&#26368;&#22810;&#24046;&#19968;&#20010;&#23545;&#25968;&#22240;&#23376;&#12290;&#20223;&#30495;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10429v1 Announce Type: cross  Abstract: We consider the fixed-confidence best arm identification (FC-BAI) problem in the Bayesian Setting. This problem aims to find the arm of the largest mean with a fixed confidence level when the bandit model has been sampled from the known prior. Most studies on the FC-BAI problem have been conducted in the frequentist setting, where the bandit model is predetermined before the game starts. We show that the traditional FC-BAI algorithms studied in the frequentist setting, such as track-and-stop and top-two algorithms, result in arbitrary suboptimal performances in the Bayesian setting. We also prove a lower bound of the expected number of samples in the Bayesian setting and introduce a variant of successive elimination that has a matching performance with the lower bound up to a logarithmic factor. Simulations verify the theoretical results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#22522;&#20110;&#22270;&#35889;&#30340;IAC&#20998;&#21106;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;IAC&#21644;ANFs&#39044;&#20808;&#23450;&#20301;&#22312;&#21333;&#20010;&#22270;&#35889;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#20934;&#30830;&#25512;&#26029;ANFs&#30340;&#20301;&#32622;&#12290;</title><link>https://arxiv.org/abs/2402.10425</link><description>&lt;p&gt;
DABS-LS: &#20351;&#29992;&#21306;&#22495;&#27700;&#24179;&#38598;&#33258;&#30417;&#30563;&#30340;&#28145;&#24230;&#22522;&#20110;&#22270;&#35889;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
DABS-LS: Deep Atlas-Based Segmentation Using Regional Level Set Self-Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#22522;&#20110;&#22270;&#35889;&#30340;IAC&#20998;&#21106;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;IAC&#21644;ANFs&#39044;&#20808;&#23450;&#20301;&#22312;&#21333;&#20010;&#22270;&#35889;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#20934;&#30830;&#25512;&#26029;ANFs&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40831;&#21608;&#26893;&#20837;&#29289;&#65288;CIs&#65289;&#26159;&#29992;&#20110;&#27835;&#30103;&#20005;&#37325;&#33267;&#28145;&#24230;&#21548;&#21147;&#20007;&#22833;&#24739;&#32773;&#30340;&#31070;&#32463;&#20551;&#20307;&#12290;&#23545;CIs&#21050;&#28608;&#21548;&#31070;&#32463;&#32420;&#32500;&#65288;ANFs&#65289;&#36827;&#34892;&#24739;&#32773;&#29305;&#24322;&#24615;&#24314;&#27169;&#21487;&#20197;&#24110;&#21161;&#21548;&#21147;&#23398;&#23478;&#25913;&#21892;CI&#32534;&#31243;&#12290;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#23450;&#20301;ANFs&#30456;&#23545;&#20110;&#21608;&#22260;&#35299;&#21078;&#32467;&#26500;&#21644;CI&#30340;&#20301;&#32622;&#12290;&#23450;&#20301;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;ANFs&#38750;&#24120;&#23567;&#65292;&#22312;&#20020;&#24202;&#25104;&#20687;&#20013;&#19981;&#33021;&#30452;&#25509;&#30475;&#21040;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;ANFs&#30340;&#20301;&#32622;&#21487;&#20197;&#20174;&#20869;&#21548;&#36947;&#65288;IAC&#65289;&#30340;&#20301;&#32622;&#20934;&#30830;&#25512;&#26029;&#20986;&#26469;&#65292;&#22312;CT&#20013;&#65292;IAC&#20855;&#26377;&#24456;&#39640;&#30340;&#23545;&#27604;&#24230;&#65292;&#22240;&#20026;ANFs&#22312;&#32819;&#34583;&#21644;&#22823;&#33041;&#20043;&#38388;&#36890;&#36807;&#36825;&#20010;&#31649;&#36947;&#12290;&#21463;VoxelMorph&#21551;&#21457;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#22270;&#35889;&#30340;IAC&#20998;&#21106;&#32593;&#32476;&#12290;&#25105;&#20204;&#21019;&#24314;&#19968;&#20010;&#21333;&#19968;&#30340;&#22270;&#35889;&#65292;&#20854;&#20013;IAC&#21644;ANFs&#24050;&#32463;&#34987;&#39044;&#23450;&#20301;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#34987;&#35757;&#32451;&#20197;&#29983;&#25104;&#23558;&#22352;&#26631;&#20174;&#22270;&#35889;&#26144;&#23556;&#21040;&#26032;&#30446;&#26631;&#30340;&#21464;&#24418;&#22330;&#65288;DFs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10425v1 Announce Type: cross  Abstract: Cochlear implants (CIs) are neural prosthetics used to treat patients with severe-to-profound hearing loss. Patient-specific modeling of CI stimulation of the auditory nerve fiber (ANFs) can help audiologists improve the CI programming. These models require localization of the ANFs relative to surrounding anatomy and the CI. Localization is challenging because the ANFs are so small they are not directly visible in clinical imaging. In this work, we hypothesize the position of the ANFs can be accurately inferred from the location of the internal auditory canal (IAC), which has high contrast in CT, since the ANFs pass through this canal between the cochlea and the brain. Inspired by VoxelMorph, in this paper we propose a deep atlas-based IAC segmentation network. We create a single atlas in which the IAC and ANFs are pre-localized. Our network is trained to produce deformation fields (DFs) mapping coordinates from the atlas to new target
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FEWL&#30340;&#24187;&#35273;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;LLM&#31572;&#26696;&#36827;&#34892;&#21152;&#26435;&#35780;&#20272;&#20107;&#23454;&#24615;&#65292;&#36866;&#29992;&#20110;&#27809;&#26377;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.10412</link><description>&lt;p&gt;
&#36890;&#36807;&#19987;&#23478;&#21152;&#26435;&#26469;&#34913;&#37327;&#21644;&#20943;&#23569;LLM&#22312;&#27809;&#26377;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#24773;&#20917;&#19979;&#30340;&#34394;&#26500;
&lt;/p&gt;
&lt;p&gt;
Measuring and Reducing LLM Hallucination without Gold-Standard Answers via Expertise-Weighting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10412
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FEWL&#30340;&#24187;&#35273;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;LLM&#31572;&#26696;&#36827;&#34892;&#21152;&#26435;&#35780;&#20272;&#20107;&#23454;&#24615;&#65292;&#36866;&#29992;&#20110;&#27809;&#26377;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#24187;&#35273;&#65292;&#21363;&#29983;&#25104;&#20107;&#23454;&#19981;&#27491;&#30830;&#20294;&#30475;&#20284;&#20196;&#20154;&#20449;&#26381;&#30340;&#31572;&#26696;&#65292;&#30446;&#21069;&#26159;LLM&#21487;&#20449;&#24230;&#21644;&#21487;&#38752;&#24615;&#30340;&#20027;&#35201;&#23041;&#32961;&#12290;&#35299;&#20915;&#36825;&#19968;&#22797;&#26434;&#38382;&#39064;&#30340;&#31532;&#19968;&#27493;&#26159;&#23545;&#20854;&#36827;&#34892;&#34913;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24187;&#35273;&#24230;&#37327;&#26631;&#20934;&#38656;&#35201;&#20855;&#26377;&#20855;&#26377;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21363;&#20154;&#31867;&#32534;&#20889;&#30340;&#8220;&#26368;&#20339;&#8221;&#25110;&#8220;&#27491;&#30830;&#8221;&#31572;&#26696;&#12290;&#36825;&#31181;&#35201;&#27714;&#20351;&#24187;&#35273;&#27979;&#37327;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#23481;&#26131;&#20986;&#29616;&#20154;&#20026;&#35823;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#21152;&#26435;LLM&#23545;&#20107;&#23454;&#24615;&#36827;&#34892;&#35780;&#20272;&#65288;FEWL&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#20026;&#37329;&#26631;&#20934;&#31572;&#26696;&#32570;&#22833;&#26102;&#35774;&#35745;&#30340;&#24187;&#35273;&#24230;&#37327;&#26631;&#20934;&#12290;FEWL&#21033;&#29992;&#20102;&#29616;&#25104;&#30340;LLM&#31572;&#26696;&#20316;&#20026;&#40644;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#20195;&#29702;&#12290;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#26377;&#25928;&#22320;&#37327;&#21270;&#21442;&#32771;LLM&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#23637;&#31034;FEWL&#20855;&#26377;&#19968;&#23450;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#23454;&#35777;&#20013;&#35777;&#26126;&#23427;&#26356;&#20934;&#30830;&#12290;&#24230;&#37327;&#34394;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10412v1 Announce Type: cross  Abstract: LLM hallucination, i.e. generating factually incorrect yet seemingly convincing answers, is currently a major threat to the trustworthiness and reliability of LLMs. The first step towards solving this complicated problem is to measure it. However, existing hallucination metrics require to have a benchmark dataset with gold-standard answers, i.e. "best" or "correct" answers written by humans. Such requirement makes hallucination measurement costly and prone to human errors. In this work, we propose Factualness Evaluations via Weighting LLMs (FEWL), the first hallucination metric that is specifically designed for the scenario when gold-standard answers are absent. FEWL leverages the answers from off-the-shelf LLMs that serve as a proxy of gold-standard answers. The key challenge is how to quantify the expertise of reference LLMs resourcefully. We show FEWL has certain theoretical guarantees and demonstrate empirically it gives more accur
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22270;&#32467;&#26500;&#20449;&#24687;&#22312;&#20849;&#31867;&#21035;&#22270;&#19978;&#21033;&#29992;&#22270;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;LLMs&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#24494;&#35843;&#21644;&#38646;-shot/few-shot&#20998;&#31867;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#24369;&#26631;&#31614;&#24494;&#35843;LLMs&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10409</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#34920;&#31034;&#23398;&#20064;&#29702;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#26597;&#35770;&#25991;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10409
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22270;&#32467;&#26500;&#20449;&#24687;&#22312;&#20849;&#31867;&#21035;&#22270;&#19978;&#21033;&#29992;&#22270;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;LLMs&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#24494;&#35843;&#21644;&#38646;-shot/few-shot&#20998;&#31867;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#24369;&#26631;&#31614;&#24494;&#35843;LLMs&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26032;&#30740;&#31350;&#25345;&#32493;&#36827;&#34892;&#65292;&#38590;&#20197;&#36319;&#19978;&#26032;&#30340;&#30740;&#31350;&#21644;&#27169;&#22411;&#12290;&#20026;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#32508;&#21512;&#26032;&#30740;&#31350;&#25104;&#26524;&#65292;&#35768;&#22810;&#20154;&#20889;&#20102;&#35843;&#30740;&#35770;&#25991;&#65292;&#20294;&#21363;&#20351;&#36825;&#20123;&#35770;&#25991;&#20063;&#21464;&#24471;&#36234;&#26469;&#36234;&#22810;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23558;&#35843;&#30740;&#35770;&#25991;&#20998;&#37197;&#21040;&#20998;&#31867;&#27861;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;144&#31687;LLM&#35843;&#30740;&#35770;&#25991;&#30340;&#20803;&#25968;&#25454;&#65292;&#24182;&#25506;&#35752;&#20102;&#19977;&#31181;&#33539;&#20363;&#26469;&#23545;&#20998;&#31867;&#27861;&#20869;&#30340;&#35770;&#25991;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#20849;&#31867;&#21035;&#22270;&#19978;&#21033;&#29992;&#22270;&#32467;&#26500;&#20449;&#24687;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#20004;&#20010;&#33539;&#20363;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;; &#20351;&#29992;LLMs&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#38646;-shot/few-shot&#20998;&#31867;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#27169;&#22411;&#36229;&#36807;&#20102;&#24179;&#22343;&#20154;&#31867;&#35782;&#21035;&#27700;&#24179;&#65292;&#24182;&#19988;&#21033;&#29992;&#36739;&#23567;&#27169;&#22411;&#29983;&#25104;&#30340;&#24369;&#26631;&#31614;&#26469;&#24494;&#35843;LLMs&#65288;&#26412;&#30740;&#31350;&#20013;&#30340;GCN&#31561;&#65289;&#21487;&#33021;&#27604;&#20351;&#29992;&#22320;&#38754;&#23454;&#20917;&#26631;&#31614;&#26356;&#26377;&#25928;&#65292;&#25581;&#31034;&#20102;&#20174;&#24369;&#21040;&#24378;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10409v1 Announce Type: cross  Abstract: As new research on Large Language Models (LLMs) continues, it is difficult to keep up with new research and models. To help researchers synthesize the new research many have written survey papers, but even those have become numerous. In this paper, we develop a method to automatically assign survey papers to a taxonomy. We collect the metadata of 144 LLM survey papers and explore three paradigms to classify papers within the taxonomy. Our work indicates that leveraging graph structure information on co-category graphs can significantly outperform the language models in two paradigms; pre-trained language models' fine-tuning and zero-shot/few-shot classifications using LLMs. We find that our model surpasses an average human recognition level and that fine-tuning LLMs using weak labels generated by a smaller model, such as the GCN in this study, can be more effective than using ground-truth labels, revealing the potential of weak-to-stro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#19977;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#29702;&#35770;&#35265;&#35299;&#21644;&#20998;&#26512;&#32593;&#26684;&#25552;&#21462;&#26041;&#27861;&#65292;&#23558;&#39640;&#32500;&#26354;&#38754;&#36716;&#25442;&#20026;&#24179;&#38754;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36817;&#20284;&#20132;&#28857;&#30340;&#26041;&#27861;&#65292;&#25299;&#23637;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.10403</link><description>&lt;p&gt;
&#20174;&#20998;&#27573;&#19977;&#32447;&#24615;&#32593;&#32476;&#20013;&#23548;&#20986;&#22810;&#38754;&#20307;&#22797;&#21512;&#20307;
&lt;/p&gt;
&lt;p&gt;
Polyhedral Complex Derivation from Piecewise Trilinear Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#19977;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#29702;&#35770;&#35265;&#35299;&#21644;&#20998;&#26512;&#32593;&#26684;&#25552;&#21462;&#26041;&#27861;&#65292;&#23558;&#39640;&#32500;&#26354;&#38754;&#36716;&#25442;&#20026;&#24179;&#38754;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36817;&#20284;&#20132;&#28857;&#30340;&#26041;&#27861;&#65292;&#25299;&#23637;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#35270;&#21270;&#30340;&#36827;&#23637;&#25581;&#31034;&#20102;&#23427;&#20204;&#32467;&#26500;&#30340;&#35265;&#35299;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#36830;&#32493;&#20998;&#27573;&#20223;&#23556;&#65288;CPWA&#65289;&#20989;&#25968;&#20013;&#25552;&#21462;&#32593;&#26684;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31070;&#32463;&#34920;&#38754;&#34920;&#31034;&#23398;&#20064;&#30340;&#21457;&#23637;&#21253;&#25324;&#38750;&#32447;&#24615;&#20301;&#32622;&#32534;&#30721;&#65292;&#35299;&#20915;&#20102;&#35832;&#22914;&#35889;&#20559;&#24046;&#20043;&#31867;&#30340;&#38382;&#39064;&#65307;&#28982;&#32780;&#65292;&#36825;&#22312;&#24212;&#29992;&#22522;&#20110;CPWA&#20989;&#25968;&#30340;&#32593;&#26684;&#25552;&#21462;&#25216;&#26415;&#26041;&#38754;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#19977;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#35265;&#35299;&#21644;&#20998;&#26512;&#30340;&#32593;&#26684;&#25552;&#21462;&#65292;&#23637;&#31034;&#20102;&#22312;&#22855;&#25343;&#23572;&#32422;&#26463;&#19979;&#23558;&#39640;&#32500;&#26354;&#38754;&#36716;&#25442;&#20026;&#19977;&#32447;&#24615;&#21306;&#22495;&#20869;&#30340;&#24179;&#38754;&#30340;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36817;&#20284;&#19977;&#20010;&#39640;&#32500;&#26354;&#38754;&#20043;&#38388;&#30340;&#20132;&#28857;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#27721;&#26126;&#36317;&#31163;&#21644;&#25928;&#29575;&#20197;&#21450;&#35282;&#36317;&#31163;&#26469;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#27491;&#30830;&#24615;&#21644;&#31616;&#27905;&#24615;&#65292;&#21516;&#26102;&#26816;&#26597;&#20102;t&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10403v1 Announce Type: cross  Abstract: Recent advancements in visualizing deep neural networks provide insights into their structures and mesh extraction from Continuous Piecewise Affine (CPWA) functions. Meanwhile, developments in neural surface representation learning incorporate non-linear positional encoding, addressing issues like spectral bias; however, this poses challenges in applying mesh extraction techniques based on CPWA functions. Focusing on trilinear interpolating methods as positional encoding, we present theoretical insights and an analytical mesh extraction, showing the transformation of hypersurfaces to flat planes within the trilinear region under the eikonal constraint. Moreover, we introduce a method for approximating intersecting points among three hypersurfaces contributing to broader applications. We empirically validate correctness and parsimony through chamfer distance and efficiency, and angular distance, while examining the correlation between t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26126;&#30830;&#23450;&#20102;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24037;&#20214;&#21644;&#25351;&#32441;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#35745;&#31639;&#23427;&#20204;&#30340;&#31639;&#27861;&#65292;&#21457;&#29616;&#20351;&#29992;&#35813;&#23450;&#20041;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35782;&#21035;&#28508;&#22312;&#29983;&#25104;&#36807;&#31243;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10401</link><description>&lt;p&gt;
ManiFPT: &#23450;&#20041;&#21644;&#20998;&#26512;&#29983;&#25104;&#27169;&#22411;&#30340;&#25351;&#32441;
&lt;/p&gt;
&lt;p&gt;
ManiFPT: Defining and Analyzing Fingerprints of Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26126;&#30830;&#23450;&#20102;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24037;&#20214;&#21644;&#25351;&#32441;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#35745;&#31639;&#23427;&#20204;&#30340;&#31639;&#27861;&#65292;&#21457;&#29616;&#20351;&#29992;&#35813;&#23450;&#20041;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35782;&#21035;&#28508;&#22312;&#29983;&#25104;&#36807;&#31243;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#30041;&#19979;&#20102;&#23427;&#20204;&#28508;&#22312;&#29983;&#25104;&#36807;&#31243;&#30340;&#30165;&#36857;&#65292;&#24191;&#27867;&#31216;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#25351;&#32441;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#26816;&#27979;&#30495;&#23454;&#22270;&#20687;&#21644;&#21512;&#25104;&#22270;&#20687;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25351;&#32441;&#33021;&#22815;&#21306;&#20998;&#21508;&#31181;&#31867;&#22411;&#30340;&#21512;&#25104;&#22270;&#20687;&#20197;&#21450;&#24110;&#21161;&#35782;&#21035;&#28508;&#22312;&#29983;&#25104;&#36807;&#31243;&#30340;&#31243;&#24230;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#23588;&#20854;&#26159;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25351;&#32441;&#30340;&#23450;&#20041;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26126;&#30830;&#23450;&#20041;&#20102;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24037;&#20214;&#21644;&#25351;&#32441;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#38469;&#35745;&#31639;&#23427;&#20204;&#30340;&#31639;&#27861;&#65292;&#24182;&#26368;&#32456;&#30740;&#31350;&#20102;&#23427;&#22312;&#21306;&#20998;&#22823;&#37327;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#23450;&#20041;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20174;&#26679;&#26412;&#20013;&#35782;&#21035;&#28508;&#22312;&#29983;&#25104;&#36807;&#31243;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10401v1 Announce Type: new  Abstract: Recent works have shown that generative models leave traces of their underlying generative process on the generated samples, broadly referred to as fingerprints of a generative model, and have studied their utility in detecting synthetic images from real ones. However, the extend to which these fingerprints can distinguish between various types of synthetic image and help identify the underlying generative process remain under-explored. In particular, the very definition of a fingerprint remains unclear, to our knowledge. To that end, in this work, we formalize the definition of artifact and fingerprint in generative models, propose an algorithm for computing them in practice, and finally study its effectiveness in distinguishing a large array of different generative models. We find that using our proposed definition can significantly improve the performance on the task of identifying the underlying generative process from samples (model
&lt;/p&gt;</description></item><item><title>LogELECTRA&#26159;&#19968;&#20010;&#26032;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#21333;&#34892;&#26085;&#24535;&#28040;&#24687;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#27169;&#26495;&#20986;&#29616;&#27169;&#24335;&#30340;&#26816;&#27979;&#26041;&#27861;&#23384;&#22312;&#30340;&#23616;&#38480;&#24615;&#21644;&#21487;&#33021;&#23548;&#33268;&#19981;&#24517;&#35201;&#24310;&#36831;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10397</link><description>&lt;p&gt;
LogELECTRA&#65306;&#33258;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#38750;&#32467;&#26500;&#21270;&#26085;&#24535;
&lt;/p&gt;
&lt;p&gt;
LogELECTRA: Self-supervised Anomaly Detection for Unstructured Logs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10397
&lt;/p&gt;
&lt;p&gt;
LogELECTRA&#26159;&#19968;&#20010;&#26032;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#21333;&#34892;&#26085;&#24535;&#28040;&#24687;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#27169;&#26495;&#20986;&#29616;&#27169;&#24335;&#30340;&#26816;&#27979;&#26041;&#27861;&#23384;&#22312;&#30340;&#23616;&#38480;&#24615;&#21644;&#21487;&#33021;&#23548;&#33268;&#19981;&#24517;&#35201;&#24310;&#36831;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#26085;&#24535;&#26159;&#32500;&#25252;&#36719;&#20214;&#31995;&#32479;&#30340;&#19968;&#20123;&#26368;&#37325;&#35201;&#20449;&#24687;&#65292;&#36817;&#24180;&#26469;&#36719;&#20214;&#31995;&#32479;&#21464;&#24471;&#26356;&#21152;&#24222;&#22823;&#21644;&#22797;&#26434;&#12290;&#22522;&#20110;&#26085;&#24535;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20998;&#26512;&#30701;&#26102;&#38388;&#20869;&#29983;&#25104;&#30340;&#22823;&#37327;&#26085;&#24535;&#33258;&#21160;&#26816;&#27979;&#31995;&#32479;&#24322;&#24120;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#26085;&#24535;&#35299;&#26512;&#22120;&#20174;&#38750;&#32467;&#26500;&#21270;&#26085;&#24535;&#25968;&#25454;&#20013;&#25552;&#21462;&#27169;&#26495;&#65292;&#24182;&#26681;&#25454;&#27169;&#26495;&#20986;&#29616;&#30340;&#27169;&#24335;&#26816;&#27979;&#24322;&#24120;&#12290;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#20855;&#26377;&#26410;&#30693;&#27169;&#26495;&#30340;&#26085;&#24535;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#26085;&#24535;&#24322;&#24120;&#34987;&#35748;&#20026;&#26159;&#28857;&#24322;&#24120;&#32780;&#19981;&#26159;&#19978;&#19979;&#25991;&#24322;&#24120;&#65292;&#22522;&#20110;&#20986;&#29616;&#27169;&#24335;&#30340;&#26816;&#27979;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#19981;&#24517;&#35201;&#30340;&#24310;&#36831;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LogELECTRA&#65292;&#19968;&#31181;&#26032;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#65292;&#23427;&#22522;&#20110;&#33258;&#30417;&#30563;&#24322;&#24120;&#26356;&#28145;&#20837;&#22320;&#20998;&#26512;&#21333;&#34892;&#26085;&#24535;&#28040;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10397v1 Announce Type: new  Abstract: System logs are some of the most important information for the maintenance of software systems, which have become larger and more complex in recent years. The goal of log-based anomaly detection is to automatically detect system anomalies by analyzing the large number of logs generated in a short period of time, which is a critical challenge in the real world. Previous studies have used a log parser to extract templates from unstructured log data and detect anomalies on the basis of patterns of the template occurrences. These methods have limitations for logs with unknown templates. Furthermore, since most log anomalies are known to be point anomalies rather than contextual anomalies, detection methods based on occurrence patterns can cause unnecessary delays in detection. In this paper, we propose LogELECTRA, a new log anomaly detection model that analyzes a single line of log messages more deeply on the basis of self-supervised anomaly
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#23545;&#40784;&#39564;&#35777;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;&#20107;&#20214;&#24207;&#21015;&#30340;&#22522;&#30784;&#34920;&#31034;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#21644;&#25968;&#25454;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2402.10392</link><description>&lt;p&gt;
&#38024;&#23545;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pretext Training Algorithms for Event Sequence Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10392
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#23545;&#40784;&#39564;&#35777;&#20219;&#21153;&#65292;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;&#20107;&#20214;&#24207;&#21015;&#30340;&#22522;&#30784;&#34920;&#31034;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#21644;&#25968;&#25454;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#21518;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#24494;&#35843;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#23450;&#21046;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#38024;&#23545;&#20107;&#20214;&#24207;&#21015;&#29305;&#21270;&#30340;&#26032;&#22411;&#23545;&#40784;&#39564;&#35777;&#20219;&#21153;&#65292;&#20511;&#37492;&#20102;&#25513;&#30721;&#37325;&#26500;&#21644;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#33391;&#22909;&#23454;&#36341;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#21487;&#20197;&#37322;&#25918;&#22522;&#30784;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#21253;&#25324;&#29992;&#20110;&#26102;&#38388;&#28857;&#36807;&#31243;&#27169;&#22411;&#30340;&#19979;&#19968;&#20107;&#20214;&#39044;&#27979;&#65292;&#20107;&#20214;&#24207;&#21015;&#20998;&#31867;&#21644;&#32570;&#22833;&#20107;&#20214;&#25554;&#20540;&#12290;&#23545;&#27969;&#34892;&#30340;&#20844;&#20849;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#39046;&#22495;&#19978;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10392v1 Announce Type: cross  Abstract: Pretext training followed by task-specific fine-tuning has been a successful approach in vision and language domains. This paper proposes a self-supervised pretext training framework tailored to event sequence data. We introduce a novel alignment verification task that is specialized to event sequences, building on good practices in masked reconstruction and contrastive learning. Our pretext tasks unlock foundational representations that are generalizable across different down-stream tasks, including next-event prediction for temporal point process models, event sequence classification, and missing event interpolation. Experiments on popular public benchmarks demonstrate the potential of the proposed method across different tasks and data domains.
&lt;/p&gt;</description></item><item><title>MFBind&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20445;&#30495;&#24230;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#23545;&#25509;&#21644;&#32467;&#21512;&#33258;&#30001;&#33021;&#27169;&#25311;&#22120;&#65292;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#35757;&#32451;&#22810;&#20445;&#30495;&#24230;&#28145;&#24230;&#20195;&#29702;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#30340;&#26368;&#20339;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.10387</link><description>&lt;p&gt;
MFBind&#65306;&#19968;&#31181;&#29992;&#20110;&#22312;&#23454;&#38469;&#29983;&#25104;&#24314;&#27169;&#20013;&#35780;&#20272;&#33647;&#29289;&#21270;&#21512;&#29289;&#30340;&#22810;&#20445;&#30495;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MFBind: a Multi-Fidelity Approach for Evaluating Drug Compounds in Practical Generative Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10387
&lt;/p&gt;
&lt;p&gt;
MFBind&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20445;&#30495;&#24230;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#23545;&#25509;&#21644;&#32467;&#21512;&#33258;&#30001;&#33021;&#27169;&#25311;&#22120;&#65292;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#35757;&#32451;&#22810;&#20445;&#30495;&#24230;&#28145;&#24230;&#20195;&#29702;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#30340;&#26368;&#20339;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#33647;&#29289;&#21457;&#29616;&#30340;&#29983;&#25104;&#27169;&#22411;&#20027;&#35201;&#20351;&#29992;&#20998;&#23376;&#23545;&#25509;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;&#21270;&#21512;&#29289;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#36890;&#24120;&#24182;&#19981;&#23454;&#29992;&#65292;&#22240;&#20026;&#21363;&#20351;&#24471;&#20998;&#25968;&#39640;&#30340;&#21270;&#21512;&#29289;&#20063;&#19981;&#19968;&#23450;&#22987;&#32456;&#20855;&#26377;&#23454;&#39564;&#27963;&#24615;&#12290;&#23384;&#22312;&#26356;&#20934;&#30830;&#30340;&#27963;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;&#20998;&#23376;&#21160;&#21147;&#23398;&#30340;&#32467;&#21512;&#33258;&#30001;&#33021;&#35745;&#31639;&#65292;&#20294;&#23427;&#20204;&#22312;&#29983;&#25104;&#27169;&#22411;&#20013;&#20351;&#29992;&#36215;&#26469;&#35745;&#31639;&#25104;&#26412;&#22826;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20445;&#30495;&#24230;&#26041;&#27861;&#65292;Multi-Fidelity Bind&#65288;MFBind&#65289;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#30340;&#26368;&#20339;&#26435;&#34913;&#12290;MFBind&#38598;&#25104;&#20102;&#23545;&#25509;&#21644;&#32467;&#21512;&#33258;&#30001;&#33021;&#27169;&#25311;&#22120;&#65292;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#35757;&#32451;&#22810;&#20445;&#30495;&#24230;&#28145;&#24230;&#20195;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#28145;&#24230;&#20195;&#29702;&#27169;&#22411;&#21033;&#29992;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#25216;&#26415;&#21644;&#32447;&#24615;&#39044;&#27979;&#22836;&#65292;&#20197;&#39640;&#25928;&#25311;&#21512;&#23569;&#37327;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;MFBind&#65288;1&#65289;&#34920;&#29616;&#36229;&#36234;benchmark&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10387v1 Announce Type: cross  Abstract: Current generative models for drug discovery primarily use molecular docking to evaluate the quality of generated compounds. However, such models are often not useful in practice because even compounds with high docking scores do not consistently show experimental activity. More accurate methods for activity prediction exist, such as molecular dynamics based binding free energy calculations, but they are too computationally expensive to use in a generative model. We propose a multi-fidelity approach, Multi-Fidelity Bind (MFBind), to achieve the optimal trade-off between accuracy and computational cost. MFBind integrates docking and binding free energy simulators to train a multi-fidelity deep surrogate model with active learning. Our deep surrogate model utilizes a pretraining technique and linear prediction heads to efficiently fit small amounts of high-fidelity data. We perform extensive experiments and show that MFBind (1) outperfor
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#31616;&#21333;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#20301;&#20110;&#36755;&#20837;&#22270;&#29305;&#24449;&#31354;&#38388;&#20869;&#30340;&#21151;&#33021;&#26469;&#23454;&#29616;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#20854;&#22312;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#36890;&#29992;&#24615;</title><link>https://arxiv.org/abs/2402.10380</link><description>&lt;p&gt;
&#23376;&#22270;&#32423;&#36890;&#29992;&#25552;&#31034;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Subgraph-level Universal Prompt Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10380
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#31616;&#21333;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#20301;&#20110;&#36755;&#20837;&#22270;&#29305;&#24449;&#31354;&#38388;&#20869;&#30340;&#21151;&#33021;&#26469;&#23454;&#29616;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#20854;&#22312;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#36890;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#26469;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#21464;&#24471;&#26085;&#30410;&#31361;&#20986;&#12290;&#36825;&#19968;&#36235;&#21183;&#22312;&#22270;&#39046;&#22495;&#29305;&#21035;&#26126;&#26174;&#65292;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#20026;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#35843;&#25972;&#26041;&#27861;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#21463;&#21040;&#38480;&#21046;&#65292;&#20027;&#35201;&#38024;&#23545;&#20855;&#26377;&#36793;&#39044;&#27979;&#39044;&#35757;&#32451;&#20219;&#21153;&#30340;&#27169;&#22411;&#23450;&#21046;&#20102;&#19987;&#38376;&#30340;&#25552;&#31034;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#39044;&#35757;&#32451;&#31574;&#30053;&#20043;&#38388;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#36755;&#20837;&#22270;&#30340;&#29305;&#24449;&#31354;&#38388;&#20869;&#21457;&#25381;&#20316;&#29992;&#12290;&#36825;&#20351;&#20854;&#20174;&#29702;&#35770;&#19978;&#21487;&#20197;&#27169;&#25311;&#20219;&#20309;&#31867;&#22411;&#30340;&#25552;&#31034;&#20989;&#25968;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#19968;&#31995;&#21015;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10380v1 Announce Type: cross  Abstract: In the evolving landscape of machine learning, the adaptation of pre-trained models through prompt tuning has become increasingly prominent. This trend is particularly observable in the graph domain, where diverse pre-training strategies present unique challenges in developing effective prompt-based tuning methods for graph neural networks. Previous approaches have been limited, focusing on specialized prompting functions tailored to models with edge prediction pre-training tasks. These methods, however, suffer from a lack of generalizability across different pre-training strategies. Recently, a simple prompt tuning method has been designed for any pre-training strategy, functioning within the input graph's feature space. This allows it to theoretically emulate any type of prompting function, thereby significantly increasing its versatility for a range of downstream applications. Nevertheless, the capacity of such simple prompts to ful
&lt;/p&gt;</description></item><item><title>DataDreamer&#26159;&#19968;&#31181;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#21487;&#22797;&#29616;LLM&#24037;&#20316;&#27969;&#31243;&#30340;&#24320;&#28304;Python&#24211;&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#23454;&#29616;&#24378;&#22823;&#30340;LLM&#24037;&#20316;&#27969;&#65292;&#25552;&#20513;&#24320;&#25918;&#31185;&#23398;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10379</link><description>&lt;p&gt;
DataDreamer: &#19968;&#31181;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#21487;&#22797;&#29616;LLM&#24037;&#20316;&#27969;&#31243;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10379
&lt;/p&gt;
&lt;p&gt;
DataDreamer&#26159;&#19968;&#31181;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21644;&#21487;&#22797;&#29616;LLM&#24037;&#20316;&#27969;&#31243;&#30340;&#24320;&#28304;Python&#24211;&#65292;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#23454;&#29616;&#24378;&#22823;&#30340;LLM&#24037;&#20316;&#27969;&#65292;&#25552;&#20513;&#24320;&#25918;&#31185;&#23398;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20154;&#21592;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20027;&#35201;&#24037;&#20855;&#12290;&#22914;&#20170;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12289;&#20219;&#21153;&#35780;&#20272;&#12289;&#24494;&#35843;&#12289;&#25552;&#28860;&#20197;&#21450;&#20854;&#20182;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#30740;&#31350;&#24037;&#20316;&#27969;&#20013;&#20351;&#29992;LLMs&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#26102;&#20250;&#36935;&#21040;&#25361;&#25112;&#65292;&#36825;&#20123;&#25361;&#25112;&#28304;&#20110;&#23427;&#20204;&#30340;&#35268;&#27169;&#12289;&#38381;&#28304;&#24615;&#36136;&#20197;&#21450;&#32570;&#20047;&#38024;&#23545;&#36825;&#20123;&#26032;&#20852;&#24037;&#20316;&#27969;&#30340;&#26631;&#20934;&#21270;&#24037;&#20855;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#36805;&#36895;&#23835;&#36215;&#21644;&#36825;&#20123;&#29420;&#29305;&#25361;&#25112;&#23545;&#24320;&#25918;&#31185;&#23398;&#21644;&#20351;&#29992;&#23427;&#20204;&#30340;&#24037;&#20316;&#30340;&#21487;&#37325;&#29616;&#24615;&#20135;&#29983;&#20102;&#30452;&#25509;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DataDreamer&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#32534;&#20889;&#31616;&#21333;&#30340;&#20195;&#30721;&#26469;&#23454;&#29616;&#24378;&#22823;&#30340;LLM&#24037;&#20316;&#27969;&#12290;DataDreamer&#36824;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#36981;&#24490;&#25105;&#20204;&#25552;&#20986;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#20197;&#40723;&#21169;&#24320;&#25918;&#31185;&#23398;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;&#35813;&#24211;&#21644;&#25991;&#26723;&#21487;&#22312;h&#32593;&#31449;&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10379v1 Announce Type: new  Abstract: Large language models (LLMs) have become a dominant and important tool for NLP researchers in a wide range of tasks. Today, many researchers use LLMs in synthetic data generation, task evaluation, fine-tuning, distillation, and other model-in-the-loop research workflows. However, challenges arise when using these models that stem from their scale, their closed source nature, and the lack of standardized tooling for these new and emerging workflows. The rapid rise to prominence of these models and these unique challenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them. In this paper, we introduce DataDreamer, an open source Python library that allows researchers to write simple code to implement powerful LLM workflows. DataDreamer also helps researchers adhere to best practices that we propose to encourage open science and reproducibility. The library and documentation are available at h
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Sparse Linear Concept Embeddings&#65288;SpLiCE&#65289;&#65292;&#36890;&#36807;&#23558;CLIP&#34920;&#31034;&#36716;&#25442;&#20026;&#20154;&#21487;&#35299;&#37322;&#27010;&#24565;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;CLIP&#23884;&#20837;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.10376</link><description>&lt;p&gt;
&#29992;&#31232;&#30095;&#32447;&#24615;&#27010;&#24565;&#23884;&#20837;&#65288;SpLiCE&#65289;&#35299;&#37322;CLIP
&lt;/p&gt;
&lt;p&gt;
Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Sparse Linear Concept Embeddings&#65288;SpLiCE&#65289;&#65292;&#36890;&#36807;&#23558;CLIP&#34920;&#31034;&#36716;&#25442;&#20026;&#20154;&#21487;&#35299;&#37322;&#27010;&#24565;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;CLIP&#23884;&#20837;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CLIP&#23884;&#20837;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36825;&#20123;&#39640;&#32500;&#31264;&#23494;&#21521;&#37327;&#34920;&#31034;&#24182;&#19981;&#23481;&#26131;&#35299;&#37322;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#36879;&#26126;&#24230;&#30340;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#32463;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;CLIP&#30340;&#28508;&#22312;&#31354;&#38388;&#39640;&#24230;&#32467;&#26500;&#21270;&#65292;&#22240;&#27492;&#21487;&#20197;&#23558;CLIP&#34920;&#31034;&#20998;&#35299;&#20026;&#20854;&#28508;&#22312;&#35821;&#20041;&#32452;&#20214;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#29702;&#35299;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31232;&#30095;&#32447;&#24615;&#27010;&#24565;&#23884;&#20837;&#65288;SpLiCE&#65289;&#65292;&#29992;&#20110;&#23558;CLIP&#34920;&#31034;&#36716;&#25442;&#20026;&#20154;&#21487;&#35299;&#37322;&#27010;&#24565;&#30340;&#31232;&#30095;&#32447;&#24615;&#32452;&#21512;&#12290;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;SpLiCE&#19981;&#38656;&#35201;&#27010;&#24565;&#26631;&#31614;&#65292;&#24182;&#19988;&#21487;&#20197;&#21518;&#26399;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;SpLiCE&#36755;&#20986;&#30340;&#34920;&#31034;&#21487;&#20197;&#35299;&#37322;&#29978;&#33267;&#21462;&#20195;&#20256;&#32479;&#30340;&#23494;&#38598;CLIP&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10376v1 Announce Type: new  Abstract: CLIP embeddings have demonstrated remarkable performance across a wide range of computer vision tasks. However, these high-dimensional, dense vector representations are not easily interpretable, restricting their usefulness in downstream applications that require transparency. In this work, we empirically show that CLIP's latent space is highly structured, and consequently that CLIP representations can be decomposed into their underlying semantic components. We leverage this understanding to propose a novel method, Sparse Linear Concept Embeddings (SpLiCE), for transforming CLIP representations into sparse linear combinations of human-interpretable concepts. Distinct from previous work, SpLiCE does not require concept labels and can be applied post hoc. Through extensive experimentation with multiple real-world datasets, we validate that the representations output by SpLiCE can explain and even replace traditional dense CLIP representati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26356;&#20005;&#26684;&#30340;&#8220;&#32463;&#39564;&#37325;&#25918;&#26465;&#20214;&#8221;&#65292;&#24182;&#25581;&#31034;&#20102;&#25919;&#31574;&#25913;&#36827;&#30340;&#19981;&#31283;&#23450;&#24615;&#22240;&#32032;&#65292;&#20174;&#32780;&#23548;&#20986;&#20102;&#30456;&#24212;&#30340;&#31283;&#23450;&#21270;&#25216;&#24039;&#65292;&#26368;&#32456;&#20351;&#24471;&#32463;&#39564;&#37325;&#25918;&#21487;&#24212;&#29992;&#20110;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10374</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#32463;&#39564;&#37325;&#25918;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Revisiting Experience Replayable Conditions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26356;&#20005;&#26684;&#30340;&#8220;&#32463;&#39564;&#37325;&#25918;&#26465;&#20214;&#8221;&#65292;&#24182;&#25581;&#31034;&#20102;&#25919;&#31574;&#25913;&#36827;&#30340;&#19981;&#31283;&#23450;&#24615;&#22240;&#32032;&#65292;&#20174;&#32780;&#23548;&#20986;&#20102;&#30456;&#24212;&#30340;&#31283;&#23450;&#21270;&#25216;&#24039;&#65292;&#26368;&#32456;&#20351;&#24471;&#32463;&#39564;&#37325;&#25918;&#21487;&#24212;&#29992;&#20110;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#32463;&#39564;&#37325;&#25918;&#65288;ER&#65289;&#34987;&#35748;&#20026;&#20165;&#36866;&#29992;&#20110;&#31163;&#31574;&#30053;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#26377;&#19968;&#20123;&#24773;&#20917;&#19979;&#23558;ER&#24212;&#29992;&#20110;&#22312;&#31574;&#30053;&#31639;&#27861;&#20013;&#65292;&#36825;&#34920;&#26126;&#31163;&#31574;&#30053;&#24615;&#21487;&#33021;&#26159;&#24212;&#29992;ER&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#26356;&#20005;&#26684;&#30340;&#8220;&#32463;&#39564;&#37325;&#25918;&#26465;&#20214;&#8221;&#65288;ERC&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#25913;&#29616;&#26377;&#31639;&#27861;&#20197;&#28385;&#36275;ERC&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#20551;&#35774;&#25919;&#31574;&#25913;&#36827;&#30340;&#19981;&#31283;&#23450;&#24615;&#26159;ERC&#30340;&#20851;&#38190;&#12290;&#20174;&#24230;&#37327;&#23398;&#20064;&#30340;&#35282;&#24230;&#25581;&#31034;&#20102;&#19981;&#31283;&#23450;&#22240;&#32032;&#65292;&#21253;&#25324;i&#65289;&#26469;&#33258;&#36127;&#26679;&#26412;&#30340;&#25490;&#26021;&#21147;&#21644;ii&#65289;&#19981;&#24403;&#32463;&#39564;&#30340;&#37325;&#25918;&#12290;&#22240;&#27492;&#65292;&#23548;&#20986;&#20102;&#30456;&#24212;&#30340;&#31283;&#23450;&#21270;&#25216;&#24039;&#12290;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#65292;&#25152;&#25552;&#20986;&#30340;&#31283;&#23450;&#21270;&#25216;&#24039;&#20351;&#24471;ER&#36866;&#29992;&#20110;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#19968;&#31181;&#22312;&#31574;&#30053;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#20854;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10374v1 Announce Type: new  Abstract: Experience replay (ER) used in (deep) reinforcement learning is considered to be applicable only to off-policy algorithms. However, there have been some cases in which ER has been applied for on-policy algorithms, suggesting that off-policyness might be a sufficient condition for applying ER. This paper reconsiders more strict "experience replayable conditions" (ERC) and proposes the way of modifying the existing algorithms to satisfy ERC. To this end, instability of policy improvements is assumed to be a key in ERC. The instability factors are revealed from the viewpoint of metric learning as i) repulsive forces from negative samples and ii) replays of inappropriate experiences. Accordingly, the corresponding stabilization tricks are derived. As a result, it is confirmed through numerical simulations that the proposed stabilization tricks make ER applicable to an advantage actor-critic, an on-policy algorithm. In addition, its learning 
&lt;/p&gt;</description></item><item><title>BioMistral&#26159;&#19968;&#31181;&#38754;&#21521;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;&#65292;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#24182;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.10373</link><description>&lt;p&gt;
BioMistral&#65306;&#38754;&#21521;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10373
&lt;/p&gt;
&lt;p&gt;
BioMistral&#26159;&#19968;&#31181;&#38754;&#21521;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24320;&#28304;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#21512;&#65292;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#24182;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#24180;&#26469;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#21644;&#21307;&#23398;&#31561;&#19987;&#19994;&#39046;&#22495;&#25552;&#20379;&#28508;&#22312;&#24212;&#29992;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;&#38024;&#23545;&#20581;&#24247;&#39046;&#22495;&#23450;&#21046;&#30340;&#24320;&#28304;LLMs&#21487;&#29992;&#65292;&#20294;&#23558;&#36890;&#29992;LLMs&#35843;&#25972;&#21040;&#21307;&#23398;&#39046;&#22495;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BioMistral&#65292;&#19968;&#31181;&#19987;&#20026;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#37327;&#36523;&#23450;&#21046;&#30340;&#24320;&#28304;LLM&#65292;&#37319;&#29992;Mistral&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#22312;PubMed Central&#19978;&#36827;&#19968;&#27493;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#21253;&#21547;10&#20010;&#24050;&#24314;&#31435;&#30340;&#33521;&#25991;&#21307;&#23398;&#38382;&#31572;&#65288;QA&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#19978;&#23545;BioMistral&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#36890;&#36807;&#37327;&#21270;&#21644;&#27169;&#22411;&#21512;&#24182;&#26041;&#27861;&#33719;&#24471;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;BioMistral&#30456;&#36739;&#20110;&#29616;&#26377;&#24320;&#28304;&#21307;&#23398;&#27169;&#22411;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#19982;&#19987;&#26377;&#23545;&#25163;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10373v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, offering potential applications across specialized domains such as healthcare and medicine. Despite the availability of various open-source LLMs tailored for health contexts, adapting general-purpose LLMs to the medical domain presents significant challenges. In this paper, we introduce BioMistral, an open-source LLM tailored for the biomedical domain, utilizing Mistral as its foundation model and further pre-trained on PubMed Central. We conduct a comprehensive evaluation of BioMistral on a benchmark comprising 10 established medical question-answering (QA) tasks in English. We also explore lightweight models obtained through quantization and model merging approaches. Our results demonstrate BioMistral's superior performance compared to existing open-source medical models and its competitive edge against proprietary counterparts. Finally, to address
&lt;/p&gt;</description></item><item><title>&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#22256;&#38590;&#24615;&#20855;&#26377;&#32039;&#20945;&#30340;&#26377;&#38480;&#29305;&#24615;&#34920;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.10360</link><description>&lt;p&gt;
&#23398;&#20064;&#24615;&#26159;&#19968;&#31181;&#32039;&#20945;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Learnability is a Compact Property
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10360
&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#22256;&#38590;&#24615;&#20855;&#26377;&#32039;&#20945;&#30340;&#26377;&#38480;&#29305;&#24615;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#23398;&#20064;&#30340;&#24037;&#20316;&#21462;&#24471;&#20102;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#32467;&#26524;&#65306;&#21508;&#31181;&#38382;&#39064;&#30340;&#21487;&#23398;&#20064;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#21028;&#23450;&#30340;&#65292;&#25110;&#32773;&#19982;&#26631;&#20934;&#38598;&#21512;&#35770;ZFC&#20844;&#29702;&#26080;&#20851;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#38382;&#39064;&#30340;&#21487;&#23398;&#20064;&#24615;&#21487;&#33021;&#19981;&#26159;&#20855;&#26377;&#26377;&#38480;&#29305;&#24615;&#30340;&#23646;&#24615;&#65306;&#38750;&#27491;&#24335;&#22320;&#35828;&#65292;&#23427;&#19981;&#33021;&#36890;&#36807;&#26816;&#26597;&#38382;&#39064;&#30340;&#26377;&#38480;&#25237;&#24433;&#26469;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10360v1 Announce Type: new  Abstract: Recent work on learning has yielded a striking result: the learnability of various problems can be undecidable, or independent of the standard ZFC axioms of set theory. Furthermore, the learnability of such problems can fail to be a property of finite character: informally, it cannot be detected by examining finite projections of the problem.   On the other hand, learning theory abounds with notions of dimension that characterize learning and consider only finite restrictions of the problem, i.e., are properties of finite character. How can these results be reconciled? More precisely, which classes of learning problems are vulnerable to logical undecidability, and which are within the grasp of finite characterizations?   We demonstrate that the difficulty of supervised learning with metric losses admits a tight finite characterization. In particular, we prove that the sample complexity of learning a hypothesis class can be detected by ex
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;GraphPrompter&#26694;&#26550;&#65292;&#36890;&#36807;&#36719;&#25552;&#31034;&#23558;&#22270;&#20449;&#24687;&#19982;LLMs&#23545;&#40784;&#65292;&#20197;&#36827;&#19968;&#27493;&#25506;&#31350;LLMs&#29702;&#35299;&#22270;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10359</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#29992;&#36719;&#25552;&#31034;LLMs&#26469;&#36827;&#34892;&#22270;&#23398;&#20064;&#20219;&#21153;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can we soft prompt LLMs for graph learning tasks?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10359
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;GraphPrompter&#26694;&#26550;&#65292;&#36890;&#36807;&#36719;&#25552;&#31034;&#23558;&#22270;&#20449;&#24687;&#19982;LLMs&#23545;&#40784;&#65292;&#20197;&#36827;&#19968;&#27493;&#25506;&#31350;LLMs&#29702;&#35299;&#22270;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#22312;&#34920;&#31034;&#31038;&#20132;&#32593;&#32476;&#12289;&#29983;&#29289;&#25968;&#25454;&#21644;&#24341;&#29992;&#32593;&#32476;&#31561;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#22797;&#26434;&#20851;&#31995;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#36825;&#20351;&#24471;&#23558;LLMs&#24212;&#29992;&#20110;&#22270;&#34920;&#26684;&#23588;&#20026;&#35825;&#20154;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#23558;LLMs&#24212;&#29992;&#20110;&#22270;&#34920;&#26684;&#24418;&#24335;&#23384;&#22312;&#29420;&#29305;&#25361;&#25112;&#65292;&#22240;&#20026;&#22270;&#34920;&#26684;&#24418;&#24335;&#19982;&#25991;&#26412;&#24418;&#24335;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#21644;&#19981;&#21305;&#37197;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#25506;&#31350;LLMs&#29702;&#35299;&#22270;&#20449;&#24687;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GraphPrompter&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#36719;&#25552;&#31034;&#26469;&#23558;&#22270;&#20449;&#24687;&#19982;LLMs&#23545;&#40784;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;GraphPrompter&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32534;&#30721;&#22797;&#26434;&#30340;&#22270;&#20449;&#24687;&#65292;&#20197;&#21450;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#25991;&#26412;&#20449;&#24687;&#30340;LLM&#12290;&#22312;&#19981;&#21516;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10359v1 Announce Type: cross  Abstract: Graph plays an important role in representing complex relationships in real-world applications such as social networks, biological data and citation networks. In recent years, Large Language Models (LLMs) have achieved tremendous success in various domains, which makes applying LLMs to graphs particularly appealing. However, directly applying LLMs to graph modalities presents unique challenges due to the discrepancy and mismatch between the graph and text modalities. Hence, to further investigate LLMs' potential for comprehending graph information, we introduce GraphPrompter, a novel framework designed to align graph information with LLMs via soft prompts. Specifically, GraphPrompter consists of two main components: a graph neural network to encode complex graph information and an LLM that effectively processes textual information. Comprehensive experiments on various benchmark datasets under node classification and link prediction tas
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Langevin MCMC&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#39640;&#25928;&#37319;&#26679;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#36845;&#20195;&#27493;&#25968;&#20026;$\tilde{O}(\epsilon^{-2})$&#26102;&#65292;Langevin MCMC&#30340;&#36845;&#20195;&#20250;&#19982;&#30446;&#26631;&#20998;&#24067;&#22312;$\epsilon$-Wasserstein&#36317;&#31163;&#20869;&#12290;</title><link>https://arxiv.org/abs/2402.10357</link><description>&lt;p&gt;
&#36890;&#36807;Langevin MCMC&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#39640;&#25928;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Efficient Sampling on Riemannian Manifolds via Langevin MCMC
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10357
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Langevin MCMC&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#39640;&#25928;&#37319;&#26679;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#36845;&#20195;&#27493;&#25968;&#20026;$\tilde{O}(\epsilon^{-2})$&#26102;&#65292;Langevin MCMC&#30340;&#36845;&#20195;&#20250;&#19982;&#30446;&#26631;&#20998;&#24067;&#22312;$\epsilon$-Wasserstein&#36317;&#31163;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#65288;&#20960;&#20309;&#65289;Langevin MCMC&#22312;&#40654;&#26364;&#27969;&#24418;$M$&#19978;&#39640;&#25928;&#22320;&#20174;Gibbs&#20998;&#24067;$d \pi^* = e^{-h} d {vol}_g$&#20013;&#37319;&#26679;&#30340;&#20219;&#21153;&#65307;&#35813;&#31639;&#27861;&#28041;&#21450;&#22312;&#38543;&#26426;&#39640;&#26031;&#26041;&#21521;&#19978;&#35745;&#31639;&#25351;&#25968;&#26144;&#23556;&#65292;&#22312;&#23454;&#36341;&#20013;&#21487;&#20197;&#39640;&#25928;&#23454;&#29616;&#12290;&#25105;&#20204;&#23545;Langevin MCMC&#30340;&#20998;&#26512;&#30340;&#20851;&#38190;&#22312;&#20110;&#23545;&#20960;&#20309;Euler-Murayama&#26041;&#26696;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#20570;&#20986;&#30340;&#30028;&#38480;&#65292;&#20551;&#35774;$\nabla h$&#26159;Lipschitz&#30340;&#65292;&#19988;$M$&#20855;&#26377;&#26377;&#30028;&#30340;&#26354;&#29575;&#25130;&#38754;&#12290;&#25105;&#20204;&#30340;&#35823;&#24046;&#30028;&#38480;&#19982;&#27431;&#20960;&#37324;&#24471;Euler-Murayama&#22312;&#27493;&#38271;&#20381;&#36182;&#24615;&#26041;&#38754;&#30340;&#35823;&#24046;&#30456;&#21305;&#37197;&#12290;&#32467;&#21512;Kendall-Cranston&#32806;&#21512;&#19979;&#23545;&#20960;&#20309;Langevin&#25193;&#25955;&#30340;&#25910;&#32553;&#20445;&#35777;&#65292;&#25105;&#20204;&#35777;&#26126;Langevin MCMC&#36845;&#20195;&#22312;&#32463;&#36807;$\tilde{O}(\epsilon^{-2})$&#27493;&#21518;&#19982;$\pi^*$&#20043;&#38388;&#30340;$\epsilon$-Wasserstein&#36317;&#31163;&#20869;&#65292;&#36825;&#19982;&#27431;&#20960;&#37324;&#24471;Langevin MCMC&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#19968;&#33324;&#35774;&#32622;&#65292;&#20854;&#20013;$h$&#21487;&#20197;&#26159;&#38750;&#20984;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10357v1 Announce Type: cross  Abstract: We study the task of efficiently sampling from a Gibbs distribution $d \pi^* = e^{-h} d {vol}_g$ over a Riemannian manifold $M$ via (geometric) Langevin MCMC; this algorithm involves computing exponential maps in random Gaussian directions and is efficiently implementable in practice. The key to our analysis of Langevin MCMC is a bound on the discretization error of the geometric Euler-Murayama scheme, assuming $\nabla h$ is Lipschitz and $M$ has bounded sectional curvature. Our error bound matches the error of Euclidean Euler-Murayama in terms of its stepsize dependence. Combined with a contraction guarantee for the geometric Langevin Diffusion under Kendall-Cranston coupling, we prove that the Langevin MCMC iterates lie within $\epsilon$-Wasserstein distance of $\pi^*$ after $\tilde{O}(\epsilon^{-2})$ steps, which matches the iteration complexity for Euclidean Langevin MCMC. Our results apply in general settings where $h$ can be nonc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#36755;&#20837;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#20934;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22266;&#26377;&#20559;&#24046;&#65292;&#20174;&#32780;&#25552;&#21319;&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10353</link><description>&lt;p&gt;
&#25552;&#21319;&#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20559;&#24046;&#26657;&#20934;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#36755;&#20837;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#20934;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22266;&#26377;&#20559;&#24046;&#65292;&#20174;&#32780;&#25552;&#21319;&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#22266;&#26377;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#22522;&#20110;&#25552;&#31034;&#30340;&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#36755;&#20837;&#25552;&#31034;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#20934;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#22266;&#26377;&#20559;&#24046;&#12290;&#19982;&#20197;&#24448;&#20027;&#35201;&#33268;&#21147;&#20110;&#31038;&#20250;&#20844;&#24179;&#30340;&#22266;&#26377;&#20559;&#24046;&#20462;&#27491;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#24378;&#35843;&#22266;&#26377;&#20559;&#24046;&#26657;&#20934;&#30340;&#25928;&#29575;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20174;GPT-4&#29983;&#25104;&#30340;&#19968;&#32452;&#33258;&#21160;&#36873;&#21462;&#30340;&#26080;&#24847;&#20041;&#36755;&#20837;&#26469;&#25552;&#31034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#25506;&#27979;&#22266;&#26377;&#20559;&#24046;&#12290;&#21033;&#29992;&#20559;&#24046;&#21453;&#26144;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24046;&#24322;&#25439;&#22833;&#29992;&#20110;&#20559;&#24046;&#26657;&#20934;&#65292;&#20854;&#20013;&#25105;&#20204;&#20165;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#24046;&#21442;&#25968;&#65288;&#24635;&#21442;&#25968;&#30340;0.1%&#65289;&#20197;&#26397;&#21521;&#30456;&#31561;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10353v1 Announce Type: new  Abstract: Prompt learning is susceptible to intrinsic bias present in pre-trained language models (LMs), resulting in sub-optimal performance of prompt-based zero/few-shot learning. In this work, we propose a null-input prompting method to calibrate intrinsic bias encoded in pre-trained LMs. Different from prior efforts that address intrinsic bias primarily for social fairness and often involve excessive computational cost, our objective is to explore enhancing LMs' performance in downstream zero/few-shot learning while emphasizing the efficiency of intrinsic bias calibration. Specifically, we leverage a diverse set of auto-selected null-meaning inputs generated from GPT-4 to prompt pre-trained LMs for intrinsic bias probing. Utilizing the bias-reflected probability distribution, we formulate a distribution disparity loss for bias calibration, where we exclusively update bias parameters ($0.1\%$ of total parameters) of LMs towards equal probabilit
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#21644;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#23637;&#29616;&#20986;&#26174;&#33879;&#28508;&#21147;&#65292;&#20294;&#38754;&#20020;&#30528;&#25361;&#25112;&#21253;&#25324;&#20381;&#36182;&#21382;&#21490;&#25968;&#25454;&#38598;&#12289;&#27867;&#21270;&#22256;&#38590;&#12289;&#27169;&#22411;&#24187;&#35273;&#31561;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#31561;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.10350</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#21644;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Forecasting and Anomaly Detection: A Systematic Literature Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10350
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#21644;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#23637;&#29616;&#20986;&#26174;&#33879;&#28508;&#21147;&#65292;&#20294;&#38754;&#20020;&#30528;&#25361;&#25112;&#21253;&#25324;&#20381;&#36182;&#21382;&#21490;&#25968;&#25454;&#38598;&#12289;&#27867;&#21270;&#22256;&#38590;&#12289;&#27169;&#22411;&#24187;&#35273;&#31561;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#31561;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#20840;&#38754;&#23457;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#39044;&#27979;&#21644;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#31361;&#20986;&#24403;&#21069;&#30740;&#31350;&#29616;&#29366;&#12289;&#22266;&#26377;&#25361;&#25112;&#20197;&#21450;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;LLMs&#24050;&#32463;&#22312;&#35299;&#26512;&#21644;&#20998;&#26512;&#22823;&#37327;&#25968;&#25454;&#38598;&#65292;&#35782;&#21035;&#27169;&#24335;&#65292;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#26816;&#27979;&#24322;&#24120;&#34892;&#20026;&#26041;&#38754;&#23637;&#29616;&#20986;&#26174;&#33879;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#35813;&#32508;&#36848;&#30830;&#23450;&#20102;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#26356;&#24191;&#27867;&#30340;&#37319;&#29992;&#21644;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20381;&#36182;&#20110;&#24222;&#22823;&#30340;&#21382;&#21490;&#25968;&#25454;&#38598;&#65292;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#27169;&#22411;&#24187;&#35273;&#29616;&#35937;&#65292;&#27169;&#22411;&#30693;&#35782;&#36793;&#30028;&#30340;&#38480;&#21046;&#20197;&#21450;&#25152;&#38656;&#30340;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#36890;&#36807;&#35814;&#32454;&#20998;&#26512;&#65292;&#35813;&#32508;&#36848;&#35752;&#35770;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#21644;&#31574;&#30053;&#65292;&#22914;&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10350v1 Announce Type: cross  Abstract: This systematic literature review comprehensively examines the application of Large Language Models (LLMs) in forecasting and anomaly detection, highlighting the current state of research, inherent challenges, and prospective future directions. LLMs have demonstrated significant potential in parsing and analyzing extensive datasets to identify patterns, predict future events, and detect anomalous behavior across various domains. However, this review identifies several critical challenges that impede their broader adoption and effectiveness, including the reliance on vast historical datasets, issues with generalizability across different contexts, the phenomenon of model hallucinations, limitations within the models' knowledge boundaries, and the substantial computational resources required. Through detailed analysis, this review discusses potential solutions and strategies to overcome these obstacles, such as integrating multimodal dat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25506;&#32034;&#39537;&#21160;&#31574;&#30053;&#20248;&#21270;&#30340;RLHF&#31639;&#27861;&#65292;&#36890;&#36807;&#36712;&#36857;&#27604;&#36739;&#21453;&#39304;&#25512;&#26029;&#22870;&#21169;&#20989;&#25968;&#65292;&#20026;&#35299;&#37322;&#23569;&#37327;&#20154;&#31867;&#21453;&#39304;&#36275;&#20197;&#23454;&#29616;&#33391;&#22909;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;</title><link>https://arxiv.org/abs/2402.10342</link><description>&lt;p&gt;
&#22312;RLHF&#20013;&#22522;&#20110;&#25506;&#32034;&#39537;&#21160;&#30340;&#31574;&#30053;&#20248;&#21270;&#65306;&#20851;&#20110;&#26377;&#25928;&#25968;&#25454;&#21033;&#29992;&#30340;&#29702;&#35770;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on Efficient Data Utilization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25506;&#32034;&#39537;&#21160;&#31574;&#30053;&#20248;&#21270;&#30340;RLHF&#31639;&#27861;&#65292;&#36890;&#36807;&#36712;&#36857;&#27604;&#36739;&#21453;&#39304;&#25512;&#26029;&#22870;&#21169;&#20989;&#25968;&#65292;&#20026;&#35299;&#37322;&#23569;&#37327;&#20154;&#31867;&#21453;&#39304;&#36275;&#20197;&#23454;&#29616;&#33391;&#22909;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#22312;&#20381;&#36182;&#23569;&#37327;&#20154;&#31867;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32463;&#39564;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#31181;&#29616;&#35937;&#23384;&#22312;&#30528;&#26377;&#38480;&#30340;&#29702;&#35770;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#26368;&#36817;&#30340;&#32463;&#39564;&#25104;&#21151;&#37319;&#29992;&#20102;&#22522;&#20110;&#31574;&#30053;&#30340;&#31639;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#26368;&#36817;&#30340;&#30740;&#31350;&#20173;&#20391;&#37325;&#20110;&#22522;&#20110;&#20215;&#20540;&#30340;&#31639;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#65288;PO-RLHF&#65289;&#30340;RLHF&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;&#27969;&#34892;&#30340;&#31574;&#30053;&#35206;&#30422;-&#31574;&#30053;&#26799;&#24230;&#65288;PC-PG&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20551;&#35774;&#23545;&#22870;&#21169;&#20989;&#25968;&#26377;&#30693;&#35782;&#12290;&#22312;PO-RLHF&#20013;&#65292;&#19981;&#20551;&#35774;&#30693;&#36947;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#19988;&#35813;&#31639;&#27861;&#20381;&#36182;&#20110;&#22522;&#20110;&#36712;&#36857;&#30340;&#27604;&#36739;&#21453;&#39304;&#26469;&#25512;&#26029;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#20026;PO-RLHF&#25552;&#20379;&#20102;&#20302;&#26597;&#35810;&#22797;&#26434;&#24230;&#30340;&#24615;&#33021;&#30028;&#38480;&#65292;&#36825;&#20026;&#35299;&#37322;&#20026;&#20160;&#20040;&#23569;&#37327;&#30340;&#20154;&#31867;&#21453;&#39304;&#21487;&#33021;&#36275;&#20197;&#22312;RLHF&#20013;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#21019;&#26032;&#26159;&#25105;&#20204;&#30340;&#36712;&#36857;&#32423;el
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10342v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (RLHF) has achieved impressive empirical successes while relying on a small amount of human feedback. However, there is limited theoretical justification for this phenomenon. Additionally, most recent studies focus on value-based algorithms despite the recent empirical successes of policy-based algorithms. In this work, we consider an RLHF algorithm based on policy optimization (PO-RLHF). The algorithm is based on the popular Policy Cover-Policy Gradient (PC-PG) algorithm, which assumes knowledge of the reward function. In PO-RLHF, knowledge of the reward function is not assumed and the algorithm relies on trajectory-based comparison feedback to infer the reward function. We provide performance bounds for PO-RLHF with low query complexity, which provides insight into why a small amount of human feedback may be sufficient to get good performance with RLHF. A key novelty is our trajectory-level el
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#20013;&#22788;&#29702;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#32487;&#32493;&#36335;&#24452;&#26041;&#27861;&#21644;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#26469;&#24341;&#23548;&#27169;&#22411;&#36798;&#21040;&#33391;&#22909;&#35299;&#20915;&#26041;&#26696;&#30340;&#20004;&#31181;&#19981;&#21516;&#36884;&#24452;</title><link>https://arxiv.org/abs/2402.10339</link><description>&lt;p&gt;
&#24403;&#20320;&#30340;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;&#30340;&#35268;&#27169;&#31561;&#21516;&#20110;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26102;&#35813;&#24590;&#20040;&#21150;&#65311;
&lt;/p&gt;
&lt;p&gt;
What to Do When Your Discrete Optimization Is the Size of a Neural Network?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10339
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#20013;&#22788;&#29702;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#32487;&#32493;&#36335;&#24452;&#26041;&#27861;&#21644;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#26469;&#24341;&#23548;&#27169;&#22411;&#36798;&#21040;&#33391;&#22909;&#35299;&#20915;&#26041;&#26696;&#30340;&#20004;&#31181;&#19981;&#21516;&#36884;&#24452;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#28041;&#21450;&#35299;&#20915;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;&#65292;&#27604;&#22914;&#22312;&#21098;&#26525;&#12289;&#22522;&#20110;&#21442;&#25968;&#38548;&#31163;&#30340;&#25345;&#32493;&#23398;&#20064;&#21644;&#20108;&#36827;&#21046;&#32593;&#32476;&#35757;&#32451;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31163;&#25955;&#38382;&#39064;&#20855;&#26377;&#32452;&#21512;&#29305;&#24615;&#65292;&#19981;&#36866;&#21512;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#22312;&#31163;&#25955;&#35774;&#32622;&#20013;&#32463;&#20856;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#36843;&#20351;&#31185;&#23398;&#23478;&#21644;&#23454;&#35777;&#32773;&#20381;&#36182;&#20110;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#26377;&#20004;&#20010;&#20027;&#35201;&#19981;&#21516;&#30340;&#39030;&#32423;&#20449;&#24687;&#26469;&#28304;&#21487;&#20197;&#29992;&#26469;&#24341;&#23548;&#27169;&#22411;&#36798;&#21040;&#33391;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#65288;1&#65289;&#20174;&#35299;&#20915;&#26041;&#26696;&#38598;&#20043;&#22806;&#30340;&#28857;&#22806;&#25512;&#26799;&#24230;&#20449;&#24687;&#65288;2&#65289;&#22312;&#19968;&#32452;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#30340;&#25104;&#21592;&#20043;&#38388;&#27604;&#36739;&#35780;&#20272;&#12290;&#25105;&#20204;&#37319;&#29992;&#32487;&#32493;&#36335;&#24452;&#65288;CP&#65289;&#26041;&#27861;&#20195;&#34920;&#32431;&#31929;&#20351;&#29992;&#21069;&#32773;&#65292;&#33945;&#29305;&#21345;&#32599;&#65288;MC&#65289;&#26041;&#27861;&#20195;&#34920;&#21518;&#32773;&#65292;&#21516;&#26102;&#20063;&#25351;&#20986;&#19968;&#20123;&#28151;&#21512;&#26041;&#27861;&#32467;&#21512;&#20102;&#36825;&#20004;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10339v1 Announce Type: new  Abstract: Oftentimes, machine learning applications using neural networks involve solving discrete optimization problems, such as in pruning, parameter-isolation-based continual learning and training of binary networks. Still, these discrete problems are combinatorial in nature and are also not amenable to gradient-based optimization. Additionally, classical approaches used in discrete settings do not scale well to large neural networks, forcing scientists and empiricists to rely on alternative methods. Among these, two main distinct sources of top-down information can be used to lead the model to good solutions: (1) extrapolating gradient information from points outside of the solution set (2) comparing evaluations between members of a subset of the valid solutions. We take continuation path (CP) methods to represent using purely the former and Monte Carlo (MC) methods to represent the latter, while also noting that some hybrid methods combine th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HI-GAN&#30340;&#23618;&#27425;&#21270;&#20462;&#22797;GAN&#65292;&#36890;&#36807;&#19977;&#20010;GAN&#20197;&#23618;&#27425;&#32467;&#26500;&#30340;&#26041;&#24335;&#36827;&#34892;RGBD&#20462;&#22797;&#65292;&#20854;&#20013;EdgeGAN&#21644;LabelGAN&#20998;&#21035;&#20462;&#22797;&#36974;&#32617;&#36793;&#32536;&#21644;&#20998;&#21106;&#26631;&#31614;&#22270;&#20687;&#65292;&#32780;CombinedRGBD-GAN&#32467;&#21512;&#23427;&#20204;&#30340;&#28508;&#22312;&#34920;&#31034;&#36755;&#20986;&#24182;&#36827;&#34892;RGB&#21644;&#28145;&#24230;&#20462;&#22797;&#12290;</title><link>https://arxiv.org/abs/2402.10334</link><description>&lt;p&gt;
HI-GAN&#65306;&#20855;&#26377;&#36741;&#21161;&#36755;&#20837;&#30340;&#23618;&#27425;&#21270;&#20462;&#22797;GAN&#29992;&#20110;&#28151;&#21512;RGB&#21644;&#28145;&#24230;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
HI-GAN: Hierarchical Inpainting GAN with Auxiliary Inputs for Combined RGB and Depth Inpainting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10334
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HI-GAN&#30340;&#23618;&#27425;&#21270;&#20462;&#22797;GAN&#65292;&#36890;&#36807;&#19977;&#20010;GAN&#20197;&#23618;&#27425;&#32467;&#26500;&#30340;&#26041;&#24335;&#36827;&#34892;RGBD&#20462;&#22797;&#65292;&#20854;&#20013;EdgeGAN&#21644;LabelGAN&#20998;&#21035;&#20462;&#22797;&#36974;&#32617;&#36793;&#32536;&#21644;&#20998;&#21106;&#26631;&#31614;&#22270;&#20687;&#65292;&#32780;CombinedRGBD-GAN&#32467;&#21512;&#23427;&#20204;&#30340;&#28508;&#22312;&#34920;&#31034;&#36755;&#20986;&#24182;&#36827;&#34892;RGB&#21644;&#28145;&#24230;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20462;&#22797;&#28041;&#21450;&#22635;&#34917;&#22270;&#20687;&#20013;&#20002;&#22833;&#30340;&#20687;&#32032;&#25110;&#21306;&#22495;&#65292;&#36825;&#26159;&#28151;&#21512;&#29616;&#23454;&#29615;&#22659;&#20013;&#20351;&#29992;&#30340;&#19968;&#39033;&#20851;&#38190;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#20943;&#23569;&#29616;&#23454;&#65288;DR&#65289;&#20013;&#65292;&#20854;&#20013;&#20174;&#29992;&#25143;&#30340;&#35270;&#35273;&#29615;&#22659;&#20013;&#21024;&#38500;&#20869;&#23481;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#25968;&#23383;&#26367;&#25442;&#25216;&#26415;&#65292;&#38656;&#35201;&#22810;&#20010;&#25668;&#20687;&#22836;&#24182;&#20135;&#29983;&#39640;&#25104;&#26412;&#12290;AR&#35774;&#22791;&#21644;&#26234;&#33021;&#25163;&#26426;&#20351;&#29992;ToF&#28145;&#24230;&#20256;&#24863;&#22120;&#25429;&#33719;&#19982;RGB&#22270;&#20687;&#23545;&#40784;&#30340;&#22330;&#26223;&#28145;&#24230;&#22270;&#12290;&#23613;&#31649;&#36895;&#24230;&#24555;&#19988;&#20215;&#26684;&#23454;&#24800;&#65292;&#20294;ToF&#30456;&#26426;&#20250;&#20135;&#29983;&#20855;&#26377;&#20002;&#22833;&#20687;&#32032;&#30340;&#19981;&#23436;&#32654;&#28145;&#24230;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23618;&#27425;&#21270;&#20462;&#22797;GAN&#65288;HI-GAN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#30001;&#19977;&#20010;&#20197;&#23618;&#27425;&#32467;&#26500;&#26041;&#24335;&#32452;&#25104;&#30340;GAN&#26500;&#25104;&#65292;&#29992;&#20110;RGBD&#20462;&#22797;&#12290;EdgeGAN&#21644;LabelGAN&#20998;&#21035;&#20462;&#22797;&#36974;&#32617;&#36793;&#32536;&#21644;&#20998;&#21106;&#26631;&#31614;&#22270;&#20687;&#65292;&#32780;CombinedRGBD-GAN&#32467;&#21512;&#23427;&#20204;&#30340;&#28508;&#22312;&#34920;&#31034;&#36755;&#20986;&#24182;&#36827;&#34892;RGB&#21644;&#28145;&#24230;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10334v1 Announce Type: cross  Abstract: Inpainting involves filling in missing pixels or areas in an image, a crucial technique employed in Mixed Reality environments for various applications, particularly in Diminished Reality (DR) where content is removed from a user's visual environment. Existing methods rely on digital replacement techniques which necessitate multiple cameras and incur high costs. AR devices and smartphones use ToF depth sensors to capture scene depth maps aligned with RGB images. Despite speed and affordability, ToF cameras create imperfect depth maps with missing pixels. To address the above challenges, we propose Hierarchical Inpainting GAN (HI-GAN), a novel approach comprising three GANs in a hierarchical fashion for RGBD inpainting. EdgeGAN and LabelGAN inpaint masked edge and segmentation label images respectively, while CombinedRGBD-GAN combines their latent representation outputs and performs RGB and Depth inpainting. Edge images and particularly
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#20449;&#21495;&#26102;&#24207;&#36923;&#36753;&#65288;STL&#65289;&#25512;&#26029;&#21644;&#25511;&#21046;&#21512;&#25104;&#30340;&#26032;&#39062;&#20223;&#30495;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26126;&#30830;&#34920;&#31034;&#20219;&#21153;&#20026;STL&#20844;&#24335;&#65292;&#21516;&#26102;&#36890;&#36807;&#20154;&#20026;&#35843;&#25972;STL&#20844;&#24335;&#23454;&#29616;&#23545;&#20154;&#31867;&#30693;&#35782;&#30340;&#32435;&#20837;&#21644;&#26032;&#22330;&#26223;&#30340;&#36866;&#24212;&#65292;&#36824;&#37319;&#29992;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21551;&#21457;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#26377;&#25928;&#32553;&#23567;&#20102;&#19987;&#23478;&#31574;&#30053;&#21644;&#23398;&#20064;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#36317;</title><link>https://arxiv.org/abs/2402.10310</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Interpretable Generative Adversarial Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10310
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#20449;&#21495;&#26102;&#24207;&#36923;&#36753;&#65288;STL&#65289;&#25512;&#26029;&#21644;&#25511;&#21046;&#21512;&#25104;&#30340;&#26032;&#39062;&#20223;&#30495;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26126;&#30830;&#34920;&#31034;&#20219;&#21153;&#20026;STL&#20844;&#24335;&#65292;&#21516;&#26102;&#36890;&#36807;&#20154;&#20026;&#35843;&#25972;STL&#20844;&#24335;&#23454;&#29616;&#23545;&#20154;&#31867;&#30693;&#35782;&#30340;&#32435;&#20837;&#21644;&#26032;&#22330;&#26223;&#30340;&#36866;&#24212;&#65292;&#36824;&#37319;&#29992;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21551;&#21457;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#26377;&#25928;&#32553;&#23567;&#20102;&#19987;&#23478;&#31574;&#30053;&#21644;&#23398;&#20064;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#36890;&#36807;&#19987;&#23478;&#28436;&#31034;&#22312;&#25945;&#25480;&#33258;&#20027;&#31995;&#32479;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#22312;&#20110;&#23427;&#20204;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#29702;&#35299;&#23398;&#20064;&#20195;&#29702;&#35797;&#22270;&#23436;&#25104;&#30340;&#20855;&#20307;&#20219;&#21153;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#20449;&#21495;&#26102;&#24207;&#36923;&#36753;&#65288;STL&#65289;&#25512;&#26029;&#21644;&#25511;&#21046;&#21512;&#25104;&#30340;&#26032;&#39062;&#20223;&#30495;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#20219;&#21153;&#21487;&#20197;&#26126;&#30830;&#34920;&#31034;&#20026;STL&#20844;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#28165;&#26224;&#22320;&#29702;&#35299;&#20219;&#21153;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#25163;&#21160;&#35843;&#25972;STL&#20844;&#24335;&#26469;&#23558;&#20154;&#31867;&#30693;&#35782;&#32435;&#20837;&#24182;&#36866;&#24212;&#26032;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21463;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21551;&#21457;&#30340;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#25512;&#26029;&#21644;&#25511;&#21046;&#31574;&#30053;&#65292;&#26377;&#25928;&#22320;&#32553;&#23567;&#20102;&#19987;&#23478;&#31574;&#30053;&#21644;&#23398;&#20064;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10310v1 Announce Type: new  Abstract: Imitation learning methods have demonstrated considerable success in teaching autonomous systems complex tasks through expert demonstrations. However, a limitation of these methods is their lack of interpretability, particularly in understanding the specific task the learning agent aims to accomplish. In this paper, we propose a novel imitation learning method that combines Signal Temporal Logic (STL) inference and control synthesis, enabling the explicit representation of the task as an STL formula. This approach not only provides a clear understanding of the task but also allows for the incorporation of human knowledge and adaptation to new scenarios through manual adjustments of the STL formulae. Additionally, we employ a Generative Adversarial Network (GAN)-inspired training approach for both the inference and the control policy, effectively narrowing the gap between the expert and learned policies. The effectiveness of our algorithm
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Generative Flow Networks (GFlowNets)&#23398;&#20064;&#19968;&#20010;&#38543;&#26426;&#31574;&#30053;&#65292;&#20197;&#36817;&#20284;&#23454;&#29616;&#22312;&#25972;&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;&#27969;&#37327;&#30340;&#23432;&#24658;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22810;&#36335;&#24452;&#29983;&#25104;&#30456;&#21516;&#23545;&#35937;&#30340;&#20559;&#20506;&#20998;&#24067;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10309</link><description>&lt;p&gt;
&#31163;&#25955;&#27010;&#29575;&#25512;&#26029;&#20316;&#20026;&#22810;&#36335;&#24452;&#29615;&#22659;&#20013;&#30340;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Discrete Probabilistic Inference as Control in Multi-path Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10309
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Generative Flow Networks (GFlowNets)&#23398;&#20064;&#19968;&#20010;&#38543;&#26426;&#31574;&#30053;&#65292;&#20197;&#36817;&#20284;&#23454;&#29616;&#22312;&#25972;&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;&#27969;&#37327;&#30340;&#23432;&#24658;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22810;&#36335;&#24452;&#29983;&#25104;&#30456;&#21516;&#23545;&#35937;&#30340;&#20559;&#20506;&#20998;&#24067;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#20174;&#31163;&#25955;&#19988;&#32467;&#26500;&#21270;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#38382;&#39064;&#35270;&#20026;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#31181;&#38543;&#26426;&#31574;&#30053;&#65292;&#20351;&#29289;&#20307;&#22312;&#36825;&#20010;&#39034;&#24207;&#36807;&#31243;&#32467;&#26463;&#26102;&#20197;&#26576;&#20123;&#39044;&#23450;&#20041;&#22870;&#21169;&#30340;&#27604;&#20363;&#34987;&#37319;&#26679;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#26368;&#36817;&#32416;&#27491;&#22870;&#21169;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#30001;&#26368;&#20339;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#24341;&#36215;&#30340;&#36793;&#38469;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10309v1 Announce Type: new  Abstract: We consider the problem of sampling from a discrete and structured distribution as a sequential decision problem, where the objective is to find a stochastic policy such that objects are sampled at the end of this sequential process proportionally to some predefined reward. While we could use maximum entropy Reinforcement Learning (MaxEnt RL) to solve this problem for some distributions, it has been shown that in general, the distribution over states induced by the optimal policy may be biased in cases where there are multiple ways to generate the same object. To address this issue, Generative Flow Networks (GFlowNets) learn a stochastic policy that samples objects proportionally to their reward by approximately enforcing a conservation of flows across the whole Markov Decision Process (MDP). In this paper, we extend recent methods correcting the reward in order to guarantee that the marginal distribution induced by the optimal MaxEnt RL
&lt;/p&gt;</description></item><item><title>KCUSUM&#31639;&#27861;&#26159;&#19968;&#31181;&#38750;&#21442;&#25968;&#25193;&#23637;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#39640;&#23481;&#37327;&#25968;&#25454;&#24773;&#26223;&#19979;&#23454;&#26102;&#26816;&#27979;&#31361;&#21464;&#21464;&#21270;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#20854;&#33021;&#22815;&#26356;&#28789;&#27963;&#22320;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#36827;&#34892;&#21464;&#28857;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.10291</link><description>&lt;p&gt;
&#20351;&#29992;KCUSUM&#31639;&#27861;&#35780;&#20272;&#23454;&#26102;&#33258;&#36866;&#24212;&#37319;&#26679;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
An Evaluation of Real-time Adaptive Sampling Change Point Detection Algorithm using KCUSUM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10291
&lt;/p&gt;
&lt;p&gt;
KCUSUM&#31639;&#27861;&#26159;&#19968;&#31181;&#38750;&#21442;&#25968;&#25193;&#23637;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#39640;&#23481;&#37327;&#25968;&#25454;&#24773;&#26223;&#19979;&#23454;&#26102;&#26816;&#27979;&#31361;&#21464;&#21464;&#21270;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#20854;&#33021;&#22815;&#26356;&#28789;&#27963;&#22320;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#36827;&#34892;&#21464;&#28857;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31185;&#23398;&#27169;&#25311;&#25968;&#25454;&#27969;&#20013;&#23454;&#26102;&#26816;&#27979;&#31361;&#21464;&#21464;&#21270;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#35201;&#27714;&#37096;&#32626;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#31639;&#27861;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#26680;&#30340;&#32047;&#31215;&#21644;&#65288;KCUSUM&#65289;&#31639;&#27861;&#65292;&#19968;&#31181;&#20256;&#32479;&#32047;&#31215;&#21644;&#65288;CUSUM&#65289;&#26041;&#27861;&#30340;&#38750;&#21442;&#25968;&#25193;&#23637;&#65292;&#20197;&#20854;&#22312;&#36739;&#23569;&#38480;&#21046;&#26465;&#20214;&#19979;&#22312;&#32447;&#21464;&#28857;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10291v1 Announce Type: new  Abstract: Detecting abrupt changes in real-time data streams from scientific simulations presents a challenging task, demanding the deployment of accurate and efficient algorithms. Identifying change points in live data stream involves continuous scrutiny of incoming observations for deviations in their statistical characteristics, particularly in high-volume data scenarios. Maintaining a balance between sudden change detection and minimizing false alarms is vital. Many existing algorithms for this purpose rely on known probability distributions, limiting their feasibility. In this study, we introduce the Kernel-based Cumulative Sum (KCUSUM) algorithm, a non-parametric extension of the traditional Cumulative Sum (CUSUM) method, which has gained prominence for its efficacy in online change point detection under less restrictive conditions. KCUSUM splits itself by comparing incoming samples directly with reference samples and computes a statistic gr
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#32769;&#34382;&#26426;&#20013;&#20351;&#29992;Thompson Sampling&#31574;&#30053;&#65292;&#20197;&#23398;&#20064;&#36873;&#25321;&#26368;&#20339;&#33218;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.10289</link><description>&lt;p&gt;
Thompson Sampling&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#33218;&#32769;&#34382;&#26426;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Thompson Sampling in Partially Observable Contextual Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10289
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#32769;&#34382;&#26426;&#20013;&#20351;&#29992;Thompson Sampling&#31574;&#30053;&#65292;&#20197;&#23398;&#20064;&#36873;&#25321;&#26368;&#20339;&#33218;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#29305;&#24449;&#33218;&#32769;&#34382;&#26426;&#26500;&#25104;&#20102;&#19968;&#20010;&#32463;&#20856;&#30340;&#19981;&#30830;&#23450;&#24615;&#20915;&#31574;&#26694;&#26550;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#26159;&#22312;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#26465;&#20214;&#19979;&#23398;&#20064;&#20855;&#26377;&#26368;&#39640;&#22870;&#21169;&#30340;&#33218;&#65292;&#21516;&#26102;&#38656;&#35201;&#36890;&#36807;&#23454;&#39564;&#26469;&#23398;&#20064;&#27599;&#20010;&#33218;&#30340;&#26410;&#30693;&#22870;&#21169;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#22312;&#25506;&#32034;&#65288;&#21363;&#25289;&#21160;&#19981;&#21516;&#33218;&#20197;&#23398;&#20064;&#23427;&#20204;&#30340;&#21442;&#25968;&#65289;&#21644;&#24320;&#21457;&#65288;&#21363;&#25289;&#21160;&#26368;&#20339;&#33218;&#20197;&#33719;&#24471;&#22870;&#21169;&#65289;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#29616;&#26377;&#25991;&#29486;&#22823;&#22810;&#32771;&#34385;&#23436;&#20840;&#35266;&#23519;&#21040;&#30340;&#19978;&#19979;&#25991;&#24773;&#22659;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#29702;&#35770;&#19978;&#26356;&#19968;&#33324;&#19988;&#22312;&#23454;&#36341;&#20013;&#26356;&#26377;&#22810;&#26679;&#24615;&#65292;&#20294;&#37096;&#20998;&#19978;&#19979;&#25991;&#35266;&#23519;&#24773;&#26223;&#33267;&#20170;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#35266;&#23519;&#25968;&#25454;&#26159;&#26410;&#35266;&#23519;&#21040;&#30340;&#19978;&#19979;&#25991;&#21521;&#37327;&#30340;&#22122;&#22768;&#32447;&#24615;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#36873;&#25321;&#26368;&#20339;&#33218;&#30340;&#32769;&#34382;&#26426;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;Thompson&#37319;&#26679;&#31639;&#27861;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10289v1 Announce Type: cross  Abstract: Contextual bandits constitute a classical framework for decision-making under uncertainty. In this setting, the goal is to learn the arms of highest reward subject to contextual information, while the unknown reward parameters of each arm need to be learned by experimenting that specific arm. Accordingly, a fundamental problem is that of balancing exploration (i.e., pulling different arms to learn their parameters), versus exploitation (i.e., pulling the best arms to gain reward). To study this problem, the existing literature mostly considers perfectly observed contexts. However, the setting of partial context observations remains unexplored to date, despite being theoretically more general and practically more versatile. We study bandit policies for learning to select optimal arms based on the data of observations, which are noisy linear functions of the unobserved context vectors. Our theoretical analysis shows that the Thompson sam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#31574;&#30053;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#33391;&#24615;&#27491;&#24120;&#25968;&#25454;&#20013;&#21046;&#20316;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#20854;&#27880;&#20837;&#27169;&#22411;&#65292;&#25104;&#21151;&#22949;&#21327;&#20102;&#20004;&#20010;&#19968;&#31867;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.10283</link><description>&lt;p&gt;
&#23545;&#19968;&#31867;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Backdoor Attack against One-Class Sequential Anomaly Detection Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#31574;&#30053;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#33391;&#24615;&#27491;&#24120;&#25968;&#25454;&#20013;&#21046;&#20316;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#20854;&#27880;&#20837;&#27169;&#22411;&#65292;&#25104;&#21151;&#22949;&#21327;&#20102;&#20004;&#20010;&#19968;&#31867;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10283v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#30028; &#25688;&#35201;&#65306;&#28145;&#24230;&#23398;&#20064;&#22312;&#24207;&#21015;&#25968;&#25454;&#19978;&#30340;&#24322;&#24120;&#26816;&#27979;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#28982;&#32780;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#38754;&#20020;&#19968;&#31181;&#20851;&#38190;&#30340;&#23433;&#20840;&#23041;&#32961; - &#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#21518;&#38376;&#25915;&#20987;&#31574;&#30053;&#26469;&#22949;&#21327;&#28145;&#24230;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#12290;&#25915;&#20987;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#65292;&#35302;&#21457;&#22120;&#29983;&#25104;&#21644;&#21518;&#38376;&#27880;&#20837;&#12290; &#35302;&#21457;&#22120;&#29983;&#25104;&#26159;&#36890;&#36807;&#20174;&#33391;&#24615;&#27491;&#24120;&#25968;&#25454;&#20013;&#21046;&#20316;&#25200;&#21160;&#26679;&#26412;&#26469;&#23548;&#20986;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#35302;&#21457;&#22120;&#65292;&#20854;&#20013;&#25200;&#21160;&#26679;&#26412;&#20173;&#28982;&#27491;&#24120;&#12290; &#21518;&#38376;&#27880;&#20837;&#21017;&#26159;&#36866;&#24403;&#22320;&#23558;&#21518;&#38376;&#35302;&#21457;&#22120;&#27880;&#20837;&#27169;&#22411;&#65292;&#21482;&#20026;&#20855;&#26377;&#35302;&#21457;&#22120;&#30340;&#26679;&#26412;&#12290; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#22312;&#20004;&#20010;&#30693;&#21517;&#30340;&#19968;&#31867;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#19978;&#27880;&#20837;&#21518;&#38376;&#35302;&#21457;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10283v1 Announce Type: cross  Abstract: Deep anomaly detection on sequential data has garnered significant attention due to the wide application scenarios. However, deep learning-based models face a critical security threat - their vulnerability to backdoor attacks. In this paper, we explore compromising deep sequential anomaly detection models by proposing a novel backdoor attack strategy. The attack approach comprises two primary steps, trigger generation and backdoor injection. Trigger generation is to derive imperceptible triggers by crafting perturbed samples from the benign normal data, of which the perturbed samples are still normal. The backdoor injection is to properly inject the backdoor triggers to comprise the model only for the samples with triggers. The experimental results demonstrate the effectiveness of our proposed attack strategy by injecting backdoors on two well-established one-class anomaly detection models.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#23481;&#37327;&#30340;&#26032;&#36951;&#25022;&#30028;&#38480;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#20013;&#20171;&#21453;&#39304;&#30340;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#25932;&#23545;&#21644;&#38543;&#26426;&#35774;&#32622;&#20013;&#25552;&#20379;&#20102;&#36817;&#20046;&#21305;&#37197;&#30340;&#19979;&#30028;&#12290;</title><link>https://arxiv.org/abs/2402.10282</link><description>&lt;p&gt;
&#20855;&#26377;&#20013;&#20171;&#21453;&#39304;&#30340;&#36172;&#21338;&#26426;&#20449;&#24687;&#23481;&#37327;&#36951;&#25022;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Information Capacity Regret Bounds for Bandits with Mediator Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10282
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#23481;&#37327;&#30340;&#26032;&#36951;&#25022;&#30028;&#38480;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#20013;&#20171;&#21453;&#39304;&#30340;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#25932;&#23545;&#21644;&#38543;&#26426;&#35774;&#32622;&#20013;&#25552;&#20379;&#20102;&#36817;&#20046;&#21305;&#37197;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#20013;&#20171;&#21453;&#39304;&#38382;&#39064;&#65292;&#21363;&#20915;&#31574;&#38598;&#21253;&#25324;&#22810;&#20010;&#31574;&#30053;&#65292;&#27599;&#20010;&#31574;&#30053;&#19982;&#20849;&#21516;&#32467;&#26524;&#31354;&#38388;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#30456;&#20851;&#32852;&#12290;&#36873;&#25321;&#19968;&#20010;&#31574;&#30053;&#21518;&#65292;&#23398;&#20064;&#32773;&#35266;&#23519;&#20174;&#20854;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#24403;&#21069;&#22238;&#21512;&#20013;&#25215;&#25285;&#20998;&#37197;&#32473;&#35813;&#32467;&#26524;&#30340;&#25439;&#22833;&#12290;&#25105;&#20204;&#24341;&#20837;&#31574;&#30053;&#38598;&#23481;&#37327;&#20316;&#20026;&#34913;&#37327;&#31574;&#30053;&#38598;&#22797;&#26434;&#24615;&#30340;&#20449;&#24687;&#35770;&#25351;&#26631;&#12290;&#37319;&#29992;&#32463;&#20856;&#30340;EXP4&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#21462;&#20915;&#20110;&#31574;&#30053;&#38598;&#23481;&#37327;&#22312;&#25932;&#23545;&#21644;&#38543;&#26426;&#35774;&#32622;&#20013;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#19968;&#20123;&#31574;&#30053;&#38598;&#23478;&#26063;&#30340;&#36873;&#25321;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36817;&#20046;&#21305;&#37197;&#30340;&#19979;&#30028;&#65292;&#19982;&#23481;&#37327;&#31867;&#20284;&#22320;&#25193;&#23637;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#31574;&#30053;&#20998;&#24067;&#22312;&#22238;&#21512;&#20043;&#38388;&#21487;&#20197;&#21464;&#21270;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#30456;&#20851;&#30340;&#20855;&#26377;&#19987;&#23478;&#24314;&#35758;&#30340;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#20854;&#20808;&#21069;&#32467;&#26524;&#19978;&#26377;&#25152;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10282v1 Announce Type: new  Abstract: This work addresses the mediator feedback problem, a bandit game where the decision set consists of a number of policies, each associated with a probability distribution over a common space of outcomes. Upon choosing a policy, the learner observes an outcome sampled from its distribution and incurs the loss assigned to this outcome in the present round. We introduce the policy set capacity as an information-theoretic measure for the complexity of the policy set. Adopting the classical EXP4 algorithm, we provide new regret bounds depending on the policy set capacity in both the adversarial and the stochastic settings. For a selection of policy set families, we prove nearly-matching lower bounds, scaling similarly with the capacity. We also consider the case when the policies' distributions can vary between rounds, thus addressing the related bandits with expert advice problem, which we improve upon its prior results. Additionally, we prov
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SusFL&#31995;&#32479;&#65292;&#21033;&#29992;&#33021;&#37327;&#24863;&#30693;&#32852;&#21512;&#23398;&#20064;&#25216;&#26415;&#20026;&#26234;&#33021;&#20892;&#22330;&#30417;&#27979;&#25552;&#20379;&#21487;&#25345;&#32493;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#26234;&#33021;&#23458;&#25143;&#31471;&#36873;&#25321;&#20248;&#21270;&#30417;&#27979;&#36136;&#37327;&#65292;&#21516;&#26102;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#65292;&#24182;&#20855;&#22791;&#25269;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10280</link><description>&lt;p&gt;
SusFL: &#21487;&#25345;&#32493;&#26234;&#33021;&#20892;&#22330;&#30340;&#33021;&#37327;&#24863;&#30693;&#32852;&#21512;&#23398;&#20064;&#30417;&#25511;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SusFL: Energy-Aware Federated Learning-based Monitoring for Sustainable Smart Farms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10280
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SusFL&#31995;&#32479;&#65292;&#21033;&#29992;&#33021;&#37327;&#24863;&#30693;&#32852;&#21512;&#23398;&#20064;&#25216;&#26415;&#20026;&#26234;&#33021;&#20892;&#22330;&#30417;&#27979;&#25552;&#20379;&#21487;&#25345;&#32493;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#26234;&#33021;&#23458;&#25143;&#31471;&#36873;&#25321;&#20248;&#21270;&#30417;&#27979;&#36136;&#37327;&#65292;&#21516;&#26102;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#65292;&#24182;&#20855;&#22791;&#25269;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33021;&#37327;&#24863;&#30693;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#31995;&#32479;&#65292;&#21517;&#20026;SusFL&#65292;&#29992;&#20110;&#21487;&#25345;&#32493;&#26234;&#33021;&#20892;&#19994;&#65292;&#26088;&#22312;&#35299;&#20915;&#30001;&#20110;&#22826;&#38451;&#33021;&#20256;&#24863;&#22120;&#33021;&#37327;&#27700;&#24179;&#27874;&#21160;&#32780;&#23548;&#33268;&#30340;&#19981;&#19968;&#33268;&#30340;&#20581;&#24247;&#30417;&#27979;&#25361;&#25112;&#12290;&#35813;&#31995;&#32479;&#37197;&#22791;&#20102;&#20855;&#26377;&#35745;&#31639;&#33021;&#21147;&#30340;&#22826;&#38451;&#33021;&#20256;&#24863;&#22120;&#65292;&#22914;&#26641;&#33683;&#27966;&#65292;&#29992;&#20110;&#22312;&#20581;&#24247;&#25968;&#25454;&#19978;&#35757;&#32451;&#26412;&#22320;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#20123;&#20256;&#24863;&#22120;&#23450;&#26399;&#26356;&#26032;LoRa&#32593;&#20851;&#65292;&#24418;&#25104;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#65288;WSN&#65289;&#65292;&#20197;&#20415;&#26816;&#27979;&#20083;&#33146;&#28814;&#31561;&#30142;&#30149;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;SusFL&#31995;&#32479;&#34701;&#21512;&#20102;&#26426;&#21046;&#35774;&#35745;&#65292;&#21363;&#21338;&#24328;&#35770;&#27010;&#24565;&#65292;&#29992;&#20110;&#26234;&#33021;&#23458;&#25143;&#31471;&#36873;&#25321;&#20197;&#20248;&#21270;&#30417;&#27979;&#36136;&#37327;&#21516;&#26102;&#26368;&#23567;&#21270;&#33021;&#37327;&#20351;&#29992;&#12290;&#35813;&#31574;&#30053;&#30830;&#20445;&#31995;&#32479;&#23545;&#25239;&#30772;&#22351;FL&#25805;&#20316;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#21253;&#25324;&#21487;&#33021;&#30772;&#22351;FL&#25805;&#20316;&#30340;&#25968;&#25454;&#25237;&#27602;&#21644;&#38544;&#31169;&#23041;&#32961;&#12290;&#36890;&#36807;&#20351;&#29992;&#23454;&#26102;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10280v1 Announce Type: new  Abstract: We propose a novel energy-aware federated learning (FL)-based system, namely SusFL, for sustainable smart farming to address the challenge of inconsistent health monitoring due to fluctuating energy levels of solar sensors. This system equips animals, such as cattle, with solar sensors with computational capabilities, including Raspberry Pis, to train a local deep-learning model on health data. These sensors periodically update Long Range (LoRa) gateways, forming a wireless sensor network (WSN) to detect diseases like mastitis. Our proposed SusFL system incorporates mechanism design, a game theory concept, for intelligent client selection to optimize monitoring quality while minimizing energy use. This strategy ensures the system's sustainability and resilience against adversarial attacks, including data poisoning and privacy threats, that could disrupt FL operations. Through extensive comparative analysis using real-time datasets, we de
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934; StrongREJECT&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#39640;&#36136;&#37327;&#30340;&#38382;&#39064;&#65292;&#26356;&#22909;&#22320;&#21306;&#20998;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#31354;&#30772;&#35299;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10260</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#31354;&#30772;&#35299;&#30340;&#24378;REJECT&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A StrongREJECT for Empty Jailbreaks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10260
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934; StrongREJECT&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#39640;&#36136;&#37327;&#30340;&#38382;&#39064;&#65292;&#26356;&#22909;&#22320;&#21306;&#20998;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#31354;&#30772;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#24341;&#36215;&#20102;&#23545;&#8220;&#30772;&#35299;&#8221;&#30340;&#20851;&#27880;&#65292;&#36825;&#31181;&#30772;&#35299;&#20801;&#35768;&#27169;&#22411;&#34987;&#24694;&#24847;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#26631;&#20934;&#30340;&#22522;&#20934;&#26469;&#34913;&#37327;&#30772;&#35299;&#30340;&#20005;&#37325;&#31243;&#24230;&#65292;&#23548;&#33268;&#30772;&#35299;&#35770;&#25991;&#30340;&#20316;&#32773;&#19981;&#24471;&#19981;&#33258;&#34892;&#21019;&#24314;&#26631;&#20934;&#12290;&#25105;&#20204;&#34920;&#26126;&#36825;&#20123;&#22522;&#20934;&#32463;&#24120;&#21253;&#21547;&#27169;&#26865;&#20004;&#21487;&#25110;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20542;&#21521;&#20110;&#39640;&#20272;&#20302;&#36136;&#37327;&#27169;&#22411;&#21709;&#24212;&#30340;&#28389;&#29992;&#28508;&#21147;&#30340;&#35780;&#20998;&#26631;&#20934;&#12290;&#19968;&#20123;&#30772;&#35299;&#25216;&#26415;&#20351;&#38382;&#39064;&#26356;&#21152;&#20005;&#37325;&#65292;&#22240;&#20026;&#23427;&#20204;&#21363;&#20351;&#23545;&#20110;&#33391;&#24615;&#38382;&#39064;&#20063;&#20250;&#38477;&#20302;&#27169;&#22411;&#21709;&#24212;&#30340;&#36136;&#37327;&#65306;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#31181;&#30772;&#35299;&#25216;&#26415;&#26174;&#30528;&#38477;&#20302;&#20102;GPT-4&#22312;MMLU&#19978;&#30340;&#38646;&#23556;&#20987;&#34920;&#29616;&#12290;&#30772;&#35299;&#36824;&#20250;&#20351;&#20174;&#8220;&#26410;&#32463;&#23457;&#26597;&#8221;&#30340;&#24320;&#28304;&#27169;&#22411;&#20013;&#33719;&#21462;&#26377;&#23475;&#21709;&#24212;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;StrongREJECT&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#39640;&#36136;&#37327;&#30340;&#38382;&#39064;&#26356;&#22909;&#22320;&#21306;&#20998;&#26377;&#25928;&#21644;&#26080;&#25928;&#30340;&#30772;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10260v1 Announce Type: cross  Abstract: The rise of large language models (LLMs) has drawn attention to the existence of "jailbreaks" that allow the models to be used maliciously. However, there is no standard benchmark for measuring the severity of a jailbreak, leaving authors of jailbreak papers to create their own. We show that these benchmarks often include vague or unanswerable questions and use grading criteria that are biased towards overestimating the misuse potential of low-quality model responses. Some jailbreak techniques make the problem worse by decreasing the quality of model responses even on benign questions: we show that several jailbreaking techniques substantially reduce the zero-shot performance of GPT-4 on MMLU. Jailbreaks can also make it harder to elicit harmful responses from an "uncensored" open-source model. We present a new benchmark, StrongREJECT, which better discriminates between effective and ineffective jailbreaks by using a higher-quality que
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#30340;&#24403;&#21069;&#30740;&#31350;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#35299;&#20915;&#32479;&#35745;&#24322;&#36136;&#24615;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#38556;&#30861;&#12290;</title><link>https://arxiv.org/abs/2402.10254</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Personalized Federated Learning for Statistical Heterogeneity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10254
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#30340;&#24403;&#21069;&#30740;&#31350;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#35299;&#20915;&#32479;&#35745;&#24322;&#36136;&#24615;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#35752;&#35770;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#38543;&#20043;&#32780;&#26469;&#30340;&#26159;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#20851;&#20110;&#25968;&#25454;&#38544;&#31169;&#30340;&#26085;&#30410;&#20851;&#27880;&#12290; FL&#20419;&#36827;&#20102;&#21327;&#20316;&#24335;&#22810;&#26041;&#27169;&#22411;&#23398;&#20064;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#25968;&#25454;&#20445;&#23494;&#24615;&#12290; &#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#22312;&#22810;&#26679;&#21270;&#23458;&#25143;&#25968;&#25454;&#20998;&#24067;&#32780;&#24341;&#36215;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#24102;&#26469;&#20102;&#19968;&#23450;&#30340;&#25361;&#25112;&#65292;&#22914;&#20010;&#24615;&#21270;&#19981;&#36275;&#21644;&#25910;&#25947;&#36895;&#24230;&#24930;&#12290; &#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#31616;&#35201;&#24635;&#32467;&#20102;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#39046;&#22495;&#30340;&#24403;&#21069;&#30740;&#31350;&#36827;&#23637;&#12290; &#23427;&#27010;&#36848;&#20102;PFL&#27010;&#24565;&#65292;&#23457;&#35270;&#20102;&#30456;&#20851;&#25216;&#26415;&#65292;&#24182;&#31361;&#20986;&#20102;&#24403;&#21069;&#30340;&#21162;&#21147;&#12290; &#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#19982;PFL&#30456;&#20851;&#30340;&#28508;&#22312;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10254v1 Announce Type: new  Abstract: The popularity of federated learning (FL) is on the rise, along with growing concerns about data privacy in artificial intelligence applications. FL facilitates collaborative multi-party model learning while simultaneously ensuring the preservation of data confidentiality. Nevertheless, the problem of statistical heterogeneity caused by the presence of diverse client data distributions gives rise to certain challenges, such as inadequate personalization and slow convergence. In order to address the above issues, this paper offers a brief summary of the current research progress in the field of personalized federated learning (PFL). It outlines the PFL concept, examines related techniques, and highlights current endeavors. Furthermore, this paper also discusses potential further research and obstacles associated with PFL.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#22312;&#32447;&#25511;&#21046;&#38382;&#39064;&#20013;&#65292;&#23545;&#20110;&#20984;&#25104;&#26412;&#65292;&#21487;&#20197;&#23454;&#29616; $ \widetilde{O}(\sqrt{T}) $ &#30340;&#36951;&#25022;&#30028;&#65292;&#29978;&#33267;&#22312;&#23384;&#22312;&#26080;&#30028;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65307;&#21516;&#26102;&#65292;&#22312;&#25104;&#26412;&#20855;&#26377;&#24378;&#20984;&#24615;&#26102;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#22122;&#22768;&#21327;&#26041;&#24046;&#26159;&#38750;&#36864;&#21270;&#30340;&#24773;&#20917;&#19979;&#24314;&#31435; $ O({\rm poly} (\log T)) $ &#30340;&#36951;&#25022;&#30028;&#12290;</title><link>https://arxiv.org/abs/2402.10252</link><description>&lt;p&gt;
&#20855;&#26377;&#26080;&#30028;&#21644;&#36864;&#21270;&#22122;&#22768;&#30340;&#32447;&#24615;&#31995;&#32479;&#22312;&#32447;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Online Control of Linear Systems with Unbounded and Degenerate Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10252
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#22312;&#32447;&#25511;&#21046;&#38382;&#39064;&#20013;&#65292;&#23545;&#20110;&#20984;&#25104;&#26412;&#65292;&#21487;&#20197;&#23454;&#29616; $ \widetilde{O}(\sqrt{T}) $ &#30340;&#36951;&#25022;&#30028;&#65292;&#29978;&#33267;&#22312;&#23384;&#22312;&#26080;&#30028;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65307;&#21516;&#26102;&#65292;&#22312;&#25104;&#26412;&#20855;&#26377;&#24378;&#20984;&#24615;&#26102;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#22122;&#22768;&#21327;&#26041;&#24046;&#26159;&#38750;&#36864;&#21270;&#30340;&#24773;&#20917;&#19979;&#24314;&#31435; $ O({\rm poly} (\log T)) $ &#30340;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21487;&#33021;&#23384;&#22312;&#26080;&#30028;&#21644;&#36864;&#21270;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#25511;&#21046;&#32447;&#24615;&#31995;&#32479;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#25104;&#26412;&#20989;&#25968;&#26410;&#30693;&#65292;&#34987;&#31216;&#20026;&#22312;&#32447;&#25511;&#21046;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#30340;&#20165;&#20551;&#35774;&#22122;&#22768;&#26377;&#30028;&#24615;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23545;&#20110;&#20984;&#25104;&#26412;&#65292;&#21363;&#20351;&#22312;&#23384;&#22312;&#26080;&#30028;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#23454;&#29616; $ \widetilde{O}(\sqrt{T}) $ &#30340;&#36951;&#25022;&#30028;&#65292;&#20854;&#20013; $ T $ &#34920;&#31034;&#26102;&#38388;&#36328;&#24230;&#12290;&#27492;&#22806;&#65292;&#24403;&#25104;&#26412;&#20855;&#26377;&#24378;&#20984;&#24615;&#26102;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010; $ O({\rm poly} (\log T)) $ &#30340;&#36951;&#25022;&#30028;&#65292;&#32780;&#19981;&#38656;&#35201;&#22122;&#22768;&#21327;&#26041;&#24046;&#26159;&#38750;&#36864;&#21270;&#30340;&#20551;&#35774;&#65292;&#36825;&#22312;&#25991;&#29486;&#20013;&#26159;&#24517;&#38656;&#30340;&#12290;&#28040;&#38500;&#22122;&#22768;&#31209;&#30340;&#20851;&#38190;&#26159;&#19982;&#22122;&#22768;&#21327;&#26041;&#24046;&#30456;&#20851;&#32852;&#30340;&#31995;&#32479;&#36716;&#21270;&#12290;&#36825;&#21516;&#26102;&#23454;&#29616;&#20102;&#22312;&#32447;&#25511;&#21046;&#31639;&#27861;&#30340;&#21442;&#25968;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10252v1 Announce Type: cross  Abstract: This paper investigates the problem of controlling a linear system under possibly unbounded and degenerate noise with unknown cost functions, known as an online control problem. In contrast to the existing work, which assumes the boundedness of noise, we reveal that for convex costs, an $ \widetilde{O}(\sqrt{T}) $ regret bound can be achieved even for unbounded noise, where $ T $ denotes the time horizon. Moreover, when the costs are strongly convex, we establish an $ O({\rm poly} (\log T)) $ regret bound without the assumption that noise covariance is non-degenerate, which has been required in the literature. The key ingredient in removing the rank assumption on noise is a system transformation associated with the noise covariance. This simultaneously enables the parameter reduction of an online control algorithm.
&lt;/p&gt;</description></item><item><title>Brant-2&#26159;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;Brant&#65292;&#23427;&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36824;&#33021;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.10251</link><description>&lt;p&gt;
Brant-2&#65306;&#33041;&#20449;&#21495;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Brant-2: Foundation Model for Brain Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10251
&lt;/p&gt;
&lt;p&gt;
Brant-2&#26159;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;Brant&#65292;&#23427;&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36824;&#33021;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#21463;&#30410;&#20110;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#31181;&#27169;&#22411;&#22312;&#20998;&#26512;&#33041;&#20449;&#21495;&#26041;&#38754;&#29305;&#21035;&#26377;&#25928;&#65292;&#22240;&#20026;&#36825;&#19968;&#39046;&#22495;&#28085;&#30422;&#20102;&#20247;&#22810;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#19988;&#36827;&#34892;&#22823;&#35268;&#27169;&#27880;&#37322;&#26159;&#25104;&#26412;&#39640;&#26114;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;Brant-2&#12290;&#19982;&#29992;&#20110;&#39045;&#20869;&#31070;&#32463;&#20449;&#21495;&#30340;&#22522;&#30784;&#27169;&#22411;Brant&#30456;&#27604;&#65292;Brant-2&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#34920;&#29616;&#20986;&#31283;&#20581;&#24615;&#65292;&#32780;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#22823;&#37327;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Brant-2&#23545;&#33041;&#20449;&#21495;&#20013;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#30340;&#36866;&#24212;&#24615;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#25581;&#31034;&#20102;Brant-2&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#39564;&#35777;&#20102;&#27599;&#20010;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#20445;&#25345;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10251v1 Announce Type: cross  Abstract: Foundational models benefit from pre-training on large amounts of unlabeled data and enable strong performance in a wide variety of applications with a small amount of labeled data. Such models can be particularly effective in analyzing brain signals, as this field encompasses numerous application scenarios, and it is costly to perform large-scale annotation. In this work, we present the largest foundation model in brain signals, Brant-2. Compared to Brant, a foundation model designed for intracranial neural signals, Brant-2 not only exhibits robustness towards data variations and modeling scales but also can be applied to a broader range of brain neural data. By experimenting on an extensive range of tasks, we demonstrate that Brant-2 is adaptive to various application scenarios in brain signals. Further analyses reveal the scalability of the Brant-2, validate each component's effectiveness, and showcase our model's ability to maintai
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#20272;&#35745;&#20840;&#29699;&#29615;&#22659;&#31354;&#27668;&#27745;&#26579;&#27987;&#24230;&#65292;&#24182;&#25552;&#20379;&#39044;&#27979;&#21306;&#38388;&#65292;&#20026;&#21508;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#26816;&#39564;&#27169;&#22411;&#22312;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#19978;&#30340;&#24615;&#33021;&#20026;&#30740;&#31350;&#25552;&#20379;&#27934;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.10248</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#20272;&#35745;&#20840;&#29699;&#29615;&#22659;&#31354;&#27668;&#27745;&#26579;&#27987;&#24230;&#21450;&#20854;&#30456;&#20851;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
A Data-Driven Supervised Machine Learning Approach to Estimating Global Ambient Air Pollution Concentrations With Associated Prediction Intervals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10248
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#20272;&#35745;&#20840;&#29699;&#29615;&#22659;&#31354;&#27668;&#27745;&#26579;&#27987;&#24230;&#65292;&#24182;&#25552;&#20379;&#39044;&#27979;&#21306;&#38388;&#65292;&#20026;&#21508;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#26816;&#39564;&#27169;&#22411;&#22312;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#19978;&#30340;&#24615;&#33021;&#20026;&#30740;&#31350;&#25552;&#20379;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#29615;&#22659;&#31354;&#27668;&#27745;&#26579;&#26159;&#19968;&#20010;&#36328;&#30028;&#25361;&#25112;&#65292;&#36890;&#24120;&#36890;&#36807;&#20381;&#36182;&#31354;&#38388;&#31232;&#30095;&#19988;&#24322;&#26500;&#25918;&#32622;&#30340;&#30417;&#27979;&#31449;&#25968;&#25454;&#30340;&#24178;&#39044;&#26469;&#35299;&#20915;&#12290;&#36825;&#20123;&#31449;&#28857;&#32463;&#24120;&#30001;&#20110;&#35832;&#22914;&#20572;&#30005;&#31561;&#38382;&#39064;&#32780;&#20986;&#29616;&#26102;&#38388;&#25968;&#25454;&#32570;&#22833;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#12289;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#34917;&#20805;&#32570;&#22833;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#27979;&#37327;&#25968;&#25454;&#65292;&#20174;&#32780;&#29983;&#25104;&#21253;&#25324;NO$_2$&#12289;O$_3$&#12289;PM$_{10}$&#12289;PM$_{2.5}$&#21644;SO$_2$&#31561;&#27745;&#26579;&#29289;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#22312;&#27599;&#19968;&#20272;&#35745;&#20540;&#38468;&#24102;&#39044;&#27979;&#21306;&#38388;&#65292;&#24182;&#20026;&#20381;&#36182;&#23460;&#22806;&#31354;&#27668;&#27745;&#26579;&#25968;&#25454;&#36827;&#34892;&#19979;&#28216;&#35780;&#20272;&#30340;&#24191;&#27867;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#26381;&#21153;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#36827;&#34892;&#26356;&#35814;&#32454;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#19978;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#20379;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10248v1 Announce Type: cross  Abstract: Global ambient air pollution, a transboundary challenge, is typically addressed through interventions relying on data from spatially sparse and heterogeneously placed monitoring stations. These stations often encounter temporal data gaps due to issues such as power outages. In response, we have developed a scalable, data-driven, supervised machine learning framework. This model is designed to impute missing temporal and spatial measurements, thereby generating a comprehensive dataset for pollutants including NO$_2$, O$_3$, PM$_{10}$, PM$_{2.5}$, and SO$_2$. The dataset, with a fine granularity of 0.25$^{\circ}$ at hourly intervals and accompanied by prediction intervals for each estimate, caters to a wide range of stakeholders relying on outdoor air pollution data for downstream assessments. This enables more detailed studies. Additionally, the model's performance across various geographical locations is examined, providing insights an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#27010;&#29575;&#22270;&#27169;&#22411;&#26469;&#29702;&#35299;&#22242;&#38431;&#30340;&#21160;&#24577;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#22242;&#38431;&#23849;&#28291;&#29616;&#35937;&#30340;&#20027;&#35201;&#21407;&#22240;&#20197;&#21450;&#26500;&#24314;&#24377;&#24615;&#22242;&#38431;&#30340;&#21407;&#21017;&#26469;&#23637;&#31034;&#20854;&#22312;&#22242;&#38431;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.10243</link><description>&lt;p&gt;
&#36890;&#36807;&#27010;&#29575;&#22270;&#27169;&#22411;&#29702;&#35299;&#22242;&#38431;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Understanding team collapse via probabilistic graphical models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#27010;&#29575;&#22270;&#27169;&#22411;&#26469;&#29702;&#35299;&#22242;&#38431;&#30340;&#21160;&#24577;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#22242;&#38431;&#23849;&#28291;&#29616;&#35937;&#30340;&#20027;&#35201;&#21407;&#22240;&#20197;&#21450;&#26500;&#24314;&#24377;&#24615;&#22242;&#38431;&#30340;&#21407;&#21017;&#26469;&#23637;&#31034;&#20854;&#22312;&#22242;&#38431;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22270;&#27169;&#22411;&#26469;&#25429;&#25417;&#22242;&#38431;&#21160;&#24577;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#35813;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20854;&#21442;&#25968;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#20174;&#35745;&#31639;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22242;&#38431;&#23849;&#28291;&#29616;&#35937;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#26469;&#25214;&#20986;&#22242;&#38431;&#23849;&#28291;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26500;&#24314;&#24377;&#24615;&#22242;&#38431;&#30340;&#21407;&#21017;&#65292;&#21363;&#36991;&#20813;&#23849;&#28291;&#30340;&#22242;&#38431;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#27169;&#22411;&#26469;&#20998;&#26512;NBA&#29699;&#38431;&#30340;&#32467;&#26500;&#65292;&#24182;&#28145;&#20837;&#30740;&#31350;&#24863;&#20852;&#36259;&#30340;&#27604;&#36187;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10243v1 Announce Type: cross  Abstract: In this work, we develop a graphical model to capture team dynamics. We analyze the model and show how to learn its parameters from data. Using our model we study the phenomenon of team collapse from a computational perspective. We use simulations and real-world experiments to find the main causes of team collapse. We also provide the principles of building resilient teams, i.e., teams that avoid collapsing. Finally, we use our model to analyze the structure of NBA teams and dive deeper into games of interest.
&lt;/p&gt;</description></item><item><title>&#20445;&#30041;&#36793;&#30340;&#31526;&#21495;&#22312;&#32593;&#32476;&#26500;&#24314;&#36807;&#31243;&#20013;&#25552;&#39640;&#20102;&#20272;&#35745;&#21644;&#32858;&#31867;&#31934;&#24230;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10242</link><description>&lt;p&gt;
&#26377;&#31526;&#21495;&#22810;&#26679;&#21270;&#22810;&#37325;&#32593;&#32476;&#65306;&#32858;&#31867;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Signed Diverse Multiplex Networks: Clustering and Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10242
&lt;/p&gt;
&lt;p&gt;
&#20445;&#30041;&#36793;&#30340;&#31526;&#21495;&#22312;&#32593;&#32476;&#26500;&#24314;&#36807;&#31243;&#20013;&#25552;&#39640;&#20102;&#20272;&#35745;&#21644;&#32858;&#31867;&#31934;&#24230;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#31526;&#21495;&#30340;&#24191;&#20041;&#38543;&#26426;&#28857;&#31215;&#22270;&#65288;SGRDPG&#65289;&#27169;&#22411;&#65292;&#36825;&#26159;&#24191;&#20041;&#38543;&#26426;&#28857;&#31215;&#22270;&#65288;GRDPG&#65289;&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#20854;&#20013;&#36793;&#21487;&#20197;&#26159;&#27491;&#30340;&#20063;&#21487;&#20197;&#26159;&#36127;&#30340;&#12290;&#35813;&#35774;&#32622;&#34987;&#25193;&#23637;&#20026;&#22810;&#37325;&#32593;&#32476;&#29256;&#26412;&#65292;&#20854;&#20013;&#25152;&#26377;&#23618;&#20855;&#26377;&#30456;&#21516;&#30340;&#33410;&#28857;&#38598;&#21512;&#24182;&#36981;&#24490;SGRDPG&#12290;&#32593;&#32476;&#23618;&#30340;&#21807;&#19968;&#20844;&#20849;&#29305;&#24449;&#26159;&#23427;&#20204;&#21487;&#20197;&#34987;&#21010;&#20998;&#20026;&#20855;&#26377;&#20849;&#21516;&#23376;&#31354;&#38388;&#32467;&#26500;&#30340;&#32452;&#65292;&#32780;&#20854;&#20182;&#24773;&#20917;&#19979;&#25152;&#26377;&#36830;&#25509;&#27010;&#29575;&#30697;&#38453;&#21487;&#33021;&#26159;&#23436;&#20840;&#19981;&#21516;&#30340;&#12290;&#19978;&#36848;&#35774;&#32622;&#38750;&#24120;&#28789;&#27963;&#65292;&#24182;&#21253;&#25324;&#21508;&#31181;&#29616;&#26377;&#22810;&#37325;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#20854;&#29305;&#20363;&#12290;&#35770;&#25991;&#23454;&#29616;&#20102;&#20004;&#20010;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#23427;&#34920;&#26126;&#22312;&#32593;&#32476;&#26500;&#24314;&#36807;&#31243;&#20013;&#20445;&#30041;&#36793;&#30340;&#31526;&#21495;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#20272;&#35745;&#21644;&#32858;&#31867;&#31934;&#24230;&#65292;&#22240;&#27492;&#26377;&#21161;&#20110;&#24212;&#23545;&#35832;&#22914;&#22823;&#33041;&#32593;&#32476;&#20998;&#26512;&#20043;&#31867;&#30340;&#29616;&#23454;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10242v1 Announce Type: cross  Abstract: The paper introduces a Signed Generalized Random Dot Product Graph (SGRDPG) model, which is a variant of the Generalized Random Dot Product Graph (GRDPG), where, in addition, edges can be positive or negative. The setting is extended to a multiplex version, where all layers have the same collection of nodes and follow the SGRDPG. The only common feature of the layers of the network is that they can be partitioned into groups with common subspace structures, while otherwise all matrices of connection probabilities can be all different. The setting above is extremely flexible and includes a variety of existing multiplex network models as its particular cases. The paper fulfills two objectives. First, it shows that keeping signs of the edges in the process of network construction leads to a better precision of estimation and clustering and, hence, is beneficial for tackling real world problems such as analysis of brain networks. Second, b
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#36807;&#31243;&#20013;&#30452;&#25509;&#24314;&#31435;&#20107;&#20214;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#35745;&#31639;&#22240;&#26524;&#36129;&#29486;&#30340;&#20004;&#20010;&#20851;&#38190;&#24341;&#29702;&#65292;&#21487;&#20197;&#25581;&#31034;&#21644;&#37327;&#21270;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.10240</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#38382;&#39064;&#30340;&#21160;&#24577;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Dynamical View of the Question of Why
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10240
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26102;&#38388;&#36807;&#31243;&#20013;&#30452;&#25509;&#24314;&#31435;&#20107;&#20214;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#35745;&#31639;&#22240;&#26524;&#36129;&#29486;&#30340;&#20004;&#20010;&#20851;&#38190;&#24341;&#29702;&#65292;&#21487;&#20197;&#25581;&#31034;&#21644;&#37327;&#21270;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#30001;&#38543;&#26426;&#36807;&#31243;&#29983;&#25104;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#25512;&#29702;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#23616;&#38480;&#20110;&#38745;&#24577;&#35774;&#32622;&#65292;&#24573;&#30053;&#20102;&#26102;&#38388;&#19978;&#30340;&#36830;&#32493;&#24615;&#21644;&#21464;&#21270;&#30340;&#21457;&#23556;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#33539;&#24335;&#65292;&#30452;&#25509;&#22312;&#26102;&#38388;&#36807;&#31243;&#20013;&#24314;&#31435;&#20107;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#24341;&#29702;&#26469;&#35745;&#31639;&#22240;&#26524;&#36129;&#29486;&#65292;&#24182;&#23558;&#20854;&#26500;&#36896;&#20026;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#25581;&#31034;&#21644;&#37327;&#21270;&#25193;&#25955;&#36807;&#31243;&#20013;&#22240;&#26524;&#20851;&#31995;&#30340;&#24418;&#24335;&#21270;&#21644;&#35745;&#31639;&#24037;&#20855;&#65292;&#21253;&#25324;&#21508;&#31181;&#37325;&#35201;&#35774;&#32622;&#65292;&#22914;&#31163;&#25955;&#26102;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#30456;&#24403;&#22797;&#26434;&#30340;&#23454;&#39564;&#21644;&#36890;&#36807;&#32431;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25581;&#31034;&#21644;&#37327;&#21270;&#20102;&#22240;&#26524;&#32852;&#31995;&#65292;&#21542;&#21017;&#30475;&#20284;&#33707;&#21517;&#20854;&#22937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10240v1 Announce Type: cross  Abstract: We address causal reasoning in multivariate time series data generated by stochastic processes. Existing approaches are largely restricted to static settings, ignoring the continuity and emission of variations across time. In contrast, we propose a learning paradigm that directly establishes causation between events in the course of time. We present two key lemmas to compute causal contributions and frame them as reinforcement learning problems. Our approach offers formal and computational tools for uncovering and quantifying causal relationships in diffusion processes, subsuming various important settings such as discrete-time Markov decision processes. Finally, in fairly intricate experiments and through sheer learning, our framework reveals and quantifies causal links, which otherwise seem inexplicable.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#32479;&#19968;&#19981;&#21516;&#31890;&#23376;&#36319;&#36394;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20986;&#33021;&#22815;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#30340;&#31890;&#23376;&#36319;&#36394;BERT&#27169;&#22411;&#65292;&#20026;&#31890;&#23376;&#25506;&#27979;&#22120;&#29702;&#35299;&#22880;&#23450;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.10239</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#31890;&#23376;&#36319;&#36394;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Language Model for Particle Tracking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10239
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#32479;&#19968;&#19981;&#21516;&#31890;&#23376;&#36319;&#36394;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20986;&#33021;&#22815;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#30340;&#31890;&#23376;&#36319;&#36394;BERT&#27169;&#22411;&#65292;&#20026;&#31890;&#23376;&#25506;&#27979;&#22120;&#29702;&#35299;&#22880;&#23450;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31890;&#23376;&#36319;&#36394;&#23545;&#20110;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#20013;&#20960;&#20046;&#25152;&#26377;&#29289;&#29702;&#20998;&#26512;&#31243;&#24207;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#20110;&#19982;&#31890;&#23376;&#36319;&#36394;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#20570;&#27861;&#26159;&#20026;&#19968;&#20010;&#20219;&#21153;&#35774;&#35745;&#21644;&#35757;&#32451;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#12290;&#35757;&#32451;&#30340;&#27169;&#22411;&#23545;&#20854;&#35757;&#32451;&#30340;&#20219;&#21153;&#25928;&#26524;&#33391;&#22909;&#65292;&#20294;&#26174;&#31034;&#20986;&#20960;&#20046;&#27809;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#32479;&#19968;&#36825;&#20123;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#35760;&#21270;&#25506;&#27979;&#22120;&#34920;&#31034;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#31890;&#23376;&#36319;&#36394;&#35757;&#32451;&#19968;&#20010;BERT&#27169;&#22411;&#12290;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#65292;&#21363;TrackingBERT&#65292;&#25552;&#20379;&#20102;&#21487;&#20197;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#30340;&#28508;&#22312;&#25506;&#27979;&#22120;&#27169;&#22359;&#23884;&#20837;&#12290;&#36825;&#39033;&#24037;&#20316;&#20195;&#34920;&#20102;&#20026;&#31890;&#23376;&#25506;&#27979;&#22120;&#29702;&#35299;&#24320;&#21457;&#22522;&#30784;&#27169;&#22411;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10239v1 Announce Type: cross  Abstract: Particle tracking is crucial for almost all physics analysis programs at the Large Hadron Collider. Deep learning models are pervasively used in particle tracking related tasks. However, the current practice is to design and train one deep learning model for one task with supervised learning techniques. The trained models work well for tasks they are trained on but show no or little generalization capabilities. We propose to unify these models with a language model. In this paper, we present a tokenized detector representation that allows us to train a BERT model for particle tracking. The trained BERT model, namely TrackingBERT, offers latent detector module embedding that can be used for other tasks. This work represents the first step towards developing a foundational model for particle detector understanding.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;FNO&#21644;CNN&#65289;&#23398;&#20064;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26102;&#38388;&#25512;&#36827;&#31639;&#23376;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22312;&#19981;&#21516;&#21442;&#25968;&#26465;&#20214;&#19979;&#20934;&#30830;&#39044;&#27979;&#30701;&#26399;&#35299;&#24182;&#25552;&#20379;&#31283;&#20581;&#30340;&#38271;&#26399;&#32479;&#35745;&#20449;&#24687;&#65292;&#26377;&#21161;&#20110;&#33410;&#32422;&#35745;&#31639;&#25104;&#26412;&#24182;&#21152;&#36895;&#24037;&#31243;&#20223;&#30495;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2402.10238</link><description>&lt;p&gt;
&#29992;&#20110;&#19981;&#31283;&#23450;&#28779;&#28976;&#28436;&#21270;&#30340;&#26102;&#38388;&#25512;&#36827;&#31639;&#23376;&#30340;&#21442;&#25968;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Parametric Learning of Time-Advancement Operators for Unstable Flame Evolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;FNO&#21644;CNN&#65289;&#23398;&#20064;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26102;&#38388;&#25512;&#36827;&#31639;&#23376;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22312;&#19981;&#21516;&#21442;&#25968;&#26465;&#20214;&#19979;&#20934;&#30830;&#39044;&#27979;&#30701;&#26399;&#35299;&#24182;&#25552;&#20379;&#31283;&#20581;&#30340;&#38271;&#26399;&#32479;&#35745;&#20449;&#24687;&#65292;&#26377;&#21161;&#20110;&#33410;&#32422;&#35745;&#31639;&#25104;&#26412;&#24182;&#21152;&#36895;&#24037;&#31243;&#20223;&#30495;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#65292;&#20855;&#20307;&#32780;&#35328;&#26159;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNO&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#24212;&#29992;&#20110;&#23398;&#20064;&#21442;&#25968;&#21270;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#26102;&#38388;&#25512;&#36827;&#31639;&#23376;&#12290;&#25105;&#20204;&#20851;&#27880;&#20110;&#25193;&#23637;&#29616;&#26377;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#22788;&#29702;&#20195;&#34920;PDE&#21442;&#25968;&#30340;&#38468;&#21152;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#32479;&#19968;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20934;&#30830;&#39044;&#27979;&#30701;&#26399;&#35299;&#24182;&#22312;&#19981;&#21516;&#21442;&#25968;&#26465;&#20214;&#19979;&#25552;&#20379;&#31283;&#20581;&#30340;&#38271;&#26399;&#32479;&#35745;&#20449;&#24687;&#65292;&#20197;&#20419;&#36827;&#24037;&#31243;&#20223;&#30495;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#33410;&#32422;&#21644;&#24320;&#21457;&#21152;&#36895;&#12290;&#25105;&#20204;&#24320;&#21457;&#24182;&#27604;&#36739;&#20102;&#22522;&#20110;FNO&#21644;CNN&#30340;&#21442;&#25968;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#23398;&#20064;&#19968;&#32500;PDE&#21644;&#30001;Navier-Stokes&#26041;&#31243;&#30340;&#30452;&#25509;&#25968;&#20540;&#27169;&#25311;&#33719;&#24471;&#30340;&#23454;&#38469;&#28779;&#28976;&#21069;&#27839;&#28436;&#21270;&#25968;&#25454;&#30340;&#21442;&#25968;&#20381;&#36182;&#35299;&#26102;&#38388;&#25512;&#36827;&#31639;&#23376;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10238v1 Announce Type: new  Abstract: This study investigates the application of machine learning, specifically Fourier Neural Operator (FNO) and Convolutional Neural Network (CNN), to learn time-advancement operators for parametric partial differential equations (PDEs). Our focus is on extending existing operator learning methods to handle additional inputs representing PDE parameters. The goal is to create a unified learning approach that accurately predicts short-term solutions and provides robust long-term statistics under diverse parameter conditions, facilitating computational cost savings and accelerating development in engineering simulations. We develop and compare parametric learning methods based on FNO and CNN, evaluating their effectiveness in learning parametric-dependent solution time-advancement operators for one-dimensional PDEs and realistic flame front evolution data obtained from direct numerical simulations of the Navier-Stokes equations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#22810;&#26679;&#24615;&#25628;&#32034;&#12289;&#35838;&#31243;&#23398;&#20064;&#21644;&#26799;&#24230;&#19979;&#38477;&#31561;&#31639;&#27861;&#65292;&#33258;&#21160;&#25628;&#32034;&#20803;&#32990;&#33258;&#21160;&#26426;&#20013;&#33021;&#22815;&#33258;&#32452;&#32455;&#20986;&#20855;&#26377;&#22522;&#30784;&#24863;&#35273;&#36816;&#21160;&#26426;&#26500;&#30340;&#8220;&#20010;&#20307;&#8221;&#65292;&#20026;&#23547;&#25214;&#36825;&#31181;&#22522;&#26412;&#26426;&#26500;&#33258;&#32452;&#32455;&#30340;&#29615;&#22659;&#26465;&#20214;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10236</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#26679;&#24615;&#25628;&#32034;&#22312;&#20803;&#32990;&#33258;&#21160;&#26426;&#20013;&#21457;&#29616;&#24863;&#35273;&#36816;&#21160;&#26426;&#26500;
&lt;/p&gt;
&lt;p&gt;
Discovering Sensorimotor Agency in Cellular Automata using Diversity Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#22810;&#26679;&#24615;&#25628;&#32034;&#12289;&#35838;&#31243;&#23398;&#20064;&#21644;&#26799;&#24230;&#19979;&#38477;&#31561;&#31639;&#27861;&#65292;&#33258;&#21160;&#25628;&#32034;&#20803;&#32990;&#33258;&#21160;&#26426;&#20013;&#33021;&#22815;&#33258;&#32452;&#32455;&#20986;&#20855;&#26377;&#22522;&#30784;&#24863;&#35273;&#36816;&#21160;&#26426;&#26500;&#30340;&#8220;&#20010;&#20307;&#8221;&#65292;&#20026;&#23547;&#25214;&#36825;&#31181;&#22522;&#26412;&#26426;&#26500;&#33258;&#32452;&#32455;&#30340;&#29615;&#22659;&#26465;&#20214;&#25552;&#20379;&#20102;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#29983;&#21629;&#30740;&#31350;&#39046;&#22495;&#30740;&#31350;&#31867;&#20284;&#29983;&#21629;&#29616;&#35937;&#30340;&#22914;&#33258;&#20027;&#29983;&#25104;&#12289;&#26426;&#26500;&#24615;&#25110;&#33258;&#25105;&#35843;&#33410;&#31561;&#22312;&#35745;&#31639;&#26426;&#27169;&#25311;&#20013;&#22914;&#20309;&#33258;&#32452;&#32455;&#12290;&#22312;&#20803;&#32990;&#33258;&#21160;&#26426;&#65288;CA&#65289;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#26410;&#35299;&#20043;&#35868;&#26159;&#26159;&#21542;&#21487;&#33021;&#25214;&#21040;&#33021;&#22815;&#33258;&#32452;&#32455;&#20986;&#31283;&#20581;&#8220;&#20010;&#20307;&#8221;&#30340;&#29615;&#22659;&#35268;&#21017;&#65292;&#32780;&#36825;&#20123;&#20010;&#20307;&#22312;&#21021;&#22987;&#29366;&#24577;&#19979;&#27809;&#26377;&#8220;&#36523;&#20307;&#8221;&#12289;&#8220;&#22823;&#33041;&#8221;&#12289;&#8220;&#24863;&#30693;&#8221;&#25110;&#8220;&#34892;&#21160;&#8221;&#30340;&#23384;&#22312;&#12290;&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#32467;&#21512;&#22810;&#26679;&#24615;&#25628;&#32034;&#12289;&#35838;&#31243;&#23398;&#20064;&#21644;&#26799;&#24230;&#19979;&#38477;&#31561;&#31639;&#27861;&#65292;&#33258;&#21160;&#25628;&#32034;&#36825;&#20123;&#8220;&#20010;&#20307;&#8221;&#65292;&#21363;&#33021;&#22815;&#31227;&#21160;&#24182;&#26377;&#33021;&#21147;&#20197;&#19968;&#33268;&#30340;&#26041;&#24335;&#23545;&#22806;&#37096;&#38556;&#30861;&#20570;&#20986;&#21453;&#24212;&#19988;&#20445;&#25345;&#23436;&#25972;&#24615;&#30340;&#23616;&#37096;&#32467;&#26500;&#65292;&#20174;&#32780;&#24418;&#25104;&#22522;&#30784;&#24418;&#24335;&#30340;&#24863;&#35273;&#36816;&#21160;&#26426;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#33021;&#22815;&#31995;&#32479;&#22320;&#25214;&#21040;&#22312;CA&#20013;&#23548;&#33268;&#36825;&#31181;&#22522;&#26412;&#26426;&#26500;&#33258;&#32452;&#32455;&#30340;&#29615;&#22659;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10236v1 Announce Type: cross  Abstract: The research field of Artificial Life studies how life-like phenomena such as autopoiesis, agency, or self-regulation can self-organize in computer simulations. In cellular automata (CA), a key open-question has been whether it it is possible to find environment rules that self-organize robust "individuals" from an initial state with no prior existence of things like "bodies", "brain", "perception" or "action". In this paper, we leverage recent advances in machine learning, combining algorithms for diversity search, curriculum learning and gradient descent, to automate the search of such "individuals", i.e. localized structures that move around with the ability to react in a coherent manner to external obstacles and maintain their integrity, hence primitive forms of sensorimotor agency. We show that this approach enables to find systematically environmental conditions in CA leading to self-organization of such basic forms of agency. Th
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;&#32593;&#32476;&#27450;&#20940;&#25152;&#26377;&#35201;&#32032;&#30340;&#24191;&#27867;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#33258;&#21160;&#26816;&#27979;&#32593;&#32476;&#27450;&#20940;&#39046;&#22495;&#30340;&#25968;&#25454;&#32570;&#21475;</title><link>https://arxiv.org/abs/2402.10231</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#32593;&#32476;&#27450;&#20940;&#30340;&#22810;&#26041;&#38754;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Multi-faceted Semi-Synthetic Dataset for Automated Cyberbullying Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10231
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;&#32593;&#32476;&#27450;&#20940;&#25152;&#26377;&#35201;&#32032;&#30340;&#24191;&#27867;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#33258;&#21160;&#26816;&#27979;&#32593;&#32476;&#27450;&#20940;&#39046;&#22495;&#30340;&#25968;&#25454;&#32570;&#21475;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31038;&#20132;&#23186;&#20307;&#30340;&#20351;&#29992;&#26085;&#30410;&#22686;&#38271;&#65292;&#25512;&#21160;&#20102;&#33258;&#21160;&#26816;&#27979;&#32593;&#32476;&#27450;&#20940;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26631;&#20934;&#23450;&#20041;&#21644;&#26222;&#36941;&#25509;&#21463;&#30340;&#25968;&#25454;&#38598;&#65292;&#25361;&#25112;&#20381;&#28982;&#23384;&#22312;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#29616;&#22312;&#23558;&#32593;&#32476;&#27450;&#20940;&#35270;&#20026;&#32593;&#32476;&#20405;&#30053;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#37325;&#22797;&#24615;&#12289;&#21516;&#34892;&#20851;&#31995;&#21644;&#26377;&#24847;&#20260;&#23475;&#31561;&#22240;&#32032;&#65292;&#38500;&#20102;&#22312;&#32447;&#20405;&#30053;&#12290;&#20174;&#31038;&#20132;&#23186;&#20307;&#32593;&#32476;&#20013;&#33719;&#21462;&#21453;&#26144;&#25152;&#26377;&#32593;&#32476;&#27450;&#20940;&#32452;&#25104;&#37096;&#20998;&#30340;&#32508;&#21512;&#25968;&#25454;&#35777;&#26126;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#21253;&#21547;&#32593;&#32476;&#27450;&#20940;&#30340;&#25152;&#26377;&#35201;&#32032;&#30340;&#24191;&#27867;&#21322;&#21512;&#25104;&#32593;&#32476;&#27450;&#20940;&#25968;&#25454;&#38598;&#65292;&#24182;&#31616;&#35201;&#27010;&#36848;&#20102;&#21019;&#24314;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#21478;&#22806;&#36824;&#25552;&#20379;&#20102;&#21487;&#20844;&#24320;&#35775;&#38382;&#30340;&#25968;&#25454;&#38598;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10231v1 Announce Type: cross  Abstract: In recent years, the rising use of social media has propelled automated cyberbullying detection into a prominent research domain. However, challenges persist due to the absence of a standardized definition and universally accepted datasets. Many researchers now view cyberbullying as a facet of cyberaggression, encompassing factors like repetition, peer relationships, and harmful intent in addition to online aggression. Acquiring comprehensive data reflective of all cyberbullying components from social media networks proves to be a complex task. This paper provides a description of an extensive semi-synthetic cyberbullying dataset that incorporates all of the essential aspects of cyberbullying, including aggression, repetition, peer relationships, and intent to harm. The method of creating the dataset is succinctly outlined, and a detailed overview of the publicly accessible dataset is additionally presented. This accompanying data arti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22270;&#20998;&#26512;&#21644;&#25991;&#26412;&#25968;&#25454;&#27969;&#30340;&#27010;&#24565;&#65292;&#22312;&#24180;&#24230;&#24555;&#29031;&#20013;&#20998;&#26512;&#20102;&#38543;&#26102;&#38388;&#28418;&#31227;&#30340;&#26631;&#31614;&#65292;&#25581;&#31034;&#20102;&#26631;&#31614;&#31038;&#21306;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#65283;mybodymychoice&#26631;&#31614;&#30340;&#20998;&#26512;&#65292;&#24182;&#20026;&#30417;&#27979;&#31038;&#20132;&#23186;&#20307;&#20013;&#20851;&#20110;&#23454;&#20307;&#30340;&#24847;&#35265;&#21644;&#24773;&#24863;&#27169;&#24335;&#38543;&#26102;&#38388;&#21464;&#21270;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10230</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#25968;&#25454;&#27969;&#20013;&#28418;&#31227;&#26631;&#31614;&#30340;&#26102;&#38388;&#20998;&#26512;&#65306;&#22522;&#20110;&#22270;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Temporal Analysis of Drifting Hashtags in Textual Data Streams: A Graph-Based Application
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22270;&#20998;&#26512;&#21644;&#25991;&#26412;&#25968;&#25454;&#27969;&#30340;&#27010;&#24565;&#65292;&#22312;&#24180;&#24230;&#24555;&#29031;&#20013;&#20998;&#26512;&#20102;&#38543;&#26102;&#38388;&#28418;&#31227;&#30340;&#26631;&#31614;&#65292;&#25581;&#31034;&#20102;&#26631;&#31614;&#31038;&#21306;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#65283;mybodymychoice&#26631;&#31614;&#30340;&#20998;&#26512;&#65292;&#24182;&#20026;&#30417;&#27979;&#31038;&#20132;&#23186;&#20307;&#20013;&#20851;&#20110;&#23454;&#20307;&#30340;&#24847;&#35265;&#21644;&#24773;&#24863;&#27169;&#24335;&#38543;&#26102;&#38388;&#21464;&#21270;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#33258;&#20986;&#29616;&#20197;&#26469;&#23601;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20154;&#20204;&#21033;&#29992;&#20114;&#32852;&#32593;&#34920;&#36798;&#23545;&#20219;&#20309;&#20107;&#29289;&#30340;&#30475;&#27861;&#65292;&#20351;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#25104;&#20026;&#31038;&#20250;&#20256;&#24863;&#22120;&#12290;&#26368;&#21021;&#30001;Twitter&#25903;&#25345;&#65292;&#29616;&#22312;&#24050;&#22312;&#22810;&#20010;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#20351;&#29992;&#26631;&#31614;&#12290;&#26631;&#31614;&#26377;&#21161;&#20110;&#26631;&#35760;&#12289;&#36319;&#36394;&#21644;&#23545;&#31867;&#20284;&#20027;&#39064;&#30340;&#24086;&#23376;&#36827;&#34892;&#20998;&#32452;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22270;&#20998;&#26512;&#21644;&#25991;&#26412;&#25968;&#25454;&#27969;&#30340;&#27010;&#24565;&#65292;&#36816;&#29992;Girvan-Newman&#26041;&#27861;&#26469;&#20998;&#26512;&#38543;&#26102;&#38388;&#28418;&#31227;&#30340;&#26631;&#31614;&#65292;&#24182;&#22312;&#24180;&#24230;&#24555;&#29031;&#20013;&#25581;&#31034;&#26631;&#31614;&#31038;&#21306;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;2018&#24180;&#33267;2022&#24180;&#38388;&#30340;&#65283;mybodymychoice&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#30740;&#31350;&#20013;&#21457;&#29616;&#30340;&#19968;&#20123;&#26631;&#31614;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#29992;&#20110;&#30417;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#20851;&#20110;&#26576;&#20010;&#23454;&#20307;&#30340;&#24847;&#35265;&#21644;&#24773;&#24863;&#27169;&#24335;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#12290;&#23613;&#31649;&#65283;mybodymychoice&#26631;&#31614;&#26368;&#21021;&#19982;&#22919;&#22899;&#26435;&#21033;&#12289;&#22549;&#32974;&#21644;&#36523;&#20307;&#33258;&#20027;&#26377;&#20851;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10230v1 Announce Type: cross  Abstract: Social media has played an important role since its emergence. People use the internet to express opinions about anything, making social media platforms a social sensor. Initially supported by Twitter, the hashtags are now in use on several social media platforms. Hashtags are helpful to tag, track, and group posts on similar topics. In this paper, we analyze hashtag drifts over time using concepts from graph analysis and textual data streams using the Girvan-Newman method to uncover hashtag communities in annual snapshots. More specifically, we analyzed the #mybodymychoice hashtag between 2018 and 2022. In addition, we offer insights about some hashtags found in the study. Furthermore, our approach can be useful for monitoring changes over time in opinions and sentiment patterns about an entity on social media. Even though the hashtag #mybodymychoice was initially coupled with women's rights, abortion, and bodily autonomy, we observe 
&lt;/p&gt;</description></item><item><title>\texttt{Mixture-Models}&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#21508;&#31181;&#28151;&#21512;&#27169;&#22411;&#30340;Python&#24211;&#65292;&#36890;&#36807;&#33258;&#21160;&#24494;&#20998;&#24037;&#20855;&#31616;&#21270;&#27169;&#22411;&#30340;&#23454;&#29616;&#21644;&#20998;&#26512;&#65292;&#24182;&#25903;&#25345;&#39640;&#32500;&#25968;&#25454;&#65292;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2402.10229</link><description>&lt;p&gt;
Mixture-Models: &#19968;&#31181;&#38598;&#25104;&#20102;&#21508;&#31181;&#28151;&#21512;&#27169;&#22411;&#30340;Python&#24211;
&lt;/p&gt;
&lt;p&gt;
Mixture-Models: a one-stop Python Library for Model-based Clustering using various Mixture Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10229
&lt;/p&gt;
&lt;p&gt;
\texttt{Mixture-Models}&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#21508;&#31181;&#28151;&#21512;&#27169;&#22411;&#30340;Python&#24211;&#65292;&#36890;&#36807;&#33258;&#21160;&#24494;&#20998;&#24037;&#20855;&#31616;&#21270;&#27169;&#22411;&#30340;&#23454;&#29616;&#21644;&#20998;&#26512;&#65292;&#24182;&#25903;&#25345;&#39640;&#32500;&#25968;&#25454;&#65292;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
\texttt{Mixture-Models}&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24211;&#65292;&#29992;&#20110;&#25311;&#21512;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#21450;&#20854;&#21464;&#31181;&#65292;&#22914;&#31616;&#32422;GMM&#12289;&#22240;&#23376;&#20998;&#26512;&#28151;&#21512;&#27169;&#22411;&#12289;MClust&#27169;&#22411;&#12289;&#23398;&#29983;t&#20998;&#24067;&#28151;&#21512;&#27169;&#22411;&#31561;&#12290;&#23427;&#36890;&#36807;&#33258;&#21160;&#24494;&#20998;&#24037;&#20855;&#31616;&#21270;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#29616;&#21644;&#20998;&#26512;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#19968;&#38454;/&#20108;&#38454;&#20248;&#21270;&#20363;&#31243;&#65292;&#22914;&#26799;&#24230;&#19979;&#38477;&#21644;&#29275;&#39039;-CG&#12290;&#36825;&#26377;&#21161;&#20110;&#23558;&#36825;&#20123;&#27169;&#22411;&#25193;&#23637;&#21040;&#39640;&#32500;&#25968;&#25454;&#65292;&#36825;&#22312;Python&#24211;&#20013;&#36824;&#26159;&#39318;&#27425;&#12290;&#35813;&#24211;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#27169;&#22411;&#35780;&#20272;&#24037;&#20855;&#65292;&#22914;BIC&#12289;AIC&#21644;&#23545;&#25968;&#20284;&#28982;&#20272;&#35745;&#12290;&#28304;&#20195;&#30721;&#26681;&#25454;MIT&#35768;&#21487;&#35777;&#36827;&#34892;&#35768;&#21487;&#65292;&#21487;&#20197;&#22312;\url{https://github.com/kasakh/Mixture-Models}&#20013;&#35775;&#38382;&#12290;&#35813;&#36719;&#20214;&#21253;&#39640;&#24230;&#21487;&#25299;&#23637;&#65292;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#22320;&#21152;&#20837;&#26032;&#30340;&#20998;&#24067;&#21644;&#20248;&#21270;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10229v1 Announce Type: cross  Abstract: \texttt{Mixture-Models} is an open-source Python library for fitting Gaussian Mixture Models (GMM) and their variants, such as Parsimonious GMMs, Mixture of Factor Analyzers, MClust models, Mixture of Student's t distributions, etc. It streamlines the implementation and analysis of these models using various first/second order optimization routines such as Gradient Descent and Newton-CG through automatic differentiation (AD) tools. This helps in extending these models to high-dimensional data, which is first of its kind among Python libraries. The library provides user-friendly model evaluation tools, such as BIC, AIC, and log-likelihood estimation. The source-code is licensed under MIT license and can be accessed at \url{https://github.com/kasakh/Mixture-Models}. The package is highly extensible, allowing users to incorporate new distributions and optimization techniques with ease. We conduct a large scale simulation to compare the pe
&lt;/p&gt;</description></item><item><title>HyperAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#26159;&#39318;&#20010;&#36798;&#21040;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10228</link><description>&lt;p&gt;
HyperAgent&#65306;&#19968;&#31181;&#31616;&#21333;&#12289;&#21487;&#25193;&#23637;&#12289;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#29992;&#20110;&#22797;&#26434;&#29615;&#22659;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10228
&lt;/p&gt;
&lt;p&gt;
HyperAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#26159;&#39318;&#20010;&#36798;&#21040;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#38656;&#35201;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#19981;&#26029;&#31215;&#32047;&#30340;&#20132;&#20114;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HyperAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#36229;&#27169;&#22411;&#12289;&#32034;&#24341;&#25277;&#26679;&#26041;&#26696;&#21644;&#22686;&#37327;&#26356;&#26032;&#26426;&#21046;&#30340;RL&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#20215;&#20540;&#20989;&#25968;&#36924;&#36817;&#20013;&#36827;&#34892;&#35745;&#31639;&#39640;&#25928;&#30340;&#39034;&#24207;&#21518;&#39564;&#36924;&#36817;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#21160;&#20316;&#36873;&#25321;&#65292;&#36229;&#36234;&#20102;&#20849;&#36717;&#24615;&#12290;HyperAgent&#30340;&#23454;&#29616;&#31616;&#21333;&#65292;&#21482;&#38656;&#35201;&#22312;DDQN&#20013;&#28155;&#21152;&#19968;&#20010;&#27169;&#22359;&#21644;&#19968;&#34892;&#39069;&#22806;&#20195;&#30721;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;HyperAgent&#22312;&#22823;&#35268;&#27169;&#28145;&#24230;RL&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#22312;&#25968;&#25454;&#36824;&#26159;&#35745;&#31639;&#26041;&#38754;&#37117;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#22312;&#23454;&#38469;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#20013;&#65292;HyperAgent&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10228v1 Announce Type: cross  Abstract: To solve complex tasks under resource constraints, reinforcement learning (RL) agents need to be simple, efficient, and scalable with (1) large state space and (2) increasingly accumulated data of interactions. We propose the HyperAgent, a RL framework with hypermodel, index sampling schemes and incremental update mechanism, enabling computation-efficient sequential posterior approximation and data-efficient action selection under general value function approximation beyond conjugacy. The implementation of \HyperAgent is simple as it only adds one module and one line of code additional to DDQN. Practically, HyperAgent demonstrates its robust performance in large-scale deep RL benchmarks with significant efficiency gain in terms of both data and computation. Theoretically, among the practically scalable algorithms, HyperAgent is the first method to achieve provably scalable per-step computational complexity as well as sublinear regret u
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#30456;&#20851;&#25289;&#26684;&#26391;&#26085;&#34203;&#23450;&#35860;&#26725;&#65288;CLSB&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20154;&#21475;&#32423;&#27491;&#21017;&#21270;&#26469;&#23398;&#20064;&#36328;&#25130;&#38754;&#26679;&#26412;&#20013;&#30340;&#31995;&#32479;&#21160;&#24577;&#65292;&#36866;&#24212;&#20010;&#20307;&#31890;&#23376;&#34892;&#20026;&#30340;&#24322;&#36136;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10227</link><description>&lt;p&gt;
&#30456;&#20851;&#25289;&#26684;&#26391;&#26085;&#34203;&#23450;&#35860;&#26725;&#65306;&#36890;&#36807;&#20154;&#21475;&#32423;&#27491;&#21017;&#21270;&#23398;&#20064;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Correlational Lagrangian Schr\"odinger Bridge: Learning Dynamics with Population-Level Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10227
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#30456;&#20851;&#25289;&#26684;&#26391;&#26085;&#34203;&#23450;&#35860;&#26725;&#65288;CLSB&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20154;&#21475;&#32423;&#27491;&#21017;&#21270;&#26469;&#23398;&#20064;&#36328;&#25130;&#38754;&#26679;&#26412;&#20013;&#30340;&#31995;&#32479;&#21160;&#24577;&#65292;&#36866;&#24212;&#20010;&#20307;&#31890;&#23376;&#34892;&#20026;&#30340;&#24322;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#21160;&#24577;&#30340;&#20934;&#30830;&#24314;&#27169;&#22312;&#21253;&#25324;&#32454;&#32990;&#21160;&#21147;&#23398;&#21644;&#27969;&#20307;&#21147;&#23398;&#22312;&#20869;&#30340;&#24191;&#27867;&#31185;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#24341;&#20154;&#27880;&#30446;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#65288;i&#65289;&#35266;&#23519;&#20165;&#38480;&#20110;&#27178;&#25130;&#38754;&#26679;&#26412;&#65288;&#20010;&#20307;&#36712;&#36857;&#19981;&#21487;&#23398;&#20064;&#65289;&#26102;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#20010;&#20307;&#31890;&#23376;&#34892;&#20026;&#24322;&#36136;&#26102;&#65288;&#23588;&#20854;&#26159;&#30001;&#20110;&#29983;&#29289;&#22810;&#26679;&#24615;&#32780;&#20026;&#29983;&#29289;&#31995;&#32479;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#30456;&#20851;&#25289;&#26684;&#26391;&#26085;&#34203;&#23450;&#35860;&#26725;&#65288;CLSB&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#23547;&#27714;&#22312;&#27178;&#25130;&#35266;&#23519;&#20043;&#38388;&#30340;&#28436;&#21464;&#8220;&#26725;&#26753;&#8221;&#65292;&#21516;&#26102;&#20197;&#26368;&#23567;&#20154;&#21475;&#8220;&#25104;&#26412;&#8221;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#19982;&#20808;&#21069;&#20381;&#36182;\textit{&#20010;&#20307;}&#32423;&#27491;&#21017;&#21270;&#22120;&#30340;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#65288;&#20363;&#22914;&#65292;&#38480;&#21046;&#20010;&#20307;&#36816;&#21160;&#65289;&#65292;CLSB&#22312;&#20154;&#21475;&#27700;&#24179;&#36816;&#34892;&#65292;&#25509;&#21463;&#24322;&#36136;&#24615;&#26412;&#36136;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#20855;&#27867;&#21270;&#24615;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10227v1 Announce Type: new  Abstract: Accurate modeling of system dynamics holds intriguing potential in broad scientific fields including cytodynamics and fluid mechanics. This task often presents significant challenges when (i) observations are limited to cross-sectional samples (where individual trajectories are inaccessible for learning), and moreover, (ii) the behaviors of individual particles are heterogeneous (especially in biological systems due to biodiversity). To address them, we introduce a novel framework dubbed correlational Lagrangian Schr\"odinger bridge (CLSB), aiming to seek for the evolution "bridging" among cross-sectional observations, while regularized for the minimal population "cost". In contrast to prior methods relying on \textit{individual}-level regularizers for all particles \textit{homogeneously} (e.g. restraining individual motions), CLSB operates at the population level admitting the heterogeneity nature, resulting in a more generalizable mode
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#65288;WLSC&#65289;&#65292;&#20854;&#28508;&#22312;&#34920;&#31034;&#26159;&#38544;&#21547;&#30340;&#65292;&#29992;&#20110;&#23545;&#32858;&#31867;&#24402;&#32435;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;</title><link>https://arxiv.org/abs/2402.10213</link><description>&lt;p&gt;
&#20351;&#29992;&#23637;&#24320;&#32593;&#32476;&#23545;&#32858;&#31867;&#24402;&#32435;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Clustering Inductive Biases with Unrolled Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10213
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#65288;WLSC&#65289;&#65292;&#20854;&#28508;&#22312;&#34920;&#31034;&#26159;&#38544;&#21547;&#30340;&#65292;&#29992;&#20110;&#23545;&#32858;&#31867;&#24402;&#32435;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#20856;&#30340;&#31232;&#30095;&#32534;&#30721;&#65288;SC&#65289;&#27169;&#22411;&#23558;&#35270;&#35273;&#21050;&#28608;&#34920;&#31034;&#20026;&#23569;&#37327;&#23398;&#20064;&#22522;&#20989;&#25968;&#30340;&#32447;&#24615;&#32452;&#21512;&#65292;&#22312;&#23545;&#33258;&#28982;&#22270;&#20687;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#36825;&#20123;&#22522;&#20989;&#25968;&#31867;&#20284;&#20110;Gabor&#12290;&#28982;&#32780;&#65292;&#32463;&#20856;&#31232;&#30095;&#32534;&#30721;&#23398;&#20064;&#30340;&#31867;Gabor&#28388;&#27874;&#22120;&#36828;&#36828;&#36229;&#36807;&#20102;&#23454;&#38469;&#35266;&#23519;&#21040;&#30340;&#31616;&#21333;&#32454;&#32990;&#24863;&#21463;&#37326;&#36718;&#24275;&#30340;&#33391;&#22909;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#65288;WLSC&#65289;&#65292;&#20854;&#28508;&#22312;&#34920;&#31034;&#26159;&#38544;&#21547;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10213v1 Announce Type: cross  Abstract: The classical sparse coding (SC) model represents visual stimuli as a linear combination of a handful of learned basis functions that are Gabor-like when trained on natural image data. However, the Gabor-like filters learned by classical sparse coding far overpredict well-tuned simple cell receptive field profiles observed empirically. While neurons fire sparsely, neuronal populations are also organized in physical space by their sensitivity to certain features. In V1, this organization is a smooth progression of orientations along the cortical sheet. A number of subsequent models have either discarded the sparse dictionary learning framework entirely or whose updates have yet to take advantage of the surge in unrolled, neural dictionary learning architectures. A key missing theme of these updates is a stronger notion of \emph{structured sparsity}. We propose an autoencoder architecture (WLSC) whose latent representations are implicitl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Transformer&#20013;&#37325;&#29992;Softmax&#30828;&#20214;&#21333;&#20803;&#36827;&#34892;GELU&#35745;&#31639;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#20250;&#38477;&#20302;NLP&#24212;&#29992;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10118</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#37325;&#29992;Softmax&#30828;&#20214;&#21333;&#20803;&#36827;&#34892;GELU&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Reusing Softmax Hardware Unit for GELU Computation in Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Transformer&#20013;&#37325;&#29992;Softmax&#30828;&#20214;&#21333;&#20803;&#36827;&#34892;GELU&#35745;&#31639;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#20250;&#38477;&#20302;NLP&#24212;&#29992;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22823;&#22823;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;Transformer&#30340;&#35745;&#31639;&#28041;&#21450;&#30697;&#38453;&#20056;&#27861;&#21644;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;softmax&#21644;GELU&#65288;&#39640;&#26031;&#35823;&#24046;&#32447;&#24615;&#21333;&#20803;&#65289;&#65292;&#36825;&#20123;&#20989;&#25968;&#21487;&#20197;&#30452;&#25509;&#22312;&#30828;&#20214;&#20013;&#21152;&#36895;&#12290;&#30446;&#21069;&#65292;&#27599;&#20010;&#20989;&#25968;&#30340;&#35745;&#31639;&#37117;&#26159;&#20998;&#24320;&#23436;&#25104;&#30340;&#65292;&#24456;&#23569;&#33021;&#22815;&#37325;&#22797;&#20351;&#29992;&#30828;&#20214;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#23558;GELU&#35745;&#31639;&#26144;&#23556;&#21040;softmax&#36816;&#31639;&#31526;&#19978;&#12290;&#36825;&#26679;&#65292;&#24050;&#32463;&#35774;&#35745;&#29992;&#20110;softmax&#30340;&#39640;&#25928;&#30828;&#20214;&#21333;&#20803;&#20063;&#21487;&#20197;&#29992;&#20110;&#35745;&#31639;GELU&#12290;GELU&#30340;&#35745;&#31639;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;softmax&#30340;&#21521;&#37327;&#21270;&#29305;&#24615;&#65292;&#21516;&#26102;&#24182;&#34892;&#20135;&#29983;&#22810;&#20010;GELU&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#39044;&#20808;&#23384;&#22312;&#24182;&#36880;&#27493;&#20462;&#25913;&#30340;softmax&#30828;&#20214;&#21333;&#20803;&#35745;&#31639;GELU&#65288;a&#65289;&#19981;&#20250;&#38477;&#20302;&#20195;&#34920;&#24615;NLP&#24212;&#29992;&#30340;&#20934;&#30830;&#24615;&#65292;&#65288;b&#65289;&#20840;&#37096;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10118v1 Announce Type: cross  Abstract: Transformers have improved drastically the performance of natural language processing (NLP) and computer vision applications. The computation of transformers involves matrix multiplications and non-linear activation functions such as softmax and GELU (Gaussion Error Linear Unit) that are accelerated directly in hardware. Currently, function evaluation is done separately for each function and rarely allows for hardware reuse. To mitigate this problem, in this work, we map the computation of GELU to a softmax operator. In this way, the efficient hardware units designed already for softmax can be reused for computing GELU as well. Computation of GELU can enjoy the inherent vectorized nature of softmax and produce in parallel multiple GELU outcomes. Experimental results show that computing GELU via a pre-existing and incrementally modified softmax hardware unit (a) does not reduce the accuracy of representative NLP applications and (b) all
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;DDPM&#21453;&#36716;&#36827;&#34892;&#38899;&#39057;&#20449;&#21495;&#30340;&#38646;&#26679;&#26412;&#32534;&#36753;&#25216;&#26415;&#65292;&#21253;&#25324;&#22522;&#20110;&#25991;&#26412;&#30340;&#32534;&#36753;&#21644;&#26080;&#30417;&#30563;&#21457;&#29616;&#32534;&#36753;&#26041;&#21521;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#38899;&#20048;&#20449;&#21495;&#20013;&#23637;&#29616;&#20102;&#22810;&#26679;&#30340;&#38899;&#20048;&#20852;&#36259;&#20462;&#25913;&#12290;</title><link>https://arxiv.org/abs/2402.10009</link><description>&lt;p&gt;
&#20351;&#29992;DDPM&#21453;&#36716;&#36827;&#34892;&#38646;&#26679;&#26412;&#26080;&#30417;&#30563;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#38899;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;DDPM&#21453;&#36716;&#36827;&#34892;&#38899;&#39057;&#20449;&#21495;&#30340;&#38646;&#26679;&#26412;&#32534;&#36753;&#25216;&#26415;&#65292;&#21253;&#25324;&#22522;&#20110;&#25991;&#26412;&#30340;&#32534;&#36753;&#21644;&#26080;&#30417;&#30563;&#21457;&#29616;&#32534;&#36753;&#26041;&#21521;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#38899;&#20048;&#20449;&#21495;&#20013;&#23637;&#29616;&#20102;&#22810;&#26679;&#30340;&#38899;&#20048;&#20852;&#36259;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#32534;&#36753;&#24050;&#32463;&#22312;&#22270;&#20687;&#39046;&#22495;&#21462;&#24471;&#20102;&#36805;&#29467;&#30340;&#21457;&#23637;&#65292;&#20294;&#22312;&#38899;&#39057;&#39046;&#22495;&#23578;&#26410;&#20986;&#29616;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#22522;&#20110;DDPM&#21453;&#36716;&#30340;&#38899;&#39057;&#20449;&#21495;&#38646;&#26679;&#26412;&#32534;&#36753;&#25216;&#26415;&#12290;&#31532;&#19968;&#31181;&#26159;&#20174;&#22270;&#20687;&#39046;&#22495;&#37319;&#29992;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#22522;&#20110;&#25991;&#26412;&#36827;&#34892;&#32534;&#36753;&#12290;&#31532;&#20108;&#31181;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#21457;&#29616;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#32534;&#36753;&#26041;&#21521;&#12290;&#24403;&#24212;&#29992;&#20110;&#38899;&#20048;&#20449;&#21495;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23637;&#29616;&#20986;&#19968;&#31995;&#21015;&#20855;&#26377;&#38899;&#20048;&#20852;&#36259;&#30340;&#20462;&#25913;&#65292;&#20174;&#25511;&#21046;&#29305;&#23450;&#20048;&#22120;&#30340;&#21442;&#19982;&#21040;&#23545;&#26059;&#24459;&#36827;&#34892;&#21363;&#20852;&#28436;&#22863;&#12290;&#31034;&#20363;&#21487;&#20197;&#22312;&#25105;&#20204;&#30340;&#20363;&#23376;&#39029;&#38754;&#20013;&#25214;&#21040;&#65306;https://hilamanor.github.io/AudioEditing/ &#65292;&#20195;&#30721;&#21487;&#20197;&#22312; https://github.com/hilamanor/AudioEditing/ &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10009v1 Announce Type: cross  Abstract: Editing signals using large pre-trained models, in a zero-shot manner, has recently seen rapid advancements in the image domain. However, this wave has yet to reach the audio domain. In this paper, we explore two zero-shot editing techniques for audio signals, which use DDPM inversion on pre-trained diffusion models. The first, adopted from the image domain, allows text-based editing. The second, is a novel approach for discovering semantically meaningful editing directions without supervision. When applied to music signals, this method exposes a range of musically interesting modifications, from controlling the participation of specific instruments to improvisations on the melody. Samples can be found on our examples page in https://hilamanor.github.io/AudioEditing/ and code can be found in https://github.com/hilamanor/AudioEditing/ .
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;StyleGAN3&#27169;&#22411;&#20013;&#21028;&#21035;&#22120;&#30340;&#30149;&#24577;&#20559;&#35265;&#65292;&#23427;&#22312;&#22270;&#20687;&#21644;&#38754;&#37096;&#36136;&#37327;&#19978;&#30340;&#24471;&#20998;&#20998;&#23618;&#24433;&#21709;&#20102;&#19981;&#21516;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2402.09786</link><description>&lt;p&gt;
&#26816;&#26597;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21028;&#21035;&#22120;&#20013;&#30340;&#30149;&#24577;&#20559;&#35265;&#65306;&#20197;StyleGAN3&#27169;&#22411;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Examining Pathological Bias in a Generative Adversarial Network Discriminator: A Case Study on a StyleGAN3 Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09786
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#20102;StyleGAN3&#27169;&#22411;&#20013;&#21028;&#21035;&#22120;&#30340;&#30149;&#24577;&#20559;&#35265;&#65292;&#23427;&#22312;&#22270;&#20687;&#21644;&#38754;&#37096;&#36136;&#37327;&#19978;&#30340;&#24471;&#20998;&#20998;&#23618;&#24433;&#21709;&#20102;&#19981;&#21516;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21487;&#20197;&#29983;&#25104;&#36924;&#30495;&#30340;&#20154;&#33080;&#65292;&#24448;&#24448;&#38590;&#20197;&#34987;&#20154;&#31867;&#21306;&#20998;&#20986;&#26469;&#12290;&#25105;&#20204;&#21457;&#29616;&#39044;&#35757;&#32451;&#30340;StyleGAN3&#27169;&#22411;&#20013;&#30340;&#21028;&#21035;&#22120;&#22312;&#22270;&#20687;&#21644;&#38754;&#37096;&#36136;&#37327;&#19978;&#31995;&#32479;&#22320;&#23545;&#24471;&#20998;&#36827;&#34892;&#20998;&#23618;&#65292;&#24182;&#19988;&#36825;&#19981;&#25104;&#27604;&#20363;&#22320;&#24433;&#21709;&#20102;&#19981;&#21516;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#20854;&#20182;&#31867;&#21035;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#21028;&#21035;&#22120;&#22312;&#33394;&#24425;&#21644;&#20142;&#24230;&#26041;&#38754;&#23545;&#24863;&#30693;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#30340;&#20559;&#35265;&#65292;&#28982;&#21518;&#26816;&#26597;&#20102;&#31038;&#20250;&#24515;&#29702;&#23398;&#20013;&#20851;&#20110;&#21051;&#26495;&#21360;&#35937;&#30740;&#31350;&#20013;&#24120;&#35265;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09786v1 Announce Type: cross  Abstract: Generative adversarial networks generate photorealistic faces that are often indistinguishable by humans from real faces. We find that the discriminator in the pre-trained StyleGAN3 model, a popular GAN network, systematically stratifies scores by both image- and face-level qualities and that this disproportionately affects images across gender, race, and other categories. We examine the discriminator's bias for color and luminance across axes perceived race and gender; we then examine axes common in research on stereotyping in social psychology.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#35813;&#26041;&#27861;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#36827;&#34892;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.09631</link><description>&lt;p&gt;
MiMiC&#65306;&#34920;&#31034;&#31354;&#38388;&#20013;&#26368;&#23567;&#20462;&#25913;&#30340;&#23545;&#25239;&#20107;&#23454;
&lt;/p&gt;
&lt;p&gt;
MiMiC: Minimally Modified Counterfactuals in the Representation Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20197;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#34892;&#20026;&#65292;&#35813;&#26041;&#27861;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#36827;&#34892;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09631v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#23398;&#31185; &#31616;&#20171;&#65306;&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#34920;&#29616;&#20986;&#19981;&#33391;&#34892;&#20026;&#65292;&#22914;&#24615;&#21035;&#20559;&#35265;&#25110;&#26377;&#27602;&#35821;&#35328;&#12290;&#36890;&#36807;&#23545;&#34920;&#31034;&#31354;&#38388;&#36827;&#34892;&#24178;&#39044;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#20004;&#31181;&#24120;&#35265;&#30340;&#24178;&#39044;&#25216;&#26415;&#65292;&#21363;&#32447;&#24615;&#25830;&#38500;&#21644;&#23450;&#21521;&#21521;&#37327;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#39640;&#24230;&#21487;&#25511;&#21644;&#34920;&#36798;&#20016;&#23500;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24178;&#39044;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#29983;&#25104;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;&#23545;&#25239;&#20107;&#23454;&#65292;&#20351;&#28304;&#31867;&#21035;&#65288;&#20363;&#22914;&#8220;&#26377;&#27602;&#8221;&#65289;&#30340;&#34920;&#31034;&#19982;&#30446;&#26631;&#31867;&#21035;&#65288;&#20363;&#22914;&#8220;&#38750;&#26377;&#27602;&#8221;&#65289;&#30340;&#34920;&#31034;&#30456;&#20284;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#39640;&#26031;&#20551;&#35774;&#19979;&#30340;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#22320;&#29699;&#31227;&#21160;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#24182;&#23545;&#34920;&#31034;&#31354;&#38388;&#30340;&#20960;&#20309;&#32452;&#32455;&#25552;&#20379;&#20102;&#36827;&#19968;&#27493;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09631v1 Announce Type: cross  Abstract: Language models often exhibit undesirable behaviors, such as gender bias or toxic language. Interventions in the representation space were shown effective in mitigating such issues by altering the LM behavior. We first show that two prominent intervention techniques, Linear Erasure and Steering Vectors, do not enable a high degree of control and are limited in expressivity.   We then propose a novel intervention methodology for generating expressive counterfactuals in the representation space, aiming to make representations of a source class (e.g., ``toxic'') resemble those of a target class (e.g., ``non-toxic''). This approach, generalizing previous linear intervention techniques, utilizes a closed-form solution for the Earth Mover's problem under Gaussian assumptions and provides theoretical guarantees on the representation space's geometric organization. We further build on this technique and derive a nonlinear intervention that ena
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;</title><link>https://arxiv.org/abs/2402.09615</link><description>&lt;p&gt;
API Pack&#65306;&#19968;&#20010;&#29992;&#20110;API&#35843;&#29992;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
API Pack: A Massive Multilingual Dataset for API Call Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09615
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;API Pack&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#25351;&#20196;-API&#35843;&#29992;&#23545;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;API Pack&#22312;&#25552;&#21319;&#27169;&#22411;&#22312;&#36825;&#19968;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20854;&#22312;&#19968;&#33324;&#32534;&#30721;&#26041;&#38754;&#30340;&#25972;&#20307;&#29087;&#32451;&#31243;&#24230;&#12290;&#20165;&#22312;20,000&#20010;Python&#23454;&#20363;&#19978;&#23545;CodeLlama-13B&#36827;&#34892;&#24494;&#35843;&#65292;&#20854;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#30340;&#20934;&#30830;&#29575;&#27604;GPT-3.5&#21644;GPT-4&#20998;&#21035;&#39640;&#20986;10%&#21644;5%&#12290;&#25193;&#23637;&#21040;100k&#20010;&#20363;&#23376;&#21487;&#20197;&#25552;&#39640;&#23545;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;API&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;&#65292;&#32780;&#26080;&#38656;&#22823;&#37327;&#35821;&#35328;&#29305;&#23450;&#30340;&#25968;&#25454;&#12290;&#25968;&#25454;&#38598;&#12289;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#21644;&#25972;&#20307;&#20195;&#30721;&#24211;&#21487;&#22312;https://github.com/anonymous_url&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09615v1 Announce Type: cross  Abstract: We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.
&lt;/p&gt;</description></item><item><title>PMGDA&#26159;&#19968;&#20010;&#22522;&#20110;&#20559;&#22909;&#30340;&#22810;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#21487;&#20197; efficiently &#22312;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#25214;&#21040;&#19982;&#20915;&#31574;&#32773;&#20559;&#22909;&#23436;&#20840;&#21305;&#37197;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.09492</link><description>&lt;p&gt;
PMGDA: &#22522;&#20110;&#20559;&#22909;&#30340;&#22810;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
PMGDA: A Preference-based Multiple Gradient Descent Algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09492
&lt;/p&gt;
&lt;p&gt;
PMGDA&#26159;&#19968;&#20010;&#22522;&#20110;&#20559;&#22909;&#30340;&#22810;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#21487;&#20197; efficiently &#22312;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#25214;&#21040;&#19982;&#20915;&#31574;&#32773;&#20559;&#22909;&#23436;&#20840;&#21305;&#37197;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#38382;&#39064;&#65292;&#22914;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65292;&#23547;&#25214;&#19982;&#20915;&#31574;&#32773;&#32473;&#23450;&#20559;&#22909;&#23436;&#20840;&#21305;&#37197;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#35268;&#27169;&#36739;&#22823;&#65292;&#34429;&#28982;&#26377;&#21487;&#29992;&#30340;&#26799;&#24230;&#20449;&#24687;&#65292;&#20294;&#29616;&#26377;&#30340;&#31639;&#27861;&#26080;&#27861;&#24456;&#22909;&#22320;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#39044;&#27979;-&#26657;&#27491;&#8221;&#26694;&#26550;&#65292;&#29992;&#20110;&#25214;&#21040;&#20915;&#31574;&#32773;&#25152;&#38656;&#30340;&#31934;&#30830;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#32422;&#26463;&#20989;&#25968;&#26469;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#20351;&#35299;&#19982;&#29992;&#25143;&#29305;&#23450;&#20559;&#22909;&#23545;&#40784;&#65292;&#36825;&#20010;&#32422;&#26463;&#20989;&#25968;&#21487;&#20197;&#19982;&#22810;&#20010;&#30446;&#26631;&#20989;&#25968;&#21516;&#26102;&#20248;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#12289;&#22810;&#20219;&#21153;&#21644;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#31934;&#30830;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09492v1 Announce Type: new  Abstract: It is desirable in many multi-objective machine learning applications, such as multi-task learning and multi-objective reinforcement learning, to find a Pareto optimal solution that can exactly match a given preference of decision-makers. These problems are often large-scale with available gradient information but cannot be handled very well by the existing algorithms. To tackle this critical issue, this paper proposes a novel predict-and-correct framework for locating the exact Pareto optimal solutions required by a decision maker. In the proposed framework, a constraint function is introduced in the search progress to align the solution with a user-specific preference, which can be optimized simultaneously with multiple objective functions. Experimental results show that our proposed method can efficiently find exact Pareto optimal solutions for standard benchmarks, multi-task, and multi-objective reinforcement learning problems with m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoRM&#30340;&#22870;&#21169;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#33410;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#38598;&#25104;&#32858;&#31867;&#20559;&#24046;&#24471;&#20998;&#65288;ICDS&#65289;&#26469;&#26816;&#27979;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.09345</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#35770;&#22870;&#21169;&#24314;&#27169;&#26469;&#20943;&#36731;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Reward Hacking via Information-Theoretic Reward Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoRM&#30340;&#22870;&#21169;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#33410;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#38598;&#25104;&#32858;&#31867;&#20559;&#24046;&#24471;&#20998;&#65288;ICDS&#65289;&#26469;&#26816;&#27979;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#20013;&#30340;&#25104;&#21151;&#22312;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#26041;&#38754;&#65292;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;&#65292;&#20063;&#34987;&#31216;&#20026;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#20110;&#22870;&#21169;&#24314;&#27169;&#30340;&#23616;&#38480;&#24615;&#65292;&#21363;&#22870;&#21169;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#35770;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25512;&#24191;&#21644;&#40065;&#26834;&#30340;&#22870;&#21169;&#24314;&#27169;&#26694;&#26550;&#65292;&#31216;&#20026;InfoRM&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#26469;&#36807;&#28388;&#20986;&#19981;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#24320;&#21457;&#19968;&#31181;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#33410;&#26426;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#20102;&#36807;&#24230;&#20248;&#21270;&#19982;&#28508;&#21464;&#37327;&#31354;&#38388;&#30340;&#24322;&#24120;&#20540;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23558;InfoRM&#20316;&#20026;&#26816;&#27979;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;&#21463;&#21040;&#36825;&#19968;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38598;&#25104;&#32858;&#31867;&#20559;&#24046;&#24471;&#20998;&#65288;ICDS&#65289;&#65292;&#29992;&#20110;&#37327;&#21270;&#36807;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09345v1 Announce Type: cross Abstract: Despite the success of reinforcement learning from human feedback (RLHF) in aligning language models with human values, reward hacking, also termed reward overoptimization, remains a critical challenge, which primarily stems from limitations in reward modeling, i.e., generalizability of the reward model and inconsistency in the preference dataset. In this work, we tackle this problem from an information theoretic-perspective, and propose a generalizable and robust framework for reward modeling, namely InfoRM, by introducing a variational information bottleneck objective to filter out irrelevant information and developing a mechanism for model complexity modulation. Notably, we further identify a correlation between overoptimization and outliers in the latent space, establishing InfoRM as a promising tool for detecting reward overoptimization. Inspired by this finding, we propose the Integrated Cluster Deviation Score (ICDS), which quant
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65292;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#24182;&#20445;&#25345;&#22522;&#26412;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#20026;&#24320;&#21457;&#23433;&#20840;&#12289;&#21487;&#38752;&#21644;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2402.08787</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Rethinking Machine Unlearning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65292;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#24182;&#20445;&#25345;&#22522;&#26412;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#20026;&#24320;&#21457;&#23433;&#20840;&#12289;&#21487;&#38752;&#21644;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#30340;&#26426;&#22120;&#28040;&#38500;&#25216;&#26415;&#65288;MU&#65289;&#65292;&#31216;&#20026;LLM&#28040;&#38500;&#25216;&#26415;&#12290;&#36825;&#20010;&#30740;&#31350;&#26088;&#22312;&#28040;&#38500;&#19981;&#33391;&#25968;&#25454;&#30340;&#24433;&#21709;&#65288;&#20363;&#22914;&#25935;&#24863;&#25110;&#38750;&#27861;&#20449;&#24687;&#65289;&#20197;&#21450;&#30456;&#20851;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#22522;&#26412;&#30340;&#30693;&#35782;&#29983;&#25104;&#30340;&#23436;&#25972;&#24615;&#65292;&#24182;&#19981;&#24433;&#21709;&#22240;&#26524;&#26080;&#20851;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#35774;&#24819;LLM&#28040;&#38500;&#25216;&#26415;&#23558;&#25104;&#20026;LLM&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#20013;&#30340;&#20851;&#38190;&#35201;&#32032;&#65292;&#21487;&#33021;&#25104;&#20026;&#24320;&#21457;&#26082;&#23433;&#20840;&#12289;&#21487;&#38752;&#21448;&#36164;&#28304;&#39640;&#25928;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#30784;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#23436;&#20840;&#37325;&#35757;&#32451;&#12290;&#25105;&#20204;&#20174;&#27010;&#24565;&#12289;&#26041;&#27861;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#24212;&#29992;&#31561;&#26041;&#38754;&#25506;&#32034;&#20102;LLM&#28040;&#38500;&#25216;&#26415;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#29616;&#26377;LLM&#28040;&#38500;&#25216;&#26415;&#30740;&#31350;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#26041;&#38754;&#65292;&#20363;&#22914;&#28040;&#38500;&#33539;&#22260;&#12289;&#25968;&#25454;&#27169;&#22411;&#20132;&#20114;&#21644;&#22810;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08787v1 Announce Type: cross Abstract: We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We
&lt;/p&gt;</description></item><item><title>BECoTTA&#26159;&#19968;&#31181;&#22522;&#20110;&#36755;&#20837;&#30340;&#39640;&#25928;CTTA&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;MoDE&#65288;Mixture-of-Domain Low-rank Experts&#65289;&#27169;&#22411;&#65292;&#23427;&#21253;&#21547;&#39046;&#22495;&#33258;&#36866;&#24212;&#36335;&#30001;&#21644;&#39046;&#22495;&#19987;&#23478;&#21327;&#21516;&#25439;&#22833;&#20004;&#20010;&#26680;&#24515;&#32452;&#20214;&#65292;&#33021;&#22815;&#22312;&#25345;&#32493;&#30340;&#27979;&#35797;&#26102;&#38388;&#20013;&#33258;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#39046;&#22495;&#65292;&#21516;&#26102;&#21482;&#38656;&#36739;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.08712</link><description>&lt;p&gt;
BECoTTA: &#22522;&#20110;&#36755;&#20837;&#30340;&#22312;&#32447;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#25345;&#32493;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
BECoTTA: Input-dependent Online Blending of Experts for Continual Test-time Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08712
&lt;/p&gt;
&lt;p&gt;
BECoTTA&#26159;&#19968;&#31181;&#22522;&#20110;&#36755;&#20837;&#30340;&#39640;&#25928;CTTA&#26694;&#26550;&#65292;&#36890;&#36807;&#37319;&#29992;MoDE&#65288;Mixture-of-Domain Low-rank Experts&#65289;&#27169;&#22411;&#65292;&#23427;&#21253;&#21547;&#39046;&#22495;&#33258;&#36866;&#24212;&#36335;&#30001;&#21644;&#39046;&#22495;&#19987;&#23478;&#21327;&#21516;&#25439;&#22833;&#20004;&#20010;&#26680;&#24515;&#32452;&#20214;&#65292;&#33021;&#22815;&#22312;&#25345;&#32493;&#30340;&#27979;&#35797;&#26102;&#38388;&#20013;&#33258;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#39046;&#22495;&#65292;&#21516;&#26102;&#21482;&#38656;&#36739;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;CTTA&#65289;&#35201;&#27714;&#22312;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#26410;&#30693;&#39046;&#22495;&#30340;&#21516;&#26102;&#20445;&#30041;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;CTTA&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#24536;&#35760;&#36866;&#24212;&#26435;&#34913;&#21644;&#25928;&#29575;&#20173;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;CTTA&#22330;&#26223;&#20165;&#20551;&#35774;&#23384;&#22312;&#19981;&#30456;&#20132;&#30340;&#24773;&#20917;&#65292;&#32780;&#23454;&#38469;&#19990;&#30028;&#30340;&#39046;&#22495;&#26159;&#26080;&#32541;&#21464;&#21270;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BECoTTA&#30340;&#22522;&#20110;&#36755;&#20837;&#30340;&#39640;&#25928;CTTA&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MoDE&#65288;Mixture-of-Domain Low-rank Experts&#65289;&#65292;&#23427;&#21253;&#21547;&#20004;&#20010;&#26680;&#24515;&#32452;&#20214;&#65306;i&#65289;&#39046;&#22495;&#33258;&#36866;&#24212;&#36335;&#30001;&#65292;&#36890;&#36807;&#22810;&#20010;&#39046;&#22495;&#36335;&#30001;&#22120;&#26377;&#36873;&#25321;&#22320;&#25429;&#25417;&#39046;&#22495;&#33258;&#36866;&#24212;&#30693;&#35782;&#65292;&#21644;ii&#65289;&#39046;&#22495;&#19987;&#23478;&#21327;&#21516;&#25439;&#22833;&#65292;&#20197;&#22686;&#21152;&#27599;&#20010;&#39046;&#22495;&#21644;&#19987;&#23478;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22810;&#20010;CTTA&#22330;&#26223;&#65292;&#21253;&#25324;&#19981;&#30456;&#20132;&#21644;&#28176;&#21464;&#39046;&#22495;&#20999;&#25442;&#65292;&#21516;&#26102;&#21482;&#38656;&#35201;&#22823;&#32422;98&#65285;&#26356;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08712v1 Announce Type: new Abstract: Continual Test Time Adaptation (CTTA) is required to adapt efficiently to continuous unseen domains while retaining previously learned knowledge. However, despite the progress of CTTA, forgetting-adaptation trade-offs and efficiency are still unexplored. Moreover, current CTTA scenarios assume only the disjoint situation, even though real-world domains are seamlessly changed. To tackle these challenges, this paper proposes BECoTTA, an input-dependent yet efficient framework for CTTA. We propose Mixture-of-Domain Low-rank Experts (MoDE) that contains two core components: i) Domain-Adaptive Routing, which aids in selectively capturing the domain-adaptive knowledge with multiple domain routers, and (ii) Domain-Expert Synergy Loss to maximize the dependency between each domain and expert. We validate our method outperforms multiple CTTA scenarios including disjoint and gradual domain shits, while only requiring ~98% fewer trainable parameters
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#39640;&#31572;&#26696;&#30340;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#22235;&#20010;&#27979;&#35797;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08277</link><description>&lt;p&gt;
&#26397;&#30528;&#24544;&#23454;&#21644;&#24378;&#22823;&#30340;&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#19987;&#23478;&#30340;&#26041;&#21521;&#21069;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08277
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#39640;&#31572;&#26696;&#30340;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#21644;&#22235;&#20010;&#27979;&#35797;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26356;&#24544;&#23454;&#21644;&#21487;&#36861;&#36394;&#30340;&#31572;&#26696;&#30340;&#36827;&#27493;&#23545;&#20110;&#21508;&#31181;&#30740;&#31350;&#21644;&#23454;&#36341;&#27963;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#31181;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#21487;&#38752;&#30340;&#26469;&#28304;&#25552;&#20379;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#22312;&#20351;&#29992;LLM&#26102;&#24050;&#32463;&#35777;&#26126;&#22312;&#24341;&#29992;&#27491;&#30830;&#30340;&#26469;&#28304;&#65288;&#26469;&#28304;&#36136;&#37327;&#65289;&#21644;&#20934;&#30830;&#22320;&#34920;&#31034;&#26469;&#28304;&#20013;&#30340;&#20449;&#24687;&#65288;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#65289;&#26041;&#38754;&#24037;&#20316;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22914;&#20309;&#40065;&#26834;&#22320;&#24494;&#35843;LLM&#65292;&#20197;&#25552;&#39640;&#26469;&#28304;&#36136;&#37327;&#21644;&#31572;&#26696;&#24402;&#22240;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#21160;&#25968;&#25454;&#36136;&#37327;&#36807;&#28388;&#22120;&#65292;&#21487;&#20197;&#22823;&#35268;&#27169;&#21512;&#25104;&#22810;&#26679;&#21270;&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#22235;&#20010;&#27979;&#35797;&#38598;&#65292;&#20197;&#23545;&#24494;&#35843;&#21518;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#22312;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#24615;&#33021;&#12290;%&#22522;&#20110;&#35777;&#25454;&#30340;&#38382;&#31572;&#26696;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#20110;&#35780;&#20272;&#30340;&#22235;&#20010;&#27979;&#35797;&#38598;&#65292;&#20197;&#35780;&#20272;&#24494;&#35843;&#21518;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. %Evidence-Based QA cases. Furthermore, we sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-TTA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#22686;&#24378;&#20316;&#20026;&#27979;&#35797;&#26102;&#38388;&#22686;&#24378;&#65288;TTA&#65289;&#30340;&#22686;&#24378;&#20989;&#25968;&#65292;&#25552;&#39640;&#20102;&#40657;&#30418;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;BERT&#21644;T5&#27169;&#22411;&#30340;&#24773;&#24863;&#12289;&#27602;&#24615;&#21644;&#26032;&#38395;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;LLM-TTA&#20248;&#20110;&#20256;&#32479;&#30340;&#22686;&#24378;&#20989;&#25968;&#65292;&#20351;BERT&#30340;&#20998;&#24067;&#22806;&#40065;&#26834;&#24615;&#24179;&#22343;&#25552;&#39640;&#20102;4.30&#20010;&#30334;&#20998;&#28857;&#65292;&#32780;&#19981;&#38477;&#20302;&#20998;&#24067;&#20869;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08225</link><description>&lt;p&gt;
&#29992;&#19978;&#19979;&#25991;&#37325;&#20889;&#25552;&#39640;&#40657;&#30418;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Black-box Robustness with In-Context Rewriting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LLM-TTA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#22686;&#24378;&#20316;&#20026;&#27979;&#35797;&#26102;&#38388;&#22686;&#24378;&#65288;TTA&#65289;&#30340;&#22686;&#24378;&#20989;&#25968;&#65292;&#25552;&#39640;&#20102;&#40657;&#30418;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;BERT&#21644;T5&#27169;&#22411;&#30340;&#24773;&#24863;&#12289;&#27602;&#24615;&#21644;&#26032;&#38395;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;LLM-TTA&#20248;&#20110;&#20256;&#32479;&#30340;&#22686;&#24378;&#20989;&#25968;&#65292;&#20351;BERT&#30340;&#20998;&#24067;&#22806;&#40065;&#26834;&#24615;&#24179;&#22343;&#25552;&#39640;&#20102;4.30&#20010;&#30334;&#20998;&#28857;&#65292;&#32780;&#19981;&#38477;&#20302;&#20998;&#24067;&#20869;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20998;&#24067;&#20869;&#65288;ID&#65289;&#25968;&#25454;&#19978;&#34920;&#29616;&#20248;&#31168;&#65292;&#20294;&#22312;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#36755;&#20837;&#19978;&#34920;&#29616;&#22256;&#38590;&#12290;&#22823;&#22810;&#25968;&#25552;&#39640;OOD&#40065;&#26834;&#24615;&#30340;&#25216;&#26415;&#22312;&#27169;&#22411;&#26159;&#40657;&#30418;&#30340;&#24773;&#20917;&#19979;&#19981;&#36866;&#29992;&#65292;&#20363;&#22914;&#26435;&#37325;&#34987;&#20923;&#32467;&#65292;&#37325;&#26032;&#35757;&#32451;&#25104;&#26412;&#39640;&#65292;&#25110;&#32773;&#36890;&#36807;API&#20351;&#29992;&#27169;&#22411;&#12290;&#27979;&#35797;&#26102;&#38388;&#22686;&#24378;&#65288;TTA&#65289;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#20107;&#21518;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#27979;&#35797;&#36755;&#20837;&#30340;&#22810;&#20010;&#22686;&#24378;&#36827;&#34892;&#39044;&#27979;&#32858;&#21512;&#26469;&#32469;&#36807;&#40657;&#30418;&#32422;&#26463;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#30001;&#20110;&#29983;&#25104;&#26377;&#25928;&#30340;&#33258;&#28982;&#35821;&#35328;&#22686;&#24378;&#30340;&#25361;&#25112;&#65292;TTA&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM-TTA&#65292;&#23427;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#22686;&#24378;&#20316;&#20026;TTA&#30340;&#22686;&#24378;&#20989;&#25968;&#12290;LLM-TTA&#22312;BERT&#21644;T5&#27169;&#22411;&#30340;&#24773;&#24863;&#12289;&#27602;&#24615;&#21644;&#26032;&#38395;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#20256;&#32479;&#30340;&#22686;&#24378;&#20989;&#25968;&#65292;BERT&#30340;OOD&#40065;&#26834;&#24615;&#25552;&#39640;&#20102;&#24179;&#22343;4.30&#20010;&#30334;&#20998;&#28857;&#32780;&#19981;&#20250;&#20943;&#36864;&#24179;&#22343;ID pe&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models often excel on in-distribution (ID) data but struggle with unseen out-of-distribution (OOD) inputs. Most techniques for improving OOD robustness are not applicable to settings where the model is effectively a black box, such as when the weights are frozen, retraining is costly, or the model is leveraged via an API. Test-time augmentation (TTA) is a simple post-hoc technique for improving robustness that sidesteps black-box constraints by aggregating predictions across multiple augmentations of the test input. TTA has seen limited use in NLP due to the challenge of generating effective natural language augmentations. In this work, we propose LLM-TTA, which uses LLM-generated augmentations as TTA's augmentation function. LLM-TTA outperforms conventional augmentation functions across sentiment, toxicity, and news classification tasks for BERT and T5 models, with BERT's OOD robustness improving by an average of 4.30 percentage points without regressing average ID pe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22312;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#38656;&#35201;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#23545;&#20110;&#25351;&#23548;&#24615;LLMs&#30340;&#20849;&#35782;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20250;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#24230;&#37327;&#26631;&#20934;"&#65292;&#29992;&#20110;&#34913;&#37327;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.06733</link><description>&lt;p&gt;
NICE: &#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#36824;&#26159;&#19981;&#20248;&#21270;&#65311;
&lt;/p&gt;
&lt;p&gt;
NICE: To Optimize In-Context Examples or Not?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06733
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22312;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#38656;&#35201;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#23545;&#20110;&#25351;&#23548;&#24615;LLMs&#30340;&#20849;&#35782;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20250;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#24230;&#37327;&#26631;&#20934;"&#65292;&#29992;&#20110;&#34913;&#37327;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65288;ICE&#65289;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#22312;&#25552;&#31034;&#20449;&#24687;&#20013;&#35201;&#20040;&#26159;&#22266;&#23450;&#30340;&#65292;&#35201;&#20040;&#27809;&#26377;&#25552;&#20379;&#25351;&#20196;&#65292;&#23548;&#33268;&#20102;&#19968;&#20010;&#34920;&#38754;&#19978;&#30340;&#20849;&#35782;&#65306;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#38024;&#23545;&#32463;&#36807;&#25351;&#23548;&#30340;LLMs&#25361;&#25112;&#36825;&#19968;&#20849;&#35782;&#65292;&#30740;&#31350;&#22312;&#25552;&#20379;&#20102;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26159;&#21542;&#24517;&#35201;&#65292;&#24182;&#21457;&#29616;&#26377;&#19968;&#20123;&#20219;&#21153;&#23545;&#20110;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;"&#24230;&#37327;&#26631;&#20934;"&#65288;Metric&#65289;&#65292;&#29992;&#20110;&#37327;&#21270;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#20219;&#21153;&#21644;&#36880;&#27493;&#22686;&#21152;&#30340;&#25351;&#20196;&#38598;&#30340;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that large language models (LLMs) work remarkably well on a wide range of tasks through in-context learning and optimization of in-context examples (ICE). However, most of these studies assume either a fixed or no instruction provided in the prompt, leading to the apparent consensus that the optimization of in-context examples is critical for better performance. We challenge this consensus for instruction-tuned LLMs by investigating the necessity of optimizing in-context examples when task-specific instructions are provided, and find that there are tasks for which various ways of optimizing in-context examples yield diminishing returns. We introduce a task-specific metric called \metriclong{} (\metric) that quantifies the learnability of tasks from a given instruction, and provides a heuristic that helps decide whether to optimize for instructions or ICE for any new task. On a wide range of tasks and a systematically created instruction set with gradually added 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32858;&#31751;&#21487;&#25193;&#23637;&#32852;&#37030;&#23398;&#20064;&#65288;BCS-FL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#26080;&#20154;&#26426;&#32593;&#32476;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#21435;&#20013;&#24515;&#21270;&#12289;&#21327;&#35843;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#12290;&#35813;&#26694;&#26550;&#23558;&#26080;&#20154;&#26426;&#32593;&#32476;&#21010;&#20998;&#20026;&#32858;&#31751;&#65292;&#24182;&#30001;&#32858;&#31751;&#22836;&#26080;&#20154;&#26426;&#36827;&#34892;&#21327;&#35843;&#65292;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#26080;&#20154;&#26426;&#32593;&#32476;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.05973</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#25216;&#26415;&#25903;&#25345;&#30340;&#32858;&#31751;&#21487;&#25193;&#23637;&#32852;&#37030;&#23398;&#20064;&#65288;BCS-FL&#65289;&#26694;&#26550;&#22312;&#26080;&#20154;&#26426;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Blockchain-enabled Clustered and Scalable Federated Learning (BCS-FL) Framework in UAV Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32858;&#31751;&#21487;&#25193;&#23637;&#32852;&#37030;&#23398;&#20064;&#65288;BCS-FL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#21892;&#26080;&#20154;&#26426;&#32593;&#32476;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#21435;&#20013;&#24515;&#21270;&#12289;&#21327;&#35843;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#12290;&#35813;&#26694;&#26550;&#23558;&#26080;&#20154;&#26426;&#32593;&#32476;&#21010;&#20998;&#20026;&#32858;&#31751;&#65292;&#24182;&#30001;&#32858;&#31751;&#22836;&#26080;&#20154;&#26426;&#36827;&#34892;&#21327;&#35843;&#65292;&#20197;&#25552;&#39640;&#22823;&#35268;&#27169;&#26080;&#20154;&#26426;&#32593;&#32476;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#38752;&#24615;&#26159;&#26080;&#20154;&#26426;&#32593;&#32476;&#20316;&#20026;&#20998;&#24067;&#24335;&#31995;&#32479;&#25152;&#38754;&#20020;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#20132;&#25442;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#26102;&#12290;&#26368;&#36817;&#65292;&#22312;&#26080;&#20154;&#26426;&#32593;&#32476;&#20013;&#24212;&#29992;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25913;&#21892;&#20102;&#21327;&#20316;&#12289;&#38544;&#31169;&#12289;&#38887;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#25104;&#20026;&#26080;&#20154;&#26426;&#24212;&#29992;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#20026;&#26080;&#20154;&#26426;&#32593;&#32476;&#23454;&#29616;FL&#24341;&#20837;&#20102;&#36890;&#20449;&#24320;&#38144;&#12289;&#21516;&#27493;&#38382;&#39064;&#12289;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#21644;&#36164;&#28304;&#32422;&#26463;&#31561;&#32570;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32858;&#31751;&#21487;&#25193;&#23637;&#32852;&#37030;&#23398;&#20064;&#65288;BCS-FL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#20154;&#26426;&#32593;&#32476;&#12290;&#35813;&#26694;&#26550;&#23558;&#26080;&#20154;&#26426;&#32593;&#32476;&#21010;&#20998;&#20026;&#20998;&#31163;&#30340;&#32858;&#31751;&#65292;&#24182;&#30001;&#32858;&#31751;&#22836;&#26080;&#20154;&#26426;&#65288;CHs&#65289;&#36827;&#34892;&#21327;&#35843;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#36830;&#36890;&#22270;&#12290;&#32858;&#31751;&#21270;&#20351;&#24471;&#32852;&#37030;&#23398;&#20064;&#22312;&#22823;&#35268;&#27169;&#26080;&#20154;&#26426;&#32593;&#32476;&#20013;&#30340;&#21435;&#20013;&#24515;&#21270;&#12289;&#21327;&#35843;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy, scalability, and reliability are significant challenges in unmanned aerial vehicle (UAV) networks as distributed systems, especially when employing machine learning (ML) technologies with substantial data exchange. Recently, the application of federated learning (FL) to UAV networks has improved collaboration, privacy, resilience, and adaptability, making it a promising framework for UAV applications. However, implementing FL for UAV networks introduces drawbacks such as communication overhead, synchronization issues, scalability limitations, and resource constraints. To address these challenges, this paper presents the Blockchain-enabled Clustered and Scalable Federated Learning (BCS-FL) framework for UAV networks. This improves the decentralization, coordination, scalability, and efficiency of FL in large-scale UAV networks. The framework partitions UAV networks into separate clusters, coordinated by cluster head UAVs (CHs), to establish a connected graph. Clustering enables
&lt;/p&gt;</description></item><item><title>&#22810;&#35270;&#35282;&#31526;&#21495;&#22238;&#24402;(MvSR)&#26159;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#33021;&#22815;&#25214;&#21040;&#19968;&#20010;&#21442;&#25968;&#21270;&#35299;&#26469;&#20934;&#30830;&#25311;&#21512;&#25152;&#26377;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#19981;&#21516;&#23454;&#39564;&#35774;&#32622;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04298</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Multi-View Symbolic Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04298
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#31526;&#21495;&#22238;&#24402;(MvSR)&#26159;&#19968;&#31181;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#33021;&#22815;&#25214;&#21040;&#19968;&#20010;&#21442;&#25968;&#21270;&#35299;&#26469;&#20934;&#30830;&#25311;&#21512;&#25152;&#26377;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#19981;&#21516;&#23454;&#39564;&#35774;&#32622;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;(SR)&#25628;&#32034;&#34920;&#31034;&#35299;&#37322;&#21464;&#37327;&#21644;&#21709;&#24212;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#12290;&#30446;&#21069;&#30340;SR&#26041;&#27861;&#20551;&#35774;&#20174;&#21333;&#20010;&#23454;&#39564;&#20013;&#25552;&#21462;&#30340;&#21333;&#20010;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#38754;&#20020;&#26469;&#33258;&#19981;&#21516;&#35774;&#32622;&#30340;&#22810;&#20010;&#23454;&#39564;&#32467;&#26524;&#38598;&#12290;&#20256;&#32479;&#30340;SR&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#25214;&#21040;&#28508;&#22312;&#30340;&#34920;&#36798;&#24335;&#65292;&#22240;&#20026;&#27599;&#20010;&#23454;&#39564;&#30340;&#21442;&#25968;&#21487;&#33021;&#19981;&#21516;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#35270;&#35282;&#31526;&#21495;&#22238;&#24402;(MvSR)&#65292;&#23427;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#25968;&#25454;&#38598;&#65292;&#27169;&#25311;&#23454;&#39564;&#29615;&#22659;&#65292;&#24182;&#36755;&#20986;&#19968;&#20010;&#36890;&#29992;&#30340;&#21442;&#25968;&#21270;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#35780;&#20272;&#30340;&#34920;&#36798;&#24335;&#36866;&#24212;&#27599;&#20010;&#29420;&#31435;&#25968;&#25454;&#38598;&#65292;&#24182;&#21516;&#26102;&#36820;&#22238;&#33021;&#22815;&#20934;&#30830;&#25311;&#21512;&#25152;&#26377;&#25968;&#25454;&#38598;&#30340;&#21442;&#25968;&#20989;&#25968;&#26063;f(x; \theta)&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#24050;&#30693;&#34920;&#36798;&#24335;&#29983;&#25104;&#30340;&#25968;&#25454;&#20197;&#21450;&#26469;&#33258;&#23454;&#38469;&#19990;&#30028;&#30340;&#25968;&#25454;&#26469;&#23637;&#31034;MvSR&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression (SR) searches for analytical expressions representing the relationship between a set of explanatory and response variables. Current SR methods assume a single dataset extracted from a single experiment. Nevertheless, frequently, the researcher is confronted with multiple sets of results obtained from experiments conducted with different setups. Traditional SR methods may fail to find the underlying expression since the parameters of each experiment can be different. In this work we present Multi-View Symbolic Regression (MvSR), which takes into account multiple datasets simultaneously, mimicking experimental environments, and outputs a general parametric solution. This approach fits the evaluated expression to each independent dataset and returns a parametric family of functions f(x; \theta) simultaneously capable of accurately fitting all datasets. We demonstrate the effectiveness of MvSR using data generated from known expressions, as well as real-world data from 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#30340;&#22810;&#28304;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20010;&#25968;&#25454;&#28304;&#20043;&#38388;&#36136;&#37327;&#21644;&#20840;&#38754;&#24615;&#24046;&#24322;&#32473;&#31995;&#32479;&#20248;&#21270;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04146</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22810;&#28304;&#25968;&#25454;&#34701;&#21512;&#36890;&#36807;&#28508;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Interpretable Multi-Source Data Fusion Through Latent Variable Gaussian Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04146
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#30340;&#22810;&#28304;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20010;&#25968;&#25454;&#28304;&#20043;&#38388;&#36136;&#37327;&#21644;&#20840;&#38754;&#24615;&#24046;&#24322;&#32473;&#31995;&#32479;&#20248;&#21270;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#20986;&#29616;&#65292;&#21508;&#20010;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#24050;&#32463;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26367;&#20195;&#27169;&#22411;&#26469;&#24314;&#27169;&#26469;&#33258;&#22823;&#37327;&#20449;&#24687;&#28304;&#65288;&#25968;&#25454;&#65289;&#30340;&#22797;&#26434;&#31995;&#32479;&#12290;&#36825;&#31181;&#22686;&#21152;&#23548;&#33268;&#20102;&#24320;&#21457;&#20986;&#29992;&#20110;&#25191;&#34892;&#29305;&#23450;&#21151;&#33021;&#30340;&#20248;&#36234;&#31995;&#32479;&#25152;&#38656;&#30340;&#25104;&#26412;&#21644;&#26102;&#38388;&#30340;&#26174;&#33879;&#38477;&#20302;&#12290;&#36825;&#26679;&#30340;&#26367;&#20195;&#27169;&#22411;&#24448;&#24448;&#24191;&#27867;&#22320;&#34701;&#21512;&#22810;&#20010;&#25968;&#25454;&#26469;&#28304;&#65292;&#21487;&#33021;&#26159;&#21457;&#34920;&#30340;&#35770;&#25991;&#12289;&#19987;&#21033;&#12289;&#24320;&#25918;&#36164;&#28304;&#24211;&#25110;&#20854;&#20182;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#20449;&#24687;&#26469;&#28304;&#30340;&#22522;&#30784;&#29289;&#29702;&#21442;&#25968;&#30340;&#36136;&#37327;&#21644;&#20840;&#38754;&#24615;&#30340;&#24046;&#24322;&#65292;&#21487;&#33021;&#23545;&#31995;&#32479;&#20248;&#21270;&#36807;&#31243;&#20135;&#29983;&#21518;&#32493;&#24433;&#21709;&#65292;&#21364;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#21464;&#37327;&#39640;&#26031;&#36807;&#31243;&#65288;LVGP&#65289;&#30340;&#22810;&#28304;&#25968;&#25454;&#34701;&#21512;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of artificial intelligence (AI) and machine learning (ML), various domains of science and engineering communites has leveraged data-driven surrogates to model complex systems from numerous sources of information (data). The proliferation has led to significant reduction in cost and time involved in development of superior systems designed to perform specific functionalities. A high proposition of such surrogates are built extensively fusing multiple sources of data, may it be published papers, patents, open repositories, or other resources. However, not much attention has been paid to the differences in quality and comprehensiveness of the known and unknown underlying physical parameters of the information sources that could have downstream implications during system optimization. Towards resolving this issue, a multi-source data fusion framework based on Latent Variable Gaussian Process (LVGP) is proposed. The individual data sources are tagged as a characteristic cate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.03190</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unified Hallucination Detection for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;UNIHD&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;MHaluBench&#26469;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#24182;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979;MLLMs&#20013;&#30340;&#24187;&#35273;&#24050;&#25104;&#20026;&#27169;&#22411;&#35780;&#20272;&#21644;&#23454;&#38469;&#24212;&#29992;&#37096;&#32626;&#20445;&#38556;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20043;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#21463;&#21040;&#20102;&#29421;&#31364;&#30340;&#20219;&#21153;&#28966;&#28857;&#12289;&#19981;&#36275;&#30340;&#24187;&#35273;&#31867;&#21035;&#28085;&#30422;&#33539;&#22260;&#20197;&#21450;&#32570;&#20047;&#35814;&#32454;&#30340;&#32454;&#31890;&#24230;&#30340;&#38480;&#21046;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#23637;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#30740;&#31350;&#33539;&#22260;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20803;&#35780;&#20272;&#22522;&#20934;&#26041;&#27861;&#65292;MHaluBench&#65292;&#31934;&#24515;&#35774;&#35745;&#20197;&#20419;&#36827;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#30340;&#36827;&#23637;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#26694;&#26550;&#65292;UNIHD&#65292;&#23427;&#21033;&#29992;&#19968;&#22871;&#36741;&#21161;&#24037;&#20855;&#26469;&#31283;&#20581;&#22320;&#39564;&#35777;&#24187;&#35273;&#30340;&#21457;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;UNIHD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD throug
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#28145;&#20837;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#23545;&#20110;&#33258;&#21160;&#29983;&#25104;&#30340;&#25490;&#29256;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26356;&#26377;&#25928;&#30340;&#33258;&#21160;&#29983;&#25104;&#30340;&#25490;&#29256;&#25915;&#20987;&#26041;&#27861;&#65292;&#20026;&#27492;&#35774;&#35745;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#27979;&#35797;&#22522;&#20934;&#12290;&#36890;&#36807;&#20351;&#29992;&#35813;&#22522;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;&#25490;&#29256;&#25915;&#20987;&#23545;LVLM&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2402.00626</link><description>&lt;p&gt;
Vision-LLMs&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#25490;&#29256;&#25915;&#20987;&#21487;&#20197;&#33258;&#27450;&#27450;&#20154;
&lt;/p&gt;
&lt;p&gt;
Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00626
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#28145;&#20837;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#23545;&#20110;&#33258;&#21160;&#29983;&#25104;&#30340;&#25490;&#29256;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26356;&#26377;&#25928;&#30340;&#33258;&#21160;&#29983;&#25104;&#30340;&#25490;&#29256;&#25915;&#20987;&#26041;&#27861;&#65292;&#20026;&#27492;&#35774;&#35745;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#27979;&#35797;&#22522;&#20934;&#12290;&#36890;&#36807;&#20351;&#29992;&#35813;&#22522;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;&#25490;&#29256;&#25915;&#20987;&#23545;LVLM&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65307;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#26032;&#31867;&#21035;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;LVLM&#23545;&#20110;&#28041;&#21450;&#23558;&#35823;&#23548;&#24615;&#25991;&#26412;&#21472;&#21152;&#21040;&#22270;&#20687;&#19978;&#30340;&#20174;&#25490;&#29256;&#25915;&#20987;&#30340;&#23481;&#26131;&#21463;&#25915;&#20987;&#24615;&#21364;&#27809;&#26377;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#25490;&#29256;&#25915;&#20987;&#20381;&#36182;&#20110;&#20174;&#39044;&#23450;&#20041;&#31867;&#21035;&#38598;&#21512;&#20013;&#38543;&#26426;&#36873;&#25321;&#19968;&#20010;&#35823;&#23548;&#24615;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#36873;&#25321;&#30340;&#31867;&#21035;&#21487;&#33021;&#19981;&#26159;&#26368;&#26377;&#25928;&#30340;&#25915;&#20987;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#29420;&#29305;&#35774;&#35745;&#30340;&#26032;&#39062;&#22522;&#20934;&#26469;&#27979;&#35797;LVLM&#23545;&#25490;&#29256;&#25915;&#20987;&#30340;&#23481;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#32780;&#26356;&#26377;&#25928;&#30340;&#25490;&#29256;&#25915;&#20987;&#65306;&#33258;&#21160;&#29983;&#25104;&#30340;&#25490;&#29256;&#25915;&#20987;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#31616;&#21333;&#22320;&#25552;&#31034;GPT-4V&#31561;&#27169;&#22411;&#21033;&#29992;&#20854;&#24378;&#22823;&#30340;&#35821;&#35328;&#33021;&#21147;&#25512;&#33616;&#19968;&#31181;&#25490;&#29256;&#25915;&#20987;&#26469;&#20026;&#32473;&#23450;&#30340;&#22270;&#20687;&#29983;&#25104;&#25915;&#20987;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#39062;&#22522;&#20934;&#65292;&#25105;&#20204;&#21457;&#29616;&#25490;&#29256;&#25915;&#20987;&#23545;LVLM&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, significant progress has been made on Large Vision-Language Models (LVLMs); a new class of VL models that make use of large pre-trained language models. Yet, their vulnerability to Typographic attacks, which involve superimposing misleading text onto an image remain unstudied. Furthermore, prior work typographic attacks rely on sampling a random misleading class from a predefined set of classes. However, the random chosen class might not be the most effective attack. To address these issues, we first introduce a novel benchmark uniquely designed to test LVLMs vulnerability to typographic attacks. Furthermore, we introduce a new and more effective typographic attack: Self-Generated typographic attacks. Indeed, our method, given an image, make use of the strong language capabilities of models like GPT-4V by simply prompting them to recommend a typographic attack. Using our novel benchmark, we uncover that typographic attacks represent a significant threat against LVLM(s). Furth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#20102;&#21487;&#36716;&#31227;&#24615;&#23545;&#25239;&#25915;&#20987;&#30340;&#26041;&#38754;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20934;&#26694;&#26550;&#24182;&#31995;&#32479;&#20998;&#31867;&#21644;&#35780;&#20272;&#20102;&#21508;&#31181;&#22686;&#24378;&#23545;&#25239;&#25915;&#20987;&#21487;&#36716;&#31227;&#24615;&#30340;&#26041;&#27861;&#65292;&#20026;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2402.00418</link><description>&lt;p&gt;
&#30701;&#25991;: &#22522;&#20934;&#27979;&#35797;&#21487;&#36716;&#31227;&#24615;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Short: Benchmarking transferable adversarial attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#20102;&#21487;&#36716;&#31227;&#24615;&#23545;&#25239;&#25915;&#20987;&#30340;&#26041;&#38754;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20934;&#26694;&#26550;&#24182;&#31995;&#32479;&#20998;&#31867;&#21644;&#35780;&#20272;&#20102;&#21508;&#31181;&#22686;&#24378;&#23545;&#25239;&#25915;&#20987;&#21487;&#36716;&#31227;&#24615;&#30340;&#26041;&#27861;&#65292;&#20026;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#20851;&#27880;&#28857;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#20102;&#21487;&#36716;&#31227;&#24615;&#23545;&#25239;&#25915;&#20987;&#30340;&#26041;&#38754;&#12290;&#23427;&#31995;&#32479;&#22320;&#20998;&#31867;&#21644;&#25209;&#21028;&#24615;&#35780;&#20272;&#20102;&#21508;&#31181;&#22686;&#24378;&#23545;&#25239;&#25915;&#20987;&#21487;&#36716;&#31227;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#21253;&#25324;&#29983;&#25104;&#32467;&#26500;&#12289;&#35821;&#20041;&#30456;&#20284;&#24615;&#12289;&#26799;&#24230;&#32534;&#36753;&#12289;&#30446;&#26631;&#20462;&#25913;&#21644;&#38598;&#25104;&#26041;&#27861;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20934;&#26694;&#26550;"TAA-Bench"&#65292;&#38598;&#25104;&#20102;&#21313;&#31181;&#20027;&#35201;&#30340;&#23545;&#25239;&#25915;&#20987;&#21487;&#36716;&#31227;&#24615;&#26041;&#27861;&#65292;&#20174;&#32780;&#20026;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#21644;&#31995;&#32479;&#21270;&#30340;&#27604;&#36739;&#20998;&#26512;&#24179;&#21488;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23457;&#26597;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#25928;&#21147;&#21644;&#38480;&#21046;&#65292;&#24182;&#38416;&#26126;&#20102;&#23427;&#20204;&#30340;&#25805;&#20316;&#21407;&#29702;&#21644;&#23454;&#38469;&#25928;&#29992;&#12290;&#26412;&#32508;&#36848;&#35797;&#22270;&#25104;&#20026;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#23545;&#27604;&#20998;&#26512;&#30340;&#26631;&#20934;&#21270;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness of deep learning models against adversarial attacks remains a pivotal concern. This study presents, for the first time, an exhaustive review of the transferability aspect of adversarial attacks. It systematically categorizes and critically evaluates various methodologies developed to augment the transferability of adversarial attacks. This study encompasses a spectrum of techniques, including Generative Structure, Semantic Similarity, Gradient Editing, Target Modification, and Ensemble Approach. Concurrently, this paper introduces a benchmark framework \textit{TAA-Bench}, integrating ten leading methodologies for adversarial attack transferability, thereby providing a standardized and systematic platform for comparative analysis across diverse model architectures. Through comprehensive scrutiny, we delineate the efficacy and constraints of each method, shedding light on their underlying operational principles and practical utility. This review endeavors to be a quintesse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;NomaFedHAP&#36825;&#19968;&#26032;&#22411;FL-SatCom&#26041;&#27861;&#65292;&#21033;&#29992;HAPs&#20316;&#20026;PS&#26469;&#22686;&#24378;&#21355;&#26143;&#21487;&#35265;&#24615;&#65292;&#24182;&#24341;&#20837;NOMA&#25216;&#26415;&#23454;&#29616;&#24555;&#36895;&#39640;&#25928;&#30340;&#27169;&#22411;&#20256;&#36755;&#12290;</title><link>https://arxiv.org/abs/2401.00685</link><description>&lt;p&gt;
&#38598;&#25104;&#28151;&#21512;NOMA-OFDM&#30340;HAP&#19982;LEO&#21355;&#26143;&#32593;&#32476;&#30340;&#36890;&#20449;&#39640;&#25928;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Federated Learning for LEO Satellite Networks Integrated with HAPs Using Hybrid NOMA-OFDM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;NomaFedHAP&#36825;&#19968;&#26032;&#22411;FL-SatCom&#26041;&#27861;&#65292;&#21033;&#29992;HAPs&#20316;&#20026;PS&#26469;&#22686;&#24378;&#21355;&#26143;&#21487;&#35265;&#24615;&#65292;&#24182;&#24341;&#20837;NOMA&#25216;&#26415;&#23454;&#29616;&#24555;&#36895;&#39640;&#25928;&#30340;&#27169;&#22411;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22826;&#31354;&#20154;&#24037;&#26234;&#33021;&#23545;&#25919;&#24220;&#12289;&#20225;&#19994;&#21644;&#31038;&#20250;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#65292;&#26377;&#26102;&#29978;&#33267;&#25104;&#20026;&#24517;&#38656;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;LEO&#21355;&#26143;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;FL-SatCom&#26041;&#27861;NomaFedHAP&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39640;&#31354;&#24179;&#21488;(HAPs)&#20316;&#20026;&#20998;&#24067;&#24335;&#21442;&#25968;&#26381;&#21153;&#22120;(PS)&#26469;&#22686;&#24378;&#21355;&#26143;&#30340;&#21487;&#35265;&#24615;&#65292;&#24341;&#20837;&#38750;&#27491;&#20132;&#22810;&#22336;&#25509;&#20837;(NOMA)&#21040;LEO&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#21644;&#24102;&#23485;&#39640;&#25928;&#30340;&#27169;&#22411;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00685v2 Announce Type: replace-cross  Abstract: Space AI has become increasingly important and sometimes even necessary for government, businesses, and society. An active research topic under this mission is integrating federated learning (FL) with satellite communications (SatCom) so that numerous low Earth orbit (LEO) satellites can collaboratively train a machine learning model. However, the special communication environment of SatCom leads to a very slow FL training process up to days and weeks. This paper proposes NomaFedHAP, a novel FL-SatCom approach tailored to LEO satellites, that (1) utilizes high-altitude platforms (HAPs) as distributed parameter servers (PS) to enhance satellite visibility, and (2) introduces non-orthogonal multiple access (NOMA) into LEO to enable fast and bandwidth-efficient model transmissions. In addition, NomaFedHAP includes (3) a new communication topology that exploits HAPs to bridge satellites among different orbits to mitigate the Dopple
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#39640;&#26031;&#36807;&#31243;&#26679;&#26412;&#36335;&#24452;&#27491;&#21017;&#24615;&#30340;&#26032;&#39062;&#21644;&#32039;&#20945;&#30340;&#29305;&#24449;&#25551;&#36848;&#65292;&#36890;&#36807;&#21327;&#26041;&#24046;&#26680;&#23545;&#24212;&#30340;GP&#26679;&#26412;&#36335;&#24452;&#36798;&#21040;&#19968;&#23450;&#27491;&#21017;&#24615;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#65292;&#23545;&#24120;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;GPs&#30340;&#26679;&#26412;&#36335;&#24452;&#27491;&#21017;&#24615;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;</title><link>https://arxiv.org/abs/2312.14886</link><description>&lt;p&gt;
&#26469;&#33258;&#21327;&#26041;&#24046;&#26680;&#30340;&#39640;&#26031;&#36807;&#31243;&#26679;&#26412;&#36335;&#24452;&#27491;&#21017;&#24615;
&lt;/p&gt;
&lt;p&gt;
Sample Path Regularity of Gaussian Processes from the Covariance Kernel
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#39640;&#26031;&#36807;&#31243;&#26679;&#26412;&#36335;&#24452;&#27491;&#21017;&#24615;&#30340;&#26032;&#39062;&#21644;&#32039;&#20945;&#30340;&#29305;&#24449;&#25551;&#36848;&#65292;&#36890;&#36807;&#21327;&#26041;&#24046;&#26680;&#23545;&#24212;&#30340;GP&#26679;&#26412;&#36335;&#24452;&#36798;&#21040;&#19968;&#23450;&#27491;&#21017;&#24615;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#65292;&#23545;&#24120;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;GPs&#30340;&#26679;&#26412;&#36335;&#24452;&#27491;&#21017;&#24615;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#26159;&#23450;&#20041;&#20989;&#25968;&#31354;&#38388;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#26368;&#24120;&#35265;&#24418;&#24335;&#20027;&#20041;&#12290;&#23613;&#31649;GPs&#30340;&#24212;&#29992;&#24191;&#27867;&#65292;&#20294;&#23545;&#20110;GP&#26679;&#26412;&#36335;&#24452;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#21363;&#23427;&#20204;&#23450;&#20041;&#27010;&#29575;&#27979;&#24230;&#30340;&#20989;&#25968;&#31354;&#38388;&#65292;&#23578;&#32570;&#20047;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;GPs&#19981;&#26159;&#36890;&#36807;&#27010;&#29575;&#27979;&#24230;&#26500;&#24314;&#30340;&#65292;&#32780;&#26159;&#36890;&#36807;&#22343;&#20540;&#20989;&#25968;&#21644;&#21327;&#26041;&#24046;&#26680;&#26500;&#24314;&#30340;&#12290;&#26412;&#25991;&#38024;&#23545;&#21327;&#26041;&#24046;&#26680;&#25552;&#20379;&#20102;GP&#26679;&#26412;&#36335;&#24452;&#36798;&#21040;&#32473;&#23450;&#27491;&#21017;&#24615;&#25152;&#38656;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#12290;&#25105;&#20204;&#20351;&#29992;H\"older&#27491;&#21017;&#24615;&#26694;&#26550;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#29305;&#21035;&#31616;&#21333;&#30340;&#26465;&#20214;&#65292;&#22312;&#24179;&#31283;&#21644;&#21508;&#21521;&#21516;&#24615;GPs&#30340;&#24773;&#20917;&#19979;&#36827;&#19968;&#27493;&#31616;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#32467;&#26524;&#20801;&#35768;&#23545;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#24120;&#29992;&#30340;GPs&#30340;&#26679;&#26412;&#36335;&#24452;&#27491;&#21017;&#24615;&#36827;&#34892;&#26032;&#39062;&#19988;&#24322;&#24120;&#32039;&#20945;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14886v2 Announce Type: replace  Abstract: Gaussian processes (GPs) are the most common formalism for defining probability distributions over spaces of functions. While applications of GPs are myriad, a comprehensive understanding of GP sample paths, i.e. the function spaces over which they define a probability measure, is lacking. In practice, GPs are not constructed through a probability measure, but instead through a mean function and a covariance kernel. In this paper we provide necessary and sufficient conditions on the covariance kernel for the sample paths of the corresponding GP to attain a given regularity. We use the framework of H\"older regularity as it grants particularly straightforward conditions, which simplify further in the cases of stationary and isotropic GPs. We then demonstrate that our results allow for novel and unusually tight characterisations of the sample path regularities of the GPs commonly used in machine learning applications, such as the Mat\'
&lt;/p&gt;</description></item><item><title>Q-SENN&#25552;&#20986;&#20102;&#37327;&#21270;&#33258;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#25551;&#36848;&#27599;&#20010;&#31867;&#19982;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#20026;&#27491;&#12289;&#36127;&#25110;&#20013;&#24615;&#65292;&#23454;&#29616;&#26356;&#20108;&#20803;&#21270;&#12289;&#26356;&#31526;&#21512;&#20154;&#31867;&#21451;&#22909;&#30340;&#29305;&#24449;&#65292;&#21516;&#26102;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2312.13839</link><description>&lt;p&gt;
Q-SENN: &#37327;&#21270;&#33258;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Q-SENN: Quantized Self-Explaining Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13839
&lt;/p&gt;
&lt;p&gt;
Q-SENN&#25552;&#20986;&#20102;&#37327;&#21270;&#33258;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#25551;&#36848;&#27599;&#20010;&#31867;&#19982;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#20026;&#27491;&#12289;&#36127;&#25110;&#20013;&#24615;&#65292;&#23454;&#29616;&#26356;&#20108;&#20803;&#21270;&#12289;&#26356;&#31526;&#21512;&#20154;&#31867;&#21451;&#22909;&#30340;&#29305;&#24449;&#65292;&#21516;&#26102;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#24120;&#38656;&#35201;&#35299;&#37322;&#65292;&#20294;&#22823;&#22810;&#25968;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21482;&#33021;&#25552;&#20379;&#20855;&#26377;&#21487;&#30097;&#24544;&#23454;&#24230;&#30340;&#26174;&#33879;&#24615;&#22320;&#22270;&#12290;&#33258;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;(SENN)&#25552;&#21462;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#65292;&#20855;&#26377;&#24544;&#23454;&#24230;&#12289;&#22810;&#26679;&#24615;&#21644;&#22522;&#30784;&#24615;&#65292;&#23558;&#23427;&#20204;&#32447;&#24615;&#32452;&#21512;&#29992;&#20110;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#37327;&#21270;&#33258;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;Q-SENN&#12290;Q-SENN&#28385;&#36275;&#25110;&#36229;&#36807;SENN&#30340;&#26399;&#26395;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#26356;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20445;&#25345;&#20102;&#19982;&#19981;&#21487;&#35299;&#37322;&#22522;&#32447;&#27169;&#22411;&#20960;&#20046;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#65292;&#20248;&#20110;&#20197;&#24448;&#25152;&#26377;&#32771;&#34385;&#25351;&#26631;&#30340;&#24037;&#20316;&#12290;Q-SENN&#23558;&#27599;&#20010;&#31867;&#19982;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#25551;&#36848;&#20026;&#27491;&#12289;&#36127;&#25110;&#20013;&#24615;&#65292;&#32780;&#19981;&#26159;&#20219;&#24847;&#25968;&#37327;&#30340;&#21487;&#33021;&#20851;&#31995;&#65292;&#24378;&#21046;&#37319;&#29992;&#26356;&#20108;&#20803;&#21270;&#12289;&#26356;&#31526;&#21512;&#20154;&#31867;&#21451;&#22909;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.13839v2 Announce Type: replace-cross  Abstract: Explanations in Computer Vision are often desired, but most Deep Neural Networks can only provide saliency maps with questionable faithfulness. Self-Explaining Neural Networks (SENN) extract interpretable concepts with fidelity, diversity, and grounding to combine them linearly for decision-making. While they can explain what was recognized, initial realizations lack accuracy and general applicability. We propose the Quantized-Self-Explaining Neural Network Q-SENN. Q-SENN satisfies or exceeds the desiderata of SENN while being applicable to more complex datasets and maintaining most or all of the accuracy of an uninterpretable baseline model, out-performing previous work in all considered metrics. Q-SENN describes the relationship between every class and feature as either positive, negative or neutral instead of an arbitrary number of possible relations, enforcing more binary human-friendly features. Since every class is assign
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26032;VQA&#25968;&#25454;&#38598;BloomVQA&#65292;&#22522;&#20110;Bloom&#30340;&#20998;&#31867;&#27861;&#65292;&#36890;&#36807;&#23618;&#27425;&#22270;&#34920;&#31034;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#21644;&#27169;&#22411;&#19968;&#33268;&#24615;&#35780;&#20272;&#65292;&#25581;&#31034;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#32423;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2312.12716</link><description>&lt;p&gt;
BloomVQA&#65306;&#35780;&#20272;&#20998;&#23618;&#22810;&#27169;&#24577;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
BloomVQA: Assessing Hierarchical Multi-modal Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12716
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26032;VQA&#25968;&#25454;&#38598;BloomVQA&#65292;&#22522;&#20110;Bloom&#30340;&#20998;&#31867;&#27861;&#65292;&#36890;&#36807;&#23618;&#27425;&#22270;&#34920;&#31034;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#21644;&#27169;&#22411;&#19968;&#33268;&#24615;&#35780;&#20272;&#65292;&#25581;&#31034;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#32423;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;VQA&#25968;&#25454;&#38598;BloomVQA&#65292;&#26088;&#22312;&#20419;&#36827;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;&#19982;&#24403;&#21069;&#30340;&#22522;&#20934;&#19981;&#21516;&#65292;&#23427;&#20204;&#36890;&#24120;&#20391;&#37325;&#20110;&#22522;&#20110;&#20107;&#23454;&#30340;&#35760;&#24518;&#21644;&#27809;&#26377;&#29702;&#35770;&#22522;&#30784;&#30340;&#31616;&#21333;&#25512;&#29702;&#20219;&#21153;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#22522;&#20110;&#22270;&#29255;&#25925;&#20107;&#30340;&#22810;&#39033;&#36873;&#25321;&#26679;&#26412;&#65292;&#21453;&#26144;&#20102;&#19981;&#21516;&#23618;&#27425;&#30340;&#29702;&#35299;&#65292;&#27491;&#22914;&#24067;&#40065;&#22982;&#30340;&#20998;&#31867;&#27861;&#25152;&#23637;&#31034;&#30340;&#65292;&#22312;&#25945;&#32946;&#30740;&#31350;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#32463;&#20856;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#26144;&#23556;&#21040;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#22270;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#25968;&#25454;&#22686;&#24378;&#21644;&#34920;&#24449;&#27169;&#22411;&#19968;&#33268;&#24615;&#30340;&#26032;&#25514;&#26045;&#12290;&#25105;&#20204;&#23545;&#26368;&#36817;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#32423;&#35780;&#20272;&#21644;&#21487;&#38752;&#24615;&#20998;&#26512;&#12290;&#19982;&#20302;&#32423;&#20219;&#21153;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#38656;&#35201;&#39640;&#32423;&#29702;&#35299;&#21644;&#35748;&#30693;&#33021;&#21147;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#19979;&#38477;&#65292;VQA&#20934;&#30830;&#24615;&#19979;&#38477;&#20102;&#39640;&#36798;38.0%&#12290;&#19982;&#26089;&#26399;&#27169;&#22411;&#30456;&#27604;&#65292;GPT-4V&#34920;&#29616;&#20986;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12716v2 Announce Type: replace-cross  Abstract: We propose a novel VQA dataset, BloomVQA, to facilitate comprehensive evaluation of large vision-language models on comprehension tasks. Unlike current benchmarks that often focus on fact-based memorization and simple reasoning tasks without theoretical grounding, we collect multiple-choice samples based on picture stories that reflect different levels of comprehension, as laid out in Bloom's Taxonomy, a classic framework for learning assessment widely adopted in education research. Our data maps to a novel hierarchical graph representation which enables automatic data augmentation and novel measures characterizing model consistency. We perform graded evaluation and reliability analysis on recent multi-modal models. In comparison to low-level tasks, we observe decreased performance on tasks requiring advanced comprehension and cognitive skills with up to 38.0% drop in VQA accuracy. In comparison to earlier models, GPT-4V demons
&lt;/p&gt;</description></item><item><title>KGLens &#26159;&#19968;&#20010;&#26088;&#22312;&#34913;&#37327;&#30693;&#35782;&#22270;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#23545;&#40784;&#31243;&#24230;&#30340;&#26694;&#26550;&#65292;&#24110;&#21161;&#25214;&#20986;LLMs&#30456;&#23545;&#20110;&#30693;&#35782;&#22270;&#30340;&#30693;&#35782;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>https://arxiv.org/abs/2312.11539</link><description>&lt;p&gt;
KGLens&#65306;&#19968;&#20010;&#21442;&#25968;&#21270;&#30693;&#35782;&#22270;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#30693;&#36947;&#21644;&#19981;&#30693;&#36947;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM Does and Doesn't Know
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11539
&lt;/p&gt;
&lt;p&gt;
KGLens &#26159;&#19968;&#20010;&#26088;&#22312;&#34913;&#37327;&#30693;&#35782;&#22270;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#23545;&#40784;&#31243;&#24230;&#30340;&#26694;&#26550;&#65292;&#24110;&#21161;&#25214;&#20986;LLMs&#30456;&#23545;&#20110;&#30693;&#35782;&#22270;&#30340;&#30693;&#35782;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34913;&#37327;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#26159;&#35780;&#20272;&#20107;&#23454;&#24615;&#24182;&#35782;&#21035;LLMs&#30340;&#30693;&#35782;&#30450;&#28857;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#23558;KGs&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#21644;&#39640;&#25928;&#35780;&#20272;&#36825;&#20123;&#24191;&#27867;&#19988;&#22797;&#26434;&#30340;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KGLens--&#19968;&#20010;&#26088;&#22312;&#34913;&#37327;KGs&#21644;LLMs&#20043;&#38388;&#23545;&#40784;&#31243;&#24230;&#65292;&#24182;&#25214;&#20986;LLMs&#30456;&#23545;&#20110;KGs&#30340;&#30693;&#35782;&#32570;&#38519;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;KGLens&#20855;&#26377;&#19968;&#20010;&#22270;&#24341;&#23548;&#30340;&#38382;&#39064;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#23558;KGs&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#20197;&#21450;&#19968;&#20010;&#22522;&#20110;&#21442;&#25968;&#21270;KG&#32467;&#26500;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#21152;&#24555;KG&#30340;&#36941;&#21382;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;Wikidata&#30340;&#19977;&#20010;&#39046;&#22495;&#29305;&#23450;KG&#36827;&#34892;&#23454;&#39564;&#65292;&#36825;&#20123;KG&#21253;&#25324;&#36229;&#36807;19,000&#26465;&#36793;&#65292;700&#20010;&#20851;&#31995;&#21644;21,000&#20010;&#23454;&#20307;&#12290;&#25105;&#20204;&#36328;&#36234;8&#20010;LLMs&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;KGLens&#19981;&#20165;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11539v2 Announce Type: replace  Abstract: Measuring the alignment between a Knowledge Graph (KG) and Large Language Models (LLMs) is an effective method to assess the factualness and identify the knowledge blind spots of LLMs. However, this approach encounters two primary challenges including the translation of KGs into natural language and the efficient evaluation of these extensive and complex structures. In this paper, we present KGLens--a novel framework aimed at measuring the alignment between KGs and LLMs, and pinpointing the LLMs' knowledge deficiencies relative to KGs. KGLens features a graph-guided question generator for converting KGs into natural language, along with a carefully designed sampling strategy based on parameterized KG structure to expedite KG traversal. We conducted experiments using three domain-specific KGs from Wikidata, which comprise over 19,000 edges, 700 relations, and 21,000 entities. Our analysis across eight LLMs reveals that KGLens not only
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Cascade Speculative Drafting&#65288;CS Drafting&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#22402;&#30452;&#32423;&#32852;&#28040;&#38500;&#31070;&#32463;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#36890;&#36807;&#27700;&#24179;&#32423;&#32852;&#20248;&#21270;&#33609;&#31295;&#20013;&#30340;&#26102;&#38388;&#20998;&#37197;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.11462</link><description>&lt;p&gt;
&#29992;&#20110;&#26356;&#24555;&#30340;LLM&#25512;&#29702;&#30340;&#32423;&#32852;&#25512;&#27979;&#33609;&#22270;
&lt;/p&gt;
&lt;p&gt;
Cascade Speculative Drafting for Even Faster LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11462
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Cascade Speculative Drafting&#65288;CS Drafting&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#22402;&#30452;&#32423;&#32852;&#28040;&#38500;&#31070;&#32463;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#36890;&#36807;&#27700;&#24179;&#32423;&#32852;&#20248;&#21270;&#33609;&#31295;&#20013;&#30340;&#26102;&#38388;&#20998;&#37197;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#29702;&#25928;&#29575;&#30340;&#32423;&#32852;&#25512;&#27979;&#33609;&#22270;&#65292;&#36890;&#36807;&#36739;&#23567;&#30340;&#27169;&#22411;&#29983;&#25104;&#33609;&#31295;&#26469;&#36816;&#20316;&#12290;&#36739;&#22823;&#30340;&#30446;&#26631;&#27169;&#22411;&#28982;&#21518;&#26597;&#30475;&#36825;&#20010;&#33609;&#31295;&#20197;&#19982;&#20854;&#36755;&#20986;&#23545;&#40784;&#65292;&#30446;&#26631;&#27169;&#22411;&#30340;&#20219;&#20309;&#25509;&#21463;&#37117;&#23558;&#20943;&#23569;&#30446;&#26631;&#27169;&#22411;&#36816;&#34892;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#32423;&#32852;&#25512;&#27979;&#30340;&#33609;&#22270;&#36807;&#31243;&#20013;&#21253;&#25324;&#32531;&#24930;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#24182;&#20026;&#29983;&#25104;&#30340;&#26631;&#35760;&#20998;&#37197;&#30456;&#21516;&#30340;&#26102;&#38388;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#20302;&#25928;&#24615;&#20849;&#21516;&#23548;&#33268;&#32423;&#32852;&#25512;&#27979;&#30340;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25913;&#21892;LLM&#25512;&#29702;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32423;&#32852;&#25512;&#27979;&#33609;&#22270;&#65288;CS Drafting&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#25972;&#21512;&#20102;&#20004;&#31181;&#32423;&#32852;&#31867;&#22411;&#30340;&#25512;&#27979;&#25191;&#34892;&#31639;&#27861;&#12290;&#22402;&#30452;&#32423;&#32852;&#20174;&#31070;&#32463;&#27169;&#22411;&#20013;&#28040;&#38500;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#32780;&#27700;&#24179;&#32423;&#32852;&#20248;&#21270;&#20102;&#33609;&#31295;&#20013;&#30340;&#26102;&#38388;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11462v3 Announce Type: replace-cross  Abstract: Introduced to enhance the efficiency of large language model (LLM) inference, speculative decoding operates by having a smaller model generate a draft. A larger target model then reviews this draft to align with its output, and any acceptance by the target model results in a reduction of the number of the target model runs, ultimately improving efficiency. However, the drafting process in speculative decoding includes slow autoregressive generation and allocates equal time to generating tokens, irrespective of their importance. These inefficiencies collectively contribute to the suboptimal performance of speculative decoding. To further improve LLM inference, we introduce Cascade Speculative Drafting (CS Drafting), a speculative execution algorithm that incorporates two types of cascades. The Vertical Cascade eliminates autoregressive generation from neural models, while the Horizontal Cascade optimizes time allocation in draft
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#30828;&#20214;&#39640;&#25928;&#24615;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#21487;&#22312;&#30701;&#24207;&#21015;&#38271;&#24230;&#19979;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#24555;&#65292;&#21516;&#26102;&#25512;&#24191;&#21040;&#20102;&#20855;&#26377;&#25968;&#25454;&#30456;&#20851;&#38376;&#30340;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#21464;&#20307;&#12290;</title><link>https://arxiv.org/abs/2312.06635</link><description>&lt;p&gt;
&#20855;&#26377;&#30828;&#20214;&#39640;&#25928;&#35757;&#32451;&#30340;&#38376;&#25511;&#32447;&#24615;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Gated Linear Attention Transformers with Hardware-Efficient Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06635
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#30828;&#20214;&#39640;&#25928;&#24615;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#31639;&#27861;&#65292;&#21487;&#22312;&#30701;&#24207;&#21015;&#38271;&#24230;&#19979;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#24555;&#65292;&#21516;&#26102;&#25512;&#24191;&#21040;&#20102;&#20855;&#26377;&#25968;&#25454;&#30456;&#20851;&#38376;&#30340;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#21464;&#21387;&#22120;&#20801;&#35768;&#36827;&#34892;&#39640;&#25928;&#30340;&#24182;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#21487;&#20197;&#34987;&#34920;&#36848;&#20026;&#20855;&#26377;2D&#65288;&#30697;&#38453;&#20540;&#65289;&#38544;&#34255;&#29366;&#24577;&#30340;RNN&#65292;&#20174;&#32780;&#20139;&#21463;&#32447;&#24615;&#26102;&#38388;&#25512;&#26029;&#22797;&#26434;&#24230;&#12290;&#28982;&#32780;&#65292;&#32447;&#24615;&#27880;&#24847;&#21147;&#36890;&#24120;&#34920;&#29616;&#19981;&#22914;&#26222;&#36890;softmax&#27880;&#24847;&#21147;&#12290;&#32780;&#19988;&#65292;&#24403;&#21069;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#23454;&#29616;&#32570;&#20047;I/O&#24863;&#30693;&#24615;&#65292;&#22240;&#27492;&#27604;&#39640;&#24230;&#20248;&#21270;&#30340;softmax&#27880;&#24847;&#21147;&#23454;&#29616;&#26356;&#24930;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#30828;&#20214;&#39640;&#25928;&#31639;&#27861;&#65292;&#23427;&#22312;&#20869;&#23384;&#31227;&#21160;&#21644;&#21487;&#24182;&#34892;&#24615;&#20043;&#38388;&#36827;&#34892;&#25240;&#20013;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#23454;&#29616;&#65292;&#34987;&#31216;&#20026;FLASHLINEARATTENTION&#65292;&#22312;&#30701;&#24207;&#21015;&#38271;&#24230;&#65288;&#20363;&#22914;&#65292;1K&#65289;&#19979;&#65292;&#21363;&#20351;&#20316;&#20026;&#21333;&#29420;&#30340;&#23618;&#20063;&#27604;FLASHATTENTION-2(Dao, 2023)&#26356;&#24555;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#35813;&#31639;&#27861;&#25512;&#24191;&#21040;&#20855;&#26377;&#25968;&#25454;&#30456;&#20851;&#38376;&#30340;&#26356;&#20855;&#34920;&#36798;&#33021;&#21147;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#21464;&#20307;&#12290;&#24403;&#29992;&#20316;&#21464;&#25442;&#22120;&#20013;&#26631;&#20934;&#27880;&#24847;&#21147;&#23618;&#30340;&#26367;&#20195;&#26102;&#65292;&#20135;&#29983;&#30340;&#38376;&#25511;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06635v4 Announce Type: replace-cross  Abstract: Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2(Dao, 2023) as a standalone layer even at short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gate
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;Transformers&#21487;&#20197;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#33258;&#28982;&#22320;&#23398;&#20064;&#23454;&#29616;&#26799;&#24230;&#19979;&#38477;&#65292;&#20174;&#32780;&#23398;&#20064;&#19978;&#19979;&#25991;&#20013;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#26368;&#20248;&#30340;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#36873;&#25321;&#21462;&#20915;&#20110;&#38656;&#35201;&#23398;&#20064;&#30340;&#20989;&#25968;&#31867;&#21035;&#12290;</title><link>https://arxiv.org/abs/2312.06528</link><description>&lt;p&gt;
Transformers&#23454;&#29616;&#20102;&#21151;&#33021;&#24615;&#26799;&#24230;&#19979;&#38477;&#26469;&#23398;&#20064;&#19978;&#19979;&#25991;&#20013;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;Transformers&#21487;&#20197;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#33258;&#28982;&#22320;&#23398;&#20064;&#23454;&#29616;&#26799;&#24230;&#19979;&#38477;&#65292;&#20174;&#32780;&#23398;&#20064;&#19978;&#19979;&#25991;&#20013;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#26368;&#20248;&#30340;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#36873;&#25321;&#21462;&#20915;&#20110;&#38656;&#35201;&#23398;&#20064;&#30340;&#20989;&#25968;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#34987;&#35748;&#20026;&#26159;&#22270;&#28789;&#23436;&#22791;&#30340;&#65292;&#22240;&#27492;&#21407;&#21017;&#19978;&#21487;&#20197;&#23454;&#29616;&#20219;&#24847;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;Transformers&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#23427;&#20204;&#21487;&#20197;&#22312;&#31616;&#21333;&#30340;&#21442;&#25968;&#37197;&#32622;&#19979;&#23454;&#29616;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#65292;&#35777;&#26126;&#65288;&#38750;&#32447;&#24615;&#65289;Transformers&#33258;&#28982;&#22320;&#23398;&#20250;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#23454;&#29616;&#26799;&#24230;&#19979;&#38477;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#19968;&#22823;&#31867;&#38750;&#32447;&#24615;&#26550;&#26500;&#21644;&#38750;&#32447;&#24615;&#19978;&#19979;&#25991;&#23398;&#20064;&#20219;&#21153;&#30340;&#32452;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#26368;&#20248;&#36873;&#25321;&#20197;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#24335;&#21462;&#20915;&#20110;&#38656;&#35201;&#23398;&#20064;&#30340;&#20989;&#25968;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06528v4 Announce Type: replace  Abstract: Many neural network architectures are known to be Turing Complete, and can thus, in principle implement arbitrary algorithms. However, Transformers are unique in that they can implement gradient-based learning algorithms under simple parameter configurations. This paper provides theoretical and empirical evidence that (non-linear) Transformers naturally learn to implement gradient descent in function space, which in turn enable them to learn non-linear functions in context. Our results apply to a broad class of combinations of non-linear architectures and non-linear in-context learning tasks. Additionally, we show that the optimal choice of non-linear activation depends in a natural way on the class of functions that need to be learned.
&lt;/p&gt;</description></item><item><title>PULSAR&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#25163;&#25351;&#25970;&#20987;&#20219;&#21153;&#30340;&#35270;&#39057;&#26469;&#31579;&#26597;&#24085;&#37329;&#26862;&#30149;&#65292;&#37319;&#29992;&#20102;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#36890;&#36947;&#33258;&#36866;&#24212;&#21367;&#31215;&#26469;&#21160;&#24577;&#23398;&#20064;&#26102;&#31354;&#22270;&#36793;&#12290;</title><link>https://arxiv.org/abs/2312.05780</link><description>&lt;p&gt;
&#24085;&#37329;&#26862;&#30149;&#35782;&#21035;&#30340;&#22522;&#20110;&#22270;&#30340;&#27491;&#36127;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;PULSAR&#65306;&#20855;&#26377;&#22810;&#36890;&#36947;&#33258;&#36866;&#24212;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
PULSAR: Graph based Positive Unlabeled Learning with Multi Stream Adaptive Convolutions for Parkinson's Disease Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05780
&lt;/p&gt;
&lt;p&gt;
PULSAR&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#25163;&#25351;&#25970;&#20987;&#20219;&#21153;&#30340;&#35270;&#39057;&#26469;&#31579;&#26597;&#24085;&#37329;&#26862;&#30149;&#65292;&#37319;&#29992;&#20102;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22810;&#36890;&#36947;&#33258;&#36866;&#24212;&#21367;&#31215;&#26469;&#21160;&#24577;&#23398;&#20064;&#26102;&#31354;&#22270;&#36793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#37329;&#26862;&#30149;&#65288;PD&#65289;&#26159;&#19968;&#31181;&#24433;&#21709;&#36816;&#21160;&#12289;&#35328;&#35821;&#21644;&#21327;&#35843;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#12290;&#21450;&#26102;&#35786;&#26029;&#21644;&#27835;&#30103;&#21487;&#20197;&#25913;&#21892;PD&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#65288;LMICs&#65289;&#30340;&#20020;&#24202;&#35786;&#26029;&#36164;&#28304;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#29992;&#20110;PD&#30340;&#33258;&#21160;&#31579;&#26597;&#24037;&#20855;&#23545;&#31038;&#20250;&#24433;&#21709;&#24040;&#22823;&#65292;&#29305;&#21035;&#26159;&#22312;&#20844;&#20849;&#21355;&#29983;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;PULSAR&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#36816;&#21160;&#38556;&#30861;&#23398;&#20250;-&#32479;&#19968;&#24085;&#37329;&#26862;&#30149;&#35780;&#20998;&#37327;&#34920;&#65288;MDS-UPDRS&#65289;&#30340;&#25163;&#25351;&#25970;&#20987;&#20219;&#21153;&#30340;&#32593;&#32476;&#25668;&#20687;&#22836;&#24405;&#21046;&#35270;&#39057;&#20013;&#36827;&#34892;PD&#31579;&#26597;&#12290;PULSAR&#22312;382&#21517;&#21442;&#19982;&#32773;&#65288;183&#21517;&#33258;&#25105;&#25253;&#21578;&#20026;PD&#24739;&#32773;&#65289;&#25910;&#38598;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21160;&#24577;&#23398;&#20064;&#29305;&#23450;&#20110;&#25163;&#25351;&#25970;&#20987;&#20219;&#21153;&#30340;&#26102;&#31354;&#22270;&#36793;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#22810;&#36890;&#36947;&#33258;&#36866;&#24212;&#21367;&#31215;&#26469;&#22686;&#24378;&#36825;&#19968;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05780v2 Announce Type: replace-cross  Abstract: Parkinson's disease (PD) is a neuro-degenerative disorder that affects movement, speech, and coordination. Timely diagnosis and treatment can improve the quality of life for PD patients. However, access to clinical diagnosis is limited in low and middle income countries (LMICs). Therefore, development of automated screening tools for PD can have a huge social impact, particularly in the public health sector. In this paper, we present PULSAR, a novel method to screen for PD from webcam-recorded videos of the finger-tapping task from the Movement Disorder Society - Unified Parkinson's Disease Rating Scale (MDS-UPDRS). PULSAR is trained and evaluated on data collected from 382 participants (183 self-reported as PD patients). We used an adaptive graph convolutional neural network to dynamically learn the spatio temporal graph edges specific to the finger-tapping task. We enhanced this idea with a multi stream adaptive convolution m
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65288;DeepSAD&#65289;&#26041;&#27861;&#36827;&#34892;&#20581;&#24247;&#25351;&#25968;&#26500;&#24314;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#26679;&#24615;&#25439;&#22833;&#26469;&#20016;&#23500;&#26465;&#20214;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2312.02867</link><description>&lt;p&gt;
&#20351;&#29992;&#29305;&#24449;&#29983;&#25104;&#21644;&#34701;&#21512;&#30340;&#21322;&#30417;&#30563;&#20581;&#24247;&#25351;&#25968;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Health Index Monitoring with Feature Generation and Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02867
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65288;DeepSAD&#65289;&#26041;&#27861;&#36827;&#34892;&#20581;&#24247;&#25351;&#25968;&#26500;&#24314;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#26679;&#24615;&#25439;&#22833;&#26469;&#20016;&#23500;&#26465;&#20214;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#25351;&#25968;&#65288;HI&#65289;&#23545;&#20110;&#35780;&#20272;&#31995;&#32479;&#20581;&#24247;&#29366;&#24577;&#33267;&#20851;&#37325;&#35201;&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#24322;&#24120;&#65292;&#24182;&#39044;&#27979;&#23545;&#39640;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#35201;&#27714;&#39640;&#30340;&#31995;&#32479;&#30340;&#21097;&#20313;&#20351;&#29992;&#23551;&#21629;&#12290;&#22312;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#21516;&#26102;&#38477;&#20302;&#25104;&#26412;&#65292;&#32039;&#23494;&#30417;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#33719;&#21462;HI&#26631;&#31614;&#24448;&#24448;&#25104;&#26412;&#39640;&#26114;&#65292;&#38656;&#35201;&#36830;&#32493;&#12289;&#31934;&#30830;&#30340;&#20581;&#24247;&#27979;&#37327;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#21487;&#33021;&#25552;&#20379;&#28508;&#22312;&#26426;&#22120;&#30952;&#25439;&#29366;&#24577;&#25351;&#31034;&#30340;&#8220;&#36816;&#34892;&#33267;&#25925;&#38556;&#8221;&#25968;&#25454;&#38598;&#65292;&#26356;&#26041;&#20415;&#37319;&#29992;&#21322;&#30417;&#30563;&#24037;&#20855;&#26500;&#24314;HI&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02867v2 Announce Type: replace  Abstract: The Health Index (HI) is crucial for evaluating system health, aiding tasks like anomaly detection and predicting remaining useful life for systems demanding high safety and reliability. Tight monitoring is crucial for achieving high precision at a lower cost. Obtaining HI labels in real-world applications is often cost-prohibitive, requiring continuous, precise health measurements. Therefore, it is more convenient to leverage run-to failure datasets that may provide potential indications of machine wear condition, making it necessary to apply semi-supervised tools for HI construction. In this study, we adapt the Deep Semi-supervised Anomaly Detection (DeepSAD) method for HI construction. We use the DeepSAD embedding as a condition indicators to address interpretability challenges and sensitivity to system-specific factors. Then, we introduce a diversity loss to enrich condition indicators. We employ an alternating projection algorit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#39640;&#26031;LTI&#31995;&#32479;&#20013;&#24341;&#20837;&#21508;&#31181;&#24178;&#39044;&#20449;&#21495;&#65292;&#36830;&#25509;&#20102;&#23454;&#39564;&#35774;&#35745;&#21644;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#34920;&#31034;&#21487;&#35782;&#21035;&#24615;&#65292;&#20026;&#31995;&#32479;&#21442;&#25968;&#30340;&#35782;&#21035;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.18048</link><description>&lt;p&gt;
&#39640;&#26031;LTI&#31995;&#32479;&#20013;&#24178;&#39044;&#35270;&#35282;&#19979;&#30340;&#21487;&#35782;&#21035;&#24615;&#19982;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Interventional Perspective on Identifiability in Gaussian LTI Systems with Independent Component Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#39640;&#26031;LTI&#31995;&#32479;&#20013;&#24341;&#20837;&#21508;&#31181;&#24178;&#39044;&#20449;&#21495;&#65292;&#36830;&#25509;&#20102;&#23454;&#39564;&#35774;&#35745;&#21644;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#34920;&#31034;&#21487;&#35782;&#21035;&#24615;&#65292;&#20026;&#31995;&#32479;&#21442;&#25968;&#30340;&#35782;&#21035;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21160;&#24577;&#31995;&#32479;&#20013;&#31995;&#32479;&#35782;&#21035;&#21644;&#24178;&#39044;&#35774;&#35745;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35832;&#22914;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65288;ICA&#65289;&#30340;&#21487;&#35782;&#21035;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#25581;&#31034;&#22240;&#26524;&#20851;&#31995;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#19968;&#31181;&#34987;&#21160;&#30340;&#35270;&#35282;&#65292;&#27809;&#26377;&#32771;&#34385;&#22914;&#20309;&#25910;&#38598;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#39640;&#26031;&#32447;&#24615;&#26102;&#19981;&#21464;&#65288;LTI&#65289;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#22312;&#22810;&#29615;&#22659;&#35774;&#32622;&#20013;&#24341;&#20837;&#21508;&#31181;&#24178;&#39044;&#20449;&#21495;&#65292;&#21487;&#20197;&#35782;&#21035;&#31995;&#32479;&#21442;&#25968;&#12290;&#36890;&#36807;&#21033;&#29992;ICA&#25991;&#29486;&#28608;&#21457;&#30340;&#36866;&#24403;&#22810;&#26679;&#24615;&#20551;&#35774;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#23558;&#23454;&#39564;&#35774;&#35745;&#19982;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#34920;&#31034;&#35782;&#21035;&#33021;&#21147;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#65288;&#27169;&#25311;&#65289;&#29289;&#29702;&#25968;&#25454;&#19978;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65288;HMM&#65289;&#24635;&#20307;&#19978;&#20197;&#21450;&#65288;&#39640;&#26031;&#65289;LTI&#31995;&#32479;&#29305;&#21035;&#28385;&#36275;Causal de Finetti &#30340;&#19968;&#33324;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18048v2 Announce Type: replace  Abstract: We investigate the relationship between system identification and intervention design in dynamical systems. While previous research demonstrated how identifiable representation learning methods, such as Independent Component Analysis (ICA), can reveal cause-effect relationships, it relied on a passive perspective without considering how to collect data. Our work shows that in Gaussian Linear Time-Invariant (LTI) systems, the system parameters can be identified by introducing diverse intervention signals in a multi-environment setting. By harnessing appropriate diversity assumptions motivated by the ICA literature, our findings connect experiment design and representational identifiability in dynamical systems. We corroborate our findings on synthetic and (simulated) physical data. Additionally, we show that Hidden Markov Models, in general, and (Gaussian) LTI systems, in particular, fulfil a generalization of the Causal de Finetti th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;K&#31354;&#38388;&#36827;&#34892;&#22270;&#20687;&#36864;&#21270;&#21644;&#24674;&#22797;&#30340;&#20919;&#25193;&#25955;&#27169;&#22411;&#65292;&#26080;&#38656;&#22122;&#38899;&#65292;&#33021;&#22815;&#20026;&#21152;&#36895;MRI&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#22270;&#20687;</title><link>https://arxiv.org/abs/2311.10162</link><description>&lt;p&gt;
K&#31354;&#38388;&#20919;&#25193;&#25955;&#65306;&#23398;&#20064;&#22312;&#27809;&#26377;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#37325;&#24314;&#21152;&#36895;MRI
&lt;/p&gt;
&lt;p&gt;
K-space Cold Diffusion: Learning to Reconstruct Accelerated MRI without Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10162
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;K&#31354;&#38388;&#36827;&#34892;&#22270;&#20687;&#36864;&#21270;&#21644;&#24674;&#22797;&#30340;&#20919;&#25193;&#25955;&#27169;&#22411;&#65292;&#26080;&#38656;&#22122;&#38899;&#65292;&#33021;&#22815;&#20026;&#21152;&#36895;MRI&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;MRI&#37325;&#24314;&#27169;&#22411;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#12289;&#20462;&#34917;&#12289;&#36229;&#20998;&#36776;&#29575;&#12289;&#22270;&#20687;&#32534;&#36753;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20919;&#25193;&#25955;&#36827;&#19968;&#27493;&#25299;&#23485;&#20102;&#33539;&#22260;&#65292;&#24182;&#32771;&#34385;&#20102;&#22260;&#32469;&#20219;&#24847;&#22270;&#20687;&#21464;&#25442;&#26500;&#24314;&#30340;&#27169;&#22411;&#65292;&#20363;&#22914;&#27169;&#31946;&#12289;&#19979;&#37319;&#26679;&#31561;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;K&#31354;&#38388;&#20013;&#25191;&#34892;&#22270;&#20687;&#36864;&#21270;&#21644;&#24674;&#22797;&#30340;K&#31354;&#38388;&#20919;&#25193;&#25955;&#27169;&#22411;&#65292;&#26080;&#38656;&#39640;&#26031;&#22122;&#22768;&#12290;&#25105;&#20204;&#19982;&#22810;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;MRI&#37325;&#24314;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#22312;&#19968;&#20010;&#30693;&#21517;&#30340;&#22823;&#22411;&#24320;&#28304;MRI&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#39062;&#30340;&#36864;&#21270;&#26041;&#24335;&#21487;&#20197;&#20026;&#21152;&#36895;MRI&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10162v2 Announce Type: replace-cross  Abstract: Deep learning-based MRI reconstruction models have achieved superior performance these days. Most recently, diffusion models have shown remarkable performance in image generation, in-painting, super-resolution, image editing and more. As a generalized diffusion model, cold diffusion further broadens the scope and considers models built around arbitrary image transformations such as blurring, down-sampling, etc. In this paper, we propose a k-space cold diffusion model that performs image degradation and restoration in k-space without the need for Gaussian noise. We provide comparisons with multiple deep learning-based MRI reconstruction models and perform tests on a well-known large open-source MRI dataset. Our results show that this novel way of performing degradation can generate high-quality reconstruction images for accelerated MRI.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32570;&#20047;&#36275;&#22815;&#21442;&#25968;&#21270;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#34920;&#36798;&#23545;&#36229;&#20986;&#20854;&#30693;&#35782;&#33539;&#22260;&#30340;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#35802;&#23454;&#19982;&#24110;&#21161;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2311.09731</link><description>&lt;p&gt;
&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#34920;&#36798;&#23545;&#36229;&#20986;&#21442;&#25968;&#21270;&#30693;&#35782;&#33539;&#22260;&#30340;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Examining LLMs' Uncertainty Expression Towards Questions Outside Parametric Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32570;&#20047;&#36275;&#22815;&#21442;&#25968;&#21270;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#34920;&#36798;&#23545;&#36229;&#20986;&#20854;&#30693;&#35782;&#33539;&#22260;&#30340;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#35802;&#23454;&#19982;&#24110;&#21161;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#31995;&#32479;&#22320;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32570;&#20047;&#36275;&#22815;&#21442;&#25968;&#21270;&#30693;&#35782;&#20197;&#29983;&#25104;&#21512;&#29702;&#22238;&#24212;&#30340;&#24773;&#20917;&#19979;&#30340;&#34892;&#20026;&#65292;&#24378;&#35843;&#35802;&#23454;&#19982;&#24110;&#21161;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#31934;&#30830;&#30830;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#31354;&#30333;&#25361;&#25112;&#65292;&#25105;&#20204;&#35786;&#26029;&#24615;&#22320;&#21019;&#24314;&#20102;&#21253;&#21547;&#19981;&#23384;&#22312;&#27010;&#24565;&#25110;&#38169;&#35823;&#21069;&#25552;&#30340;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#30830;&#20445;&#23427;&#20204;&#36229;&#20986;&#20102;&#35821;&#35328;&#27169;&#22411;&#24222;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36890;&#36807;&#32534;&#21046;&#19968;&#20010;&#21253;&#21547;&#26082;&#26377;&#26080;&#27861;&#22238;&#31572;&#20063;&#26377;&#21487;&#22238;&#31572;&#38382;&#39064;&#30340;&#22522;&#20934;&#65292;UnknownBench&#65292;&#25105;&#20204;&#23450;&#37327;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#20445;&#25345;&#35802;&#23454;&#30340;&#21516;&#26102;&#25552;&#20379;&#24110;&#21161;&#30340;&#34920;&#29616;&#12290;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#32479;&#19968;&#20449;&#24515;&#24341;&#23548;&#26041;&#27861;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#33268;&#25298;&#32477;&#25110;&#34920;&#36798;&#23545;&#36229;&#20986;&#20854;&#21442;&#25968;&#21270;&#30693;&#35782;&#33539;&#22260;&#30340;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09731v2 Announce Type: replace-cross  Abstract: Can large language models (LLMs) express their uncertainty in situations where they lack sufficient parametric knowledge to generate reasonable responses? This work aims to systematically investigate LLMs' behaviors in such situations, emphasizing the trade-off between honesty and helpfulness. To tackle the challenge of precisely determining LLMs' knowledge gaps, we diagnostically create unanswerable questions containing non-existent concepts or false premises, ensuring that they are outside the LLMs' vast training data. By compiling a benchmark, UnknownBench, which consists of both unanswerable and answerable questions, we quantitatively evaluate the LLMs' performance in maintaining honesty while being helpful. Using a model-agnostic unified confidence elicitation approach, we observe that most LLMs fail to consistently refuse or express uncertainty towards questions outside their parametric knowledge, although instruction fin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#33258;&#28982;&#24230;&#35299;&#37322;&#19982;&#35780;&#20272;&#27169;&#22411;&#65288;CNE&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#20998;&#26512;&#21355;&#26143;&#22270;&#20687;&#65292;&#20197;&#35299;&#37322;&#21644;&#35780;&#20272;&#24418;&#25104;&#33258;&#28982;&#24230;&#30340;&#27169;&#24335;&#65292;&#24182;&#24102;&#26377;&#30456;&#24212;&#30340;&#32622;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2311.08936</link><description>&lt;p&gt;
&#33258;&#28982;&#24230;&#35299;&#37322;&#19982;&#35780;&#20272;&#27169;&#22411;&#65288;CNE&#65289;&#65306;&#35299;&#37322;&#21644;&#35780;&#20272;&#24418;&#25104;&#33258;&#28982;&#24230;&#30340;&#27169;&#24335;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Confident Naturalness Explanation (CNE): A Framework to Explain and Assess Patterns Forming Naturalness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#33258;&#28982;&#24230;&#35299;&#37322;&#19982;&#35780;&#20272;&#27169;&#22411;&#65288;CNE&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#20998;&#26512;&#21355;&#26143;&#22270;&#20687;&#65292;&#20197;&#35299;&#37322;&#21644;&#35780;&#20272;&#24418;&#25104;&#33258;&#28982;&#24230;&#30340;&#27169;&#24335;&#65292;&#24182;&#24102;&#26377;&#30456;&#24212;&#30340;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#20154;&#31867;&#27963;&#21160;&#65288;&#22914;&#22478;&#24066;&#21270;&#12289;&#20892;&#19994;&#21644;&#20854;&#20182;&#20154;&#20026;&#24178;&#39044;&#65289;&#26368;&#23567;&#24433;&#21709;&#30340;&#33258;&#28982;&#20445;&#25252;&#21306;&#26159;&#25351;&#37027;&#20123;&#22320;&#21306;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#32472;&#21046;&#36825;&#20123;&#21306;&#22495;&#30340;&#33258;&#28982;&#24230;&#65292;&#21487;&#20197;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#21355;&#26143;&#22270;&#20687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#25581;&#31034;&#36825;&#20123;&#20445;&#25252;&#29615;&#22659;&#20013;&#26377;&#21161;&#20110;&#33258;&#28982;&#24230;&#27010;&#24565;&#30340;&#27169;&#24335;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#35299;&#20915;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#30456;&#20851;&#30340;&#38382;&#39064;&#23545;&#20110;&#20840;&#38754;&#20102;&#35299;&#36825;&#20010;&#27010;&#24565;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20182;&#20204;&#35201;&#20040;&#26080;&#27861;&#25552;&#20379;&#26082;&#26377;&#25928;&#21448;&#23458;&#35266;&#30340;&#35299;&#37322;&#65292;&#35201;&#20040;&#38590;&#20197;&#25552;&#20379;&#20934;&#30830;&#34913;&#37327;&#29305;&#23450;&#27169;&#24335;&#23545;&#33258;&#28982;&#24230;&#36129;&#29486;&#30340;&#23450;&#37327;&#25351;&#26631;&#20197;&#21450;&#30456;&#20851;&#30340;&#32622;&#20449;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#33258;&#28982;&#24230;&#35299;&#37322;&#19982;&#35780;&#20272;&#27169;&#22411;&#65288;CNE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protected natural areas are regions that have been minimally affected by human activities such as urbanization, agriculture, and other human interventions. To better understand and map the naturalness of these areas, machine learning models can be used to analyze satellite imagery. Specifically, explainable machine learning methods show promise in uncovering patterns that contribute to the concept of naturalness within these protected environments. Additionally, addressing the uncertainty inherent in machine learning models is crucial for a comprehensive understanding of this concept. However, existing approaches have limitations. They either fail to provide explanations that are both valid and objective or struggle to offer a quantitative metric that accurately measures the contribution of specific patterns to naturalness, along with the associated confidence. In this paper, we propose a novel framework called the Confident Naturalness Explanation (CNE) framework. This framework combi
&lt;/p&gt;</description></item><item><title>FLrce&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#25552;&#21069;&#20572;&#27490;&#31574;&#30053;&#21644;&#20108;&#36827;&#21046;&#20462;&#21098;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#23433;&#20840;&#21644;&#36164;&#28304;&#21033;&#29992;&#19981;&#22343;&#34913;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.09789</link><description>&lt;p&gt;
FLrce: &#20855;&#26377;&#25552;&#21069;&#20572;&#27490;&#31574;&#30053;&#30340;&#36164;&#28304;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FLrce: Resource-Efficient Federated Learning with Early-Stopping Strategy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.09789
&lt;/p&gt;
&lt;p&gt;
FLrce&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#25552;&#21069;&#20572;&#27490;&#31574;&#30053;&#21644;&#20108;&#36827;&#21046;&#20462;&#21098;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#23433;&#20840;&#21644;&#36164;&#28304;&#21033;&#29992;&#19981;&#22343;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#36793;&#32536;&#35774;&#22791;&#36164;&#28304;&#30701;&#32570;&#21644;&#19981;&#24179;&#34913;&#30340;&#35757;&#32451;&#36129;&#29486;&#65292;&#25552;&#20986;&#20102;FLrce&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25552;&#21069;&#20572;&#27490;&#31574;&#30053;&#21644;&#20108;&#36827;&#21046;&#20462;&#21098;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#22312;&#19981;&#27844;&#38706;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20256;&#32479;&#32852;&#37030;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#23433;&#20840;&#21644;&#36164;&#28304;&#21033;&#29992;&#19981;&#22343;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.09789v2 Announce Type: replace  Abstract: Federated learning (FL) achieves great popularity in the Internet of Things (IoT) as a powerful interface to offer intelligent services to customers while maintaining data privacy. Under the orchestration of a server, edge devices (also called clients in FL) collaboratively train a global deep-learning model without sharing any local data. Nevertheless, the unequal training contributions among clients have made FL vulnerable, as clients with heavily biased datasets can easily compromise FL by sending malicious or heavily biased parameter updates. Furthermore, the resource shortage issue of edge devices also becomes a bottleneck. Due to overwhelming computation overheads generated by training deep-learning models on edge devices, and significant communication overheads for transmitting deep-learning models across the network, enormous amounts of resources are consumed in the FL process. This encompasses computation resources like ener
&lt;/p&gt;</description></item><item><title>&#26631;&#31614;&#24179;&#28369;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#26082;&#33021;&#25552;&#21319;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#21644;&#26657;&#20934;&#24615;&#65292;&#21448;&#21487;&#33021;&#25104;&#20026;&#27169;&#22411;&#38544;&#31169;&#27844;&#38706;&#30340;&#22240;&#32032;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#32467;&#21512;&#36127;&#22240;&#23376;&#36827;&#34892;&#24179;&#28369;&#21487;&#26377;&#25928;&#38459;&#27490;&#27169;&#22411;&#21453;&#25512;&#25915;&#20987;&#65292;&#25552;&#21319;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2310.06549</link><description>&lt;p&gt;
&#35880;&#24910;&#24179;&#28369;&#26631;&#31614;&#65306;&#26631;&#31614;&#24179;&#28369;&#26082;&#21487;&#20197;&#20316;&#20026;&#38544;&#31169;&#23631;&#38556;&#65292;&#21448;&#21487;&#20197;&#25104;&#20026;&#27169;&#22411;&#21453;&#25512;&#25915;&#20987;&#30340;&#20652;&#21270;&#21058;
&lt;/p&gt;
&lt;p&gt;
Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06549
&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24179;&#28369;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#26082;&#33021;&#25552;&#21319;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#21644;&#26657;&#20934;&#24615;&#65292;&#21448;&#21487;&#33021;&#25104;&#20026;&#27169;&#22411;&#38544;&#31169;&#27844;&#38706;&#30340;&#22240;&#32032;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#32467;&#21512;&#36127;&#22240;&#23376;&#36827;&#34892;&#24179;&#28369;&#21487;&#26377;&#25928;&#38459;&#27490;&#27169;&#22411;&#21453;&#25512;&#25915;&#20987;&#65292;&#25552;&#21319;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#24179;&#28369;&#8212;&#8212;&#20351;&#29992;&#36719;&#21270;&#30340;&#26631;&#31614;&#32780;&#19981;&#26159;&#30828;&#26631;&#31614;&#8212;&#8212;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#34920;&#29616;&#20986;&#22686;&#24378;&#27867;&#21270;&#21644;&#26657;&#20934;&#31561;&#22810;&#26679;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#23427;&#23545;&#20110;&#20445;&#25252;&#27169;&#22411;&#38544;&#31169;&#30340;&#24433;&#21709;&#20173;&#28982;&#27809;&#26377;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26631;&#31614;&#24179;&#28369;&#23545;&#27169;&#22411;&#21453;&#25512;&#25915;&#20987;&#65288;MIAs&#65289;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#20998;&#31867;&#22120;&#20013;&#32534;&#30721;&#30340;&#30693;&#35782;&#29983;&#25104;&#20855;&#26377;&#31867;&#20195;&#34920;&#24615;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#25512;&#26029;&#26377;&#20851;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#26631;&#31614;&#24179;&#28369;&#20419;&#36827;&#20102;MIAs&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#26356;&#29978;&#32773;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#29992;&#36127;&#22240;&#23376;&#36827;&#34892;&#24179;&#28369;&#21487;&#20197;&#25269;&#21046;&#36825;&#19968;&#36235;&#21183;&#65292;&#38459;&#30861;&#25552;&#21462;&#19982;&#31867;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#65292;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#36825;&#30830;&#31435;&#20102;&#19968;&#31181;&#23454;&#29992;&#19988;&#24378;&#22823;&#30340;&#26032;&#30340;&#22686;&#24378;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.06549v2 Announce Type: replace  Abstract: Label smoothing -- using softened labels instead of hard ones -- is a widely adopted regularization method for deep learning, showing diverse benefits such as enhanced generalization and calibration. Its implications for preserving model privacy, however, have remained unexplored. To fill this gap, we investigate the impact of label smoothing on model inversion attacks (MIAs), which aim to generate class-representative samples by exploiting the knowledge encoded in a classifier, thereby inferring sensitive information about its training data. Through extensive analyses, we uncover that traditional label smoothing fosters MIAs, thereby increasing a model's privacy leakage. Even more, we reveal that smoothing with negative factors counters this trend, impeding the extraction of class-related information and leading to privacy preservation, beating state-of-the-art defenses. This establishes a practical and powerful novel way for enhanc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#37327;&#23376;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;QuDDPM&#65289;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#37327;&#23376;&#25968;&#25454;&#30340;&#39640;&#25928;&#21487;&#35757;&#32451;&#30340;&#29983;&#25104;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#36275;&#22815;&#23618;&#25968;&#30340;&#30005;&#36335;&#20197;&#20445;&#35777;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#22810;&#20010;&#20013;&#38388;&#35757;&#32451;&#20219;&#21153;&#20197;&#36991;&#20813;&#36139;&#30240;&#24179;&#21407;&#24182;&#20445;&#35777;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2310.05866</link><description>&lt;p&gt;
&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24615;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generative quantum machine learning via denoising diffusion probabilistic models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05866
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#37327;&#23376;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;QuDDPM&#65289;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#37327;&#23376;&#25968;&#25454;&#30340;&#39640;&#25928;&#21487;&#35757;&#32451;&#30340;&#29983;&#25104;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#36275;&#22815;&#23618;&#25968;&#30340;&#30005;&#36335;&#20197;&#20445;&#35777;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#22810;&#20010;&#20013;&#38388;&#35757;&#32451;&#20219;&#21153;&#20197;&#36991;&#20813;&#36139;&#30240;&#24179;&#21407;&#24182;&#20445;&#35777;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#25991;&#26412;&#29983;&#25104;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#20197;&#21450;&#32467;&#26500;&#28789;&#27963;&#12289;&#35757;&#32451;&#31616;&#21333;&#30340;&#29305;&#28857;&#65292;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#21033;&#29992;&#32416;&#32544;&#21644;&#21472;&#21152;&#30340;&#33021;&#21147;&#20026;&#23398;&#20064;&#32463;&#20856;&#21644;&#37327;&#23376;&#25968;&#25454;&#24102;&#26469;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#21463;&#32463;&#20856;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#37327;&#23376;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#8221;&#65288;QuDDPM&#65289;&#65292;&#20197;&#23454;&#29616;&#23545;&#37327;&#23376;&#25968;&#25454;&#30340;&#39640;&#25928;&#21487;&#35757;&#32451;&#30340;&#29983;&#25104;&#23398;&#20064;&#12290;QuDDPM&#37319;&#29992;&#36275;&#22815;&#23618;&#25968;&#30340;&#30005;&#36335;&#26469;&#20445;&#35777;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#24341;&#20837;&#22810;&#20010;&#20013;&#38388;&#35757;&#32451;&#20219;&#21153;&#65292;&#23558;&#30446;&#26631;&#20998;&#24067;&#19982;&#22122;&#22768;&#20043;&#38388;&#30340;&#25554;&#20540;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#36991;&#20813;&#36139;&#30240;&#24179;&#21407;&#24182;&#20445;&#35777;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#23398;&#20064;&#35823;&#24046;&#30340;&#19978;&#30028;&#21644;...&#65288;&#26410;&#23436;&#24453;&#32493;&#65289;
&lt;/p&gt;
&lt;p&gt;
Deep generative models are key-enabling technology to computer vision, text generation and large language models. Denoising diffusion probabilistic models (DDPMs) have recently gained much attention due to their ability to generate diverse and high-quality samples in many computer vision tasks, as well as to incorporate flexible model architectures and relatively simple training scheme. Quantum generative models, empowered by entanglement and superposition, have brought new insight to learning classical and quantum data. Inspired by the classical counterpart, we propose the \emph{quantum denoising diffusion probabilistic model} (QuDDPM) to enable efficiently trainable generative learning of quantum data. QuDDPM adopts sufficient layers of circuits to guarantee expressivity, while introduces multiple intermediate training tasks as interpolation between the target distribution and noise to avoid barren plateau and guarantee efficient training. We provide bounds on the learning error and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#23454;&#29616;K&#22343;&#20540;&#32858;&#31867;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#20013;&#24515;&#20043;&#38388;&#32858;&#31867;&#25968;&#37327;&#21464;&#21270;&#21644;&#22312;&#19981;&#22826;&#21487;&#20998;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2310.01195</link><description>&lt;p&gt;
&#32852;&#37030;&#24335;K&#22343;&#20540;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Federated K-means Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#23454;&#29616;K&#22343;&#20540;&#32858;&#31867;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#20013;&#24515;&#20043;&#38388;&#32858;&#31867;&#25968;&#37327;&#21464;&#21270;&#21644;&#22312;&#19981;&#22826;&#21487;&#20998;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#27719;&#24635;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#20998;&#24067;&#24335;&#25968;&#25454;&#38598;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#20445;&#25252;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#25152;&#26377;&#26435;&#12290;&#23613;&#31649;&#30417;&#30563;&#24335;&#32852;&#37030;&#23398;&#20064;&#30740;&#31350;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26080;&#30417;&#30563;&#24335;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20173;&#28982;&#24456;&#23569;&#35265;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20197;&#32852;&#37030;&#26041;&#24335;&#23454;&#29616;K&#22343;&#20540;&#32858;&#31867;&#65292;&#35299;&#20915;&#20102;&#20013;&#24515;&#20043;&#38388;&#32858;&#31867;&#25968;&#37327;&#19981;&#21516;&#20197;&#21450;&#22312;&#36739;&#38590;&#20998;&#24320;&#30340;&#25968;&#25454;&#38598;&#19978;&#25910;&#25947;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.01195v2 Announce Type: replace  Abstract: Federated learning is a technique that enables the use of distributed datasets for machine learning purposes without requiring data to be pooled, thereby better preserving privacy and ownership of the data. While supervised FL research has grown substantially over the last years, unsupervised FL methods remain scarce. This work introduces an algorithm which implements K-means clustering in a federated manner, addressing the challenges of varying number of clusters between centers, as well as convergence on less separable datasets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20809;&#35874;&#23572;&#23486;&#26684;&#26725;&#24555;&#36895;&#31616;&#21333;&#27714;&#35299;&#22120;&#65292;&#23558;&#35874;&#23572;&#23486;&#26684;&#21183;&#33021;&#21442;&#25968;&#21270;&#20026;&#24635;&#21644;-&#25351;&#25968;&#20108;&#27425;&#20989;&#25968;&#65292;&#35270;&#20026;&#33021;&#37327;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#36731;&#37327;&#32423;&#12289;&#26080;&#38656;&#27169;&#25311;&#12289;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#27714;&#35299;&#22120;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2310.01174</link><description>&lt;p&gt;
&#20809;&#35874;&#23572;&#23486;&#26684;&#26725;
&lt;/p&gt;
&lt;p&gt;
Light Schr\"odinger Bridge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01174
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20809;&#35874;&#23572;&#23486;&#26684;&#26725;&#24555;&#36895;&#31616;&#21333;&#27714;&#35299;&#22120;&#65292;&#23558;&#35874;&#23572;&#23486;&#26684;&#21183;&#33021;&#21442;&#25968;&#21270;&#20026;&#24635;&#21644;-&#25351;&#25968;&#20108;&#27425;&#20989;&#25968;&#65292;&#35270;&#20026;&#33021;&#37327;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#36731;&#37327;&#32423;&#12289;&#26080;&#38656;&#27169;&#25311;&#12289;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#27714;&#35299;&#22120;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35745;&#31639;&#35874;&#23572;&#23486;&#26684;&#26725;&#65288;SB&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#26032;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;SB&#27714;&#35299;&#22120;&#20173;&#28982;&#36807;&#37325;&#65292;&#24182;&#38656;&#35201;&#23545;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22797;&#26434;&#20248;&#21270;&#12290;&#20107;&#23454;&#35777;&#26126;&#65292;&#30446;&#21069;&#19981;&#23384;&#22312;&#20027;&#35201;&#30340;&#27714;&#35299;&#22120;&#21487;&#20197;&#20687;&#32858;&#31867;&#20013;&#30340;$k$-means&#26041;&#27861;&#12289;&#20998;&#31867;&#20013;&#30340;&#36923;&#36753;&#22238;&#24402;&#25110;&#31163;&#25955;&#26368;&#20248;&#36755;&#36816;&#20013;&#30340;Sinkhorn&#31639;&#27861;&#37027;&#26679;&#36215;&#21040;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20934;&#20316;&#29992;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24555;&#36895;&#31616;&#21333;&#30340;SB&#27714;&#35299;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26368;&#36817;&#20986;&#29616;&#22312;&#35813;&#39046;&#22495;&#30340;&#20004;&#20010;&#35266;&#28857;&#30340;&#24039;&#22937;&#32467;&#21512;&#65306;&#65288;a&#65289;&#20351;&#29992;&#24635;&#21644;-&#25351;&#25968;&#20108;&#27425;&#20989;&#25968;&#23545;&#35874;&#23572;&#23486;&#26684;&#21183;&#33021;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#65288;b&#65289;&#23558;&#23545;&#25968;&#35874;&#23572;&#23486;&#26684;&#21183;&#33021;&#35270;&#20026;&#33021;&#37327;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#36825;&#20123;&#35266;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#21487;&#20197;&#20135;&#29983;&#19968;&#20010;&#36731;&#37327;&#32423;&#12289;&#26080;&#38656;&#27169;&#25311;&#19988;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#21512;&#29702;&#30340;SB&#27714;&#35299;&#22120;&#65292;&#20855;&#26377;&#31616;&#21333;&#30452;&#25509;&#30340;&#20248;&#21270;&#30446;&#26631;&#12290;&#22240;&#27492;&#65292;&#23427;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.01174v2 Announce Type: replace  Abstract: Despite the recent advances in the field of computational Schrodinger Bridges (SB), most existing SB solvers are still heavy-weighted and require complex optimization of several neural networks. It turns out that there is no principal solver which plays the role of simple-yet-effective baseline for SB just like, e.g., $k$-means method in clustering, logistic regression in classification or Sinkhorn algorithm in discrete optimal transport. We address this issue and propose a novel fast and simple SB solver. Our development is a smart combination of two ideas which recently appeared in the field: (a) parameterization of the Schrodinger potentials with sum-exp quadratic functions and (b) viewing the log-Schrodinger potentials as the energy functions. We show that combined together these ideas yield a lightweight, simulation-free and theoretically justified SB solver with a simple straightforward optimization objective. As a result, it a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20195;&#30721;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20887;&#20313;&#21644;&#27010;&#24565;&#20998;&#26512;&#65292;&#36890;&#36807;&#28040;&#38500;&#19982;&#32473;&#23450;&#20219;&#21153;&#19981;&#30456;&#20851;&#30340;&#31070;&#32463;&#20803;&#65292;&#24110;&#21161;&#29702;&#35299;&#32593;&#32476;&#20013;&#21738;&#20123;&#31070;&#32463;&#20803;&#21644;&#23618;&#21487;&#20197;&#34987;&#28040;&#38500;&#65292;&#24182;&#22312;&#21738;&#37324;&#25214;&#21040;&#37325;&#35201;&#30340;&#20195;&#30721;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2305.00875</link><description>&lt;p&gt;
&#22522;&#20110;&#20195;&#30721;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20887;&#20313;&#21644;&#27010;&#24565;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Redundancy and Concept Analysis for Code-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.00875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20195;&#30721;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20887;&#20313;&#21644;&#27010;&#24565;&#20998;&#26512;&#65292;&#36890;&#36807;&#28040;&#38500;&#19982;&#32473;&#23450;&#20219;&#21153;&#19981;&#30456;&#20851;&#30340;&#31070;&#32463;&#20803;&#65292;&#24110;&#21161;&#29702;&#35299;&#32593;&#32476;&#20013;&#21738;&#20123;&#31070;&#32463;&#20803;&#21644;&#23618;&#21487;&#20197;&#34987;&#28040;&#38500;&#65292;&#24182;&#22312;&#21738;&#37324;&#25214;&#21040;&#37325;&#35201;&#30340;&#20195;&#30721;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20195;&#30721;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20195;&#30721;&#26234;&#33021;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#29942;&#39048;&#21644;&#20869;&#23384;&#38480;&#21046;&#65292;&#23545;&#20110;&#35768;&#22810;&#36719;&#20214;&#24037;&#31243;&#24212;&#29992;&#26469;&#35828;&#65292;&#23427;&#20204;&#21487;&#33021;&#38590;&#20197;&#35757;&#32451;&#21644;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#23454;&#26045;&#26377;&#25928;&#30340;&#31574;&#30053;&#38656;&#35201;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#8220;&#40657;&#21283;&#23376;&#8221;&#27169;&#22411;&#12290;&#26412;&#25991;&#38024;&#23545;&#28304;&#20195;&#30721;&#27169;&#22411;&#36827;&#34892;&#20102;&#39318;&#27425;&#31070;&#32463;&#20803;&#32423;&#21035;&#20998;&#26512;&#65292;&#20197;&#35782;&#21035;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#8220;&#37325;&#35201;&#8221;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#36890;&#36807;&#28040;&#38500;&#19982;&#32473;&#23450;&#20219;&#21153;&#39640;&#24230;&#30456;&#20284;&#25110;&#19981;&#30456;&#20851;&#30340;&#31070;&#32463;&#20803;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#25105;&#20204;&#20102;&#35299;&#21738;&#20123;&#31070;&#32463;&#20803;&#21644;&#23618;&#21487;&#20197;&#34987;&#28040;&#38500;&#65288;&#20887;&#20313;&#20998;&#26512;&#65289;&#65292;&#20197;&#21450;&#32593;&#32476;&#20013;&#30340;&#37325;&#35201;&#20195;&#30721;&#23646;&#24615;&#20301;&#20110;&#20309;&#22788;&#65288;&#27010;&#24565;&#20998;&#26512;&#65289;&#12290;&#21033;&#29992;&#20887;&#20313;&#20998;&#26512;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19982;&#30693;&#35782;&#36716;&#31227;&#21644;&#27169;&#22411;&#20248;&#21270;&#24212;&#29992;&#30456;&#20851;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.00875v2 Announce Type: replace-cross  Abstract: Code-trained language models have proven to be highly effective for various code intelligence tasks. However, they can be challenging to train and deploy for many software engineering applications due to computational bottlenecks and memory constraints. Implementing effective strategies to address these issues requires a better understanding of these 'black box' models. In this paper, we perform the first neuron-level analysis for source code models to identify \textit{important} neurons within latent representations. We achieve this by eliminating neurons that are highly similar or irrelevant to the given task. This approach helps us understand which neurons and layers can be eliminated (redundancy analysis) and where important code properties are located within the network (concept analysis). Using redundancy analysis, we make observations relevant to knowledge transfer and model optimization applications. We find that over 9
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;iDeepViewLearn&#65288;&#21487;&#35299;&#37322;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65289;&#29992;&#20110;&#22810;&#35270;&#22270;&#25968;&#25454;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#23398;&#20064;&#21644;&#29305;&#24449;&#36873;&#25321;&#65292;&#34701;&#21512;&#28145;&#24230;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#21644;&#32479;&#35745;&#20248;&#21183;&#65292;&#32473;&#20986;&#21487;&#35299;&#37322;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2302.07930</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Interpretable Deep Learning Methods for Multiview Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.07930
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;iDeepViewLearn&#65288;&#21487;&#35299;&#37322;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65289;&#29992;&#20110;&#22810;&#35270;&#22270;&#25968;&#25454;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#23398;&#20064;&#21644;&#29305;&#24449;&#36873;&#25321;&#65292;&#34701;&#21512;&#28145;&#24230;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#21644;&#32479;&#35745;&#20248;&#21183;&#65292;&#32473;&#20986;&#21487;&#35299;&#37322;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#26415;&#36827;&#27493;&#24050;&#32463;&#23454;&#29616;&#20102;&#29983;&#25104;&#29420;&#29305;&#19988;&#20114;&#34917;&#31867;&#22411;&#30340;&#25968;&#25454;&#25110;&#35270;&#22270;&#65288;&#20363;&#22914;&#22522;&#22240;&#32452;&#23398;&#12289;&#34507;&#30333;&#36136;&#32452;&#23398;&#12289;&#20195;&#35874;&#32452;&#23398;&#65289;&#65292;&#24182;&#24320;&#21019;&#20102;&#22810;&#35270;&#22270;&#23398;&#20064;&#30740;&#31350;&#26032;&#26102;&#20195;&#65292;&#26377;&#28508;&#21147;&#24102;&#26469;&#26032;&#30340;&#29983;&#29289;&#21307;&#23398;&#21457;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986; iDeepViewLearn&#65288;Interpretable Deep Learning Method for Multiview Learning&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#26469;&#33258;&#22810;&#35270;&#22270;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#21516;&#26102;&#23454;&#29616;&#29305;&#24449;&#36873;&#25321;&#12290;iDeepViewLearn&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#19982;&#25968;&#25454;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#29305;&#24449;&#36873;&#25321;&#30340;&#32479;&#35745;&#20248;&#21183;&#65292;&#20197;&#32473;&#20986;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36890;&#36807;&#26368;&#23567;&#21270;&#35266;&#23519;&#25968;&#25454;&#19982;&#37325;&#26500;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#23545;&#37325;&#26500;&#25968;&#25454;&#26045;&#21152;&#27491;&#21017;&#21270;&#24809;&#32602;&#26469;&#23398;&#20064;&#35270;&#22270;&#29420;&#31435;&#30340;&#20302;&#32500;&#23884;&#20837;&#12290;&#22270;&#30340;&#24402;&#19968;&#21270;&#25289;&#26222;&#25289;&#26031;&#29992;&#20110;&#24314;&#27169;&#21464;&#37327;&#20043;&#38388;&#30340;&#21452;&#36793;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.07930v2 Announce Type: replace  Abstract: Technological advances have enabled the generation of unique and complementary types of data or views (e.g. genomics, proteomics, metabolomics) and opened up a new era in multiview learning research with the potential to lead to new biomedical discoveries. We propose iDeepViewLearn (Interpretable Deep Learning Method for Multiview Learning) for learning nonlinear relationships in data from multiple views while achieving feature selection. iDeepViewLearn combines deep learning flexibility with the statistical benefits of data and knowledge-driven feature selection, giving interpretable results. Deep neural networks are used to learn view-independent low-dimensional embedding through an optimization problem that minimizes the difference between observed and reconstructed data, while imposing a regularization penalty on the reconstructed data. The normalized Laplacian of a graph is used to model bilateral relationships between variables
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#21333;&#32454;&#32990;&#35270;&#20026;&#31354;&#38388;&#26631;&#35760;&#65292;&#24182;&#21033;&#29992;Transformer&#27169;&#22411;&#20419;&#36827;&#31354;&#38388;&#36716;&#24405;&#32452;&#25968;&#25454;&#22635;&#20805;&#12290;</title><link>https://arxiv.org/abs/2302.03038</link><description>&lt;p&gt;
&#21333;&#32454;&#32990;&#26159;&#31354;&#38388;&#26631;&#35760;&#65306;&#29992;&#20110;&#31354;&#38388;&#36716;&#24405;&#32452;&#25968;&#25454;&#22635;&#20805;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
Single Cells Are Spatial Tokens: Transformers for Spatial Transcriptomic Data Imputation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.03038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#21333;&#32454;&#32990;&#35270;&#20026;&#31354;&#38388;&#26631;&#35760;&#65292;&#24182;&#21033;&#29992;Transformer&#27169;&#22411;&#20419;&#36827;&#31354;&#38388;&#36716;&#24405;&#32452;&#25968;&#25454;&#22635;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#35299;&#26512;&#36716;&#24405;&#32452;&#23398;&#36890;&#36807;&#25552;&#20379;&#29289;&#29702;&#20301;&#32622;&#21644;&#22522;&#22240;&#34920;&#36798;&#24102;&#26469;&#20102;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#28608;&#21160;&#20154;&#24515;&#30340;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26497;&#39640;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#25104;&#26412;&#65292;&#32454;&#32990;&#27700;&#24179;&#30340;&#31354;&#38388;&#36716;&#24405;&#32452;&#25968;&#25454;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23384;&#22312;&#32570;&#22833;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;&#21333;&#20010;&#32454;&#32990;&#35270;&#20026;&#31354;&#38388;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#22810;&#22836;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#20301;&#32622;&#32534;&#30721;&#30340;Transformer&#27169;&#22411;&#26469;&#20419;&#36827;&#31354;&#38388;&#36716;&#24405;&#32452;&#22635;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.03038v2 Announce Type: replace-cross  Abstract: Spatially resolved transcriptomics brings exciting breakthroughs to single-cell analysis by providing physical locations along with gene expression. However, as a cost of the extremely high spatial resolution, the cellular level spatial transcriptomic data suffer significantly from missing values. While a standard solution is to perform imputation on the missing values, most existing methods either overlook spatial information or only incorporate localized spatial context without the ability to capture long-range spatial information. Using multi-head self-attention mechanisms and positional encoding, transformer models can readily grasp the relationship between tokens and encode location information. In this paper, by treating single cells as spatial tokens, we study how to leverage transformers to facilitate spatial tanscriptomics imputation. In particular, investigate the following two key questions: (1) $\textit{how to encod
&lt;/p&gt;</description></item><item><title>&#23618;&#32423;&#22870;&#21169;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22870;&#21169;&#35774;&#35745;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20445;&#35777;&#35825;&#23548;&#20986;&#26681;&#25454;&#20559;&#22909;&#20851;&#31995;&#26159;&#24085;&#32047;&#25176;&#26368;&#20248;&#30340;&#31574;&#30053;&#65292;&#24182;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#23637;&#31034;&#20854;&#24555;&#36895;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2212.03733</link><description>&lt;p&gt;
&#23618;&#32423;&#22870;&#21169;&#20989;&#25968;&#65306;&#35268;&#23450;&#21644;&#24555;&#36895;&#23398;&#20064;&#25152;&#38656;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Tiered Reward Functions: Specifying and Fast Learning of Desired Behavior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.03733
&lt;/p&gt;
&lt;p&gt;
&#23618;&#32423;&#22870;&#21169;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22870;&#21169;&#35774;&#35745;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20445;&#35777;&#35825;&#23548;&#20986;&#26681;&#25454;&#20559;&#22909;&#20851;&#31995;&#26159;&#24085;&#32047;&#25176;&#26368;&#20248;&#30340;&#31574;&#30053;&#65292;&#24182;&#22312;&#22810;&#20010;&#29615;&#22659;&#20013;&#23637;&#31034;&#20854;&#24555;&#36895;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#26368;&#22823;&#21270;&#22870;&#21169;&#20449;&#21495;&#12290;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20316;&#20026;&#20154;&#31867;&#30340;&#20219;&#21153;&#26159;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#34920;&#36798;&#25152;&#26399;&#26395;&#30340;&#34892;&#20026;&#65292;&#24182;&#20351;&#20195;&#29702;&#33021;&#22815;&#36805;&#36895;&#23398;&#20064;&#36825;&#31181;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20219;&#21153;&#20013;&#36798;&#21040;&#33391;&#22909;&#29366;&#24577;&#21644;&#36991;&#20813;&#19981;&#33391;&#29366;&#24577;&#30340;&#22870;&#21169;&#35774;&#35745;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#31574;&#30053;&#31354;&#38388;&#30340;&#37096;&#20998;&#25490;&#24207;&#65292;&#20197;&#35299;&#20915;&#34892;&#20026;&#20559;&#22909;&#20013;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#26356;&#20542;&#21521;&#20110;&#33021;&#26356;&#24555;&#36895;&#22320;&#21040;&#36798;&#33391;&#22909;&#29366;&#24577;&#24182;&#20197;&#26356;&#39640;&#30340;&#27010;&#29575;&#21040;&#36798;&#65292;&#21516;&#26102;&#33021;&#26356;&#38271;&#26102;&#38388;&#22320;&#36991;&#20813;&#19981;&#33391;&#29366;&#24577;&#30340;&#31574;&#30053;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#23618;&#32423;&#22870;&#21169;&#65292;&#19968;&#31867;&#19982;&#29615;&#22659;&#26080;&#20851;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#34920;&#26126;&#23427;&#20445;&#35777;&#35825;&#23548;&#20986;&#26681;&#25454;&#25105;&#20204;&#30340;&#20559;&#22909;&#20851;&#31995;&#26159;&#24085;&#32047;&#25176;&#26368;&#20248;&#30340;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23618;&#32423;&#22870;&#21169;&#21487;&#20197;&#36890;&#36807;&#22312;&#22810;&#20010;&#29615;&#22659;&#19978;&#20351;&#29992;&#22810;&#20010;&#34920;&#26684;&#36827;&#34892;&#35780;&#20272;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.03733v2 Announce Type: replace-cross  Abstract: Reinforcement-learning agents seek to maximize a reward signal through environmental interactions. As humans, our job in the learning process is to design reward functions to express desired behavior and enable the agent to learn such behavior swiftly. In this work, we consider the reward-design problem in tasks formulated as reaching desirable states and avoiding undesirable states. To start, we propose a strict partial ordering of the policy space to resolve trade-offs in behavior preference. We prefer policies that reach the good states faster and with higher probability while avoiding the bad states longer. Next, we introduce Tiered Reward, a class of environment-independent reward functions and show it is guaranteed to induce policies that are Pareto-optimal according to our preference relation. Finally, we demonstrate that Tiered Reward can lead to fast learning by evaluating on several environments using multiple tabular
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#27700;&#24179;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#23545;&#20110;&#35813;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2212.00322</link><description>&lt;p&gt;
&#27700;&#24179;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#21333;&#26041;&#38754;&#21163;&#25345;
&lt;/p&gt;
&lt;p&gt;
Hijack Vertical Federated Learning Models As One Party
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.00322
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#27700;&#24179;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#23545;&#20110;&#35813;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#24179;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#33539;&#24335;&#65292;&#20351;&#21512;&#20316;&#20249;&#20276;&#33021;&#22815;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#20849;&#21516;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#36825;&#20123;&#21442;&#19982;&#26041;&#26377;&#19968;&#32452;&#20849;&#21516;&#29992;&#25143;&#65292;&#20294;&#25317;&#26377;&#19981;&#21516;&#29305;&#24449;&#12290;&#29616;&#26377;&#30340;VFL&#26694;&#26550;&#20351;&#29992;&#23494;&#30721;&#25216;&#26415;&#25552;&#20379;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#20445;&#35777;&#65292;&#23548;&#33268;&#20102;&#19968;&#31995;&#21015;&#30740;&#31350;&#35745;&#31639;&#25928;&#29575;&#21644;&#24555;&#36895;&#23454;&#29616;&#30340;&#24037;&#20316;&#12290;&#28982;&#32780;&#65292;VFL&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.00322v2 Announce Type: replace-cross  Abstract: Vertical federated learning (VFL) is an emerging paradigm that enables collaborators to build machine learning models together in a distributed fashion. In general, these parties have a group of users in common but own different features. Existing VFL frameworks use cryptographic techniques to provide data privacy and security guarantees, leading to a line of works studying computing efficiency and fast implementation. However, the security of VFL's model remains underexplored.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#25193;&#23637;&#37051;&#22495;&#35268;&#21017;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#35268;&#21017;&#30830;&#23450;&#37051;&#23621;&#21644;&#27169;&#22411;&#36873;&#25321;&#31574;&#30053;&#26469;&#35299;&#20915;&#20256;&#32479;$k$&#26368;&#36817;&#37051;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#25552;&#21319;&#38598;&#25104;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2211.11278</link><description>&lt;p&gt;
&#26368;&#20248;&#25193;&#23637;&#37051;&#22495;&#35268;&#21017;$k$&#26368;&#36817;&#37051;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Optimal Extended Neighbourhood Rule $k$ Nearest Neighbours Ensemble
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.11278
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#25193;&#23637;&#37051;&#22495;&#35268;&#21017;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#35268;&#21017;&#30830;&#23450;&#37051;&#23621;&#21644;&#27169;&#22411;&#36873;&#25321;&#31574;&#30053;&#26469;&#35299;&#20915;&#20256;&#32479;$k$&#26368;&#36817;&#37051;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#25552;&#21319;&#38598;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;$k$&#26368;&#36817;&#37051;($k$NN)&#26041;&#27861;&#20351;&#29992;&#19968;&#20010;&#29699;&#24418;&#21306;&#22495;&#20869;&#30340;&#36317;&#31163;&#20844;&#24335;&#26469;&#30830;&#23450;&#35757;&#32451;&#35266;&#27979;&#20013;&#19982;&#27979;&#35797;&#26679;&#26412;&#28857;&#26368;&#25509;&#36817;&#30340;$k$&#20010;&#35266;&#27979;&#12290;&#28982;&#32780;&#65292;&#24403;&#27979;&#35797;&#28857;&#20301;&#20110;&#35813;&#21306;&#22495;&#20043;&#22806;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#19981;&#36215;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#32858;&#21512;&#35768;&#22810;&#22522;&#30784;$k$NN&#23398;&#20064;&#22120;&#21487;&#33021;&#20250;&#23548;&#33268;&#30001;&#20110;&#39640;&#20998;&#31867;&#35823;&#24046;&#32780;&#34920;&#29616;&#19981;&#20339;&#30340;&#38598;&#25104;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26368;&#20248;&#25193;&#23637;&#37051;&#22495;&#35268;&#21017;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#35813;&#35268;&#21017;&#20174;&#36317;&#31163;&#26410;&#35265;&#35266;&#27979;&#26368;&#36817;&#30340;&#26679;&#26412;&#28857;&#24320;&#22987;&#65292;&#32463;&#36807;$k$&#27493;&#30830;&#23450;&#37051;&#23621;&#65292;&#24182;&#36873;&#25321;&#30452;&#21040;&#36798;&#21040;&#25152;&#38656;&#25968;&#37327;&#30340;&#35266;&#27979;&#25968;&#25454;&#28857;&#12290;&#27599;&#20010;&#22522;&#30784;&#27169;&#22411;&#37117;&#26159;&#22312;&#19968;&#20010;&#38543;&#26426;&#29305;&#24449;&#23376;&#38598;&#19978;&#30340;&#33258;&#20030;&#26679;&#26412;&#19978;&#26500;&#24314;&#30340;&#65292;&#24182;&#19988;&#22312;&#26500;&#24314;&#36275;&#22815;&#25968;&#37327;&#30340;&#27169;&#22411;&#21518;&#22522;&#20110;&#34955;&#22806;&#34920;&#29616;&#36873;&#25321;&#26368;&#20248;&#27169;&#22411;&#12290;&#25552;&#20986;&#30340;&#38598;&#25104;&#26041;&#27861;&#19982;st&#36827;&#34892;&#20102;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.11278v2 Announce Type: replace-cross  Abstract: The traditional k nearest neighbor (kNN) approach uses a distance formula within a spherical region to determine the k closest training observations to a test sample point. However, this approach may not work well when test point is located outside this region. Moreover, aggregating many base kNN learners can result in poor ensemble performance due to high classification errors. To address these issues, a new optimal extended neighborhood rule based ensemble method is proposed in this paper. This rule determines neighbors in k steps starting from the closest sample point to the unseen observation and selecting subsequent nearest data points until the required number of observations is reached. Each base model is constructed on a bootstrap sample with a random subset of features, and optimal models are selected based on out-of-bag performance after building a sufficient number of models. The proposed ensemble is compared with st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#22810;&#20010;&#29615;&#22659;&#19979;&#23558;&#29289;&#29702;&#23618;&#31192;&#38053;&#29983;&#25104;&#38382;&#39064;&#36716;&#21270;&#20026;&#23398;&#20064;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#20256;&#36882;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#30340;&#20449;&#36947;&#29305;&#24449;&#26144;&#23556;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#26032;&#29615;&#22659;&#20013;&#24555;&#36895;&#26377;&#25928;&#22320;&#29983;&#25104;&#31192;&#38053;&#12290;</title><link>https://arxiv.org/abs/2211.03065</link><description>&lt;p&gt;
&#22312;&#22810;&#29615;&#22659;&#19979;&#20026;FDD-OFDM&#31995;&#32479;&#23454;&#29616;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29289;&#29702;&#23618;&#31192;&#38053;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enabling Deep Learning-based Physical-layer Secret Key Generation for FDD-OFDM Systems in Multi-Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.03065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22810;&#20010;&#29615;&#22659;&#19979;&#23558;&#29289;&#29702;&#23618;&#31192;&#38053;&#29983;&#25104;&#38382;&#39064;&#36716;&#21270;&#20026;&#23398;&#20064;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#20256;&#36882;&#23398;&#20064;&#21644;&#20803;&#23398;&#20064;&#30340;&#20449;&#36947;&#29305;&#24449;&#26144;&#23556;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#26032;&#29615;&#22659;&#20013;&#24555;&#36895;&#26377;&#25928;&#22320;&#29983;&#25104;&#31192;&#38053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29289;&#29702;&#23618;&#31192;&#38053;&#29983;&#25104;&#65288;PKG&#65289;&#34987;&#29992;&#20110;&#20811;&#26381;&#39057;&#20998;&#21452;&#24037;&#65288;FDD&#65289;&#27491;&#20132;&#39057;&#20998;&#22797;&#29992;&#65288;OFDM&#65289;&#31995;&#32479;&#20013;&#19981;&#23436;&#32654;&#30340;&#19978;&#19979;&#34892;&#20449;&#36947;&#20114;&#26131;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#38024;&#23545;&#29305;&#23450;&#29615;&#22659;&#20013;&#29992;&#25143;&#30340;&#31192;&#38053;&#29983;&#25104;&#65292;&#20854;&#20013;&#35757;&#32451;&#26679;&#26412;&#21644;&#27979;&#35797;&#26679;&#26412;&#36981;&#24490;&#30456;&#21516;&#30340;&#20998;&#24067;&#65292;&#36825;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26412;&#25991;&#23558;&#22810;&#29615;&#22659;&#19979;&#30340;PKG&#38382;&#39064;&#34920;&#36848;&#20026;&#19968;&#20010;&#23398;&#20064;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#20174;&#24050;&#30693;&#29615;&#22659;&#20013;&#23398;&#20064;&#25968;&#25454;&#21644;&#27169;&#22411;&#31561;&#30693;&#35782;&#65292;&#24555;&#36895;&#39640;&#25928;&#22320;&#22312;&#22810;&#20010;&#26032;&#29615;&#22659;&#20013;&#29983;&#25104;&#31192;&#38053;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#20256;&#36882;&#23398;&#20064;&#65288;DTL&#65289;&#21644;&#20803;&#23398;&#20064;&#30340;&#20449;&#36947;&#29305;&#24449;&#26144;&#23556;&#31639;&#27861;&#29992;&#20110;&#31192;&#38053;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.03065v2 Announce Type: replace-cross  Abstract: Deep learning-based physical-layer secret key generation (PKG) has been used to overcome the imperfect uplink/downlink channel reciprocity in frequency division duplexing (FDD) orthogonal frequency division multiplexing (OFDM) systems. However, existing efforts have focused on key generation for users in a specific environment where the training samples and test samples follow the same distribution, which is unrealistic for real-world applications. This paper formulates the PKG problem in multiple environments as a learning-based problem by learning the knowledge such as data and models from known environments to generate keys quickly and efficiently in multiple new environments. Specifically, we propose deep transfer learning (DTL) and meta-learning-based channel feature mapping algorithms for key generation. The two algorithms use different training methods to pre-train the model in the known environments, and then quickly ad
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#35013;&#39280;&#21644;Fourier&#20998;&#21306;&#30340;&#26032;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25945;&#25480;&#32593;&#32476;&#21508;&#31181;&#20114;&#34917;&#29305;&#24449;&#65292;&#20943;&#23569;&#22522;&#20110;&#25200;&#21160;&#30340;&#24858;&#24324;&#30340;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2207.09031</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#24515;&#30005;&#22270;&#20998;&#31867;&#35013;&#39280;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Decorrelative Network Architecture for Robust Electrocardiogram Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.09031
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#35013;&#39280;&#21644;Fourier&#20998;&#21306;&#30340;&#26032;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25945;&#25480;&#32593;&#32476;&#21508;&#31181;&#20114;&#34917;&#29305;&#24449;&#65292;&#20943;&#23569;&#22522;&#20110;&#25200;&#21160;&#30340;&#24858;&#24324;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#23398;&#25968;&#25454;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#21487;&#20449;&#24230;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#23578;&#26410;&#34987;&#24191;&#27867;&#37096;&#32626;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#35013;&#39280;&#21644;Fourier&#20998;&#21306;&#30340;&#26032;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25945;&#25480;&#32593;&#32476;&#21508;&#31181;&#20114;&#34917;&#29305;&#24449;&#65292;&#20943;&#23569;&#22522;&#20110;&#25200;&#21160;&#30340;&#24858;&#24324;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#22312;&#21333;&#36890;&#36947;&#21644;&#22810;&#36890;&#36947;&#24515;&#30005;&#22270;&#20998;&#31867;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#23545;&#25239;&#35757;&#32451;&#21644;DVERGE&#35843;&#25972;&#20026;&#36125;&#21494;&#26031;&#38598;&#25104;&#26694;&#26550;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.09031v4 Announce Type: replace-cross  Abstract: Artificial intelligence has made great progress in medical data analysis, but the lack of robustness and trustworthiness has kept these methods from being widely deployed. As it is not possible to train networks that are accurate in all scenarios, models must recognize situations where they cannot operate confidently. Bayesian deep learning methods sample the model parameter space to estimate uncertainty, but these parameters are often subject to the same vulnerabilities, which can be exploited by adversarial attacks. We propose a novel ensemble approach based on feature decorrelation and Fourier partitioning for teaching networks diverse complementary features, reducing the chance of perturbation-based fooling. We test our approach on single and multi-channel electrocardiogram classification, and adapt adversarial training and DVERGE into the Bayesian ensemble framework for comparison. Our results indicate that the combination
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;&#26465;&#20214;&#30697;&#38480;&#21046;&#38382;&#39064;&#30340;&#21151;&#33021;&#21270;&#24191;&#20041;&#32463;&#39564;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#20854;&#22312;&#23567;&#26679;&#26412;&#24615;&#33021;&#19978;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#22522;&#20110;&#26680;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#29616;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2207.04771</link><description>&lt;p&gt;
&#26465;&#20214;&#30697;&#38480;&#21046;&#30340;&#21151;&#33021;&#21270;&#24191;&#20041;&#32463;&#39564;&#20284;&#28982;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Functional Generalized Empirical Likelihood Estimation for Conditional Moment Restrictions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.04771
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;&#26465;&#20214;&#30697;&#38480;&#21046;&#38382;&#39064;&#30340;&#21151;&#33021;&#21270;&#24191;&#20041;&#32463;&#39564;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#20854;&#22312;&#23567;&#26679;&#26412;&#24615;&#33021;&#19978;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#22522;&#20110;&#26680;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#29616;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22240;&#26524;&#25512;&#26029;&#12289;&#32463;&#27982;&#23398;&#20064;&#20197;&#21450;&#26356;&#19968;&#33324;&#30340;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#37325;&#35201;&#38382;&#39064;&#21487;&#20197;&#34987;&#34920;&#36798;&#20026;&#26465;&#20214;&#30697;&#38480;&#21046;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#35299;&#20915;&#19968;&#31995;&#21015;&#30340;&#26080;&#26465;&#20214;&#30697;&#38480;&#21046;&#65292;&#20272;&#35745;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#23558;&#24191;&#20041;&#30697;&#27861;&#65288;GMM&#65289;&#25193;&#23637;&#21040;&#36830;&#32493;&#30697;&#38480;&#21046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#24191;&#20041;&#32463;&#39564;&#20284;&#28982;&#65288;GEL&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#19968;&#33324;&#30340;&#26694;&#26550;&#65292;&#24182;&#19988;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#23567;&#26679;&#26412;&#24615;&#33021;&#19978;&#20248;&#20110;&#22522;&#20110;GMM&#30340;&#20272;&#35745;&#37327;&#12290;&#20026;&#20102;&#20174;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#21457;&#23637;&#20013;&#21463;&#30410;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;GEL&#30340;&#21151;&#33021;&#37325;&#26500;&#65292;&#20854;&#20013;&#21487;&#20197;&#21033;&#29992;&#20219;&#24847;&#27169;&#22411;&#12290;&#21463;&#32467;&#26524;&#30340;&#26080;&#38480;&#32500;&#20248;&#21270;&#38382;&#39064;&#30340;&#23545;&#20598;&#24418;&#24335;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23454;&#29992;&#26041;&#27861;&#24182;&#25506;&#35752;&#20102;&#20854;&#28176;&#36817;&#24615;&#36136;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#26680;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#29616;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2207.04771v2 Announce Type: replace  Abstract: Important problems in causal inference, economics, and, more generally, robust machine learning can be expressed as conditional moment restrictions, but estimation becomes challenging as it requires solving a continuum of unconditional moment restrictions. Previous works addressed this problem by extending the generalized method of moments (GMM) to continuum moment restrictions. In contrast, generalized empirical likelihood (GEL) provides a more general framework and has been shown to enjoy favorable small-sample properties compared to GMM-based estimators. To benefit from recent developments in machine learning, we provide a functional reformulation of GEL in which arbitrary models can be leveraged. Motivated by a dual formulation of the resulting infinite dimensional optimization problem, we devise a practical method and explore its asymptotic properties. Finally, we provide kernel- and neural network-based implementations of the e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#22240;&#26524;&#35780;&#20998;&#20316;&#20026;&#19968;&#31181;&#26032;&#22411;&#26041;&#27861;&#65292;&#25903;&#25345;&#20915;&#31574;&#21046;&#23450;&#65292;&#25552;&#20379;&#27934;&#23519;&#21147;&#65292;&#24182;&#21487;&#29992;&#20110;&#25928;&#24212;&#20272;&#35745;&#12289;&#25928;&#24212;&#25490;&#24207;&#21644;&#25928;&#24212;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2206.12532</link><description>&lt;p&gt;
&#22240;&#26524;&#35780;&#20998;&#65306;&#25928;&#24212;&#20272;&#35745;&#12289;&#25928;&#24212;&#25490;&#24207;&#21644;&#25928;&#24212;&#20998;&#31867;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Causal Scoring: A Framework for Effect Estimation, Effect Ordering, and Effect Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.12532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#22240;&#26524;&#35780;&#20998;&#20316;&#20026;&#19968;&#31181;&#26032;&#22411;&#26041;&#27861;&#65292;&#25903;&#25345;&#20915;&#31574;&#21046;&#23450;&#65292;&#25552;&#20379;&#27934;&#23519;&#21147;&#65292;&#24182;&#21487;&#29992;&#20110;&#25928;&#24212;&#20272;&#35745;&#12289;&#25928;&#24212;&#25490;&#24207;&#21644;&#25928;&#24212;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22240;&#26524;&#35780;&#20998;&#24341;&#20837;&#21040;&#20915;&#31574;&#21046;&#23450;&#30340;&#32972;&#26223;&#20013;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#28041;&#21450;&#20272;&#35745;&#25903;&#25345;&#20915;&#31574;&#21046;&#23450;&#30340;&#24471;&#20998;&#65292;&#20174;&#32780;&#25552;&#20379;&#22240;&#26524;&#25928;&#24212;&#30340;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#20123;&#35780;&#20998;&#30340;&#19977;&#31181;&#26377;&#20215;&#20540;&#30340;&#22240;&#26524;&#35299;&#37322;&#65306;&#25928;&#24212;&#20272;&#35745;&#65288;EE&#65289;&#12289;&#25928;&#24212;&#25490;&#24207;&#65288;EO&#65289;&#21644;&#25928;&#24212;&#20998;&#31867;&#65288;EC&#65289;&#12290;&#22312;EE&#35299;&#37322;&#20013;&#65292;&#22240;&#26524;&#35780;&#20998;&#20195;&#34920;&#20102;&#25928;&#24212;&#26412;&#36523;&#12290;EO&#35299;&#37322;&#26263;&#31034;&#35780;&#20998;&#21487;&#20197;&#20316;&#20026;&#25928;&#24212;&#22823;&#23567;&#30340;&#20195;&#29702;&#65292;&#21487;&#20197;&#26681;&#25454;&#20854;&#22240;&#26524;&#25928;&#24212;&#23545;&#20010;&#20307;&#36827;&#34892;&#25490;&#24207;&#12290;EC&#35299;&#37322;&#36890;&#36807;&#39044;&#23450;&#20041;&#30340;&#38408;&#20540;&#65292;&#20351;&#20010;&#20307;&#20998;&#20026;&#39640;&#25928;&#24212;&#21644;&#20302;&#25928;&#24212;&#31867;&#21035;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#20851;&#38190;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#20123;&#26367;&#20195;&#22240;&#26524;&#35299;&#37322;&#65288;EO&#21644;EC&#65289;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.12532v4 Announce Type: replace-cross  Abstract: This paper introduces causal scoring as a novel approach to frame causal estimation in the context of decision making. Causal scoring entails the estimation of scores that support decision making by providing insights into causal effects. We present three valuable causal interpretations of these scores: effect estimation (EE), effect ordering (EO), and effect classification (EC). In the EE interpretation, the causal score represents the effect itself. The EO interpretation implies that the score can serve as a proxy for the magnitude of the effect, enabling the sorting of individuals based on their causal effects. The EC interpretation enables the classification of individuals into high- and low-effect categories using a predefined threshold. We demonstrate the value of these alternative causal interpretations (EO and EC) through two key results. First, we show that aligning the statistical modeling with the desired causal inte
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#22810;&#33218;&#36172;&#21338;&#26426;&#35774;&#32622;&#65292;&#25429;&#25417;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#31163;&#24320;&#30340;&#24773;&#20917;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#22312;&#25152;&#26377;&#29992;&#25143;&#20849;&#20139;&#30456;&#21516;&#31867;&#22411;&#26102;&#65292;&#22522;&#20110;UCB&#30340;&#31639;&#27861;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>https://arxiv.org/abs/2203.13423</link><description>&lt;p&gt;
&#29992;&#31163;&#24320;&#30340;&#36172;&#21338;&#26426;&#27169;&#22411;&#24314;&#35758;&#31995;&#32479;&#20013;&#30340;&#27969;&#22833;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Modeling Attrition in Recommender Systems with Departing Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2203.13423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#22810;&#33218;&#36172;&#21338;&#26426;&#35774;&#32622;&#65292;&#25429;&#25417;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#31163;&#24320;&#30340;&#24773;&#20917;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#22312;&#25152;&#26377;&#29992;&#25143;&#20849;&#20139;&#30456;&#21516;&#31867;&#22411;&#26102;&#65292;&#22522;&#20110;UCB&#30340;&#31639;&#27861;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#65292;&#25512;&#33616;&#31995;&#32479;&#30340;&#31574;&#30053;&#24433;&#21709;&#22870;&#21169;&#30340;&#33719;&#21462;&#65292;&#20294;&#19981;&#24433;&#21709;&#20132;&#20114;&#30340;&#38271;&#24230;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#19981;&#28385;&#36275;&#30340;&#29992;&#25143;&#21487;&#33021;&#20250;&#31163;&#24320;&#65288;&#24182;&#27704;&#36828;&#19981;&#20877;&#22238;&#26469;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25429;&#25417;&#36825;&#31181;&#31574;&#30053;&#20381;&#36182;&#24615;&#26102;&#27573;&#30340;&#26032;&#22411;&#22810;&#33218;&#36172;&#21338;&#26426;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#35774;&#32622;&#21253;&#25324;&#19968;&#20010;&#26377;&#38480;&#30340;&#29992;&#25143;&#31867;&#22411;&#38598;&#21512;&#65292;&#21644;&#22810;&#20010;&#20855;&#26377;&#20271;&#21162;&#21033;&#22238;&#25253;&#30340;&#33218;&#12290;&#27599;&#20010;&#65288;&#29992;&#25143;&#31867;&#22411;&#65292;&#33218;&#65289;&#20803;&#32452;&#23545;&#24212;&#19968;&#20010;&#65288;&#26410;&#30693;&#30340;&#65289;&#22870;&#21169;&#27010;&#29575;&#12290;&#27599;&#20010;&#29992;&#25143;&#30340;&#31867;&#22411;&#26368;&#21021;&#26159;&#26410;&#30693;&#30340;&#65292;&#21482;&#33021;&#36890;&#36807;&#20854;&#23545;&#25512;&#33616;&#30340;&#21709;&#24212;&#26469;&#25512;&#26029;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#29992;&#25143;&#23545;&#20182;&#20204;&#30340;&#25512;&#33616;&#19981;&#28385;&#24847;&#65292;&#20182;&#20204;&#21487;&#33021;&#20250;&#31163;&#24320;&#31995;&#32479;&#12290;&#25105;&#20204;&#39318;&#20808;&#35299;&#20915;&#20102;&#25152;&#26377;&#29992;&#25143;&#20849;&#20139;&#30456;&#21516;&#31867;&#22411;&#30340;&#24773;&#20917;&#65292;&#35777;&#26126;&#20102;&#26368;&#36817;&#22522;&#20110;UCB&#30340;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36716;&#21521;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#65292;&#21363;&#29992;&#25143;&#20998;&#20026;&#20004;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2203.13423v2 Announce Type: replace  Abstract: Traditionally, when recommender systems are formalized as multi-armed bandits, the policy of the recommender system influences the rewards accrued, but not the length of interaction. However, in real-world systems, dissatisfied users may depart (and never come back). In this work, we propose a novel multi-armed bandit setup that captures such policy-dependent horizons. Our setup consists of a finite set of user types, and multiple arms with Bernoulli payoffs. Each (user type, arm) tuple corresponds to an (unknown) reward probability. Each user's type is initially unknown and can only be inferred through their response to recommendations. Moreover, if a user is dissatisfied with their recommendation, they might depart the system. We first address the case where all users share the same type, demonstrating that a recent UCB-based algorithm is optimal. We then move forward to the more challenging case, where users are divided among two 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Fr&#233;chet&#38543;&#26426;&#26862;&#26519;&#65292;&#20801;&#35768;&#22788;&#29702;&#20540;&#22312;&#19968;&#33324;&#24230;&#37327;&#31354;&#38388;&#30340;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#26641;&#33410;&#28857;&#20998;&#35010;&#26041;&#24335;&#65292;&#25193;&#23637;&#20102;&#39044;&#27979;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#33268;&#24615;&#23450;&#29702;&#65292;&#24182;&#36866;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#20998;&#21306;&#30340;Fr&#233;chet&#32431;&#19968;&#33268;&#38543;&#26426;&#26641;</title><link>https://arxiv.org/abs/1906.01741</link><description>&lt;p&gt;
&#38024;&#23545;&#20855;&#26377;&#38750;&#27431;&#20960;&#37324;&#24503;&#39044;&#27979;&#21464;&#37327;&#30340;&#24230;&#37327;&#31354;&#38388;&#22238;&#24402;&#38382;&#39064;&#30340;Fr&#233;chet&#38543;&#26426;&#26862;&#26519;
&lt;/p&gt;
&lt;p&gt;
Fr\'echet random forests for metric space valued regression with non euclidean predictors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1906.01741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Fr&#233;chet&#38543;&#26426;&#26862;&#26519;&#65292;&#20801;&#35768;&#22788;&#29702;&#20540;&#22312;&#19968;&#33324;&#24230;&#37327;&#31354;&#38388;&#30340;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#26641;&#33410;&#28857;&#20998;&#35010;&#26041;&#24335;&#65292;&#25193;&#23637;&#20102;&#39044;&#27979;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#33268;&#24615;&#23450;&#29702;&#65292;&#24182;&#36866;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#20998;&#21306;&#30340;Fr&#233;chet&#32431;&#19968;&#33268;&#38543;&#26426;&#26641;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#31185;&#23398;&#30740;&#31350;&#39046;&#22495;&#30340;&#32479;&#35745;&#23398;&#20064;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#23398;&#20064;&#36755;&#20837;&#21644;&#36755;&#20986;&#21464;&#37327;&#20043;&#38388;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#21516;&#26102;&#20063;&#33021;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#38543;&#26426;&#26862;&#26519;&#26041;&#27861;&#23545;&#22788;&#29702;&#26354;&#32447;&#12289;&#22270;&#20687;&#21644;&#24418;&#29366;&#31561;&#24322;&#36136;&#25968;&#25454;&#30340;&#28789;&#27963;&#24615;&#19981;&#22815;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Fr&#233;chet&#26641;&#21644;Fr&#233;chet&#38543;&#26426;&#26862;&#26519;&#65292;&#20801;&#35768;&#22788;&#29702;&#36755;&#20837;&#21644;&#36755;&#20986;&#21464;&#37327;&#21462;&#20540;&#22312;&#19968;&#33324;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26641;&#33410;&#28857;&#20998;&#35010;&#26041;&#24335;&#65292;&#24182;&#24191;&#20041;&#21270;&#20102;&#26641;&#21644;&#26862;&#26519;&#30340;&#39044;&#27979;&#36807;&#31243;&#12290;&#38543;&#26426;&#26862;&#26519;&#30340;&#34955;&#22806;&#35823;&#24046;&#21644;&#21464;&#37327;&#37325;&#35201;&#24615;&#24471;&#20998;&#33258;&#28982;&#24471;&#21040;&#20102;&#35843;&#25972;&#12290;&#32473;&#20986;&#20102;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#20998;&#21306;&#30340;Fr&#233;chet&#22238;&#24402;&#22270;&#39044;&#27979;&#22120;&#30340;&#19968;&#33268;&#24615;&#23450;&#29702;&#65292;&#24182;&#24212;&#29992;&#20110;Fr&#233;chet&#32431;&#19968;&#33268;&#38543;&#26426;&#26641;&#12290;&#35813;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:1906.01741v3 Announce Type: replace-cross  Abstract: Random forests are a statistical learning method widely used in many areas of scientific research because of its ability to learn complex relationships between input and output variables and also its capacity to handle high-dimensional data. However, current random forest approaches are not flexible enough to handle heterogeneous data such as curves, images and shapes. In this paper, we introduce Fr\'echet trees and Fr\'echet random forests, which allow to handle data for which input and output variables take values in general metric spaces. To this end, a new way of splitting the nodes of trees is introduced and the prediction procedures of trees and forests are generalized. Then, random forests out-of-bag error and variable importance score are naturally adapted. A consistency theorem for Fr\'echet regressogram predictor using data-driven partitions is given and applied to Fr\'echet purely uniformly random trees. The method i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28201;&#21644;&#30340;&#35268;&#33539;&#25191;&#34892;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20132;&#27969;&#25512;&#21160;&#21512;&#20316;&#24182;&#20419;&#36827;&#35268;&#33539;&#30340;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.16461</link><description>&lt;p&gt;
&#28201;&#21644;&#30340;&#35268;&#33539;&#25191;&#34892;&#65306;&#26356;&#24555;&#30340;&#20986;&#29616;&#65292;&#26356;&#24555;&#20048;&#30340;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Norm Enforcement with a Soft Touch: Faster Emergence, Happier Agents. (arXiv:2401.16461v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16461
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28201;&#21644;&#30340;&#35268;&#33539;&#25191;&#34892;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20132;&#27969;&#25512;&#21160;&#21512;&#20316;&#24182;&#20419;&#36827;&#35268;&#33539;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21487;&#35270;&#20026;&#19968;&#20010;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#31038;&#20250;&#65292;&#36890;&#36807;&#31038;&#20250;&#35268;&#33539;&#21487;&#20197;&#26377;&#25928;&#22320;&#35843;&#25511;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#19968;&#20010;&#31038;&#20250;&#30340;&#35268;&#33539;&#24182;&#19981;&#26159;&#30828;&#32534;&#30721;&#30340;&#65292;&#32780;&#26159;&#20174;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#20013;&#20135;&#29983;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#19968;&#20010;&#31038;&#20250;&#20013;&#30340;&#26234;&#33021;&#20307;&#23545;&#21478;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#20316;&#20986;&#30340;&#21453;&#24212;&#20197;&#21450;&#23545;&#20182;&#20154;&#21453;&#24212;&#30340;&#22238;&#24212;&#65292;&#20915;&#23450;&#20102;&#31038;&#20250;&#20013;&#20986;&#29616;&#21738;&#20123;&#35268;&#33539;&#12290;&#25105;&#20204;&#23558;&#19968;&#20010;&#26234;&#33021;&#20307;&#23545;&#21478;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#28385;&#24847;&#25110;&#19981;&#28385;&#24847;&#34892;&#20026;&#30340;&#21453;&#24212;&#35270;&#20026;&#31532;&#19968;&#20010;&#26234;&#33021;&#20307;&#21521;&#31532;&#20108;&#20010;&#26234;&#33021;&#20307;&#30340;&#20132;&#27969;&#12290;&#29702;&#35299;&#36825;&#20123;&#20132;&#27969;&#26159;&#19968;&#31181;&#31038;&#20250;&#26234;&#33021;&#65306;&#36825;&#20123;&#20132;&#27969;&#36890;&#36807;&#25512;&#21160;&#26234;&#33021;&#20307;&#26397;&#30528;&#26576;&#20123;&#34892;&#20026;&#36827;&#34892;&#65292;&#20174;&#32780;&#20419;&#36827;&#35268;&#33539;&#30340;&#20986;&#29616;&#12290;&#34429;&#28982;&#20247;&#25152;&#21608;&#30693;&#24809;&#32602;&#21487;&#20197;&#23548;&#33268;&#35268;&#33539;&#30340;&#20986;&#29616;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#26356;&#23485;&#27867;&#30340;&#31038;&#20250;&#26234;&#33021;&#21487;&#33021;&#22312;&#20419;&#36827;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#21512;&#20316;&#26041;&#38754;&#26356;&#26377;&#25928;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;Ne&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A multiagent system can be viewed as a society of autonomous agents, whose interactions can be effectively regulated via social norms. In general, the norms of a society are not hardcoded but emerge from the agents' interactions. Specifically, how the agents in a society react to each other's behavior and respond to the reactions of others determines which norms emerge in the society. We think of these reactions by an agent to the satisfactory or unsatisfactory behaviors of another agent as communications from the first agent to the second agent. Understanding these communications is a kind of social intelligence: these communications provide natural drivers for norm emergence by pushing agents toward certain behaviors, which can become established as norms. Whereas it is well-known that sanctioning can lead to the emergence of norms, we posit that a broader kind of social intelligence can prove more effective in promoting cooperation in a multiagent system.  Accordingly, we develop Ne
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25299;&#25169;&#24863;&#30693;&#23884;&#20837;&#35760;&#24518;&#30340;&#23398;&#20064;&#25193;&#23637;&#22270;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#35299;&#20915;&#22312;&#19981;&#26029;&#25193;&#23637;&#30340;&#22270;&#19978;&#24212;&#29992;&#35760;&#24518;&#22238;&#25918;&#25216;&#26415;&#23548;&#33268;&#30340;&#20869;&#23384;&#29190;&#28856;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13200</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#24863;&#30693;&#23884;&#20837;&#35760;&#24518;&#30340;&#23398;&#20064;&#25193;&#23637;&#22270;
&lt;/p&gt;
&lt;p&gt;
Topology-aware Embedding Memory for Learning on Expanding Graphs. (arXiv:2401.13200v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13200
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25299;&#25169;&#24863;&#30693;&#23884;&#20837;&#35760;&#24518;&#30340;&#23398;&#20064;&#25193;&#23637;&#22270;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#35299;&#20915;&#22312;&#19981;&#26029;&#25193;&#23637;&#30340;&#22270;&#19978;&#24212;&#29992;&#35760;&#24518;&#22238;&#25918;&#25216;&#26415;&#23548;&#33268;&#30340;&#20869;&#23384;&#29190;&#28856;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35760;&#24518;&#22238;&#25918;&#30340;&#25216;&#26415;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#20294;&#26159;&#30452;&#25509;&#24212;&#29992;&#20110;&#19981;&#26029;&#25193;&#23637;&#30340;&#22270;&#20250;&#23548;&#33268;&#28508;&#22312;&#30340;&#20869;&#23384;&#29190;&#28856;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#31995;&#32479;&#20998;&#26512;&#20102;&#20869;&#23384;&#29190;&#28856;&#38382;&#39064;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21363;Parameter Decoupled Graph Neural Networks (PDGNNs) with Topology-aware Embedding Memory (TEM)&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#23558;&#20869;&#23384;&#31354;&#38388;&#22797;&#26434;&#24230;&#20174;$\mathcal{O}(nd^L)$&#38477;&#20302;&#21040;$\mathcal{O}(n)$&#65292;&#36824;&#20805;&#20998;&#21033;&#29992;&#20102;&#25299;&#25169;&#20449;&#24687;&#36827;&#34892;&#35760;&#24518;&#22238;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory replay based techniques have shown great success for continual learning with incrementally accumulated Euclidean data. Directly applying them to continually expanding graphs, however, leads to the potential memory explosion problem due to the need to buffer representative nodes and their associated topological neighborhood structures. To this end, we systematically analyze the key challenges in the memory explosion problem, and present a general framework, i.e., Parameter Decoupled Graph Neural Networks (PDGNNs) with Topology-aware Embedding Memory (TEM), to tackle this issue. The proposed framework not only reduces the memory space complexity from $\mathcal{O}(nd^L)$ to $\mathcal{O}(n)$~\footnote{$n$: memory budget, $d$: average node degree, $L$: the radius of the GNN receptive field}, but also fully utilizes the topological information for memory replay. Specifically, PDGNNs decouple trainable parameters from the computation ego-subgraph via \textit{Topology-aware Embeddings} 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#29366;&#24577;&#21644;&#29366;&#24577;&#36716;&#25442;&#23545;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#22312;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#36873;&#25321;&#19981;&#20165;&#20855;&#26377;&#36739;&#22823;&#21560;&#24341;&#27010;&#29575;&#30340;&#39033;&#30446;&#65292;&#36824;&#35201;&#36873;&#25321;&#33021;&#22815;&#23548;&#33268;&#33391;&#22909;&#21518;&#32487;&#29366;&#24577;&#30340;&#39033;&#30446;&#12290;</title><link>http://arxiv.org/abs/2401.08961</link><description>&lt;p&gt;
&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cascading Reinforcement Learning. (arXiv:2401.08961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#29366;&#24577;&#21644;&#29366;&#24577;&#36716;&#25442;&#23545;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#22312;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#36873;&#25321;&#19981;&#20165;&#20855;&#26377;&#36739;&#22823;&#21560;&#24341;&#27010;&#29575;&#30340;&#39033;&#30446;&#65292;&#36824;&#35201;&#36873;&#25321;&#33021;&#22815;&#23548;&#33268;&#33391;&#22909;&#21518;&#32487;&#29366;&#24577;&#30340;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#32423;&#32852;&#36172;&#21338;&#26426;&#22312;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#20013;&#24212;&#29992;&#24191;&#27867;&#12290;&#22312;&#32423;&#32852;&#36172;&#21338;&#26426;&#27169;&#22411;&#20013;&#65292;&#27599;&#20010;&#26102;&#21051;&#65292;&#19968;&#20010;&#20195;&#29702;&#20154;&#20174;&#19968;&#32452;&#20855;&#26377;&#26410;&#30693;&#21560;&#24341;&#27010;&#29575;&#30340;&#39033;&#30446;&#20013;&#25512;&#33616;&#19968;&#20010;&#26377;&#24207;&#30340;&#39033;&#30446;&#23376;&#38598;&#65288;&#31216;&#20026;&#39033;&#30446;&#21015;&#34920;&#65289;&#12290;&#28982;&#21518;&#65292;&#29992;&#25143;&#26816;&#26597;&#21015;&#34920;&#65292;&#24182;&#28857;&#20987;&#31532;&#19968;&#20010;&#26377;&#21560;&#24341;&#21147;&#30340;&#39033;&#30446;&#65288;&#22914;&#26524;&#26377;&#30340;&#35805;&#65289;&#65292;&#20043;&#21518;&#65292;&#20195;&#29702;&#25910;&#21040;&#19968;&#20010;&#22870;&#21169;&#12290;&#20195;&#29702;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#39044;&#26399;&#30340;&#32047;&#31215;&#22870;&#21169;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#32423;&#32852;&#36172;&#21338;&#26426;&#25991;&#29486;&#24573;&#30053;&#20102;&#29992;&#25143;&#29366;&#24577;&#65288;&#20363;&#22914;&#21382;&#21490;&#34892;&#20026;&#65289;&#23545;&#25512;&#33616;&#30340;&#24433;&#21709;&#20197;&#21450;&#20250;&#35805;&#36827;&#34892;&#36807;&#31243;&#20013;&#29366;&#24577;&#30340;&#21464;&#21270;&#12290;&#21463;&#27492;&#20107;&#23454;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#29366;&#24577;&#21644;&#29366;&#24577;&#36716;&#25442;&#23545;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#22312;&#32423;&#32852;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#36873;&#25321;&#19981;&#20165;&#20855;&#26377;&#36739;&#22823;&#21560;&#24341;&#27010;&#29575;&#30340;&#39033;&#30446;&#65292;&#36824;&#35201;&#36873;&#25321;&#33021;&#22815;&#23548;&#33268;&#33391;&#22909;&#21518;&#32487;&#29366;&#24577;&#30340;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cascading bandits have gained popularity in recent years due to their applicability to recommendation systems and online advertising. In the cascading bandit model, at each timestep, an agent recommends an ordered subset of items (called an item list) from a pool of items, each associated with an unknown attraction probability. Then, the user examines the list, and clicks the first attractive item (if any), and after that, the agent receives a reward. The goal of the agent is to maximize the expected cumulative reward. However, the prior literature on cascading bandits ignores the influences of user states (e.g., historical behaviors) on recommendations and the change of states as the session proceeds. Motivated by this fact, we propose a generalized cascading RL framework, which considers the impact of user states and state transition into decisions. In cascading RL, we need to select items not only with large attraction probabilities but also leading to good successor states. This im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PPR&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#36530;&#36991;&#25915;&#20987;&#30340;&#24615;&#33021;&#21516;&#26102;&#36991;&#20813;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#30340;&#38477;&#32423;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#20462;&#21098;&#65292;&#24182;&#36890;&#36807;&#23884;&#20837;&#23545;&#25239;&#25200;&#21160;&#26469;&#22686;&#24378;&#23545;&#25239;&#20154;&#33080;&#26679;&#26412;&#30340;&#36530;&#36991;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08903</link><description>&lt;p&gt;
PPR: &#22312;&#32500;&#25345;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#30340;&#21516;&#26102;&#22686;&#24378;&#36530;&#36991;&#25915;&#20987;&#23545;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
PPR: Enhancing Dodging Attacks while Maintaining Impersonation Attacks on Face Recognition Systems. (arXiv:2401.08903v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PPR&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#36530;&#36991;&#25915;&#20987;&#30340;&#24615;&#33021;&#21516;&#26102;&#36991;&#20813;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#30340;&#38477;&#32423;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#20462;&#21098;&#65292;&#24182;&#36890;&#36807;&#23884;&#20837;&#23545;&#25239;&#25200;&#21160;&#26469;&#22686;&#24378;&#23545;&#25239;&#20154;&#33080;&#26679;&#26412;&#30340;&#36530;&#36991;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#19978;&#30340;&#23545;&#25239;&#25915;&#20987;&#21487;&#20197;&#20998;&#20026;&#20004;&#31181;&#31867;&#22411;&#65306;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#21644;&#36530;&#36991;&#25915;&#20987;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#25104;&#21151;&#36827;&#34892;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#24182;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#22312;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#19978;&#25104;&#21151;&#36827;&#34892;&#36530;&#36991;&#25915;&#20987;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39044;&#35757;&#32451;&#20462;&#21098;&#24674;&#22797;&#25915;&#20987;&#65288;PPR&#65289;&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#36530;&#36991;&#25915;&#20987;&#30340;&#24615;&#33021;&#21516;&#26102;&#36991;&#20813;&#20882;&#21517;&#39030;&#26367;&#25915;&#20987;&#30340;&#38477;&#32423;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#20462;&#21098;&#65292;&#21487;&#20197;&#23558;&#19968;&#37096;&#20998;&#23545;&#25239;&#25200;&#21160;&#35774;&#20026;&#38646;&#65292;&#24182;&#20542;&#21521;&#20110;&#20445;&#25345;&#25915;&#20987;&#24615;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;&#23545;&#25239;&#26679;&#26412;&#20462;&#21098;&#65292;&#25105;&#20204;&#21487;&#20197;&#20462;&#21098;&#39044;&#35757;&#32451;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#24182;&#26377;&#36873;&#25321;&#24615;&#22320;&#37322;&#25918;&#26576;&#20123;&#23545;&#25239;&#25200;&#21160;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#23545;&#25239;&#25200;&#21160;&#23884;&#20837;&#21040;&#20462;&#21098;&#21306;&#22495;&#65292;&#20174;&#32780;&#22686;&#24378;&#23545;&#25239;&#20154;&#33080;&#26679;&#26412;&#30340;&#36530;&#36991;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks on Face Recognition (FR) encompass two types: impersonation attacks and evasion attacks. We observe that achieving a successful impersonation attack on FR does not necessarily ensure a successful dodging attack on FR in the black-box setting. Introducing a novel attack method named Pre-training Pruning Restoration Attack (PPR), we aim to enhance the performance of dodging attacks whilst avoiding the degradation of impersonation attacks. Our method employs adversarial example pruning, enabling a portion of adversarial perturbations to be set to zero, while tending to maintain the attack performance. By utilizing adversarial example pruning, we can prune the pre-trained adversarial examples and selectively free up certain adversarial perturbations. Thereafter, we embed adversarial perturbations in the pruned area, which enhances the dodging performance of the adversarial face examples. The effectiveness of our proposed attack method is demonstrated through our experim
&lt;/p&gt;</description></item><item><title>MADA&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#20248;&#21270;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#36229;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#23398;&#20064;&#26368;&#36866;&#21512;&#30340;&#20248;&#21270;&#22120;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;MADA&#22312;&#27425;&#20248;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#19979;&#26159;&#31283;&#20581;&#30340;&#65292;&#24182;&#19988;&#22312;&#40664;&#35748;&#36229;&#21442;&#25968;&#19979;&#24120;&#24120;&#20248;&#20110;&#20854;&#20182;&#20248;&#21270;&#22120;&#12290;&#25554;&#20540;&#20248;&#21270;&#22120;&#21487;&#20197;&#25913;&#36827;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08893</link><description>&lt;p&gt;
MADA: &#36890;&#36807;&#36229;&#26799;&#24230;&#19979;&#38477;&#30340;&#20803;&#36866;&#24212;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
MADA: Meta-Adaptive Optimizers through hyper-gradient Descent. (arXiv:2401.08893v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08893
&lt;/p&gt;
&lt;p&gt;
MADA&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#20248;&#21270;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#36229;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#23398;&#20064;&#26368;&#36866;&#21512;&#30340;&#20248;&#21270;&#22120;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;MADA&#22312;&#27425;&#20248;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#19979;&#26159;&#31283;&#20581;&#30340;&#65292;&#24182;&#19988;&#22312;&#40664;&#35748;&#36229;&#21442;&#25968;&#19979;&#24120;&#24120;&#20248;&#20110;&#20854;&#20182;&#20248;&#21270;&#22120;&#12290;&#25554;&#20540;&#20248;&#21270;&#22120;&#21487;&#20197;&#25913;&#36827;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;Adam&#34987;&#24341;&#20837;&#20197;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#22411;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#12290;&#36825;&#20123;&#20248;&#21270;&#22120;&#36890;&#24120;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#21331;&#36234;&#65292;&#20294;&#21487;&#33021;&#26080;&#27861;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;Adam&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20803;&#36866;&#24212;&#20248;&#21270;&#22120;(MADA)&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#20248;&#21270;&#22120;&#26694;&#26550;&#65292;&#21487;&#20197;&#27010;&#25324;&#22810;&#31181;&#24050;&#30693;&#30340;&#20248;&#21270;&#22120;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#23398;&#20064;&#26368;&#36866;&#21512;&#30340;&#20248;&#21270;&#22120;&#12290;MADA&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#23545;&#20248;&#21270;&#22120;&#30340;&#31354;&#38388;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24182;&#20351;&#29992;&#36229;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#25628;&#32034;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;MADA&#23545;&#20110;&#27425;&#20248;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#26159;&#31283;&#20581;&#30340;&#65292;&#24182;&#19988;&#22312;&#40664;&#35748;&#36229;&#21442;&#25968;&#19979;&#24120;&#24120;&#20248;&#20110;Adam&#12289;Lion&#21644;Adan&#65292;&#29978;&#33267;&#22312;&#20248;&#21270;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;AVGrad&#65292;&#23427;&#26159;AMSGrad&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#22312;&#20854;&#20013;&#23558;&#26368;&#22823;&#25805;&#20316;&#31526;&#26367;&#25442;&#20026;&#24179;&#22343;&#25805;&#20316;&#31526;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#22312;MADA&#20013;&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25910;&#25947;&#20998;&#26512;&#65292;&#20197;&#34920;&#26126;&#20248;&#21270;&#22120;&#30340;&#25554;&#20540;&#65288;&#29305;&#21035;&#26159;AVGrad&#21644;Adam&#65289;&#21487;&#20197;&#25913;&#36827;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since Adam was introduced, several novel adaptive optimizers for deep learning have been proposed. These optimizers typically excel in some tasks but may not outperform Adam uniformly across all tasks. In this work, we introduce Meta-Adaptive Optimizers (MADA), a unified optimizer framework that can generalize several known optimizers and dynamically learn the most suitable one during training. The key idea in MADA is to parameterize the space of optimizers and search through it using hyper-gradient descent. Numerical results suggest that MADA is robust against sub-optimally tuned hyper-parameters, and outperforms Adam, Lion, and Adan with their default hyper-parameters, often even with optimized hyper-parameters. We also propose AVGrad, a variant of AMSGrad where the maximum operator is replaced with averaging, and observe that it performs better within MADA. Finally, we provide a convergence analysis to show that interpolation of optimizers (specifically, AVGrad and Adam) can improve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25209;&#22788;&#29702;ICL&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;ICL&#35270;&#20026;&#19968;&#20010;&#20803;&#20248;&#21270;&#36807;&#31243;&#65292;&#24320;&#21457;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#12289;&#39640;&#25928;&#19988;&#26080;&#24207;&#30340;&#25512;&#29702;&#31639;&#27861;&#12290;&#36890;&#36807;&#32858;&#21512;&#20803;&#26799;&#24230;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38646;-shot&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20351;LLM&#23545;ICL&#31034;&#20363;&#39034;&#24207;&#26080;&#20851;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#25490;&#21015;&#26041;&#24335;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26631;&#20934;ICL&#30340;&#26368;&#20339;&#39034;&#24207;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06469</link><description>&lt;p&gt;
&#25209;&#22788;&#29702;ICL: &#26377;&#25928;&#65292;&#39640;&#25928;&#19988;&#26080;&#24207;&#22320;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning. (arXiv:2401.06469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25209;&#22788;&#29702;ICL&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;ICL&#35270;&#20026;&#19968;&#20010;&#20803;&#20248;&#21270;&#36807;&#31243;&#65292;&#24320;&#21457;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#12289;&#39640;&#25928;&#19988;&#26080;&#24207;&#30340;&#25512;&#29702;&#31639;&#27861;&#12290;&#36890;&#36807;&#32858;&#21512;&#20803;&#26799;&#24230;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38646;-shot&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20351;LLM&#23545;ICL&#31034;&#20363;&#39034;&#24207;&#26080;&#20851;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#25490;&#21015;&#26041;&#24335;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26631;&#20934;ICL&#30340;&#26368;&#20339;&#39034;&#24207;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#35270;&#20026;&#19968;&#20010;&#20803;&#20248;&#21270;&#36807;&#31243;&#65292;&#35299;&#37322;&#20102;LLM&#23545;ICL&#31034;&#20363;&#39034;&#24207;&#25935;&#24863;&#30340;&#21407;&#22240;&#12290;&#36825;&#31181;&#29702;&#35299;&#20351;&#25105;&#20204;&#24320;&#21457;&#20986;&#20102;Batch-ICL&#65292;&#19968;&#31181;&#29992;&#20110;ICL&#30340;&#26377;&#25928;&#12289;&#39640;&#25928;&#19988;&#26080;&#24207;&#30340;&#25512;&#29702;&#31639;&#27861;&#12290;&#19982;&#26631;&#20934;&#30340;N-shot&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;Batch-ICL&#20351;&#29992;N&#20010;&#21333;&#29420;&#30340;1-shot&#21069;&#21521;&#35745;&#31639;&#65292;&#24182;&#32858;&#21512;&#24471;&#21040;&#30340;&#20803;&#26799;&#24230;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#32858;&#21512;&#30340;&#20803;&#26799;&#24230;&#24212;&#29992;&#20110;&#38646;-shot&#23398;&#20064;&#20197;&#29983;&#25104;&#26368;&#32456;&#39044;&#27979;&#12290;&#36825;&#31181;&#25209;&#22788;&#29702;&#26041;&#27861;&#20351;LLM&#23545;ICL&#31034;&#20363;&#30340;&#39034;&#24207;&#26080;&#20851;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;Batch-ICL&#19968;&#33268;&#20248;&#20110;&#22823;&#22810;&#25968;&#31034;&#20363;&#24207;&#21015;&#30340;&#25490;&#21015;&#26041;&#24335;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26631;&#20934;ICL&#30340;&#26368;&#20339;&#39034;&#24207;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;Batch-ICL&#30340;&#19968;&#31181;&#26032;&#39062;&#21464;&#20307;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#20010;"epochs"&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, by treating in-context learning (ICL) as a meta-optimization process, we explain why LLMs are sensitive to the order of ICL examples. This understanding leads us to the development of Batch-ICL, an effective, efficient, and order-agnostic inference algorithm for ICL. Differing from the standard N-shot learning approach, Batch-ICL employs $N$ separate 1-shot forward computations and aggregates the resulting meta-gradients. These aggregated meta-gradients are then applied to a zero-shot learning to generate the final prediction. This batch processing approach renders the LLM agnostic to the order of ICL examples. Through extensive experiments and analysis, we demonstrate that Batch-ICL consistently outperforms most permutations of example sequences. In some cases, it even exceeds the performance of the optimal order for standard ICL, all while reducing the computational resources required. Furthermore, we develop a novel variant of Batch-ICL featuring multiple "epochs" of 
&lt;/p&gt;</description></item><item><title>AUTOACT&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#21512;&#25104;&#36712;&#36857;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05268</link><description>&lt;p&gt;
AUTOACT&#65306;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#23454;&#29616;&#30340;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AUTOACT: Automatic Agent Learning from Scratch via Self-Planning. (arXiv:2401.05268v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05268
&lt;/p&gt;
&lt;p&gt;
AUTOACT&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#21512;&#25104;&#36712;&#36857;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#19981;&#26029;&#30340;&#25506;&#32034;&#65292;&#20294;&#29616;&#26377;&#30340;&#35821;&#35328;&#20195;&#29702;&#31995;&#32479;&#20173;&#28982;&#38754;&#20020;&#26114;&#36149;&#12289;&#19981;&#21487;&#37325;&#22797;&#30340;&#25968;&#25454;&#20381;&#36182;&#38382;&#39064;&#65292;&#24182;&#19988;&#38754;&#20020;&#23558;&#21333;&#19968;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#20010;&#21151;&#33021;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoAct&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#21644;&#26469;&#33258;&#38381;&#28304;&#27169;&#22411;&#65288;&#22914;GPT-4&#65289;&#30340;&#21512;&#25104;&#36712;&#36857;&#12290;&#32473;&#23450;&#26377;&#38480;&#30340;&#25968;&#25454;&#21644;&#24037;&#20855;&#24211;&#65292;AutoAct&#39318;&#20808;&#33258;&#21160;&#21512;&#25104;&#35268;&#21010;&#36712;&#36857;&#65292;&#19981;&#38656;&#35201;&#20154;&#31867;&#25110;&#24378;&#38381;&#28304;&#27169;&#22411;&#30340;&#20219;&#20309;&#36741;&#21161;&#12290;&#28982;&#21518;&#65292;AutoAct&#21033;&#29992;&#20998;&#24037;&#31574;&#30053;&#65292;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#20449;&#24687;&#21644;&#21512;&#25104;&#36712;&#36857;&#33258;&#21160;&#21306;&#20998;&#65292;&#20135;&#29983;&#19968;&#20010;&#23376;&#20195;&#29702;&#32452;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#31181;LLMs&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;AutoAct&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#25110;&#19982;&#20854;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to var
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#32852;&#37030;&#36951;&#24536;&#30340;&#27010;&#24565;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#21644;&#35774;&#35745;&#20934;&#21017;&#65292;&#26088;&#22312;&#20026;&#32852;&#37030;&#23398;&#20064;&#20013;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#21644;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.05146</link><description>&lt;p&gt;
&#32852;&#37030;&#36951;&#24536;&#65306;&#26041;&#27861;&#12289;&#35774;&#35745;&#20934;&#21017;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Federated Unlearning: A Survey on Methods, Design Guidelines, and Evaluation Metrics. (arXiv:2401.05146v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05146
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#32852;&#37030;&#36951;&#24536;&#30340;&#27010;&#24565;&#21644;&#25361;&#25112;&#65292;&#20197;&#21450;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#21644;&#35774;&#35745;&#20934;&#21017;&#65292;&#26088;&#22312;&#20026;&#32852;&#37030;&#23398;&#20064;&#20013;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#21644;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20351;&#24471;&#22810;&#20010;&#21442;&#19982;&#26041;&#33021;&#22815;&#21327;&#21516;&#35757;&#32451;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20445;&#30041;&#25968;&#25454;&#22312;&#26412;&#22320;&#23384;&#20648;&#65292;&#20174;&#32780;&#32500;&#25252;&#20102;&#29992;&#25143;&#21644;&#26426;&#26500;&#30340;&#38544;&#31169;&#12290;&#19982;&#38598;&#20013;&#21270;&#21407;&#22987;&#25968;&#25454;&#19981;&#21516;&#65292;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#20132;&#25442;&#26412;&#22320;&#20248;&#21270;&#30340;&#27169;&#22411;&#21442;&#25968;&#26469;&#36880;&#27493;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#12290;&#23613;&#31649;&#32852;&#37030;&#23398;&#20064;&#26356;&#21152;&#31526;&#21512;&#26032;&#20852;&#35268;&#23450;&#65292;&#22914;&#27431;&#27954;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#65288;GDPR&#65289;&#65292;&#20294;&#22312;&#27492;&#32972;&#26223;&#19979;&#30830;&#20445;&#36951;&#24536;&#26435;&#8212;&#8212;&#20801;&#35768;&#32852;&#37030;&#23398;&#20064;&#21442;&#19982;&#26041;&#20174;&#23398;&#20064;&#30340;&#27169;&#22411;&#20013;&#21024;&#38500;&#20182;&#20204;&#30340;&#25968;&#25454;&#36129;&#29486;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#27492;&#22806;&#65292;&#20154;&#20204;&#35748;&#35782;&#21040;&#24694;&#24847;&#23458;&#25143;&#31471;&#21487;&#33021;&#36890;&#36807;&#26356;&#26032;&#23558;&#21518;&#38376;&#27880;&#20837;&#20840;&#23616;&#27169;&#22411;&#65292;&#20363;&#22914;&#23545;&#29305;&#21046;&#25968;&#25454;&#31034;&#20363;&#36827;&#34892;&#38169;&#35823;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26426;&#21046;&#26469;&#30830;&#20445;&#20010;&#20154;&#26377;&#21487;&#33021;&#22312;&#32858;&#21512;&#21518;&#31227;&#38500;&#20182;&#20204;&#30340;&#25968;&#25454;&#24182;&#28165;&#38500;&#24694;&#24847;&#36129;&#29486;&#65292;&#32780;&#19981;&#25439;&#23475;&#24050;&#33719;&#24471;&#30340;"&#20840;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) enables collaborative training of a Machine Learning (ML) model across multiple parties, facilitating the preservation of users' and institutions' privacy by keeping data stored locally. Instead of centralizing raw data, FL exchanges locally refined model parameters to build a global model incrementally. While FL is more compliant with emerging regulations such as the European General Data Protection Regulation (GDPR), ensuring the right to be forgotten in this context - allowing FL participants to remove their data contributions from the learned model - remains unclear. In addition, it is recognized that malicious clients may inject backdoors into the global model through updates, e.g. to generate mispredictions on specially crafted data examples. Consequently, there is the need for mechanisms that can guarantee individuals the possibility to remove their data and erase malicious contributions even after aggregation, without compromising the already acquired "g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22686;&#24378;&#30340;&#31616;&#21333;&#38750;&#23545;&#31216;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;GraphACL&#65292;&#36890;&#36807;&#32771;&#34385;&#37051;&#23621;&#33410;&#28857;&#30340;&#38750;&#23545;&#31216;&#35270;&#22270;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#21516;&#31867;&#21644;&#24322;&#31867;&#22270;&#19978;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#65292;&#23545;&#20110;&#24314;&#27169;&#24322;&#31867;&#22270;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.18884</link><description>&lt;p&gt;
&#26080;&#38656;&#22686;&#24378;&#30340;&#31616;&#21333;&#38750;&#23545;&#31216;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Simple and Asymmetric Graph Contrastive Learning without Augmentations. (arXiv:2310.18884v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#22686;&#24378;&#30340;&#31616;&#21333;&#38750;&#23545;&#31216;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;GraphACL&#65292;&#36890;&#36807;&#32771;&#34385;&#37051;&#23621;&#33410;&#28857;&#30340;&#38750;&#23545;&#31216;&#35270;&#22270;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#21516;&#31867;&#21644;&#24322;&#31867;&#22270;&#19978;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#65292;&#23545;&#20110;&#24314;&#27169;&#24322;&#31867;&#22270;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#20013;&#26174;&#31034;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;GCL&#26041;&#27861;&#20381;&#36182;&#20110;&#39044;&#21046;&#30340;&#22270;&#22686;&#24378;&#21644;&#21516;&#31867;&#20551;&#35774;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#36830;&#36890;&#33410;&#28857;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#31867;&#26631;&#31614;&#21644;&#19981;&#30456;&#20284;&#29305;&#24449;&#30340;&#24322;&#31867;&#22270;&#19978;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21516;&#31867;&#21644;&#24322;&#31867;&#22270;&#19978;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#32771;&#34385;&#37051;&#23621;&#33410;&#28857;&#30340;&#38750;&#23545;&#31216;&#35270;&#22270;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#31616;&#21333;&#31639;&#27861;&#65292;&#31216;&#20026;&#22270;&#30340;&#38750;&#23545;&#31216;&#23545;&#27604;&#23398;&#20064;(GraphACL)&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#19981;&#20381;&#36182;&#20110;&#22270;&#22686;&#24378;&#21644;&#21516;&#31867;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#65292;&#35777;&#26126;GraphACL&#33021;&#22815;&#25429;&#25417;&#21333;&#36339;&#26412;&#22320;&#37051;&#22495;&#20449;&#24687;&#21644;&#21452;&#36339;&#21333;&#19968;&#30456;&#20284;&#24615;&#65292;&#36825;&#20004;&#32773;&#23545;&#20110;&#24314;&#27169;&#24322;&#31867;&#22270;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Contrastive Learning (GCL) has shown superior performance in representation learning in graph-structured data. Despite their success, most existing GCL methods rely on prefabricated graph augmentation and homophily assumptions. Thus, they fail to generalize well to heterophilic graphs where connected nodes may have different class labels and dissimilar features. In this paper, we study the problem of conducting contrastive learning on homophilic and heterophilic graphs. We find that we can achieve promising performance simply by considering an asymmetric view of the neighboring nodes. The resulting simple algorithm, Asymmetric Contrastive Learning for Graphs (GraphACL), is easy to implement and does not rely on graph augmentations and homophily assumptions. We provide theoretical and empirical evidence that GraphACL can capture one-hop local neighborhood information and two-hop monophily similarity, which are both important for modeling heterophilic graphs. Experimental results s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26080;&#37197;&#23545;MRI&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#25552;&#39640;SR&#24615;&#33021;&#65292;&#25913;&#21892;MRI&#20998;&#36776;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.15767</link><description>&lt;p&gt;
&#26080;&#37197;&#23545;MRI&#36229;&#20998;&#36776;&#29575;&#19982;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unpaired MRI Super Resolution with Self-Supervised Contrastive Learning. (arXiv:2310.15767v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#26080;&#37197;&#23545;MRI&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#25552;&#39640;SR&#24615;&#33021;&#65292;&#25913;&#21892;MRI&#20998;&#36776;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20998;&#36776;&#29575;&#65288;HR&#65289;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;MRI&#20998;&#36776;&#29575;&#30340;&#22266;&#26377;&#38480;&#21046;&#38480;&#21046;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#65288;SR&#65289;&#26041;&#27861;&#23637;&#29616;&#20102;&#25552;&#21319;MRI&#20998;&#36776;&#29575;&#30340;&#28508;&#21147;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;HR MRI&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#36825;&#21487;&#33021;&#38590;&#20197;&#33719;&#21462;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#37197;&#23545;MRI SR&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#19979;&#30340;SR&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#30495;&#23454;&#30340;HR&#22270;&#20687;&#21644;&#20154;&#24037;&#29983;&#25104;&#30340;SR&#22270;&#20687;&#26500;&#24314;&#27491;&#36127;&#26679;&#26412;&#23545;&#65292;&#20174;&#32780;&#20419;&#36827;&#36776;&#21035;&#24615;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;&#26412;&#30740;&#31350;&#21576;&#29616;&#30340;&#23454;&#35777;&#32467;&#26524;&#31361;&#20986;&#20102;&#23792;&#20540;&#20449;&#22122;&#27604;&#21644;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#30340;&#26174;&#33879;&#25552;&#39640;&#65292;&#21363;&#20351;&#32570;&#20047;HR&#22270;&#20687;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-resolution (HR) magnetic resonance imaging (MRI) is crucial for enhancing diagnostic accuracy in clinical settings. Nonetheless, the inherent limitation of MRI resolution restricts its widespread applicability. Deep learning-based image super-resolution (SR) methods exhibit promise in improving MRI resolution without additional cost. However, these methods frequently require a substantial number of HR MRI images for training, which can be challenging to acquire. In this paper, we propose an unpaired MRI SR approach that employs self-supervised contrastive learning to enhance SR performance with limited training data. Our approach leverages both authentic HR images and synthetically generated SR images to construct positive and negative sample pairs, thus facilitating the learning of discriminative features. Empirical results presented in this study underscore significant enhancements in the peak signal-to-noise ratio and structural similarity index, even when a paucity of HR image
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120;&#65288;MCP&#65289;&#29992;&#20110;&#23558;&#29289;&#29702;-&#27010;&#24565;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#24314;&#27169;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#29289;&#29702;&#36807;&#31243;&#30340;&#21151;&#33021;&#24615;&#21644;&#36136;&#37327;&#20445;&#25345;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08644</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#24314;&#27169;&#20013;&#30340;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of Geoscientific Systems. (arXiv:2310.08644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08644
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120;&#65288;MCP&#65289;&#29992;&#20110;&#23558;&#29289;&#29702;-&#27010;&#24565;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#24314;&#27169;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#29289;&#29702;&#36807;&#31243;&#30340;&#21151;&#33021;&#24615;&#21644;&#36136;&#37327;&#20445;&#25345;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25968;&#21313;&#24180;&#26469;&#33268;&#21147;&#20110;&#26500;&#24314;&#29992;&#20110;&#39044;&#27979;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#26102;&#38388;&#24207;&#21015;&#28436;&#21270;&#30340;&#29289;&#29702;-&#27010;&#24565; (PC) &#27169;&#22411;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064; (ML) &#30340;&#38376;&#25511;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#26356;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20174;ML&#22522;&#30784;&#27169;&#22411;&#20013;&#25552;&#21462;&#29289;&#29702;&#29702;&#35299;&#30340;&#22256;&#38590;&#20351;&#24471;&#20854;&#22312;&#22686;&#24378;&#23545;&#31995;&#32479;&#32467;&#26500;&#21644;&#21151;&#33021;&#30340;&#31185;&#23398;&#30693;&#35782;&#26041;&#38754;&#30340;&#24212;&#29992;&#21464;&#24471;&#22797;&#26434;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35299;&#29289;&#29702;&#24615;&#30340;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120; (MCP) &#20316;&#20026;&#24357;&#21512;PC&#27169;&#22411;&#21644;ML&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;MCP&#21033;&#29992;PC&#27169;&#22411;&#21644;GRNNs&#32972;&#21518;&#30340;&#26377;&#21521;&#22270;&#32467;&#26500;&#30340;&#20869;&#22312;&#21516;&#26500;&#24615;&#65292;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#26126;&#30830;&#34920;&#31034;&#29289;&#29702;&#36807;&#31243;&#30340;&#36136;&#37327;&#20445;&#25345;&#24615;&#36136;&#65292;&#21516;&#26102;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#21644;&#29616;&#25104;&#30340;ML&#25216;&#26415;&#30452;&#25509;&#23398;&#20064;&#36825;&#31181;&#36807;&#31243;&#30340;&#21151;&#33021;&#24615;&#65288;&#21487;&#35299;&#37322;&#24615;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although decades of effort have been devoted to building Physical-Conceptual (PC) models for predicting the time-series evolution of geoscientific systems, recent work shows that Machine Learning (ML) based Gated Recurrent Neural Network technology can be used to develop models that are much more accurate. However, the difficulty of extracting physical understanding from ML-based models complicates their utility for enhancing scientific knowledge regarding system structure and function. Here, we propose a physically-interpretable Mass Conserving Perceptron (MCP) as a way to bridge the gap between PC-based and ML-based modeling approaches. The MCP exploits the inherent isomorphism between the directed graph structures underlying both PC models and GRNNs to explicitly represent the mass-conserving nature of physical processes while enabling the functional nature of such processes to be directly learned (in an interpretable manner) from available data using off-the-shelf ML technology. As
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;RDAC&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35299;&#21078;&#20102;&#25345;&#32493;&#23398;&#20064;&#20013;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#35814;&#32454;&#20998;&#26512;&#20102;&#20960;&#31181;&#24120;&#29992;&#31639;&#27861;&#22312;&#22788;&#29702;&#20219;&#21153;&#26102;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.04741</link><description>&lt;p&gt;
&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#24179;&#34913;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#65306;&#28608;&#27963;&#21464;&#21270;&#30340;&#35835;&#20986;&#20998;&#35299;&#65288;RDAC&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Balancing stability and plasticity in continual learning: the readout-decomposition of activation change (RDAC) framework. (arXiv:2310.04741v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;RDAC&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#35299;&#21078;&#20102;&#25345;&#32493;&#23398;&#20064;&#20013;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24182;&#35814;&#32454;&#20998;&#26512;&#20102;&#20960;&#31181;&#24120;&#29992;&#31639;&#27861;&#22312;&#22788;&#29702;&#20219;&#21153;&#26102;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#26088;&#22312;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#21516;&#26102;&#20445;&#30041;&#20808;&#21069;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#20173;&#28982;&#26159;&#19968;&#20010;&#20013;&#24515;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23545;&#36825;&#31181;&#24179;&#34913;&#36827;&#34892;&#20102;&#35299;&#21078;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;&#28608;&#27963;&#21464;&#21270;&#30340;&#35835;&#20986;&#20998;&#35299;&#65288;RDAC&#65289;&#26694;&#26550;&#39318;&#20808;&#35299;&#20915;&#20102;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#22256;&#22659;&#21450;&#20854;&#19982;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#20851;&#31995;&#12290;&#23427;&#23558;&#23398;&#20064;&#35825;&#23548;&#30340;&#28608;&#27963;&#21464;&#21270;&#19982;&#20808;&#21069;&#35835;&#20986;&#33539;&#22260;&#20869;&#30340;&#31283;&#23450;&#24615;&#31243;&#24230;&#21644;&#38646;&#31354;&#38388;&#30340;&#21464;&#21270;&#19982;&#21487;&#22609;&#24615;&#31243;&#24230;&#30456;&#20851;&#32852;&#12290;&#22312;&#22788;&#29702;&#20998;&#35010;CIFAR-110&#20219;&#21153;&#30340;&#28145;&#24230;&#38750;&#32447;&#24615;&#32593;&#32476;&#20013;&#65292;&#35813;&#26694;&#26550;&#38416;&#26126;&#20102;&#24120;&#29992;&#27491;&#21017;&#21270;&#31639;&#27861;&#65288;SI&#12289;EWC&#21644;LwF&#65289;&#20197;&#21450;&#37325;&#25918;&#31639;&#27861;&#65288;GEM&#21644;&#25968;&#25454;&#37325;&#25918;&#65289;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) algorithms strive to acquire new knowledge while preserving prior information. However, this stability-plasticity trade-off remains a central challenge. This paper introduces a framework that dissects this trade-off, offering valuable insights into CL algorithms. The Readout-Decomposition of Activation Change (RDAC) framework first addresses the stability-plasticity dilemma and its relation to catastrophic forgetting. It relates learning-induced activation changes in the range of prior readouts to the degree of stability and changes in the null space to the degree of plasticity. In deep non-linear networks tackling split-CIFAR-110 tasks, the framework clarifies the stability-plasticity trade-offs of the popular regularization algorithms Synaptic intelligence (SI), Elastic-weight consolidation (EWC), and learning without Forgetting (LwF), and replay-based algorithms Gradient episodic memory (GEM), and data replay. GEM and data replay preserved stability and plast
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#39640;&#25928;&#26102;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20840;&#23616;&#25552;&#31034;&#12289;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#21644;&#24863;&#30693;&#26102;&#24207;&#28418;&#31227;&#30340;&#25552;&#31034;&#65292;&#19981;&#38656;&#35201;&#30446;&#26631;&#22495;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#26102;&#24207;&#28418;&#31227;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02473</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#39640;&#25928;&#26102;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Prompting-based Efficient Temporal Domain Generalization. (arXiv:2310.02473v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02473
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#39640;&#25928;&#26102;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20840;&#23616;&#25552;&#31034;&#12289;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#21644;&#24863;&#30693;&#26102;&#24207;&#28418;&#31227;&#30340;&#25552;&#31034;&#65292;&#19981;&#38656;&#35201;&#30446;&#26631;&#22495;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#26102;&#24207;&#28418;&#31227;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#26159;&#29420;&#31435;&#19988;&#30456;&#21516;&#20998;&#24067;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#20998;&#24067;&#20250;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#23548;&#33268;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#22312;&#26410;&#26469;&#26102;&#38388;&#27573;&#30340;&#27867;&#21270;&#33021;&#21147;&#21464;&#24046;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#22495;&#27867;&#21270;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#26102;&#38388;&#39640;&#25928;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#22495;&#25968;&#25454;&#65288;&#21363;&#26410;&#30693;&#30340;&#26410;&#26469;&#26102;&#38388;&#27573;&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#20840;&#23616;&#25552;&#31034;&#12289;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#21644;&#24863;&#30693;&#21040;&#26102;&#24207;&#28418;&#31227;&#30340;&#25552;&#31034;&#30340;&#26041;&#24335;&#65292;&#23558;&#30446;&#26631;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#26102;&#24207;&#28418;&#31227;&#12290;&#23427;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#20363;&#22914;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#22312;&#26102;&#22495;&#27867;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;&#20195;&#30721;&#20179;&#24211;&#23558;&#20844;&#24320;&#20998;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning traditionally assumes that training and testing data are distributed independently and identically. However, in many real-world settings, the data distribution can shift over time, leading to poor generalization of trained models in future time periods. Our paper presents a novel prompting-based approach to temporal domain generalization that is parameter-efficient, time-efficient, and does not require access to the target domain data (i.e., unseen future time periods) during training. Our method adapts a target pre-trained model to temporal drift by learning global prompts, domain-specific prompts, and drift-aware prompts that capture underlying temporal dynamics. It is compatible across diverse tasks, such as classification, regression, and time series forecasting, and sets a new state-of-the-art benchmark in temporal domain generalization. The code repository will be publicly shared.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#28151;&#21512;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#23545;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#35777;&#26126;&#21021;&#22987;&#29983;&#25104;&#27169;&#22411;&#36275;&#22815;&#25509;&#36817;&#25968;&#25454;&#20998;&#24067;&#24182;&#19988;&#25968;&#25454;&#27604;&#20363;&#36866;&#24403;&#65292;&#36845;&#20195;&#35757;&#32451;&#26159;&#31283;&#23450;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.00429</link><description>&lt;p&gt;
&#20851;&#20110;&#29983;&#25104;&#27169;&#22411;&#22312;&#20854;&#33258;&#24049;&#30340;&#25968;&#25454;&#19978;&#36845;&#20195;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Stability of Iterative Retraining of Generative Models on their own Data. (arXiv:2310.00429v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#28151;&#21512;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#23545;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#35777;&#26126;&#21021;&#22987;&#29983;&#25104;&#27169;&#22411;&#36275;&#22815;&#25509;&#36817;&#25968;&#25454;&#20998;&#24067;&#24182;&#19988;&#25968;&#25454;&#27604;&#20363;&#36866;&#24403;&#65292;&#36845;&#20195;&#35757;&#32451;&#26159;&#31283;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#24448;&#24448;&#23637;&#29616;&#20986;&#36229;&#36807;&#20856;&#22411;&#20154;&#31867;&#33021;&#21147;&#30340;&#26679;&#26412;&#30495;&#23454;&#24615;&#36776;&#21035;&#33021;&#21147;&#12290;&#36825;&#19968;&#25104;&#21151;&#30340;&#20851;&#38190;&#39537;&#21160;&#21147;&#26080;&#30097;&#26159;&#36825;&#20123;&#27169;&#22411;&#28040;&#32791;&#28023;&#37327;&#32593;&#32476;&#35268;&#27169;&#25968;&#25454;&#30340;&#32467;&#26524;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#24778;&#20154;&#30340;&#24615;&#33021;&#21644;&#26131;&#24471;&#24615;&#65292;&#32593;&#32476;&#19978;&#23558;&#19981;&#21487;&#36991;&#20813;&#22320;&#20986;&#29616;&#36234;&#26469;&#36234;&#22810;&#30340;&#21512;&#25104;&#20869;&#23481;&#12290;&#36825;&#20010;&#20107;&#23454;&#30452;&#25509;&#24847;&#21619;&#30528;&#29983;&#25104;&#27169;&#22411;&#30340;&#26410;&#26469;&#36845;&#20195;&#24517;&#39035;&#38754;&#23545;&#19968;&#20010;&#29616;&#23454;&#65306;&#23427;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#30001;&#28165;&#27905;&#25968;&#25454;&#21644;&#20808;&#21069;&#27169;&#22411;&#29983;&#25104;&#30340;&#20154;&#24037;&#25968;&#25454;&#32452;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23545;&#28151;&#21512;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#65289;&#19978;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#23545;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#36827;&#34892;&#20005;&#26684;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#22312;&#21021;&#22987;&#29983;&#25104;&#27169;&#22411;&#36275;&#22815;&#22909;&#22320;&#36817;&#20284;&#25968;&#25454;&#20998;&#24067;&#24182;&#19988;&#30495;&#23454;&#25968;&#25454;&#19982;&#21512;&#25104;&#25968;&#25454;&#30340;&#27604;&#20363;&#36866;&#24403;&#30340;&#24773;&#20917;&#19979;&#65292;&#36845;&#20195;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models have made tremendous progress in modeling complex data, often exhibiting generation quality that surpasses a typical human's ability to discern the authenticity of samples. Undeniably, a key driver of this success is enabled by the massive amounts of web-scale data consumed by these models. Due to these models' striking performance and ease of availability, the web will inevitably be increasingly populated with synthetic content. Such a fact directly implies that future iterations of generative models must contend with the reality that their training is curated from both clean data and artificially generated data from past models. In this paper, we develop a framework to rigorously study the impact of training generative models on mixed datasets (of real and synthetic data) on their stability. We first prove the stability of iterative training under the condition that the initial generative models approximate the data distribution well enough and the proportion o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36716;&#25442;&#20197;&#23454;&#29616;&#36890;&#29992;&#30340;&#23454;&#20363;&#19981;&#21464;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#26469;&#39044;&#27979;&#22270;&#20687;&#30340;&#21464;&#25442;&#20998;&#24067;&#65292;&#24182;&#23545;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#24179;&#22343;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#19981;&#21516;&#23454;&#20363;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#20174;&#32780;&#25512;&#24191;&#19981;&#21464;&#24615;&#30340;&#31867;&#21035;&#38388;&#30340;&#24212;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#36824;&#21487;&#20197;&#36866;&#24212;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#23039;&#21183;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20064;&#26356;&#24191;&#27867;&#30340;&#21464;&#25442;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2309.16672</link><description>&lt;p&gt;
&#23398;&#20064;&#36716;&#25442;&#20197;&#23454;&#29616;&#36890;&#29992;&#30340;&#23454;&#20363;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning to Transform for Generalizable Instance-wise Invariance. (arXiv:2309.16672v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36716;&#25442;&#20197;&#23454;&#29616;&#36890;&#29992;&#30340;&#23454;&#20363;&#19981;&#21464;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#26469;&#39044;&#27979;&#22270;&#20687;&#30340;&#21464;&#25442;&#20998;&#24067;&#65292;&#24182;&#23545;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#24179;&#22343;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#19981;&#21516;&#23454;&#20363;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#20174;&#32780;&#25512;&#24191;&#19981;&#21464;&#24615;&#30340;&#31867;&#21035;&#38388;&#30340;&#24212;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#36824;&#21487;&#20197;&#36866;&#24212;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#23039;&#21183;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20064;&#26356;&#24191;&#27867;&#30340;&#21464;&#25442;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#19968;&#30452;&#33268;&#21147;&#20110;&#26500;&#24314;&#23545;&#33258;&#28982;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#21464;&#25442;&#20855;&#26377;&#24378;&#40065;&#26834;&#24615;&#30340;&#31995;&#32479;&#12290;&#20256;&#32479;&#19978;&#65292;&#21487;&#20197;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25110;&#23558;&#19981;&#21464;&#24615;&#30828;&#32534;&#30721;&#21040;&#26550;&#26500;&#20013;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#36807;&#22810;&#25110;&#36807;&#23569;&#30340;&#19981;&#21464;&#24615;&#37117;&#21487;&#33021;&#20250;&#24433;&#21709;&#32467;&#26524;&#65292;&#27491;&#30830;&#30340;&#19981;&#21464;&#24615;&#31243;&#24230;&#22312;&#20808;&#39564;&#20013;&#26159;&#26410;&#30693;&#30340;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#23454;&#20363;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#24212;&#35813;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#36866;&#24403;&#30340;&#19981;&#21464;&#24615;&#65292;&#24182;&#22312;&#27979;&#35797;&#26102;&#25512;&#26029;&#12290;&#25105;&#20204;&#23558;&#19981;&#21464;&#24615;&#35270;&#20026;&#19968;&#20010;&#39044;&#27979;&#38382;&#39064;&#12290;&#32473;&#23450;&#20219;&#20309;&#22270;&#20687;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24402;&#19968;&#21270;&#27969;&#26469;&#39044;&#27979;&#21464;&#25442;&#30340;&#20998;&#24067;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#39044;&#27979;&#36827;&#34892;&#24179;&#22343;&#12290;&#30001;&#20110;&#36825;&#20010;&#20998;&#24067;&#20165;&#21462;&#20915;&#20110;&#23454;&#20363;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#20998;&#31867;&#20043;&#21069;&#23545;&#23454;&#20363;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#22312;&#31867;&#21035;&#20043;&#38388;&#25512;&#24191;&#19981;&#21464;&#24615;&#12290;&#21516;&#26679;&#30340;&#20998;&#24067;&#20063;&#21487;&#20197;&#29992;&#20110;&#36866;&#24212;&#36229;&#20986;&#20998;&#24067;&#30340;&#23039;&#21183;&#12290;&#36825;&#20010;&#24402;&#19968;&#21270;&#27969;&#26159;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20064;&#27604;Augerino&#21644;InstaAug&#26356;&#22810;&#33539;&#22260;&#30340;&#21464;&#25442;&#12290;&#24403;&#29992;&#20316;&#25968;&#25454;&#22686;&#24378;&#26102;&#65292;&#25105;&#20204;&#30340;m
&lt;/p&gt;
&lt;p&gt;
Computer vision research has long aimed to build systems that are robust to spatial transformations found in natural data. Traditionally, this is done using data augmentation or hard-coding invariances into the architecture. However, too much or too little invariance can hurt, and the correct amount is unknown a priori and dependent on the instance. Ideally, the appropriate invariance would be learned from data and inferred at test-time.  We treat invariance as a prediction problem. Given any image, we use a normalizing flow to predict a distribution over transformations and average the predictions over them. Since this distribution only depends on the instance, we can align instances before classifying them and generalize invariance across classes. The same distribution can also be used to adapt to out-of-distribution poses. This normalizing flow is trained end-to-end and can learn a much larger range of transformations than Augerino and InstaAug. When used as data augmentation, our m
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SO3krates&#30340;&#27431;&#20960;&#37324;&#24471;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#23427;&#36890;&#36807;&#32452;&#21512;&#31232;&#30095;&#31561;&#21464;&#34920;&#31034;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#21147;&#22330;&#20013;&#23454;&#29616;&#20102;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#36895;&#24230;&#30340;&#29420;&#29305;&#32452;&#21512;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#21069;&#25152;&#26410;&#26377;&#30340;&#26102;&#38388;&#21644;&#31995;&#32479;&#23610;&#24230;&#19978;&#23545;&#29289;&#36136;&#30340;&#37327;&#23376;&#23646;&#24615;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.15126</link><description>&lt;p&gt;
&#20174;&#32957;&#21040;&#32435;&#31859;&#32467;&#26500;&#65306;&#19968;&#31181;&#29992;&#20110;&#24555;&#36895;&#31283;&#23450;&#30340;&#26426;&#22120;&#23398;&#20064;&#21147;&#22330;&#30340;&#27431;&#20960;&#37324;&#24471;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
From Peptides to Nanostructures: A Euclidean Transformer for Fast and Stable Machine Learned Force Fields. (arXiv:2309.15126v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15126
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SO3krates&#30340;&#27431;&#20960;&#37324;&#24471;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#23427;&#36890;&#36807;&#32452;&#21512;&#31232;&#30095;&#31561;&#21464;&#34920;&#31034;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#21147;&#22330;&#20013;&#23454;&#29616;&#20102;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#36895;&#24230;&#30340;&#29420;&#29305;&#32452;&#21512;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#21069;&#25152;&#26410;&#26377;&#30340;&#26102;&#38388;&#21644;&#31995;&#32479;&#23610;&#24230;&#19978;&#23545;&#29289;&#36136;&#30340;&#37327;&#23376;&#23646;&#24615;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#20174;&#22836;&#35745;&#31639;&#30340;&#26426;&#22120;&#23398;&#20064;&#21147;&#22330;&#65288;MLFFs&#65289;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#23613;&#31649;&#22312;&#27979;&#35797;&#35823;&#24046;&#19978;&#21462;&#24471;&#20102;&#36739;&#20302;&#30340;&#25928;&#26524;&#65292;&#20294;MLFFs&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#20013;&#30340;&#36866;&#29992;&#24615;&#36234;&#26469;&#36234;&#21463;&#21040;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#20854;&#31283;&#23450;&#24615;&#21463;&#21040;&#36136;&#30097;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;MLFFs&#20013;&#30340;&#31561;&#21464;&#34920;&#31034;&#19982;MD&#27169;&#25311;&#31283;&#23450;&#24615;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#28508;&#22312;&#32852;&#31995;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#21487;&#33021;&#38480;&#21046;&#20102;&#23427;&#20204;&#24102;&#26469;&#30340;&#23454;&#38469;&#20248;&#21183;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;SO3krates&#30340;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#23427;&#32467;&#21512;&#20102;&#31232;&#30095;&#31561;&#21464;&#34920;&#31034;&#65288;&#27431;&#20960;&#37324;&#24471;&#21464;&#37327;&#65289;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#21487;&#20197;&#20998;&#31163;&#19981;&#21464;&#21644;&#31561;&#21464;&#20449;&#24687;&#65292;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#24352;&#37327;&#31215;&#25805;&#20316;&#12290;SO3krates&#23454;&#29616;&#20102;&#31934;&#24230;&#12289;&#31283;&#23450;&#24615;&#21644;&#36895;&#24230;&#30340;&#29420;&#29305;&#32452;&#21512;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#22312;&#21069;&#25152;&#26410;&#26377;&#30340;&#26102;&#38388;&#21644;&#31995;&#32479;&#23610;&#24230;&#19978;&#23545;&#29289;&#36136;&#30340;&#37327;&#23376;&#23646;&#24615;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen vast progress in the development of machine learned force fields (MLFFs) based on ab-initio reference calculations. Despite achieving low test errors, the suitability of MLFFs in molecular dynamics (MD) simulations is being increasingly scrutinized due to concerns about instability. Our findings suggest a potential connection between MD simulation stability and the presence of equivariant representations in MLFFs, but their computational cost can limit practical advantages they would otherwise bring.  To address this, we propose a transformer architecture called SO3krates that combines sparse equivariant representations (Euclidean variables) with a self-attention mechanism that can separate invariant and equivariant information, eliminating the need for expensive tensor products. SO3krates achieves a unique combination of accuracy, stability, and speed that enables insightful analysis of quantum properties of matter on unprecedented time and system size scales. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20934;&#33945;&#29305;&#21345;&#27931;&#65288;QMC&#65289;&#26041;&#27861;&#29992;&#20110;&#19977;&#32500;&#20999;&#29255;Wasserstein&#65288;SW&#65289;&#30340;&#36817;&#20284;&#35745;&#31639;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#26041;&#27861;&#22312;&#19977;&#32500;&#21333;&#20301;&#36229;&#29699;&#38754;&#19978;&#26500;&#36896;&#20102;QMC&#28857;&#38598;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#23558;QSW&#25193;&#23637;&#20026;&#38543;&#26426;&#20934;&#20999;&#29255;Wasserstein&#65288;RQSW&#65289;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.11713</link><description>&lt;p&gt;
&#19977;&#32500;&#20999;&#29255;Wasserstein&#30340;&#20934;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quasi-Monte Carlo for 3D Sliced Wasserstein. (arXiv:2309.11713v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20934;&#33945;&#29305;&#21345;&#27931;&#65288;QMC&#65289;&#26041;&#27861;&#29992;&#20110;&#19977;&#32500;&#20999;&#29255;Wasserstein&#65288;SW&#65289;&#30340;&#36817;&#20284;&#35745;&#31639;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#26041;&#27861;&#22312;&#19977;&#32500;&#21333;&#20301;&#36229;&#29699;&#38754;&#19978;&#26500;&#36896;&#20102;QMC&#28857;&#38598;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#23558;QSW&#25193;&#23637;&#20026;&#38543;&#26426;&#20934;&#20999;&#29255;Wasserstein&#65288;RQSW&#65289;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Monte Carlo (MC)&#26041;&#27861;&#34987;&#29992;&#20316;&#35745;&#31639;&#20999;&#29255;Wasserstein (SW)&#36317;&#31163;&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#22312;&#20998;&#26512;&#24418;&#24335;&#20013;&#20855;&#26377;&#26840;&#25163;&#30340;&#26399;&#26395;&#12290;&#28982;&#32780;&#65292;MC&#26041;&#27861;&#22312;&#26368;&#23567;&#21270;&#32477;&#23545;&#36817;&#20284;&#35823;&#24046;&#26041;&#38754;&#24182;&#19981;&#20248;&#21270;&#12290;&#20026;&#20102;&#25552;&#20379;&#26356;&#22909;&#30340;&#32463;&#39564;SW&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20934;&#33945;&#29305;&#21345;&#27931;&#65288;QMC&#65289;&#26041;&#27861;&#30340;&#20934;&#20999;&#29255;Wasserstein&#65288;QSW&#65289;&#36924;&#36817;&#12290;&#20026;&#20102;&#23545;SW&#30340;QMC&#36827;&#34892;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#19977;&#32500;&#35774;&#32622;&#65292;&#29305;&#21035;&#26159;&#35745;&#31639;&#19977;&#32500;&#27010;&#29575;&#27979;&#24230;&#20043;&#38388;&#30340;SW&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#20102;&#22312;&#19977;&#32500;&#21333;&#20301;&#36229;&#29699;&#38754;&#19978;&#26500;&#36896;QMC&#28857;&#38598;&#30340;&#22810;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#39640;&#26031;&#30340;&#26144;&#23556;&#65292;&#31561;&#38754;&#31215;&#26144;&#23556;&#65292;&#24191;&#20041;&#34746;&#26059;&#28857;&#21644;&#26368;&#20248;&#21270;&#24046;&#24322;&#33021;&#37327;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#33719;&#24471;&#38543;&#26426;&#20248;&#21270;&#30340;&#26080;&#20559;&#20272;&#35745;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#25152;&#35752;&#35770;&#30340;&#20302;&#32500;&#35774;&#32622;&#20013;&#24341;&#20837;&#38543;&#26426;&#24615;&#65292;&#23558;QSW&#25193;&#23637;&#20026;&#38543;&#26426;&#20934;&#20999;&#29255;Wasserstein&#65288;RQSW&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monte Carlo (MC) approximation has been used as the standard computation approach for the Sliced Wasserstein (SW) distance, which has an intractable expectation in its analytical form. However, the MC method is not optimal in terms of minimizing the absolute approximation error. To provide a better class of empirical SW, we propose quasi-sliced Wasserstein (QSW) approximations that rely on Quasi-Monte Carlo (QMC) methods. For a comprehensive investigation of QMC for SW, we focus on the 3D setting, specifically computing the SW between probability measures in three dimensions. In greater detail, we empirically verify various ways of constructing QMC points sets on the 3D unit-hypersphere, including Gaussian-based mapping, equal area mapping, generalized spiral points, and optimizing discrepancy energies. Furthermore, to obtain an unbiased estimation for stochastic optimization, we extend QSW into Randomized Quasi-Sliced Wasserstein (RQSW) by introducing randomness to the discussed low-d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25512;&#24191;&#20102;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#28201;&#24230;&#25351;&#25968;&#27979;&#24230;&#20013;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#26377;&#25928;&#30340;&#31639;&#27861;&#21644;&#21487;&#25511;&#30340;&#31232;&#30095;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04015</link><description>&lt;p&gt;
&#20351;&#29992;&#28201;&#24230;&#25351;&#25968;&#27979;&#24230;&#30340;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport with Tempered Exponential Measures. (arXiv:2309.04015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25512;&#24191;&#20102;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#28201;&#24230;&#25351;&#25968;&#27979;&#24230;&#20013;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#26377;&#25928;&#30340;&#31639;&#27861;&#21644;&#21487;&#25511;&#30340;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#20248;&#36755;&#36816;&#39046;&#22495;&#20013;&#65292;&#20004;&#20010;&#37325;&#35201;&#30340;&#23376;&#39046;&#22495;&#30456;&#20114;&#23545;&#31435;&#65306;&#65288;i&#65289;&#38750;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#65292;&#8220;&#21345;&#25176;&#35834;&#32500;&#22855;&#26041;&#24335;&#8221;&#65292;&#23548;&#33268;&#20102;&#38750;&#24120;&#31232;&#30095;&#30340;&#35268;&#21010;&#65292;&#20294;&#31639;&#27861;&#25928;&#29575;&#36739;&#20302;&#65307;&#65288;ii&#65289;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#65292;&#8220;&#36763;&#20811;&#38669;&#24681;-&#24211;&#37117;&#37324;&#26041;&#24335;&#8221;&#65292;&#33719;&#24471;&#20102;&#36817;&#20284;&#32447;&#24615;&#31639;&#27861;&#65292;&#20294;&#26368;&#22823;&#31243;&#24230;&#19978;&#26080;&#27861;&#31232;&#30095;&#35268;&#21010;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#21518;&#19968;&#31181;&#26041;&#27861;&#25512;&#24191;&#21040;&#28201;&#24230;&#25351;&#25968;&#27979;&#24230;&#65292;&#21363;&#20855;&#26377;&#38388;&#25509;&#27979;&#24230;&#24402;&#19968;&#21270;&#30340;&#25351;&#25968;&#26063;&#27867;&#21270;&#65292;&#21462;&#24471;&#20102;&#38750;&#24120;&#26041;&#20415;&#30340;&#25240;&#20013;&#25928;&#26524;&#65292;&#20855;&#26377;&#38750;&#24120;&#24555;&#30340;&#36817;&#20284;&#31639;&#27861;&#21644;&#21487;&#25511;&#30340;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20063;&#36866;&#29992;&#20110;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of optimal transport, two prominent subfields face each other: (i) unregularized optimal transport, ``\`a-la-Kantorovich'', which leads to extremely sparse plans but with algorithms that scale poorly, and (ii) entropic-regularized optimal transport, ``\`a-la-Sinkhorn-Cuturi'', which gets near-linear approximation algorithms but leads to maximally un-sparse plans. In this paper, we show that a generalization of the latter to tempered exponential measures, a generalization of exponential families with indirect measure normalization, gets to a very convenient middle ground, with both very fast approximation algorithms and sparsity which is under control up to sparsity patterns. In addition, it fits naturally in the unbalanced optimal transport problem setting as well.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24102;&#26377;&#27745;&#26579;&#25968;&#25454;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21435;&#27745;&#21644;&#21464;&#37327;&#20381;&#36182;&#24314;&#27169;&#23454;&#29616;&#20102;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#23545;&#20110;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.12563</link><description>&lt;p&gt;
&#38024;&#23545;&#24102;&#26377;&#27745;&#26579;&#25968;&#25454;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65306;&#23545;&#29983;&#29702;&#20449;&#21495;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multivariate Time-Series Anomaly Detection with Contaminated Data: Application to Physiological Signals. (arXiv:2308.12563v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12563
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24102;&#26377;&#27745;&#26579;&#25968;&#25454;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21435;&#27745;&#21644;&#21464;&#37327;&#20381;&#36182;&#24314;&#27169;&#23454;&#29616;&#20102;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#23545;&#20110;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#27969;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#36890;&#24120;&#22312;&#23398;&#26415;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#21463;&#21040;&#25511;&#21046;&#23454;&#39564;&#26465;&#20214;&#19979;&#30340;&#28165;&#27905;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#23427;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#21253;&#21547;&#22122;&#22768;&#30340;&#25361;&#25112;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#26412;&#30740;&#31350;&#22312;&#24863;&#30693;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65288;TSAD&#65289;&#20013;&#28145;&#20837;&#30740;&#31350;&#20102;&#26631;&#31614;&#32423;&#22122;&#22768;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#23454;&#29992;&#30340;&#31471;&#21040;&#31471;&#26080;&#30417;&#30563;TSAD&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#20013;&#21253;&#21547;&#24322;&#24120;&#30340;&#24773;&#20917;&#19979;&#12290;&#35813;&#26041;&#27861;&#31216;&#20026;TSAD-C&#65292;&#20854;&#22312;&#35757;&#32451;&#38454;&#27573;&#19981;&#38656;&#35201;&#35775;&#38382;&#24322;&#24120;&#26631;&#31614;&#12290;TSAD-C&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;&#19968;&#20010;&#21435;&#27745;&#22120;&#29992;&#20110;&#32416;&#27491;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#24322;&#24120;&#65288;&#20063;&#31216;&#20026;&#22122;&#22768;&#65289;&#65292;&#19968;&#20010;&#21464;&#37327;&#20381;&#36182;&#24314;&#27169;&#27169;&#22359;&#29992;&#20110;&#25429;&#25417;&#21435;&#27745;&#21518;&#25968;&#25454;&#20013;&#30340;&#38271;&#26399;&#20869;&#37096;&#21644;&#36328;&#21464;&#37327;&#20381;&#36182;&#20851;&#31995;&#65292;&#21487;&#20197;&#35270;&#20026;&#26367;&#20195;&#24615;&#30340;&#24322;&#24120;&#24615;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mainstream unsupervised anomaly detection algorithms often excel in academic datasets, yet their real-world performance is restricted due to the controlled experimental conditions involving clean training data. Addressing the challenge of training with noise, a prevalent issue in practical anomaly detection, is frequently overlooked. In a pioneering endeavor, this study delves into the realm of label-level noise within sensory time-series anomaly detection (TSAD). This paper presents a novel and practical end-to-end unsupervised TSAD when the training data are contaminated with anomalies. The introduced approach, called TSAD-C, is devoid of access to abnormality labels during the training phase. TSAD-C encompasses three modules: a Decontaminator to rectify the abnormalities (aka noise) present in the training data, a Variable Dependency Modeling module to capture both long-term intra- and inter-variable dependencies within the decontaminated data that can be considered as a surrogate o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#19978;&#65292;ZSP&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;ChatGPT&#65292;F1&#24471;&#20998;&#25552;&#39640;&#20102;40%&#12290;</title><link>http://arxiv.org/abs/2308.07876</link><description>&lt;p&gt;
&#36890;&#36807;&#32534;&#30721;&#26412;&#30693;&#35782;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;ChatGPT&#26469;&#21512;&#25104;&#25919;&#27835;&#38646;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Political Zero-Shot Relation Classification via Codebook Knowledge, NLI, and ChatGPT. (arXiv:2308.07876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#19978;&#65292;ZSP&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;ChatGPT&#65292;F1&#24471;&#20998;&#25552;&#39640;&#20102;40%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#20107;&#20214;&#32534;&#30721;&#30340;&#30417;&#30563;&#27169;&#22411;&#22312;&#24615;&#33021;&#26041;&#38754;&#36828;&#36828;&#36229;&#36807;&#27169;&#24335;&#21305;&#37197;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20165;&#20165;&#20381;&#36182;&#20110;&#26032;&#30340;&#27880;&#37322;&#65292;&#24573;&#35270;&#20102;&#19987;&#23478;&#25968;&#25454;&#24211;&#20013;&#30340;&#22823;&#37327;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#32454;&#31890;&#24230;&#20998;&#31867;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#27880;&#37322;&#32534;&#30721;&#26412;&#30340;&#30693;&#35782;&#65292;&#25506;&#32034;&#38646;&#26679;&#26412;&#26041;&#27861;&#29992;&#20110;&#25919;&#27835;&#20107;&#20214;&#26412;&#20307;&#20851;&#31995;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;ChatGPT&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ZSP&#12290;ZSP&#37319;&#29992;&#20102;&#19968;&#31181;&#26641;&#26597;&#35810;&#26694;&#26550;&#65292;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#19978;&#19979;&#25991;&#12289;&#35821;&#24577;&#21644;&#31867;&#21035;&#28040;&#27495;&#30340;&#19981;&#21516;&#23618;&#27425;&#12290;&#35813;&#26694;&#26550;&#25552;&#39640;&#20102;&#35299;&#37322;&#24615;&#12289;&#25928;&#29575;&#21644;&#23545;&#27169;&#24335;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#22312;&#25105;&#20204;&#26032;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;ChatGPT&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#31361;&#20986;&#20102;ZSP&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;ZSP&#22312;&#32454;&#31890;&#24230;&#26681;&#20195;&#30721;&#20998;&#31867;&#30340;F1&#24471;&#20998;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25552;&#39640;40%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent supervised models for event coding vastly outperform pattern-matching methods. However, their reliance solely on new annotations disregards the vast knowledge within expert databases, hindering their applicability to fine-grained classification. To address these limitations, we explore zero-shot approaches for political event ontology relation classification, by leveraging knowledge from established annotation codebooks. Our study encompasses both ChatGPT and a novel natural language inference (NLI) based approach named ZSP. ZSP adopts a tree-query framework that deconstructs the task into context, modality, and class disambiguation levels. This framework improves interpretability, efficiency, and adaptability to schema changes. By conducting extensive experiments on our newly curated datasets, we pinpoint the instability issues within ChatGPT and highlight the superior performance of ZSP. ZSP achieves an impressive 40% improvement in F1 score for fine-grained Rootcode classific
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25645;&#24314;&#19968;&#24231;&#26725;&#26753;&#65292;&#23558;&#21518;&#26399;&#35299;&#37322;&#24615;&#21644;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#37322;&#22797;&#26434;&#30340;&#40657;&#30418;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#24182;&#35299;&#20915;&#35299;&#37322;&#30340;&#24544;&#23454;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#20043;&#38388;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15007</link><description>&lt;p&gt;
&#21487;&#39564;&#35777;&#30340;&#29305;&#24449;&#24402;&#22240;&#65306;&#21518;&#26399;&#35299;&#37322;&#24615;&#21644;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#30340;&#26725;&#26753;
&lt;/p&gt;
&lt;p&gt;
Verifiable Feature Attributions: A Bridge between Post Hoc Explainability and Inherent Interpretability. (arXiv:2307.15007v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25645;&#24314;&#19968;&#24231;&#26725;&#26753;&#65292;&#23558;&#21518;&#26399;&#35299;&#37322;&#24615;&#21644;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#37322;&#22797;&#26434;&#30340;&#40657;&#30418;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#24182;&#35299;&#20915;&#35299;&#37322;&#30340;&#24544;&#23454;&#24615;&#21644;&#21487;&#39564;&#35777;&#24615;&#20043;&#38388;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#22686;&#21152;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#19968;&#30452;&#24378;&#35843;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#35299;&#37322;&#38656;&#27714;&#12290;&#20026;&#27492;&#65292;&#22312;&#20197;&#21069;&#30340;&#25991;&#29486;&#20013;&#27010;&#36848;&#20102;&#20004;&#31181;&#24191;&#27867;&#30340;&#31574;&#30053;&#26469;&#35299;&#37322;&#27169;&#22411;&#12290;&#21518;&#26399;&#35299;&#37322;&#26041;&#27861;&#36890;&#36807;&#31361;&#20986;&#26174;&#31034;&#23545;&#27169;&#22411;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#30340;&#29305;&#24449;&#26469;&#35299;&#37322;&#22797;&#26434;&#30340;&#40657;&#30418;&#27169;&#22411;&#30340;&#34892;&#20026;&#65307;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#36825;&#20123;&#35299;&#37322;&#21487;&#33021;&#19981;&#24544;&#23454;&#65292;&#26356;&#20196;&#20154;&#25285;&#24551;&#30340;&#26159;&#25105;&#20204;&#26080;&#27861;&#39564;&#35777;&#23427;&#20204;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20869;&#22312;&#21487;&#35299;&#37322;&#27169;&#22411;&#36890;&#36807;&#23558;&#35299;&#37322;&#24615;&#20449;&#24687;&#26126;&#30830;&#32534;&#30721;&#21040;&#27169;&#22411;&#26550;&#26500;&#20013;&#26469;&#35268;&#36991;&#36825;&#20123;&#38382;&#39064;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#30340;&#35299;&#37322;&#33258;&#28982;&#24544;&#23454;&#19988;&#21487;&#39564;&#35777;&#65292;&#20294;&#30001;&#20110;&#20854;&#26377;&#38480;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23427;&#20204;&#36890;&#24120;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25645;&#24314;&#19968;&#24231;&#26725;&#26753;&#65292;&#23558;&#21518;&#26399;&#35299;&#37322;&#24615;&#21644;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increased deployment of machine learning models in various real-world applications, researchers and practitioners alike have emphasized the need for explanations of model behaviour. To this end, two broad strategies have been outlined in prior literature to explain models. Post hoc explanation methods explain the behaviour of complex black-box models by highlighting features that are critical to model predictions; however, prior work has shown that these explanations may not be faithful, and even more concerning is our inability to verify them. Specifically, it is nontrivial to evaluate if a given attribution is correct with respect to the underlying model. Inherently interpretable models, on the other hand, circumvent these issues by explicitly encoding explanations into model architecture, meaning their explanations are naturally faithful and verifiable, but they often exhibit poor predictive performance due to their limited expressive power. In this work, we aim to bridge t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#21487;&#38752;&#32780;&#39640;&#25928;&#22320;&#36951;&#24536;&#25968;&#25454;&#23454;&#20363;&#24182;&#20445;&#25345;&#20844;&#24179;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.14754</link><description>&lt;p&gt;
&#20844;&#24179;&#26426;&#22120;&#36951;&#24536;&#65306;&#22312;&#20943;&#23569;&#24046;&#24322;&#30340;&#21516;&#26102;&#21024;&#38500;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Fair Machine Unlearning: Data Removal while Mitigating Disparities. (arXiv:2307.14754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#21487;&#38752;&#32780;&#39640;&#25928;&#22320;&#36951;&#24536;&#25968;&#25454;&#23454;&#20363;&#24182;&#20445;&#25345;&#20844;&#24179;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20844;&#20247;&#23545;&#20225;&#19994;&#25910;&#38598;&#21644;&#20351;&#29992;&#20010;&#20154;&#20449;&#24687;&#30340;&#24847;&#35782;&#22686;&#24378;&#65292;&#28040;&#36153;&#32773;&#31215;&#26497;&#21442;&#19982;&#20225;&#19994;&#25968;&#25454;&#38598;&#30340;&#31649;&#29702;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25968;&#25454;&#31649;&#29702;&#26694;&#26550;&#65288;&#22914;&#27431;&#27954;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#65289;&#24050;&#32463;&#25552;&#20986;&#20102;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#65292;&#20801;&#35768;&#20010;&#20154;&#35831;&#27714;&#23558;&#20854;&#20010;&#20154;&#25968;&#25454;&#20174;&#32452;&#32455;&#20351;&#29992;&#30340;&#25968;&#25454;&#24211;&#21644;&#27169;&#22411;&#20013;&#21024;&#38500;&#12290;&#20026;&#20102;&#23454;&#29616;&#36951;&#24536;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26426;&#22120;&#23398;&#20064;&#36951;&#24536;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#27599;&#20010;&#36951;&#24536;&#35831;&#27714;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#38382;&#39064;&#12290;&#34429;&#28982;&#36825;&#20123;&#22312;&#32447;&#26367;&#20195;&#26041;&#26696;&#21487;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#36951;&#24536;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#20854;&#20182;&#20851;&#38190;&#30340;&#23454;&#38469;&#24212;&#29992;&#23646;&#24615;&#65288;&#22914;&#20844;&#24179;&#24615;&#65289;&#30340;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#21487;&#38752;&#32780;&#39640;&#25928;&#22320;&#36951;&#24536;&#25968;&#25454;&#23454;&#20363;&#24182;&#20445;&#25345;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As public consciousness regarding the collection and use of personal information by corporations grows, it is of increasing importance that consumers be active participants in the curation of corporate datasets. In light of this, data governance frameworks such as the General Data Protection Regulation (GDPR) have outlined the right to be forgotten as a key principle allowing individuals to request that their personal data be deleted from the databases and models used by organizations. To achieve forgetting in practice, several machine unlearning methods have been proposed to address the computational inefficiencies of retraining a model from scratch with each unlearning request. While efficient online alternatives to retraining, it is unclear how these methods impact other properties critical to real-world applications, such as fairness. In this work, we propose the first fair machine unlearning method that can provably and efficiently unlearn data instances while preserving group fai
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#24378;&#30423;&#21453;&#39304;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#65292;&#33021;&#20934;&#30830;&#35780;&#20272;&#22870;&#21169;&#19982;&#31995;&#32479;&#20581;&#24247;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.11655</link><description>&lt;p&gt;
&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bandits with Deterministically Evolving States. (arXiv:2307.11655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#24378;&#30423;&#21453;&#39304;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#65292;&#33021;&#20934;&#30830;&#35780;&#20272;&#22870;&#21169;&#19982;&#31995;&#32479;&#20581;&#24247;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#19982;&#24378;&#30423;&#21453;&#39304;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#30830;&#23450;&#24615;&#28436;&#21270;&#21644;&#19981;&#21487;&#35266;&#27979;&#30340;&#29366;&#24577;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20027;&#35201;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#30340;&#23398;&#20064;&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#22312;&#27599;&#19968;&#36718;&#33719;&#24471;&#30340;&#22870;&#21169;&#26159;&#36873;&#25321;&#34892;&#21160;&#30340;&#30701;&#26399;&#22870;&#21169;&#21644;&#31995;&#32479;&#30340;&#8220;&#20581;&#24247;&#8221;&#31243;&#24230;&#65288;&#21363;&#36890;&#36807;&#20854;&#29366;&#24577;&#27979;&#37327;&#65289;&#30340;&#20989;&#25968;&#12290;&#20363;&#22914;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#24179;&#21488;&#20174;&#29992;&#25143;&#23545;&#29305;&#23450;&#31867;&#22411;&#20869;&#23481;&#30340;&#21442;&#19982;&#20013;&#33719;&#24471;&#30340;&#22870;&#21169;&#19981;&#20165;&#21462;&#20915;&#20110;&#20855;&#20307;&#20869;&#23481;&#30340;&#22266;&#26377;&#29305;&#24449;&#65292;&#36824;&#21462;&#20915;&#20110;&#29992;&#25143;&#19982;&#24179;&#21488;&#19978;&#20854;&#20182;&#31867;&#22411;&#20869;&#23481;&#20114;&#21160;&#21518;&#20854;&#20559;&#22909;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#30340;&#36890;&#29992;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#955;&#8712;[0,1]&#65288;&#20363;&#22914;&#65292;&#29992;&#25143;&#30340;&#20559;&#22909;&#22240;&#20808;&#21069;&#20869;&#23481;&#28040;&#36153;&#32780;&#24555;&#36895;&#21464;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a model for learning with bandit feedback while accounting for deterministically evolving and unobservable states that we call Bandits with Deterministically Evolving States. The workhorse applications of our model are learning for recommendation systems and learning for online ads. In both cases, the reward that the algorithm obtains at each round is a function of the short-term reward of the action chosen and how ``healthy'' the system is (i.e., as measured by its state). For example, in recommendation systems, the reward that the platform obtains from a user's engagement with a particular type of content depends not only on the inherent features of the specific content, but also on how the user's preferences have evolved as a result of interacting with other types of content on the platform. Our general model accounts for the different rate $\lambda \in [0,1]$ at which the state evolves (e.g., how fast a user's preferences shift as a result of previous content consumption
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#22270;&#30340;&#30452;&#35273;&#27169;&#31946;RVFL&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#37319;&#29992;&#21152;&#26435;&#26426;&#21046;&#22788;&#29702;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#22270;&#23884;&#20837;&#25552;&#21462;&#35821;&#20041;&#20016;&#23500;&#30340;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07881</link><description>&lt;p&gt;
&#23884;&#20837;&#22270;&#30340;&#30452;&#35273;&#27169;&#31946;RVFL&#29992;&#20110;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Embedded Intuitionistic Fuzzy RVFL for Class Imbalance Learning. (arXiv:2307.07881v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07881
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#22270;&#30340;&#30452;&#35273;&#27169;&#31946;RVFL&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#37319;&#29992;&#21152;&#26435;&#26426;&#21046;&#22788;&#29702;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#22270;&#23884;&#20837;&#25552;&#21462;&#35821;&#20041;&#20016;&#23500;&#30340;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#38754;&#20020;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#21363;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#65292;&#22312;&#23545;&#23569;&#25968;&#31867;&#21035;&#36827;&#34892;&#20934;&#30830;&#20998;&#31867;&#26041;&#38754;&#23384;&#22312;&#24456;&#22823;&#30340;&#22256;&#38590;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#33021;&#23548;&#33268;&#20559;&#21521;&#20110;&#22810;&#25968;&#31867;&#21035;&#30340;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20248;&#20808;&#32771;&#34385;&#65292;&#20174;&#32780;&#23548;&#33268;&#23569;&#25968;&#31867;&#21035;&#30340;&#20195;&#34920;&#19981;&#36275;&#12290;&#38543;&#26426;&#21521;&#37327;&#20989;&#25968;&#38142;&#25509;&#65288;RVFL&#65289;&#32593;&#32476;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#19988;&#26377;&#25928;&#30340;&#20998;&#31867;&#23398;&#20064;&#27169;&#22411;&#65292;&#22240;&#20854;&#36895;&#24230;&#21644;&#25928;&#29575;&#39640;&#32780;&#21463;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#26102;&#65292;&#23427;&#30340;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23884;&#20837;&#22270;&#30340;&#30452;&#35273;&#27169;&#31946;RVFL&#29992;&#20110;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#65288;GE-IFRVFL-CIL&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#21152;&#26435;&#26426;&#21046;&#26469;&#22788;&#29702;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#12290;&#25152;&#25552;&#20986;&#30340;GE-IFRVFL-CIL&#27169;&#22411;&#20855;&#26377;&#35768;&#22810;&#20248;&#28857;&#65292;&#20363;&#22914;&#65306;&#65288;i&#65289;&#23427;&#21033;&#29992;&#22270;&#23884;&#20837;&#20174;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#35821;&#20041;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#65288;ii&#65289;&#23427;&#20351;&#29992;&#30452;&#35273;&#27169;&#31946;&#38598;&#26469;&#36827;&#34892;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The domain of machine learning is confronted with a crucial research area known as class imbalance learning, which presents considerable hurdles in the precise classification of minority classes. This issue can result in biased models where the majority class takes precedence in the training process, leading to the underrepresentation of the minority class. The random vector functional link (RVFL) network is a widely-used and effective learning model for classification due to its speed and efficiency. However, it suffers from low accuracy when dealing with imbalanced datasets. To overcome this limitation, we propose a novel graph embedded intuitionistic fuzzy RVFL for class imbalance learning (GE-IFRVFL-CIL) model incorporating a weighting mechanism to handle imbalanced datasets. The proposed GE-IFRVFL-CIL model has a plethora of benefits, such as $(i)$ it leverages graph embedding to extract semantically rich information from the dataset, $(ii)$ it uses intuitionistic fuzzy sets to ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#21644;&#20462;&#21098;&#36825;&#20004;&#31181;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#37327;&#21270;&#20248;&#20110;&#20462;&#21098;&#12290;</title><link>http://arxiv.org/abs/2307.02973</link><description>&lt;p&gt;
&#20462;&#21098;&#19982;&#37327;&#21270;&#65306;&#21738;&#20010;&#26356;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
Pruning vs Quantization: Which is Better?. (arXiv:2307.02973v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#21644;&#20462;&#21098;&#36825;&#20004;&#31181;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#37327;&#21270;&#20248;&#20110;&#20462;&#21098;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#21644;&#37327;&#21270;&#25216;&#26415;&#20960;&#20046;&#21644;&#31070;&#32463;&#32593;&#32476;&#26412;&#36523;&#19968;&#26679;&#21476;&#32769;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#21482;&#26377;&#20004;&#32773;&#20043;&#38388;&#30340;&#20020;&#26102;&#27604;&#36739;&#21457;&#34920;&#36807;&#12290;&#26412;&#25991;&#26088;&#22312;&#22238;&#31572;&#21738;&#20010;&#26356;&#22909;&#65306;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#36824;&#26159;&#20462;&#21098;&#65311;&#36890;&#36807;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24076;&#26395;&#20026;&#31070;&#32463;&#32593;&#32476;&#30828;&#20214;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20449;&#24687;&#12290;&#25105;&#20204;&#23545;&#21387;&#32553;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36825;&#20004;&#31181;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#19968;&#33324;&#25968;&#25454;&#20998;&#24067;&#30340;&#26399;&#26395;&#37327;&#21270;&#21644;&#20462;&#21098;&#35823;&#24046;&#36827;&#34892;&#20102;&#20998;&#26512;&#27604;&#36739;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#20013;&#27599;&#23618;&#20462;&#21098;&#21644;&#37327;&#21270;&#35823;&#24046;&#30340;&#19979;&#30028;&#65292;&#24182;&#23558;&#20854;&#19982;&#20248;&#21270;&#21518;&#30340;&#32463;&#39564;&#35823;&#24046;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;3&#20010;&#20219;&#21153;&#19978;&#30340;8&#20010;&#22823;&#35268;&#27169;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#37327;&#21270;&#20248;&#20110;&#20462;&#21098;&#12290;&#21482;&#26377;&#22312;&#19968;&#20123;&#26497;&#39640;&#21387;&#32553;&#27604;&#30340;&#24773;&#20917;&#19979;&#65292;&#20462;&#21098;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network pruning and quantization techniques are almost as old as neural networks themselves. However, to date only ad-hoc comparisons between the two have been published. In this paper, we set out to answer the question on which is better: neural network quantization or pruning? By answering this question, we hope to inform design decisions made on neural network hardware going forward. We provide an extensive comparison between the two techniques for compressing deep neural networks. First, we give an analytical comparison of expected quantization and pruning error for general data distributions. Then, we provide lower bounds for the per-layer pruning and quantization error in trained networks, and compare these to empirical error after optimization. Finally, we provide an extensive experimental comparison for training 8 large-scale models on 3 tasks. Our results show that in most cases quantization outperforms pruning. Only in some scenarios with very high compression ratio, p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#24179;&#34913;&#26368;&#22823;&#21270;&#25237;&#36164;&#32452;&#21512;&#25910;&#20837;&#21644;&#26368;&#23567;&#21270;&#20934;&#22791;&#37329;&#30340;&#23545;&#25239;&#30446;&#26631;&#65292;&#33258;&#21160;&#21270;&#23547;&#25214;&#26368;&#20248;&#20449;&#29992;&#21345;&#39069;&#24230;&#35843;&#25972;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.15585</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#23545;&#25239;&#30446;&#26631;&#19979;&#30340;&#20449;&#29992;&#39069;&#24230;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Optimizing Credit Limit Adjustments Under Adversarial Goals Using Reinforcement Learning. (arXiv:2306.15585v1 [q-fin.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#24179;&#34913;&#26368;&#22823;&#21270;&#25237;&#36164;&#32452;&#21512;&#25910;&#20837;&#21644;&#26368;&#23567;&#21270;&#20934;&#22791;&#37329;&#30340;&#23545;&#25239;&#30446;&#26631;&#65292;&#33258;&#21160;&#21270;&#23547;&#25214;&#26368;&#20248;&#20449;&#29992;&#21345;&#39069;&#24230;&#35843;&#25972;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#22312;&#24456;&#22810;&#38382;&#39064;&#20013;&#24471;&#21040;&#24212;&#29992;&#65292;&#20174;&#20855;&#26377;&#30830;&#23450;&#24615;&#29615;&#22659;&#30340;&#35270;&#39057;&#28216;&#25103;&#21040;&#20855;&#26377;&#38543;&#26426;&#22330;&#26223;&#30340;&#25237;&#36164;&#32452;&#21512;&#21644;&#36816;&#33829;&#31649;&#29702;&#65307;&#28982;&#32780;&#65292;&#22312;&#38134;&#34892;&#38382;&#39064;&#20013;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#27979;&#35797;&#23581;&#35797;&#24456;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#25214;&#21040;&#24182;&#33258;&#21160;&#21270;&#26368;&#20248;&#20449;&#29992;&#21345;&#39069;&#24230;&#35843;&#25972;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30001;&#20110;&#26377;&#21382;&#21490;&#25968;&#25454;&#21487;&#29992;&#65292;&#25105;&#20204;&#32771;&#34385;&#27599;&#20010;&#23458;&#25143;&#30340;&#20004;&#31181;&#21487;&#33021;&#25805;&#20316;&#65292;&#21363;&#22686;&#21152;&#25110;&#20445;&#25345;&#20010;&#20154;&#24403;&#21069;&#30340;&#20449;&#29992;&#39069;&#24230;&#12290;&#20026;&#20102;&#25214;&#21040;&#36825;&#20010;&#31574;&#30053;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#36825;&#20010;&#20915;&#31574;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;&#20854;&#20013;&#26368;&#22823;&#21270;&#39044;&#26399;&#21033;&#28070;&#65307;&#22240;&#27492;&#65292;&#25105;&#20204;&#24179;&#34913;&#20102;&#20004;&#20010;&#23545;&#25239;&#30446;&#26631;&#65306;&#26368;&#22823;&#21270;&#25237;&#36164;&#32452;&#21512;&#30340;&#25910;&#20837;&#21644;&#26368;&#23567;&#21270;&#25237;&#36164;&#32452;&#21512;&#30340;&#20934;&#22791;&#37329;&#12290;&#20854;&#27425;&#65292;&#32771;&#34385;&#21040;&#25105;&#20204;&#38382;&#39064;&#30340;&#29305;&#27530;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#31163;&#32447;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#27169;&#25311;&#34892;&#21160;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning has been explored for many problems, from video games with deterministic environments to portfolio and operations management in which scenarios are stochastic; however, there have been few attempts to test these methods in banking problems. In this study, we sought to find and automatize an optimal credit card limit adjustment policy by employing reinforcement learning techniques. In particular, because of the historical data available, we considered two possible actions per customer, namely increasing or maintaining an individual's current credit limit. To find this policy, we first formulated this decision-making question as an optimization problem in which the expected profit was maximized; therefore, we balanced two adversarial goals: maximizing the portfolio's revenue and minimizing the portfolio's provisions. Second, given the particularities of our problem, we used an offline learning strategy to simulate the impact of the action based on historical data f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20301;&#29699;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#34920;&#31034;&#25512;&#21521;&#22266;&#23450;&#26041;&#21521;&#65292;&#20351;&#24471;&#23398;&#20064;&#31574;&#30053;&#23545;&#25968;&#25454;&#28418;&#31227;&#20855;&#26377;&#24377;&#24615;&#65292;&#20174;&#32780;&#33021;&#22815;&#24212;&#23545;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03364</link><description>&lt;p&gt;
&#22312;&#21333;&#20301;&#29699;&#19978;&#23398;&#20064;&#34920;&#31034;&#65306;&#24212;&#29992;&#20110;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Representations on the Unit Sphere: Application to Online Continual Learning. (arXiv:2306.03364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03364
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#20301;&#29699;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#34920;&#31034;&#25512;&#21521;&#22266;&#23450;&#26041;&#21521;&#65292;&#20351;&#24471;&#23398;&#20064;&#31574;&#30053;&#23545;&#25968;&#25454;&#28418;&#31227;&#20855;&#26377;&#24377;&#24615;&#65292;&#20174;&#32780;&#33021;&#22815;&#24212;&#23545;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#21407;&#29702;&#26469;&#23398;&#20064;&#20998;&#24067;&#22312;&#21333;&#20301;&#29699;&#19978;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#38024;&#23545;&#23545;&#31216;&#26041;&#21521;&#25968;&#25454;&#24314;&#31435;&#20102; von Mises-Fisher &#20998;&#24067;&#21644;&#35282;&#39640;&#26031;&#20998;&#24067;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#65292;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#34987;&#25512;&#21521;&#22266;&#23450;&#30340;&#26041;&#21521;&#65292;&#20351;&#24471;&#23398;&#20064;&#31574;&#30053;&#23545;&#25968;&#25454;&#28418;&#31227;&#20855;&#26377;&#24377;&#24615;&#12290;&#36825;&#20351;&#24471;&#23427;&#36866;&#21512;&#20110;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#65292;&#21363;&#22312;&#36830;&#32493;&#30340;&#25968;&#25454;&#27969;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#22810;&#20010;&#20998;&#31867;&#20219;&#21153;&#25353;&#39034;&#24207;&#21576;&#29616;&#65292;&#22240;&#27492;&#36807;&#21435;&#20219;&#21153;&#30340;&#25968;&#25454;&#19981;&#20877;&#21487;&#29992;&#65292;&#24403;&#21069;&#20219;&#21153;&#30340;&#25968;&#25454;&#21482;&#33021;&#30475;&#19968;&#27425;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#37197;&#22791;&#20102;&#25105;&#20204;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#36127;&#25968;&#25454;&#25110;&#20219;&#21153;&#36793;&#30028;&#30340;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#36739;&#23567;&#30340;&#25209;&#22788;&#29702;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We use the maximum a posteriori estimation principle for learning representations distributed on the unit sphere. We derive loss functions for the von Mises-Fisher distribution and the angular Gaussian distribution, both designed for modeling symmetric directional data. A noteworthy feature of our approach is that the learned representations are pushed toward fixed directions, allowing for a learning strategy that is resilient to data drift. This makes it suitable for online continual learning, which is the problem of training neural networks on a continuous data stream, where multiple classification tasks are presented sequentially so that data from past tasks are no longer accessible, and data from the current task can be seen only once. To address this challenging scenario, we propose a memory-based representation learning technique equipped with our new loss functions. Our approach does not require negative data or knowledge of task boundaries and performs well with smaller batch s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DiffPack&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#22238;&#24402;&#30340;&#25197;&#36716;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25197;&#26354;&#31354;&#38388;&#20013;&#36827;&#34892;&#25193;&#25955;&#21644;&#21435;&#22122;&#26469;&#23398;&#20064;&#20391;&#38142;&#25197;&#36716;&#35282;&#24230;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#20174;&#32780;&#20934;&#30830;&#22320;&#39044;&#27979;&#34507;&#30333;&#36136;&#20391;&#38142;&#30340;&#26500;&#35937;&#65292;&#26377;&#25928;&#24212;&#29992;&#20110;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#12289;&#35774;&#35745;&#21644;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2306.01794</link><description>&lt;p&gt;
DiffPack&#65306;&#33258;&#22238;&#24402;&#34507;&#30333;&#36136;&#20391;&#38142;&#21253;&#35013;&#30340;&#25197;&#36716;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiffPack: A Torsional Diffusion Model for Autoregressive Protein Side-Chain Packing. (arXiv:2306.01794v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DiffPack&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#22238;&#24402;&#30340;&#25197;&#36716;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25197;&#26354;&#31354;&#38388;&#20013;&#36827;&#34892;&#25193;&#25955;&#21644;&#21435;&#22122;&#26469;&#23398;&#20064;&#20391;&#38142;&#25197;&#36716;&#35282;&#24230;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#20174;&#32780;&#20934;&#30830;&#22320;&#39044;&#27979;&#34507;&#30333;&#36136;&#20391;&#38142;&#30340;&#26500;&#35937;&#65292;&#26377;&#25928;&#24212;&#29992;&#20110;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#12289;&#35774;&#35745;&#21644;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#22312;&#29983;&#29289;&#21151;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#23427;&#20204;&#30340;&#19977;&#32500;&#32467;&#26500;&#23545;&#20110;&#20915;&#23450;&#23427;&#20204;&#30340;&#21151;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#20934;&#30830;&#22320;&#39044;&#27979;&#32473;&#23450;&#39592;&#26550;&#30340;&#34507;&#30333;&#36136;&#20391;&#38142;&#30340;&#26500;&#35937;&#23545;&#20110;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#12289;&#35774;&#35745;&#21644;&#34507;&#30333;&#36136;-&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#31561;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#65292;&#20934;&#30830;&#24230;&#26377;&#38480;&#65292;&#32780;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23558;&#38382;&#39064;&#35270;&#20026;&#22238;&#24402;&#20219;&#21153;&#65292;&#24573;&#30053;&#20102;&#22240;&#24120;&#37327;&#20849;&#20215;&#38190;&#38271;&#24230;&#21644;&#35282;&#24230;&#25152;&#38480;&#21046;&#32780;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiffPack&#65292;&#36825;&#26159;&#19968;&#20010;&#25197;&#36716;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25197;&#26354;&#31354;&#38388;&#19978;&#36827;&#34892;&#25193;&#25955;&#21644;&#21435;&#22122;&#65292;&#23398;&#20064;&#20391;&#38142;&#25197;&#36716;&#35282;&#24230;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#36825;&#26159;&#20391;&#38142;&#21253;&#35013;&#20013;&#21807;&#19968;&#30340;&#33258;&#30001;&#24230;&#12290;&#20026;&#20102;&#36991;&#20813;&#21516;&#26102;&#25200;&#21160;&#25152;&#26377;&#22235;&#20010;&#25197;&#36716;&#35282;&#24230;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#20174;\c {hi}1&#21040;\c {hi}4&#33258;&#22238;&#24402;&#29983;&#25104;&#22235;&#20010;&#25197;&#36716;&#35282;&#24230;&#65292;&#24182;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proteins play a critical role in carrying out biological functions, and their 3D structures are essential in determining their functions. Accurately predicting the conformation of protein side-chains given their backbones is important for applications in protein structure prediction, design and protein-protein interactions. Traditional methods are computationally intensive and have limited accuracy, while existing machine learning methods treat the problem as a regression task and overlook the restrictions imposed by the constant covalent bond lengths and angles. In this work, we present DiffPack, a torsional diffusion model that learns the joint distribution of side-chain torsional angles, the only degrees of freedom in side-chain packing, by diffusing and denoising on the torsional space. To avoid issues arising from simultaneous perturbation of all four torsional angles, we propose autoregressively generating the four torsional angles from \c{hi}1 to \c{hi}4 and training diffusion m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21478;&#19968;&#31181;&#24120;&#35265;&#12289;&#29616;&#23454;&#30340;&#22810;&#26234;&#33021;&#20307;RL&#25915;&#20987;&#35774;&#32622;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#25915;&#20987;&#32773;&#23545;&#20195;&#29702;$\alpha$&#25511;&#21046;&#30340;&#26356;&#19968;&#33324;&#21270;&#25915;&#20987;&#24418;&#24335;&#12290;&#24182;&#35299;&#20915;&#20102;&#20808;&#21069;&#25915;&#20987;&#27169;&#22411;&#20013;&#32570;&#20047;&#21487;&#35777;&#26126;&#38450;&#24481;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17342</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#23545;&#25239;&#31574;&#30053;&#65306;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24191;&#20041;&#25915;&#20987;&#24418;&#24335;&#21644;&#21487;&#35777;&#26126;&#30340;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in Multi-Agent RL. (arXiv:2305.17342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21478;&#19968;&#31181;&#24120;&#35265;&#12289;&#29616;&#23454;&#30340;&#22810;&#26234;&#33021;&#20307;RL&#25915;&#20987;&#35774;&#32622;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#25915;&#20987;&#32773;&#23545;&#20195;&#29702;$\alpha$&#25511;&#21046;&#30340;&#26356;&#19968;&#33324;&#21270;&#25915;&#20987;&#24418;&#24335;&#12290;&#24182;&#35299;&#20915;&#20102;&#20808;&#21069;&#25915;&#20987;&#27169;&#22411;&#20013;&#32570;&#20047;&#21487;&#35777;&#26126;&#38450;&#24481;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#30740;&#31350;&#30452;&#25509;&#25200;&#21160;&#21463;&#23475;&#32773;&#30340;&#29366;&#24577;/&#21160;&#20316;&#25110;&#22522;&#30784;&#36716;&#31227;&#21160;&#24577;&#20197;&#23637;&#31034;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#33030;&#24369;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#30452;&#25509;&#25805;&#32437;&#22312;&#23454;&#36341;&#20013;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#21478;&#19968;&#31181;&#24120;&#35265;&#19988;&#29616;&#23454;&#30340;&#25915;&#20987;&#35774;&#32622;&#65306;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#22810;&#26234;&#33021;&#20307;RL&#30340;&#35774;&#32622;&#20013;&#65292;&#22312;&#37096;&#32626;&#26399;&#38388;&#65292;&#21463;&#23475;&#20195;&#29702;$\nu$&#34987;&#25915;&#20987;&#32773;&#25511;&#21046;&#21478;&#19968;&#20010;&#20195;&#29702;$\alpha$&#20197;&#25932;&#23545;&#26041;&#24335;&#34892;&#21160;&#65292;&#20351;&#29992;&#8220;&#23545;&#25239;&#31574;&#30053;&#8221;&#23545;&#21463;&#23475;&#20195;&#29702;&#36827;&#34892;&#25915;&#20987;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#25915;&#20987;&#27169;&#22411;&#32771;&#34385;&#20102;&#36825;&#31181;&#35774;&#32622;&#65292;&#20294;&#20182;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#25915;&#20987;&#32773;&#21487;&#20197;&#36935;&#21040;&#25269;&#25239;&#65292;&#22240;&#27492;&#21482;&#33021;&#37096;&#20998;&#25511;&#21046;&#20195;&#29702;$\alpha$&#65292;&#21516;&#26102;&#24341;&#20837;&#21487;&#23519;&#35273;&#30340;&#8220;&#24322;&#24120;&#8221;&#34892;&#20026;&#65292;&#36825;&#20123;&#34892;&#20026;&#24456;&#23481;&#26131;&#34987;&#26816;&#27979;&#21040;&#12290;&#24182;&#19988;&#32570;&#20047;&#38024;&#23545;&#36825;&#20123;&#23545;&#25239;&#31574;&#30053;&#30340;&#21487;&#35777;&#26126;&#30340;&#38450;&#24481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#19968;&#33324;&#21270;&#30340;&#25915;&#20987;&#24418;&#24335;&#65292;&#27169;&#25311;&#20102;&#25915;&#20987;&#32773;&#22312;&#20309;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#25511;&#21046;&#20195;&#29702;$\alpha$&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing works consider direct perturbations of victim's state/action or the underlying transition dynamics to show vulnerability of reinforcement learning agents under adversarial attacks. However, such direct manipulation may not always be feasible in practice. In this paper, we consider another common and realistic attack setup: in a multi-agent RL setting with well-trained agents, during deployment time, the victim agent $\nu$ is exploited by an attacker who controls another agent $\alpha$ to act adversarially against the victim using an \textit{adversarial policy}. Prior attack models under such setup do not consider that the attacker can confront resistance and thus can only take partial control of the agent $\alpha$, as well as introducing perceivable ``abnormal'' behaviors that are easily detectable. A provable defense against these adversarial policies is also lacking. To resolve these issues, we introduce a more general attack formulation that models to what extent the a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MA-FL&#65292;&#24212;&#29992;&#24322;&#27493;&#27169;&#22411;&#20256;&#36755;&#20307;&#31995;&#32467;&#26500;&#26469;&#23454;&#29616;&#26377;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#38656;&#35201;&#35757;&#32451;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#26412;&#25991;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#25581;&#31034;&#20102;&#36164;&#28304;&#20998;&#37197;&#12289;&#35774;&#22791;&#35843;&#24230;&#21644;&#20010;&#20307;&#27169;&#22411;&#29366;&#24577;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;MA-FL&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#27169;&#22411;&#31934;&#24230;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13503</link><description>&lt;p&gt;
&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#24322;&#27493;&#22810;&#27169;&#22411;&#32852;&#37030;&#23398;&#20064;&#65306;&#29702;&#35770;&#12289;&#24314;&#27169;&#19982;&#20248;&#21270;(arXiv:2305.13503v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Asynchronous Multi-Model Federated Learning over Wireless Networks: Theory, Modeling, and Optimization. (arXiv:2305.13503v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MA-FL&#65292;&#24212;&#29992;&#24322;&#27493;&#27169;&#22411;&#20256;&#36755;&#20307;&#31995;&#32467;&#26500;&#26469;&#23454;&#29616;&#26377;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#38656;&#35201;&#35757;&#32451;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#26412;&#25991;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#25581;&#31034;&#20102;&#36164;&#28304;&#20998;&#37197;&#12289;&#35774;&#22791;&#35843;&#24230;&#21644;&#20010;&#20307;&#27169;&#22411;&#29366;&#24577;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;MA-FL&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#27169;&#22411;&#31934;&#24230;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#30446;&#21069;&#65292;&#32852;&#37030;&#23398;&#20064;&#30340;&#22823;&#37096;&#20998;&#25991;&#29486;&#37117;&#20851;&#27880;&#21333;&#19968;&#20219;&#21153;/&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#37319;&#29992;&#21516;&#27493;&#27169;&#22411;&#21442;&#25968;&#20256;&#36755;&#35774;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;MA-FL&#65292;&#23427;&#32771;&#34385;&#21033;&#29992;&#24322;&#27493;&#27169;&#22411;&#20256;&#36755;&#20307;&#31995;&#32467;&#26500;&#65292;&#23454;&#29616;&#26377;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#38656;&#35201;&#35757;&#32451;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#24341;&#20837;&#19968;&#26063;&#35843;&#24230;&#24352;&#37327;&#26469;&#25429;&#25417;&#35774;&#22791;&#30340;&#35843;&#24230;&#65292;&#24182;&#23545;MA-FL&#19979;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#25105;&#20204;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#25581;&#31034;&#20102;&#36164;&#28304;&#20998;&#37197;&#65288;&#20363;&#22914;&#65292;&#23567;&#25209;&#37327;&#22823;&#23567;&#21644;&#26799;&#24230;&#19979;&#38477;&#36845;&#20195;&#27425;&#25968;&#65289;&#12289;&#35774;&#22791;&#35843;&#24230;&#21644;&#20010;&#20307;&#27169;&#22411;&#29366;&#24577;&#65288;&#21363;&#39044;&#28909;&#19982;&#20919;&#21551;&#21160;&#21021;&#22987;&#21270;&#65289;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20026;MA-FL&#21046;&#23450;&#20102;&#19968;&#20010;&#38750;&#20984;&#28151;&#25972;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#29992;&#20110;&#20849;&#21516;&#37197;&#32622;&#36164;&#28304;&#20998;&#37197;&#21644;&#35774;&#22791;&#35843;&#24230;&#12290;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;MA-FL&#22312;&#25910;&#25947;&#36895;&#24230;&#21644;&#27169;&#22411;&#31934;&#24230;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has emerged as a key technique for distributed machine learning (ML). Most literature on FL has focused on systems with (i) ML model training for a single task/model, (ii) a synchronous setting for uplink/downlink transfer of model parameters, which is often unrealistic. To address this, we develop MA-FL, which considers FL with multiple downstream tasks to be trained over an asynchronous model transmission architecture. We first characterize the convergence of ML model training under MA-FL via introducing a family of scheduling tensors to capture the scheduling of devices. Our convergence analysis sheds light on the impact of resource allocation (e.g., the mini-batch size and number of gradient descent iterations), device scheduling, and individual model states (i.e., warmed vs. cold initialization) on the performance of ML models. We then formulate a non-convex mixed integer optimization problem for jointly configuring the resource allocation and device schedu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#35821;&#35328;&#20266;&#26631;&#27880;&#30340;&#26080;&#30417;&#30563;ASR&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#26631;&#27880;&#25968;&#25454;&#26469;&#24341;&#23548;&#26032;&#35821;&#35328;&#30340;&#26080;&#30417;&#30563;AM&#12290;&#22312;Common Voice&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#23454;&#29616;18% WER&#12290;&#32780;&#19988;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13330</link><description>&lt;p&gt;
&#22522;&#20110;&#36328;&#35821;&#35328;&#20266;&#26631;&#27880;&#30340;&#26080;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Unsupervised ASR via Cross-Lingual Pseudo-Labeling. (arXiv:2305.13330v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#35821;&#35328;&#20266;&#26631;&#27880;&#30340;&#26080;&#30417;&#30563;ASR&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#26631;&#27880;&#25968;&#25454;&#26469;&#24341;&#23548;&#26032;&#35821;&#35328;&#30340;&#26080;&#30417;&#30563;AM&#12290;&#22312;Common Voice&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#21487;&#20197;&#23454;&#29616;18% WER&#12290;&#32780;&#19988;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#19978;&#37117;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#38750;&#37197;&#23545;&#30340;&#38899;&#39057;&#21644;&#25991;&#26412;&#26469;&#35757;&#32451;&#26080;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#12290;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;ASR&#26041;&#27861;&#20551;&#23450;&#19981;&#33021;&#20351;&#29992;&#20219;&#20309;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#21363;&#20351;&#27809;&#26377;&#32473;&#23450;&#35821;&#35328;&#30340;&#20219;&#20309;&#26631;&#27880;&#38899;&#39057;&#65292;&#20063;&#22987;&#32456;&#21487;&#20197;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#26631;&#27880;&#25968;&#25454;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;&#30340;&#23383;&#31526;&#32423;&#22768;&#23398;&#27169;&#22411;&#65288;AM&#65289;&#65292;&#26469;&#24341;&#23548;&#26032;&#35821;&#35328;&#30340;&#26080;&#30417;&#30563;AM&#12290; &#36825;&#37324;&#65292;&#8220;&#26080;&#30417;&#30563;&#8221;&#24847;&#21619;&#30528;&#27809;&#26377;&#21487;&#29992;&#20110;&#30446;&#26631;&#35821;&#35328;&#30340;&#26631;&#27880;&#38899;&#39057;&#12290;&#26412;&#25991;&#30340;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#65288;i&#65289;&#20351;&#29992;&#20854;&#20182;&#35821;&#35328;AM&#29983;&#25104;&#8220;&#30446;&#26631;&#8221;&#35821;&#35328;&#30340;&#20266;&#26631;&#31614;&#65288;PLs&#65289;&#65307;&#65288;ii&#65289;&#20351;&#29992;&#8220;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#8221;&#38480;&#21046;&#36825;&#20123;PLs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Common Voice&#19978;&#38750;&#24120;&#26377;&#25928;&#65306;&#20363;&#22914;&#65292;&#23558;&#33521;&#35821;AM&#20256;&#36882;&#21040;&#26031;&#29926;&#24076;&#37324;&#35821;&#21487;&#20197;&#23454;&#29616;18&#65285;&#30340;WER&#12290; &#23427;&#36824;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#22522;&#20110;&#23383;&#31526;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that it is possible to train an $\textit{unsupervised}$ automatic speech recognition (ASR) system using only unpaired audio and text. Existing unsupervised ASR methods assume that no labeled data can be used for training. We argue that even if one does not have any labeled audio for a given language, there is $\textit{always}$ labeled data available for other languages. We show that it is possible to use character-level acoustic models (AMs) from other languages to bootstrap an $\textit{unsupervised}$ AM in a new language. Here, "unsupervised" means no labeled audio is available for the $\textit{target}$ language. Our approach is based on two key ingredients: (i) generating pseudo-labels (PLs) of the $\textit{target}$ language using some $\textit{other}$ language AM and (ii) constraining these PLs with a $\textit{target language model}$. Our approach is effective on Common Voice: e.g. transfer of English AM to Swahili achieves 18% WER. It also outperforms characte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36947;&#23545;&#40784;&#40065;&#26834;&#21452;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;Transformer&#32467;&#26500;&#21644;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#30340;&#24341;&#20837;&#65292;&#35299;&#20915;&#20102;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20851;&#38190;&#32570;&#28857;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.12095</link><description>&lt;p&gt;
&#20351;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#20877;&#27425;&#21331;&#36234;&#65306;&#36890;&#36947;&#23545;&#40784;&#40065;&#26834;&#21452;Transformer
&lt;/p&gt;
&lt;p&gt;
Make Transformer Great Again for Time Series Forecasting: Channel Aligned Robust Dual Transformer. (arXiv:2305.12095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36947;&#23545;&#40784;&#40065;&#26834;&#21452;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;Transformer&#32467;&#26500;&#21644;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#30340;&#24341;&#20837;&#65292;&#35299;&#20915;&#20102;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20851;&#38190;&#32570;&#28857;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;Transformer&#21644;MLP&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#20248;&#21183;&#12290;&#23613;&#31649;&#22312;NLP&#21644;CV&#26041;&#38754;&#33719;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#35768;&#22810;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;MLP&#30456;&#27604;&#65292;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#30340;&#25928;&#26524;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;Transformer&#65292;&#21363;&#36890;&#36947;&#23545;&#40784;&#40065;&#26834;&#21452;Transformer&#65288;CARD&#65289;&#65292;&#20197;&#35299;&#20915;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20851;&#38190;&#32570;&#28857;&#12290;&#39318;&#20808;&#65292;CARD&#24341;&#20837;&#20102;&#21452;Transformer&#32467;&#26500;&#65292;&#20351;&#20854;&#33021;&#22815;&#25429;&#25417;&#20449;&#21495;&#20043;&#38388;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#21644;&#22810;&#20010;&#21464;&#37327;&#22312;&#26102;&#38388;&#19978;&#30340;&#21160;&#24577;&#20381;&#36182;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#20943;&#36731;&#28508;&#22312;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#12290;&#36825;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#22522;&#20110;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21152;&#26435;&#39044;&#27979;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#23545;&#22810;&#20010;&#38271;&#26399;&#21644;&#30701;&#26399;&#39044;&#27979;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;CARD&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have demonstrated the great power of deep learning methods, particularly Transformer and MLP, for time series forecasting. Despite its success in NLP and CV, many studies found that Transformer is less effective than MLP for time series forecasting. In this work, we design a special Transformer, i.e., channel-aligned robust dual Transformer (CARD for short), that addresses key shortcomings of Transformer in time series forecasting. First, CARD introduces a dual Transformer structure that allows it to capture both temporal correlations among signals and dynamical dependence among multiple variables over time. Second, we introduce a robust loss function for time series forecasting to alleviate the potential overfitting issue. This new loss function weights the importance of forecasting over a finite horizon based on prediction uncertainties. Our evaluation of multiple long-term and short-term forecasting datasets demonstrates that CARD significantly outperforms state-of-th
&lt;/p&gt;</description></item><item><title>GSPNs&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#21487;&#35745;&#31639;&#22320;&#22238;&#31572;&#27010;&#29575;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#26435;&#37325;&#20849;&#20139;&#21644;&#26641;&#29366;&#35745;&#31639;&#22270;&#30340;&#20248;&#21183;&#33719;&#24471;&#20102;&#32431;&#27010;&#29575;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#28145;&#24230;&#22270;&#32593;&#32476;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10544</link><description>&lt;p&gt;
&#21487;&#35745;&#31639;&#30340;&#22522;&#20110;&#22270;&#35825;&#23548;&#30340;&#21644;&#31215;&#32593;&#32476;&#36827;&#34892;&#27010;&#29575;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Tractable Probabilistic Graph Representation Learning with Graph-Induced Sum-Product Networks. (arXiv:2305.10544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10544
&lt;/p&gt;
&lt;p&gt;
GSPNs&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#21487;&#35745;&#31639;&#22320;&#22238;&#31572;&#27010;&#29575;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#26435;&#37325;&#20849;&#20139;&#21644;&#26641;&#29366;&#35745;&#31639;&#22270;&#30340;&#20248;&#21183;&#33719;&#24471;&#20102;&#32431;&#27010;&#29575;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#28145;&#24230;&#22270;&#32593;&#32476;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#22270;&#35825;&#23548;&#30340;&#21644;&#31215;&#32593;&#32476; (GSPN)&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#21487;&#35745;&#31639;&#22320;&#22238;&#31572;&#27010;&#29575;&#26597;&#35810;&#12290;&#21463;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#20013;&#30001;&#39030;&#28857;&#24341;&#36215;&#30340;&#35745;&#31639;&#26641;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#32452;&#21644;&#31215;&#32593;&#32476;&#65288;SPN&#65289;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20854;&#20013;&#29238;SPN&#30340;&#21442;&#25968;&#26159;&#20854;&#23376;&#32423;&#30340;&#21518;&#39564;&#28151;&#21512;&#27010;&#29575;&#30340;&#21487;&#23398;&#20064;&#21464;&#25442;&#12290;&#30001;&#20110;&#26435;&#37325;&#20849;&#20139;&#21644;GSPN&#30340;&#26641;&#29366;&#35745;&#31639;&#22270;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#32431;&#27010;&#29575;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#28145;&#24230;&#22270;&#32593;&#32476;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;&#32570;&#20047;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#22270;&#20998;&#31867;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30456;&#23545;&#20110;&#27969;&#34892;&#30340;&#31070;&#32463;&#27169;&#22411;&#30340;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#36229;&#21442;&#25968;&#21644;&#27169;&#22411;&#22238;&#31572;&#27010;&#29575;&#26597;&#35810;&#30340;&#33021;&#21147;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Graph-Induced Sum-Product Networks (GSPNs), a new probabilistic framework for graph representation learning that can tractably answer probabilistic queries. Inspired by the computational trees induced by vertices in the context of message-passing neural networks, we build hierarchies of sum-product networks (SPNs) where the parameters of a parent SPN are learnable transformations of the a-posterior mixing probabilities of its children's sum units. Due to weight sharing and the tree-shaped computation graphs of GSPNs, we obtain the efficiency and efficacy of deep graph networks with the additional advantages of a purely probabilistic model. We show the model's competitiveness on scarce supervision scenarios, handling missing data, and graph classification in comparison to popular neural models. We complement the experiments with qualitative analyses on hyper-parameters and the model's ability to answer probabilistic queries.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#30001;&#23450;&#20041;&#25152;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.07303</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions. (arXiv:2305.07303v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#30001;&#23450;&#20041;&#25152;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#20998;&#24067;&#20449;&#24687;&#30340;&#31070;&#32463;&#35789;&#21521;&#37327;&#19968;&#30452;&#20197;&#26469;&#37117;&#33021;&#20026;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#26377;&#29992;&#30340;&#21547;&#20041;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20250;&#23548;&#33268;&#38590;&#20197;&#35299;&#37322;&#21644;&#25511;&#21046;&#30340;&#34920;&#31034;&#12290;&#30456;&#21453;&#65292;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20855;&#26377;&#36882;&#24402;&#30340;&#65292;&#33258;&#35828;&#26126;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#21487;&#20197;&#25903;&#25345;&#33021;&#22815;&#20445;&#30041;&#21521;&#37327;&#31354;&#38388;&#20013;&#26174;&#24335;&#27010;&#24565;&#20851;&#31995;&#21644;&#32422;&#26463;&#30340;&#26032;&#22411;&#34920;&#31034;&#23398;&#20064;&#33539; paradigm&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#12289;&#22810;&#20851;&#31995;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#26144;&#23556;&#23450;&#20041;&#21644;&#23450;&#20041;&#26415;&#35821;&#21450;&#20854;&#30456;&#24212;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#20165;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#35789;&#21521;&#37327;&#12290;&#36890;&#36807;&#33258;&#21160;&#20174;&#23450;&#20041;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#32763;&#35793;&#30446;&#26631;&#35268;&#33539;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26694;&#26550;&#19987;&#38376;&#35774;&#23450;&#20026;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#25429;&#33719;&#30001;&#23450;&#20041;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural-based word embeddings using solely distributional information have consistently produced useful meaning representations for downstream tasks. However, existing approaches often result in representations that are hard to interpret and control. Natural language definitions, on the other side, possess a recursive, self-explanatory semantic structure that can support novel representation learning paradigms able to preserve explicit conceptual relations and constraints in the vector space.  This paper proposes a neuro-symbolic, multi-relational framework to learn word embeddings exclusively from natural language definitions by jointly mapping defined and defining terms along with their corresponding semantic relations. By automatically extracting the relations from definitions corpora and formalising the learning problem via a translational objective, we specialise the framework in hyperbolic space to capture the hierarchical and multi-resolution structure induced by the definitions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36138;&#24515;&#31639;&#27861;&#36896;&#25104;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03515</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Learning Decision Trees with Gradient Descent. (arXiv:2305.03515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#36138;&#24515;&#31639;&#27861;&#36896;&#25104;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#26159;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#24120;&#35265;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#39640;&#24230;&#30340;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20915;&#31574;&#26641;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#26159;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#19968;&#31181;&#36138;&#23146;&#29983;&#38271;&#31639;&#27861;&#26469;&#23398;&#20064;&#20915;&#31574;&#26641;&#65292;&#22312;&#27599;&#20010;&#20869;&#37096;&#33410;&#28857;&#19978;&#23616;&#37096;&#26368;&#23567;&#21270;&#19981;&#32431;&#24230;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#36138;&#24515;&#36807;&#31243;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#20915;&#31574;&#26641;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#38590;&#20197;&#22788;&#29702;&#30340;&#36724;&#23545;&#40784;&#20915;&#31574;&#26641;&#30340;&#26032;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#21644;&#30452;&#36890;&#31639;&#23376;&#22312;&#23494;&#38598;&#30340;&#20915;&#31574;&#26641;&#34920;&#31034;&#19978;&#32852;&#21512;&#20248;&#21270;&#25152;&#26377;&#26641;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20108;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision Trees (DTs) are commonly used for many machine learning tasks due to their high degree of interpretability. However, learning a DT from data is a difficult optimization problem, as it is non-convex and non-differentiable. Therefore, common approaches learn DTs using a greedy growth algorithm that minimizes the impurity locally at each internal node. Unfortunately, this greedy procedure can lead to suboptimal trees. In this paper, we present a novel approach for learning hard, axis-aligned DTs with gradient descent. The proposed method uses backpropagation with a straight-through operator on a dense DT representation to jointly optimize all tree parameters. Our approach outperforms existing methods on binary classification benchmarks and achieves competitive results for multi-class tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#25237;&#24433;&#26041;&#27861;&#21644;&#20248;&#21270;&#26041;&#27861;&#27714;&#35299;&#20998;&#24067;&#24335;&#32447;&#24615;&#31995;&#32479;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#35282;&#24322;&#26500;&#24615;&#30340;&#20960;&#20309;&#27010;&#24565;&#65292;&#24182;&#23545;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;(APC&#21644;D-HBM)&#30340;&#25910;&#25947;&#36895;&#24230;&#36827;&#34892;&#20102;&#32422;&#26463;&#21644;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2304.10640</link><description>&lt;p&gt;
&#35770;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;&#20998;&#24067;&#24335;&#32447;&#24615;&#31995;&#32479;&#27714;&#35299;&#22120;&#25910;&#25947;&#36895;&#24230;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Effects of Data Heterogeneity on the Convergence Rates of Distributed Linear System Solvers. (arXiv:2304.10640v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#25237;&#24433;&#26041;&#27861;&#21644;&#20248;&#21270;&#26041;&#27861;&#27714;&#35299;&#20998;&#24067;&#24335;&#32447;&#24615;&#31995;&#32479;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#35282;&#24322;&#26500;&#24615;&#30340;&#20960;&#20309;&#27010;&#24565;&#65292;&#24182;&#23545;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;(APC&#21644;D-HBM)&#30340;&#25910;&#25947;&#36895;&#24230;&#36827;&#34892;&#20102;&#32422;&#26463;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#35299;&#20915;&#22823;&#35268;&#27169;&#32447;&#24615;&#26041;&#31243;&#32452;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20219;&#21153;&#36127;&#36131;&#20154;&#25171;&#31639;&#22312;&#19968;&#32452;&#20855;&#26377;&#19968;&#20123;&#26041;&#31243;&#32452;&#23376;&#38598;&#30340;&#26426;&#22120;&#30340;&#20998;&#24067;&#24335;/&#32852;&#21512;&#24110;&#21161;&#19979;&#35299;&#20915;&#35813;&#31995;&#32479;&#30340;&#35774;&#32622;&#12290;&#34429;&#28982;&#26377;&#20960;&#31181;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#32570;&#23569;&#23545;&#25237;&#24433;&#26041;&#27861;&#21644;&#20248;&#21270;&#26041;&#27861;&#25910;&#25947;&#36895;&#24230;&#30340;&#20005;&#26684;&#27604;&#36739;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#24182;&#27604;&#36739;&#36825;&#20004;&#31867;&#31639;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#27599;&#20010;&#31867;&#21035;&#20013;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21363;&#26368;&#36817;&#25552;&#20986;&#30340;&#21152;&#36895;&#25237;&#24433;&#19968;&#33268;&#24615;(APC)&#21644;&#20998;&#24067;&#24335;&#37325;&#29699;&#26041;&#27861;(D-HBM)&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#31216;&#20026;&#35282;&#24322;&#26500;&#24615;&#30340;&#20960;&#20309;&#27010;&#24565;&#65292;&#24182;&#35752;&#35770;&#20854;&#26222;&#36941;&#24615;&#12290;&#20351;&#29992;&#35813;&#27010;&#24565;&#65292;&#25105;&#20204;&#32422;&#26463;&#24182;&#27604;&#36739;&#25152;&#30740;&#31350;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#25429;&#25417;&#20004;&#31181;&#26041;&#27861;&#30340;&#24322;&#26500;&#25968;&#25454;&#30340;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the fundamental problem of solving a large-scale system of linear equations. In particular, we consider the setting where a taskmaster intends to solve the system in a distributed/federated fashion with the help of a set of machines, who each have a subset of the equations. Although there exist several approaches for solving this problem, missing is a rigorous comparison between the convergence rates of the projection-based methods and those of the optimization-based ones. In this paper, we analyze and compare these two classes of algorithms with a particular focus on the most efficient method from each class, namely, the recently proposed Accelerated Projection-Based Consensus (APC) and the Distributed Heavy-Ball Method (D-HBM). To this end, we first propose a geometric notion of data heterogeneity called angular heterogeneity and discuss its generality. Using this notion, we bound and compare the convergence rates of the studied algorithms and capture the effects of both 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#23610;&#24230;MIL&#31639;&#27861;&#65292;&#23558;&#36328;&#23610;&#24230;&#20851;&#31995;&#26174;&#24335;&#32858;&#21512;&#21040;&#19968;&#20010;&#30149;&#29702;&#22270;&#20687;&#35786;&#26029;&#30340;MIL&#32593;&#32476;&#20013;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#24573;&#30053;&#23545;&#20154;&#31867;&#30149;&#29702;&#23398;&#23478;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#30340;&#36328;&#23610;&#24230;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00216</link><description>&lt;p&gt;
&#30149;&#29702;&#22270;&#20687;&#35786;&#26029;&#30340;&#36328;&#23610;&#24230;&#22810;&#23454;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-scale Multi-instance Learning for Pathological Image Diagnosis. (arXiv:2304.00216v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#23610;&#24230;MIL&#31639;&#27861;&#65292;&#23558;&#36328;&#23610;&#24230;&#20851;&#31995;&#26174;&#24335;&#32858;&#21512;&#21040;&#19968;&#20010;&#30149;&#29702;&#22270;&#20687;&#35786;&#26029;&#30340;MIL&#32593;&#32476;&#20013;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#24573;&#30053;&#23545;&#20154;&#31867;&#30149;&#29702;&#23398;&#23478;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#30340;&#36328;&#23610;&#24230;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#65292;&#36328;&#22810;&#20010;&#23610;&#24230;&#20998;&#26512;&#39640;&#20998;&#36776;&#29575;&#30340;&#20840;&#24133;&#22270;&#20687; (WSIs) &#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#22810;&#23454;&#20363;&#23398;&#20064; (MIL) &#26159;&#21033;&#29992;&#20998;&#31867;&#23545;&#35937;&#38598; (&#20363;&#22914;&#36739;&#23567;&#30340;&#22270;&#20687;&#22359;&#38598;) &#23545;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#36827;&#34892;&#22788;&#29702;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22788;&#29702;&#36890;&#24120;&#22312;WSIs&#30340;&#21333;&#20010;&#23610;&#24230;&#65288;&#20363;&#22914;20&#20493;&#25918;&#22823;&#65289;&#19978;&#36827;&#34892;&#65292;&#24573;&#30053;&#20102;&#23545;&#20154;&#31867;&#30149;&#29702;&#23398;&#23478;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#30340;&#36328;&#23610;&#24230;&#20449;&#24687;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#23610;&#24230;MIL&#31639;&#27861;&#65292;&#23558;&#36328;&#23610;&#24230;&#20851;&#31995;&#26174;&#24335;&#32858;&#21512;&#21040;&#19968;&#20010;&#30149;&#29702;&#22270;&#20687;&#35786;&#26029;&#30340;MIL&#32593;&#32476;&#20013;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;(1) &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#23610;&#24230;MIL (CS-MIL)&#31639;&#27861;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#23610;&#24230;&#20449;&#24687;&#21644;&#36328;&#23610;&#24230;&#20851;&#31995;&#65307;(2) &#21019;&#24314;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#29609;&#20855;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#23610;&#24230;&#29305;&#24322;&#24615;&#24418;&#24577;&#29305;&#24449;&#65292;&#20197;&#26816;&#26597;&#21644;&#21487;&#35270;&#21270;&#19981;&#21516;&#30340;&#36328;&#23610;&#24230;&#20851;&#31995;&#65307;(3)&#22312;&#22235;&#20010;WSI&#30340;&#32454;&#32990;&#32954;&#30284;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;CS-MIL&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing high resolution whole slide images (WSIs) with regard to information across multiple scales poses a significant challenge in digital pathology. Multi-instance learning (MIL) is a common solution for working with high resolution images by classifying bags of objects (i.e. sets of smaller image patches). However, such processing is typically performed at a single scale (e.g., 20x magnification) of WSIs, disregarding the vital inter-scale information that is key to diagnoses by human pathologists. In this study, we propose a novel cross-scale MIL algorithm to explicitly aggregate inter-scale relationships into a single MIL network for pathological image diagnosis. The contribution of this paper is three-fold: (1) A novel cross-scale MIL (CS-MIL) algorithm that integrates the multi-scale information and the inter-scale relationships is proposed; (2) A toy dataset with scale-specific morphological features is created and released to examine and visualize differential cross-scale a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#26631;&#20934;InfoNCE&#25439;&#22833;&#19979;&#30340;&#23545;&#27604;&#23398;&#20064;&#31561;&#21516;&#20110;&#30456;&#20284;&#24615;&#22270;&#19978;&#30340;&#35889;&#32858;&#31867;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20869;&#22312;&#31561;&#20215;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#36825;&#31181;&#20998;&#26512;&#25193;&#23637;&#21040;CLIP&#27169;&#22411;&#65292;&#25552;&#20986;&#26032;&#30340;&#26680;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.15103</link><description>&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#30456;&#20284;&#24615;&#22270;&#35889;&#19978;&#30340;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning Is Spectral Clustering On Similarity Graph. (arXiv:2303.15103v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#26631;&#20934;InfoNCE&#25439;&#22833;&#19979;&#30340;&#23545;&#27604;&#23398;&#20064;&#31561;&#21516;&#20110;&#30456;&#20284;&#24615;&#22270;&#19978;&#30340;&#35889;&#32858;&#31867;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20869;&#22312;&#31561;&#20215;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#36825;&#31181;&#20998;&#26512;&#25193;&#23637;&#21040;CLIP&#27169;&#22411;&#65292;&#25552;&#20986;&#26032;&#30340;&#26680;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#23545;&#20854;&#36816;&#20316;&#21407;&#29702;&#21644;&#21407;&#22240;&#30340;&#29702;&#35770;&#29702;&#35299;&#26377;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#26631;&#20934;InfoNCE&#25439;&#22833;&#19979;&#30340;&#23545;&#27604;&#23398;&#20064;&#31561;&#21516;&#20110;&#30456;&#20284;&#24615;&#22270;&#19978;&#30340;&#35889;&#32858;&#31867;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20869;&#22312;&#31561;&#20215;&#24615;&#12290;&#21033;&#29992;&#36825;&#31181;&#31561;&#20215;&#24615;&#20316;&#20026;&#22522;&#30707;&#65292;&#25105;&#20204;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;CLIP&#27169;&#22411;&#65292;&#24182;&#20005;&#26684;&#25551;&#36848;&#22810;&#27169;&#24577;&#23545;&#35937;&#22914;&#20309;&#34987;&#23884;&#20837;&#21040;&#19968;&#36215;&#12290;&#22312;&#29702;&#35770;&#27934;&#35265;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#28151;&#21512;&#25439;&#22833;&#65292;&#32467;&#21512;&#26032;&#39062;&#30340;&#26680;&#20989;&#25968;&#65292;&#22312;&#22810;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26631;&#20934;&#39640;&#26031;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning is a powerful self-supervised learning method, but we have a limited theoretical understanding of how it works and why it works. In this paper, we prove that contrastive learning with the standard InfoNCE loss is equivalent to spectral clustering on the similarity graph. Using this equivalence as the building block, we extend our analysis to the CLIP model and rigorously characterize how similar multi-modal objects are embedded together. Motivated by our theoretical insights, we introduce the kernel mixture loss, incorporating novel kernel functions that outperform the standard Gaussian kernel on several vision datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20960;&#20046;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#31995;&#32479;&#20013;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#20197;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.08431</link><description>&lt;p&gt;
&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#25910;&#25947;&#20110;&#20960;&#20046;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#30340;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Policy Gradient Converges to the Globally Optimal Policy for Nearly Linear-Quadratic Regulators. (arXiv:2303.08431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20960;&#20046;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#31995;&#32479;&#20013;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#20197;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#32773;&#21482;&#33719;&#24471;&#20102;&#38750;&#23436;&#25972;&#20449;&#24687;&#30340;&#38750;&#32447;&#24615;&#25511;&#21046;&#31995;&#32479;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#20960;&#20046;&#32447;&#24615;&#20108;&#27425;&#22411;&#35843;&#33410;&#22120;&#31995;&#32479;&#20013;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#21160;&#24577;&#31995;&#32479;&#65292;&#32467;&#21512;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#30001;&#30456;&#21516;&#32467;&#26500;&#30340;&#31574;&#30053;&#36827;&#34892;&#31649;&#29702;&#12290;&#22312;&#20551;&#35774;&#38750;&#32447;&#24615;&#32452;&#25104;&#37096;&#20998;&#21253;&#21547;&#20855;&#26377;&#23567;&#22411;Lipschitz&#31995;&#25968;&#30340;&#20869;&#26680;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23545;&#25104;&#26412;&#20989;&#25968;&#30340;&#20248;&#21270;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#34429;&#28982;&#25104;&#26412;&#20989;&#25968;&#36890;&#24120;&#26159;&#38750;&#20984;&#30340;&#65292;&#20294;&#25105;&#20204;&#30830;&#31435;&#20102;&#20840;&#23616;&#26368;&#20248;&#35299;&#38468;&#36817;&#23616;&#37096;&#30340;&#24378;&#20984;&#24615;&#21644;&#20809;&#28369;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21021;&#22987;&#21270;&#26426;&#21046;&#65292;&#20197;&#21033;&#29992;&#36825;&#20123;&#23646;&#24615;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonlinear control systems with partial information to the decision maker are prevalent in a variety of applications. As a step toward studying such nonlinear systems, this work explores reinforcement learning methods for finding the optimal policy in the nearly linear-quadratic regulator systems. In particular, we consider a dynamic system that combines linear and nonlinear components, and is governed by a policy with the same structure. Assuming that the nonlinear component comprises kernels with small Lipschitz coefficients, we characterize the optimization landscape of the cost function. Although the cost function is nonconvex in general, we establish the local strong convexity and smoothness in the vicinity of the global optimizer. Additionally, we propose an initialization mechanism to leverage these properties. Building on the developments, we design a policy gradient algorithm that is guaranteed to converge to the globally optimal policy with a linear rate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DGAI&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#22909;&#25163;&#33218;&#35782;&#21035;&#38382;&#39064;&#20013;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#24335;&#20943;&#23569;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#22312;&#20855;&#26377;&#32473;&#23450;&#38408;&#20540;&#30340;&#24773;&#20917;&#19979;&#36827;&#19968;&#27493;&#25552;&#39640;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07154</link><description>&lt;p&gt;
&#19981;&#21516;&#30340;&#22909;&#25163;&#33218;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Differential Good Arm Identification. (arXiv:2303.07154v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DGAI&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#22909;&#25163;&#33218;&#35782;&#21035;&#38382;&#39064;&#20013;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#24335;&#20943;&#23569;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#22312;&#20855;&#26377;&#32473;&#23450;&#38408;&#20540;&#30340;&#24773;&#20917;&#19979;&#36827;&#19968;&#27493;&#25552;&#39640;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19968;&#31181;&#21464;&#20307;&#30340;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#31216;&#20043;&#20026;&#22909;&#25163;&#33218;&#35782;&#21035;&#65288;GAI&#65289;&#12290; GAI&#26159;&#19968;&#20010;&#32431;&#25506;&#32034;&#30340;&#36172;&#21338;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#23613;&#21487;&#33021;&#23569;&#30340;&#26679;&#26412;&#25968;&#19979;&#36755;&#20986;&#23613;&#21487;&#33021;&#22810;&#30340;&#22909;&#25163;&#33218;&#65292;&#20854;&#20013;&#22909;&#25163;&#33218;&#34987;&#23450;&#20041;&#20026;&#20854;&#26399;&#26395;&#22870;&#21169;&#22823;&#20110;&#32473;&#23450;&#38408;&#20540;&#30340;&#25163;&#33218;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;DGAI-&#19968;&#31181;&#21487;&#24494;&#30340;&#22909;&#25163;&#33218;&#35782;&#21035;&#31639;&#27861;&#65292;&#20197;&#25968;&#25454;&#39537;&#21160;&#26041;&#24335;&#25913;&#36827;&#20102;&#29616;&#26377;&#25216;&#26415;HDoC&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290; &#25105;&#20204;&#36824;&#23637;&#31034;&#20102;DGAI&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;&#36890;&#29992;&#22810;&#33218;&#36172;&#21338;&#65288;MAB&#65289;&#38382;&#39064;&#30340;&#24615;&#33021;&#65292;&#32473;&#23450;&#19968;&#20010;&#38408;&#20540;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#24212;&#29992;&#20110;&#25163;&#33218;&#38598;&#12290; &#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#20013;&#30340;GAI&#21644;MAB&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper targets a variant of the stochastic multi-armed bandit problem called good arm identification (GAI). GAI is a pure-exploration bandit problem with the goal to output as many good arms using as few samples as possible, where a good arm is defined as an arm whose expected reward is greater than a given threshold. In this work, we propose DGAI - a differentiable good arm identification algorithm to improve the sample complexity of the state-of-the-art HDoC algorithm in a data-driven fashion. We also showed that the DGAI can further boost the performance of a general multi-arm bandit (MAB) problem given a threshold as a prior knowledge to the arm set. Extensive experiments confirm that our algorithm outperform the baseline algorithms significantly in both synthetic and real world datasets for both GAI and MAB tasks.
&lt;/p&gt;</description></item><item><title>Box$^2$EL&#26041;&#27861;&#36890;&#36807;&#23558;&#27010;&#24565;&#21644;&#35282;&#33394;&#34920;&#31034;&#20026;&#30418;&#23376;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#35282;&#33394;&#34920;&#31034;&#21463;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.11118</link><description>&lt;p&gt;
Box$^2$EL: EL++&#25551;&#36848;&#36923;&#36753;&#20013;&#30340;&#27010;&#24565;&#21644;&#35282;&#33394;&#30418;&#23376;&#23884;&#20837;&#30340;&#27010;&#24565;&#21644;&#35282;&#33394;&#30418;&#23376;&#23884;&#20837;&#26041;&#27861;&#21450;&#20854;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Box$^2$EL: Concept and Role Box Embeddings for the Description Logic EL++. (arXiv:2301.11118v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11118
&lt;/p&gt;
&lt;p&gt;
Box$^2$EL&#26041;&#27861;&#36890;&#36807;&#23558;&#27010;&#24565;&#21644;&#35282;&#33394;&#34920;&#31034;&#20026;&#30418;&#23376;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#35282;&#33394;&#34920;&#31034;&#21463;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25551;&#36848;&#36923;&#36753;&#26412;&#20307;&#35770;&#25193;&#23637;&#20102;&#30693;&#35782;&#22270;&#35889;&#19982;&#27010;&#24565;&#20449;&#24687;&#21644;&#36923;&#36753;&#32972;&#26223;&#30693;&#35782;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#36825;&#31181;&#26412;&#20307;&#35770;&#30340;&#24402;&#32435;&#25512;&#29702;&#25216;&#26415;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#65292;&#36825;&#20123;&#25216;&#26415;&#26377;&#26395;&#34917;&#20805;&#20256;&#32479;&#30340;&#28436;&#32462;&#25512;&#29702;&#31639;&#27861;&#12290;&#31867;&#20284;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#23436;&#21892;&#65292;&#29616;&#26377;&#30340;&#19968;&#20123;&#26041;&#27861;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#23398;&#20064;&#26412;&#20307;&#35770;&#23884;&#20837;&#65292;&#21516;&#26102;&#30830;&#20445;&#36825;&#20123;&#23884;&#20837;&#33021;&#22815;&#20934;&#30830;&#22320;&#25429;&#25417;&#21040;&#24213;&#23618;&#25551;&#36848;&#36923;&#36753;&#30340;&#36923;&#36753;&#35821;&#20041;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#21463;&#38480;&#30340;&#35282;&#33394;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Box$^2$EL&#26041;&#27861;&#65292;&#23558;&#27010;&#24565;&#21644;&#35282;&#33394;&#37117;&#34920;&#31034;&#20026;&#30418;&#23376;&#65288;&#21363;&#36724;&#23545;&#40784;&#36229;&#30697;&#24418;&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#20811;&#26381;&#20043;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#12290;&#20316;&#20026;&#25105;&#20204;&#35780;&#20272;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Description logic (DL) ontologies extend knowledge graphs (KGs) with conceptual information and logical background knowledge. In recent years, there has been growing interest in inductive reasoning techniques for such ontologies, which promise to complement classical deductive reasoning algorithms. Similar to KG completion, several existing approaches learn ontology embeddings in a latent space, while additionally ensuring that they faithfully capture the logical semantics of the underlying DL. However, they suffer from several shortcomings, mainly due to a limiting role representation. We propose Box$^2$EL, which represents both concepts and roles as boxes (i.e., axis-aligned hyperrectangles) and demonstrate how it overcomes the limitations of previous methods. We theoretically prove the soundness of our model and conduct an extensive experimental evaluation, achieving state-of-the-art results across a variety of datasets. As part of our evaluation, we introduce a novel benchmark for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;JKO&#26041;&#26696;&#30340;&#21487;&#36870;&#24402;&#19968;&#21270;&#27969;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#25353;&#22359;&#36827;&#34892;&#27531;&#24046;&#22359;&#30340;&#35757;&#32451;&#65292;&#20943;&#23569;&#20102;&#20869;&#23384;&#36127;&#36733;&#21644;&#28145;&#24230;&#27969;&#32593;&#32476;&#35757;&#32451;&#30340;&#38590;&#24230;&#12290;&#24182;&#19988;&#36890;&#36807;&#33258;&#36866;&#24212;&#26102;&#38388;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#27969;&#32593;&#32476;&#65292;&#22312;&#27010;&#29575;&#31354;&#38388;&#20013;&#36880;&#27493;&#32454;&#21270;&#36712;&#36857;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.14424</link><description>&lt;p&gt;
&#22522;&#20110;JKO&#26041;&#26696;&#30340;&#21487;&#36870;&#24402;&#19968;&#21270;&#27969;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Invertible normalizing flow neural networks by JKO scheme. (arXiv:2212.14424v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;JKO&#26041;&#26696;&#30340;&#21487;&#36870;&#24402;&#19968;&#21270;&#27969;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#25353;&#22359;&#36827;&#34892;&#27531;&#24046;&#22359;&#30340;&#35757;&#32451;&#65292;&#20943;&#23569;&#20102;&#20869;&#23384;&#36127;&#36733;&#21644;&#28145;&#24230;&#27969;&#32593;&#32476;&#35757;&#32451;&#30340;&#38590;&#24230;&#12290;&#24182;&#19988;&#36890;&#36807;&#33258;&#36866;&#24212;&#26102;&#38388;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#27969;&#32593;&#32476;&#65292;&#22312;&#27010;&#29575;&#31354;&#38388;&#20013;&#36880;&#27493;&#32454;&#21270;&#36712;&#36857;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#19968;&#21270;&#27969;&#26159;&#19968;&#31867;&#29992;&#20110;&#39640;&#25928;&#37319;&#26679;&#21644;&#23494;&#24230;&#20272;&#35745;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290;&#23454;&#38469;&#20013;&#65292;&#27969;&#36890;&#24120;&#34920;&#31034;&#20026;&#19968;&#31995;&#21015;&#21487;&#36870;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;&#38142;; &#20026;&#20102;&#20415;&#20110;&#35757;&#32451;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#23545;&#27969;&#36712;&#36857;&#36827;&#34892;&#20102;&#27491;&#21017;&#21270;&#65292;&#24182;&#35774;&#35745;&#20102;&#29305;&#27530;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21463;Jordan-Kinderleherer-Otto (JKO)&#26041;&#26696;&#21551;&#21457;&#30340;&#31070;&#32463;ODE&#27969;&#32593;&#32476;&#65292;&#23427;&#20801;&#35768;&#26377;&#25928;&#22320;&#25353;&#22359;&#36827;&#34892;&#27531;&#24046;&#22359;&#30340;&#35757;&#32451;&#65292;&#26080;&#38656;&#37319;&#26679;SDE&#36712;&#36857;&#25110;&#20998;&#25968;&#21305;&#37197;&#25110;&#21464;&#20998;&#23398;&#20064;&#30340;&#20869;&#24490;&#29615;&#12290;&#30001;&#20110;JKO&#26041;&#26696;&#23637;&#24320;&#20102;&#26799;&#24230;&#27969;&#30340;&#21160;&#24577;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#33258;&#28982;&#22320;&#36880;&#20010;&#22534;&#21472;&#27531;&#24046;&#32593;&#32476;&#22359;&#65292;&#38477;&#20302;&#20102;&#20869;&#23384;&#36127;&#36733;&#21644;&#36827;&#34892;&#31471;&#21040;&#31471;&#28145;&#24230;&#27969;&#32593;&#32476;&#35757;&#32451;&#30340;&#38590;&#24230;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#33258;&#36866;&#24212;&#26102;&#38388;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#27969;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#27010;&#29575;&#31354;&#38388;&#20013;&#36880;&#27493;&#32454;&#21270;&#36712;&#36857;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalizing flow is a class of deep generative models for efficient sampling and density estimation. In practice, the flow often appears as a chain of invertible neural network blocks; to facilitate training, existing works have regularized flow trajectories and designed special network architectures. The current paper develops a neural ODE flow network inspired by the Jordan-Kinderleherer-Otto (JKO) scheme, which allows efficient block-wise training of the residual blocks without sampling SDE trajectories or inner loops of score matching or variational learning. As the JKO scheme unfolds the dynamic of gradient flow, the proposed model naturally stacks residual network blocks one by one, reducing the memory load and difficulty in performing end-to-end deep flow network training. We also develop adaptive time reparameterization of the flow network with a progressive refinement of the trajectory in probability space, which improves the model training efficiency and accuracy in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.13508</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65306;&#19968;&#20221;&#32508;&#36848;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation techniques in time series domain: A survey and taxonomy. (arXiv:2206.13508v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21033;&#29992;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#20013;&#20986;&#33394;&#24615;&#33021;&#30340;&#26041;&#24335;&#24182;&#19981;&#38656;&#35201;&#22826;&#38271;&#26102;&#38388;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#20005;&#37325;&#20381;&#36182;&#20110;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#19968;&#33268;&#24615;&#12290;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24182;&#19981;&#20016;&#23500;&#65292;&#36890;&#24120;&#21463;&#21040;&#38480;&#21046;&#21644;&#38656;&#35201;&#20445;&#35777;&#30340;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;&#25552;&#39640;&#25968;&#25454;&#37327;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;&#28155;&#21152;&#22122;&#22768;&#25110;&#32622;&#25442;&#36824;&#26159;&#29983;&#25104;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#29616;&#29366;&#65292;&#25552;&#20379;&#20102;&#25152;&#26377;&#21487;&#29992;&#31639;&#27861;&#30340;&#27010;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#30456;&#20851;&#30740;&#31350;&#30340;&#20998;&#31867;&#27861;&#12290;&#19981;&#21516;&#21464;&#20307;&#30340;&#25928;&#29575;&#23558;&#20316;&#20026;&#35813;&#36807;&#31243;&#30340;&#20013;&#24515;&#37096;&#20998;&#36827;&#34892;&#35780;&#20272;&#65292;&#21516;&#26102;&#36824;&#23558;&#35780;&#20272;&#19981;&#21516;&#30340;&#24615;&#33021;&#25351;&#26631;&#20197;&#21450;&#27599;&#20010;&#27169;&#22411;&#30340;&#20027;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the latest advances in Deep Learning-based} generative models, it has not taken long to take advantage of their remarkable performance in the area of time series. Deep neural networks used to work with time series heavily depend on the size and consistency of the datasets used in training. These features are not usually abundant in the real world, where they are usually limited and often have constraints that must be guaranteed. Therefore, an effective way to increase the amount of data is by using Data Augmentation techniques, either by adding noise or permutations and by generating new synthetic data. This work systematically reviews the current state-of-the-art in the area to provide an overview of all available algorithms and proposes a taxonomy of the most relevant research. The efficiency of the different variants will be evaluated as a central part of the process, as well as the different metrics to evaluate the performance and the main problems concerning each model will b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;GAN&#30340;&#26368;&#26032;&#26550;&#26500;&#12289;&#25439;&#22833;&#20989;&#25968;&#20248;&#21270;&#12289;&#39564;&#35777;&#25351;&#26631;&#21644;&#24212;&#29992;&#39046;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;GAN&#30340;&#29616;&#29366;&#12290;</title><link>http://arxiv.org/abs/2203.11242</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;GAN&#32508;&#36848;&#65306;&#26368;&#26032;&#30740;&#31350;&#12289;&#20998;&#26512;&#21644;&#20998;&#31867;&#65288;arXiv&#65306;2203.11242v2 [cs.LG] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
A survey on GANs for computer vision: Recent research, analysis and taxonomy. (arXiv:2203.11242v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;GAN&#30340;&#26368;&#26032;&#26550;&#26500;&#12289;&#25439;&#22833;&#20989;&#25968;&#20248;&#21270;&#12289;&#39564;&#35777;&#25351;&#26631;&#21644;&#24212;&#29992;&#39046;&#22495;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;GAN&#30340;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#24050;&#32463;&#36827;&#34892;&#20102;&#20960;&#27425;&#38761;&#21629;&#65292;&#20854;&#20013;&#26368;&#21463;&#20851;&#27880;&#30340;&#26159;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#24040;&#22823;&#24433;&#21709;&#12290;GAN&#19981;&#20165;&#22312;&#23450;&#20041;&#20854;&#27169;&#22411;&#26102;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#26550;&#26500;&#65292;&#32780;&#19988;&#29983;&#25104;&#20102;&#24341;&#20154;&#27880;&#30446;&#30340;&#32467;&#26524;&#65292;&#23545;&#31038;&#20250;&#20135;&#29983;&#20102;&#30452;&#25509;&#24433;&#21709;&#12290;&#30001;&#20110;GAN&#24102;&#26469;&#30340;&#37325;&#22823;&#25913;&#36827;&#21644;&#26032;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#31038;&#21306;&#19981;&#26029;&#25552;&#20986;&#26032;&#30340;&#30740;&#31350;&#65292;&#20351;&#24471;&#36319;&#19978;&#26102;&#20195;&#20960;&#20046;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;GAN&#30340;&#27010;&#36848;&#65292;&#23637;&#31034;&#26368;&#26032;&#30340;&#26550;&#26500;&#12289;&#25439;&#22833;&#20989;&#25968;&#30340;&#20248;&#21270;&#12289;&#39564;&#35777;&#25351;&#26631;&#21644;&#26368;&#24191;&#27867;&#35748;&#21487;&#30340;&#21464;&#20307;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;&#23558;&#35780;&#20272;&#19981;&#21516;&#21464;&#20307;&#30340;&#27169;&#22411;&#26550;&#26500;&#25928;&#29575;&#65292;&#23637;&#31034;&#26368;&#20339;&#30340;&#24212;&#29992;&#39046;&#22495;&#65307;&#20316;&#20026;&#35813;&#36807;&#31243;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23558;&#20998;&#26512;&#35780;&#20272;GAN&#24615;&#33021;&#30340;&#19981;&#21516;&#24230;&#37327;&#26631;&#20934;&#21644;&#32463;&#24120;&#20351;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#26368;&#21518;&#65292;&#23558;&#25552;&#20986;&#19968;&#20010;&#20998;&#31867;&#27861;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;GAN&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#30340;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last few years, there have been several revolutions in the field of deep learning, mainly headlined by the large impact of Generative Adversarial Networks (GANs). GANs not only provide an unique architecture when defining their models, but also generate incredible results which have had a direct impact on society. Due to the significant improvements and new areas of research that GANs have brought, the community is constantly coming up with new researches that make it almost impossible to keep up with the times. Our survey aims to provide a general overview of GANs, showing the latest architectures, optimizations of the loss functions, validation metrics and application areas of the most widely recognized variants. The efficiency of the different variants of the model architecture will be evaluated, as well as showing the best application area; as a vital part of the process, the different metrics for evaluating the performance of GANs and the frequently used loss functions will
&lt;/p&gt;</description></item></channel></rss>