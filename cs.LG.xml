<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#12289;&#37325;&#25918;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21305;&#37197;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08763</link><description>&lt;p&gt;
&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#21487;&#25193;&#23637;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Simple and Scalable Strategies to Continually Pre-train Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08763
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#12289;&#37325;&#25918;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21305;&#37197;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#22312;&#25968;&#21313;&#20159;&#30340;&#26631;&#35760;&#19978;&#36827;&#34892;&#24120;&#35268;&#39044;&#35757;&#32451;&#65292;&#19968;&#26086;&#26377;&#26032;&#25968;&#25454;&#21487;&#29992;&#23601;&#37325;&#26032;&#24320;&#22987;&#35813;&#36807;&#31243;&#12290;&#19968;&#20010;&#26356;&#26377;&#25928;&#29575;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#25345;&#32493;&#39044;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#65292;&#19982;&#37325;&#26032;&#35757;&#32451;&#30456;&#27604;&#33021;&#33410;&#30465;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#26032;&#25968;&#25454;&#24341;&#36215;&#30340;&#20998;&#24067;&#36716;&#31227;&#36890;&#24120;&#20250;&#23548;&#33268;&#22312;&#20197;&#21069;&#25968;&#25454;&#19978;&#38477;&#20302;&#24615;&#33021;&#25110;&#26080;&#27861;&#36866;&#24212;&#26032;&#25968;&#25454;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#65288;LR&#65289;&#37325;&#26032;&#21319;&#28201;&#12289;LR&#37325;&#26032;&#34928;&#20943;&#21644;&#37325;&#25918;&#19978;&#19968;&#25968;&#25454;&#30340;&#32452;&#21512;&#36275;&#20197;&#19982;&#23436;&#20840;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#22312;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#20174;&#26368;&#32456;&#25439;&#22833;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#35780;&#20272;&#22522;&#20934;&#30340;&#35282;&#24230;&#34913;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20004;&#20010;&#24120;&#29992;&#30340;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&#33521;&#35821;&#8594;&#33521;&#35821;&#65289;&#20043;&#38388;&#30340;&#24369;&#20294;&#29616;&#23454;&#30340;&#20998;&#24067;&#36716;&#31227;&#20197;&#21450;&#26356;&#24378;&#28872;&#30340;&#20998;&#24067;&#36716;&#31227;&#65288;&#33521;&#35821;&#8594;&#24503;&#35821;&#65289;&#19979;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08763v1 Announce Type: cross  Abstract: Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\rightarrow$English) and a stronger distribution shift (English$\rightarrow$German) at th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28909;&#25193;&#25955;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#32452;&#21512;&#20248;&#21270;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25628;&#32034;&#20840;&#23616;&#26368;&#20248;&#26102;&#25928;&#29575;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08757</link><description>&lt;p&gt;
&#36890;&#36807;&#28909;&#25193;&#25955;&#23454;&#29616;&#39640;&#25928;&#30340;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient Combinatorial Optimization via Heat Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08757
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28909;&#25193;&#25955;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#32452;&#21512;&#20248;&#21270;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#25628;&#32034;&#20840;&#23616;&#26368;&#20248;&#26102;&#25928;&#29575;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#28909;&#25193;&#25955;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#32452;&#21512;&#20248;&#21270;&#12290;&#38024;&#23545;&#29616;&#26377;&#26041;&#27861;&#21482;&#33021;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#35775;&#38382;&#35299;&#31354;&#38388;&#30340;&#19968;&#23567;&#37096;&#20998;&#36825;&#19968;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#26469;&#35299;&#20915;&#19968;&#33324;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#19968;&#31995;&#21015;&#26368;&#20855;&#25361;&#25112;&#24615;&#21644;&#24191;&#27867;&#36935;&#21040;&#30340;&#32452;&#21512;&#20248;&#21270;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08757v1 Announce Type: cross  Abstract: Combinatorial optimization problems are widespread but inherently challenging due to their discrete nature.The primary limitation of existing methods is that they can only access a small fraction of the solution space at each iteration, resulting in limited efficiency for searching the global optimal. To overcome this challenge, diverging from conventional efforts of expanding the solver's search scope, we focus on enabling information to actively propagate to the solver through heat diffusion. By transforming the target function while preserving its optima, heat diffusion facilitates information flow from distant regions to the solver, providing more efficient navigation. Utilizing heat diffusion, we propose a framework for solving general combinatorial optimization problems. The proposed methodology demonstrates superior performance across a range of the most challenging and widely encountered combinatorial optimizations. Echoing rec
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#35270;&#39057;&#38382;&#31572;&#23398;&#20064;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;&#26041;&#27861;DAM&#65292;&#33021;&#22815;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12289;&#26377;&#25928;&#36866;&#24212;&#19981;&#26029;&#21040;&#26469;&#30340;&#25968;&#25454;&#38598;&#12289;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#38598;&#36755;&#20837;&#65292;&#24182;&#20801;&#35768;&#22312;&#31867;&#20284;&#25968;&#25454;&#38598;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.08755</link><description>&lt;p&gt;
DAM:&#29992;&#20110;&#25345;&#32493;&#35270;&#39057;&#38382;&#31572;&#23398;&#20064;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
DAM: Dynamic Adapter Merging for Continual Video QA Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08755
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#35270;&#39057;&#38382;&#31572;&#23398;&#20064;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;&#26041;&#27861;DAM&#65292;&#33021;&#22815;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12289;&#26377;&#25928;&#36866;&#24212;&#19981;&#26029;&#21040;&#26469;&#30340;&#25968;&#25454;&#38598;&#12289;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#38598;&#36755;&#20837;&#65292;&#24182;&#20801;&#35768;&#22312;&#31867;&#20284;&#25968;&#25454;&#38598;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25345;&#32493;&#35270;&#39057;&#38382;&#31572;&#65288;VidQA&#65289;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;DAM&#65292;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;&#26469;&#65288;i&#65289;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#65288;ii&#65289;&#23454;&#29616;&#23545;&#25345;&#32493;&#21040;&#36798;&#30340;&#25968;&#25454;&#38598;&#30340;&#39640;&#25928;&#36866;&#24212;&#65292;&#65288;iii&#65289;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#22788;&#29702;&#26469;&#33258;&#26410;&#30693;&#25968;&#25454;&#38598;&#30340;&#36755;&#20837;&#65292;&#65288;iv&#65289;&#23454;&#29616;&#36328;&#30456;&#20284;&#25968;&#25454;&#38598;&#39046;&#22495;&#30340;&#30693;&#35782;&#20849;&#20139;&#12290;&#22312;&#32473;&#23450;&#19968;&#32452;&#25345;&#32493;&#27969;&#24335;&#20256;&#36755;&#30340;VidQA&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#39034;&#24207;&#35757;&#32451;&#29305;&#23450;&#20110;&#25968;&#25454;&#38598;&#30340;&#36866;&#37197;&#22120;&#65292;&#21516;&#26102;&#20923;&#32467;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#39057;&#35821;&#35328;&#39592;&#24178;&#30340;&#21442;&#25968;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#32473;&#23450;&#26469;&#33258;&#26410;&#30693;&#39046;&#22495;&#30340;&#35270;&#39057;&#38382;&#39064;&#31034;&#20363;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#38750;&#21442;&#25968;&#36335;&#30001;&#22120;&#20989;&#25968;&#35745;&#31639;&#27599;&#20010;&#36866;&#37197;&#22120;&#30340;&#27010;&#29575;&#65292;&#21453;&#26144;&#20986;&#35813;&#36866;&#37197;&#22120;&#19982;&#24403;&#21069;&#35270;&#39057;&#38382;&#39064;&#36755;&#20837;&#23454;&#20363;&#30340;&#30456;&#20851;&#24615;&#12290;&#38543;&#21518;&#65292;&#25152;&#25552;&#20986;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;&#26041;&#26696;&#32858;&#21512;&#25152;&#26377;&#36866;&#37197;&#22120;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08755v1 Announce Type: cross  Abstract: We present a parameter-efficient method for continual video question-answering (VidQA) learning. Our method, named DAM, uses the proposed Dynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable efficient adaptation to continually arriving datasets, (iii) handle inputs from unknown datasets during inference, and (iv) enable knowledge sharing across similar dataset domains. Given a set of continually streaming VidQA datasets, we sequentially train dataset-specific adapters for each dataset while freezing the parameters of a large pretrained video-language backbone. During inference, given a video-question sample from an unknown domain, our method first uses the proposed non-parametric router function to compute a probability for each adapter, reflecting how relevant that adapter is to the current video-question input instance. Subsequently, the proposed dynamic adapter merging scheme aggregates all the adapter weight
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#20102;&#36866;&#24403;&#30340;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#65292;&#22312;&#36825;&#20123;&#31354;&#38388;&#20013;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#21450;&#20854;&#34920;&#31034;&#20013;&#28508;&#22312;&#32467;&#26500;&#65292;&#36890;&#36807;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#29702;&#35770;&#21644;&#21464;&#20998;&#32467;&#26524;&#24471;&#20986;&#20102;&#36866;&#29992;&#20110;&#23454;&#38469;&#20013;&#24120;&#35265;&#26377;&#38480;&#28145;&#24230;&#32593;&#32476;&#30340;&#34920;&#29616;&#23450;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.08750</link><description>&lt;p&gt;
&#31070;&#32463;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#21644;&#28145;&#24230;&#32593;&#32476;&#30340;&#34920;&#29616;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Neural reproducing kernel Banach spaces and representer theorems for deep networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#20102;&#36866;&#24403;&#30340;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#65292;&#22312;&#36825;&#20123;&#31354;&#38388;&#20013;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#21450;&#20854;&#34920;&#31034;&#20013;&#28508;&#22312;&#32467;&#26500;&#65292;&#36890;&#36807;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#29702;&#35770;&#21644;&#21464;&#20998;&#32467;&#26524;&#24471;&#20986;&#20102;&#36866;&#29992;&#20110;&#23454;&#38469;&#20013;&#24120;&#35265;&#26377;&#38480;&#28145;&#24230;&#32593;&#32476;&#30340;&#34920;&#29616;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#30001;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#30340;&#20989;&#25968;&#31354;&#38388;&#26377;&#21161;&#20110;&#29702;&#35299;&#30456;&#24212;&#30340;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#24402;&#32435;&#20559;&#24046;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#20102;&#36866;&#24403;&#30340;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#65292;&#36825;&#20123;&#31354;&#38388;&#37197;&#22791;&#26377;&#24378;&#21046;&#31232;&#30095;&#24615;&#30340;&#33539;&#25968;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#21450;&#20854;&#34920;&#31034;&#20013;&#28508;&#22312;&#32467;&#26500;&#12290;&#22522;&#20110;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#29702;&#35770;&#65292;&#32467;&#21512;&#21464;&#20998;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#35777;&#26126;&#22312;&#24212;&#29992;&#20013;&#24120;&#29992;&#30340;&#26377;&#38480;&#26550;&#26500;&#30340;&#34920;&#29616;&#23450;&#29702;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25193;&#23637;&#20102;&#27973;&#23618;&#32593;&#32476;&#30340;&#31867;&#20284;&#32467;&#26524;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#26397;&#30528;&#26356;&#23454;&#29992;&#30340;&#26041;&#21521;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08750v1 Announce Type: cross  Abstract: Studying the function spaces defined by neural networks helps to understand the corresponding learning models and their inductive bias. While in some limits neural networks correspond to function spaces that are reproducing kernel Hilbert spaces, these regimes do not capture the properties of the networks used in practice. In contrast, in this paper we show that deep neural networks define suitable reproducing kernel Banach spaces.   These spaces are equipped with norms that enforce a form of sparsity, enabling them to adapt to potential latent structures within the input data and their representations. In particular, leveraging the theory of reproducing kernel Banach spaces, combined with variational results, we derive representer theorems that justify the finite architectures commonly employed in applications. Our study extends analogous results for shallow networks and can be seen as a step towards considering more practically plaus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#21435;&#20559;&#20542;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#26426;&#21046;&#25351;&#23548;&#35774;&#35745;&#25552;&#31034;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20135;&#29983;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.08743</link><description>&lt;p&gt;
&#23558;LLMs&#24341;&#23548;&#21040;&#26080;&#20559;&#21709;&#24212;&#65306;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#21435;&#20559;&#20542;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#21435;&#20559;&#20542;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#26426;&#21046;&#25351;&#23548;&#35774;&#35745;&#25552;&#31034;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20135;&#29983;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24456;&#23481;&#26131;&#20135;&#29983;&#20559;&#35265;&#21644;&#27495;&#35270;&#24615;&#30340;&#21709;&#24212;&#12290;&#30001;&#20110;LLMs&#28041;&#21450;&#21040;&#37325;&#35201;&#30340;&#20915;&#31574;&#21046;&#23450;&#65288;&#20363;&#22914;&#25307;&#32856;&#21644;&#21307;&#30103;&#20445;&#20581;&#65289;&#65292;&#24320;&#21457;&#20943;&#36731;&#36825;&#20123;&#20559;&#35265;&#30340;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#31038;&#20250;&#20559;&#35265;&#65292;&#35299;&#20915;&#20102;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#19982;LLM&#36755;&#20986;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#21435;&#20559;&#20542;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;LLMs&#36755;&#20837;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20197;&#21450;LLM&#25512;&#29702;&#30340;&#20869;&#37096;&#25512;&#29702;&#36807;&#31243;&#30340;&#22240;&#26524;&#29702;&#35299;&#65292;&#36890;&#36807;&#36873;&#25321;&#26426;&#21046;&#25351;&#23548;&#21435;&#20559;&#20542;LLM&#36755;&#20986;&#30340;&#25552;&#31034;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#21435;&#20559;&#25351;&#31034;&#26041;&#27861;&#65292;&#22914;&#25233;&#21046;&#25351;&#20196;&#21644;&#19978;&#19979;&#25991;&#23545;&#27604;&#20363;&#23376;&#65292;&#24182;&#36890;&#36807;&#40723;&#21169;&#26080;&#20559;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21551;&#31034;&#20102;&#26032;&#30340;&#21435;&#20559;&#20542;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24378;&#22823;&#23454;&#35777;&#34920;&#29616;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08743v1 Announce Type: cross  Abstract: Large language models (LLMs) can easily generate biased and discriminative responses. As LLMs tap into consequential decision-making (e.g., hiring and healthcare), it is of crucial importance to develop strategies to mitigate these biases. This paper focuses on social bias, tackling the association between demographic information and LLM outputs. We propose a causality-guided debiasing framework that utilizes causal understandings of (1) the data-generating process of the training corpus fed to LLMs, and (2) the internal reasoning process of LLM inference, to guide the design of prompts for debiasing LLM outputs through selection mechanisms. Our framework unifies existing de-biasing prompting approaches such as inhibitive instructions and in-context contrastive examples, and sheds light on new ways of debiasing by encouraging bias-free reasoning. Our strong empirical performance on real-world datasets demonstrates that our framework pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#19968;&#20010;&#21457;&#36865;&#32773;&#19982;&#27599;&#19968;&#36718;&#25932;&#24847;&#36873;&#25321;&#30340;&#26410;&#30693;&#31867;&#22411;&#25509;&#25910;&#32773;&#20043;&#38388;&#30340;&#20449;&#24687;&#35774;&#35745;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#20840;&#20449;&#24687;&#21453;&#39304;&#26102;&#21487;&#20197;&#23454;&#29616;$O(\sqrt{T})$&#30340;&#21518;&#24724;&#12290;</title><link>https://arxiv.org/abs/2403.08741</link><description>&lt;p&gt;
&#23398;&#20064;&#22914;&#20309;&#31574;&#30053;&#24615;&#25259;&#38706;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Learning How to Strategically Disclose Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#19968;&#20010;&#21457;&#36865;&#32773;&#19982;&#27599;&#19968;&#36718;&#25932;&#24847;&#36873;&#25321;&#30340;&#26410;&#30693;&#31867;&#22411;&#25509;&#25910;&#32773;&#20043;&#38388;&#30340;&#20449;&#24687;&#35774;&#35745;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#20840;&#20449;&#24687;&#21453;&#39304;&#26102;&#21487;&#20197;&#23454;&#29616;$O(\sqrt{T})$&#30340;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25112;&#30053;&#20449;&#24687;&#25259;&#38706;&#65292;&#31616;&#21333;&#26469;&#35828;&#65292;&#32771;&#34385;&#20102;&#20449;&#24687;&#25552;&#20379;&#32773;&#65288;&#21457;&#36865;&#26041;&#65289;&#21644;&#23545;&#26576;&#20123;&#31169;&#20154;&#20449;&#24687;&#24863;&#20852;&#36259;&#30340;&#20449;&#24687;&#25509;&#25910;&#32773;&#20043;&#38388;&#30340;&#21338;&#24328;&#12290;&#21457;&#20214;&#20154;&#21487;&#20197;&#36890;&#36807;&#20449;&#21495;&#25215;&#35834;&#35774;&#35745;&#20449;&#24687;&#65288;&#25110;&#20462;&#25913;&#25509;&#25910;&#26041;&#30340;&#20449;&#24565;&#65289;&#65292;&#20174;&#32780;&#26500;&#25104;&#19968;&#22330;&#26031;&#22612;&#20811;&#36125;&#26684;&#21338;&#24328;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#19978;&#65292;&#20026;&#20102;&#22312;&#36825;&#22330;&#27604;&#36187;&#20013;&#33719;&#24471;&#26031;&#22612;&#20811;&#36125;&#26684;&#22343;&#34913;&#65292;&#21457;&#20214;&#20154;&#38656;&#35201;&#35775;&#38382;&#25509;&#25910;&#32773;&#30340;&#30446;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#20449;&#24687;&#35774;&#35745;&#30340;&#22312;&#32447;&#29256;&#26412;&#65292;&#20854;&#20013;&#21457;&#20214;&#20154;&#19982;&#27599;&#19968;&#36718;&#25932;&#24847;&#36873;&#25321;&#30340;&#26410;&#30693;&#31867;&#22411;&#30340;&#25509;&#25910;&#32773;&#36827;&#34892;&#20132;&#20114;&#12290;&#22312;&#32771;&#34385;&#20102;&#21457;&#20214;&#20154;&#21644;&#25509;&#25910;&#32773;&#30340;&#39640;&#26031;&#20808;&#39564;&#21644;&#20108;&#27425;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#20840;&#20449;&#24687;&#21453;&#39304;&#21487;&#20197;&#23454;&#29616;$O(\sqrt{T})$&#30340;&#21518;&#24724;&#65292;&#20854;&#20013;$T$&#26159;&#24635;&#20132;&#20114;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08741v1 Announce Type: cross  Abstract: Strategic information disclosure, in its simplest form, considers a game between an information provider (sender) who has access to some private information that an information receiver is interested in. While the receiver takes an action that affects the utilities of both players, the sender can design information (or modify beliefs) of the receiver through signal commitment, hence posing a Stackelberg game. However, obtaining a Stackelberg equilibrium for this game traditionally requires the sender to have access to the receiver's objective. In this work, we consider an online version of information design where a sender interacts with a receiver of an unknown type who is adversarially chosen at each round. Restricting attention to Gaussian prior and quadratic costs for the sender and the receiver, we show that $\mathcal{O}(\sqrt{T})$ regret is achievable with full information feedback, where $T$ is the total number of interactions b
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29615;&#22659;&#25193;&#25955;&#21518;&#39564;&#37319;&#26679;&#35299;&#20915;&#36870;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22312;&#21463;&#25439;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#22270;&#20687;&#24674;&#22797;&#21644;MRI&#27169;&#22411;&#35757;&#32451;&#20013;&#21462;&#24471;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08728</link><description>&lt;p&gt;
&#20351;&#29992;&#29615;&#22659;&#25193;&#25955;&#21518;&#39564;&#37319;&#26679;&#65306;&#22312;&#21463;&#25439;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#35299;&#20915;&#36870;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Ambient Diffusion Posterior Sampling: Solving Inverse Problems with Diffusion Models trained on Corrupted Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08728
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29615;&#22659;&#25193;&#25955;&#21518;&#39564;&#37319;&#26679;&#35299;&#20915;&#36870;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22312;&#21463;&#25439;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#22270;&#20687;&#24674;&#22797;&#21644;MRI&#27169;&#22411;&#35757;&#32451;&#20013;&#21462;&#24471;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#20174;&#32447;&#24615;&#21463;&#25439;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#25193;&#25955;&#27169;&#22411;&#35299;&#20915;&#36870;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;Ambient Diffusion Posterior Sampling (A-DPS)&#65292;&#21033;&#29992;&#19968;&#20010;&#39044;&#20808;&#22312;&#19968;&#31181;&#31867;&#22411;&#30340;&#25439;&#22351;&#25968;&#25454;&#19978;&#36827;&#34892;&#36807;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#22312;&#21487;&#33021;&#26469;&#33258;&#19981;&#21516;&#21069;&#21521;&#36807;&#31243;&#65288;&#20363;&#22914;&#22270;&#20687;&#27169;&#31946;&#65289;&#30340;&#27979;&#37327;&#26465;&#20214;&#19979;&#25191;&#34892;&#21518;&#39564;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#33258;&#28982;&#22270;&#20687;&#25968;&#25454;&#38598;&#65288;CelebA&#12289;FFHQ &#21644; AFHQ&#65289;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102; A-DPS &#26377;&#26102;&#22312;&#36895;&#24230;&#21644;&#24615;&#33021;&#19978;&#37117;&#33021;&#32988;&#36807;&#22312;&#28165;&#27905;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20960;&#20010;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#29615;&#22659;&#25193;&#25955;&#26694;&#26550;&#65292;&#20197;&#20165;&#35775;&#38382;&#20613;&#37324;&#21494;&#23376;&#37319;&#26679;&#30340;&#22810;&#32447;&#22280; MRI &#27979;&#37327;&#25968;&#25454;&#26469;&#35757;&#32451; MRI &#27169;&#22411;&#65292;&#20854;&#21152;&#36895;&#22240;&#23376;&#20026;&#19981;&#21516;&#30340;&#21152;&#36895;&#22240;&#23376;&#65288;R=2&#12289;4&#12289;6&#12289;8&#65289;&#12290;&#25105;&#20204;&#20877;&#27425;&#35266;&#23519;&#21040;&#65292;&#22312;&#39640;&#24230;&#23376;&#37319;&#26679;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#35299;&#20915;&#39640;&#21152;&#36895; MRI &#36870;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08728v1 Announce Type: cross  Abstract: We provide a framework for solving inverse problems with diffusion models learned from linearly corrupted data. Our method, Ambient Diffusion Posterior Sampling (A-DPS), leverages a generative model pre-trained on one type of corruption (e.g. image inpainting) to perform posterior sampling conditioned on measurements from a potentially different forward process (e.g. image blurring). We test the efficacy of our approach on standard natural image datasets (CelebA, FFHQ, and AFHQ) and we show that A-DPS can sometimes outperform models trained on clean data for several image restoration tasks in both speed and performance. We further extend the Ambient Diffusion framework to train MRI models with access only to Fourier subsampled multi-coil MRI measurements at various acceleration factors (R=2, 4, 6, 8). We again observe that models trained on highly subsampled data are better priors for solving inverse problems in the high acceleration r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#36845;&#20195;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#36924;&#30495;&#30340;&#39640;&#36136;&#37327;&#26631;&#20934;&#24179;&#38754;&#65292;&#23545;&#25552;&#39640;&#20020;&#24202;&#21307;&#29983;&#30340;&#22521;&#35757;&#12289;&#25913;&#21892;&#22270;&#20687;&#36136;&#37327;&#20197;&#21450;&#25552;&#21319;&#19979;&#28216;&#35786;&#26029;&#21644;&#30417;&#27979;&#20855;&#26377;&#28508;&#22312;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2403.08700</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#36845;&#20195;&#21453;&#20107;&#23454;&#35299;&#37322;&#29992;&#20110;&#32974;&#20799;&#36229;&#22768;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based Iterative Counterfactual Explanations for Fetal Ultrasound Image Quality Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08700
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#36845;&#20195;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#36924;&#30495;&#30340;&#39640;&#36136;&#37327;&#26631;&#20934;&#24179;&#38754;&#65292;&#23545;&#25552;&#39640;&#20020;&#24202;&#21307;&#29983;&#30340;&#22521;&#35757;&#12289;&#25913;&#21892;&#22270;&#20687;&#36136;&#37327;&#20197;&#21450;&#25552;&#21319;&#19979;&#28216;&#35786;&#26029;&#21644;&#30417;&#27979;&#20855;&#26377;&#28508;&#22312;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24576;&#23381;&#26399;&#36229;&#22768;&#22270;&#20687;&#36136;&#37327;&#23545;&#20934;&#30830;&#35786;&#26029;&#21644;&#30417;&#27979;&#32974;&#20799;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26631;&#20934;&#24179;&#38754;&#24456;&#22256;&#38590;&#65292;&#21463;&#21040;&#36229;&#22768;&#27874;&#25216;&#26415;&#20154;&#21592;&#30340;&#19987;&#19994;&#30693;&#35782;&#20197;&#21450;&#20687;&#23381;&#22919;BMI&#25110;&#32974;&#20799;&#21160;&#24577;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#21453;&#20107;&#23454;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65292;&#20174;&#20302;&#36136;&#37327;&#30340;&#38750;&#26631;&#20934;&#24179;&#38754;&#29983;&#25104;&#36924;&#30495;&#30340;&#39640;&#36136;&#37327;&#26631;&#20934;&#24179;&#38754;&#12290;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#36136;&#37327;&#22686;&#21152;&#30340;&#21487;&#20449;&#21453;&#20107;&#23454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#20026;&#36890;&#36807;&#25552;&#20379;&#35270;&#35273;&#21453;&#39304;&#21152;&#24378;&#20020;&#24202;&#21307;&#29983;&#22521;&#35757;&#20197;&#21450;&#25913;&#36827;&#22270;&#20687;&#36136;&#37327;&#65292;&#20174;&#32780;&#25913;&#21892;&#19979;&#28216;&#35786;&#26029;&#21644;&#30417;&#27979;&#25552;&#20379;&#20102;&#26410;&#26469;&#30340;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08700v1 Announce Type: cross  Abstract: Obstetric ultrasound image quality is crucial for accurate diagnosis and monitoring of fetal health. However, producing high-quality standard planes is difficult, influenced by the sonographer's expertise and factors like the maternal BMI or the fetus dynamics. In this work, we propose using diffusion-based counterfactual explainable AI to generate realistic high-quality standard planes from low-quality non-standard ones. Through quantitative and qualitative evaluation, we demonstrate the effectiveness of our method in producing plausible counterfactuals of increased quality. This shows future promise both for enhancing training of clinicians by providing visual feedback, as well as for improving image quality and, consequently, downstream diagnosis and monitoring.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#19968;&#23618;Softmax&#27880;&#24847;&#21147;&#27169;&#22411;&#19978;&#25351;&#25968;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#27969;&#65292;&#21457;&#29616;&#22312;&#28176;&#36827;&#26368;&#23567;&#21270;&#25439;&#22833;&#20540;&#26102;&#38544;&#24335;&#26368;&#23567;&#21270;&#20102;&#20851;&#38190;&#21644;&#26597;&#35810;&#26435;&#37325;&#30697;&#38453;&#20056;&#31215;&#30340;&#26680;&#33539;&#25968;&#65292;&#36825;&#31181;&#38544;&#24335;&#27491;&#21017;&#21270;&#21487;&#36890;&#36807;&#19982;&#27880;&#24847;&#21147;&#26435;&#37325;&#30456;&#20851;&#30340;SVM&#38382;&#39064;&#25551;&#36848;&#12290;</title><link>https://arxiv.org/abs/2403.08699</link><description>&lt;p&gt;
&#19968;&#23618;Softmax&#27880;&#24847;&#21147;&#27169;&#22411;&#19978;&#26799;&#24230;&#27969;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Implicit Regularization of Gradient Flow on One-Layer Softmax Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08699
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#19968;&#23618;Softmax&#27880;&#24847;&#21147;&#27169;&#22411;&#19978;&#25351;&#25968;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#27969;&#65292;&#21457;&#29616;&#22312;&#28176;&#36827;&#26368;&#23567;&#21270;&#25439;&#22833;&#20540;&#26102;&#38544;&#24335;&#26368;&#23567;&#21270;&#20102;&#20851;&#38190;&#21644;&#26597;&#35810;&#26435;&#37325;&#30697;&#38453;&#20056;&#31215;&#30340;&#26680;&#33539;&#25968;&#65292;&#36825;&#31181;&#38544;&#24335;&#27491;&#21017;&#21270;&#21487;&#36890;&#36807;&#19982;&#27880;&#24847;&#21147;&#26435;&#37325;&#30456;&#20851;&#30340;SVM&#38382;&#39064;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19968;&#23618;Softmax&#27880;&#24847;&#21147;&#27169;&#22411;&#19978;&#25351;&#25968;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#27969;&#65292;&#20854;&#20013;&#20851;&#38190;&#21644;&#26597;&#35810;&#26435;&#37325;&#30697;&#38453;&#26159;&#20998;&#21035;&#35757;&#32451;&#30340;&#12290;&#22312;&#25968;&#25454;&#21487;&#20998;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#26799;&#24230;&#27969;&#36798;&#21040;&#26368;&#23567;&#25439;&#22833;&#20540;&#26102;&#65292;&#23427;&#36827;&#19968;&#27493;&#38544;&#24335;&#22320;&#26368;&#23567;&#21270;&#20102;&#20851;&#38190;&#21644;&#26597;&#35810;&#26435;&#37325;&#30697;&#38453;&#20056;&#31215;&#30340;&#26680;&#33539;&#25968;&#12290;&#36825;&#31181;&#38544;&#24335;&#27491;&#21017;&#21270;&#21487;&#20197;&#36890;&#36807;&#19982;&#27880;&#24847;&#21147;&#26435;&#37325;&#30456;&#20851;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#38382;&#39064;&#26469;&#25551;&#36848;&#12290;&#36825;&#19968;&#21457;&#29616;&#19982;&#20808;&#21069;&#30340;&#32467;&#26524;&#24418;&#25104;&#23545;&#27604;&#65292;&#20808;&#21069;&#30340;&#32467;&#26524;&#26174;&#31034;&#24403;&#23558;&#20851;&#38190;&#21644;&#26597;&#35810;&#30697;&#38453;&#21512;&#24182;&#20026;&#21333;&#20010;&#26435;&#37325;&#30697;&#38453;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#26799;&#24230;&#19979;&#38477;&#20250;&#22312;&#20056;&#31215;&#26435;&#37325;&#30697;&#38453;&#19978;&#23454;&#26045;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#26368;&#23567;&#21270;&#24343;&#32599;&#36125;&#23612;&#20044;&#26031;&#33539;&#25968;&#12290;&#23545;&#20110;&#23545;&#35282;&#20851;&#38190;&#21644;&#26597;&#35810;&#30697;&#38453;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#24314;&#31435;&#22312;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#26415;&#21644;&#21033;&#29992;&#19982;&#20998;&#31867;&#20219;&#21153;&#30456;&#20851;&#30340;SVM&#30340;&#36817;&#20284;KKT&#26465;&#20214;&#30340;&#22522;&#30784;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08699v1 Announce Type: cross  Abstract: We study gradient flow on the exponential loss for a classification problem with a one-layer softmax attention model, where the key and query weight matrices are trained separately. Under a separability assumption on the data, we show that when gradient flow achieves the minimal loss value, it further implicitly minimizes the nuclear norm of the product of the key and query weight matrices. Such implicit regularization can be described by a Support Vector Machine (SVM) problem with respect to the attention weights. This finding contrasts with prior results showing that the gradient descent induces an implicit regularization on the Frobenius norm on the product weight matrix when the key and query matrices are combined into a single weight matrix for training. For diagonal key and query matrices, our analysis builds upon the reparameterization technique and exploits approximate KKT conditions of the SVM associated with the classificatio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#30340;&#26032;&#22411;&#24494;&#26381;&#21153;&#21368;&#36733;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#20013;&#20302;&#25928;&#24494;&#26381;&#21153;&#21368;&#36733;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08687</link><description>&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#36793;&#32536;&#35745;&#31639;&#20013;&#30340;&#36164;&#28304;&#24863;&#30693;&#24494;&#26381;&#21153;&#21368;&#36733;
&lt;/p&gt;
&lt;p&gt;
Digital Twin-assisted Reinforcement Learning for Resource-aware Microservice Offloading in Edge Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08687
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#30340;&#26032;&#22411;&#24494;&#26381;&#21153;&#21368;&#36733;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#20013;&#20302;&#25928;&#24494;&#26381;&#21153;&#21368;&#36733;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#24335;&#36793;&#32536;&#35745;&#31639;&#65288;CEC&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#65292;&#20351;&#36793;&#32536;&#33410;&#28857;&#33021;&#22815;&#21327;&#20316;&#24182;&#20174;&#32456;&#31471;&#35774;&#22791;&#25191;&#34892;&#24494;&#26381;&#21153;&#12290; &#24494;&#26381;&#21153;&#21368;&#36733;&#26159;&#19968;&#20010;&#26681;&#26412;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#23427;&#22312;&#26381;&#21153;&#21040;&#36798;&#26102;&#20915;&#23450;&#20309;&#26102;&#20197;&#21450;&#22312;&#20309;&#22788;&#25191;&#34892;&#24494;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;CEC&#29615;&#22659;&#30340;&#21160;&#24577;&#29305;&#24615;&#32463;&#24120;&#23548;&#33268;&#20302;&#25928;&#30340;&#24494;&#26381;&#21153;&#21368;&#36733;&#31574;&#30053;&#65292;&#20174;&#32780;&#23548;&#33268;&#36164;&#28304;&#21033;&#29992;&#19981;&#36275;&#21644;&#32593;&#32476;&#25317;&#22622;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#22312;&#32447;&#32852;&#21512;&#24494;&#26381;&#21153;&#21368;&#36733;&#21644;&#24102;&#23485;&#20998;&#37197;&#38382;&#39064;JMOBA&#65292;&#20197;&#26368;&#23567;&#21270;&#26381;&#21153;&#30340;&#24179;&#22343;&#23436;&#25104;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#26381;&#21153;&#21368;&#36733;&#31639;&#27861;DTDRLMO&#65292;&#23427;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#21644;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#37319;&#29992;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#23454;&#26102;&#39044;&#27979;&#24182;&#36866;&#24212;CEC&#36793;&#32536;&#33410;&#28857;&#36127;&#36733;&#21644;&#32593;&#32476;&#26465;&#20214;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08687v1 Announce Type: cross  Abstract: Collaborative edge computing (CEC) has emerged as a promising paradigm, enabling edge nodes to collaborate and execute microservices from end devices. Microservice offloading, a fundamentally important problem, decides when and where microservices are executed upon the arrival of services. However, the dynamic nature of the real-world CEC environment often leads to inefficient microservice offloading strategies, resulting in underutilized resources and network congestion. To address this challenge, we formulate an online joint microservice offloading and bandwidth allocation problem, JMOBA, to minimize the average completion time of services. In this paper, we introduce a novel microservice offloading algorithm, DTDRLMO, which leverages deep reinforcement learning (DRL) and digital twin technology. Specifically, we employ digital twin techniques to predict and adapt to changing edge node loads and network conditions of CEC in real-time
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20855;&#26377;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#23545;&#27604;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#25581;&#31034;&#20102;&#22312;&#20309;&#31181;&#24773;&#20917;&#19979;&#36825;&#20123;&#27169;&#22411;&#25509;&#36817;&#20110;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#25110;&#26680;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#23545;&#27604;&#25439;&#22833;&#30340;NTK&#25910;&#25947;&#32467;&#26524;&#65292;&#20026;&#23545;&#27604;&#23398;&#20064;&#19982;&#26680;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#32852;&#25552;&#20379;&#20102;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.08673</link><description>&lt;p&gt;
&#20309;&#26102;&#33021;&#29992;&#31070;&#32463;&#20999;&#32447;&#26680;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#36817;&#20284;&#23485;&#23545;&#27604;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
When can we Approximate Wide Contrastive Models with Neural Tangent Kernels and Principal Component Analysis?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20855;&#26377;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#23545;&#27604;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#25581;&#31034;&#20102;&#22312;&#20309;&#31181;&#24773;&#20917;&#19979;&#36825;&#20123;&#27169;&#22411;&#25509;&#36817;&#20110;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#25110;&#26680;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#23545;&#27604;&#25439;&#22833;&#30340;NTK&#25910;&#25947;&#32467;&#26524;&#65292;&#20026;&#23545;&#27604;&#23398;&#20064;&#19982;&#26680;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#32852;&#25552;&#20379;&#20102;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#33539;&#24335;&#65292;&#23545;&#20110;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#38750;&#24120;&#25104;&#21151;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#24037;&#20316;&#32771;&#23519;&#20102;&#23545;&#27604;&#25439;&#22833;&#65292;&#22768;&#31216;&#23545;&#27604;&#27169;&#22411;&#26377;&#25928;&#22320;&#23398;&#20064;&#20102;&#35889;&#23884;&#20837;&#65292;&#32780;&#23569;&#25968;&#24037;&#20316;&#23637;&#31034;&#20102;&#65288;&#23485;&#65289;&#23545;&#27604;&#27169;&#22411;&#19982;&#26680;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#35757;&#32451;&#22909;&#30340;&#23545;&#27604;&#27169;&#22411;&#26159;&#21542;&#30830;&#23454;&#23545;&#24212;&#20110;&#26680;&#26041;&#27861;&#25110;PCA&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20855;&#26377;&#38750;&#32447;&#24615;&#28608;&#27963;&#30340;&#20004;&#23618;&#23545;&#27604;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#22238;&#31572;&#20102;&#36825;&#20123;&#27169;&#22411;&#20309;&#26102;&#25509;&#36817;PCA&#25110;&#26680;&#26041;&#27861;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#22312;&#21463;&#30417;&#30563;&#35774;&#32622;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#31561;&#25928;&#20110;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;NTK&#65289;&#26426;&#22120;&#65292;&#24182;&#19988;&#26080;&#31351;&#23485;&#32593;&#32476;&#30340;NTK&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#24658;&#23450;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#27604;&#25439;&#22833;NTK&#30340;&#31532;&#19968;&#20010;&#25910;&#25947;&#32467;&#26524;&#65292;&#24182;&#21576;&#29616;&#20102;&#19968;&#20010;&#32454;&#33268;&#30340;&#30011;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08673v1 Announce Type: new  Abstract: Contrastive learning is a paradigm for learning representations from unlabelled data that has been highly successful for image and text data. Several recent works have examined contrastive losses to claim that contrastive models effectively learn spectral embeddings, while few works show relations between (wide) contrastive models and kernel principal component analysis (PCA). However, it is not known if trained contrastive models indeed correspond to kernel methods or PCA. In this work, we analyze the training dynamics of two-layer contrastive models, with non-linear activation, and answer when these models are close to PCA or kernel methods. It is well known in the supervised setting that neural networks are equivalent to neural tangent kernel (NTK) machines, and that the NTK of infinitely wide networks remains constant during training. We provide the first convergence results of NTK for contrastive losses, and present a nuanced pictur
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21307;&#30103;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#31574;&#30053;&#65292;&#36991;&#20813;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30495;&#23454;&#24739;&#32773;&#25968;&#25454;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08664</link><description>&lt;p&gt;
&#29992;&#20110;&#20154;&#24037;&#20020;&#24202;&#35760;&#24405;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#29983;&#25104;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Zero-shot and Few-shot Generation Strategies for Artificial Clinical Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08664
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21307;&#30103;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#31574;&#30053;&#65292;&#36991;&#20813;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30495;&#23454;&#24739;&#32773;&#25968;&#25454;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#21307;&#30103;&#35760;&#24405;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;Llama 2 LLM&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#31574;&#30053;&#29983;&#25104;&#21487;&#20934;&#30830;&#21453;&#26144;&#30495;&#23454;&#24739;&#32773;&#20449;&#24687;&#30340;&#21512;&#25104;&#21307;&#30103;&#35760;&#24405;&#65292;&#19982;&#38656;&#35201;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#25935;&#24863;&#24739;&#32773;&#25968;&#25454;&#30340;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08664v1 Announce Type: new  Abstract: The challenge of accessing historical patient data for clinical research, while adhering to privacy regulations, is a significant obstacle in medical science. An innovative approach to circumvent this issue involves utilising synthetic medical records that mirror real patient data without compromising individual privacy. The creation of these synthetic datasets, particularly without using actual patient data to train Large Language Models (LLMs), presents a novel solution as gaining access to sensitive patient information to train models is also a challenge. This study assesses the capability of the Llama 2 LLM to create synthetic medical records that accurately reflect real patient information, employing zero-shot and few-shot prompting strategies for comparison against fine-tuned methodologies that do require sensitive patient data during training. We focus on generating synthetic narratives for the History of Present Illness section, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#21327;&#26041;&#24046;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#23616;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#24182;&#22312;&#25512;&#26029;&#26102;&#23616;&#37096;&#24212;&#29992;&#65292;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#20840;&#23616;&#29305;&#24449;&#65292;&#20855;&#26377;&#33258;&#21160;&#21270;&#30340;&#20840;&#23616;&#29305;&#24449;&#21033;&#29992;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.08662</link><description>&lt;p&gt;
&#29992;&#20110;&#21327;&#26041;&#24046;&#20272;&#35745;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning for Covariance Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08662
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#21327;&#26041;&#24046;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#23616;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#24182;&#22312;&#25512;&#26029;&#26102;&#23616;&#37096;&#24212;&#29992;&#65292;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#20840;&#23616;&#29305;&#24449;&#65292;&#20855;&#26377;&#33258;&#21160;&#21270;&#30340;&#20840;&#23616;&#29305;&#24449;&#21033;&#29992;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#21327;&#26041;&#24046;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20840;&#23616;&#23398;&#20064;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#28982;&#21518;&#22312;&#25512;&#26029;&#26102;&#23616;&#37096;&#24212;&#29992;&#35813;&#32593;&#32476;&#12290;&#21033;&#29992;&#33258;&#30417;&#30563;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#22320;&#23631;&#34109;&#19981;&#21516;&#26679;&#26412;&#24182;&#23398;&#20064;&#39044;&#27979;&#23427;&#20204;&#21608;&#22260;&#37051;&#23621;&#30340;&#21327;&#26041;&#24046;&#26469;&#35757;&#32451;&#32593;&#32476;&#65292;&#26080;&#38656;&#20219;&#20309;&#26631;&#31614;&#12290;&#35813;&#26550;&#26500;&#22522;&#20110;&#27969;&#34892;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#20854;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#22312;&#27809;&#26377;&#20219;&#20309;&#20998;&#24067;&#20551;&#35774;&#25110;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#21033;&#29992;&#20840;&#23616;&#29305;&#24449;&#12290;&#23427;&#21487;&#20197;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20877;&#20026;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#37325;&#29992;&#65292;&#20363;&#22914;&#38647;&#36798;&#25110;&#39640;&#20809;&#35889;&#22270;&#20687;&#20013;&#30340;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08662v1 Announce Type: cross  Abstract: We consider the use of deep learning for covariance estimation. We propose to globally learn a neural network that will then be applied locally at inference time. Leveraging recent advancements in self-supervised foundational models, we train the network without any labeling by simply masking different samples and learning to predict their covariance given their surrounding neighbors. The architecture is based on the popular attention mechanism. Its main advantage over classical methods is the automatic exploitation of global characteristics without any distributional assumptions or regularization. It can be pre-trained as a foundation model and then be repurposed for various downstream tasks, e.g., adaptive target detection in radar or hyperspectral imagery.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#20174;&#40657;&#31665;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#35299;&#37322;&#12289;&#35777;&#26126;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;DNN&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08652</link><description>&lt;p&gt;
&#20174;&#40657;&#31665;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#35299;&#37322;&#12289;&#35777;&#26126;&#21644;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Extracting Explanations, Justification, and Uncertainty from Black-Box Deep Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#20174;&#40657;&#31665;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#35299;&#37322;&#12289;&#35777;&#26126;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;DNN&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#26412;&#36523;&#19981;&#20250;&#35745;&#31639;&#25110;&#23637;&#31034;&#32463;&#39564;&#35777;&#26126;&#30340;&#20219;&#21153;&#32622;&#20449;&#24230;&#12290;&#22312;&#20851;&#38190;&#20219;&#21153;&#24212;&#29992;&#20013;&#65292;&#20102;&#35299;&#30456;&#20851;&#30340;DNN&#25512;&#29702;&#21450;&#20854;&#25903;&#25345;&#35777;&#25454;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;DNNs&#20013;&#25552;&#21462;&#35299;&#37322;&#12289;&#35777;&#26126;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20869;&#23384;&#21644;&#35745;&#31639;&#26041;&#38754;&#37117;&#24456;&#26377;&#25928;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#40657;&#31665;DNN&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#21253;&#25324;&#24322;&#24120;&#26816;&#27979;&#21644;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;DNN&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08652v1 Announce Type: new  Abstract: Deep Neural Networks (DNNs) do not inherently compute or exhibit empirically-justified task confidence. In mission critical applications, it is important to both understand associated DNN reasoning and its supporting evidence. In this paper, we propose a novel Bayesian approach to extract explanations, justifications, and uncertainty estimates from DNNs. Our approach is efficient both in terms of memory and computation, and can be applied to any black box DNN without any retraining, including applications to anomaly detection and out-of-distribution detection tasks. We validate our approach on the CIFAR-10 dataset, and show that it can significantly improve the interpretability and reliability of DNNs.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#32570;&#22833;&#20013;&#20171;&#21464;&#37327;&#23545;&#20256;&#36755;&#30340;&#20013;&#20171;&#25928;&#24212;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25935;&#24863;&#24615;&#20998;&#26512;&#26694;&#26550;&#65292;&#33021;&#22815;&#30830;&#23450;&#22312;&#21738;&#20123;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#32570;&#22833;&#20013;&#20171;&#25968;&#25454;&#30340;&#23376;&#32676;&#30340;&#26465;&#20214;&#20256;&#36755;&#20013;&#20171;&#25928;&#24212;&#21464;&#24471;&#19981;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2403.08638</link><description>&lt;p&gt;
&#32570;&#22833;&#20013;&#20171;&#30340;&#24322;&#36136;&#25928;&#24212;&#23545;&#22240;&#26524;&#25928;&#24212;&#21487;&#20256;&#36882;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Disparate Effect Of Missing Mediators On Transportability of Causal Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08638
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#32570;&#22833;&#20013;&#20171;&#21464;&#37327;&#23545;&#20256;&#36755;&#30340;&#20013;&#20171;&#25928;&#24212;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25935;&#24863;&#24615;&#20998;&#26512;&#26694;&#26550;&#65292;&#33021;&#22815;&#30830;&#23450;&#22312;&#21738;&#20123;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#32570;&#22833;&#20013;&#20171;&#25968;&#25454;&#30340;&#23376;&#32676;&#30340;&#26465;&#20214;&#20256;&#36755;&#20013;&#20171;&#25928;&#24212;&#21464;&#24471;&#19981;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#36755;&#30340;&#20013;&#20171;&#25928;&#24212;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#29702;&#35299;&#19978;&#28216;&#24178;&#39044;&#65288;&#20363;&#22914;&#25913;&#21892;&#31038;&#21306;&#26465;&#20214;&#22914;&#32511;&#22320;&#65289;&#22312;&#24212;&#29992;&#20110;&#19981;&#21516;&#20154;&#32676;&#26102;&#20250;&#22240;&#20013;&#20171;&#25928;&#24212;&#30340;&#19981;&#21516;&#32780;&#20135;&#29983;&#24046;&#24322;&#25928;&#26524;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#24403;&#20013;&#20171;&#21464;&#37327;&#22312;&#35201;&#20256;&#36755;&#25928;&#24212;&#30340;&#20154;&#32676;&#20013;&#32570;&#22833;&#26102;&#65292;&#36825;&#20123;&#20272;&#35745;&#20540;&#21487;&#33021;&#20250;&#23384;&#22312;&#20559;&#35823;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#32570;&#22833;&#20013;&#20171;&#21464;&#37327;&#36825;&#19968;&#38382;&#39064;&#65292;&#20854;&#21160;&#26426;&#28304;&#20110;&#20844;&#20849;&#21355;&#29983;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#20854;&#20013;&#20013;&#20171;&#21464;&#37327;&#21487;&#33021;&#26159;&#38750;&#38543;&#26426;&#32570;&#22833;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25935;&#24863;&#24615;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#32570;&#22833;&#20013;&#20171;&#25968;&#25454;&#23545;&#20256;&#36755;&#30340;&#20013;&#20171;&#25928;&#24212;&#30340;&#24433;&#21709;&#12290;&#35813;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#22312;&#21738;&#20123;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#32570;&#22833;&#20013;&#20171;&#25968;&#25454;&#23376;&#32676;&#30340;&#26465;&#20214;&#20256;&#36755;&#20013;&#20171;&#25928;&#24212;&#21464;&#24471;&#19981;&#26174;&#33879;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20256;&#36755;&#20013;&#20171;&#25928;&#24212;&#30340;&#30028;&#38480;&#65292;&#20316;&#20026;&#32570;&#22833;&#31243;&#24230;&#30340;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;fra
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08638v1 Announce Type: new  Abstract: Transported mediation effects provide an avenue to understand how upstream interventions (such as improved neighborhood conditions like green spaces) would work differently when applied to different populations as a result of factors that mediate the effects. However, when mediators are missing in the population where the effect is to be transported, these estimates could be biased. We study this issue of missing mediators, motivated by challenges in public health, wherein mediators can be missing, not at random. We propose a sensitivity analysis framework that quantifies the impact of missing mediator data on transported mediation effects. This framework enables us to identify the settings under which the conditional transported mediation effect is rendered insignificant for the subgroup with missing mediator data. Specifically, we provide the bounds on the transported mediation effect as a function of missingness. We then apply the fra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#20004;&#31181;&#26368;&#36817;&#23545;&#40784;&#26041;&#27861;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#27867;&#21270;&#29256;&#26412;&#65292;&#26377;&#21161;&#20110;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#30340;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.08635</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#20559;&#22909;&#20248;&#21270;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Human Alignment of Large Language Models through Online Preference Optimisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#20004;&#31181;&#26368;&#36817;&#23545;&#40784;&#26041;&#27861;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#27867;&#21270;&#29256;&#26412;&#65292;&#26377;&#21161;&#20110;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#23545;&#20110;&#30830;&#20445;&#29992;&#25143;&#20307;&#39564;&#30340;&#26377;&#29992;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#24841;&#24742;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#20154;&#31867;&#23545;&#40784;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20986;&#29616;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#20363;&#22914;&#24378;&#21270;&#23398;&#20064;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#12289;&#30452;&#25509;&#31574;&#30053;&#20248;&#21270;&#65288;DPO&#65289;&#21644;&#24207;&#21015;&#20284;&#28982;&#26657;&#20934;&#65288;SLiC&#65289;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20004;&#31181;&#26368;&#36817;&#23545;&#40784;&#26041;&#27861;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#21363;&#36523;&#20221;&#31574;&#30053;&#20248;&#21270;&#65288;IPO&#65289;&#21644;&#32435;&#20160;&#38236;&#20687;&#19979;&#38477;&#65288;Nash-MD&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;IPO&#30340;&#19968;&#31181;&#27867;&#21270;&#29256;&#26412;&#65292;&#21517;&#20026;IPO-MD&#65292;&#23427;&#21033;&#29992;&#20102;Nash-MD&#25552;&#20986;&#30340;&#27491;&#21017;&#21270;&#25277;&#26679;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08635v1 Announce Type: cross  Abstract: Ensuring alignment of language models' outputs with human preferences is critical to guarantee a useful, safe, and pleasant user experience. Thus, human alignment has been extensively studied recently and several methods such as Reinforcement Learning from Human Feedback (RLHF), Direct Policy Optimisation (DPO) and Sequence Likelihood Calibration (SLiC) have emerged. In this paper, our contribution is two-fold. First, we show the equivalence between two recent alignment methods, namely Identity Policy Optimisation (IPO) and Nash Mirror Descent (Nash-MD). Second, we introduce a generalisation of IPO, named IPO-MD, that leverages the regularised sampling approach proposed by Nash-MD.   This equivalence may seem surprising at first sight, since IPO is an offline method whereas Nash-MD is an online method using a preference model. However, this equivalence can be proven when we consider the online version of IPO, that is when both generati
&lt;/p&gt;</description></item><item><title>&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#31867;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#21487;&#25512;&#24191;&#21644;&#21487;&#36716;&#31227;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#25968;&#25454;&#38598;&#20559;&#35265;&#35748;&#30693;&#12290;</title><link>https://arxiv.org/abs/2403.08632</link><description>&lt;p&gt;
&#21313;&#24180;&#25968;&#25454;&#38598;&#20559;&#35265;&#20043;&#25112;&#65306;&#25105;&#20204;&#24050;&#32463;&#25104;&#21151;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
A Decade's Battle on Dataset Bias: Are We There Yet?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08632
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#31867;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#21487;&#25512;&#24191;&#21644;&#21487;&#36716;&#31227;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#25968;&#25454;&#38598;&#20559;&#35265;&#35748;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#26032;&#26102;&#20195;&#37325;&#26032;&#23457;&#35270;Torralba&#21644;Efros&#21313;&#24180;&#21069;&#25552;&#20986;&#30340;&#8220;&#25968;&#25454;&#38598;&#20998;&#31867;&#8221;&#23454;&#39564;&#65292;&#22312;&#25317;&#26377;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#21644;&#24076;&#26395;&#26356;&#23569;&#20559;&#35265;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#26356;&#24378;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26032;&#26102;&#20195;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;&#20998;&#31867;&#22270;&#20687;&#26469;&#33258;&#21738;&#20010;&#25968;&#25454;&#38598;&#26041;&#38754;&#21462;&#24471;&#20986;&#33394;&#30340;&#20934;&#30830;&#24615;&#65306;&#20363;&#22914;&#65292;&#23545;&#20110;&#21253;&#21547;YFCC&#12289;CC&#21644;DataComp&#25968;&#25454;&#38598;&#30340;&#19977;&#20998;&#31867;&#38382;&#39064;&#30340;&#39564;&#35777;&#25968;&#25454;&#65292;&#25105;&#20204;&#25253;&#21578;84.7%&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#20998;&#31867;&#22120;&#21487;&#20197;&#23398;&#20064;&#21040;&#21487;&#25512;&#24191;&#21644;&#21487;&#36716;&#31227;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#36825;&#19981;&#33021;&#31616;&#21333;&#22320;&#35299;&#37322;&#20026;&#35760;&#24518;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#21457;&#29616;&#33021;&#28608;&#21169;&#31038;&#21306;&#37325;&#26032;&#24605;&#32771;&#28041;&#21450;&#25968;&#25454;&#38598;&#20559;&#35265;&#21644;&#27169;&#22411;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08632v1 Announce Type: cross  Abstract: We revisit the "dataset classification" experiment suggested by Torralba and Efros a decade ago, in the new era with large-scale, diverse, and hopefully less biased datasets as well as more capable neural network architectures. Surprisingly, we observe that modern neural networks can achieve excellent accuracy in classifying which dataset an image is from: e.g., we report 84.7% accuracy on held-out validation data for the three-way classification problem consisting of the YFCC, CC, and DataComp datasets. Our further experiments show that such a dataset classifier could learn semantic features that are generalizable and transferable, which cannot be simply explained by memorization. We hope our discovery will inspire the community to rethink the issue involving dataset bias and model capabilities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32467;&#21512;&#20102;&#23567;&#27874;&#20998;&#26512;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#21033;&#29992;&#19981;&#21516;&#28040;&#22833;&#30697;&#30340;Daubechies&#23567;&#27874;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#65292;&#24182;&#27604;&#36739;&#20102;&#38750;&#38477;&#23567;&#27874;&#21464;&#25442;&#21644;&#38750;&#38477;&#23567;&#27874;&#21253;&#21464;&#25442;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#26356;&#24191;&#27867;&#30340;&#39044;&#27979;&#26041;&#27861;&#19978;&#35780;&#20272;&#20102;&#36825;&#20123;&#23567;&#27874;&#29305;&#24449;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.08630</link><description>&lt;p&gt;
&#21033;&#29992;&#38750;&#38477;&#23567;&#27874;&#21253;&#29305;&#24449;&#21644;Transformer&#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Leveraging Non-Decimated Wavelet Packet Features and Transformer Models for Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32467;&#21512;&#20102;&#23567;&#27874;&#20998;&#26512;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#21033;&#29992;&#19981;&#21516;&#28040;&#22833;&#30697;&#30340;Daubechies&#23567;&#27874;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#65292;&#24182;&#27604;&#36739;&#20102;&#38750;&#38477;&#23567;&#27874;&#21464;&#25442;&#21644;&#38750;&#38477;&#23567;&#27874;&#21253;&#21464;&#25442;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#26356;&#24191;&#27867;&#30340;&#39044;&#27979;&#26041;&#27861;&#19978;&#35780;&#20272;&#20102;&#36825;&#20123;&#23567;&#27874;&#29305;&#24449;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#32467;&#21512;&#20102;&#23567;&#27874;&#20998;&#26512;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#20132;&#21449;&#39564;&#35777;&#38454;&#27573;&#36873;&#25321;&#20351;&#29992;&#19981;&#21516;&#28040;&#22833;&#30697;&#30340;Daubechies&#23567;&#27874;&#20316;&#20026;&#38750;&#26102;&#38388;&#21644;&#26102;&#38388;&#39044;&#27979;&#26041;&#27861;&#30340;&#36755;&#20837;&#29305;&#24449;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#27604;&#36739;&#20351;&#29992;&#38750;&#38477;&#23567;&#27874;&#21464;&#25442;&#21644;&#38750;&#38477;&#23567;&#27874;&#21253;&#21464;&#25442;&#26469;&#35745;&#31639;&#36825;&#20123;&#29305;&#24449;&#65292;&#21518;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#22823;&#30340;&#28508;&#22312;&#26377;&#29992;&#31995;&#25968;&#21521;&#37327;&#38598;&#21512;&#12290;&#23567;&#27874;&#31995;&#25968;&#26159;&#20351;&#29992;&#20856;&#22411;&#37329;&#23383;&#22612;&#31639;&#27861;&#30340;&#24179;&#31227;&#29256;&#26412;&#35745;&#31639;&#30340;&#65292;&#20197;&#30830;&#20445;&#27809;&#26377;&#23558;&#26410;&#26469;&#20449;&#24687;&#27844;&#28431;&#21040;&#36825;&#20123;&#36755;&#20837;&#20013;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20123;&#23567;&#27874;&#29305;&#24449;&#22312;&#27604;&#20197;&#21069;&#30340;&#30740;&#31350;&#26356;&#24191;&#27867;&#30340;&#19968;&#32452;&#39044;&#27979;&#26041;&#27861;&#19978;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#26102;&#38388;&#21644;&#38750;&#26102;&#38388;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08630v1 Announce Type: cross  Abstract: This article combines wavelet analysis techniques with machine learning methods for univariate time series forecasting, focusing on three main contributions. Firstly, we consider the use of Daubechies wavelets with different numbers of vanishing moments as input features to both non-temporal and temporal forecasting methods, by selecting these numbers during the cross-validation phase. Secondly, we compare the use of both the non-decimated wavelet transform and the non-decimated wavelet packet transform for computing these features, the latter providing a much larger set of potentially useful coefficient vectors. The wavelet coefficients are computed using a shifted version of the typical pyramidal algorithm to ensure no leakage of future information into these inputs. Thirdly, we evaluate the use of these wavelet features on a significantly wider set of forecasting methods than previous studies, including both temporal and non-tempora
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20449;&#24230;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#31232;&#32570;&#32780;&#26114;&#36149;&#30340;&#39640;&#20445;&#30495;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08627</link><description>&lt;p&gt;
&#22810;&#20449;&#24230;&#32447;&#24615;&#22238;&#24402;&#29992;&#20110;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#31232;&#32570;&#25968;&#25454;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multifidelity linear regression for scientific machine learning from scarce data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08627
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20449;&#24230;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#31232;&#32570;&#32780;&#26114;&#36149;&#30340;&#39640;&#20445;&#30495;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25311;&#21512;&#32473;&#23450;&#21442;&#25968;&#21270;&#27169;&#22411;&#31867;&#30340;&#21442;&#25968;&#26469;&#36866;&#24212;&#25968;&#25454;&#65292;&#20316;&#20026;&#23398;&#20064;&#22797;&#26434;&#24037;&#31243;&#31995;&#32479;&#30340;&#20195;&#29702;&#27169;&#22411;&#30340;&#28508;&#22312;&#26041;&#27861;&#24050;&#32463;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#29615;&#22659;&#20013;&#65292;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;ML&#27169;&#22411;&#30340;&#39640;&#20445;&#30495;&#25968;&#25454;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#29992;&#20110;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#39044;&#31639;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#22810;&#20449;&#24230;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#25968;&#25454;&#30340;&#21508;&#31181;&#20445;&#30495;&#24230;&#21644;&#25104;&#26412;&#21487;&#29992;&#30340;&#31185;&#23398;&#32972;&#26223;&#65307;&#20363;&#22914;&#65292;&#39640;&#20445;&#30495;&#25968;&#25454;&#21487;&#33021;&#30001;&#26114;&#36149;&#30340;&#20840;&#38754;&#35299;&#26512;&#30340;&#29289;&#29702;&#27169;&#25311;&#29983;&#25104;&#65292;&#32780;&#20302;&#20445;&#30495;&#25968;&#25454;&#21487;&#33021;&#26469;&#33258;&#22522;&#20110;&#31616;&#21270;&#30340;&#26356;&#20415;&#23452;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08627v1 Announce Type: cross  Abstract: Machine learning (ML) methods, which fit to data the parameters of a given parameterized model class, have garnered significant interest as potential methods for learning surrogate models for complex engineering systems for which traditional simulation is expensive. However, in many scientific and engineering settings, generating high-fidelity data on which to train ML models is expensive, and the available budget for generating training data is limited. ML models trained on the resulting scarce high-fidelity data have high variance and are sensitive to vagaries of the training data set. We propose a new multifidelity training approach for scientific machine learning that exploits the scientific context where data of varying fidelities and costs are available; for example high-fidelity data may be generated by an expensive fully resolved physics simulation whereas lower-fidelity data may arise from a cheaper model based on simplifying 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21518;&#35757;&#32451;&#26657;&#27491;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#31639;&#27861;Verifix&#22312;&#21021;&#22987;&#35757;&#32451;&#21518;&#26657;&#27491;&#27169;&#22411;&#26435;&#37325;&#20197;&#20943;&#36731;&#26631;&#31614;&#22122;&#22768;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#30340;&#38656;&#27714;</title><link>https://arxiv.org/abs/2403.08618</link><description>&lt;p&gt;
Verifix: &#21518;&#35757;&#32451;&#26657;&#27491;&#20197;&#25913;&#21892;&#20855;&#26377;&#32463;&#36807;&#39564;&#35777;&#26679;&#26412;&#30340;&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Verifix: Post-Training Correction to Improve Label Noise Robustness with Verified Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08618
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21518;&#35757;&#32451;&#26657;&#27491;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#31639;&#27861;Verifix&#22312;&#21021;&#22987;&#35757;&#32451;&#21518;&#26657;&#27491;&#27169;&#22411;&#26435;&#37325;&#20197;&#20943;&#36731;&#26631;&#31614;&#22122;&#22768;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#38169;&#35823;&#65292;&#21363;&#35757;&#32451;&#26679;&#26412;&#20855;&#26377;&#19981;&#27491;&#30830;&#30340;&#26631;&#31614;&#65292;&#21487;&#33021;&#20005;&#37325;&#25439;&#23475;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#38169;&#35823;&#24448;&#24448;&#26469;&#33258;&#38750;&#19987;&#23478;&#26631;&#27880;&#25110;&#25932;&#23545;&#25915;&#20987;&#12290;&#33719;&#21462;&#22823;&#22411;&#12289;&#23436;&#20840;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#25104;&#26412;&#39640;&#65292;&#24403;&#26377;&#24178;&#20928;&#30340;&#25968;&#25454;&#38598;&#21487;&#29992;&#26102;&#65292;&#37325;&#26032;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#23601;&#21464;&#24471;&#35745;&#31639;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21518;&#35757;&#32451;&#26657;&#27491;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#21021;&#22987;&#35757;&#32451;&#21518;&#35843;&#25972;&#27169;&#22411;&#21442;&#25968;&#20197;&#20943;&#36731;&#26631;&#31614;&#22122;&#22768;&#30340;&#26032;&#33539;&#24335;&#65292;&#28040;&#38500;&#20102;&#37325;&#26032;&#35757;&#32451;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Verifix&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#21033;&#29992;&#19968;&#20010;&#23567;&#30340;&#12289;&#32463;&#36807;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21333;&#20010;&#26356;&#26032;&#26657;&#27491;&#27169;&#22411;&#26435;&#37325;&#12290;Verifix&#20351;&#29992;SVD&#20272;&#35745;&#24178;&#20928;&#28608;&#27963;&#31354;&#38388;&#65292;&#28982;&#21518;&#23558;&#27169;&#22411;&#30340;&#26435;&#37325;&#25237;&#24433;&#21040;&#36825;&#20010;&#31354;&#38388;&#19978;&#65292;&#20197;&#25233;&#21046;&#23545;&#24212;&#20110;&#25439;&#22351;&#25968;&#25454;&#30340;&#28608;&#27963;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Verifix&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08618v1 Announce Type: cross  Abstract: Label corruption, where training samples have incorrect labels, can significantly degrade the performance of machine learning models. This corruption often arises from non-expert labeling or adversarial attacks. Acquiring large, perfectly labeled datasets is costly, and retraining large models from scratch when a clean dataset becomes available is computationally expensive. To address this challenge, we propose Post-Training Correction, a new paradigm that adjusts model parameters after initial training to mitigate label noise, eliminating the need for retraining. We introduce Verifix, a novel Singular Value Decomposition (SVD) based algorithm that leverages a small, verified dataset to correct the model weights using a single update. Verifix uses SVD to estimate a Clean Activation Space and then projects the model's weights onto this space to suppress activations corresponding to corrupted data. We demonstrate Verifix's effectiveness 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21551;&#21457;&#24335;&#29305;&#24449;&#21644;&#23398;&#20064;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31038;&#20132;&#32593;&#32476;&#20013;&#32570;&#22833;&#38142;&#25509;&#30340;&#39044;&#27979;&#20219;&#21153;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#25552;&#21319;</title><link>https://arxiv.org/abs/2403.08613</link><description>&lt;p&gt;
&#21033;&#29992;&#34920;&#31034;&#23398;&#20064;&#21644;&#22522;&#20110;&#21551;&#21457;&#24335;&#29305;&#24449;&#30340;&#26041;&#27861;&#36827;&#34892;&#31038;&#20132;&#32593;&#32476;&#30340;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Link Prediction for Social Networks using Representation Learning and Heuristic-based Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08613
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21551;&#21457;&#24335;&#29305;&#24449;&#21644;&#23398;&#20064;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31038;&#20132;&#32593;&#32476;&#20013;&#32570;&#22833;&#38142;&#25509;&#30340;&#39044;&#27979;&#20219;&#21153;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#32593;&#32476;&#30340;&#35268;&#27169;&#21644;&#30456;&#20851;&#24615;&#21576;&#25351;&#25968;&#22686;&#38271;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#31038;&#20132;&#32593;&#32476;&#20013;&#32570;&#22833;&#30340;&#38142;&#25509;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#65292;&#20197;&#29983;&#25104;&#31038;&#20132;&#32593;&#32476;&#20013;&#33410;&#28857;&#21644;&#36793;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#24110;&#21161;&#39044;&#27979;&#32570;&#22833;&#30340;&#38142;&#25509;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#21313;&#31181;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#30340;&#32467;&#26524;&#65292;&#36825;&#20123;&#25216;&#26415;&#20998;&#20026;&#32467;&#26500;&#23884;&#20837;&#12289;&#22522;&#20110;&#37051;&#23621;&#30340;&#23884;&#20837;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#21551;&#21457;&#24335;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#20351;&#29992;&#38598;&#25104;&#20998;&#31867;&#22120;&#21644;&#23450;&#21046;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#22522;&#20110;&#21551;&#21457;&#24335;&#29305;&#24449;&#21644;&#23398;&#20064;&#34920;&#31034;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08613v1 Announce Type: cross  Abstract: The exponential growth in scale and relevance of social networks enable them to provide expansive insights. Predicting missing links in social networks efficiently can help in various modern-day business applications ranging from generating recommendations to influence analysis. Several categories of solutions exist for the same. Here, we explore various feature extraction techniques to generate representations of nodes and edges in a social network that allow us to predict missing links. We compare the results of using ten feature extraction techniques categorized across Structural embeddings, Neighborhood-based embeddings, Graph Neural Networks, and Graph Heuristics, followed by modeling with ensemble classifiers and custom Neural Networks. Further, we propose combining heuristic-based features and learned representations that demonstrate improved performance for the link prediction task on social network datasets. Using this method 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23558;&#33258;&#36866;&#24212;&#27493;&#38271;&#24341;&#20837;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#20174;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25910;&#25947;&#21040;&#27491;&#30830;&#30340;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2403.08609</link><description>&lt;p&gt;
&#28145;&#24230;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#30340;&#23616;&#37096;&#33258;&#36866;&#24212;&#21644;&#21487;&#25193;&#23637;&#25193;&#25955;&#37319;&#26679;&#26041;&#27861;&#30340;&#25910;&#25947;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Locally Adaptive and Scalable Diffusion-Based Sampling Methods for Deep Bayesian Neural Network Posteriors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23558;&#33258;&#36866;&#24212;&#27493;&#38271;&#24341;&#20837;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#20174;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25910;&#25947;&#21040;&#27491;&#30830;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22312;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#22914;&#21307;&#23398;&#25104;&#20687;&#20013;&#38656;&#35201;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#12290;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26159;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20174;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#20998;&#24067;&#20013;&#29983;&#25104;&#26679;&#26412;&#26159;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#26397;&#30528;&#36825;&#20010;&#26041;&#21521;&#30340;&#19968;&#20010;&#37325;&#35201;&#36827;&#23637;&#23558;&#26159;&#23558;&#31867;&#20284;&#20110;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#22120;&#30340;&#33258;&#36866;&#24212;&#27493;&#38271;&#32435;&#20837;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#31639;&#27861;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#35745;&#31639;&#38656;&#27714;&#12290;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#19968;&#20123;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#23454;&#29616;&#36825;&#19968;&#23646;&#24615;&#30340;&#37319;&#26679;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26159;&#21542;&#30830;&#23454;&#25910;&#25947;&#21040;&#27491;&#30830;&#30340;&#20998;&#24067;&#21602;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08609v1 Announce Type: new  Abstract: Achieving robust uncertainty quantification for deep neural networks represents an important requirement in many real-world applications of deep learning such as medical imaging where it is necessary to assess the reliability of a neural network's prediction. Bayesian neural networks are a promising approach for modeling uncertainties in deep neural networks. Unfortunately, generating samples from the posterior distribution of neural networks is a major challenge. One significant advance in that direction would be the incorporation of adaptive step sizes, similar to modern neural network optimizers, into Monte Carlo Markov chain sampling algorithms without significantly increasing computational demand. Over the past years, several papers have introduced sampling algorithms with claims that they achieve this property. However, do they indeed converge to the correct distribution? In this paper, we demonstrate that these methods can have a 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39044;&#27979;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#30340;&#39057;&#29575;&#20869;&#23481;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#21644;&#23569;&#21463;&#35797;&#32773;&#24773;&#20917;&#19979;&#36229;&#36234;&#23436;&#20840;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.08592</link><description>&lt;p&gt;
&#29992;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#23454;&#29616;&#39640;&#25928;&#30340;&#30561;&#30496;&#20998;&#26399;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Sleep Staging with Synthetic Time Series Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08592
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39044;&#27979;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#30340;&#39057;&#29575;&#20869;&#23481;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#21644;&#23569;&#21463;&#35797;&#32773;&#24773;&#20917;&#19979;&#36229;&#36234;&#23436;&#20840;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#26102;&#38388;&#24207;&#21015;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#30001;&#20110;&#20154;&#31867;&#21463;&#35797;&#32773;&#20043;&#38388;&#30340;&#22823;&#37327;&#21464;&#24322;&#21644;&#36890;&#24120;&#35268;&#27169;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#31574;&#30053;&#65292;&#20363;&#22914;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#24191;&#27867;&#30340;&#23454;&#35777;&#25968;&#25454;&#38598;&#12290;&#21463;&#35745;&#31639;&#26426;&#35270;&#35273;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#31216;&#20026;&#8220;&#39057;&#29575;&#39044;&#35757;&#32451;&#8221;&#65292;&#36890;&#36807;&#39044;&#27979;&#38543;&#26426;&#29983;&#25104;&#30340;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#30340;&#39057;&#29575;&#20869;&#23481;&#26469;&#20026;&#30561;&#30496;&#20998;&#26399;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;&#38480;&#25968;&#25454;&#21644;&#23569;&#21463;&#35797;&#32773;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#23436;&#20840;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#22312;&#35768;&#22810;&#21463;&#35797;&#32773;&#30340;&#24773;&#22659;&#20013;&#34920;&#29616;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#39057;&#29575;&#20449;&#24687;&#23545;&#20110;&#30561;&#30496;&#20998;&#26399;&#35780;&#20998;&#30340;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#34920;&#26126;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#20102;&#36229;&#20986;&#39057;&#29575;&#20449;&#24687;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08592v1 Announce Type: new  Abstract: Analyzing electroencephalographic (EEG) time series can be challenging, especially with deep neural networks, due to the large variability among human subjects and often small datasets. To address these challenges, various strategies, such as self-supervised learning, have been suggested, but they typically rely on extensive empirical datasets. Inspired by recent advances in computer vision, we propose a pretraining task termed "frequency pretraining" to pretrain a neural network for sleep staging by predicting the frequency content of randomly generated synthetic time series. Our experiments demonstrate that our method surpasses fully supervised learning in scenarios with limited data and few subjects, and matches its performance in regimes with many subjects. Furthermore, our results underline the relevance of frequency information for sleep stage scoring, while also demonstrating that deep neural networks utilize information beyond fr
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#22312;&#35757;&#32451;&#38454;&#27573;&#24341;&#20837;&#29289;&#29702;&#20449;&#24687;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#22312;&#27827;&#27969;&#27700;&#21147;&#23398;&#20013;&#30340;&#39044;&#27979;&#33021;&#21147;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08589</link><description>&lt;p&gt;
&#30828;&#20214;&#24314;&#27169;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#33021;&#21147;&#21450;&#29289;&#29702;&#20449;&#24687;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can physical information aid the generalization ability of Neural Networks for hydraulic modeling?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08589
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#22312;&#35757;&#32451;&#38454;&#27573;&#24341;&#20837;&#29289;&#29702;&#20449;&#24687;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#22312;&#27827;&#27969;&#27700;&#21147;&#23398;&#20013;&#30340;&#39044;&#27979;&#33021;&#21147;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#27827;&#27969;&#27700;&#21147;&#23398;&#39046;&#22495;&#36973;&#21463;&#25968;&#25454;&#31232;&#32570;&#30340;&#22256;&#25200;&#65292;&#26159;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#25361;&#25112;&#20043;&#19968;&#65292;&#20294;&#31070;&#32463;&#32593;&#32476;&#22312;&#27827;&#27969;&#27700;&#21147;&#23398;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#22788;&#20110;&#33804;&#33469;&#38454;&#27573;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#34987;&#35777;&#26126;&#32570;&#20047;&#39044;&#27979;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22312;&#35757;&#32451;&#38454;&#27573;&#24341;&#20837;&#29289;&#29702;&#20449;&#24687;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#19968;&#24819;&#27861;&#28304;&#33258;&#20854;&#20182;&#32972;&#26223;&#19979;&#26368;&#36817;&#25552;&#20986;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#12290;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#20197;&#25511;&#21046;&#29616;&#35937;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#27531;&#24046;&#30340;&#24418;&#24335;&#23884;&#20837;&#29289;&#29702;&#20449;&#24687;&#65292;&#22240;&#27492;&#34987;&#26500;&#24819;&#20026;&#31070;&#32463;&#27714;&#35299;&#22120;&#65292;&#21363;&#20256;&#32479;&#25968;&#20540;&#27714;&#35299;&#22120;&#30340;&#26367;&#20195;&#21697;&#12290;&#36825;&#31181;&#26041;&#27861;&#24456;&#23569;&#36866;&#29992;&#20110;&#29615;&#22659;&#27700;&#21147;&#23398;&#65292;&#37027;&#37324;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#24456;&#22823;&#65292;&#24182;&#19988;&#35745;&#31639;PDE&#30340;&#27531;&#24046;&#23384;&#22312;&#30528;&#31867;&#20284;&#20110;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08589v1 Announce Type: new  Abstract: Application of Neural Networks to river hydraulics is fledgling, despite the field suffering from data scarcity, a challenge for machine learning techniques. Consequently, many purely data-driven Neural Networks proved to lack predictive capabilities. In this work, we propose to mitigate such problem by introducing physical information into the training phase. The idea is borrowed from Physics-Informed Neural Networks which have been recently proposed in other contexts. Physics-Informed Neural Networks embed physical information in the form of the residual of the Partial Differential Equations (PDEs) governing the phenomenon and, as such, are conceived as neural solvers, i.e. an alternative to traditional numerical solvers. Such approach is seldom suitable for environmental hydraulics, where epistemic uncertainties are large, and computing residuals of PDEs exhibits difficulties similar to those faced by classical numerical methods. Inst
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39044;&#26465;&#20214;&#21270;&#65292;&#30740;&#31350;&#20102;SGD&#22312;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#23545;&#27604;&#20102;&#39044;&#26465;&#20214;&#21270;SGD&#21644;&#65288;&#26631;&#20934;&#21644;&#39044;&#26465;&#20214;&#21270;&#65289;&#23725;&#22238;&#24402;&#65292;&#20026;&#25913;&#21892;SGD&#29702;&#35299;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#20851;&#38190;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.08585</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#26465;&#20214;&#21270;&#25913;&#21892;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Improving Implicit Regularization of SGD with Preconditioning for Least Square Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08585
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39044;&#26465;&#20214;&#21270;&#65292;&#30740;&#31350;&#20102;SGD&#22312;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#23545;&#27604;&#20102;&#39044;&#26465;&#20214;&#21270;SGD&#21644;&#65288;&#26631;&#20934;&#21644;&#39044;&#26465;&#20214;&#21270;&#65289;&#23725;&#22238;&#24402;&#65292;&#20026;&#25913;&#21892;SGD&#29702;&#35299;&#21644;&#24212;&#29992;&#25552;&#20379;&#20102;&#20851;&#38190;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#31639;&#27861;&#27491;&#21017;&#21270;&#25928;&#26524;&#65292;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#30340;&#27867;&#21270;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;SGD&#30340;&#27867;&#21270;&#24615;&#33021;&#26377;&#26102;&#20250;&#27604;&#23725;&#22238;&#24402;&#24046;&#65292;&#36825;&#26159;&#30001;&#20110;&#27839;&#19981;&#21516;&#32500;&#24230;&#30340;&#20248;&#21270;&#19981;&#22343;&#21248;&#36896;&#25104;&#30340;&#12290;&#39044;&#26465;&#20214;&#21270;&#36890;&#36807;&#37325;&#26032;&#24179;&#34913;&#27839;&#19981;&#21516;&#26041;&#21521;&#30340;&#20248;&#21270;&#26469;&#25552;&#20379;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#33258;&#28982;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#39044;&#26465;&#20214;&#21270;&#33021;&#22815;&#25552;&#21319;SGD&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#31243;&#24230;&#20197;&#21450;&#23427;&#26159;&#21542;&#33021;&#22815;&#22635;&#34917;&#29616;&#26377;&#19982;&#23725;&#22238;&#24402;&#20043;&#38388;&#30340;&#24046;&#36317;&#20173;&#19981;&#30830;&#23450;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#26465;&#20214;&#21270;&#23545;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#20013;SGD&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20840;&#38754;&#27604;&#36739;&#20102;&#39044;&#26465;&#20214;&#21270;SGD&#19982;&#65288;&#26631;&#20934;&#21644;&#39044;&#26465;&#20214;&#21270;&#65289;&#23725;&#22238;&#24402;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#20102;&#35299;&#21644;&#25913;&#21892;SGD&#20570;&#20986;&#20102;&#20960;&#39033;&#20851;&#38190;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08585v1 Announce Type: new  Abstract: Stochastic gradient descent (SGD) exhibits strong algorithmic regularization effects in practice and plays an important role in the generalization of modern machine learning. However, prior research has revealed instances where the generalization performance of SGD is worse than ridge regression due to uneven optimization along different dimensions. Preconditioning offers a natural solution to this issue by rebalancing optimization across different directions. Yet, the extent to which preconditioning can enhance the generalization performance of SGD and whether it can bridge the existing gap with ridge regression remains uncertain. In this paper, we study the generalization performance of SGD with preconditioning for the least squared problem. We make a comprehensive comparison between preconditioned SGD and (standard \&amp; preconditioned) ridge regression. Our study makes several key contributions toward understanding and improving SGD wit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#35757;&#32451;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#30340;&#23616;&#37096;&#24212;&#29992;&#65292;&#20197;&#20811;&#26381;&#37327;&#23376;&#36864;&#28779;&#22120;&#21463;&#38480;&#36830;&#36890;&#24615;&#25152;&#23548;&#33268;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#38480;&#21046;</title><link>https://arxiv.org/abs/2403.08584</link><description>&lt;p&gt;
&#22522;&#20110;&#37327;&#23376;&#36864;&#28779;&#22120;&#35757;&#32451;&#30340;&#23616;&#37096;&#20108;&#36827;&#21046;&#21644;&#22810;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;
&lt;/p&gt;
&lt;p&gt;
Local Binary and Multiclass SVMs Trained on a Quantum Annealer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08584
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37327;&#23376;&#35757;&#32451;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#30340;&#23616;&#37096;&#24212;&#29992;&#65292;&#20197;&#20811;&#26381;&#37327;&#23376;&#36864;&#28779;&#22120;&#21463;&#38480;&#36830;&#36890;&#24615;&#25152;&#23548;&#33268;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#20363;&#22914;&#22312;&#36965;&#24863;&#20013;&#65289;&#65292;&#26082;&#29992;&#20110;&#20998;&#31867;&#21448;&#29992;&#20110;&#22238;&#24402;&#20219;&#21153;&#30340;&#34920;&#36798;&#24335;&#12290;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#24037;&#20316;&#37327;&#23376;&#36864;&#28779;&#22120;&#30340;&#20986;&#29616;&#65292;&#25552;&#20986;&#20102;&#28151;&#21512;SVM&#27169;&#22411;&#65292;&#20854;&#29305;&#24449;&#26159;&#37327;&#23376;&#35757;&#32451;&#21644;&#32463;&#20856;&#25191;&#34892;&#12290;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#19982;&#20854;&#32463;&#20856;&#23545;&#24212;&#29289;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;&#37327;&#23376;&#36864;&#28779;&#22120;&#30340;&#21463;&#38480;&#36830;&#36890;&#24615;&#65292;&#23427;&#20204;&#22312;&#35757;&#32451;&#38598;&#22823;&#23567;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#21033;&#29992;&#22823;&#22411;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;&#19982;&#22320;&#29699;&#35266;&#27979;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#65289;&#65292;&#38656;&#35201;&#19968;&#31181;&#31574;&#30053;&#12290;&#22312;&#32463;&#20856;&#39046;&#22495;&#20013;&#65292;&#24050;&#32463;&#25104;&#21151;&#35777;&#26126;&#20102;&#23616;&#37096;SVM&#65292;&#21363;&#30001;k&#20010;&#26368;&#36817;&#37051;&#27169;&#22411;&#36873;&#25321;&#30340;&#25968;&#25454;&#26679;&#26412;&#35757;&#32451;&#30340;SVM&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#32463;&#39564;&#24615;&#35780;&#20272;&#20102;&#37327;&#23376;&#35757;&#32451;&#30340;SVM&#27169;&#22411;&#30340;&#23616;&#37096;&#24212;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#20811;&#26381;&#20102;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08584v1 Announce Type: cross  Abstract: Support vector machines (SVMs) are widely used machine learning models (e.g., in remote sensing), with formulations for both classification and regression tasks. In the last years, with the advent of working quantum annealers, hybrid SVM models characterised by quantum training and classical execution have been introduced. These models have demonstrated comparable performance to their classical counterparts. However, they are limited in the training set size due to the restricted connectivity of the current quantum annealers. Hence, to take advantage of large datasets (like those related to Earth observation), a strategy is required. In the classical domain, local SVMs, namely, SVMs trained on the data samples selected by a k-nearest neighbors model, have already proven successful. Here, the local application of quantum-trained SVM models is proposed and empirically assessed. In particular, this approach allows overcoming the constrain
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#23545;&#27491;&#20132;&#22522;&#20998;&#27573;&#22810;&#39033;&#24335;&#36924;&#36817;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.08579</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#30340;&#27491;&#20132;&#22522;&#20998;&#27573;&#22810;&#39033;&#24335;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Optimized Orthogonal Basis Piecewise Polynomial Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#23545;&#27491;&#20132;&#22522;&#20998;&#27573;&#22810;&#39033;&#24335;&#36924;&#36817;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#27573;&#22810;&#39033;&#24335;&#65288;PPs&#65289;&#22312;&#20960;&#20010;&#24037;&#31243;&#23398;&#31185;&#20013;&#34987;&#20351;&#29992;&#65292;&#27604;&#22914;&#22312;&#36712;&#36857;&#35268;&#21010;&#20013;&#65292;&#29992;&#26469;&#36924;&#36817;&#20197;&#19968;&#32452;&#28857;&#32473;&#20986;&#30340;&#20301;&#32622;&#36718;&#24275;&#12290; &#37492;&#20110;&#36924;&#36817;&#30446;&#26631;&#20197;&#21450;&#29305;&#23450;&#39046;&#22495;&#35201;&#27714;&#65292;&#27604;&#22914;Ck-&#36830;&#32493;&#24615;&#65292;&#21487;&#20197;&#34987;&#26500;&#36896;&#20026;&#19968;&#20010;&#26041;&#31243;&#32452;&#65292;&#32467;&#26524;&#21487;&#20197;&#30452;&#25509;&#35745;&#31639;&#65292;&#36825;&#26679;&#30340;&#38381;&#24335;&#35299;&#23545;&#20110;&#22810;&#39033;&#24335;&#27425;&#25968;&#12289;&#22810;&#39033;&#24335;&#22522;&#30784;&#25110;&#32773;&#28155;&#21152;&#36827;&#19968;&#27493;&#30340;&#29305;&#23450;&#39046;&#22495;&#35201;&#27714;&#26041;&#38754;&#20855;&#26377;&#26377;&#38480;&#30340;&#28789;&#27963;&#24615;&#12290;&#36275;&#22815;&#22797;&#26434;&#30340;&#20248;&#21270;&#30446;&#26631;&#24456;&#24555;&#35201;&#27714;&#20351;&#29992;&#25968;&#20540;&#26041;&#27861;&#65292;&#27604;&#22914;&#26799;&#24230;&#19979;&#38477;&#12290;&#30001;&#20110;&#26799;&#24230;&#19979;&#38477;&#26159;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#26680;&#24515;&#65292;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26694;&#26550;&#27604;&#22914;TensorFlow&#25552;&#20379;&#20102;&#19968;&#22871;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#65292;&#21487;&#33021;&#36866;&#29992;&#20110;&#35757;&#32451;&#20219;&#21153;&#20043;&#22806;&#30340;&#24191;&#27867;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;PP&#27169;&#24335;&#30340;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08579v1 Announce Type: new  Abstract: Piecewise Polynomials (PPs) are utilized in several engineering disciplines, like trajectory planning, to approximate position profiles given in the form of a set of points. While the approximation target along with domain-specific requirements, like Ck -continuity, can be formulated as a system of equations and a result can be computed directly, such closed-form solutions posses limited flexibility with respect to polynomial degrees, polynomial bases or adding further domain-specific requirements. Sufficiently complex optimization goals soon call for the use of numerical methods, like gradient descent. Since gradient descent lies at the heart of training Artificial Neural Networks (ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set of gradient-based optimizers potentially suitable for a wide range of optimization problems beyond the training task for ANNs. Our approach is to utilize the versatility of PP mode
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Caformer&#30340;&#26032;&#26694;&#26550;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#25324;&#21160;&#24577;&#23398;&#20064;&#22120;&#12289;&#29615;&#22659;&#23398;&#20064;&#22120;&#21644;&#20381;&#36182;&#20851;&#31995;&#23398;&#20064;&#22120;&#65292;&#26088;&#22312;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#35299;&#20915;&#22312;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#20013;&#25429;&#25417;&#36328;&#32500;&#24230;&#21644;&#36328;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.08572</link><description>&lt;p&gt;
&#37325;&#26032;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#24605;&#32771;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Caformer: Rethinking Time Series Analysis from Causal Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08572
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Caformer&#30340;&#26032;&#26694;&#26550;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#25324;&#21160;&#24577;&#23398;&#20064;&#22120;&#12289;&#29615;&#22659;&#23398;&#20064;&#22120;&#21644;&#20381;&#36182;&#20851;&#31995;&#23398;&#20064;&#22120;&#65292;&#26088;&#22312;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#35299;&#20915;&#22312;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#20013;&#25429;&#25417;&#36328;&#32500;&#24230;&#21644;&#36328;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26159;&#19968;&#20010;&#22312;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#20013;&#26377;&#25928;&#25429;&#25417;&#36328;&#32500;&#24230;&#21644;&#36328;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#22312;&#29615;&#22659;&#22240;&#32032;&#30340;&#32972;&#26223;&#19979;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#29615;&#22659;&#24341;&#36215;&#30340;&#34394;&#20551;&#30456;&#20851;&#28151;&#28102;&#20102;&#36328;&#32500;&#24230;&#21644;&#36328;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;Caformer(&#22240;&#26524;Transformer)&#65292;&#29992;&#20110;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#32452;&#20214;&#65306;&#21160;&#24577;&#23398;&#20064;&#22120;&#65292;&#29615;&#22659;&#23398;&#20064;&#22120;&#21644;&#20381;&#36182;&#20851;&#31995;&#23398;&#20064;&#22120;&#12290;&#21160;&#24577;&#23398;&#20064;&#22120;&#25581;&#31034;&#20102;&#32500;&#24230;&#20043;&#38388;&#30340;&#21160;&#24577;&#20132;&#20114;&#20316;&#29992;&#65292;&#29615;&#22659;&#23398;&#20064;&#22120;&#36890;&#36807;&#32972;&#38376;&#35843;&#25972;&#20943;&#36731;&#20102;&#29615;&#22659;&#24341;&#36215;&#30340;&#34394;&#20551;&#30456;&#20851;&#65292;&#20381;&#36182;&#20851;&#31995;&#23398;&#20064;&#22120;&#26088;&#22312;&#25512;&#26029;&#36328;&#36234;&#20004;&#32773;&#30340;&#31283;&#20581;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08572v1 Announce Type: new  Abstract: Time series analysis is a vital task with broad applications in various domains. However, effectively capturing cross-dimension and cross-time dependencies in non-stationary time series poses significant challenges, particularly in the context of environmental factors. The spurious correlation induced by the environment confounds the causal relationships between cross-dimension and cross-time dependencies. In this paper, we introduce a novel framework called Caformer (\underline{\textbf{Ca}}usal Trans\underline{\textbf{former}}) for time series analysis from a causal perspective. Specifically, our framework comprises three components: Dynamic Learner, Environment Learner, and Dependency Learner. The Dynamic Learner unveils dynamic interactions among dimensions, the Environment Learner mitigates spurious correlations caused by environment with a back-door adjustment, and the Dependency Learner aims to infer robust interactions across both
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#39537;&#21160;&#30340;GraphSAGE&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30001;&#19981;&#35268;&#21017;PDE&#31649;&#25511;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#20855;&#26377;&#38477;&#20302;&#31934;&#24230;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.08569</link><description>&lt;p&gt;
&#30001;&#29289;&#29702;&#39537;&#21160;&#30340;GraphSAGE&#26041;&#27861;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#25551;&#36848;&#30340;&#29289;&#29702;&#36807;&#31243;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
A Physics-driven GraphSAGE Method for Physical Process Simulations Described by Partial Differential Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08569
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#39537;&#21160;&#30340;GraphSAGE&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30001;&#19981;&#35268;&#21017;PDE&#31649;&#25511;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#20855;&#26377;&#38477;&#20302;&#31934;&#24230;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#21508;&#31181;&#35745;&#31639;&#29289;&#29702;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#22312;&#22788;&#29702;&#19982;&#22855;&#24322;&#24615;&#21644;&#25391;&#33633;&#31561;&#19981;&#35268;&#21017;&#24615;&#30456;&#20851;&#30340;&#38382;&#39064;&#26102;&#65292;&#35757;&#32451;&#30340;&#35299;&#36890;&#24120;&#20250;&#20986;&#29616;&#20302;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#24037;&#20316;&#21482;&#20026;&#39044;&#23450;&#36755;&#20837;&#21442;&#25968;&#25552;&#20379;&#35757;&#32451;&#35299;&#20915;&#26041;&#26696;&#12290;&#22914;&#26524;&#36755;&#20837;&#21442;&#25968;&#21457;&#29983;&#20219;&#20309;&#26356;&#25913;&#65292;&#21017;&#38656;&#35201;&#36827;&#34892;&#36716;&#31227;&#23398;&#20064;&#25110;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#19988;&#20256;&#32479;&#30340;&#25968;&#20540;&#25216;&#26415;&#20063;&#38656;&#35201;&#29420;&#31435;&#27169;&#25311;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Galerkin&#26041;&#27861;&#21644;&#20998;&#27573;&#22810;&#39033;&#24335;&#33410;&#28857;&#22522;&#30784;&#20989;&#25968;&#30340;&#29289;&#29702;&#39537;&#21160;GraphSAGE&#26041;&#27861;&#65288;PD-GraphSAGE&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#30001;&#19981;&#35268;&#21017;PDE&#31649;&#25511;&#30340;&#35745;&#31639;&#38382;&#39064;&#24182;&#21457;&#23637;&#21442;&#25968;PDE&#20195;&#29702;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#29289;&#29702;&#39046;&#22495;&#30340;&#22270;&#34920;&#31034;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#30001;&#20110;&#23616;&#37096;&#32454;&#21270;&#32780;&#20135;&#29983;&#30340;&#35780;&#20272;&#28857;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08569v1 Announce Type: new  Abstract: Physics-informed neural networks (PINNs) have successfully addressed various computational physics problems based on partial differential equations (PDEs). However, while tackling issues related to irregularities like singularities and oscillations, trained solutions usually suffer low accuracy. In addition, most current works only offer the trained solution for predetermined input parameters. If any change occurs in input parameters, transfer learning or retraining is required, and traditional numerical techniques also need an independent simulation. In this work, a physics-driven GraphSAGE approach (PD-GraphSAGE) based on the Galerkin method and piecewise polynomial nodal basis functions is presented to solve computational problems governed by irregular PDEs and to develop parametric PDE surrogate models. This approach employs graph representations of physical domains, thereby reducing the demands for evaluated points due to local refi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#33268;&#25552;&#31034;&#65288;CPrompt&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#26399;&#38388;&#25152;&#26377;&#29616;&#26377;&#20998;&#31867;&#22120;&#25509;&#21463;&#25552;&#31034;&#35757;&#32451;&#65292;&#23454;&#29616;&#26356;&#21152;&#23545;&#40784;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2403.08568</link><description>&lt;p&gt;
&#26080;&#38656;&#22797;&#20064;&#30340;&#19968;&#33268;&#25552;&#31034;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Consistent Prompting for Rehearsal-Free Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08568
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#33268;&#25552;&#31034;&#65288;CPrompt&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#26399;&#38388;&#25152;&#26377;&#29616;&#26377;&#20998;&#31867;&#22120;&#25509;&#21463;&#25552;&#31034;&#35757;&#32451;&#65292;&#23454;&#29616;&#26356;&#21152;&#23545;&#40784;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#20351;&#27169;&#22411;&#33021;&#22815;&#33258;&#20027;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#25110;&#25968;&#25454;&#27969;&#65292;&#32780;&#19981;&#20250;&#24536;&#35760;&#26087;&#30693;&#35782;&#12290;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#24314;&#31435;&#22312;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#65292;&#20197;&#39640;&#25928;&#22320;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#25552;&#31034;&#21644;&#20998;&#31867;&#22120;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#25581;&#31034;&#20102;&#20004;&#31181;&#31867;&#22411;&#12290;&#27979;&#35797;&#39044;&#27979;&#26159;&#20174;&#25152;&#26377;&#20998;&#31867;&#22120;&#20013;&#36827;&#34892;&#30340;&#65292;&#32780;&#35757;&#32451;&#21482;&#20851;&#27880;&#24403;&#21069;&#20219;&#21153;&#20998;&#31867;&#22120;&#32780;&#27809;&#26377;&#36827;&#34892;&#25972;&#20307;&#23545;&#40784;&#65292;&#23548;&#33268;&#20998;&#31867;&#22120;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#25552;&#31034;&#30340;&#19981;&#19968;&#33268;&#24615;&#34920;&#31034;&#27979;&#35797;&#26399;&#38388;&#36873;&#25321;&#30340;&#25552;&#31034;&#21487;&#33021;&#19982;&#35757;&#32451;&#26399;&#38388;&#19982;&#35813;&#20219;&#21153;&#20851;&#32852;&#30340;&#25552;&#31034;&#19981;&#23545;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#19968;&#33268;&#25552;&#31034;&#65288;CPrompt&#65289;&#65292;&#29992;&#20110;&#26356;&#21152;&#23545;&#40784;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25152;&#26377;&#29616;&#26377;&#30340;&#20998;&#31867;&#22120;&#37117;&#25509;&#21463;&#25552;&#31034;&#35757;&#32451;&#65292;&#20174;&#32780;&#24418;&#25104;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08568v1 Announce Type: cross  Abstract: Continual learning empowers models to adapt autonomously to the ever-changing environment or data streams without forgetting old knowledge. Prompt-based approaches are built on frozen pre-trained models to learn the task-specific prompts and classifiers efficiently. Existing prompt-based methods are inconsistent between training and testing, limiting their effectiveness. Two types of inconsistency are revealed. Test predictions are made from all classifiers while training only focuses on the current task classifier without holistic alignment, leading to Classifier inconsistency. Prompt inconsistency indicates that the prompt selected during testing may not correspond to the one associated with this task during training. In this paper, we propose a novel prompt-based method, Consistent Prompting (CPrompt), for more aligned training and testing. Specifically, all existing classifiers are exposed to prompt training, resulting in classifie
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20174;&#32467;&#26500;&#35282;&#24230;&#30740;&#31350;&#20102;&#22522;&#20110;&#32422;&#26463;&#30340;&#23398;&#20064;&#39532;&#23572;&#21487;&#22827;&#32593;&#32476;&#65292;&#21457;&#29616;&#20102;&#22270;&#30340;&#32467;&#26500;&#29305;&#24615;&#21644;&#23398;&#20064;&#25152;&#38656;&#27979;&#35797;&#25968;&#37327;&#20043;&#38388;&#30340;&#37325;&#35201;&#20851;&#31995;</title><link>https://arxiv.org/abs/2403.08562</link><description>&lt;p&gt;
&#32467;&#26500;&#35270;&#35282;&#19979;&#22522;&#20110;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#32593;&#32476;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Structural perspective on constraint-based learning of Markov networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20174;&#32467;&#26500;&#35282;&#24230;&#30740;&#31350;&#20102;&#22522;&#20110;&#32422;&#26463;&#30340;&#23398;&#20064;&#39532;&#23572;&#21487;&#22827;&#32593;&#32476;&#65292;&#21457;&#29616;&#20102;&#22270;&#30340;&#32467;&#26500;&#29305;&#24615;&#21644;&#23398;&#20064;&#25152;&#38656;&#27979;&#35797;&#25968;&#37327;&#20043;&#38388;&#30340;&#37325;&#35201;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#32593;&#32476;&#26159;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#20351;&#29992;&#26080;&#21521;&#22270;&#26469;&#34920;&#31034;&#21464;&#37327;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#20851;&#31995;&#12290;&#25105;&#20204;&#20851;&#27880;&#32422;&#26463;-based&#32467;&#26500;&#23398;&#20064;&#65292;&#36890;&#36807;&#25191;&#34892;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26080;&#21521;&#22270;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20851;&#20110;&#39532;&#23572;&#21487;&#22827;&#32593;&#32476;&#32422;&#26463;-based&#23398;&#20064;&#30340;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#30340;&#29702;&#35770;&#38480;&#21046;&#65306;&#27979;&#35797;&#25968;&#37327;&#21644;&#26465;&#20214;&#35774;&#32622;&#30340;&#22823;&#23567;&#12290;&#36825;&#20123;&#30028;&#38480;&#25581;&#31034;&#20102;&#22270;&#30340;&#32467;&#26500;&#29305;&#24615;&#19982;&#23398;&#20064;&#39532;&#23572;&#21487;&#22827;&#32593;&#32476;&#25152;&#38656;&#27979;&#35797;&#37327;&#20043;&#38388;&#30340;&#26377;&#36259;&#20114;&#21160;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#20986;&#21457;&#28857;&#26159;&#22270;&#21442;&#25968;&#26368;&#22823;&#25104;&#23545;&#36830;&#36890;&#24615; $\kappa$&#65292;&#21363;&#65292;&#22270;&#20013;&#36830;&#25509;&#19968;&#23545;&#39030;&#28857;&#30340;&#26368;&#22823;&#25968;&#37327;&#30340;&#39030;&#28857;&#19981;&#30456;&#20132;&#36335;&#24452;&#65292;&#36127;&#36131;&#29420;&#31435;&#24615;&#27979;&#35797;&#25152;&#38656;&#30340;&#22823;&#23567;&#65292;&#20197;&#23398;&#20064;&#22270;&#12290;&#19968;&#26041;&#38754;, &#25105;&#20204;&#34920;&#26126;&#33267;&#23569; s&#229;orest
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08562v1 Announce Type: cross  Abstract: Markov networks are probabilistic graphical models that employ undirected graphs to depict conditional independence relationships among variables. Our focus lies in constraint-based structure learning, which entails learning the undirected graph from data through the execution of conditional independence tests. We establish theoretical limits concerning two critical aspects of constraint-based learning of Markov networks: the number of tests and the sizes of the conditioning sets. These bounds uncover an exciting interplay between the structural properties of the graph and the amount of tests required to learn a Markov network. The starting point of our work is that the graph parameter maximum pairwise connectivity, $\kappa$, that is, the maximum number of vertex-disjoint paths connecting a pair of vertices in the graph, is responsible for the sizes of independence tests required to learn the graph. On one hand, we show that at least o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FedDM&#65292;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#37030;&#30693;&#35782;&#22270;&#20013;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.08554</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#32852;&#37030;&#30693;&#35782;&#22270;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Knowledge Graph Unlearning via Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08554
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FedDM&#65292;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#37030;&#30693;&#35782;&#22270;&#20013;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08554v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#20419;&#36827;&#27169;&#22411;&#20849;&#20139;&#21644;&#21327;&#20316;&#65292;&#21516;&#26102;&#32500;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#25512;&#21160;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#12290;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#23884;&#20837;&#34920;&#31034;&#36890;&#36807;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#26144;&#23556;&#21040;&#21521;&#37327;&#31354;&#38388;&#65292;&#20026;&#30693;&#35782;&#25512;&#29702;&#21644;&#24212;&#29992;&#25552;&#20379;&#22522;&#30784;&#12290;&#32852;&#37030;&#30693;&#35782;&#22270;&#23884;&#20837;&#33021;&#22815;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#23458;&#25143;&#31471;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#20445;&#25252;&#31561;&#38656;&#27714;&#20197;&#21450;&#38656;&#35201;&#36866;&#24212;&#21160;&#24577;&#25968;&#25454;&#21464;&#21270;&#65292;&#26426;&#22120;&#21435;&#23398;&#20064;&#65288;MU&#65289;&#30340;&#30740;&#31350;&#24471;&#20197;&#23637;&#24320;&#12290;&#28982;&#32780;&#65292;&#22312;&#24536;&#35760;&#29305;&#23450;&#36951;&#24536;&#25968;&#25454;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;KG&#23884;&#20837;&#27169;&#22411;&#30340;&#24615;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FedDM&#65292;&#19968;&#20010;&#38024;&#23545;&#32852;&#37030;&#30693;&#35782;&#22270;&#20013;&#26426;&#22120;&#21435;&#23398;&#20064;&#32780;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#29983;&#25104;&#24102;&#26377;&#22122;&#22768;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08554v1 Announce Type: cross  Abstract: Federated learning (FL) promotes the development and application of artificial intelligence technologies by enabling model sharing and collaboration while safeguarding data privacy. Knowledge graph (KG) embedding representation provides a foundation for knowledge reasoning and applications by mapping entities and relations into vector space. Federated KG embedding enables the utilization of knowledge from diverse client sources while safeguarding the privacy of local data. However, due to demands such as privacy protection and the need to adapt to dynamic data changes, investigations into machine unlearning (MU) have been sparked. However, it is challenging to maintain the performance of KG embedding models while forgetting the influence of specific forgotten data on the model. In this paper, we propose FedDM, a novel framework tailored for machine unlearning in federated knowledge graphs. Leveraging diffusion models, we generate noisy
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#32447;&#20048;&#35266;&#29275;&#39039;&#27969;&#24418;&#65288;OONM&#65289;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#22522;&#20110;&#20989;&#25968;&#24207;&#21015;&#30340;&#31532;&#19968;&#21644;&#31532;&#20108;&#38454;&#20449;&#24687;&#39044;&#27979;&#30340;&#22312;&#32447;&#25511;&#21046;&#22120;&#65292;&#29992;&#20110;&#22312;&#32447;LQG&#32447;&#24615;&#32422;&#26463;&#25919;&#31574;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.08553</link><description>&lt;p&gt;
&#22312;&#32447;LQG&#32447;&#24615;&#32422;&#26463;&#25919;&#31574;&#20248;&#21270;&#30340;&#36951;&#25022;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Regret Analysis of Policy Optimization over Submanifolds for Linearly Constrained Online LQG
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08553
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#32447;&#20048;&#35266;&#29275;&#39039;&#27969;&#24418;&#65288;OONM&#65289;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#22522;&#20110;&#20989;&#25968;&#24207;&#21015;&#30340;&#31532;&#19968;&#21644;&#31532;&#20108;&#38454;&#20449;&#24687;&#39044;&#27979;&#30340;&#22312;&#32447;&#25511;&#21046;&#22120;&#65292;&#29992;&#20110;&#22312;&#32447;LQG&#32447;&#24615;&#32422;&#26463;&#25919;&#31574;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20248;&#21270;&#21644;&#25511;&#21046;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#30740;&#31350;&#22312;&#32447;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#24037;&#20855;&#65292;&#20854;&#20013;&#25104;&#26412;&#30697;&#38453;&#38543;&#26102;&#38388;&#21464;&#21270;&#23545;&#25239;&#24615;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20316;&#21697;&#30340;&#25511;&#21046;&#22120;&#21442;&#25968;&#21270;&#21487;&#33021;&#19981;&#28385;&#36275;&#23454;&#38469;&#26465;&#20214;&#65292;&#22914;&#30001;&#20110;&#29289;&#29702;&#36830;&#25509;&#32780;&#23548;&#33268;&#30340;&#31232;&#30095;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#32447;&#24615;&#20108;&#27425;&#39640;&#26031;&#38382;&#39064;&#65292;&#20854;&#20013;&#23545;&#25511;&#21046;&#22120;&#26045;&#21152;&#20102;&#32473;&#23450;&#30340;&#32447;&#24615;&#32422;&#26463;&#12290;&#21463;[1]&#26368;&#36817;&#25552;&#20986;&#30340;&#20851;&#20110;&#32447;&#24615;&#32422;&#26463;&#30340;&#32447;&#19979;LQR&#25919;&#31574;&#20248;&#21270;&#30340;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#20108;&#38454;&#26041;&#27861;&#65292;&#37197;&#22791;&#20102;&#19968;&#31181;&#22312;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#33258;&#28982;&#20135;&#29983;&#30340;&#40654;&#26364;&#24230;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#32447;&#20048;&#35266;&#29275;&#39039;&#27969;&#24418;&#65288;OONM&#65289;&#65292;&#25552;&#20379;&#22522;&#20110;&#20989;&#25968;&#24207;&#21015;&#30340;&#31532;&#19968;&#21644;&#31532;&#20108;&#38454;&#20449;&#24687;&#39044;&#27979;&#30340;&#22312;&#32447;&#25511;&#21046;&#22120;&#12290;&#20026;&#20102;&#37327;&#21270;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#36951;&#25022;&#30340;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08553v1 Announce Type: cross  Abstract: Recent advancement in online optimization and control has provided novel tools to study online linear quadratic regulator (LQR) problems, where cost matrices are varying adversarially over time. However, the controller parameterization of existing works may not satisfy practical conditions like sparsity due to physical connections. In this work, we study online linear quadratic Gaussian problems with a given linear constraint imposed on the controller. Inspired by the recent work of [1] which proposed, for a linearly constrained policy optimization of an offline LQR, a second order method equipped with a Riemannian metric that emerges naturally in the context of optimal control problems, we propose online optimistic Newton on manifold (OONM) which provides an online controller based on the prediction on the first and second order information of the function sequence. To quantify the proposed algorithm, we leverage the notion of regret 
&lt;/p&gt;</description></item><item><title>CINA&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#38544;&#24335;&#31070;&#32463;&#22270;&#35889;&#65288;CINA&#65289;&#65292;&#21487;&#20197;&#29983;&#25104;&#32974;&#20799;&#22823;&#33041;&#30340;&#26102;&#31354;&#22270;&#35889;&#65292;&#32780;&#26080;&#38656;&#20223;&#23556;&#25110;&#38750;&#21018;&#24615;&#37197;&#20934;&#65292;&#35757;&#32451;&#21518;&#21487;&#26500;&#24314;&#24544;&#23454;&#30340;&#32452;&#32455;&#27010;&#29575;&#22270;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#31070;&#32463;&#22411;&#21644;&#30149;&#29702;&#24615;&#22823;&#33041;&#12290;</title><link>https://arxiv.org/abs/2403.08550</link><description>&lt;p&gt;
CINA&#65306;&#32974;&#20799;&#33041;&#30340;&#26102;&#31354;&#34920;&#31034;&#30340;&#26465;&#20214;&#38544;&#24335;&#31070;&#32463;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
CINA: Conditional Implicit Neural Atlas for Spatio-Temporal Representation of Fetal Brains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08550
&lt;/p&gt;
&lt;p&gt;
CINA&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#38544;&#24335;&#31070;&#32463;&#22270;&#35889;&#65288;CINA&#65289;&#65292;&#21487;&#20197;&#29983;&#25104;&#32974;&#20799;&#22823;&#33041;&#30340;&#26102;&#31354;&#22270;&#35889;&#65292;&#32780;&#26080;&#38656;&#20223;&#23556;&#25110;&#38750;&#21018;&#24615;&#37197;&#20934;&#65292;&#35757;&#32451;&#21518;&#21487;&#26500;&#24314;&#24544;&#23454;&#30340;&#32452;&#32455;&#27010;&#29575;&#22270;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#31070;&#32463;&#22411;&#21644;&#30149;&#29702;&#24615;&#22823;&#33041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#31070;&#32463;&#22411;&#21644;&#30149;&#29702;&#24615;&#32974;&#20799;&#22823;&#33041;&#30340;&#30913;&#20849;&#25391;&#22270;&#20687;&#65288;MRI&#65289;&#29983;&#25104;&#26102;&#31354;&#22270;&#35889;&#30340;&#26465;&#20214;&#38544;&#24335;&#31070;&#32463;&#22270;&#35889;&#65288;CINA&#65289;&#65292;&#23436;&#20840;&#29420;&#31435;&#20110;&#20223;&#23556;&#25110;&#38750;&#21018;&#24615;&#37197;&#20934;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;CINA&#23398;&#20064;&#20102;&#32974;&#20799;&#22823;&#33041;&#30340;&#19968;&#33324;&#34920;&#31034;&#65292;&#24182;&#23558;&#29305;&#23450;&#20110;&#20027;&#20307;&#30340;&#20449;&#24687;&#32534;&#30721;&#21040;&#28508;&#22312;&#20195;&#30721;&#20013;&#12290;&#35757;&#32451;&#21518;&#65292;CINA&#21487;&#20197;&#20026;&#20219;&#20309;&#23381;&#40836;&#65288;GA&#65289;&#21644;&#35757;&#32451;&#39046;&#22495;&#20869;&#28085;&#30422;&#30340;&#35299;&#21078;&#21464;&#21270;&#26500;&#24314;&#24544;&#23454;&#30340;&#32974;&#20799;&#22823;&#33041;&#22270;&#35889;&#30340;&#32452;&#32455;&#27010;&#29575;&#22270;&#12290;&#22240;&#27492;&#65292;CINA&#33021;&#22815;&#34920;&#31034;&#31070;&#32463;&#22411;&#21644;&#30149;&#29702;&#24615;&#22823;&#33041;&#12290;&#27492;&#22806;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;CINA&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#27979;&#35797;&#26102;&#20248;&#21270;&#28508;&#22312;&#20195;&#30721;&#26469;&#36866;&#24212;&#26410;&#30693;&#20027;&#20307;&#30340;&#22823;&#33041;MRI&#12290;&#28982;&#21518;&#65292;CINA&#21487;&#20197;&#29983;&#25104;&#38024;&#23545;&#29305;&#23450;&#20027;&#20307;&#30340;&#27010;&#29575;&#32452;&#32455;&#22270;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;t&#30340;&#24635;&#20849;198&#20363;&#27491;&#24120;&#21644;&#24322;&#24120;&#32974;&#20799;&#22823;&#33041;&#30340;T2&#21152;&#26435;MRI&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08550v1 Announce Type: new  Abstract: We introduce a conditional implicit neural atlas (CINA) for spatio-temporal atlas generation from Magnetic Resonance Images (MRI) of the neurotypical and pathological fetal brain, that is fully independent of affine or non-rigid registration. During training, CINA learns a general representation of the fetal brain and encodes subject specific information into latent code. After training, CINA can construct a faithful atlas with tissue probability maps of the fetal brain for any gestational age (GA) and anatomical variation covered within the training domain. Thus, CINA is competent to represent both, neurotypical and pathological brains. Furthermore, a trained CINA model can be fit to brain MRI of unseen subjects via test-time optimization of the latent code. CINA can then produce probabilistic tissue maps tailored to a particular subject. We evaluate our method on a total of 198 T2 weighted MRI of normal and abnormal fetal brains from t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#32553;&#25918;&#30740;&#31350;&#20013;&#36807;&#24230;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#35780;&#20272;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.08540</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#19982;&#36807;&#24230;&#35757;&#32451;&#20197;&#21450;&#19979;&#28216;&#20219;&#21153;&#21487;&#38752;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Language models scale reliably with over-training and on downstream tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#32553;&#25918;&#30740;&#31350;&#20013;&#36807;&#24230;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#35780;&#20272;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32553;&#25918;&#35268;&#24459;&#23545;&#20110;&#24320;&#21457;&#35821;&#35328;&#27169;&#22411;&#26159;&#26377;&#29992;&#30340;&#25351;&#23548;&#65292;&#20294;&#24403;&#21069;&#30340;&#32553;&#25918;&#30740;&#31350;&#19982;&#35821;&#35328;&#27169;&#22411;&#26368;&#32456;&#35757;&#32451;&#21644;&#35780;&#20272;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#24046;&#36317;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36807;&#24230;&#35757;&#32451;&#21644;&#22522;&#20110;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#36827;&#34892;&#27604;&#36739;&#26041;&#38754;&#30340;&#36825;&#20004;&#20010;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08540v1 Announce Type: new  Abstract: Scaling laws are useful guides for developing language models, but there are still gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute-optimal training regime (i.e., "Chinchilla optimal" regime); however, in practice, models are often over-trained to reduce inference costs. Moreover, scaling laws mostly predict loss on next-token prediction, but ultimately models are compared based on downstream task performance. In this paper, we address both shortcomings. To do so, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions. First, we investigate scaling in the over-trained regime. We fit scaling laws that extrapolate in both the number of model parameters and the ratio of training tokens to parameters. This enables us to predict the validation loss of a 1.4B para
&lt;/p&gt;</description></item><item><title>HOLMES&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#26631;&#31614;&#20998;&#35299;&#20026;&#19968;&#32452;&#30456;&#20851;&#27010;&#24565;&#24182;&#25552;&#20379;&#37096;&#20214;&#32423;&#35299;&#37322;&#65292;&#26469;&#24110;&#21161;&#29702;&#35299;&#21644;&#35299;&#37322;&#21367;&#31215;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.08536</link><description>&lt;p&gt;
HOLMES: &#22522;&#20110;HOLonym-MEronym&#30340;&#35821;&#20041;&#26816;&#26597;&#25216;&#26415;&#29992;&#20110;&#21367;&#31215;&#22270;&#20687;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
HOLMES: HOLonym-MEronym based Semantic inspection for Convolutional Image Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08536
&lt;/p&gt;
&lt;p&gt;
HOLMES&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#26631;&#31614;&#20998;&#35299;&#20026;&#19968;&#32452;&#30456;&#20851;&#27010;&#24565;&#24182;&#25552;&#20379;&#37096;&#20214;&#32423;&#35299;&#37322;&#65292;&#26469;&#24110;&#21161;&#29702;&#35299;&#21644;&#35299;&#37322;&#21367;&#31215;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#22914;&#20170;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#39318;&#36873;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#33258;&#21160;&#21270;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#33719;&#24471;&#30340;&#30693;&#35782;&#23436;&#20840;&#26159;&#20122;&#31526;&#21495;&#30340;&#65292;&#22240;&#27492;&#38590;&#20197;&#29702;&#35299;&#21644;&#35299;&#37322;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HOLMES (HOLonym-MEronym based Semantic inspection)&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#23558;&#26631;&#31614;&#20998;&#35299;&#20026;&#19968;&#32452;&#30456;&#20851;&#27010;&#24565;&#65292;&#24182;&#20026;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#25552;&#20379;&#37096;&#20214;&#32423;&#35299;&#37322;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;HOLMES&#21033;&#29992;&#26412;&#20307;&#35770;&#12289;&#32593;&#32476;&#25235;&#21462;&#21644;&#36801;&#31227;&#23398;&#20064;&#26469;&#33258;&#21160;&#26500;&#24314;&#32473;&#23450;holonym (&#31867;&#21035;)&#30340;meronym (&#37096;&#20214;)&#26816;&#27979;&#22120;&#12290;&#28982;&#21518;&#65292;&#23427;&#22312;meronym&#32423;&#21035;&#29983;&#25104;&#28909;&#22270;&#65292;&#26368;&#21518;&#36890;&#36807;&#20351;&#29992;&#36974;&#25377;&#22270;&#20687;&#23545;CNN&#30340;holonym&#36827;&#34892;&#25506;&#27979;&#65292;&#31361;&#20986;&#27599;&#20010;&#37096;&#20214;&#23545;&#20998;&#31867;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#26174;&#33879;&#24615;&#26041;&#27861;&#30456;&#27604;&#65292;HOLMES
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08536v1 Announce Type: cross  Abstract: Convolutional Neural Networks (CNNs) are nowadays the model of choice in Computer Vision, thanks to their ability to automatize the feature extraction process in visual tasks. However, the knowledge acquired during training is fully subsymbolic, and hence difficult to understand and explain to end users. In this paper, we propose a new technique called HOLMES (HOLonym-MEronym based Semantic inspection) that decomposes a label into a set of related concepts, and provides component-level explanations for an image classification model. Specifically, HOLMES leverages ontologies, web scraping and transfer learning to automatically construct meronym (parts)-based detectors for a given holonym (class). Then, it produces heatmaps at the meronym level and finally, by probing the holonym CNN with occluded images, it highlights the importance of each part on the classification output. Compared to state-of-the-art saliency methods, HOLMES takes a 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#21464;&#28857;&#26816;&#27979;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#38899;&#39057;&#24405;&#21046;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#27169;&#22411;&#21644;&#21464;&#28857;&#26816;&#27979;&#36880;&#27493;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24378;&#26631;&#31614;&#12290;</title><link>https://arxiv.org/abs/2403.08525</link><description>&lt;p&gt;
&#20174;&#24369;&#21040;&#24378;&#65306;&#20351;&#29992;&#33258;&#36866;&#24212;&#21464;&#28857;&#26816;&#27979;&#21644;&#20027;&#21160;&#23398;&#20064;&#36827;&#34892;&#22768;&#38899;&#20107;&#20214;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
From Weak to Strong Sound Event Labels using Adaptive Change-Point Detection and Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08525
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#21464;&#28857;&#26816;&#27979;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#38899;&#39057;&#24405;&#21046;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#27169;&#22411;&#21644;&#21464;&#28857;&#26816;&#27979;&#36880;&#27493;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24378;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#21464;&#28857;&#26816;&#27979;&#65288;A-CPD&#65289;&#30340;&#38899;&#39057;&#24405;&#21046;&#20998;&#21106;&#26041;&#27861;&#65292;&#29992;&#20110;&#26426;&#22120;&#24341;&#23548;&#30340;&#38899;&#39057;&#24405;&#21046;&#27573;&#30340;&#24369;&#26631;&#31614;&#27880;&#37322;&#12290;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#20851;&#20110;&#30446;&#26631;&#22768;&#38899;&#26102;&#38388;&#28608;&#27963;&#30340;&#20449;&#24687;&#33719;&#21462;&#37327;&#12290;&#23545;&#20110;&#27599;&#20010;&#26410;&#26631;&#35760;&#30340;&#38899;&#39057;&#24405;&#21046;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#27979;&#27169;&#22411;&#26469;&#25512;&#23548;&#27010;&#29575;&#26354;&#32447;&#65292;&#29992;&#20110;&#25351;&#23548;&#27880;&#37322;&#12290;&#39044;&#27979;&#27169;&#22411;&#26368;&#21021;&#22312;&#21487;&#29992;&#30340;&#24102;&#26631;&#27880;&#22768;&#38899;&#20107;&#20214;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#36825;&#20123;&#25968;&#25454;&#30340;&#31867;&#19982;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#19981;&#30456;&#20132;&#12290;&#28982;&#21518;&#65292;&#39044;&#27979;&#27169;&#22411;&#36880;&#28176;&#36866;&#24212;&#27880;&#37322;&#32773;&#22312;&#20027;&#21160;&#23398;&#20064;&#24490;&#29615;&#20013;&#25552;&#20379;&#30340;&#27880;&#37322;&#12290;&#29992;&#20110;&#24341;&#23548;&#24369;&#26631;&#31614;&#27880;&#37322;&#32773;&#36208;&#21521;&#24378;&#26631;&#31614;&#30340;&#26597;&#35810;&#26159;&#20351;&#29992;&#36825;&#20123;&#27010;&#29575;&#19978;&#30340;&#21464;&#28857;&#26816;&#27979;&#23548;&#20986;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#27880;&#37322;&#39044;&#31639;&#19979;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#24378;&#26631;&#31614;&#65292;&#24182;&#23637;&#31034;&#20102;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08525v1 Announce Type: cross  Abstract: In this work we propose an audio recording segmentation method based on an adaptive change point detection (A-CPD) for machine guided weak label annotation of audio recording segments. The goal is to maximize the amount of information gained about the temporal activation's of the target sounds. For each unlabeled audio recording, we use a prediction model to derive a probability curve used to guide annotation. The prediction model is initially pre-trained on available annotated sound event data with classes that are disjoint from the classes in the unlabeled dataset. The prediction model then gradually adapts to the annotations provided by the annotator in an active learning loop. The queries used to guide the weak label annotator towards strong labels are derived using change point detection on these probabilities. We show that it is possible to derive strong labels of high quality even with a limited annotation budget, and show favor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiPrompT&#30340;&#35299;&#32806;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36866;&#24212;&#25552;&#31034;&#26469;&#35299;&#20915;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#39046;&#22495;&#27867;&#21270;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.08506</link><description>&lt;p&gt;
DiPrompT: &#22810;&#28508;&#22312;&#39046;&#22495;&#27867;&#21270;&#30340;&#35299;&#32806;&#25552;&#31034;&#35843;&#25972;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;
&lt;/p&gt;
&lt;p&gt;
DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08506
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiPrompT&#30340;&#35299;&#32806;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36866;&#24212;&#25552;&#31034;&#26469;&#35299;&#20915;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#39046;&#22495;&#27867;&#21270;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08506v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495;  &#25688;&#35201;: &#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#20174;&#20998;&#25955;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#32780;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#36827;&#19968;&#27493;&#32771;&#34385;&#27979;&#35797;&#25968;&#25454;&#38598;&#65288;&#30446;&#26631;&#39046;&#22495;&#65289;&#19981;&#23384;&#22312;&#20110;&#20998;&#25955;&#30340;&#35757;&#32451;&#25968;&#25454;&#65288;&#28304;&#39046;&#22495;&#65289;&#20013;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;FL&#26041;&#27861;&#20551;&#35774;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#20102;&#39046;&#22495;&#26631;&#31614;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#35780;&#20272;&#23545;&#39046;&#22495;&#25968;&#37327;&#26045;&#21152;&#26126;&#30830;&#30340;&#32422;&#26463;&#65292;&#36825;&#20123;&#32422;&#26463;&#24517;&#39035;&#20005;&#26684;&#21305;&#37197;&#23458;&#25143;&#31471;&#30340;&#25968;&#37327;&#12290;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#20247;&#22810;&#36793;&#32536;&#35774;&#22791;&#30340;&#34987;&#20302;&#25928;&#21033;&#29992;&#20197;&#21450;&#39069;&#22806;&#30340;&#36328;&#23458;&#25143;&#31471;&#39046;&#22495;&#27880;&#37322;&#65292;&#36825;&#20123;&#38480;&#21046;&#21487;&#33021;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#24182;&#28041;&#21450;&#28508;&#22312;&#30340;&#38544;&#31169;&#27844;&#28431;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#32780;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#35299;&#32806;&#25552;&#31034;&#35843;&#25972;&#65288;DiPrompT&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#24067;&#24335;&#23398;&#20064;&#36866;&#24212;&#25552;&#31034;&#26469;&#22788;&#29702;&#19978;&#36848;&#38480;&#21046;&#65292;&#26469;&#23454;&#29616;&#39046;&#22495;&#27867;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#20004;&#31181;&#25552;&#31034;&#31867;&#22411;&#65292;&#21363;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08506v1 Announce Type: cross  Abstract: Federated learning (FL) has emerged as a powerful paradigm for learning from decentralized data, and federated domain generalization further considers the test dataset (target domain) is absent from the decentralized training data (source domains). However, most existing FL methods assume that domain labels are provided during training, and their evaluation imposes explicit constraints on the number of domains, which must strictly match the number of clients. Because of the underutilization of numerous edge devices and additional cross-client domain annotations in the real world, such restrictions may be impractical and involve potential privacy leaks. In this paper, we propose an efficient and novel approach, called Disentangled Prompt Tuning (DiPrompT), a method that tackles the above restrictions by learning adaptive prompts for domain generalization in a distributed manner. Specifically, we first design two types of prompts, i.e., 
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#31995;&#32479;&#35780;&#20272;&#20102;&#22823;&#22411;&#32463;&#36807;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#20197;&#21450;&#30456;&#20851;&#22240;&#32032;&#21644;&#19981;&#21516;&#38450;&#24481;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08481</link><description>&lt;p&gt;
SoK&#65306;&#20943;&#23569;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08481
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#31995;&#32479;&#35780;&#20272;&#20102;&#22823;&#22411;&#32463;&#36807;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#20197;&#21450;&#30456;&#20851;&#22240;&#32032;&#21644;&#19981;&#21516;&#38450;&#24481;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#36817;&#24180;&#26469;&#32463;&#21382;&#20102;&#26174;&#33879;&#22686;&#38271;&#65292;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#37117;&#26159;&#22522;&#20110;&#23427;&#20204;&#26500;&#24314;&#30340;&#12290;&#35768;&#22810;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#38656;&#35201;&#22312;&#23450;&#21046;&#30340;&#19987;&#26377;&#25968;&#25454;&#38598;&#19978;&#23545;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#31181;&#24494;&#35843;&#25968;&#25454;&#29305;&#21035;&#23481;&#26131;&#21253;&#21547;&#20010;&#20154;&#25110;&#25935;&#24863;&#20449;&#24687;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#38544;&#31169;&#39118;&#38505;&#12290;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26159;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38544;&#31169;&#27844;&#28431;&#30340;&#24120;&#29992;&#25915;&#20987;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#35821;&#35328;&#27169;&#22411;&#23545;&#36825;&#31181;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#24433;&#21709;&#22240;&#32032;&#21450;&#22312;&#35821;&#35328;&#39046;&#22495;&#20013;&#19981;&#21516;&#38450;&#24481;&#31574;&#30053;&#30340;&#36866;&#29992;&#24615;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#22823;&#22411;&#32463;&#36807;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#12289;&#28041;&#21450;&#30340;&#21508;&#31181;&#22240;&#32032;&#20197;&#21450;&#19981;&#21516;&#38450;&#24481;&#31574;&#30053;&#26377;&#25928;&#24615;&#30340;&#31532;&#19968;&#27425;&#31995;&#32479;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08481v1 Announce Type: new  Abstract: Natural language processing models have experienced a significant upsurge in recent years, with numerous applications being built upon them. Many of these applications require fine-tuning generic base models on customized, proprietary datasets. This fine-tuning data is especially likely to contain personal or sensitive information about individuals, resulting in increased privacy risk. Membership inference attacks are the most commonly employed attack to assess the privacy leakage of a machine learning model. However, limited research is available on the factors that affect the vulnerability of language models to this kind of attack, or on the applicability of different defense strategies in the language domain. We provide the first systematic review of the vulnerability of fine-tuned large language models to membership inference attacks, the various factors that come into play, and the effectiveness of different defense strategies. We f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sparse MetA-Tuning&#65288;SMAT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28789;&#24863;&#26469;&#33258;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#26041;&#27861;&#65292;&#25104;&#21151;&#20811;&#26381;&#20102;&#22495;&#22806;&#20219;&#21153;&#25935;&#24863;&#24615;&#65292;&#23454;&#29616;&#20102;&#22686;&#24378;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#36716;&#31227;&#33021;&#21147;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.08477</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#25554;&#20540;&#19987;&#23478;&#37322;&#25918;&#20803;&#35843;&#25972;&#30340;&#21147;&#37327;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sparse MetA-Tuning&#65288;SMAT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28789;&#24863;&#26469;&#33258;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#26041;&#27861;&#65292;&#25104;&#21151;&#20811;&#26381;&#20102;&#22495;&#22806;&#20219;&#21153;&#25935;&#24863;&#24615;&#65292;&#23454;&#29616;&#20102;&#22686;&#24378;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#36716;&#31227;&#33021;&#21147;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#26234;&#24935;&#24314;&#35758;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#65292;&#26159;&#35270;&#35273;&#36801;&#31227;&#23398;&#20064;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#21462;&#20195;&#20102;&#35832;&#22914;&#20803;&#23398;&#20064;&#20043;&#31867;&#30340;&#20016;&#23500;&#25991;&#29486;&#12290;&#20026;&#20102;&#20860;&#39038;&#20004;&#32773;&#30340;&#21033;&#30410;&#65292;&#20803;&#35843;&#25972;&#24341;&#20837;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#38543;&#21518;&#20248;&#21270;&#38454;&#27573;&#65292;&#20294;&#36804;&#20170;&#21482;&#23637;&#29616;&#20102;&#26377;&#38480;&#30340;&#25104;&#21151;&#65292;&#20851;&#38190;&#22320;&#22312;&#22495;&#22806;&#65288;OOD&#65289;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28789;&#24863;&#26469;&#33258;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#26041;&#27861;&#30340; Sparse MetA-Tuning&#65288;SMAT&#65289;&#26041;&#27861;&#65292;&#23427;&#32463;&#36807;&#35757;&#32451;&#20197;&#33258;&#21160;&#22320;&#20026;&#27599;&#20010;&#20219;&#21153;&#38548;&#31163;&#39044;&#35757;&#32451;&#21442;&#25968;&#23376;&#38598;&#20197;&#36827;&#34892;&#20803;&#35843;&#25972;&#12290;SMAT&#25104;&#21151;&#20811;&#26381;&#20102;OOD&#25935;&#24863;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#22686;&#24378;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#36716;&#31227;&#33021;&#21147;&#30340;&#25215;&#35834;&#12290;&#25105;&#20204;&#22312;Meta-Dataset&#19982;&#39069;&#22806;&#30340;OO&#25361;&#25112;&#32452;&#21512;&#19978;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08477v1 Announce Type: cross  Abstract: Conventional wisdom suggests parameter-efficient fine-tuning of foundation models as the state-of-the-art method for transfer learning in vision, replacing the rich literature of alternatives such as meta-learning. In trying to harness the best of both worlds, meta-tuning introduces a subsequent optimization stage of foundation models but has so far only shown limited success and crucially tends to underperform on out-of-domain (OOD) tasks. In this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse mixture-of-experts approaches and trained to isolate subsets of pre-trained parameters automatically for meta-tuning on each task. SMAT successfully overcomes OOD sensitivity and delivers on the promise of enhancing the transfer abilities of vision foundation models beyond parameter-efficient finetuning. We establish new state-of-the-art results on a challenging combination of Meta-Dataset augmented with additional OO
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25490;&#21015;&#23545;&#20154;&#31867;&#21709;&#24212;&#30340;&#23545;&#40784;&#24773;&#20917;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#30340;&#23545;&#40784;&#24615;&#19982;ImageNet-1k&#30456;&#24403;&#65292;&#21435;&#22122;U-Net&#30340;&#26368;&#23545;&#40784;&#23618;&#26159;&#20013;&#38388;&#23618;&#32780;&#19981;&#26159;&#29942;&#39048;&#65292;&#24182;&#19988;&#25991;&#26412;&#35843;&#33410;&#22312;&#39640;&#22122;&#22768;&#27700;&#24179;&#19979;&#25552;&#39640;&#20102;&#23545;&#40784;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08469</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25490;&#21015;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Human Alignment of Latent Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08469
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25490;&#21015;&#23545;&#20154;&#31867;&#21709;&#24212;&#30340;&#23545;&#40784;&#24773;&#20917;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#30340;&#23545;&#40784;&#24615;&#19982;ImageNet-1k&#30456;&#24403;&#65292;&#21435;&#22122;U-Net&#30340;&#26368;&#23545;&#40784;&#23618;&#26159;&#20013;&#38388;&#23618;&#32780;&#19981;&#26159;&#29942;&#39048;&#65292;&#24182;&#19988;&#25991;&#26412;&#35843;&#33410;&#22312;&#39640;&#22122;&#22768;&#27700;&#24179;&#19979;&#25552;&#39640;&#20102;&#23545;&#40784;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#22312;&#22823;&#37327;&#25968;&#25454;&#35757;&#32451;&#19979;&#65292;&#23637;&#29616;&#20986;&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#24403;&#29992;&#20110;&#20998;&#31867;&#26102;&#65292;&#23427;&#20204;&#19982;&#20154;&#31867;&#26377;&#39640;&#35823;&#24046;&#19968;&#33268;&#24615;&#21644;&#20302;&#32441;&#29702;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#29942;&#39048;&#23618;&#34920;&#31034;&#21487;&#20197;&#20998;&#35299;&#20026;&#35821;&#20041;&#26041;&#21521;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#34920;&#31034;&#19982;&#20154;&#31867;&#22312;&#19977;&#20803;&#22855;&#19968;&#22855;&#20219;&#21153;&#19978;&#23545;&#40784;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#21069;&#36848;&#35266;&#23519;&#21040;&#65306;I&#65289;&#19982;&#20154;&#31867;&#30340;&#34920;&#31034;&#23545;&#40784;&#24615;&#19982;&#20165;&#22312;ImageNet-1k&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;II&#65289;&#21435;&#22122;U-Net&#30340;&#26368;&#23545;&#40784;&#23618;&#26159;&#20013;&#38388;&#23618;&#32780;&#19981;&#26159;&#29942;&#39048;&#12290;III&#65289;&#25991;&#26412;&#35843;&#33410;&#20026;&#39640;&#22122;&#22768;&#27700;&#24179;&#25552;&#39640;&#20102;&#23545;&#40784;&#24615;&#65292;&#26263;&#31034;&#30528;&#25277;&#35937;&#25991;&#26412;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#29983;&#25104;&#30340;&#26089;&#26399;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08469v1 Announce Type: new  Abstract: Diffusion models, trained on large amounts of data, showed remarkable performance for image synthesis. They have high error consistency with humans and low texture bias when used for classification. Furthermore, prior work demonstrated the decomposability of their bottleneck layer representations into semantic directions. In this work, we analyze how well such representations are aligned to human responses on a triplet odd-one-out task. We find that despite the aforementioned observations: I) The representational alignment with humans is comparable to that of models trained only on ImageNet-1k. II) The most aligned layers of the denoiser U-Net are intermediate layers and not the bottleneck. III) Text conditioning greatly improves alignment at high noise levels, hinting at the importance of abstract textual information, especially in the early stage of generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;THOR&#65288;Temporal Harmonization for Optimal Restoration&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26102;&#38388;&#24322;&#24120;&#22270;&#20013;&#25972;&#21512;&#38544;&#24335;&#24341;&#23548;&#26469;&#25913;&#36827;&#21435;&#22122;&#36807;&#31243;&#65292;&#26088;&#22312;&#20445;&#25345;&#26410;&#21463;&#30149;&#21464;&#24433;&#21709;&#21306;&#22495;&#20581;&#24247;&#32452;&#32455;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08464</link><description>&lt;p&gt;
&#20855;&#26377;&#38544;&#24335;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#21307;&#23398;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models with Implicit Guidance for Medical Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;THOR&#65288;Temporal Harmonization for Optimal Restoration&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26102;&#38388;&#24322;&#24120;&#22270;&#20013;&#25972;&#21512;&#38544;&#24335;&#24341;&#23548;&#26469;&#25913;&#36827;&#21435;&#22122;&#36807;&#31243;&#65292;&#26088;&#22312;&#20445;&#25345;&#26410;&#21463;&#30149;&#21464;&#24433;&#21709;&#21306;&#22495;&#20581;&#24247;&#32452;&#32455;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#25913;&#21892;&#30149;&#29702;&#22270;&#20687;&#21521;&#20266;&#20581;&#24247;&#31561;&#20215;&#29289;&#30340;&#36716;&#25442;&#65292;&#25512;&#21160;&#20102;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;THOR&#65288;Temporal Harmonization for Optimal Restoration&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26102;&#38388;&#24322;&#24120;&#22270;&#20013;&#25972;&#21512;&#38544;&#24335;&#24341;&#23548;&#26469;&#25913;&#36827;&#21435;&#22122;&#36807;&#31243;&#65292;&#26088;&#22312;&#20445;&#25345;&#26410;&#21463;&#30149;&#21464;&#24433;&#21709;&#21306;&#22495;&#20581;&#24247;&#32452;&#32455;&#30340;&#23436;&#25972;&#24615;&#12290;&#27604;&#36739;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;THOR&#22312;&#26816;&#27979;&#21644;&#20998;&#21106;&#33041;&#37096;MRI&#21644;&#25163;&#33109;X&#20809;&#20013;&#30340;&#24322;&#24120;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08464v1 Announce Type: cross  Abstract: Diffusion models have advanced unsupervised anomaly detection by improving the transformation of pathological images into pseudo-healthy equivalents. Nonetheless, standard approaches may compromise critical information during pathology removal, leading to restorations that do not align with unaffected regions in the original scans. Such discrepancies can inadvertently increase false positive rates and reduce specificity, complicating radiological evaluations. This paper introduces Temporal Harmonization for Optimal Restoration (THOR), which refines the de-noising process by integrating implicit guidance through temporal anomaly maps. THOR aims to preserve the integrity of healthy tissue in areas unaffected by pathology. Comparative evaluations show that THOR surpasses existing diffusion-based methods in detecting and segmenting anomalies in brain MRIs and wrist X-rays. Code: https://github.com/ci-ber/THOR_DDPM.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35745;&#31639;&#20316;&#32773;&#25991;&#20214;&#22312;&#20505;&#36873;&#20316;&#32773;&#35821;&#27861;&#27169;&#22411;&#19982;&#21442;&#32771;&#32676;&#20307;&#35821;&#27861;&#27169;&#22411;&#19979;&#30340;&#21487;&#33021;&#24615;&#27604;&#29575;&#30340;&#26041;&#27861;&#65292;&#29992;&#20197;&#35299;&#20915;&#20316;&#32773;&#36523;&#20221;&#39564;&#35777;&#20013;&#23384;&#22312;&#30340;&#31185;&#23398;&#35299;&#37322;&#19981;&#36275;&#21644;&#38590;&#20197;&#35299;&#37322;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.08462</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#27169;&#22411;&#20284;&#28982;&#27604;&#30340;&#20316;&#32773;&#36523;&#20221;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Authorship Verification based on the Likelihood Ratio of Grammar Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08462
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35745;&#31639;&#20316;&#32773;&#25991;&#20214;&#22312;&#20505;&#36873;&#20316;&#32773;&#35821;&#27861;&#27169;&#22411;&#19982;&#21442;&#32771;&#32676;&#20307;&#35821;&#27861;&#27169;&#22411;&#19979;&#30340;&#21487;&#33021;&#24615;&#27604;&#29575;&#30340;&#26041;&#27861;&#65292;&#29992;&#20197;&#35299;&#20915;&#20316;&#32773;&#36523;&#20221;&#39564;&#35777;&#20013;&#23384;&#22312;&#30340;&#31185;&#23398;&#35299;&#37322;&#19981;&#36275;&#21644;&#38590;&#20197;&#35299;&#37322;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#36523;&#20221;&#39564;&#35777;&#65288;AV&#65289;&#26159;&#20998;&#26512;&#19968;&#32452;&#25991;&#20214;&#20197;&#30830;&#23450;&#23427;&#20204;&#26159;&#21542;&#30001;&#29305;&#23450;&#20316;&#32773;&#25776;&#20889;&#30340;&#36807;&#31243;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;AV&#26041;&#27861;&#20351;&#29992;&#35745;&#31639;&#35299;&#20915;&#26041;&#26696;&#65292;&#23545;&#20110;&#20854;&#21151;&#33021;&#27809;&#26377;&#21512;&#29702;&#30340;&#31185;&#23398;&#35299;&#37322;&#65292;&#24182;&#19988;&#24120;&#24120;&#38590;&#20197;&#35299;&#37322;&#32473;&#20998;&#26512;&#20154;&#21592;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#35745;&#31639;&#19968;&#20010;&#25105;&#20204;&#31216;&#20043;&#20026; $\lambda_G$&#65288;LambdaG&#65289;&#30340;&#37327;&#65306;&#20505;&#36873;&#20316;&#32773;&#30340;&#19978;&#19979;&#25991;&#35821;&#27861;&#27169;&#22411;&#32473;&#20986;&#30340;&#25991;&#26723;&#30340;&#21487;&#33021;&#24615;&#19982;&#21442;&#32771;&#32676;&#20307;&#30340;&#19978;&#19979;&#25991;&#35821;&#27861;&#27169;&#22411;&#32473;&#20986;&#30340;&#30456;&#21516;&#25991;&#26723;&#30340;&#21487;&#33021;&#24615;&#20043;&#38388;&#30340;&#27604;&#29575;&#12290;&#36825;&#20123;&#35821;&#27861;&#27169;&#22411;&#26159;&#20351;&#29992;&#20165;&#38024;&#23545;&#35821;&#27861;&#29305;&#24449;&#36827;&#34892;&#35757;&#32451;&#30340; $n$-gram&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20272;&#35745;&#30340;&#12290;&#23613;&#31649;&#19981;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;LambdaG...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08462v1 Announce Type: new  Abstract: Authorship Verification (AV) is the process of analyzing a set of documents to determine whether they were written by a specific author. This problem often arises in forensic scenarios, e.g., in cases where the documents in question constitute evidence for a crime. Existing state-of-the-art AV methods use computational solutions that are not supported by a plausible scientific explanation for their functioning and that are often difficult for analysts to interpret. To address this, we propose a method relying on calculating a quantity we call $\lambda_G$ (LambdaG): the ratio between the likelihood of a document given a model of the Grammar for the candidate author and the likelihood of the same document given a model of the Grammar for a reference population. These Grammar Models are estimated using $n$-gram language models that are trained solely on grammatical features. Despite not needing large amounts of data for training, LambdaG st
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31062;&#21338;&#22827;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#65292;&#20197;&#21450;&#23545;&#24212;&#30340;&#26446;&#38597;&#26222;&#35834;&#22827;&#35777;&#20070;&#65292;&#20197;&#26368;&#22823;&#21270;&#21306;&#22495;&#21560;&#24341;&#21147;&#65292;&#24182;&#23562;&#37325;&#28608;&#21169;&#32422;&#26463;&#12290;</title><link>https://arxiv.org/abs/2403.08448</link><description>&lt;p&gt;
&#12298;&#22522;&#20110;&#28436;&#21592;&#35780;&#35770;&#32773;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#26446;&#38597;&#26222;&#35834;&#22827;&#25511;&#21046;&#12299;
&lt;/p&gt;
&lt;p&gt;
Actor-Critic Physics-informed Neural Lyapunov Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08448
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31062;&#21338;&#22827;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#65292;&#20197;&#21450;&#23545;&#24212;&#30340;&#26446;&#38597;&#26222;&#35834;&#22827;&#35777;&#20070;&#65292;&#20197;&#26368;&#22823;&#21270;&#21306;&#22495;&#21560;&#24341;&#21147;&#65292;&#24182;&#23562;&#37325;&#28608;&#21169;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20855;&#26377;&#21487;&#35777;&#20445;&#35777;&#30340;&#31283;&#23450;&#21270;&#20219;&#21153;&#25511;&#21046;&#31574;&#30053;&#26159;&#38750;&#32447;&#24615;&#25511;&#21046;&#20013;&#30340;&#19968;&#20010;&#38271;&#26399;&#38382;&#39064;&#12290;&#20851;&#38190;&#30340;&#24615;&#33021;&#25351;&#26631;&#26159;&#20135;&#29983;&#21306;&#22495;&#21560;&#24341;&#21147;&#30340;&#22823;&#23567;&#65292;&#36825;&#22522;&#26412;&#19978;&#20805;&#24403;&#20102;&#23553;&#38381;&#29615;&#31995;&#32479;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#24377;&#24615;&#8220;&#36793;&#30028;&#8221;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#19968;&#20010;&#31283;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#20197;&#21450;&#20854;&#23545;&#24212;&#30340;&#26446;&#38597;&#26222;&#35834;&#22827;&#35777;&#20070;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#20135;&#29983;&#30340;&#21306;&#22495;&#21560;&#24341;&#21147;&#65292;&#21516;&#26102;&#23562;&#37325;&#28608;&#21169;&#32422;&#26463;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#20043;&#22788;&#22312;&#20110;&#20351;&#29992;&#31062;&#21338;&#22827;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#65292;&#35813;&#26041;&#31243;&#31934;&#30830;&#22320;&#34920;&#24449;&#20102;&#32473;&#23450;&#25511;&#21046;&#31574;&#30053;&#30340;&#30495;&#23454;&#21306;&#22495;&#21560;&#24341;&#21147;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36981;&#24490;&#28436;&#21592;&#35780;&#35770;&#32773;&#27169;&#24335;&#65292;&#25105;&#20204;&#22312;&#25913;&#36827;&#25511;&#21046;&#31574;&#30053;&#65288;&#28436;&#21592;&#65289;&#21644;&#23398;&#20064;&#31062;&#21338;&#22827;&#20989;&#25968;&#65288;&#35780;&#35770;&#32773;&#65289;&#20043;&#38388;&#20132;&#26367;&#36827;&#34892;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35843;&#29992;SMT&#27714;&#35299;&#22120;&#35745;&#31639;&#20986;&#26368;&#22823;&#30340;&#21487;&#35777;&#21306;&#22495;&#21560;&#24341;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08448v1 Announce Type: new  Abstract: Designing control policies for stabilization tasks with provable guarantees is a long-standing problem in nonlinear control. A crucial performance metric is the size of the resulting region of attraction, which essentially serves as a robustness "margin" of the closed-loop system against uncertainties. In this paper, we propose a new method to train a stabilizing neural network controller along with its corresponding Lyapunov certificate, aiming to maximize the resulting region of attraction while respecting the actuation constraints. Crucial to our approach is the use of Zubov's Partial Differential Equation (PDE), which precisely characterizes the true region of attraction of a given control policy. Our framework follows an actor-critic pattern where we alternate between improving the control policy (actor) and learning a Zubov function (critic). Finally, we compute the largest certifiable region of attraction by invoking an SMT solver
&lt;/p&gt;</description></item><item><title>COSTREAM&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#22312;&#36793;&#32536;-&#20113;&#29615;&#22659;&#20013;&#20934;&#30830;&#39044;&#27979;&#27969;&#26597;&#35810;&#25191;&#34892;&#25104;&#26412;&#30340;&#23398;&#20064;&#25104;&#26412;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36816;&#31639;&#31526;&#30340;&#25918;&#32622;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;21&#20493;&#30340;&#20013;&#20301;&#25968;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2403.08444</link><description>&lt;p&gt;
COSTREAM&#65306;&#36793;&#32536;-&#20113;&#29615;&#22659;&#20013;&#36816;&#31639;&#31526;&#25918;&#32622;&#30340;&#23398;&#20064;&#25104;&#26412;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
COSTREAM: Learned Cost Models for Operator Placement in Edge-Cloud Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08444
&lt;/p&gt;
&lt;p&gt;
COSTREAM&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#22312;&#36793;&#32536;-&#20113;&#29615;&#22659;&#20013;&#20934;&#30830;&#39044;&#27979;&#27969;&#26597;&#35810;&#25191;&#34892;&#25104;&#26412;&#30340;&#23398;&#20064;&#25104;&#26412;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36816;&#31639;&#31526;&#30340;&#25918;&#32622;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;21&#20493;&#30340;&#20013;&#20301;&#25968;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COSTREAM&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#25104;&#26412;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#20998;&#24067;&#24335;&#27969;&#22788;&#29702;&#31995;&#32479;&#65292;&#22312;&#36793;&#32536;-&#20113;&#29615;&#22659;&#20013;&#23545;&#27969;&#26597;&#35810;&#30340;&#25191;&#34892;&#25104;&#26412;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#12290;&#35813;&#25104;&#26412;&#27169;&#22411;&#21487;&#29992;&#20110;&#22312;&#24322;&#26500;&#30828;&#20214;&#19978;&#25214;&#21040;&#36816;&#31639;&#31526;&#30340;&#21021;&#22987;&#25918;&#32622;&#65292;&#36825;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;COSTREAM&#21487;&#20197;&#20026;&#21021;&#22987;&#36816;&#31639;&#31526;&#25918;&#32622;&#20135;&#29983;&#39640;&#24230;&#20934;&#30830;&#30340;&#25104;&#26412;&#20272;&#31639;&#65292;&#29978;&#33267;&#33021;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#25918;&#32622;&#12289;&#26597;&#35810;&#21644;&#30828;&#20214;&#12290;&#24403;&#20351;&#29992;COSTREAM&#26469;&#20248;&#21270;&#27969;&#22788;&#29702;&#36816;&#31639;&#31526;&#30340;&#25918;&#32622;&#26102;&#65292;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#21487;&#23454;&#29616;&#22823;&#32422;21&#20493;&#30340;&#20013;&#20301;&#25968;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08444v1 Announce Type: cross  Abstract: In this work, we present COSTREAM, a novel learned cost model for Distributed Stream Processing Systems that provides accurate predictions of the execution costs of a streaming query in an edge-cloud environment. The cost model can be used to find an initial placement of operators across heterogeneous hardware, which is particularly important in these environments. In our evaluation, we demonstrate that COSTREAM can produce highly accurate cost estimates for the initial operator placement and even generalize to unseen placements, queries, and hardware. When using COSTREAM to optimize the placements of streaming operators, a median speed-up of around 21x can be achieved compared to baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#20013;&#30340;&#20877;&#29616;&#24615;&#21644;&#20960;&#20309;&#20869;&#22312;&#32500;&#24230;&#24615;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20877;&#29616;&#24615;&#26412;&#20307;&#35770;&#65292;&#20197;&#21450;&#25506;&#35752;&#20102;&#32500;&#24230;&#35781;&#21650;&#23545;&#25968;&#25454;&#25910;&#38598;&#12289;&#34920;&#31034;&#21644;&#20998;&#26512;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.08438</link><description>&lt;p&gt;
&#20877;&#29616;&#24615;&#21644;&#20960;&#20309;&#20869;&#22312;&#32500;&#24230;&#24615;&#65306;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reproducibility and Geometric Intrinsic Dimensionality: An Investigation on Graph Neural Network Research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#20013;&#30340;&#20877;&#29616;&#24615;&#21644;&#20960;&#20309;&#20869;&#22312;&#32500;&#24230;&#24615;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20877;&#29616;&#24615;&#26412;&#20307;&#35770;&#65292;&#20197;&#21450;&#25506;&#35752;&#20102;&#32500;&#24230;&#35781;&#21650;&#23545;&#25968;&#25454;&#25910;&#38598;&#12289;&#34920;&#31034;&#21644;&#20998;&#26512;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#22797;&#21046;&#21644;&#21487;&#20877;&#29616;&#24615;&#30340;&#22256;&#38590;&#36817;&#24180;&#26469;&#25104;&#20026;&#19968;&#20010;&#31361;&#20986;&#30340;&#35805;&#39064;&#12290;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#38656;&#35201;&#21487;&#20877;&#29616;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#30456;&#21516;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#39564;&#35777;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;&#36825;&#20419;&#36827;&#20102;&#24320;&#25918;&#21644;&#21487;&#35775;&#38382;&#30340;&#30740;&#31350;&#12289;&#31283;&#20581;&#30340;&#23454;&#39564;&#24037;&#20316;&#27969;&#31243;&#20197;&#21450;&#26032;&#21457;&#29616;&#30340;&#24555;&#36895;&#25972;&#21512;&#12290;&#35780;&#20272;&#30740;&#31350;&#20986;&#29256;&#29289;&#25903;&#25345;&#20877;&#29616;&#24615;&#30340;&#31243;&#24230;&#26159;&#26412;&#25991;&#30340;&#19968;&#20010;&#30446;&#26631;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20877;&#29616;&#24615;&#26412;&#20307;&#35770;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#20123;&#21162;&#21147;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36716;&#21521;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21478;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#32500;&#24230;&#35781;&#21650;&#65292;&#23427;&#22312;&#25968;&#25454;&#25910;&#38598;&#12289;&#34920;&#31034;&#21644;&#20998;&#26512;&#26041;&#38754;&#24102;&#26469;&#25361;&#25112;&#65292;&#20351;&#24471;&#26356;&#38590;&#25214;&#21040;&#20195;&#34920;&#24615;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08438v1 Announce Type: cross  Abstract: Difficulties in replication and reproducibility of empirical evidences in machine learning research have become a prominent topic in recent years. Ensuring that machine learning research results are sound and reliable requires reproducibility, which verifies the reliability of research findings using the same code and data. This promotes open and accessible research, robust experimental workflows, and the rapid integration of new findings. Evaluating the degree to which research publications support these different aspects of reproducibility is one goal of the present work. For this we introduce an ontology of reproducibility in machine learning and apply it to methods for graph neural networks. Building on these efforts we turn towards another critical challenge in machine learning, namely the curse of dimensionality, which poses challenges in data collection, representation, and analysis, making it harder to find representative data 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DeepCSHAP&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;Shapley Values&#35299;&#37322;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#22635;&#34917;&#20102;&#23545;&#20110;&#36825;&#31181;&#31867;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#31639;&#27861;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.08428</link><description>&lt;p&gt;
DeepCSHAP: &#21033;&#29992;Shapley Values&#35299;&#37322;&#28145;&#24230;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DeepCSHAP: Utilizing Shapley Values to Explain Deep Complex-Valued Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DeepCSHAP&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;Shapley Values&#35299;&#37322;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#22635;&#34917;&#20102;&#23545;&#20110;&#36825;&#31181;&#31867;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#31639;&#27861;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08428v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25688;&#35201;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24191;&#27867;&#24212;&#29992;&#20110;&#23398;&#26415;&#30028;&#20197;&#21450;&#20225;&#19994;&#21644;&#20844;&#20849;&#24212;&#29992;&#31243;&#24207;&#65292;&#21253;&#25324;&#35832;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#12290;&#35299;&#37322;&#20854;&#36755;&#20986;&#30340;&#33021;&#21147;&#23545;&#20110;&#23433;&#20840;&#21407;&#22240;&#20197;&#21450;&#30003;&#35831;&#32773;&#30340;&#25509;&#21463;&#33267;&#20851;&#37325;&#35201;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#22823;&#37327;&#26041;&#27861;&#26469;&#35299;&#37322;&#23454;&#25968;&#31070;&#32463;&#32593;&#32476;&#12290;&#26368;&#36817;&#65292;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#20986;&#29616;&#65292;&#22788;&#29702;&#22797;&#25968;&#36755;&#20837;&#25968;&#25454;&#32780;&#26080;&#38656;&#23558;&#20854;&#25237;&#24433;&#21040;$\mathbb{R}^2$&#12290;&#36825;&#24102;&#26469;&#20102;&#20026;&#36825;&#31181;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#35299;&#37322;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#20123;&#21457;&#23637;&#12290;&#34429;&#28982;&#25105;&#20204;&#19987;&#27880;&#20110;&#23558;&#24191;&#27867;&#24212;&#29992;&#30340;DeepSHAP&#31639;&#27861;&#35843;&#25972;&#21040;&#22797;&#25968;&#22495;&#65292;&#20294;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#36866;&#29992;&#20110;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#22235;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#29256;&#26412;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25152;&#26377;&#25552;&#20986;&#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#35299;&#37322;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08428v1 Announce Type: new  Abstract: Deep Neural Networks are widely used in academy as well as corporate and public applications, including safety critical applications such as health care and autonomous driving. The ability to explain their output is critical for safety reasons as well as acceptance among applicants. A multitude of methods have been proposed to explain real-valued neural networks. Recently, complex-valued neural networks have emerged as a new class of neural networks dealing with complex-valued input data without the necessity of projecting them onto $\mathbb{R}^2$. This brings up the need to develop explanation algorithms for this kind of neural networks. In this paper we provide these developments. While we focus on adapting the widely used DeepSHAP algorithm to the complex domain, we also present versions of four gradient based explanation methods suitable for use in complex-valued neural networks. We evaluate the explanation quality of all presented a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#31227;&#21160;&#24179;&#21488;&#65292;&#21487;&#20197;&#35270;&#35273;&#30830;&#23450;&#38452;&#33550;&#30149;&#29702;&#30340;&#30149;&#22240;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#24615;&#20581;&#24247;&#26381;&#21153;&#30340;&#33719;&#21462;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08417</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31227;&#21160;&#24179;&#21488;&#65292;&#29992;&#20110;&#35270;&#35273;&#30830;&#23450;&#38452;&#33550;&#30149;&#29702;&#23398;&#30149;&#22240;&#30340;&#24320;&#21457;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
The Development and Performance of a Machine Learning Based Mobile Platform for Visually Determining the Etiology of Penile Pathology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08417
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#31227;&#21160;&#24179;&#21488;&#65292;&#21487;&#20197;&#35270;&#35273;&#30830;&#23450;&#38452;&#33550;&#30149;&#29702;&#30340;&#30149;&#22240;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#24615;&#20581;&#24247;&#26381;&#21153;&#30340;&#33719;&#21462;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#20419;&#36827;&#20302;&#25104;&#26412;&#12289;&#29992;&#25143;&#24341;&#23548;&#30340;&#35270;&#35273;&#35786;&#26029;&#24179;&#21488;&#65292;&#20197;&#35299;&#20915;&#33719;&#21462;&#24615;&#20581;&#24247;&#26381;&#21153;&#30340;&#19981;&#24179;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#21407;&#22987;&#21644;&#22686;&#24191;&#22270;&#20687;&#20026;&#20116;&#31181;&#38452;&#33550;&#30142;&#30149;&#24320;&#21457;&#20102;&#20020;&#24202;&#22270;&#20687;&#25968;&#25454;&#38598;: &#30129;&#30137;&#29190;&#21457;&#12289;&#26757;&#27602;&#24615;&#28291;&#30113;&#12289;&#38452;&#33550;&#24565;&#29664;&#33740;&#30149;&#12289;&#38452;&#33550;&#30284;&#21644;&#29983;&#27542;&#30115;&#12290;&#25105;&#20204;&#20351;&#29992;U-net&#26550;&#26500;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#20687;&#32032;&#20998;&#21106;&#65292;&#23558;&#22270;&#20687;&#20998;&#20026;&#32972;&#26223;&#25110;&#20027;&#20307;&#22270;&#20687;&#65292;&#20351;&#29992;Inception-ResNet&#29256;&#26412;2&#31070;&#32463;&#26550;&#26500;&#23558;&#27599;&#20010;&#20687;&#32032;&#20998;&#31867;&#20026;&#24739;&#30149;&#25110;&#38750;&#24739;&#30149;&#65292;&#24182;&#20351;&#29992;GradCAM++&#29983;&#25104;&#26174;&#33879;&#24615;&#22270;&#12290;&#25105;&#20204;&#23545;&#22270;&#20687;&#25968;&#25454;&#24211;&#20013;&#38543;&#26426;&#36873;&#21462;&#30340;91%&#26679;&#26412;&#36827;&#34892;&#20102;150&#27425;&#22270;&#20687;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#20854;&#20313;9%&#30340;&#22270;&#20687;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#65292;&#35780;&#20272;&#20102;&#21484;&#22238;&#29575;&#65288;&#25110;&#28789;&#25935;&#24230;&#65289;&#12289;&#31934;&#30830;&#24230;&#12289;&#29305;&#24322;&#24230;&#21644;F1&#20998;&#25968;&#65288;&#20934;&#30830;&#24230;&#65289;&#12290;&#22312;&#39564;&#35777;&#25968;&#25454;&#38598;&#20013;&#30340;239&#24352;&#22270;&#20687;&#20013;&#65292;45&#24352;&#65288;18.8%&#65289;&#26159;&#29983;&#27542;&#30115;&#65292;43&#24352;&#65288;18.0%&#65289;&#26159;HSV&#24863;&#26579;&#65292;29&#24352;&#65288;12.1%&#65289;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08417v1 Announce Type: cross  Abstract: Machine-learning algorithms can facilitate low-cost, user-guided visual diagnostic platforms for addressing disparities in access to sexual health services. We developed a clinical image dataset using original and augmented images for five penile diseases: herpes eruption, syphilitic chancres, penile candidiasis, penile cancer, and genital warts. We used a U-net architecture model for semantic pixel segmentation into background or subject image, the Inception-ResNet version 2 neural architecture to classify each pixel as diseased or non-diseased, and a salience map using GradCAM++. We trained the model on a random 91% sample of the image database using 150 epochs per image, and evaluated the model on the remaining 9% of images, assessing recall (or sensitivity), precision, specificity, and F1-score (accuracy). Of the 239 images in the validation dataset, 45 (18.8%) were of genital warts, 43 (18.0%) were of HSV infection, 29 (12.1%) wer
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22240;&#26524;&#24615;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#27169;&#22411;&#33021;&#22815;&#26174;&#24335;&#22320;&#24314;&#27169;&#22797;&#26434;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#26426;&#29702;&#65292;&#25552;&#39640;&#20102;&#28779;&#28798;&#27169;&#24335;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08414</link><description>&lt;p&gt;
&#29992;&#20110;&#28779;&#28798;&#21361;&#38505;&#39044;&#27979;&#30340;&#22240;&#26524;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Causal Graph Neural Networks for Wildfire Danger Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08414
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22240;&#26524;&#24615;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#27169;&#22411;&#33021;&#22815;&#26174;&#24335;&#22320;&#24314;&#27169;&#22797;&#26434;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#26426;&#29702;&#65292;&#25552;&#39640;&#20102;&#28779;&#28798;&#27169;&#24335;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28779;&#28798;&#39044;&#27979;&#22240;&#22825;&#27668;&#26465;&#20214;&#12289;&#26893;&#34987;&#31867;&#22411;&#21644;&#20154;&#31867;&#27963;&#21160;&#31561;&#19981;&#21516;&#22240;&#32032;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#32780;&#21464;&#24471;&#38590;&#20197;&#39044;&#27979;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23637;&#29616;&#20102;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#22788;&#29702;&#36825;&#31181;&#22797;&#26434;&#24615;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#25903;&#25345;&#20851;&#38190;&#20915;&#31574;&#65292;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#38656;&#35201;&#36866;&#21512;&#27491;&#30830;&#21407;&#22240;&#30340;&#27169;&#22411;&#65307;&#20063;&#23601;&#26159;&#35828;&#65292;&#23398;&#21040;&#30340;&#38544;&#24335;&#35268;&#21017;&#24212;&#35813;&#20197;&#25512;&#21160;&#28779;&#28798;&#30340;&#22522;&#26412;&#36807;&#31243;&#20026;&#22522;&#30784;&#12290;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#22240;&#26524;&#24615;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#22270;&#23398;&#20064;&#26174;&#24335;&#22320;&#23545;&#22797;&#26434;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#26426;&#29702;&#24314;&#27169;&#12290;&#22240;&#26524;&#37051;&#25509;&#30697;&#38453;&#32771;&#34385;&#20102;&#19981;&#21516;&#21464;&#37327;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#24182;&#28040;&#38500;&#20102;&#39640;&#24230;&#30456;&#20851;&#24433;&#21709;&#20043;&#38388;&#30340;&#34394;&#20551;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#22312;&#27431;&#27954;&#21271;&#37096;&#21644;&#22320;&#20013;&#28023;&#29983;&#29289;&#32676;&#31995;&#20013;&#20248;&#36234;&#24615;&#33021;&#39044;&#27979;&#28779;&#28798;&#27169;&#24335;&#32780;&#24471;&#21040;&#35777;&#26126;&#12290;&#25910;&#30410;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08414v1 Announce Type: cross  Abstract: Wildfire forecasting is notoriously hard due to the complex interplay of different factors such as weather conditions, vegetation types and human activities. Deep learning models show promise in dealing with this complexity by learning directly from data. However, to inform critical decision making, we argue that we need models that are right for the right reasons; that is, the implicit rules learned should be grounded by the underlying processes driving wildfires. In that direction, we propose integrating causality with Graph Neural Networks (GNNs) that explicitly model the causal mechanism among complex variables via graph learning. The causal adjacency matrix considers the synergistic effect among variables and removes the spurious links from highly correlated impacts. Our methodology's effectiveness is demonstrated through superior performance forecasting wildfire patterns in the European boreal and mediterranean biome. The gain is
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20943;&#23567;Jeffries-Matusita&#36317;&#31163;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#20998;&#31867;&#27169;&#22411;&#20197;&#20943;&#23569;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08408</link><description>&lt;p&gt;
&#20943;&#23567;Jeffries-Matusita&#36317;&#31163;&#65306;&#25552;&#39640;&#28145;&#24230;&#20998;&#31867;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#30340;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Reduced Jeffries-Matusita distance: A Novel Loss Function to Improve Generalization Performance of Deep Classification Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08408
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20943;&#23567;Jeffries-Matusita&#36317;&#31163;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#20998;&#31867;&#27169;&#22411;&#20197;&#20943;&#23569;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#12290;&#23613;&#31649;&#24191;&#27867;&#37319;&#29992;&#35832;&#22914;&#25968;&#25454;&#22686;&#24378;&#12289;&#20266;&#26631;&#35760;&#12289;&#27491;&#21017;&#21270;&#21644;&#38598;&#25104;&#23398;&#20064;&#31561;&#25216;&#26415;&#26469;&#20943;&#23569;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#20294;&#20173;&#38656;&#35201;&#36890;&#36807;&#20854;&#20182;&#26041;&#27861;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#25439;&#22833;&#20989;&#25968;&#29305;&#24615;&#65292;&#22914;&#20854;Lipschitz&#24615;&#21644;&#26368;&#22823;&#20540;&#65292;&#24433;&#21709;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36825;&#21487;&#20197;&#34987;&#29992;&#20316;&#25552;&#20986;&#26032;&#30340;&#36317;&#31163;&#24230;&#37327;&#30340;&#25351;&#23548;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#19978;&#36848;&#29305;&#24615;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#20943;&#23567;Jeffries-Matusita&#36317;&#31163;&#30340;&#36317;&#31163;&#20316;&#20026;&#28145;&#24230;&#20998;&#31867;&#27169;&#22411;&#35757;&#32451;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#20943;&#23569;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#38382;&#39064;&#20013;&#35780;&#20272;&#20102;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65306;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22270;&#20687;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08408v1 Announce Type: new  Abstract: The generalization performance of deep neural networks in classification tasks is a major concern in machine learning research. Despite widespread techniques used to diminish the over-fitting issue such as data augmentation, pseudo-labeling, regularization, and ensemble learning, this performance still needs to be enhanced with other approaches. In recent years, it has been theoretically demonstrated that the loss function characteristics i.e. its Lipschitzness and maximum value affect the generalization performance of deep neural networks which can be utilized as a guidance to propose novel distance measures. In this paper, by analyzing the aforementioned characteristics, we introduce a distance called Reduced Jeffries-Matusita as a loss function for training deep classification models to reduce the over-fitting issue. In our experiments, we evaluate the new loss function in two different problems: image classification in computer visio
&lt;/p&gt;</description></item><item><title>FSDR&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#36890;&#36807;&#31163;&#25955;&#26494;&#24347;&#23398;&#20064;&#37325;&#35201;&#29305;&#24449;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#30340;&#20266;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;</title><link>https://arxiv.org/abs/2403.08403</link><description>&lt;p&gt;
FSDR&#65306;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31163;&#25955;&#26494;&#24347;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#29992;&#20110;&#20266;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
FSDR: A Novel Deep Learning-based Feature Selection Algorithm for Pseudo Time-Series Data using Discrete Relaxation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08403
&lt;/p&gt;
&lt;p&gt;
FSDR&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#36890;&#36807;&#31163;&#25955;&#26494;&#24347;&#23398;&#20064;&#37325;&#35201;&#29305;&#24449;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#30340;&#20266;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#20110;&#20266;&#26102;&#38388;&#24207;&#21015;&#65288;PTS&#65289;&#25968;&#25454;&#30340;&#20256;&#32479;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#30001;&#20110;&#20855;&#26377;&#39640;&#32500;&#25968;&#25454;&#30340;&#19981;&#20999;&#23454;&#38469;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;-&#22522;&#20110;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65306;&#31163;&#25955;&#26494;&#24347;&#29305;&#24449;&#36873;&#25321;&#65288;FSDR&#65289;&#65292;&#19987;&#38376;&#38024;&#23545;PTS&#25968;&#25454;&#12290;&#19982;&#29616;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#19981;&#21516;&#65292;FSDR&#36890;&#36807;&#31163;&#25955;&#26494;&#24347;&#23398;&#20064;&#37325;&#35201;&#29305;&#24449;&#20316;&#20026;&#27169;&#22411;&#21442;&#25968;&#65292;&#36825;&#26159;&#25351;&#36890;&#36807;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#36817;&#20284;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;&#30340;&#36807;&#31243;&#12290;FSDR&#33021;&#22815;&#36866;&#24212;&#22823;&#37327;&#29305;&#24449;&#32500;&#24230;&#65292;&#36825;&#26159;&#29616;&#26377;DL&#25110;&#20256;&#32479;&#26041;&#27861;&#38590;&#20197;&#23454;&#29616;&#30340;&#12290;&#36890;&#36807;&#23545;&#39640;&#20809;&#35889;&#25968;&#25454;&#38598;&#65288;&#21363;PTS&#25968;&#25454;&#30340;&#19968;&#31181;&#65289;&#36827;&#34892;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;FSDR
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08403v1 Announce Type: new  Abstract: Conventional feature selection algorithms applied to Pseudo Time-Series (PTS) data, which consists of observations arranged in sequential order without adhering to a conventional temporal dimension, often exhibit impractical computational complexities with high dimensional data. To address this challenge, we introduce a Deep Learning (DL)-based feature selection algorithm: Feature Selection through Discrete Relaxation (FSDR), tailored for PTS data. Unlike the existing feature selection algorithms, FSDR learns the important features as model parameters using discrete relaxation, which refers to the process of approximating a discrete optimisation problem with a continuous one. FSDR is capable of accommodating a high number of feature dimensions, a capability beyond the reach of existing DL-based or traditional methods. Through testing on a hyperspectral dataset (i.e., a type of PTS data), our experimental results demonstrate that FSDR out
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#29702;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#23398;&#20064;&#22914;&#20309;&#22312;&#28151;&#21512;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#20013;&#26368;&#20339;&#20998;&#37197;&#20915;&#31574;&#36131;&#20219;&#65292;&#24182;&#26368;&#23567;&#21270;&#19981;&#33391;&#22242;&#38431;&#34892;&#20026;&#23548;&#33268;&#30340;&#22996;&#27966;&#21464;&#26356;&#27425;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.08386</link><description>&lt;p&gt;
&#20248;&#21270;&#39118;&#38505;&#25935;&#24863;&#30340;&#20154;&#24037;&#26234;&#33021;&#28151;&#21512;&#22242;&#38431;
&lt;/p&gt;
&lt;p&gt;
Optimizing Risk-averse Human-AI Hybrid Teams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08386
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#29702;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#23398;&#20064;&#22914;&#20309;&#22312;&#28151;&#21512;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#20013;&#26368;&#20339;&#20998;&#37197;&#20915;&#31574;&#36131;&#20219;&#65292;&#24182;&#26368;&#23567;&#21270;&#19981;&#33391;&#22242;&#38431;&#34892;&#20026;&#23548;&#33268;&#30340;&#22996;&#27966;&#21464;&#26356;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39044;&#35745;&#38543;&#30528;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#25152;&#35859;&#30340;&#28151;&#21512;&#22242;&#38431;&#20013;&#21512;&#20316;&#30340;&#22686;&#21152;&#65292;&#36825;&#31181;&#21512;&#20316;&#30340;&#39057;&#29575;&#23558;&#22686;&#21152;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#29087;&#32451;&#24230;&#25552;&#39640;&#21644;&#20854;&#37319;&#29992;&#21464;&#24471;&#26356;&#21152;&#24191;&#27867;&#65292;&#39044;&#35745;&#21327;&#20316;&#23558;&#22686;&#21152;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#30340;&#34892;&#20026;&#24182;&#38750;&#26080;&#35823;&#65292;&#20174;&#32780;&#20351;&#28151;&#21512;&#22242;&#38431;&#25104;&#20026;&#19968;&#31181;&#38750;&#24120;&#21512;&#36866;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#25913;&#36827;&#36825;&#20123;&#30001;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#32452;&#25104;&#30340;&#22242;&#38431;&#30340;&#32489;&#25928;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#28151;&#21512;&#22242;&#38431;&#65292;&#25105;&#20204;&#23558;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#37117;&#31216;&#20026;&#20195;&#29702;&#12290;&#20026;&#20102;&#25552;&#39640;&#22242;&#38431;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#29702;&#65292;&#35813;&#32463;&#29702;&#36890;&#36807;&#26631;&#20934;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#23398;&#20064;&#22914;&#20309;&#22312;&#19968;&#27573;&#26102;&#38388;&#20869;&#26368;&#22909;&#22320;&#22996;&#27966;&#20915;&#31574;&#36131;&#20219;&#32473;&#20219;&#20309;&#19968;&#20010;&#20195;&#29702;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#23548;&#32463;&#29702;&#30340;&#23398;&#20064;&#65292;&#35753;&#20182;&#20204;&#26368;&#23567;&#21270;&#22240;&#19981;&#33391;&#22242;&#38431;&#34892;&#20026;&#32780;&#23548;&#33268;&#30340;&#22996;&#27966;&#21464;&#26356;&#27425;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31649;&#29702;&#32773;&#32489;&#25928;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08386v1 Announce Type: new  Abstract: We anticipate increased instances of humans and AI systems working together in what we refer to as a hybrid team. The increase in collaboration is expected as AI systems gain proficiency and their adoption becomes more widespread. However, their behavior is not error-free, making hybrid teams a very suitable solution. As such, we consider methods for improving performance for these teams of humans and AI systems. For hybrid teams, we will refer to both the humans and AI systems as agents. To improve team performance over that seen for agents operating individually, we propose a manager which learns, through a standard Reinforcement Learning scheme, how to best delegate, over time, the responsibility of taking a decision to any of the agents. We further guide the manager's learning so they also minimize how many changes in delegation are made resulting from undesirable team behavior. We demonstrate the optimality of our manager's performa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#26367;&#20195;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#26469;&#36890;&#36807;&#38750;&#32447;&#24615;&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#20174;&#25289;&#26364;&#20809;&#35889;&#27979;&#37327;&#20013;&#20934;&#30830;&#21487;&#38752;&#22320;&#30830;&#23450;&#32858;&#21512;&#29289;&#23610;&#23544;&#12290;</title><link>https://arxiv.org/abs/2403.08376</link><description>&lt;p&gt;
&#29992;&#20110;&#25289;&#26364;&#20809;&#35889;&#30830;&#23450;&#24494;&#20957;&#33014;&#23610;&#23544;&#30340;&#38750;&#32447;&#24615;&#27969;&#24418;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Nonlinear Manifold Learning Determines Microgel Size from Raman Spectroscopy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#26367;&#20195;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#26469;&#36890;&#36807;&#38750;&#32447;&#24615;&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#20174;&#25289;&#26364;&#20809;&#35889;&#27979;&#37327;&#20013;&#20934;&#30830;&#21487;&#38752;&#22320;&#30830;&#23450;&#32858;&#21512;&#29289;&#23610;&#23544;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#21512;&#29289;&#31890;&#23376;&#23610;&#23544;&#26500;&#25104;&#20102;&#32858;&#21512;&#36807;&#31243;&#20013;&#20135;&#21697;&#36136;&#37327;&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24449;&#12290;&#25289;&#26364;&#20809;&#35889;&#26159;&#19968;&#31181;&#24050;&#24314;&#31435;&#19988;&#21487;&#38752;&#30340;&#36807;&#31243;&#20998;&#26512;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#22312;&#32447;&#27987;&#24230;&#30417;&#27979;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#21644;&#19968;&#20123;&#29702;&#35770;&#32771;&#34385;&#26174;&#31034;&#20102;&#25289;&#26364;&#20449;&#21495;&#19982;&#39063;&#31890;&#23610;&#23544;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#23578;&#19981;&#33021;&#31934;&#30830;&#21487;&#38752;&#22320;&#20174;&#25289;&#26364;&#20809;&#35889;&#27979;&#37327;&#20013;&#30830;&#23450;&#32858;&#21512;&#29289;&#23610;&#23544;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26367;&#20195;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#26469;&#25191;&#34892;&#36825;&#39033;&#20219;&#21153;&#65292;&#20854;&#20013;&#37117;&#28041;&#21450;&#25193;&#25955;&#26144;&#23556;&#65292;&#36825;&#26159;&#19968;&#31181;&#38750;&#32447;&#24615;&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#29992;&#20110;&#38477;&#32500;: (i) &#30452;&#25509;&#20174;&#25193;&#25955;&#26144;&#23556;&#65292;(ii) &#26367;&#20195;&#25193;&#25955;&#26144;&#23556;&#65292;&#21644; (iii) &#31526;&#21512;&#33258;&#21160;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#24037;&#20316;&#27969;&#31243;&#24212;&#29992;&#20110;&#21253;&#21547;47&#20010;&#24494;&#20957;&#33014;&#65288;&#20132;&#32852;&#32858;&#21512;&#29289;&#65289;&#26679;&#26412;&#30340;&#25289;&#26364;&#20809;&#35889;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#26679;&#26412;&#30340;&#30452;&#24452;&#33539;&#22260;&#20026;208nm&#21040;483 nm&#65292;&#19988;&#20854;&#23610;&#23544;&#26159;&#36890;&#36807;&#21160;&#24577;&#20809;&#25955;&#23556;&#27979;&#37327;&#24471;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08376v1 Announce Type: new  Abstract: Polymer particle size constitutes a crucial characteristic of product quality in polymerization. Raman spectroscopy is an established and reliable process analytical technology for in-line concentration monitoring. Recent approaches and some theoretical considerations show a correlation between Raman signals and particle sizes but do not determine polymer size from Raman spectroscopic measurements accurately and reliably. With this in mind, we propose three alternative machine learning workflows to perform this task, all involving diffusion maps, a nonlinear manifold learning technique for dimensionality reduction: (i) directly from diffusion maps, (ii) alternating diffusion maps, and (iii) conformal autoencoder neural networks. We apply the workflows to a data set of Raman spectra with associated size measured via dynamic light scattering of 47 microgel (cross-linked polymer) samples in a diameter range of 208nm to 483 nm. The conformal
&lt;/p&gt;</description></item><item><title>SMART&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#24182;&#22312;&#24494;&#35843;&#20013;&#37325;&#26032;&#20998;&#37197;&#39044;&#31639;&#65292;&#20174;&#32780;&#22312;&#25351;&#20196;&#35843;&#25972;&#20219;&#21153;&#20013;&#21462;&#24471;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.08370</link><description>&lt;p&gt;
SMART: &#29992;&#20110;&#25351;&#20196;&#35843;&#25972;&#30340;&#23376;&#27169;&#22359;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SMART: Submodular Data Mixture Strategy for Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08370
&lt;/p&gt;
&lt;p&gt;
SMART&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#24182;&#22312;&#24494;&#35843;&#20013;&#37325;&#26032;&#20998;&#37197;&#39044;&#31639;&#65292;&#20174;&#32780;&#22312;&#25351;&#20196;&#35843;&#25972;&#20219;&#21153;&#20013;&#21462;&#24471;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#28041;&#21450;&#22312;&#19968;&#32452;&#20197;&#25351;&#20196;&#26684;&#24335;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#24179;&#34913;&#19981;&#21516;&#20219;&#21153;&#27604;&#20363;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#25214;&#21040;&#21512;&#36866;&#30340;&#24179;&#34913;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#38500;&#20102;&#25163;&#21160;&#35843;&#25972;&#25110;&#20381;&#36182;&#20174;&#19994;&#32773;&#30340;&#30452;&#35273;&#22806;&#65292;&#23578;&#26080;&#31995;&#32479;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SMART&#65288;Submodular data Mixture strAtegy for instRuction Tuning&#65289;- &#19968;&#31181;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#30340;&#26032;&#39062;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#20998;&#25968;&#26469;&#30830;&#23450;&#28151;&#21512;&#26435;&#37325;&#12290;&#32473;&#23450;&#24494;&#35843;&#39044;&#31639;&#65292;SMART&#37325;&#26032;&#20998;&#37197;&#20219;&#21153;&#38388;&#30340;&#39044;&#31639;&#65292;&#24182;&#20174;&#27599;&#20010;&#20219;&#21153;&#20013;&#36873;&#25321;&#38750;&#20887;&#20313;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SMART&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;&#20363;&#23376;&#27604;&#20363;&#28151;&#21512;&#21644;&#22343;&#31561;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08370v1 Announce Type: cross  Abstract: Instruction Tuning involves finetuning a language model on a collection of instruction-formatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) - a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#38271;&#23614;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#29305;&#24449;&#32479;&#35745;&#20998;&#24320;&#24335;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#26041;&#24335;&#35299;&#20915;&#23614;&#37096;&#31867;&#21035;&#31232;&#30095;&#20998;&#24067;&#23548;&#33268;&#30340;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.08364</link><description>&lt;p&gt;
&#38024;&#23545;&#38271;&#23614;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#29305;&#24449;&#32479;&#35745;&#20998;&#24320;&#24335;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decoupled Federated Learning on Long-Tailed and Non-IID data with Feature Statistics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#38271;&#23614;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#29305;&#24449;&#32479;&#35745;&#20998;&#24320;&#24335;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#26041;&#24335;&#35299;&#20915;&#23614;&#37096;&#31867;&#21035;&#31232;&#30095;&#20998;&#24067;&#23548;&#33268;&#30340;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#22686;&#24378;&#25968;&#25454;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#65292;&#20294;&#22312;&#22788;&#29702;&#38271;&#23614;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#24322;&#26500;&#25968;&#25454;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#24773;&#26223;&#65292;&#21363;&#23614;&#37096;&#31867;&#21035;&#22312;&#23569;&#25968;&#23458;&#25143;&#31471;&#19978;&#31232;&#30095;&#20998;&#24067;&#65292;&#23548;&#33268;&#20351;&#29992;&#36825;&#20123;&#31867;&#21035;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#23458;&#25143;&#31471;&#32858;&#21512;&#36807;&#31243;&#20013;&#34987;&#36873;&#25321;&#30340;&#27010;&#29575;&#36739;&#20302;&#65292;&#20174;&#32780;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#21644;&#27169;&#22411;&#24615;&#33021;&#36739;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#29305;&#24449;&#32479;&#35745;&#30340;&#20004;&#38454;&#27573;&#20998;&#24320;&#24335;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65288;DFL-FS&#65289;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#26381;&#21153;&#22120;&#36890;&#36807;&#33945;&#29256;&#23616;&#37096;&#29305;&#24449;&#32479;&#35745;&#32858;&#31867;&#20272;&#35745;&#23458;&#25143;&#31471;&#30340;&#31867;&#21035;&#35206;&#30422;&#20998;&#24067;&#65292;&#20197;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#21644;&#22686;&#24378;&#29305;&#24449;&#23398;&#20064;&#32780;&#19981;&#27844;&#38706;&#38544;&#31169;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;DFL-FS&#22522;&#20110;&#20840;&#23616;&#29305;&#24449;&#32479;&#35745;&#37319;&#29992;&#32852;&#37030;&#29305;&#24449;&#20877;&#29983;&#65292;&#24182;&#21033;&#29992;&#37325;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08364v1 Announce Type: cross  Abstract: Federated learning is designed to enhance data security and privacy, but faces challenges when dealing with heterogeneous data in long-tailed and non-IID distributions. This paper explores an overlooked scenario where tail classes are sparsely distributed over a few clients, causing the models trained with these classes to have a lower probability of being selected during client aggregation, leading to slower convergence rates and poorer model performance. To address this issue, we propose a two-stage Decoupled Federated learning framework using Feature Statistics (DFL-FS). In the first stage, the server estimates the client's class coverage distributions through masked local feature statistics clustering to select models for aggregation to accelerate convergence and enhance feature learning without privacy leakage. In the second stage, DFL-FS employs federated feature regeneration based on global feature statistics and utilizes resamp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22343;&#22330;&#24494;&#27491;&#21017;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#37319;&#26679;&#22810;&#20010;&#24369;&#32806;&#21512;&#25968;&#25454;&#28857;&#65292;&#22312;&#25511;&#21046;&#29109;&#25439;&#22833;&#30340;&#21516;&#26102;&#22312;&#20284;&#28982;&#25311;&#21512;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.08362</link><description>&lt;p&gt;
&#22343;&#22330;&#24494;&#27491;&#21017;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Mean-Field Microcanonical Gradient Descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08362
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22343;&#22330;&#24494;&#27491;&#21017;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#37319;&#26679;&#22810;&#20010;&#24369;&#32806;&#21512;&#25968;&#25454;&#28857;&#65292;&#22312;&#25511;&#21046;&#29109;&#25439;&#22833;&#30340;&#21516;&#26102;&#22312;&#20284;&#28982;&#25311;&#21512;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#27491;&#21017;&#26799;&#24230;&#19979;&#38477;&#26159;&#19968;&#31181;&#33021;&#37327;&#27169;&#22411;&#30340;&#37319;&#26679;&#36807;&#31243;&#65292;&#21487;&#23454;&#29616;&#39640;&#32500;&#20998;&#24067;&#30340;&#39640;&#25928;&#37319;&#26679;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23558;&#26679;&#26412;&#20174;&#39640;&#29109;&#20998;&#24067;&#65288;&#22914;&#39640;&#26031;&#30333;&#22122;&#22768;&#65289;&#36716;&#36816;&#33267;&#20302;&#33021;&#21306;&#22495;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#27169;&#22411;&#32622;&#20110;&#27491;&#21017;&#21270;&#27969;&#30340;&#26694;&#26550;&#20013;&#65292;&#26174;&#31034;&#23427;&#36890;&#24120;&#20250;&#30001;&#20110;&#22312;&#19979;&#38477;&#36807;&#31243;&#20013;&#22833;&#21435;&#19981;&#24517;&#35201;&#30340;&#29109;&#32780;&#36807;&#24230;&#25311;&#21512;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22343;&#22330;&#24494;&#27491;&#21017;&#26799;&#24230;&#19979;&#38477;&#65292;&#21516;&#26102;&#37319;&#26679;&#22810;&#20010;&#24369;&#32806;&#21512;&#25968;&#25454;&#28857;&#65292;&#20801;&#35768;&#26356;&#22909;&#22320;&#25511;&#21046;&#29109;&#25439;&#22833;&#65292;&#21516;&#26102;&#22312;&#20284;&#28982;&#25311;&#21512;&#26041;&#38754;&#20184;&#20986;&#36739;&#23567;&#20195;&#20215;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#32972;&#26223;&#20013;&#65292;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08362v1 Announce Type: cross  Abstract: Microcanonical gradient descent is a sampling procedure for energy-based models allowing for efficient sampling of distributions in high dimension. It works by transporting samples from a high-entropy distribution, such as Gaussian white noise, to a low-energy region using gradient descent. We put this model in the framework of normalizing flows, showing how it can often overfit by losing an unnecessary amount of entropy in the descent. As a remedy, we propose a mean-field microcanonical gradient descent that samples several weakly coupled data points simultaneously, allowing for better control of the entropy loss while paying little in terms of likelihood fit. We study these models in the context of financial time series, illustrating the improvements on both synthetic and real data.
&lt;/p&gt;</description></item><item><title>&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26088;&#22312;&#33258;&#21160;&#21270;&#25968;&#25454;&#22686;&#24378;&#36807;&#31243;&#65292;&#20026;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.08352</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21450;&#19982;&#20256;&#32479;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24615;&#33021;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Data augmentation with automated machine learning: approaches and performance comparison with classical data augmentation methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08352
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26088;&#22312;&#33258;&#21160;&#21270;&#25968;&#25454;&#22686;&#24378;&#36807;&#31243;&#65292;&#20026;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#34987;&#35748;&#20026;&#26159;&#24120;&#29992;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#30340;&#26368;&#37325;&#35201;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#23427;&#20027;&#35201;&#28041;&#21450;&#24212;&#29992;&#36866;&#24403;&#30340;&#25968;&#25454;&#36716;&#25442;&#25805;&#20316;&#65292;&#20197;&#21019;&#24314;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#26032;&#25968;&#25454;&#26679;&#26412;&#12290;&#23613;&#31649;&#20854;&#26377;&#25928;&#24615;&#65292;&#36825;&#19968;&#36807;&#31243;&#36890;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25163;&#21160;&#21019;&#24314;&#21644;&#27979;&#35797;&#19981;&#21516;&#20505;&#36873;&#22686;&#24378;&#21450;&#20854;&#36229;&#21442;&#25968;&#38656;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#12290;&#33258;&#21160;&#21270;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26088;&#22312;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#12290;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#21407;&#21017;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20110;AutoML&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20351;&#29992;AutoML&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#25805;&#20316;&#12289;&#25968;&#25454;&#38598;&#25104;&#21644;&#25968;&#25454;&#21512;&#25104;&#25216;&#26415;&#12290;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08352v1 Announce Type: cross  Abstract: Data augmentation is arguably the most important regularization technique commonly used to improve generalization performance of machine learning models. It primarily involves the application of appropriate data transformation operations to create new data samples with desired properties. Despite its effectiveness, the process is often challenging because of the time-consuming trial and error procedures for creating and testing different candidate augmentations and their hyperparameters manually. Automated data augmentation methods aim to automate the process. State-of-the-art approaches typically rely on automated machine learning (AutoML) principles. This work presents a comprehensive survey of AutoML-based data augmentation techniques. We discuss various approaches for accomplishing data augmentation with AutoML, including data manipulation, data integration and data synthesis techniques. We present extensive discussion of technique
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#36719;&#32452;&#32455;&#27169;&#25311;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;SMPL&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#24341;&#20837;&#36719;&#32452;&#32455;&#23618;&#21644;&#22806;&#37096;&#21147;&#20316;&#29992;&#30340;&#30452;&#35266;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#23545;&#20154;&#20307;&#24418;&#29366;&#21644;&#36719;&#32452;&#32455;&#36827;&#34892;&#24555;&#36895;&#36924;&#30495;&#27169;&#25311;&#12290;</title><link>https://arxiv.org/abs/2403.08344</link><description>&lt;p&gt;
STMPL&#65306;&#20154;&#20307;&#36719;&#32452;&#32455;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
STMPL: Human Soft-Tissue Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#36719;&#32452;&#32455;&#27169;&#25311;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;SMPL&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#24341;&#20837;&#36719;&#32452;&#32455;&#23618;&#21644;&#22806;&#37096;&#21147;&#20316;&#29992;&#30340;&#30452;&#35266;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#23545;&#20154;&#20307;&#24418;&#29366;&#21644;&#36719;&#32452;&#32455;&#36827;&#34892;&#24555;&#36895;&#36924;&#30495;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65292;&#22914;&#34394;&#25311;&#29616;&#23454;&#21644;&#28216;&#25103;&#20013;&#65292;&#27169;&#25311;&#20154;&#20307;&#36719;&#32452;&#32455;&#22312;&#19982;&#22806;&#37096;&#29289;&#20307;&#20132;&#20114;&#36807;&#31243;&#20013;&#30340;&#21464;&#24418;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#19978;&#65292;&#26377;&#38480;&#20803;&#26041;&#27861;&#65288;FEM&#65289;&#34987;&#29992;&#20110;&#27492;&#30446;&#30340;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#36895;&#24230;&#36739;&#24930;&#19988;&#36164;&#28304;&#23494;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#20307;&#24418;&#29366;&#21644;&#36719;&#32452;&#32455;&#30340;&#32479;&#19968;&#34920;&#31034;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#38750;&#21018;&#20307;&#21464;&#24418;&#27169;&#25311;&#22120;&#12290;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#24555;&#36895;&#27169;&#25311;&#36924;&#30495;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08344v1 Announce Type: cross  Abstract: In various applications, such as virtual reality and gaming, simulating the deformation of soft tissues in the human body during interactions with external objects is essential. Traditionally, Finite Element Methods (FEM) have been employed for this purpose, but they tend to be slow and resource-intensive. In this paper, we propose a unified representation of human body shape and soft tissue with a data-driven simulator of non-rigid deformations. This approach enables rapid simulation of realistic interactions.   Our method builds upon the SMPL model, which generates human body shapes considering rigid transformations. We extend SMPL by incorporating a soft tissue layer and an intuitive representation of external forces applied to the body during object interactions. Specifically, we mapped the 3D body shape and soft tissue and applied external forces to 2D UV maps. Leveraging a UNET architecture designed for 2D data, our approach achi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25972;&#21512;&#21040;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;(TSC)&#31995;&#32479;&#20013;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;TSC&#31995;&#32479;&#22312;&#36866;&#24212;&#19981;&#29087;&#24713;&#22330;&#26223;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;&#65292;&#20351;&#24471;LLMs&#19982;&#19968;&#31995;&#21015;&#24863;&#30693;&#21644;&#20915;&#31574;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#25552;&#21319;TSC&#31995;&#32479;&#23545;&#22478;&#24066;&#20132;&#36890;&#22797;&#26434;&#24615;&#21644;&#21464;&#24322;&#24615;&#30340;&#31649;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08337</link><description>&lt;p&gt;
LLM&#36741;&#21161;&#19979;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#22478;&#24066;&#29615;&#22659;&#20013;&#23454;&#29616;&#20154;&#31867;&#20223;&#29983;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25972;&#21512;&#21040;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;(TSC)&#31995;&#32479;&#20013;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;TSC&#31995;&#32479;&#22312;&#36866;&#24212;&#19981;&#29087;&#24713;&#22330;&#26223;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;&#65292;&#20351;&#24471;LLMs&#19982;&#19968;&#31995;&#21015;&#24863;&#30693;&#21644;&#20915;&#31574;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#25552;&#21319;TSC&#31995;&#32479;&#23545;&#22478;&#24066;&#20132;&#36890;&#22797;&#26434;&#24615;&#21644;&#21464;&#24322;&#24615;&#30340;&#31649;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37117;&#24066;&#22320;&#21306;&#30340;&#20132;&#36890;&#25317;&#22581;&#26159;&#19968;&#20010;&#20855;&#26377;&#28145;&#36828;&#32463;&#27982;&#12289;&#29615;&#22659;&#21644;&#31038;&#20250;&#24433;&#21709;&#30340;&#24040;&#22823;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26377;&#25928;&#30340;&#25317;&#22581;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;(TSC)&#31995;&#32479;&#22312;&#36825;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#22238;&#24212;&#20256;&#32479;TSC&#31995;&#32479;&#22312;&#31649;&#29702;&#22478;&#24066;&#20132;&#36890;&#27969;&#21160;&#30340;&#22797;&#26434;&#24615;&#21644;&#21464;&#24322;&#24615;&#26041;&#38754;&#32463;&#24120;&#34920;&#29616;&#20986;&#30340;&#19981;&#36275;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25972;&#21512;&#21040;TSC&#20013;&#65292;&#21033;&#29992;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;&#65292;&#23558;LLMs&#19982;&#19968;&#22871;&#24863;&#30693;&#21644;&#20915;&#31574;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#26377;&#21161;&#20110;&#25506;&#35752;&#38745;&#24577;&#21644;&#21160;&#24577;&#20132;&#36890;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08337v1 Announce Type: cross  Abstract: Traffic congestion in metropolitan areas presents a formidable challenge with far-reaching economic, environmental, and societal ramifications. Therefore, effective congestion management is imperative, with traffic signal control (TSC) systems being pivotal in this endeavor. Conventional TSC systems, designed upon rule-based algorithms or reinforcement learning (RL), frequently exhibit deficiencies in managing the complexities and variabilities of urban traffic flows, constrained by their limited capacity for adaptation to unfamiliar scenarios. In response to these limitations, this work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties. Specifically, a hybrid framework that augments LLMs with a suite of perception and decision-making tools is proposed, facilitating the interrogation of both the static and dynamic traffic informatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#31232;&#30095;&#21407;&#21017;&#65292;&#24314;&#31435;&#20102;&#20004;&#20010;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#65292;&#20026;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#21644;&#20998;&#27573;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#35774;&#32622;&#20102;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.08335</link><description>&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#27979;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#31232;&#30095;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Sparsity Principle for Partially Observable Causal Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08335
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#31232;&#30095;&#21407;&#21017;&#65292;&#24314;&#31435;&#20102;&#20004;&#20010;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#65292;&#20026;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#21644;&#20998;&#27573;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#35774;&#32622;&#20102;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#20174;&#24863;&#30693;&#25968;&#25454;&#20013;&#35782;&#21035;&#39640;&#23618;&#27425;&#30340;&#22240;&#26524;&#21464;&#37327;&#12290;&#26412;&#25991;&#32771;&#34385;&#37096;&#20998;&#35266;&#27979;&#35774;&#32622;&#65292;&#20854;&#20013;&#27599;&#27425;&#27979;&#37327;&#20165;&#25552;&#20379;&#20851;&#20110;&#28508;&#22312;&#22240;&#26524;&#29366;&#24577;&#23376;&#38598;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20174;&#25968;&#25454;&#38598;&#20013;&#19981;&#37197;&#23545;&#35266;&#23519;&#23398;&#20064;&#65292;&#20854;&#20013;&#23384;&#22312;&#23454;&#20363;&#30456;&#20851;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20026;&#35813;&#35774;&#32622;&#24314;&#31435;&#20004;&#20010;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#65306;&#19968;&#20010;&#26159;&#20851;&#20110;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#30340;&#32467;&#26524;&#65292;&#26080;&#38656;&#23545;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#20570;&#21442;&#25968;&#20551;&#35774;&#65292;&#21478;&#19968;&#20010;&#26159;&#23545;&#20855;&#26377;&#39640;&#26031;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#30340;&#20998;&#27573;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#30340;&#32467;&#26524;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#20272;&#35745;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08335v1 Announce Type: cross  Abstract: Causal representation learning aims at identifying high-level causal variables from perceptual data. Most methods assume that all latent causal variables are captured in the high-dimensional observations. We instead consider a partially observed setting, in which each measurement only provides information about a subset of the underlying causal state. Prior work has studied this setting with multiple domains or views, each depending on a fixed subset of latents. Here, we focus on learning from unpaired observations from a dataset with an instance-dependent partial observability pattern. Our main contribution is to establish two identifiability results for this setting: one for linear mixing functions without parametric assumptions on the underlying causal model, and one for piecewise linear mixing functions with Gaussian latent causal variables. Based on these insights, we propose two methods for estimating the underlying causal variab
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#35757;&#32451;&#22909;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#31227;&#38500;&#33410;&#28857;&#21518;&#30340;&#39044;&#27979;&#21464;&#21270;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.08333</link><description>&lt;p&gt;
&#24555;&#36895;&#25512;&#26029;&#22522;&#20110;&#31227;&#38500;&#30340;&#33410;&#28857;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Fast Inference of Removal-Based Node Influence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08333
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#35757;&#32451;&#22909;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#31227;&#38500;&#33410;&#28857;&#21518;&#30340;&#39044;&#27979;&#21464;&#21270;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#25429;&#33719;&#22270;&#20013;&#20449;&#24687;&#20256;&#25773;&#27169;&#24335;&#12290;&#34429;&#28982;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#30340;&#26032;&#36235;&#21183;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#34913;&#37327;&#35757;&#32451;&#22909;&#30340;GNN&#27169;&#22411;&#22312;&#31227;&#38500;&#33410;&#28857;&#21518;&#30340;&#39044;&#27979;&#21464;&#21270;&#12290;&#19968;&#20010;&#30495;&#23454;&#24212;&#29992;&#26159;&#65292;&#8220;&#22312;&#39044;&#27979;Twitter&#36134;&#25143;&#26497;&#24615;&#30340;&#20219;&#21153;&#20013;&#65292;&#22914;&#26524;&#31227;&#38500;&#29305;&#23450;&#36134;&#25143;&#65292;&#20854;&#20182;&#36134;&#25143;&#30340;&#26497;&#24615;&#20250;&#22914;&#20309;&#25913;&#21464;&#65311;&#8221;&#25105;&#20204;&#23558;GNN&#20316;&#20026;&#19968;&#20010;&#20195;&#29702;&#27169;&#22411;&#65292;&#20854;&#39044;&#27979;&#21487;&#20197;&#27169;&#25311;&#31227;&#38500;&#33410;&#28857;&#24341;&#36215;&#30340;&#33410;&#28857;&#25110;&#36793;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#33719;&#24471;&#27599;&#20010;&#33410;&#28857;&#30340;&#24433;&#21709;&#65292;&#19968;&#31181;&#30452;&#25509;&#30340;&#26041;&#27861;&#26159;&#20132;&#26367;&#31227;&#38500;&#27599;&#20010;&#33410;&#28857;&#65292;&#24182;&#22312;&#20462;&#25913;&#21518;&#30340;&#22270;&#19978;&#24212;&#29992;&#35757;&#32451;&#22909;&#30340;GNN&#12290;&#36825;&#26159;&#21487;&#38752;&#30340;&#20294;&#32791;&#26102;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08333v1 Announce Type: cross  Abstract: Graph neural networks (GNNs) are widely utilized to capture the information spreading patterns in graphs. While remarkable performance has been achieved, there is a new trending topic of evaluating node influence. We propose a new method of evaluating node influence, which measures the prediction change of a trained GNN model caused by removing a node. A real-world application is, "In the task of predicting Twitter accounts' polarity, had a particular account been removed, how would others' polarity change?". We use the GNN as a surrogate model whose prediction could simulate the change of nodes or edges caused by node removal. To obtain the influence for every node, a straightforward way is to alternately remove every node and apply the trained GNN on the modified graph. It is reliable but time-consuming, so we need an efficient method. The related lines of work, such as graph adversarial attack and counterfactual explanation, cannot 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Bayesian&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25628;&#32034;&#21306;&#22495;&#38480;&#21046;&#22312;&#36739;&#20302;&#32500;&#24230;&#65292;&#24182;&#21033;&#29992;&#26412;&#22320;GPR&#27169;&#22411;&#65292;&#22312;&#39640;&#32500;&#24230;&#20013;&#25552;&#39640;&#20102;&#25628;&#32034;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08331</link><description>&lt;p&gt;
Bayesian&#20248;&#21270;&#23558;&#25628;&#32034;&#21306;&#22495;&#38480;&#21046;&#22312;&#36739;&#20302;&#32500;&#24230;&#20013;&#65292;&#21033;&#29992;&#26412;&#22320;GPR
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization that Limits Search Region to Lower Dimensions Utilizing Local GPR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08331
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Bayesian&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25628;&#32034;&#21306;&#22495;&#38480;&#21046;&#22312;&#36739;&#20302;&#32500;&#24230;&#65292;&#24182;&#21033;&#29992;&#26412;&#22320;GPR&#27169;&#22411;&#65292;&#22312;&#39640;&#32500;&#24230;&#20013;&#25552;&#39640;&#20102;&#25628;&#32034;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39046;&#22495;&#65292;&#21253;&#25324;&#35774;&#35745;&#21644;&#25511;&#21046;&#65292;&#37117;&#38656;&#35201;&#20248;&#21270;&#20135;&#21697;&#21644;&#31995;&#32479;&#29305;&#24615;&#12290; &#24403;&#35266;&#27979;&#25104;&#26412;&#39640;&#26102;&#65292;&#36890;&#24120;&#20351;&#29992;Bayesian&#20248;&#21270;&#65288;BO&#65289;&#65292;&#22240;&#20026;BO&#20174;&#29702;&#35770;&#19978;&#20445;&#35777;&#20102;&#36951;&#25022;&#30340;&#19978;&#38480;&#12290; &#28982;&#32780;&#65292;&#38543;&#30528;&#35201;&#20248;&#21270;&#21442;&#25968;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#35745;&#31639;&#25104;&#26412;&#21576;&#25351;&#25968;&#32423;&#22686;&#21152;&#65292;&#23548;&#33268;&#25628;&#32034;&#25928;&#29575;&#38477;&#20302;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25628;&#32034;&#21306;&#22495;&#38480;&#21046;&#22312;&#36739;&#20302;&#32500;&#24230;&#24182;&#21033;&#29992;&#26412;&#22320;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;LGPR&#65289;&#23558;BO&#25193;&#23637;&#21040;&#26356;&#39640;&#32500;&#24230;&#30340;&#26041;&#27861;&#12290; LGPR&#23558;&#20302;&#32500;&#25628;&#32034;&#21306;&#22495;&#35270;&#20026;&#8220;&#26412;&#22320;&#8221;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#37027;&#37324;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290; LGPR&#27169;&#22411;&#26159;&#22312;&#29305;&#23450;&#21306;&#22495;&#30340;&#26412;&#22320;&#25968;&#25454;&#23376;&#38598;&#19978;&#35757;&#32451;&#30340;&#12290; &#36825;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#21644;&#25628;&#32034;&#25928;&#29575;&#65292;&#24182;&#20943;&#23569;&#20102;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20013;&#30697;&#38453;&#27714;&#36870;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290; &#22312;&#35780;&#20272;20D Ackley&#21644;Rosenbrock&#20989;&#25968;&#26102;&#65292;&#25628;&#32034;&#25928;&#29575;&#19982;&#25110;&#39640;&#20110;&#26082;&#23450;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08331v1 Announce Type: new  Abstract: Optimization of product and system characteristics is required in many fields, including design and control. Bayesian optimization (BO) is often used when there are high observing costs, because BO theoretically guarantees an upper bound on regret. However, computational costs increase exponentially with the number of parameters to be optimized, decreasing search efficiency. We propose a BO that limits the search region to lower dimensions and utilizes local Gaussian process regression (LGPR) to scale the BO to higher dimensions. LGPR treats the low-dimensional search region as "local," improving prediction accuracies there. The LGPR model is trained on a local subset of data specific to that region. This improves prediction accuracy and search efficiency and reduces the time complexity of matrix inversion in the Gaussian process regression. In evaluations with 20D Ackley and Rosenbrock functions, search efficiencies are equal to or high
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#28145;&#20837;&#20998;&#26512;&#20102;LLMs&#22312;&#34701;&#21512;&#19978;&#19979;&#25991;&#21644;&#21442;&#25968;&#21270;&#30693;&#35782;&#26102;&#25152;&#38754;&#20020;&#30340;&#30693;&#35782;&#20914;&#31361;&#65292;&#25506;&#35752;&#20102;&#19977;&#31867;&#30693;&#35782;&#20914;&#31361;&#23545;&#20854;&#21487;&#20449;&#24230;&#21644;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;LLMs&#31283;&#20581;&#24615;&#31574;&#30053;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.08319</link><description>&lt;p&gt;
LLMs&#30340;&#30693;&#35782;&#20914;&#31361;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Knowledge Conflicts for LLMs: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08319
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#28145;&#20837;&#20998;&#26512;&#20102;LLMs&#22312;&#34701;&#21512;&#19978;&#19979;&#25991;&#21644;&#21442;&#25968;&#21270;&#30693;&#35782;&#26102;&#25152;&#38754;&#20020;&#30340;&#30693;&#35782;&#20914;&#31361;&#65292;&#25506;&#35752;&#20102;&#19977;&#31867;&#30693;&#35782;&#20914;&#31361;&#23545;&#20854;&#21487;&#20449;&#24230;&#21644;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;LLMs&#31283;&#20581;&#24615;&#31574;&#30053;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#20914;&#31361;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#24403;&#23427;&#20204;&#34701;&#21512;&#19978;&#19979;&#25991;&#21644;&#21442;&#25968;&#21270;&#30693;&#35782;&#26102;&#25152;&#36935;&#21040;&#30340;&#22797;&#26434;&#25361;&#25112;&#12290;&#25105;&#20204;&#20851;&#27880;&#19977;&#31867;&#30693;&#35782;&#20914;&#31361;&#65306;&#19978;&#19979;&#25991;-&#35760;&#24518;&#20914;&#31361;&#12289;&#36328;&#19978;&#19979;&#25991;&#20914;&#31361;&#21644;&#20869;&#37096;&#35760;&#24518;&#20914;&#31361;&#12290;&#36825;&#20123;&#20914;&#31361;&#21487;&#33021;&#20250;&#26174;&#33879;&#24433;&#21709;LLMs&#30340;&#21487;&#20449;&#24230;&#21644;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#22122;&#38899;&#21644;&#38169;&#35823;&#20449;&#24687;&#24456;&#24120;&#35265;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#20914;&#31361;&#36827;&#34892;&#20998;&#31867;&#65292;&#25506;&#35752;&#20854;&#21407;&#22240;&#65292;&#30740;&#31350;LLMs&#22312;&#36825;&#20123;&#20914;&#31361;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#22238;&#39038;&#21487;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#35843;&#26597;&#26088;&#22312;&#20026;&#25913;&#36827;LLMs&#30340;&#31283;&#20581;&#24615;&#31574;&#30053;&#25552;&#20379;&#21551;&#31034;&#65292;&#20174;&#32780;&#25104;&#20026;&#25512;&#21160;&#36825;&#19968;&#19981;&#26029;&#21457;&#23637;&#39046;&#22495;&#30740;&#31350;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08319v1 Announce Type: cross  Abstract: This survey provides an in-depth analysis of knowledge conflicts for large language models (LLMs), highlighting the complex challenges they encounter when blending contextual and parametric knowledge. Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common. By categorizing these conflicts, exploring the causes, examining the behaviors of LLMs under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of LLMs, thereby serving as a valuable resource for advancing research in this evolving area.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;HRLAIF&#26041;&#27861;&#26469;&#25913;&#21892;&#24320;&#25918;&#22495;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#21709;&#24212;&#24110;&#21161;&#24615;&#65292;&#36890;&#36807;&#22686;&#24378;AI&#27880;&#37322;&#21709;&#24212;&#30340;&#20934;&#30830;&#24615;&#26469;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#40065;&#26834;&#24615;</title><link>https://arxiv.org/abs/2403.08309</link><description>&lt;p&gt;
HRLAIF: &#36890;&#36807;AI&#21453;&#39304;&#25913;&#36827;&#24320;&#25918;&#22495;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24110;&#21161;&#24615;&#21644;&#26080;&#23475;&#24615;
&lt;/p&gt;
&lt;p&gt;
HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08309
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;HRLAIF&#26041;&#27861;&#26469;&#25913;&#21892;&#24320;&#25918;&#22495;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#21709;&#24212;&#24110;&#21161;&#24615;&#65292;&#36890;&#36807;&#22686;&#24378;AI&#27880;&#37322;&#21709;&#24212;&#30340;&#20934;&#30830;&#24615;&#26469;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;AI&#21453;&#39304;&#65288;RLAIF&#65289;&#30456;&#27604;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#20855;&#26377;&#26356;&#30701;&#30340;&#27880;&#37322;&#21608;&#26399;&#21644;&#26356;&#20302;&#30340;&#25104;&#26412;&#20248;&#21183;&#65292;&#20351;&#20854;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35757;&#32451;&#30340;&#24555;&#36895;&#31574;&#30053;&#36845;&#20195;&#38454;&#27573;&#38750;&#24120;&#39640;&#25928;&#12290;&#20351;&#29992;ChatGPT&#20316;&#20026;&#26631;&#27880;&#21592;&#65292;&#22312;RLAIF&#35757;&#32451;&#20013;&#20026;&#24320;&#25918;&#22495;&#25552;&#31034;&#25552;&#20379;&#21453;&#39304;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20154;&#31867;&#35780;&#20272;&#32773;&#23545;&#27169;&#22411;&#21709;&#24212;&#30340;&#20559;&#22909;&#32988;&#29575;&#22686;&#21152;&#65292;&#20294;&#35780;&#20272;&#32773;&#30340;&#28385;&#24847;&#24230;&#19979;&#38477;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#28385;&#24847;&#24230;&#19979;&#38477;&#20027;&#35201;&#26159;&#22240;&#20026;&#19968;&#20123;&#21709;&#24212;&#21464;&#24471;&#19981;&#22815;&#26377;&#24110;&#21161;&#65292;&#29305;&#21035;&#26159;&#22312;&#27491;&#30830;&#24615;&#21644;&#30495;&#23454;&#24615;&#26041;&#38754;&#65292;&#31361;&#26174;&#20102;&#22522;&#26412;RLAIF&#30340;&#23454;&#38469;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#20174;AI&#21453;&#39304;&#65288;HRLAIF&#65289;&#12290;&#35813;&#26041;&#27861;&#22686;&#24378;&#20102;AI&#27880;&#37322;&#21709;&#24212;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#27169;&#22411;&#30340;&#24110;&#21161;&#24615;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08309v1 Announce Type: cross  Abstract: Reinforcement Learning from AI Feedback (RLAIF) has the advantages of shorter annotation cycles and lower costs over Reinforcement Learning from Human Feedback (RLHF), making it highly efficient during the rapid strategy iteration periods of large language model (LLM) training. Using ChatGPT as a labeler to provide feedback on open-domain prompts in RLAIF training, we observe an increase in human evaluators' preference win ratio for model responses, but a decrease in evaluators' satisfaction rate. Analysis suggests that the decrease in satisfaction rate is mainly due to some responses becoming less helpful, particularly in terms of correctness and truthfulness, highlighting practical limitations of basic RLAIF. In this paper, we propose Hybrid Reinforcement Learning from AI Feedback (HRLAIF). This method enhances the accuracy of AI annotations for responses, making the model's helpfulness more robust in training process. Additionally, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22768;&#26126;&#24615;&#12289;&#32479;&#19968;API&#30340;Python&#24211;&#65292;&#36890;&#36807;&#31616;&#27905;&#30340;API&#35843;&#29992;&#31616;&#21270;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27969;&#31243;</title><link>https://arxiv.org/abs/2403.08291</link><description>&lt;p&gt;
CleanAgent&#65306;&#22522;&#20110;LLM&#20195;&#29702;&#33258;&#21160;&#21270;&#25968;&#25454;&#26631;&#20934;&#21270;
&lt;/p&gt;
&lt;p&gt;
CleanAgent: Automating Data Standardization with LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08291
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22768;&#26126;&#24615;&#12289;&#32479;&#19968;API&#30340;Python&#24211;&#65292;&#36890;&#36807;&#31616;&#27905;&#30340;API&#35843;&#29992;&#31616;&#21270;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26631;&#20934;&#21270;&#26159;&#25968;&#25454;&#31185;&#23398;&#29983;&#21629;&#21608;&#26399;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#37096;&#20998;&#12290;&#34429;&#28982;&#35832;&#22914;Pandas&#20043;&#31867;&#30340;&#24037;&#20855;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#21151;&#33021;&#65292;&#20294;&#23427;&#20204;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#38656;&#35201;&#23450;&#21046;&#20195;&#30721;&#20197;&#36866;&#24212;&#19981;&#21516;&#21015;&#31867;&#22411;&#30340;&#25163;&#21160;&#25805;&#20316;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#24050;&#32463;&#23637;&#29616;&#20986;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#33258;&#21160;&#21270;&#27492;&#36807;&#31243;&#30340;&#28508;&#21147;&#65292;&#20294;&#20173;&#38656;&#35201;&#19987;&#19994;&#31243;&#24230;&#30340;&#32534;&#31243;&#30693;&#35782;&#21644;&#25345;&#32493;&#20114;&#21160;&#20197;&#36827;&#34892;&#21450;&#26102;&#30340;&#23436;&#21892;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#24819;&#27861;&#26159;&#25552;&#20986;&#19968;&#20010;&#20855;&#26377;&#22768;&#26126;&#24615;&#12289;&#32479;&#19968;API&#30340;Python&#24211;&#65292;&#29992;&#20110;&#26631;&#20934;&#21270;&#21015;&#31867;&#22411;&#65292;&#36890;&#36807;&#31616;&#27905;&#30340;API&#35843;&#29992;&#31616;&#21270;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27969;&#31243;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;Dataprep.Clean&#65292;&#20316;&#20026;Dataprep&#24211;&#30340;&#19968;&#20010;&#32452;&#20214;&#65292;&#36890;&#36807;&#19968;&#34892;&#20195;&#30721;&#23454;&#29616;&#29305;&#23450;&#21015;&#31867;&#22411;&#30340;&#26631;&#20934;&#21270;&#65292;&#26497;&#22823;&#38477;&#20302;&#20102;&#22797;&#26434;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#20171;&#32461;&#20102;CleanAgen
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08291v1 Announce Type: cross  Abstract: Data standardization is a crucial part in data science life cycle. While tools like Pandas offer robust functionalities, their complexity and the manual effort required for customizing code to diverse column types pose significant challenges. Although large language models (LLMs) like ChatGPT have shown promise in automating this process through natural language understanding and code generation, it still demands expert-level programming knowledge and continuous interaction for prompt refinement. To solve these challenges, our key idea is to propose a Python library with declarative, unified APIs for standardizing column types, simplifying the code generation of LLM with concise API calls. We first propose Dataprep.Clean which is written as a component of the Dataprep Library, offers a significant reduction in complexity by enabling the standardization of specific column types with a single line of code. Then we introduce the CleanAgen
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;SNOW-SCA&#65292;&#23454;&#29616;&#20102;&#23545;SNOW-V&#30340;&#20391;&#20449;&#36947;&#25915;&#20987;&#65292;&#25104;&#21151;&#30772;&#35299;&#20102;&#25972;&#20010;256&#20301;&#31192;&#23494;&#23494;&#38053;&#12290;</title><link>https://arxiv.org/abs/2403.08267</link><description>&lt;p&gt;
SNOW-SCA: SNOW-V&#19978;&#30340;ML&#36741;&#21161;&#20391;&#20449;&#36947;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
SNOW-SCA: ML-assisted Side-Channel Attack on SNOW-V
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08267
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;SNOW-SCA&#65292;&#23454;&#29616;&#20102;&#23545;SNOW-V&#30340;&#20391;&#20449;&#36947;&#25915;&#20987;&#65292;&#25104;&#21151;&#30772;&#35299;&#20102;&#25972;&#20010;256&#20301;&#31192;&#23494;&#23494;&#38053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SNOW-SCA&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;5G&#31227;&#21160;&#36890;&#20449;&#23433;&#20840;&#26631;&#20934;&#20505;&#36873;SNOW-V&#30340;&#20391;&#20449;&#36947;&#20998;&#26512;&#65288;SCA&#65289;&#25915;&#20987;&#65292;&#35813;&#26631;&#20934;&#22312;32&#20301;ARM Cortex-M4&#24494;&#25511;&#21046;&#22120;&#19978;&#36816;&#34892;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#36890;&#29992;&#30340;&#24050;&#30693;&#23494;&#38053;&#30456;&#20851;&#24615;&#65288;KKC&#65289;&#20998;&#26512;&#20197;&#35782;&#21035;&#27844;&#28431;&#28857;&#12290;&#25509;&#30528;&#65292;&#36827;&#34892;&#20102;&#30456;&#20851;&#21151;&#32791;&#20998;&#26512;&#65288;CPA&#65289;&#25915;&#20987;&#65292;&#23558;&#25915;&#20987;&#22797;&#26434;&#24615;&#38477;&#20302;&#21040;&#27599;&#20010;&#23494;&#38053;&#23383;&#33410;&#20004;&#27425;&#23494;&#38053;&#29468;&#27979;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#21807;&#19968;&#30830;&#23450;&#27491;&#30830;&#30340;&#31192;&#23494;&#23494;&#38053;&#12290;&#36890;&#36807;&#20351;&#29992;&#24102;&#26377;LDA&#30340;&#37197;&#32622;&#25991;&#20214;&#21270;SCA&#25915;&#20987;&#22312;&#35757;&#32451;$&lt;200$&#36319;&#36394;&#21518;&#23454;&#29616;100%&#20934;&#30830;&#29575;&#65292;&#36825;&#24847;&#21619;&#30528;&#20165;&#20351;&#29992;&#21333;&#20010;&#36319;&#36394;&#21363;&#21487;&#25104;&#21151;&#21457;&#21160;&#25915;&#20987;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#20351;&#29992;\textit{&#32452;&#21512;CPA&#21644;LDA&#25915;&#20987;}&#27169;&#22411;&#65292;&#20351;&#29992;ChipWhisperer&#24179;&#21488;&#25910;&#38598;&lt;50&#36319;&#36394;&#21363;&#21487;&#24674;&#22797;SNOW-V&#30340;&#27491;&#30830;&#31192;&#23494;&#23494;&#38053;&#23383;&#33410;&#12290;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;SCA&#25915;&#20987;&#65292;&#21487;&#20197;&#36880;&#27493;&#24674;&#22797;SNOW-V&#30340;&#25972;&#20010;256&#20301;&#31192;&#23494;&#23494;&#38053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08267v1 Announce Type: cross  Abstract: This paper presents SNOW-SCA, the first power side-channel analysis (SCA) attack of a 5G mobile communication security standard candidate, SNOW-V, running on a 32-bit ARM Cortex-M4 microcontroller. First, we perform a generic known-key correlation (KKC) analysis to identify the leakage points. Next, a correlation power analysis (CPA) attack is performed, which reduces the attack complexity to two key guesses for each key byte. The correct secret key is then uniquely identified utilizing linear discriminant analysis (LDA). The profiled SCA attack with LDA achieves 100% accuracy after training with $&lt;200$ traces, which means the attack succeeds with just a single trace. Overall, using the \textit{combined CPA and LDA attack} model, the correct secret key byte is recovered with &lt;50 traces collected using the ChipWhisperer platform. The entire 256-bit secret key of SNOW-V can be recovered incrementally using the proposed SCA attack. Finall
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20110;&#38543;&#26426;&#25628;&#32034;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#39640;&#36136;&#37327;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#37197;&#32622;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#32570;&#20047;&#21487;&#38752;&#27604;&#36739;&#21644;&#21487;&#37325;&#29616;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08265</link><description>&lt;p&gt;
&#38543;&#26426;&#25628;&#32034;&#20316;&#20026;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#30340;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;
Random Search as a Baseline for Sparse Neural Network Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08265
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20110;&#38543;&#26426;&#25628;&#32034;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#39640;&#36136;&#37327;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#37197;&#32622;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#32570;&#20047;&#21487;&#38752;&#27604;&#36739;&#21644;&#21487;&#37325;&#29616;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#22312;&#21442;&#25968;&#25928;&#29575;&#26356;&#39640;&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#19982;&#23494;&#38598;&#32593;&#32476;&#31867;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36825;&#20419;&#20351;&#35768;&#22810;&#24037;&#20316;&#23398;&#20064;&#12289;&#35825;&#23548;&#25110;&#25628;&#32034;&#24615;&#33021;&#39640;&#30340;&#31232;&#30095;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36136;&#37327;&#25110;&#25928;&#29575;&#30340;&#25552;&#21319;&#20540;&#24471;&#27880;&#24847;&#65292;&#20294;&#26631;&#20934;&#22522;&#32447;&#32570;&#20047;&#65292;&#22240;&#27492;&#22952;&#30861;&#20102;&#26041;&#27861;&#20043;&#38388;&#30340;&#21487;&#38752;&#27604;&#36739;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#21644;&#19968;&#20010;&#31616;&#21333;&#30340;&#38543;&#26426;&#25628;&#32034;&#22522;&#32447;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#33391;&#22909;&#30340;&#31232;&#30095;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#32593;&#32476;&#30340;&#33410;&#28857;&#31354;&#38388;&#19978;&#24212;&#29992;&#38543;&#26426;&#25628;&#32034;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#22312;&#25439;&#22833;&#26223;&#35266;&#20013;&#20301;&#32622;&#26356;&#26377;&#20248;&#21183;&#30340;&#26356;&#22909;&#21021;&#22987;&#21270;&#30340;&#31232;&#30095;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#35760;&#24405;&#20102;&#19981;&#21516;&#31232;&#30095;&#31243;&#24230;&#19979;&#31232;&#30095;&#32593;&#32476;&#30340;&#35757;&#32451;&#21518;&#24615;&#33021;&#65292;&#24182;&#19982;&#23427;&#20204;&#30340;&#23436;&#20840;&#36830;&#25509;&#29238;&#32593;&#32476;&#20197;&#21450;&#38543;&#26426;&#31232;&#30095;&#37197;&#32622;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08265v1 Announce Type: cross  Abstract: Sparse neural networks have shown similar or better generalization performance than their dense counterparts while having higher parameter efficiency. This has motivated a number of works to learn, induce, or search for high performing sparse networks. While reports of quality or efficiency gains are impressive, standard baselines are lacking, therefore hindering having reliable comparability and reproducibility across methods. In this work, we provide an evaluation approach and a naive Random Search baseline method for finding good sparse configurations. We apply Random Search on the node space of an overparameterized network with the goal of finding better initialized sparse sub-networks that are positioned more advantageously in the loss landscape. We record sparse network post-training performances at various levels of sparsity and compare against both their fully connected parent networks and random sparse configurations at the sa
&lt;/p&gt;</description></item><item><title>Skipformer&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#36339;&#36807;&#21644;&#24674;&#22797;&#8221;&#30340;Conformer&#26550;&#26500;&#65292;&#21487;&#20197;&#21160;&#24577;&#12289;&#19981;&#22343;&#21248;&#22320;&#21387;&#32553;&#24207;&#21015;&#36755;&#20837;&#38271;&#24230;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#39044;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08258</link><description>&lt;p&gt;
Skipformer&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#35821;&#38899;&#35782;&#21035;&#30340;&#36339;&#36807;&#21644;&#24674;&#22797;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Skipformer: A Skip-and-Recover Strategy for Efficient Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08258
&lt;/p&gt;
&lt;p&gt;
Skipformer&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#36339;&#36807;&#21644;&#24674;&#22797;&#8221;&#30340;Conformer&#26550;&#26500;&#65292;&#21487;&#20197;&#21160;&#24577;&#12289;&#19981;&#22343;&#21248;&#22320;&#21387;&#32553;&#24207;&#21015;&#36755;&#20837;&#38271;&#24230;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#39044;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Conformer&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#24050;&#25104;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#30340;&#20107;&#23454;&#26631;&#26438;&#27169;&#22411;&#12290;&#36890;&#24120;&#24341;&#20837;&#19968;&#20010;&#31354;&#30333;&#31526;&#21495;&#26469;&#23545;&#40784;CTC&#25110;RNN-T&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#24207;&#21015;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#38271;&#36755;&#20837;&#38271;&#24230;&#20250;&#23545;&#35745;&#31639;&#39044;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#36896;&#25104;&#20108;&#27425;&#36127;&#33655;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Skipformer&#30340;&#8220;&#36339;&#36807;&#21644;&#24674;&#22797;&#8221;Conformer&#26550;&#26500;&#65292;&#20197;&#21160;&#24577;&#21644;&#19981;&#22343;&#21248;&#22320;&#21387;&#32553;&#24207;&#21015;&#36755;&#20837;&#38271;&#24230;&#12290;Skipformer&#20351;&#29992;&#20013;&#38388;CTC&#36755;&#20986;&#20316;&#20026;&#26631;&#20934;&#23558;&#24103;&#20998;&#20026;&#19977;&#32452;&#65306;&#20851;&#38190;&#12289;&#36339;&#36807;&#21644;&#24573;&#30053;&#12290;&#20851;&#38190;&#32452;&#39304;&#36865;&#21040;&#19979;&#19968;&#20010;Conformer&#22359;&#65292;&#20854;&#36755;&#20986;&#19982;&#36339;&#36807;&#32452;&#36890;&#36807;&#21407;&#22987;&#26102;&#38388;&#39034;&#24207;&#32852;&#25509;&#20316;&#20026;&#26368;&#32456;&#32534;&#30721;&#22120;&#36755;&#20986;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;Aishell-1&#19978;&#23558;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#20943;&#23569;&#20102;31&#20493;&#65292;&#22312;Librispeech&#35821;&#26009;&#24211;&#19978;&#20943;&#23569;&#20102;22&#20493;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#21487;&#23454;&#29616;&#26356;&#22909;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08258v1 Announce Type: new  Abstract: Conformer-based attention models have become the de facto backbone model for Automatic Speech Recognition tasks. A blank symbol is usually introduced to align the input and output sequences for CTC or RNN-T models. Unfortunately, the long input length overloads computational budget and memory consumption quadratically by attention mechanism. In this work, we propose a "Skip-and-Recover" Conformer architecture, named Skipformer, to squeeze sequence input length dynamically and inhomogeneously. Skipformer uses an intermediate CTC output as criteria to split frames into three groups: crucial, skipping and ignoring. The crucial group feeds into next conformer blocks and its output joint with skipping group by original temporal order as the final encoder output. Experiments show that our model reduces the input sequence length by 31 times on Aishell-1 and 22 times on Librispeech corpus. Meanwhile, the model can achieve better recognition accu
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#28041;&#21450;&#23545;&#20010;&#20154;&#25968;&#23383;&#25968;&#25454;&#30340;&#21024;&#38500;&#21644;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#36843;&#20999;&#38656;&#35201;&#20840;&#38754;&#35843;&#26597;&#26469;&#25429;&#25417;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.08254</link><description>&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#65306;&#20998;&#31867;&#12289;&#24230;&#37327;&#12289;&#24212;&#29992;&#12289;&#25361;&#25112;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning: Taxonomy, Metrics, Applications, Challenges, and Prospects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08254
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#28041;&#21450;&#23545;&#20010;&#20154;&#25968;&#23383;&#25968;&#25454;&#30340;&#21024;&#38500;&#21644;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#36843;&#20999;&#38656;&#35201;&#20840;&#38754;&#35843;&#26597;&#26469;&#25429;&#25417;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20154;&#25968;&#23383;&#25968;&#25454;&#26159;&#19968;&#39033;&#20851;&#38190;&#36164;&#20135;&#65292;&#21508;&#22269;&#25919;&#24220;&#24050;&#32463;&#23454;&#26045;&#27861;&#24459;&#21644;&#27861;&#35268;&#26469;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#25968;&#25454;&#29992;&#25143;&#24050;&#34987;&#36171;&#20104;&#20102;&#36951;&#24536;&#20854;&#25968;&#25454;&#30340;&#26435;&#21033;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#36807;&#31243;&#20013;&#65292;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#35201;&#27714;&#27169;&#22411;&#25552;&#20379;&#32773;&#22312;&#29992;&#25143;&#35831;&#27714;&#26102;&#21024;&#38500;&#29992;&#25143;&#25968;&#25454;&#65292;&#21450;&#20854;&#23545;ML&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#24433;&#21709;&#12290;&#26426;&#22120;&#36951;&#24536;&#24212;&#36816;&#32780;&#29983;&#65292;&#21463;&#21040;&#20102;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#26085;&#30410;&#20851;&#27880;&#12290;&#23613;&#31649;&#35813;&#39046;&#22495;&#21457;&#23637;&#36805;&#36895;&#65292;&#20294;&#32570;&#20047;&#20840;&#38754;&#35843;&#26597;&#20197;&#25429;&#25417;&#26368;&#26032;&#36827;&#23637;&#12290;&#35748;&#35782;&#21040;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#25506;&#32034;&#65292;&#20197;&#25551;&#32472;&#26426;&#22120;&#36951;&#24536;&#30340;&#26684;&#23616;&#65292;&#21253;&#25324;&#22312;&#38598;&#20013;&#21644;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#36951;&#24536;&#31639;&#27861;&#30340;&#65288;&#32454;&#31890;&#24230;&#65289;&#20998;&#31867;&#12289;&#23545;&#36817;&#20284;&#36951;&#24536;&#30340;&#35752;&#35770;&#12289;&#39564;&#35777;&#21644;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#19979;&#30340;&#36951;&#24536;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08254v1 Announce Type: new  Abstract: Personal digital data is a critical asset, and governments worldwide have enforced laws and regulations to protect data privacy. Data users have been endowed with the right to be forgotten of their data. In the course of machine learning (ML), the forgotten right requires a model provider to delete user data and its subsequent impact on ML models upon user requests. Machine unlearning emerges to address this, which has garnered ever-increasing attention from both industry and academia. While the area has developed rapidly, there is a lack of comprehensive surveys to capture the latest advancements. Recognizing this shortage, we conduct an extensive exploration to map the landscape of machine unlearning including the (fine-grained) taxonomy of unlearning algorithms under centralized and distributed settings, debate on approximate unlearning, verification and evaluation metrics, challenges and solutions for unlearning under different appli
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25512;&#33616;&#30340;&#36731;&#37327;&#32423;&#31526;&#21495;&#22270;&#21367;&#31215;&#32593;&#32476;LSGRec&#65292;&#37319;&#29992;&#32479;&#19968;&#24314;&#27169;&#26041;&#27861;&#21516;&#26102;&#23545;&#39640;&#38454;&#29992;&#25143;&#30340;&#27491;&#36127;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;</title><link>https://arxiv.org/abs/2403.08246</link><description>&lt;p&gt;
&#38754;&#21521;&#27491;&#36127;&#20559;&#22909;&#30340;&#32479;&#19968;&#24314;&#27169;&#30340;&#31526;&#21495;&#24863;&#30693;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Towards Unified Modeling for Positive and Negative Preferences in Sign-Aware Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08246
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25512;&#33616;&#30340;&#36731;&#37327;&#32423;&#31526;&#21495;&#22270;&#21367;&#31215;&#32593;&#32476;LSGRec&#65292;&#37319;&#29992;&#32479;&#19968;&#24314;&#27169;&#26041;&#27861;&#21516;&#26102;&#23545;&#39640;&#38454;&#29992;&#25143;&#30340;&#27491;&#36127;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31526;&#21495;&#24863;&#30693;&#22270;&#25512;&#33616;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#23558;&#20174;&#29992;&#25143;&#19982;&#39033;&#30446;&#20043;&#38388;&#30340;&#27491;&#36127;&#20132;&#20114;&#65288;&#21363;&#65292;&#22270;&#20013;&#30340;&#38142;&#25509;&#65289;&#20013;&#23398;&#20064;&#29992;&#25143;&#30340;&#36127;&#20559;&#22909;&#65292;&#38500;&#20102;&#27491;&#20559;&#22909;&#12290;&#20026;&#20102;&#36866;&#24212;&#36127;&#38142;&#25509;&#21644;&#27491;&#38142;&#25509;&#30340;&#19981;&#21516;&#35821;&#20041;&#65292;&#29616;&#26377;&#20316;&#21697;&#21033;&#29992;&#20004;&#20010;&#29420;&#31435;&#30340;&#32534;&#30721;&#22120;&#20998;&#21035;&#24314;&#27169;&#29992;&#25143;&#30340;&#27491;&#36127;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#20174;&#30001;&#22810;&#20010;&#24102;&#26377;&#19981;&#21516;&#31526;&#21495;&#30340;&#38142;&#25509;&#24418;&#25104;&#30340;&#39640;&#38454;&#24322;&#26500;&#20132;&#20114;&#20013;&#23398;&#20064;&#36127;&#20559;&#22909;&#65292;&#23548;&#33268;&#36127;&#29992;&#25143;&#20559;&#22909;&#19981;&#20934;&#30830;&#21644;&#19981;&#23436;&#25972;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#26840;&#25163;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#25512;&#33616;&#30340;&#36731;&#37327;&#32423;&#31526;&#21495;&#22270;&#21367;&#31215;&#32593;&#32476;LSGRec&#65292;&#37319;&#29992;&#32479;&#19968;&#24314;&#27169;&#26041;&#27861;&#21516;&#26102;&#23545;&#39640;&#38454;&#29992;&#25143;&#30340;&#27491;&#36127;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08246v1 Announce Type: cross  Abstract: Recently, sign-aware graph recommendation has drawn much attention as it will learn users' negative preferences besides positive ones from both positive and negative interactions (i.e., links in a graph) with items. To accommodate the different semantics of negative and positive links, existing works utilize two independent encoders to model users' positive and negative preferences, respectively. However, these approaches cannot learn the negative preferences from high-order heterogeneous interactions between users and items formed by multiple links with different signs, resulting in inaccurate and incomplete negative user preferences. To cope with these intractable issues, we propose a novel \textbf{L}ight \textbf{S}igned \textbf{G}raph Convolution Network specifically for \textbf{Rec}ommendation (\textbf{LSGRec}), which adopts a unified modeling approach to simultaneously model high-order users' positive and negative preferences on a
&lt;/p&gt;</description></item><item><title>ScatterMoE&#26159;&#19968;&#31181;&#22312;GPU&#19978;&#23454;&#29616;&#30340;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#36991;&#20813;&#22635;&#20805;&#21644;&#36807;&#22810;&#22797;&#21046;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#25512;&#29702;&#21644;&#35757;&#32451;&#36895;&#24230;&#65292;&#24182;&#20943;&#23569;&#20102;&#20869;&#23384;&#21344;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.08245</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#30340;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Scattered Mixture-of-Experts Implementation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08245
&lt;/p&gt;
&lt;p&gt;
ScatterMoE&#26159;&#19968;&#31181;&#22312;GPU&#19978;&#23454;&#29616;&#30340;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#36991;&#20813;&#22635;&#20805;&#21644;&#36807;&#22810;&#22797;&#21046;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#25512;&#29702;&#21644;&#35757;&#32451;&#36895;&#24230;&#65292;&#24182;&#20943;&#23569;&#20102;&#20869;&#23384;&#21344;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ScatterMoE&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;GPU&#19978;&#23454;&#29616;&#30340;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;SMoE&#65289;&#12290;ScatterMoE&#22312;&#29616;&#26377;&#23454;&#29616;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#65292;&#20811;&#26381;&#20102;&#19968;&#20123;&#38480;&#21046;&#20197;&#25552;&#39640;&#25512;&#29702;&#21644;&#35757;&#32451;&#36895;&#24230;&#65292;&#24182;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#12290;&#35813;&#23454;&#29616;&#36890;&#36807;&#36991;&#20813;&#22635;&#20805;&#21644;&#36807;&#22810;&#22797;&#21046;&#36755;&#20837;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ParallelLinear&#65292;&#36825;&#26159;&#25105;&#20204;&#29992;&#26469;&#26500;&#24314;&#23454;&#29616;&#30340;&#20027;&#35201;&#32452;&#20214;&#65292;&#20197;&#21450;&#29992;&#20110;&#21152;&#36895;&#25805;&#20316;&#30340;&#21508;&#31181;&#20869;&#26680;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#23454;&#29616;&#36827;&#34892;&#20102;&#19982;Megablocks&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#23427;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#21534;&#21520;&#37327;&#21644;&#26356;&#20302;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;ParallelLinear&#22914;&#20309;&#36890;&#36807;&#23637;&#31034;Mixture of Attention&#30340;&#23454;&#29616;&#26469;&#25193;&#23637;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08245v1 Announce Type: new  Abstract: We present ScatterMoE, an implementation of Sparse Mixture-of-Experts (SMoE) on GPUs. ScatterMoE builds upon existing implementations, and overcoming some of the limitations to improve inference and training speed, and memory footprint. This implementation achieves this by avoiding padding and making excessive copies of the input.   We introduce ParallelLinear, the main component we use to build our implementation and the various kernels used to speed up the operation. We benchmark our implementation against Megablocks, and show that it enables a higher throughput and lower memory footprint. We also show how ParallelLinear enables extension of the Mixture-of-Experts concept by demonstrating with an implementation of Mixture of Attention.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#21475;&#35821;&#21033;&#29992;&#39044;&#35757;&#32451;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#28921;&#39274;&#26426;&#22120;&#20154;&#36830;&#32493;&#39135;&#29289;&#29366;&#24577;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#36830;&#32493;&#25429;&#25417;&#39135;&#29289;&#29366;&#24577;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.08239</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21644;&#40657;&#30418;&#20248;&#21270;&#30340;&#28921;&#39274;&#26426;&#22120;&#20154;&#36830;&#32493;&#29289;&#20307;&#29366;&#24577;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Continuous Object State Recognition for Cooking Robots Using Pre-Trained Vision-Language Models and Black-box Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08239
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#21475;&#35821;&#21033;&#29992;&#39044;&#35757;&#32451;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#28921;&#39274;&#26426;&#22120;&#20154;&#36830;&#32493;&#39135;&#29289;&#29366;&#24577;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#36830;&#32493;&#25429;&#25417;&#39135;&#29289;&#29366;&#24577;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23545;&#29615;&#22659;&#21644;&#29289;&#20307;&#29366;&#24577;&#30340;&#35782;&#21035;&#36890;&#24120;&#22522;&#20110;&#23558;&#24403;&#21069;&#29366;&#24577;&#35270;&#20026;&#20998;&#31867;&#38382;&#39064;&#12290;&#22312;&#28921;&#39274;&#20013;&#65292;&#39135;&#29289;&#30340;&#29366;&#24577;&#21464;&#21270;&#26159;&#36830;&#32493;&#21457;&#29983;&#30340;&#65292;&#38656;&#35201;&#19981;&#20165;&#22312;&#26576;&#20010;&#26102;&#38388;&#28857;&#25429;&#33719;&#65292;&#36824;&#35201;&#22312;&#25972;&#20010;&#26102;&#38388;&#27573;&#20869;&#25345;&#32493;&#36827;&#34892;&#25429;&#33719;&#12290;&#27492;&#22806;&#65292;&#39135;&#29289;&#30340;&#29366;&#24577;&#21464;&#21270;&#22797;&#26434;&#65292;&#19981;&#33021;&#36890;&#36807;&#25163;&#21160;&#32534;&#31243;&#36731;&#26131;&#25551;&#36848;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21475;&#35821;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;&#28921;&#39274;&#26426;&#22120;&#20154;&#36830;&#32493;&#39135;&#29289;&#29366;&#24577;&#21464;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#33021;&#22815;&#25345;&#32493;&#35745;&#31639;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30456;&#20284;&#24230;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#28921;&#39274;&#36807;&#31243;&#20013;&#25429;&#25417;&#39135;&#29289;&#30340;&#29366;&#24577;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08239v1 Announce Type: cross  Abstract: The state recognition of the environment and objects by robots is generally based on the judgement of the current state as a classification problem. On the other hand, state changes of food in cooking happen continuously and need to be captured not only at a certain time point but also continuously over time. In addition, the state changes of food are complex and cannot be easily described by manual programming. Therefore, we propose a method to recognize the continuous state changes of food for cooking robots through the spoken language using pre-trained large-scale vision-language models. By using models that can compute the similarity between images and texts continuously over time, we can capture the state changes of food while cooking. We also show that by adjusting the weighting of each text prompt based on fitting the similarity changes to a sigmoid function and then performing black-box optimization, more accurate and robust co
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#32771;&#34385;&#20102;&#22312;&#26082;&#26377;&#30495;&#23454;&#19987;&#23478;&#21448;&#26377;&#23545;&#25239;&#24615;&#19987;&#23478;&#30340;&#24773;&#20917;&#19979;&#30340;&#20108;&#20803;&#20915;&#31574;&#32858;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#35774;&#35745;&#40065;&#26834;&#32858;&#21512;&#22120;&#20197;&#26368;&#23567;&#21270;&#36951;&#25022;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#30495;&#23454;&#19987;&#23478;&#26159;&#23545;&#31216;&#30340;&#19988;&#23545;&#25239;&#24615;&#19987;&#23478;&#19981;&#22826;&#22810;&#26102;&#65292;&#25130;&#23614;&#22343;&#20540;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.08222</link><description>&lt;p&gt;
&#20855;&#26377;&#23545;&#25239;&#24615;&#19987;&#23478;&#30340;&#40065;&#26834;&#20915;&#31574;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Robust Decision Aggregation with Adversarial Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08222
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#32771;&#34385;&#20102;&#22312;&#26082;&#26377;&#30495;&#23454;&#19987;&#23478;&#21448;&#26377;&#23545;&#25239;&#24615;&#19987;&#23478;&#30340;&#24773;&#20917;&#19979;&#30340;&#20108;&#20803;&#20915;&#31574;&#32858;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#35774;&#35745;&#40065;&#26834;&#32858;&#21512;&#22120;&#20197;&#26368;&#23567;&#21270;&#36951;&#25022;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#30495;&#23454;&#19987;&#23478;&#26159;&#23545;&#31216;&#30340;&#19988;&#23545;&#25239;&#24615;&#19987;&#23478;&#19981;&#22826;&#22810;&#26102;&#65292;&#25130;&#23614;&#22343;&#20540;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#26082;&#26377;&#30495;&#23454;&#19987;&#23478;&#21448;&#26377;&#23545;&#25239;&#24615;&#19987;&#23478;&#30340;&#24773;&#20917;&#19979;&#30340;&#20108;&#20803;&#20915;&#31574;&#32858;&#21512;&#38382;&#39064;&#12290;&#30495;&#23454;&#19987;&#23478;&#23558;&#20250;&#22914;&#23454;&#25253;&#21578;&#20182;&#20204;&#30340;&#31169;&#20154;&#20449;&#21495;&#65292;&#24182;&#33719;&#24471;&#36866;&#24403;&#30340;&#28608;&#21169;&#65292;&#32780;&#23545;&#25239;&#24615;&#19987;&#23478;&#21487;&#20197;&#20219;&#24847;&#25253;&#21578;&#12290;&#20915;&#31574;&#32773;&#38656;&#35201;&#35774;&#35745;&#19968;&#20010;&#40065;&#26834;&#30340;&#32858;&#21512;&#22120;&#65292;&#26681;&#25454;&#19987;&#23478;&#30340;&#25253;&#21578;&#26469;&#39044;&#27979;&#19990;&#30028;&#30340;&#30495;&#23454;&#29366;&#24577;&#12290;&#20915;&#31574;&#32773;&#19981;&#20102;&#35299;&#20855;&#20307;&#30340;&#20449;&#24687;&#32467;&#26500;&#65292;&#21363;&#20449;&#21495;&#12289;&#29366;&#24577;&#20197;&#21450;&#23545;&#25239;&#24615;&#19987;&#23478;&#30340;&#31574;&#30053;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;&#25105;&#20204;&#24076;&#26395;&#25214;&#21040;&#22312;&#26368;&#22351;&#20449;&#24687;&#32467;&#26500;&#19979;&#26368;&#23567;&#21270;&#36951;&#25022;&#30340;&#26368;&#20248;&#32858;&#21512;&#22120;&#12290;&#36951;&#25022;&#34987;&#23450;&#20041;&#20026;&#32858;&#21512;&#22120;&#21644;&#19968;&#20010;&#22522;&#20934;&#20043;&#38388;&#30340;&#26399;&#26395;&#25439;&#22833;&#24046;&#65292;&#35813;&#22522;&#20934;&#26681;&#25454;&#32852;&#21512;&#20998;&#24067;&#21644;&#30495;&#23454;&#19987;&#23478;&#30340;&#25253;&#21578;&#20570;&#20986;&#26368;&#20248;&#20915;&#31574;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#30495;&#23454;&#19987;&#23478;&#26159;&#23545;&#31216;&#30340;&#19988;&#23545;&#25239;&#24615;&#19987;&#23478;&#19981;&#22826;&#22810;&#26102;&#65292;&#25130;&#23614;&#22343;&#20540;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08222v1 Announce Type: cross  Abstract: We consider a binary decision aggregation problem in the presence of both truthful and adversarial experts. The truthful experts will report their private signals truthfully with proper incentive, while the adversarial experts can report arbitrarily. The decision maker needs to design a robust aggregator to forecast the true state of the world based on the reports of experts. The decision maker does not know the specific information structure, which is a joint distribution of signals, states, and strategies of adversarial experts. We want to find the optimal aggregator minimizing regret under the worst information structure. The regret is defined by the difference in expected loss between the aggregator and a benchmark who makes the optimal decision given the joint distribution and reports of truthful experts.   We prove that when the truthful experts are symmetric and adversarial experts are not too numerous, the truncated mean is opt
&lt;/p&gt;</description></item><item><title>&#36816;&#29992;&#23548;&#25968;&#20449;&#24687;&#30340;&#31070;&#32463;&#31639;&#23376;&#21152;&#36895;&#20102;&#20960;&#20309;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#26174;&#33879;&#21152;&#24555;&#20102;&#35299;&#20915;&#38750;&#32447;&#24615;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#30340;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.08220</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#30340;&#39640;&#25928;&#20960;&#20309;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65306;&#21033;&#29992;&#23548;&#25968;&#20449;&#24687;&#30340;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Efficient geometric Markov chain Monte Carlo for nonlinear Bayesian inversion enabled by derivative-informed neural operators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08220
&lt;/p&gt;
&lt;p&gt;
&#36816;&#29992;&#23548;&#25968;&#20449;&#24687;&#30340;&#31070;&#32463;&#31639;&#23376;&#21152;&#36895;&#20102;&#20960;&#20309;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#26174;&#33879;&#21152;&#24555;&#20102;&#35299;&#20915;&#38750;&#32447;&#24615;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36816;&#31639;&#23398;&#20064;&#26041;&#27861;&#26469;&#21152;&#36895;&#20960;&#20309;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#20197;&#35299;&#20915;&#26080;&#38480;&#32500;&#38750;&#32447;&#24615;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#12290;&#34429;&#28982;&#20960;&#20309;MCMC&#37319;&#29992;&#36866;&#24212;&#21518;&#39564;&#23616;&#37096;&#20960;&#20309;&#30340;&#39640;&#36136;&#37327;&#25552;&#35758;&#65292;&#20294;&#22312;&#21442;&#25968;&#21040;&#21487;&#35266;&#27979;&#65288;PtO&#65289;&#26144;&#23556;&#36890;&#36807;&#26114;&#36149;&#30340;&#27169;&#22411;&#27169;&#25311;&#23450;&#20041;&#26102;&#65292;&#38656;&#35201;&#35745;&#31639;&#23545;&#25968;&#20284;&#28982;&#30340;&#23616;&#37096;&#26799;&#24230;&#21644;Hessian&#20449;&#24687;&#65292;&#36896;&#25104;&#39640;&#25104;&#26412;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#30001;PtO&#26144;&#23556;&#30340;&#31070;&#32463;&#31639;&#23376;&#26367;&#20195;&#39537;&#21160;&#30340;&#24310;&#36831;&#25509;&#21463;&#20960;&#20309;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#20854;&#20013;&#25552;&#35758;&#34987;&#35774;&#35745;&#20026;&#21033;&#29992;&#23545;&#25968;&#20284;&#28982;&#21644;&#20854;&#26799;&#24230;&#21644;Hessian&#30340;&#24555;&#36895;&#26367;&#20195;&#20272;&#35745;&#12290;&#20026;&#20102;&#23454;&#29616;&#26174;&#33879;&#21152;&#36895;&#65292;&#26367;&#20195;&#21697;&#38656;&#35201;&#20934;&#30830;&#39044;&#27979;&#21487;&#35266;&#27979;&#21450;&#20854;&#21442;&#25968;&#23548;&#25968;&#65288;&#21487;&#35266;&#27979;&#19982;&#21442;&#25968;&#20043;&#38388;&#30340;&#23548;&#25968;&#65289;&#12290;&#36890;&#36807;&#20256;&#32479;&#30340;&#26041;&#27861;&#23545;&#36825;&#26679;&#30340;&#26367;&#20195;&#21697;&#36827;&#34892;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08220v1 Announce Type: cross  Abstract: We propose an operator learning approach to accelerate geometric Markov chain Monte Carlo (MCMC) for solving infinite-dimensional nonlinear Bayesian inverse problems. While geometric MCMC employs high-quality proposals that adapt to posterior local geometry, it requires computing local gradient and Hessian information of the log-likelihood, incurring a high cost when the parameter-to-observable (PtO) map is defined through expensive model simulations. We consider a delayed-acceptance geometric MCMC method driven by a neural operator surrogate of the PtO map, where the proposal is designed to exploit fast surrogate approximations of the log-likelihood and, simultaneously, its gradient and Hessian. To achieve a substantial speedup, the surrogate needs to be accurate in predicting both the observable and its parametric derivative (the derivative of the observable with respect to the parameter). Training such a surrogate via conventional o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#25506;&#35752;&#20102;BERT&#27169;&#22411;&#30340;&#26550;&#26500;&#12289;&#29305;&#24615;&#20197;&#21450;&#20248;&#21270;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;BERT&#27169;&#22411;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#34920;&#29616;&#20986;&#30340;&#31283;&#20581;&#24615;&#33021;&#21644;&#25552;&#21319;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08217</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;BERT&#27169;&#22411;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Research on the Application of Deep Learning-based BERT Model in Sentiment Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#25506;&#35752;&#20102;BERT&#27169;&#22411;&#30340;&#26550;&#26500;&#12289;&#29305;&#24615;&#20197;&#21450;&#20248;&#21270;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;BERT&#27169;&#22411;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#34920;&#29616;&#20986;&#30340;&#31283;&#20581;&#24615;&#33021;&#21644;&#25552;&#21319;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#19987;&#27880;&#20110;BERT&#27169;&#22411;&#12290;&#25991;&#31456;&#39318;&#20808;&#20171;&#32461;&#20102;&#24773;&#24863;&#20998;&#26512;&#30340;&#22522;&#26412;&#27010;&#24565;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35813;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#38543;&#21518;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;BERT&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#29305;&#24615;&#12290;&#36890;&#36807;&#35814;&#32454;&#35299;&#37322;&#65292;&#38416;&#26126;&#20102;BERT&#27169;&#22411;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#25928;&#26524;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#24182;&#24471;&#21040;&#23454;&#39564;&#35777;&#23454;&#25903;&#25345;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BERT&#27169;&#22411;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#22312;&#24494;&#35843;&#21518;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;&#26368;&#21518;&#65292;&#25991;&#31456;&#24635;&#32467;&#20102;BERT&#27169;&#22411;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08217v1 Announce Type: new  Abstract: This paper explores the application of deep learning techniques, particularly focusing on BERT models, in sentiment analysis. It begins by introducing the fundamental concept of sentiment analysis and how deep learning methods are utilized in this domain. Subsequently, it delves into the architecture and characteristics of BERT models. Through detailed explanation, it elucidates the application effects and optimization strategies of BERT models in sentiment analysis, supported by experimental validation. The experimental findings indicate that BERT models exhibit robust performance in sentiment analysis tasks, with notable enhancements post fine-tuning. Lastly, the paper concludes by summarizing the potential applications of BERT models in sentiment analysis and suggests directions for future research and practical implementations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PaddingFlow&#30340;&#21435;&#37327;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#22635;&#20805;&#32500;&#24230;&#22122;&#22768;&#25913;&#36827;&#20102;&#27491;&#35268;&#21270;&#27969;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#27969;&#30340;&#27169;&#22411;&#22312;&#27969;&#24418;&#21644;&#31163;&#25955;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08216</link><description>&lt;p&gt;
PaddingFlow&#65306;&#21033;&#29992;&#22635;&#20805;&#32500;&#24230;&#22122;&#22768;&#25913;&#36827;&#24402;&#19968;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
PaddingFlow: Improving Normalizing Flows with Padding-Dimensional Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08216
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PaddingFlow&#30340;&#21435;&#37327;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#22635;&#20805;&#32500;&#24230;&#22122;&#22768;&#25913;&#36827;&#20102;&#27491;&#35268;&#21270;&#27969;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#27969;&#30340;&#27169;&#22411;&#22312;&#27969;&#24418;&#21644;&#31163;&#25955;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#35268;&#21270;&#27969;&#26159;&#19968;&#31181;&#20855;&#26377;&#39640;&#25928;&#37319;&#26679;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#27969;&#30340;&#27169;&#22411;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65292;&#21363;&#27969;&#24418;&#21644;&#31163;&#25955;&#25968;&#25454;&#12290; &#22914;&#26524;&#30446;&#26631;&#20998;&#24067;&#26159;&#19968;&#20010;&#27969;&#24418;&#65292;&#20063;&#23601;&#26159;&#35828;&#28508;&#22312;&#30446;&#26631;&#20998;&#24067;&#30340;&#32500;&#24230;&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#32500;&#24230;&#19981;&#21305;&#37197;&#65292;&#22522;&#20110;&#27969;&#30340;&#27169;&#22411;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#12290;&#31163;&#25955;&#25968;&#25454;&#20250;&#23548;&#33268;&#22522;&#20110;&#27969;&#30340;&#27169;&#22411;&#22349;&#32553;&#20026;&#36864;&#21270;&#30340;&#28857;&#36136;&#37327;&#28151;&#21512;&#12290; &#20026;&#20102;&#35268;&#36991;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;PaddingFlow&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21435;&#37327;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#22635;&#20805;&#32500;&#24230;&#22122;&#22768;&#25913;&#36827;&#20102;&#27491;&#35268;&#21270;&#27969;&#12290;PaddingFlow&#26131;&#20110;&#23454;&#29616;&#12289;&#35745;&#31639;&#24265;&#20215;&#12289;&#24191;&#27867;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#29983;&#25104;&#26080;&#20559;&#20272;&#35745;&#30340;&#25968;&#25454;&#26679;&#26412;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20811;&#26381;&#29616;&#26377;&#21435;&#37327;&#21270;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#32780;&#36825;&#20123;&#26041;&#27861;&#24517;&#39035;&#25913;&#21464;&#25968;&#25454;&#20998;&#24067;&#65292;&#21487;&#33021;&#20250;&#38477;&#20302;&#24615;&#33021;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08216v1 Announce Type: new  Abstract: Normalizing flow is a generative modeling approach with efficient sampling. However, Flow-based models suffer two issues, which are manifold and discrete data. If the target distribution is a manifold, which means the dimension of the latent target distribution and the dimension of the data distribution are unmatched, flow-based models might perform badly. Discrete data makes flow-based models collapse into a degenerate mixture of point masses. In this paper, to sidestep such two issues we propose PaddingFlow, a novel dequantization method, which improves normalizing flows with padding-dimensional noise. PaddingFlow is easy to implement, computationally cheap, widely suitable for various tasks, and generates samples that are unbiased estimations of the data. Especially, our method can overcome the limitation of existing dequantization methods that have to change the data distribution, which might degrade performance. We validate our meth
&lt;/p&gt;</description></item><item><title>&#23558;&#21452;&#32534;&#30721;&#22120;&#25945;&#24072;&#27169;&#22411;&#33719;&#24471;&#30340;&#31354;&#38388;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#38544;&#24335;&#27880;&#20837;&#21333;&#32534;&#30721;&#22120;&#23398;&#29983;&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#30340;logit&#33976;&#39311;&#21644;&#29305;&#24449;&#33976;&#39311;&#26041;&#27861;&#65292;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#35270;&#35273;&#35821;&#20041;&#20998;&#21106;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08215</link><description>&lt;p&gt;
LIX&#65306;&#23558;&#31354;&#38388;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#38544;&#24335;&#27880;&#20837;&#35270;&#35273;&#35821;&#20041;&#20998;&#21106;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual Semantic Segmentation for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08215
&lt;/p&gt;
&lt;p&gt;
&#23558;&#21452;&#32534;&#30721;&#22120;&#25945;&#24072;&#27169;&#22411;&#33719;&#24471;&#30340;&#31354;&#38388;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#38544;&#24335;&#27880;&#20837;&#21333;&#32534;&#30721;&#22120;&#23398;&#29983;&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#30340;logit&#33976;&#39311;&#21644;&#29305;&#24449;&#33976;&#39311;&#26041;&#27861;&#65292;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#35270;&#35273;&#35821;&#20041;&#20998;&#21106;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25968;&#25454;&#34701;&#21512;&#32593;&#32476;&#22312;&#35270;&#35273;&#35821;&#20041;&#20998;&#21106;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24403;&#32570;&#20047;&#31354;&#38388;&#20960;&#20309;&#25968;&#25454;&#26102;&#65292;&#21452;&#32534;&#30721;&#22120;&#21464;&#24471;&#26080;&#25928;&#12290;&#23558;&#21452;&#32534;&#30721;&#22120;&#25945;&#24072;&#27169;&#22411;&#33719;&#24471;&#30340;&#31354;&#38388;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#38544;&#24335;&#27880;&#20837;&#21333;&#32534;&#30721;&#22120;&#23398;&#29983;&#27169;&#22411;&#26159;&#19968;&#20010;&#23454;&#29992;&#20294;&#19981;&#22826;&#25506;&#32034;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20010;&#20027;&#39064;&#65292;&#24182;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Learning to Infuse "X" (LIX) &#26694;&#26550;&#65292;&#22312;logit&#33976;&#39311;&#21644;&#29305;&#24449;&#33976;&#39311;&#26041;&#38754;&#36827;&#34892;&#20102;&#26032;&#39062;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#35777;&#26126;&#65292;&#24378;&#35843;&#22312;&#35299;&#32806;&#30693;&#35782;&#33976;&#39311;&#20013;&#20351;&#29992;&#21333;&#19968;&#22266;&#23450;&#26435;&#37325;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;logit&#26234;&#33021;&#21160;&#24577;&#26435;&#37325;&#25511;&#21046;&#22120;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37325;&#26032;&#26657;&#20934;&#30340;&#29305;&#24449;&#33976;&#39311;&#31639;&#27861;&#65292;&#21253;&#25324;&#20004;&#31181;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08215v1 Announce Type: cross  Abstract: Despite the impressive performance achieved by data-fusion networks with duplex encoders for visual semantic segmentation, they become ineffective when spatial geometric data are not available. Implicitly infusing the spatial geometric prior knowledge acquired by a duplex-encoder teacher model into a single-encoder student model is a practical, albeit less explored research avenue. This paper delves into this topic and resorts to knowledge distillation approaches to address this problem. We introduce the Learning to Infuse "X" (LIX) framework, with novel contributions in both logit distillation and feature distillation aspects. We present a mathematical proof that underscores the limitation of using a single fixed weight in decoupled knowledge distillation and introduce a logit-wise dynamic weight controller as a solution to this issue. Furthermore, we develop an adaptively-recalibrated feature distillation algorithm, including two tec
&lt;/p&gt;</description></item><item><title>BG-HGNN&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#29616;&#26377;HGNNs&#22312;&#22797;&#26434;&#24322;&#26500;&#22270;&#19978;&#38754;&#20020;&#30340;&#21442;&#25968;&#29190;&#28856;&#21644;&#20851;&#31995;&#22349;&#22604;&#31561;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.08207</link><description>&lt;p&gt;
BG-HGNN: &#26397;&#21521;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
BG-HGNN: Toward Scalable and Efficient Heterogeneous Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08207
&lt;/p&gt;
&lt;p&gt;
BG-HGNN&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#29616;&#26377;HGNNs&#22312;&#22797;&#26434;&#24322;&#26500;&#22270;&#19978;&#38754;&#20020;&#30340;&#21442;&#25968;&#29190;&#28856;&#21644;&#20851;&#31995;&#22349;&#22604;&#31561;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#34987;&#24314;&#27169;&#20026;&#22312;&#24322;&#26500;&#22270;&#19978;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#20855;&#26377;&#26469;&#33258;&#19981;&#21516;&#31867;&#22411;&#33410;&#28857;&#21644;&#36793;&#30340;&#21508;&#31181;&#20851;&#31995;&#12290;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;(HGNNs)&#26159;&#19968;&#31181;&#20026;&#24322;&#26500;&#22270;&#35774;&#35745;&#30340;&#26377;&#21069;&#36884;&#30340;&#31070;&#32463;&#27169;&#22411;&#31867;&#12290;&#29616;&#26377;HGNNs&#24314;&#31435;&#22312;&#20256;&#32479;GNNs&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#19981;&#21516;&#30340;&#21442;&#25968;&#31354;&#38388;&#26469;&#24314;&#27169;&#19981;&#21516;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;HGNNs&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#36890;&#24120;&#23616;&#38480;&#20110;&#31616;&#21333;&#30340;&#24322;&#26500;&#22270;&#65292;&#20855;&#26377;&#23569;&#37327;&#20851;&#31995;&#31867;&#22411;&#12290;&#26412;&#25991;&#39318;&#20808;&#31361;&#20986;&#21644;&#35777;&#26126;&#29616;&#26377;HGNNs&#20351;&#29992;&#30340;&#26631;&#20934;&#26041;&#27861;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#21442;&#25968;&#29190;&#28856;&#21644;&#20851;&#31995;&#22349;&#22604;&#65292;&#20351;&#24471;HGNNs&#23545;&#20855;&#26377;&#22823;&#37327;&#20851;&#31995;&#31867;&#22411;&#30340;&#22797;&#26434;&#24322;&#26500;&#22270; less&#26377;&#25928;&#25110;&#19981;&#23454;&#29992;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;Blend&amp;Grind-HGNN (BG-HGNN)&#65292;&#36890;&#36807;&#20180;&#32454;&#22788;&#29702;&#25361;&#25112;&#26469;&#26377;&#25928;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08207v1 Announce Type: new  Abstract: Many computer vision and machine learning problems are modelled as learning tasks on heterogeneous graphs, featuring a wide array of relations from diverse types of nodes and edges. Heterogeneous graph neural networks (HGNNs) stand out as a promising neural model class designed for heterogeneous graphs. Built on traditional GNNs, existing HGNNs employ different parameter spaces to model the varied relationships. However, the practical effectiveness of existing HGNNs is often limited to simple heterogeneous graphs with few relation types. This paper first highlights and demonstrates that the standard approach employed by existing HGNNs inevitably leads to parameter explosion and relation collapse, making HGNNs less effective or impractical for complex heterogeneous graphs with numerous relation types. To overcome this issue, we introduce a novel framework, Blend&amp;Grind-HGNN (BG-HGNN), which effectively tackles the challenges by carefully i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;AutoDFP&#30340;&#33258;&#21160;&#26080;&#25968;&#25454;&#21098;&#26525;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#30041;&#30456;&#20284;&#36890;&#36947;&#30340;&#37325;&#28857;&#20449;&#24687;&#23454;&#29616;&#33258;&#21160;&#21098;&#26525;&#21644;&#37325;&#24314;&#65292;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.08204</link><description>&lt;p&gt;
AutoDFP: &#36890;&#36807;&#36890;&#36947;&#30456;&#20284;&#24615;&#37325;&#24314;&#23454;&#29616;&#33258;&#21160;&#26080;&#25968;&#25454;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
AutoDFP: Automatic Data-Free Pruning via Channel Similarity Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08204
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;AutoDFP&#30340;&#33258;&#21160;&#26080;&#25968;&#25454;&#21098;&#26525;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#30041;&#30456;&#20284;&#36890;&#36947;&#30340;&#37325;&#28857;&#20449;&#24687;&#23454;&#29616;&#33258;&#21160;&#21098;&#26525;&#21644;&#37325;&#24314;&#65292;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#30340;&#21457;&#23637;&#22635;&#34917;&#20102;&#31070;&#32463;&#32593;&#32476;&#35268;&#27169;&#24222;&#22823;&#21644;&#26377;&#38480;&#30828;&#20214;&#36164;&#28304;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#26469;&#24494;&#35843;&#21387;&#32553;&#27169;&#22411;&#65292;&#23548;&#33268;&#39640;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#22312;&#23545;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#35201;&#27714;&#20005;&#26684;&#30340;&#22330;&#26223;&#20013;&#26080;&#27861;&#24212;&#29992;&#12290;&#20316;&#20026;&#26367;&#20195;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26080;&#25968;&#25454;&#26041;&#27861;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#25163;&#24037;&#21442;&#25968;&#35843;&#25972;&#65292;&#20165;&#33021;&#23454;&#29616;&#19981;&#28789;&#27963;&#30340;&#37325;&#24314;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26080;&#25968;&#25454;&#21098;&#26525;(AutoDFP)&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21098;&#26525;&#21644;&#37325;&#24314;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36825;&#26679;&#30340;&#20551;&#35774;&#65306;&#20449;&#24687;&#30340;&#20002;&#22833;&#21487;&#20197;&#36890;&#36807;&#20445;&#30041;&#26469;&#33258;&#30456;&#20284;&#36890;&#36947;&#30340;&#37325;&#28857;&#20449;&#24687;&#26469;&#37096;&#20998;&#34917;&#20607;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#26080;&#25968;&#25454;&#21098;&#26525;&#21046;&#23450;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#24471;&#21040;&#26377;&#25928;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08204v1 Announce Type: new  Abstract: Structured pruning methods are developed to bridge the gap between the massive scale of neural networks and the limited hardware resources. Most current structured pruning methods rely on training datasets to fine-tune the compressed model, resulting in high computational burdens and being inapplicable for scenarios with stringent requirements on privacy and security. As an alternative, some data-free methods have been proposed, however, these methods often require handcraft parameter tuning and can only achieve inflexible reconstruction. In this paper, we propose the Automatic Data-Free Pruning (AutoDFP) method that achieves automatic pruning and reconstruction without fine-tuning. Our approach is based on the assumption that the loss of information can be partially compensated by retaining focused information from similar channels. Specifically, We formulate data-free pruning as an optimization problem, which can be effectively address
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;&#31038;&#21306;&#32858;&#31867;&#21644;&#20998;&#31867;&#30340;&#21487;&#23398;&#20064;&#30340;&#38754;&#21521;&#31038;&#21306;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.08203</link><description>&lt;p&gt;
&#21487;&#23398;&#20064;&#30340;&#38754;&#21521;&#31038;&#21306;&#30340;&#21464;&#21387;&#22120;&#29992;&#20110;&#22823;&#33041;&#32467;&#26500;&#32593;&#32476;&#20998;&#26512;&#19982;&#26631;&#35760;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Learnable Community-Aware Transformer for Brain Connectome Analysis with Token Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08203
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;&#31038;&#21306;&#32858;&#31867;&#21644;&#20998;&#31867;&#30340;&#21487;&#23398;&#20064;&#30340;&#38754;&#21521;&#31038;&#21306;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#25581;&#31034;&#20102;&#22797;&#26434;&#30340;&#22823;&#33041;&#32593;&#32476;&#21487;&#20197;&#34987;&#32452;&#32455;&#20026;&#19981;&#21516;&#30340;&#21151;&#33021;&#31038;&#21306;&#65292;&#27599;&#20010;&#31038;&#21306;&#30001;&#19968;&#32452;&#20855;&#26377;&#24378;&#36830;&#25509;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROIs&#65289;&#32452;&#25104;&#12290;&#36825;&#20123;&#31038;&#21306;&#22312;&#29702;&#35299;&#22823;&#33041;&#30340;&#21151;&#33021;&#32452;&#32455;&#21450;&#20854;&#23545;&#31070;&#32463;&#30142;&#30149;&#65292;&#21253;&#25324;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#21644;&#24615;&#21035;&#31561;&#29983;&#29289;&#24046;&#24322;&#30340;&#24433;&#21709;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#32852;&#21512;&#31038;&#21306;&#32858;&#31867;&#21644;&#20998;&#31867;&#30340;&#26631;&#35760;&#32858;&#31867;&#22823;&#33041;&#21464;&#21387;&#22120;&#27169;&#22411; ($\texttt{TC-BrainTF}$)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08203v1 Announce Type: cross  Abstract: Neuroscientific research has revealed that the complex brain network can be organized into distinct functional communities, each characterized by a cohesive group of regions of interest (ROIs) with strong interconnections. These communities play a crucial role in comprehending the functional organization of the brain and its implications for neurological conditions, including Autism Spectrum Disorder (ASD) and biological differences, such as in gender. Traditional models have been constrained by the necessity of predefined community clusters, limiting their flexibility and adaptability in deciphering the brain's functional organization. Furthermore, these models were restricted by a fixed number of communities, hindering their ability to accurately represent the brain's dynamic nature. In this study, we present a token clustering brain transformer-based model ($\texttt{TC-BrainTF}$) for joint community clustering and classification. Ou
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#28145;&#24230;&#23376;&#27169;&#36870;&#28857;&#32593;&#32476;&#65288;DSPNs&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21551;&#21457;&#30340;GPC-ready&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#23376;&#27169;&#20989;&#25968;&#23398;&#20064;&#20013;&#30340;&#20004;&#22823;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.08199</link><description>&lt;p&gt;
&#28145;&#24230;&#23376;&#27169;&#36870;&#28857;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep Submodular Peripteral Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08199
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#28145;&#24230;&#23376;&#27169;&#36870;&#28857;&#32593;&#32476;&#65288;DSPNs&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21551;&#21457;&#30340;GPC-ready&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#23376;&#27169;&#20989;&#25968;&#23398;&#20064;&#20013;&#30340;&#20004;&#22823;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#27169;&#20989;&#25968;&#23545;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36890;&#24120;&#32570;&#20047;&#23454;&#29992;&#30340;&#23398;&#20064;&#26041;&#27861;&#26469;&#33719;&#21462;&#23427;&#20204;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#28145;&#24230;&#23376;&#27169;&#36870;&#28857;&#32593;&#32476;&#65288;DSPNs&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23376;&#27169;&#20989;&#25968;&#21442;&#25968;&#21270;&#26063;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21551;&#21457;&#30340;GPC-ready&#31574;&#30053;&#23545;&#20854;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#36830;&#25509;&#24182;&#35299;&#20915;&#19978;&#36848;&#20004;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08199v1 Announce Type: cross  Abstract: Submodular functions, crucial for various applications, often lack practical learning methods for their acquisition. Seemingly unrelated, learning a scaling from oracles offering graded pairwise preferences (GPC) is underexplored, despite a rich history in psychometrics. In this paper, we introduce deep submodular peripteral networks (DSPNs), a novel parametric family of submodular functions, and methods for their training using a contrastive-learning inspired GPC-ready strategy to connect and then tackle both of the above challenges. We introduce newly devised GPC-style "peripteral" loss which leverages numerically graded relationships between pairs of objects (sets in our case). Unlike traditional contrastive learning, our method utilizes graded comparisons, extracting more nuanced information than just binary-outcome comparisons, and contrasts sets of any size (not just two). We also define a novel suite of automatic sampling strate
&lt;/p&gt;</description></item><item><title>PAGE&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26234;&#33021;&#21307;&#30103;&#30340;&#39046;&#22495;&#22686;&#37327;&#36866;&#24212;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;&#20110;&#20445;&#30041;&#25968;&#25454;&#25110;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#29983;&#25104;&#22238;&#25918;&#65292;&#26377;&#25928;&#24179;&#34913;&#39046;&#22495;&#36866;&#24212;&#21644;&#30693;&#35782;&#20445;&#30041;&#65292;&#24182;&#32467;&#21512;&#25193;&#23637;&#30340;&#24402;&#32435;&#30830;&#35748;&#39044;&#27979;&#26041;&#27861;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#30142;&#30149;&#26816;&#27979;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.08197</link><description>&lt;p&gt;
PAGE: &#20855;&#26377;&#38754;&#21521;&#36807;&#21435;&#26080;&#20851;&#29983;&#25104;&#22238;&#25918;&#30340;&#39046;&#22495;&#22686;&#37327;&#36866;&#24212;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
PAGE: Domain-Incremental Adaptation with Past-Agnostic Generative Replay for Smart Healthcare
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08197
&lt;/p&gt;
&lt;p&gt;
PAGE&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26234;&#33021;&#21307;&#30103;&#30340;&#39046;&#22495;&#22686;&#37327;&#36866;&#24212;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;&#20110;&#20445;&#30041;&#25968;&#25454;&#25110;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#29983;&#25104;&#22238;&#25918;&#65292;&#26377;&#25928;&#24179;&#34913;&#39046;&#22495;&#36866;&#24212;&#21644;&#30693;&#35782;&#20445;&#30041;&#65292;&#24182;&#32467;&#21512;&#25193;&#23637;&#30340;&#24402;&#32435;&#30830;&#35748;&#39044;&#27979;&#26041;&#27861;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#30142;&#30149;&#26816;&#27979;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PAGE&#65292;&#19968;&#31181;&#20855;&#26377;&#38754;&#21521;&#36807;&#21435;&#26080;&#20851;&#29983;&#25104;&#22238;&#25918;&#30340;&#39046;&#22495;&#22686;&#37327;&#36866;&#24212;&#31574;&#30053;&#65292;&#29992;&#20110;&#26234;&#33021;&#21307;&#30103;&#12290;PAGE&#33021;&#22815;&#23454;&#29616;&#29983;&#25104;&#22238;&#25918;&#65292;&#32780;&#26080;&#38656;&#20445;&#30041;&#26469;&#33258;&#20808;&#21069;&#39046;&#22495;&#30340;&#20219;&#20309;&#25968;&#25454;&#25110;&#20449;&#24687;&#12290;&#22312;&#36866;&#24212;&#26032;&#39046;&#22495;&#26102;&#65292;&#23427;&#21033;&#29992;&#26469;&#33258;&#26032;&#20998;&#24067;&#21644;&#24403;&#21069;&#27169;&#22411;&#30340;&#30495;&#23454;&#25968;&#25454;&#26469;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#20445;&#30041;&#20808;&#21069;&#39046;&#22495;&#30340;&#23398;&#20064;&#30693;&#35782;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#37325;&#26032;&#25773;&#25918;&#21512;&#25104;&#25968;&#25454;&#21644;&#26032;&#23454;&#38469;&#25968;&#25454;&#65292;PAGE&#22312;&#39046;&#22495;&#36866;&#24212;&#21644;&#30693;&#35782;&#20445;&#30041;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#24179;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25193;&#23637;&#30340;&#24402;&#32435;&#30830;&#35748;&#39044;&#27979;&#65288;EICP&#65289;&#26041;&#27861;&#25972;&#21512;&#21040;PAGE&#20013;&#65292;&#20026;&#27599;&#20010;&#26816;&#27979;&#32467;&#26524;&#29983;&#25104;&#32622;&#20449;&#24230;&#20998;&#25968;&#21644;&#21487;&#20449;&#24230;&#20540;&#12290;&#36825;&#20351;&#24471;&#39044;&#27979;&#32467;&#26524;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#20026;&#26234;&#33021;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#30142;&#30149;&#26816;&#27979;&#25552;&#20379;&#32479;&#35745;&#20445;&#35777;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;PAGE&#22312;&#39046;&#22495;&#22686;&#37327;&#30142;&#30149;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08197v1 Announce Type: cross  Abstract: We propose PAGE, a domain-incremental adaptation strategy with past-agnostic generative replay for smart healthcare. PAGE enables generative replay without the aid of any preserved data or information from prior domains. When adapting to a new domain, it exploits real data from the new distribution and the current model to generate synthetic data that retain the learned knowledge of previous domains. By replaying the synthetic data with the new real data during training, PAGE achieves a good balance between domain adaptation and knowledge retention. In addition, we incorporate an extended inductive conformal prediction (EICP) method into PAGE to produce a confidence score and a credibility value for each detection result. This makes the predictions interpretable and provides statistical guarantees for disease detection in smart healthcare applications. We demonstrate PAGE's effectiveness in domain-incremental disease detection with thr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#20803;&#23398;&#20064;&#26694;&#26550;&#26469;&#23398;&#20064;&#28151;&#21512;&#28508;&#22312;&#21160;&#24577;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#29289;&#29702;&#24402;&#32435;&#20559;&#24046;&#21644;&#23398;&#20064;&#35782;&#21035;&#31574;&#30053;&#65292;&#22312;&#25429;&#25417;&#26410;&#30693;&#21160;&#24577;&#21644;&#20998;&#36776;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.08194</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#28151;&#21512;&#28508;&#22312;&#21160;&#24577;&#65306;&#19968;&#31181;&#23398;&#20064;&#35782;&#21035;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Learning of Hybrid Latent Dynamics: A Learn-to-Identify Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08194
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#20803;&#23398;&#20064;&#26694;&#26550;&#26469;&#23398;&#20064;&#28151;&#21512;&#28508;&#22312;&#21160;&#24577;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#29289;&#29702;&#24402;&#32435;&#20559;&#24046;&#21644;&#23398;&#20064;&#35782;&#21035;&#31574;&#30053;&#65292;&#22312;&#25429;&#25417;&#26410;&#30693;&#21160;&#24577;&#21644;&#20998;&#36776;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#24212;&#29992;&#36234;&#26469;&#36234;&#38656;&#35201;&#20174;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#20013;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#28508;&#22312;&#21160;&#24577;&#12290;&#36825;&#24102;&#26469;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#21487;&#36776;&#35782;&#24615;&#25361;&#25112;&#65306;&#35768;&#22810;&#25277;&#35937;&#30340;&#28508;&#22312;&#34920;&#31034;&#21487;&#20197;&#37325;&#26500;&#35266;&#27979;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#26159;&#21542;&#20445;&#35777;&#20102;&#23545;&#32479;&#27835;&#21160;&#24577;&#30340;&#20805;&#20998;&#35782;&#21035;&#65311;&#26412;&#25991;&#20174;&#20004;&#20010;&#35282;&#24230;&#25506;&#35752;&#20102;&#36825;&#19968;&#25361;&#25112;&#65306;&#19968;&#26159;&#20351;&#29992;&#29305;&#23450;&#20110;&#25152;&#24314;&#27169;&#25968;&#25454;&#30340;&#29289;&#29702;&#24402;&#32435;&#20559;&#24046;&#65292;&#20108;&#26159;&#20351;&#29992;&#19968;&#20010;&#23398;&#20064;&#35782;&#21035;&#30340;&#31574;&#30053;&#65292;&#23558;&#39044;&#27979;&#30446;&#26631;&#19982;&#29992;&#20110;&#35782;&#21035;&#30340;&#25968;&#25454;&#20998;&#24320;&#12290;&#25105;&#20204;&#23558;&#36825;&#20004;&#31181;&#31574;&#30053;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#20803;&#23398;&#20064;&#28151;&#21512;&#28508;&#22312;&#21160;&#24577;&#26694;&#26550;&#65288;Meta-HyLaD&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#65306;1&#65289;&#19968;&#20010;&#28151;&#21512;&#20808;&#21069;&#29289;&#29702;&#25968;&#23398;&#34920;&#36798;&#24335;&#19982;&#25551;&#36848;&#20854;&#26410;&#30693;&#35823;&#24046;&#30340;&#31070;&#32463;&#20989;&#25968;&#30340;&#28508;&#22312;&#21160;&#24577;&#20989;&#25968;&#65292;&#20197;&#21450;2&#65289;&#19968;&#20010;&#20803;&#23398;&#20064;&#20844;&#24335;&#65292;&#29992;&#20110;&#23398;&#20064;&#20998;&#21035;&#35782;&#21035;&#28151;&#21512;&#21160;&#24577;&#30340;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08194v1 Announce Type: new  Abstract: Modern applications increasingly require unsupervised learning of latent dynamics from high-dimensional time-series. This presents a significant challenge of identifiability: many abstract latent representations may reconstruct observations, yet do they guarantee an adequate identification of the governing dynamics? This paper investigates this challenge from two angles: the use of physics inductive bias specific to the data being modeled, and a learn-to-identify strategy that separates forecasting objectives from the data used for the identification. We combine these two strategies in a novel framework for unsupervised meta-learning of hybrid latent dynamics (Meta-HyLaD) with: 1) a latent dynamic function that hybridize known mathematical expressions of prior physics with neural functions describing its unknown errors, and 2) a meta-learning formulation to learn to separately identify both components of the hybrid dynamics. Through exte
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#39537;&#21160;&#30340;&#29289;&#29702;&#24863;&#30693;&#38376;&#23610;&#23544;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#20248;&#21270;&#22823;&#35268;&#27169;&#30005;&#36335;&#30340;&#26102;&#24207;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08193</link><description>&lt;p&gt;
&#23398;&#20064;&#39537;&#21160;&#30340;&#29289;&#29702;&#24863;&#30693;&#22823;&#35268;&#27169;&#30005;&#36335;&#38376;&#23610;&#23544;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning-driven Physically-aware Large-scale Circuit Gate Sizing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#39537;&#21160;&#30340;&#29289;&#29702;&#24863;&#30693;&#38376;&#23610;&#23544;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#20248;&#21270;&#22823;&#35268;&#27169;&#30005;&#36335;&#30340;&#26102;&#24207;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38376;&#23610;&#23544;&#22312;&#29289;&#29702;&#35774;&#35745;&#21518;&#30340;&#26102;&#24207;&#20248;&#21270;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#29616;&#26377;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#38376;&#23610;&#23544;&#20248;&#21270;&#26041;&#27861;&#26080;&#27861;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#26102;&#24207;&#36335;&#24452;&#19978;&#30340;&#26102;&#24207;&#65292;&#24182;&#24573;&#30053;&#24067;&#23616;&#30340;&#29289;&#29702;&#32422;&#26463;&#12290;&#20026;&#20102;&#39640;&#25928;&#20248;&#21270;&#22823;&#35268;&#27169;&#30005;&#36335;&#30340;&#26102;&#24207;&#24615;&#33021;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#39537;&#21160;&#30340;&#29289;&#29702;&#24863;&#30693;&#38376;&#23610;&#23544;&#20248;&#21270;&#26694;&#26550;&#12290;&#36890;&#36807;&#23398;&#20064;&#22810;&#20010;&#26102;&#24207;&#36335;&#24452;&#19978;&#30340;&#26102;&#24207;&#20449;&#24687;&#21644;&#22810;&#23610;&#24230;&#24067;&#23616;&#19978;&#30340;&#29289;&#29702;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#38376;&#23610;&#23544;&#24863;&#30693;&#26102;&#24207;&#27169;&#22411;&#65292;&#29992;&#20110;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#12290;&#21516;&#26102;&#65292;&#22522;&#20110;&#23610;&#23544;&#23548;&#21521;&#20272;&#35745;&#22120;&#30340;&#26799;&#24230;&#29983;&#25104;&#21644;&#33258;&#36866;&#24212;&#21453;&#21521;&#20256;&#25773;&#34987;&#24320;&#21457;&#29992;&#20110;&#26356;&#26032;&#38376;&#23610;&#23544;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#21830;&#19994;&#24037;&#20855;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08193v1 Announce Type: new  Abstract: Gate sizing plays an important role in timing optimization after physical design. Existing machine learning-based gate sizing works cannot optimize timing on multiple timing paths simultaneously and neglect the physical constraint on layouts. They cause sub-optimal sizing solutions and low-efficiency issues when compared with commercial gate sizing tools. In this work, we propose a learning-driven physically-aware gate sizing framework to optimize timing performance on large-scale circuits efficiently. In our gradient descent optimization-based work, for obtaining accurate gradients, a multi-modal gate sizing-aware timing model is achieved via learning timing information on multiple timing paths and physical information on multiple-scaled layouts jointly. Then, gradient generation based on the sizing-oriented estimator and adaptive back-propagation are developed to update gate sizes. Our results demonstrate that our work achieves higher 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#27010;&#24565;&#65292;$(\varepsilon, \Phi(\delta))$-&#23616;&#37096;&#22343;&#34913;&#65292;&#20197;&#35299;&#20915;&#22312;&#38750;&#20985;&#28216;&#25103;&#20013;&#23616;&#37096;&#22343;&#34913;&#23384;&#22312;&#20294;&#38590;&#20197;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08171</link><description>&lt;p&gt;
&#22312;&#38750;&#20985;&#28216;&#25103;&#20013;&#21487;&#22788;&#29702;&#30340;&#23616;&#37096;&#22343;&#34913;
&lt;/p&gt;
&lt;p&gt;
Tractable Local Equilibria in Non-Concave Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08171
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#27010;&#24565;&#65292;$(\varepsilon, \Phi(\delta))$-&#23616;&#37096;&#22343;&#34913;&#65292;&#20197;&#35299;&#20915;&#22312;&#38750;&#20985;&#28216;&#25103;&#20013;&#23616;&#37096;&#22343;&#34913;&#23384;&#22312;&#20294;&#38590;&#20197;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20247;&#25152;&#21608;&#30693;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#21644;&#20854;&#20182;&#26080;&#24724;&#23398;&#20064;&#31243;&#24207;&#21487;&#20197;&#26377;&#25928;&#22320;&#25910;&#25947;&#21040;&#21327;&#35843;&#22343;&#34913;&#65292;&#22312;&#27599;&#20010;Agent&#30340;&#25928;&#29992;&#23545;&#20110;&#20854;&#33258;&#36523;&#31574;&#30053;&#21576;&#20985;&#24418;&#30340;&#24773;&#20917;&#19979;&#65292;&#20294;&#24403;&#25928;&#29992;&#26159;&#38750;&#20985;&#30340;&#26102;&#65292;&#36825;&#31181;&#24773;&#20917;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#65292;&#20854;&#20013;Agent&#30340;&#31574;&#30053;&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#65292;&#25110;&#32773;Agent&#30340;&#25928;&#29992;&#30001;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#65292;&#25110;&#20004;&#32773;&#20860;&#32780;&#26377;&#20043;&#12290;&#23454;&#38469;&#19978;&#65292;&#38750;&#20985;&#28216;&#25103;&#23384;&#22312;&#19968;&#31995;&#21015;&#21338;&#24328;&#35770;&#21644;&#20248;&#21270;&#25361;&#25112;&#65306;(i) Nash&#22343;&#34913;&#21487;&#33021;&#19981;&#23384;&#22312;&#65307;(ii) &#23616;&#37096;Nash&#22343;&#34913;&#23384;&#22312;&#20294;&#26159;&#19981;&#21487;&#22788;&#29702;&#65307;(iii) &#28151;&#21512;Nash&#12289;&#21327;&#35843;&#21644;&#31895;&#31961;&#21327;&#35843;&#22343;&#34913;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#20855;&#26377;&#26080;&#38480;&#25903;&#25345;&#65292;&#24182;&#19988;&#26159;&#19981;&#21487;&#22788;&#29702;&#30340;&#12290;&#20026;&#20102;&#36991;&#24320;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#27010;&#24565;&#65292;&#31216;&#20026;$(\varepsilon, \Phi(\delta))$-&#23616;&#37096;&#22343;&#34913;&#65292;&#35813;&#27010;&#24565;&#22312;&#38750;&#20985;&#28216;&#25103;&#20013;&#27010;&#25324;&#20102;&#23616;&#37096;Nash&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08171v1 Announce Type: cross  Abstract: While Online Gradient Descent and other no-regret learning procedures are known to efficiently converge to coarse correlated equilibrium in games where each agent's utility is concave in their own strategy, this is not the case when the utilities are non-concave, a situation that is common in machine learning applications where the agents' strategies are parameterized by deep neural networks, or the agents' utilities are computed by a neural network, or both. Indeed, non-concave games present a host of game-theoretic and optimization challenges: (i) Nash equilibria may fail to exist; (ii) local Nash equilibria exist but are intractable; and (iii) mixed Nash, correlated, and coarse correlated equilibria have infinite support in general, and are intractable. To sidestep these challenges we propose a new solution concept, termed $(\varepsilon, \Phi(\delta))$-local equilibrium, which generalizes local Nash equilibrium in non-concave games,
&lt;/p&gt;</description></item><item><title>MolBind &#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20026;&#22810;&#31181;&#27169;&#24577;&#35757;&#32451;&#32534;&#30721;&#22120;&#65292;&#23558;&#25152;&#26377;&#27169;&#24577;&#26144;&#23556;&#21040;&#20849;&#20139;&#29305;&#24449;&#31354;&#38388;&#65292;&#23454;&#29616;&#22810;&#27169;&#24577;&#35821;&#20041;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.08167</link><description>&lt;p&gt;
MolBind: &#22810;&#27169;&#24577;&#23545;&#40784;&#35821;&#35328;&#12289;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;
&lt;/p&gt;
&lt;p&gt;
MolBind: Multimodal Alignment of Language, Molecules, and Proteins
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08167
&lt;/p&gt;
&lt;p&gt;
MolBind &#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20026;&#22810;&#31181;&#27169;&#24577;&#35757;&#32451;&#32534;&#30721;&#22120;&#65292;&#23558;&#25152;&#26377;&#27169;&#24577;&#26144;&#23556;&#21040;&#20849;&#20139;&#29305;&#24449;&#31354;&#38388;&#65292;&#23454;&#29616;&#22810;&#27169;&#24577;&#35821;&#20041;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#23398;&#21644;&#21270;&#23398;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#21033;&#29992;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#23558;&#20998;&#23376;&#21450;&#20854;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25972;&#21512;&#36215;&#26469;&#65292;&#20197;&#22686;&#24378;&#33647;&#29289;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#20165;&#38480;&#20110;&#20004;&#31181;&#27169;&#24577;&#65292;&#35774;&#35745;&#19968;&#20010;&#32479;&#19968;&#30340;&#32593;&#32476;&#26469;&#22788;&#29702;&#19981;&#21516;&#27169;&#24577;&#65288;&#20363;&#22914;&#33258;&#28982;&#35821;&#35328;&#12289;2D&#20998;&#23376;&#22270;&#12289;3D&#20998;&#23376;&#26500;&#35937;&#21644;3D&#34507;&#30333;&#36136;&#65289;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08167v1 Announce Type: cross  Abstract: Recent advancements in biology and chemistry have leveraged multi-modal learning, integrating molecules and their natural language descriptions to enhance drug discovery. However, current pre-training frameworks are limited to two modalities, and designing a unified network to process different modalities (e.g., natural language, 2D molecular graphs, 3D molecular conformations, and 3D proteins) remains challenging due to inherent gaps among them. In this work, we propose MolBind, a framework that trains encoders for multiple modalities through contrastive learning, mapping all modalities to a shared feature space for multi-modal semantic alignment. To facilitate effective pre-training of MolBind on multiple modalities, we also build and collect a high-quality dataset with four modalities, MolBind-M4, including graph-language, conformation-language, graph-conformation, and conformation-protein paired data. MolBind shows superior zero-sh
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36731;&#37327;&#32423;TTS&#31995;&#32479;&#65292;&#37319;&#29992;&#20004;&#38454;&#27573;&#35757;&#32451;&#32780;&#38750;&#36882;&#24402;&#21333;&#20803;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#21644;&#32463;&#27982;&#25104;&#26412;</title><link>https://arxiv.org/abs/2403.08164</link><description>&lt;p&gt;
EM-TTS&#65306;&#39640;&#25928;&#35757;&#32451;&#30340;&#20302;&#36164;&#28304;&#33945;&#21476;&#35821;&#36731;&#37327;&#32423;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
EM-TTS: Efficiently Trained Low-Resource Mongolian Lightweight Text-to-Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08164
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36731;&#37327;&#32423;TTS&#31995;&#32479;&#65292;&#37319;&#29992;&#20004;&#38454;&#27573;&#35757;&#32451;&#32780;&#38750;&#36882;&#24402;&#21333;&#20803;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#21644;&#32463;&#27982;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#31995;&#32479;&#21462;&#24471;&#20102;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#21512;&#25104;&#32467;&#26524;&#12290;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;TTS&#31995;&#32479;&#20013;&#24207;&#21015;&#25968;&#25454;&#30340;&#26631;&#20934;&#24314;&#27169;&#25216;&#26415;&#65292;&#24182;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#21253;&#21547;RNN&#32452;&#20214;&#30340;TTS&#27169;&#22411;&#38656;&#35201;&#24378;&#22823;&#30340;GPU&#24615;&#33021;&#24182;&#19988;&#26102;&#38388;&#38271;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;CNN&#30340;&#24207;&#21015;&#21512;&#25104;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;TTS&#27169;&#22411;&#30340;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#30001;&#20110;&#20854;&#39640;&#24182;&#34892;&#24615;&#65292;&#21487;&#20197;&#20445;&#35777;&#19968;&#23450;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#20943;&#36731;&#36825;&#20123;&#35757;&#32451;&#30340;&#32463;&#27982;&#25104;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36731;&#37327;&#32423;TTS&#31995;&#32479;&#65292;&#36825;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#30340;&#31471;&#21040;&#31471;TTS&#27169;&#22411;&#65292;&#19981;&#20351;&#29992;&#20219;&#20309;&#36882;&#24402;&#21333;&#20803;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;Text2Spectrum &#21644; SSRN&#12290;&#21069;&#32773;&#29992;&#20110;&#23558;&#38899;&#32032;&#32534;&#30721;&#20026;&#31895;&#31961;&#30340;&#26757;&#23572;&#39057;&#35889;&#22270;&#65292;&#21518;&#32773;&#29992;&#20110;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08164v1 Announce Type: cross  Abstract: Recently, deep learning-based Text-to-Speech (TTS) systems have achieved high-quality speech synthesis results. Recurrent neural networks have become a standard modeling technique for sequential data in TTS systems and are widely used. However, training a TTS model which includes RNN components requires powerful GPU performance and takes a long time. In contrast, CNN-based sequence synthesis techniques can significantly reduce the parameters and training time of a TTS model while guaranteeing a certain performance due to their high parallelism, which alleviate these economic costs of training. In this paper, we propose a lightweight TTS system based on deep convolutional neural networks, which is a two-stage training end-to-end TTS model and does not employ any recurrent units. Our model consists of two stages: Text2Spectrum and SSRN. The former is used to encode phonemes into a coarse mel spectrogram and the latter is used to synthesi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36845;&#20195;&#23398;&#20064;&#22788;&#29702;&#24102;&#26377;&#22122;&#22768;&#21644;&#36816;&#21160;&#20266;&#24433;&#30340;MRI&#30340;&#32852;&#21512;&#22270;&#20687;&#21435;&#22122;&#21644;&#36816;&#21160;&#20266;&#24433;&#20462;&#27491;&#26694;&#26550;</title><link>https://arxiv.org/abs/2403.08162</link><description>&lt;p&gt;
&#19977;&#32500;&#33041;MRI&#32852;&#21512;&#22270;&#20687;&#21435;&#22122;&#21644;&#36816;&#21160;&#20266;&#24433;&#26657;&#27491;&#30340;&#36845;&#20195;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Iterative Learning for Joint Image Denoising and Motion Artifact Correction of 3D Brain MRI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08162
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36845;&#20195;&#23398;&#20064;&#22788;&#29702;&#24102;&#26377;&#22122;&#22768;&#21644;&#36816;&#21160;&#20266;&#24433;&#30340;MRI&#30340;&#32852;&#21512;&#22270;&#20687;&#21435;&#22122;&#21644;&#36816;&#21160;&#20266;&#24433;&#20462;&#27491;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#22122;&#22768;&#21644;&#36816;&#21160;&#20266;&#24433;&#20005;&#37325;&#24433;&#21709;&#33041;&#37096;MRI&#30340;&#36136;&#37327;&#65292;&#23545;&#19979;&#28216;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36845;&#20195;&#23398;&#20064;&#22788;&#29702;&#24102;&#26377;&#22122;&#22768;&#21644;&#36816;&#21160;&#20266;&#24433;&#30340;MRI&#30340;&#32852;&#21512;&#22270;&#20687;&#21435;&#22122;&#21644;&#36816;&#21160;&#20266;&#24433;&#20462;&#27491;&#65288;JDAC&#65289;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#31181;&#33258;&#36866;&#24212;&#21435;&#22122;&#27169;&#22411;&#21644;&#19968;&#31181;&#25239;&#20266;&#24433;&#27169;&#22411;&#12290;&#22312;&#33258;&#36866;&#24212;&#21435;&#22122;&#27169;&#22411;&#20013;&#65292;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22122;&#22768;&#27700;&#24179;&#20272;&#35745;&#31574;&#30053;&#65292;&#28982;&#21518;&#36890;&#36807;&#20855;&#26377;&#29305;&#24449;&#24402;&#19968;&#21270;&#30340;U-Net&#39592;&#24178;&#33258;&#36866;&#24212;&#22320;&#20943;&#23569;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08162v1 Announce Type: cross  Abstract: Image noise and motion artifacts greatly affect the quality of brain MRI and negatively influence downstream medical image analysis. Previous studies often focus on 2D methods that process each volumetric MR image slice-by-slice, thus losing important 3D anatomical information. Additionally, these studies generally treat image denoising and artifact correction as two standalone tasks, without considering their potential relationship, especially on low-quality images where severe noise and motion artifacts occur simultaneously. To address these issues, we propose a Joint image Denoising and motion Artifact Correction (JDAC) framework via iterative learning to handle noisy MRIs with motion artifacts, consisting of an adaptive denoising model and an anti-artifact model. In the adaptive denoising model, we first design a novel noise level estimation strategy, and then adaptively reduce the noise through a U-Net backbone with feature normal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#29305;&#24449;&#23725;&#22238;&#24402;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#21442;&#25968;&#21270;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#22914;&#20309;&#36873;&#25321;&#21442;&#25968;&#25968;&#37327;$p$&#30456;&#23545;&#20110;&#26679;&#26412;&#22823;&#23567;$n$&#20197;&#23454;&#29616;&#26368;&#20339;&#27979;&#35797;&#38169;&#35823;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.08160</link><description>&lt;p&gt;
&#36229;&#36234;&#32447;&#24615;&#32553;&#25918;&#21306;&#22495;&#30340;&#38543;&#26426;&#29305;&#24449;&#22238;&#24402;&#30340;&#28176;&#36817;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Asymptotics of Random Feature Regression Beyond the Linear Scaling Regime
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#29305;&#24449;&#23725;&#22238;&#24402;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#21442;&#25968;&#21270;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#22914;&#20309;&#36873;&#25321;&#21442;&#25968;&#25968;&#37327;$p$&#30456;&#23545;&#20110;&#26679;&#26412;&#22823;&#23567;$n$&#20197;&#23454;&#29616;&#26368;&#20339;&#27979;&#35797;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#26159;&#36890;&#36807;&#20351;&#29992;&#36229;&#21442;&#25968;&#21270;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#30452;&#21040;&#25509;&#36817;&#35757;&#32451;&#25968;&#25454;&#30340;&#25554;&#20540;&#20026;&#27490;&#12290;&#36890;&#36807;&#21452;&#35895;&#29616;&#35937;&#31561;&#29616;&#35937;&#24050;&#32463;&#34920;&#26126;&#65292;&#21442;&#25968;&#30340;&#25968;&#37327;&#26159;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#19981;&#33391;&#20195;&#29702;&#65292;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#21442;&#25968;&#21270;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#20160;&#20040;&#24433;&#21709;&#65311;&#27169;&#22411;&#22797;&#26434;&#24615;&#21644;&#27867;&#21270;&#22914;&#20309;&#21462;&#20915;&#20110;&#21442;&#25968;&#30340;&#25968;&#37327;$p$&#65311;&#25105;&#20204;&#24212;&#35813;&#22914;&#20309;&#36873;&#25321;$p$&#30456;&#23545;&#20110;&#26679;&#26412;&#22823;&#23567;$n$&#26469;&#23454;&#29616;&#26368;&#20248;&#30340;&#27979;&#35797;&#35823;&#24046;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38543;&#26426;&#29305;&#24449;&#23725;&#22238;&#24402;&#65288;RFRR&#65289;&#30340;&#20363;&#23376;&#12290;&#36825;&#20010;&#27169;&#22411;&#26082;&#21487;&#20197;&#30475;&#20316;&#26159;&#26680;&#23725;&#22238;&#24402;&#65288;KRR&#65289;&#30340;&#26377;&#38480;&#31209;&#36924;&#36817;&#65292;&#20063;&#21487;&#20197;&#30475;&#20316;&#26159;&#22312;&#25152;&#35859;&#30340;&#25042;&#24816;&#21306;&#22495;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#31616;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;$d$&#32500;&#29699;&#19978;&#22343;&#21248;&#20998;&#24067;&#30340;&#21327;&#21464;&#37327;&#65292;&#24182;&#35745;&#31639;&#23574;&#38160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08160v1 Announce Type: cross  Abstract: Recent advances in machine learning have been achieved by using overparametrized models trained until near interpolation of the training data. It was shown, e.g., through the double descent phenomenon, that the number of parameters is a poor proxy for the model complexity and generalization capabilities. This leaves open the question of understanding the impact of parametrization on the performance of these models. How does model complexity and generalization depend on the number of parameters $p$? How should we choose $p$ relative to the sample size $n$ to achieve optimal test error?   In this paper, we investigate the example of random feature ridge regression (RFRR). This model can be seen either as a finite-rank approximation to kernel ridge regression (KRR), or as a simplified model for neural networks trained in the so-called lazy regime. We consider covariates uniformly distributed on the $d$-dimensional sphere and compute sharp
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#32467;&#21512;&#20102;&#27700;&#20998;&#20256;&#36755;&#21644;&#27700;&#20998;&#20256;&#24863;&#20449;&#21495;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#21407;&#21017;&#65292;&#37319;&#29992;Adam&#12289;RMSprop&#21644;GD&#19977;&#31181;&#19981;&#21516;&#30340;&#20248;&#21270;&#22120;&#65292;&#22312;&#22303;&#22756;&#28287;&#24230;&#20272;&#35745;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25910;&#25947;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.08154</link><description>&lt;p&gt;
&#19981;&#21516;&#20248;&#21270;&#31574;&#30053;&#23545;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#22303;&#22756;&#28287;&#24230;&#20272;&#35745;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of Different Optimization Strategies to Physics-Constrained Deep Learning for Soil Moisture Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08154
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#32467;&#21512;&#20102;&#27700;&#20998;&#20256;&#36755;&#21644;&#27700;&#20998;&#20256;&#24863;&#20449;&#21495;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#21407;&#21017;&#65292;&#37319;&#29992;Adam&#12289;RMSprop&#21644;GD&#19977;&#31181;&#19981;&#21516;&#30340;&#20248;&#21270;&#22120;&#65292;&#22312;&#22303;&#22756;&#28287;&#24230;&#20272;&#35745;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25910;&#25947;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22303;&#22756;&#28287;&#24230;&#26159;&#19968;&#20010;&#23545;&#20154;&#31867;&#31038;&#20250;&#21644;&#29615;&#22659;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#30340;&#20851;&#38190;&#27700;&#25991;&#21442;&#25968;&#12290;&#22312;&#20316;&#29289;&#30000;&#22320;&#20013;&#20934;&#30830;&#24314;&#27169;&#21644;&#30417;&#27979;&#22303;&#22756;&#28287;&#24230;&#65292;&#23588;&#20854;&#26159;&#22312;&#26681;&#31995;&#21306;&#65288;&#22303;&#22756;&#34920;&#23618;100&#21400;&#31859;&#65289;&#20013;&#65292;&#23545;&#20110;&#20511;&#21161;&#31934;&#20934;&#28748;&#28297;&#21644;&#32789;&#20316;&#24037;&#20855;&#26469;&#25552;&#39640;&#20892;&#19994;&#29983;&#20135;&#21644;&#20316;&#29289;&#20135;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#20805;&#20998;&#23454;&#29616;&#20256;&#24863;&#22120;&#25968;&#25454;&#28508;&#21147;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#20808;&#36827;&#30340;&#39046;&#22495;&#24863;&#30693;&#27169;&#22411;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;P-DL&#65289;&#26694;&#26550;&#65292;&#23558;&#27700;&#20998;&#20256;&#36755;&#21644;&#27700;&#20998;&#20256;&#24863;&#20449;&#21495;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#21407;&#21017;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#26377;&#25928;&#37325;&#24314;&#22303;&#22756;&#28287;&#24230;&#21160;&#24577;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#20248;&#21270;&#22120;&#65292;&#21363;Adam&#12289;RMSprop&#21644;GD&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26368;&#23567;&#21270;P-DL&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#22312;&#35828;&#26126;&#24615;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Adam&#20248;&#21270;&#22120;&#30340;&#32463;&#39564;&#25910;&#25947;&#20248;&#20110;&#20854;&#20182;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08154v1 Announce Type: new  Abstract: Soil moisture is a key hydrological parameter that has significant importance to human society and the environment. Accurate modeling and monitoring of soil moisture in crop fields, especially in the root zone (top 100 cm of soil), is essential for improving agricultural production and crop yield with the help of precision irrigation and farming tools. Realizing the full sensor data potential depends greatly on advanced analytical and predictive domain-aware models. In this work, we propose a physics-constrained deep learning (P-DL) framework to integrate physics-based principles on water transport and water sensing signals for effective reconstruction of the soil moisture dynamics. We adopt three different optimizers, namely Adam, RMSprop, and GD, to minimize the loss function of P-DL during the training process. In the illustrative case study, we demonstrate the empirical convergence of Adam optimizers outperforms the other optimizatio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#19981;&#26029;&#22686;&#38271;&#30340;&#33021;&#32791;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32771;&#34385;&#32593;&#32476;&#23610;&#23544;&#12289;&#35745;&#31639;&#21644;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#30340;&#33021;&#32791;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.08151</link><description>&lt;p&gt;
&#34913;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#32791;&#21644;&#25928;&#29575;&#65306;&#23454;&#35777;&#20998;&#26512;&#19982;&#35774;&#35745;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Measuring the Energy Consumption and Efficiency of Deep Neural Networks: An Empirical Analysis and Design Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08151
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#19981;&#26029;&#22686;&#38271;&#30340;&#33021;&#32791;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32771;&#34385;&#32593;&#32476;&#23610;&#23544;&#12289;&#35745;&#31639;&#21644;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#30340;&#33021;&#32791;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#26085;&#30410;&#22686;&#38271;&#30340;&#33021;&#32791;&#38382;&#39064;&#65288;&#25152;&#35859;&#30340;&#8220;&#32418;&#33394;AI&#8221;&#36235;&#21183;&#65289;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#33410;&#28857;&#32423;&#29926;&#29305;&#34920;&#27979;&#37327;&#20102;&#35757;&#32451;&#21508;&#31181;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#23454;&#38469;&#33021;&#32791;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;BUTTER-E&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;BUTTER&#23454;&#35777;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#19968;&#20010;&#25193;&#20805;&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;63,527&#20010;&#21333;&#29420;&#23454;&#39564;&#36816;&#34892;&#30340;&#33021;&#32791;&#21644;&#24615;&#33021;&#25968;&#25454;&#65292;&#28085;&#30422;&#20102;30,582&#20010;&#19981;&#21516;&#30340;&#37197;&#32622;&#65306;13&#20010;&#25968;&#25454;&#38598;&#12289;20&#20010;&#22823;&#23567;&#65288;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#65289;&#12289;8&#20010;&#32593;&#32476;&#8220;&#24418;&#29366;&#8221;&#21644;14&#20010;&#28145;&#24230;&#65292;&#20197;&#21450;&#22312;CPU&#21644;GPU&#30828;&#20214;&#19978;&#20351;&#29992;&#33410;&#28857;&#32423;&#29926;&#29305;&#34920;&#25910;&#38598;&#30340;&#25968;&#25454;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#32593;&#32476;&#32467;&#26500;&#21644;&#33021;&#32791;&#20043;&#38388;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#24182;&#31361;&#20986;&#20102;&#32531;&#23384;&#25928;&#24212;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33021;&#32791;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#32593;&#32476;&#22823;&#23567;&#12289;&#35745;&#31639;&#21644;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#25581;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08151v1 Announce Type: cross  Abstract: Addressing the so-called ``Red-AI'' trend of rising energy consumption by large-scale neural networks, this study investigates the actual energy consumption, as measured by node-level watt-meters, of training various fully connected neural network architectures. We introduce the BUTTER-E dataset, an augmentation to the BUTTER Empirical Deep Learning dataset, containing energy consumption and performance data from 63,527 individual experimental runs spanning 30,582 distinct configurations: 13 datasets, 20 sizes (number of trainable parameters), 8 network ``shapes'', and 14 depths on both CPU and GPU hardware collected using node-level watt-meters. This dataset reveals the complex relationship between dataset size, network structure, and energy use, and highlights the impact of cache effects. We propose a straightforward and effective energy model that accounts for network size, computing, and memory hierarchy. Our analysis also uncovers
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23376;&#34920;&#31034;&#27169;&#22411;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#22270;&#25991;&#27861;&#25551;&#36848;&#20998;&#23376;&#30340;&#23618;&#27425;&#21270;&#35774;&#35745;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#22312;&#35774;&#35745;&#31354;&#38388;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#23376;&#29983;&#25104;&#21644;&#23646;&#24615;&#39044;&#27979;&#30340;&#24615;&#33021;&#12289;&#25928;&#29575;&#21644;&#21487;&#21512;&#25104;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08147</link><description>&lt;p&gt;
&#23558;&#20998;&#23376;&#34920;&#31034;&#20026;&#21487;&#35299;&#37322;&#30340;&#25991;&#27861;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;
&lt;/p&gt;
&lt;p&gt;
Representing Molecules as Random Walks Over Interpretable Grammars
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08147
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23376;&#34920;&#31034;&#27169;&#22411;&#65292;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#22270;&#25991;&#27861;&#25551;&#36848;&#20998;&#23376;&#30340;&#23618;&#27425;&#21270;&#35774;&#35745;&#31354;&#38388;&#65292;&#23454;&#29616;&#20102;&#22312;&#35774;&#35745;&#31354;&#38388;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#23376;&#29983;&#25104;&#21644;&#23646;&#24615;&#39044;&#27979;&#30340;&#24615;&#33021;&#12289;&#25928;&#29575;&#21644;&#21487;&#21512;&#25104;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20998;&#23376;&#25506;&#32034;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23567;&#22411;&#12289;&#31867;&#20284;&#33647;&#29289;&#30340;&#20998;&#23376;&#19978;&#65292;&#23548;&#33268;&#35768;&#22810;&#22312;&#26448;&#26009;&#35774;&#35745;&#20013;&#21516;&#26679;&#37325;&#35201;&#30340;&#24212;&#29992;&#32570;&#20047;&#36275;&#22815;&#30340;&#25216;&#26415;&#25903;&#25345;&#12290;&#36825;&#20123;&#24212;&#29992;&#36890;&#24120;&#20381;&#36182;&#20110;&#26356;&#22797;&#26434;&#30340;&#20998;&#23376;&#32467;&#26500;&#65292;&#26377;&#26356;&#23569;&#30340;&#20363;&#23376;&#65292;&#26159;&#20351;&#29992;&#24050;&#30693;&#30340;&#20122;&#32467;&#26500;&#31934;&#24515;&#35774;&#35745;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20197;&#22270;&#25991;&#27861;&#30340;&#24418;&#24335;&#34920;&#31034;&#21644;&#25512;&#29702;&#36825;&#20123;&#20998;&#23376;&#65292;&#26126;&#30830;&#25551;&#36848;&#20102;&#29305;&#24449;&#20026;&#35774;&#35745;&#22522;&#30784;&#30340;&#23618;&#27425;&#21270;&#35774;&#35745;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#21363;&#22312;&#35774;&#35745;&#31354;&#38388;&#19978;&#36827;&#34892;&#38543;&#26426;&#28216;&#36208;&#65292;&#26082;&#26377;&#21161;&#20110;&#20998;&#23376;&#29983;&#25104;&#65292;&#21448;&#26377;&#21161;&#20110;&#23646;&#24615;&#39044;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#22312;&#24615;&#33021;&#12289;&#25928;&#29575;&#21644;&#39044;&#27979;&#20998;&#23376;&#21487;&#21512;&#25104;&#24615;&#26041;&#38754;&#30340;&#26126;&#26174;&#20248;&#21183;&#65292;&#24182;&#35814;&#32454;&#38416;&#36848;&#20102;&#35813;&#26041;&#27861;&#30340;&#21270;&#23398;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08147v1 Announce Type: new  Abstract: Recent research in molecular discovery has primarily been devoted to small, drug-like molecules, leaving many similarly important applications in material design without adequate technology. These applications often rely on more complex molecular structures with fewer examples that are carefully designed using known substructures. We propose a data-efficient and interpretable model for representing and reasoning over such molecules in terms of graph grammars that explicitly describe the hierarchical design space featuring motifs to be the design basis. We present a novel representation in the form of random walks over the design space, which facilitates both molecule generation and property prediction. We demonstrate clear advantages over existing methods in terms of performance, efficiency, and synthesizability of predicted molecules, and we provide detailed insights into the method's chemical interpretability.
&lt;/p&gt;</description></item><item><title>&#20248;&#21270;&#25628;&#32034;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20445;&#35777;&#35745;&#31639;&#21487;&#34892;&#24615;&#30340;&#21069;&#25552;&#19979;&#26368;&#22823;&#21270;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#25910;&#30410;&#12290;</title><link>https://arxiv.org/abs/2403.08131</link><description>&lt;p&gt;
&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#20013;&#22797;&#26434;&#35843;&#21442;&#25628;&#32034;&#30340;&#25104;&#26412;&#26377;&#25928;&#26041;&#27861;&#35770;&#65306;&#23548;&#33322;&#30456;&#20114;&#20381;&#36182;&#21644;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Cost-Effective Methodology for Complex Tuning Searches in HPC: Navigating Interdependencies and Dimensionality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08131
&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#25628;&#32034;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20445;&#35777;&#35745;&#31639;&#21487;&#34892;&#24615;&#30340;&#21069;&#25552;&#19979;&#26368;&#22823;&#21270;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#25628;&#32034;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#35299;&#20915;&#35745;&#31639;&#24212;&#29992;&#20013;&#30340;&#22797;&#26434;&#20248;&#21270;&#25361;&#25112;&#12290;&#22797;&#26434;&#24615;&#19981;&#20165;&#26469;&#33258;&#20110;&#23545;&#31243;&#24207;&#20013;&#21442;&#25968;&#30340;&#31934;&#32454;&#35843;&#20248;&#65292;&#36824;&#26469;&#33258;&#20110;&#23427;&#20204;&#20043;&#38388;&#28508;&#22312;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#20351;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#25928;&#29575;&#20302;&#19979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35843;&#25972;&#21644;&#23436;&#21892;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#35745;&#31639;&#21487;&#34892;&#24615;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08131v1 Announce Type: cross  Abstract: Tuning searches are pivotal in High-Performance Computing (HPC), addressing complex optimization challenges in computational applications. The complexity arises not only from finely tuning parameters within routines but also potential interdependencies among them, rendering traditional optimization methods inefficient. Instead of scrutinizing interdependencies among parameters and routines, practitioners often face the dilemma of conducting independent tuning searches for each routine, thereby overlooking interdependence, or pursuing a more resource-intensive joint search for all routines. This decision is driven by the consideration that some interdependence analysis and high-dimensional decomposition techniques in literature may be prohibitively expensive in HPC tuning searches. Our methodology adapts and refines these methods to ensure computational feasibility while maximizing performance gains in real-world scenarios. Our methodol
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24433;&#21709;&#20989;&#25968;&#21644;&#20998;&#24067;&#29420;&#31435;&#21407;&#21017;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#26426;&#22120;&#36951;&#24536;&#20013;&#38750;&#22343;&#21248;&#29305;&#24449;&#21644;&#26631;&#31614;&#21024;&#38500;&#30340;&#25361;&#25112;&#65292;&#20445;&#25252;&#38544;&#31169;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;</title><link>https://arxiv.org/abs/2403.08124</link><description>&lt;p&gt;
&#22312;&#26426;&#22120;&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#36951;&#24536;&#20013;&#26397;&#21521;&#29420;&#31435;&#20934;&#21017;
&lt;/p&gt;
&lt;p&gt;
Towards Independence Criterion in Machine Unlearning of Features and Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08124
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24433;&#21709;&#20989;&#25968;&#21644;&#20998;&#24067;&#29420;&#31435;&#21407;&#21017;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#26426;&#22120;&#36951;&#24536;&#20013;&#38750;&#22343;&#21248;&#29305;&#24449;&#21644;&#26631;&#31614;&#21024;&#38500;&#30340;&#25361;&#25112;&#65292;&#20445;&#25252;&#38544;&#31169;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#28145;&#20837;&#25506;&#35752;&#20102;&#38754;&#20020;&#20998;&#24067;&#36716;&#31227;&#26102;&#30340;&#26426;&#22120;&#36951;&#24536;&#22797;&#26434;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#38750;&#22343;&#21248;&#29305;&#24449;&#21644;&#26631;&#31614;&#21024;&#38500;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#38543;&#30528;GDPR&#31561;&#27861;&#35268;&#24378;&#35843;&#25968;&#25454;&#38544;&#31169;&#21644;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38754;&#20020;&#30528;&#36951;&#24536;&#25935;&#24863;&#20449;&#24687;&#32780;&#19981;&#25439;&#23475;&#20854;&#23436;&#25972;&#24615;&#25110;&#24615;&#33021;&#30340;&#33392;&#24040;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#24433;&#21709;&#20989;&#25968;&#21644;&#20998;&#24067;&#29420;&#31435;&#21407;&#21017;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20840;&#38754;&#30340;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#65292;&#25105;&#20204;&#26088;&#22312;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#20998;&#24067;&#19979;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#26377;&#21161;&#20110;&#39640;&#25928;&#22320;&#21024;&#38500;&#25968;&#25454;&#65292;&#36824;&#21160;&#24577;&#35843;&#25972;&#27169;&#22411;&#20197;&#20445;&#25345;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08124v1 Announce Type: cross  Abstract: This work delves into the complexities of machine unlearning in the face of distributional shifts, particularly focusing on the challenges posed by non-uniform feature and label removal. With the advent of regulations like the GDPR emphasizing data privacy and the right to be forgotten, machine learning models face the daunting task of unlearning sensitive information without compromising their integrity or performance. Our research introduces a novel approach that leverages influence functions and principles of distributional independence to address these challenges. By proposing a comprehensive framework for machine unlearning, we aim to ensure privacy protection while maintaining model performance and adaptability across varying distributions. Our method not only facilitates efficient data removal but also dynamically adjusts the model to preserve its generalization capabilities. Through extensive experimentation, we demonstrate the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#28145;&#24230;&#40784;&#27425;&#31070;&#32463;&#32593;&#32476;&#26102;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#30340;&#21160;&#24577;&#24615;&#65292;&#21457;&#29616;&#22312;&#36275;&#22815;&#23567;&#30340;&#21021;&#22987;&#21270;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#22312;&#35757;&#32451;&#26089;&#26399;&#38454;&#27573;&#20445;&#25345;&#36739;&#23567;&#35268;&#33539;&#65292;&#24182;&#19988;&#27839;&#30528;&#31070;&#32463;&#30456;&#20851;&#20989;&#25968;&#30340;KKT&#28857;&#26041;&#21521;&#36817;&#20284;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2403.08121</link><description>&lt;p&gt;
&#26089;&#26399;&#26041;&#21521;&#24615;&#25910;&#25947;&#22312;&#28145;&#24230;&#40784;&#27425;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#23567;&#21021;&#22987;&#21270;&#26102;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Early Directional Convergence in Deep Homogeneous Neural Networks for Small Initializations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#28145;&#24230;&#40784;&#27425;&#31070;&#32463;&#32593;&#32476;&#26102;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#30340;&#21160;&#24577;&#24615;&#65292;&#21457;&#29616;&#22312;&#36275;&#22815;&#23567;&#30340;&#21021;&#22987;&#21270;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#22312;&#35757;&#32451;&#26089;&#26399;&#38454;&#27573;&#20445;&#25345;&#36739;&#23567;&#35268;&#33539;&#65292;&#24182;&#19988;&#27839;&#30528;&#31070;&#32463;&#30456;&#20851;&#20989;&#25968;&#30340;KKT&#28857;&#26041;&#21521;&#36817;&#20284;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#28145;&#24230;&#40784;&#27425;&#31070;&#32463;&#32593;&#32476;&#26102;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#30340;&#21160;&#24577;&#24615;&#65292;&#36825;&#20123;&#32593;&#32476;&#20174;&#23567;&#21021;&#22987;&#21270;&#24320;&#22987;&#12290;&#26412;&#25991;&#32771;&#34385;&#21040;&#20855;&#26377;&#23616;&#37096;Lipschitz&#26799;&#24230;&#21644;&#38454;&#25968;&#20005;&#26684;&#22823;&#20110;&#20004;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25991;&#31456;&#35777;&#26126;&#20102;&#23545;&#20110;&#36275;&#22815;&#23567;&#30340;&#21021;&#22987;&#21270;&#65292;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#20445;&#25345;&#35268;&#33539;&#36739;&#23567;&#65292;&#24182;&#19988;&#22312;Karush-Kuhn-Tucker (KKT)&#28857;&#22788;&#36817;&#20284;&#27839;&#30528;&#31070;&#32463;&#30456;&#20851;&#20989;&#25968;&#30340;&#26041;&#21521;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#24179;&#26041;&#25439;&#22833;&#24182;&#22312;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#19978;&#36827;&#34892;&#21487;&#20998;&#31163;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#36824;&#23637;&#31034;&#20102;&#22312;&#25439;&#22833;&#20989;&#25968;&#30340;&#26576;&#20123;&#38797;&#28857;&#38468;&#36817;&#26799;&#24230;&#27969;&#21160;&#21160;&#24577;&#30340;&#31867;&#20284;&#26041;&#21521;&#24615;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08121v1 Announce Type: new  Abstract: This paper studies the gradient flow dynamics that arise when training deep homogeneous neural networks, starting with small initializations. The present work considers neural networks that are assumed to have locally Lipschitz gradients and an order of homogeneity strictly greater than two. This paper demonstrates that for sufficiently small initializations, during the early stages of training, the weights of the neural network remain small in norm and approximately converge in direction along the Karush-Kuhn-Tucker (KKT) points of the neural correlation function introduced in [1]. Additionally, for square loss and under a separability assumption on the weights of neural networks, a similar directional convergence of gradient flow dynamics is shown near certain saddle points of the loss function.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25351;&#20986;&#22312;&#26500;&#24314;&#22810;&#20445;&#30495;&#24230;&#20195;&#29702;&#27169;&#22411;&#26102;&#65292;&#26377;&#23475;&#25968;&#25454;&#28304;&#30340;&#29305;&#24449;&#21270;&#26377;&#21161;&#20110;&#25351;&#23548;&#20174;&#19994;&#32773;&#22312;&#36873;&#25321;&#26102;&#20309;&#26102;&#24573;&#30053;&#26576;&#20010;&#25968;&#25454;&#28304;&#12290;</title><link>https://arxiv.org/abs/2403.08118</link><description>&lt;p&gt;
&#30740;&#31350;&#26500;&#24314;&#22810;&#20445;&#30495;&#24230;&#20195;&#29702;&#27169;&#22411;&#26102;&#26377;&#23475;&#25968;&#25454;&#26469;&#28304;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Characterising harmful data sources when constructing multi-fidelity surrogate models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08118
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25351;&#20986;&#22312;&#26500;&#24314;&#22810;&#20445;&#30495;&#24230;&#20195;&#29702;&#27169;&#22411;&#26102;&#65292;&#26377;&#23475;&#25968;&#25454;&#28304;&#30340;&#29305;&#24449;&#21270;&#26377;&#21161;&#20110;&#25351;&#23548;&#20174;&#19994;&#32773;&#22312;&#36873;&#25321;&#26102;&#20309;&#26102;&#24573;&#30053;&#26576;&#20010;&#25968;&#25454;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24403;&#24212;&#29992;&#20110;&#24037;&#19994;&#35774;&#35745;&#38382;&#39064;&#30340;&#24314;&#27169;&#21644;&#20248;&#21270;&#20013;&#65292;&#20195;&#29702;&#24314;&#27169;&#25216;&#26415;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#24403;&#35780;&#20272;&#29305;&#23450;&#35774;&#35745;&#30340;&#24615;&#33021;&#25104;&#26412;&#24456;&#39640;&#26102;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#27169;&#22411;&#20197;&#20195;&#26367;&#21487;&#29992;&#30340;&#39640;&#25104;&#26412;&#26469;&#28304;&#26469;&#26597;&#35810;&#21487;&#20197;&#38477;&#20302;&#24635;&#25104;&#26412;&#12290;&#26500;&#24314;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#21487;&#20197;&#21033;&#29992;&#20854;&#20182;&#20415;&#23452;&#19988;&#19981;&#22826;&#20934;&#30830;&#30340;&#20449;&#24687;&#28304;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20449;&#24687;&#28304;&#30340;&#23384;&#22312;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22312;&#26500;&#24314;&#27169;&#22411;&#26102;&#24212;&#35813;&#20351;&#29992;&#21738;&#20123;&#20449;&#24687;&#28304;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#23545;&#26377;&#23475;&#25968;&#25454;&#28304;&#36827;&#34892;&#29305;&#24449;&#21270;&#65292;&#20197;&#25351;&#23548;&#20174;&#19994;&#32773;&#20309;&#26102;&#24573;&#30053;&#26576;&#20010;&#20449;&#24687;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08118v1 Announce Type: cross  Abstract: Surrogate modelling techniques have seen growing attention in recent years when applied to both modelling and optimisation of industrial design problems. These techniques are highly relevant when assessing the performance of a particular design carries a high cost, as the overall cost can be mitigated via the construction of a model to be queried in lieu of the available high-cost source. The construction of these models can sometimes employ other sources of information which are both cheaper and less accurate. The existence of these sources however poses the question of which sources should be used when constructing a model. Recent studies have attempted to characterise harmful data sources to guide practitioners in choosing when to ignore a certain source. These studies have done so in a synthetic setting, characterising sources using a large amount of data that is not available in practice. Some of these studies have also been shown
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#23610;&#24230;&#19981;&#21464;&#24615;&#30340;&#32806;&#21512;&#36755;&#20837;&#36951;&#24536;&#38376;&#36882;&#24402;&#32593;&#32476;&#65292;&#36890;&#36807;&#20462;&#25913;&#24490;&#29615;&#21333;&#20803;&#20013;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#24555;&#22320;&#25910;&#25947;&#24182;&#22312;&#36328;&#35774;&#22791;&#32852;&#37030;&#23398;&#20064;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.08100</link><description>&lt;p&gt;
&#29992;&#20110;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#30340;&#39640;&#25928;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Efficient Language Model Architectures for Differentially Private Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08100
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#23610;&#24230;&#19981;&#21464;&#24615;&#30340;&#32806;&#21512;&#36755;&#20837;&#36951;&#24536;&#38376;&#36882;&#24402;&#32593;&#32476;&#65292;&#36890;&#36807;&#20462;&#25913;&#24490;&#29615;&#21333;&#20803;&#20013;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#20351;&#20854;&#33021;&#22815;&#26356;&#24555;&#22320;&#25910;&#25947;&#24182;&#22312;&#36328;&#35774;&#22791;&#32852;&#37030;&#23398;&#20064;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35774;&#22791;&#32852;&#37030;&#23398;&#20064;(FL)&#26159;&#19968;&#31181;&#22312;&#36890;&#24120;&#20998;&#24067;&#22312;&#25968;&#30334;&#19975;&#21488;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#32780;&#25968;&#25454;&#19981;&#31163;&#24320;&#35774;&#22791;&#12290; SGD&#26159;&#20132;&#21449;&#35774;&#22791;FL&#20013;&#26631;&#20934;&#30340;&#23458;&#25143;&#31471;&#20248;&#21270;&#22120;&#65292;&#22240;&#20854;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#32780;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#22312;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#30340;&#38598;&#20013;&#24335;&#35757;&#32451;&#20013;&#65292;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#34987;&#35748;&#20026;&#26356;&#31283;&#23450;&#21644;&#24615;&#33021;&#26356;&#22909;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#24819;&#30693;&#36947;&#26159;&#21542;&#21487;&#20197;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#21487;&#20197;&#36890;&#36807;SGD&#23458;&#25143;&#31471;&#20248;&#21270;&#22120;&#36827;&#34892;&#39640;&#25928;&#35757;&#32451;&#65292;&#24182;&#32943;&#23450;&#22320;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#23610;&#24230;&#19981;&#21464;&#24615;&#30340;&#32806;&#21512;&#36755;&#20837;&#36951;&#24536;&#38376;(SI CIFG)&#36882;&#24402;&#32593;&#32476;&#65292;&#36890;&#36807;&#20462;&#25913;&#24490;&#29615;&#21333;&#20803;&#20013;&#30340;Sigmoid&#21644;tanh&#28608;&#27963;&#65292;&#24182;&#23637;&#31034;&#36825;&#20010;&#26032;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#27604;&#26631;&#20934;CIFG&#36882;&#24402;&#27169;&#22411;&#26356;&#24555;&#22320;&#25910;&#25947;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08100v1 Announce Type: new  Abstract: Cross-device federated learning (FL) is a technique that trains a model on data distributed across typically millions of edge devices without data leaving the devices. SGD is the standard client optimizer for on device training in cross-device FL, favored for its memory and computational efficiency. However, in centralized training of neural language models, adaptive optimizers are preferred as they offer improved stability and performance. In light of this, we ask if language models can be modified such that they can be efficiently trained with SGD client optimizers and answer this affirmatively.   We propose a scale-invariant Coupled Input Forget Gate (SI CIFG) recurrent network by modifying the sigmoid and tanh activations in the recurrent cell and show that this new model converges faster and achieves better utility than the standard CIFG recurrent model in cross-device FL in large scale experiments. We further show that the proposed
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#33258;&#27880;&#24847;&#21147;&#23398;&#20064;&#21040;&#19968;&#20010;&#33258;&#21160;&#26426;&#65292;&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20013;&#29983;&#25104;&#26631;&#35760;&#30340;&#20004;&#20010;&#19981;&#21516;&#27493;&#39588;&#26159;&#65306;&#30828;&#26816;&#32034;&#21644;&#36719;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.08081</link><description>&lt;p&gt;
&#20855;&#26377;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Mechanics of Next Token Prediction with Self-Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08081
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#33258;&#27880;&#24847;&#21147;&#23398;&#20064;&#21040;&#19968;&#20010;&#33258;&#21160;&#26426;&#65292;&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20013;&#29983;&#25104;&#26631;&#35760;&#30340;&#20004;&#20010;&#19981;&#21516;&#27493;&#39588;&#26159;&#65306;&#30828;&#26816;&#32034;&#21644;&#36719;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#20197;&#39044;&#27979;&#32473;&#23450;&#36755;&#20837;&#24207;&#21015;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#23613;&#31649;&#35757;&#32451;&#30446;&#26631;&#31616;&#21333;&#65292;&#20294;&#23427;&#20204;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#36827;&#23637;&#12290;&#36825;&#19968;&#25104;&#21151;&#30340;&#22522;&#30784;&#26159;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#19968;&#20010;&#21333;&#29420;&#30340;&#33258;&#27880;&#24847;&#21147;&#23618;&#20174;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20013;&#23398;&#21040;&#20102;&#20160;&#20040;&#65311;&#25105;&#20204;&#23637;&#31034;&#65306;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#33258;&#27880;&#24847;&#21147;&#23398;&#20064;&#21040;&#19968;&#20010;&#33258;&#21160;&#26426;&#65292;&#35813;&#33258;&#21160;&#26426;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;&#27493;&#39588;&#29983;&#25104;&#19979;&#19968;&#20010;&#26631;&#35760;&#65306;(1) &#30828;&#26816;&#32034;&#65306;&#22312;&#32473;&#23450;&#36755;&#20837;&#24207;&#21015;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#27880;&#24847;&#21147;&#31934;&#30830;&#36873;&#25321;&#19982;&#19978;&#19968;&#20010;&#36755;&#20837;&#26631;&#35760;&#30456;&#20851;&#30340;&#39640;&#20248;&#20808;&#32423;&#36755;&#20837;&#26631;&#35760;&#12290;(2) &#36719;&#32452;&#21512;&#65306;&#28982;&#21518;&#65292;&#23427;&#21019;&#24314;&#39640;&#20248;&#20808;&#32423;&#26631;&#35760;&#30340;&#20984;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08081v1 Announce Type: cross  Abstract: Transformer-based language models are trained on large datasets to predict the next token given an input sequence. Despite this simple training objective, they have led to revolutionary advances in natural language processing. Underlying this success is the self-attention mechanism. In this work, we ask: $\textit{What}$ $\textit{does}$ $\textit{a}$ $\textit{single}$ $\textit{self-attention}$ $\textit{layer}$ $\textit{learn}$ $\textit{from}$ $\textit{next-token}$ $\textit{prediction?}$ We show that training self-attention with gradient descent learns an automaton which generates the next token in two distinct steps: $\textbf{(1)}$ $\textbf{Hard}$ $\textbf{retrieval:}$ Given input sequence, self-attention precisely selects the $\textit{high-priority}$ $\textit{input}$ $\textit{tokens}$ associated with the last input token. $\textbf{(2)}$ $\textbf{Soft}$ $\textbf{composition:}$ It then creates a convex combination of the high-priority tok
&lt;/p&gt;</description></item><item><title>FluoroSAM&#26159;&#29992;&#20110;X&#20809;&#22270;&#20687;&#30340;&#20998;&#21106;&#30340;&#35821;&#35328;&#23545;&#40784;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;X&#20809;&#25104;&#20687;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#33258;&#21160;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.08059</link><description>&lt;p&gt;
FluoroSAM: &#29992;&#20110;X&#20809;&#22270;&#20687;&#20998;&#21106;&#30340;&#35821;&#35328;&#23545;&#40784;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FluoroSAM: A Language-aligned Foundation Model for X-ray Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08059
&lt;/p&gt;
&lt;p&gt;
FluoroSAM&#26159;&#29992;&#20110;X&#20809;&#22270;&#20687;&#30340;&#20998;&#21106;&#30340;&#35821;&#35328;&#23545;&#40784;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;X&#20809;&#25104;&#20687;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#33258;&#21160;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;X&#20809;&#22270;&#20687;&#20998;&#21106;&#23558;&#21152;&#36895;&#35786;&#26029;&#21644;&#20171;&#20837;&#31934;&#20934;&#21307;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35299;&#20915;&#29305;&#23450;&#22270;&#20687;&#20998;&#26512;&#38382;&#39064;&#30340;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#25928;&#29992;&#21463;&#38480;&#20110;&#29305;&#23450;&#20219;&#21153;&#39046;&#22495;&#65292;&#35201;&#25299;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#21017;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#12289;&#26631;&#31614;&#21644;&#37325;&#26032;&#35757;&#32451;&#24037;&#20316;&#12290;&#26368;&#36817;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289; - &#35757;&#32451;&#22312;&#22823;&#37327;&#39640;&#24230;&#21464;&#21270;&#25968;&#25454;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22240;&#27492;&#20351;&#24471;&#24191;&#27867;&#36866;&#29992;&#24615;&#25104;&#20026;&#21487;&#33021; - &#24050;&#32463;&#25104;&#20026;&#33258;&#21160;&#22270;&#20687;&#20998;&#26512;&#30340;&#26377;&#24076;&#26395;&#30340;&#24037;&#20855;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;FMs&#32858;&#28966;&#20110;&#23545;&#35937;&#34987;&#26126;&#26174;&#21487;&#35265;&#36793;&#30028;&#28165;&#26224;&#23450;&#20041;&#30340;&#22330;&#26223;&#21644;&#27169;&#24335;&#65292;&#22914;&#20869;&#31397;&#38236;&#25163;&#26415;&#24037;&#20855;&#20998;&#21106;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;X&#20809;&#25104;&#20687;&#36890;&#24120;&#27809;&#26377;&#25552;&#20379;&#36825;&#31181;&#28165;&#26224;&#30340;&#36793;&#30028;&#25110;&#32467;&#26500;&#20808;&#39564;&#12290;&#22312;X&#20809;&#22270;&#20687;&#24418;&#25104;&#26399;&#38388;&#65292;&#22797;&#26434;&#30340;&#19977;&#32500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08059v1 Announce Type: cross  Abstract: Automated X-ray image segmentation would accelerate research and development in diagnostic and interventional precision medicine. Prior efforts have contributed task-specific models capable of solving specific image analysis problems, but the utility of these models is restricted to their particular task domain, and expanding to broader use requires additional data, labels, and retraining efforts. Recently, foundation models (FMs) -- machine learning models trained on large amounts of highly variable data thus enabling broad applicability -- have emerged as promising tools for automated image analysis. Existing FMs for medical image analysis focus on scenarios and modalities where objects are clearly defined by visually apparent boundaries, such as surgical tool segmentation in endoscopy. X-ray imaging, by contrast, does not generally offer such clearly delineated boundaries or structure priors. During X-ray image formation, complex 3D
&lt;/p&gt;</description></item><item><title>CHAI&#25552;&#20986;&#20102;Clustered Head Attention&#65288;CHAI&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36816;&#34892;&#26102;&#32467;&#21512;&#20855;&#26377;&#39640;&#30456;&#20851;&#24615;&#30340;&#27880;&#24847;&#21147;&#22836;&#37096;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#21644;&#35745;&#31639;&#37327;&#65292;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;&#23384;&#20648;K,V&#32531;&#23384;&#30340;&#20869;&#23384;&#38656;&#27714;&#38477;&#20302;21.4&#65285;&#65292;&#25512;&#29702;&#26102;&#38388;&#24310;&#36831;&#38477;&#20302;1.73&#20493;&#12290;</title><link>https://arxiv.org/abs/2403.08058</link><description>&lt;p&gt;
CHAI&#65306;&#39640;&#25928;LLM&#25512;&#29702;&#30340;&#32858;&#31867;&#22836;&#37096;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
CHAI: Clustered Head Attention for Efficient LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08058
&lt;/p&gt;
&lt;p&gt;
CHAI&#25552;&#20986;&#20102;Clustered Head Attention&#65288;CHAI&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36816;&#34892;&#26102;&#32467;&#21512;&#20855;&#26377;&#39640;&#30456;&#20851;&#24615;&#30340;&#27880;&#24847;&#21147;&#22836;&#37096;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#21644;&#35745;&#31639;&#37327;&#65292;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;&#23384;&#20648;K,V&#32531;&#23384;&#30340;&#20869;&#23384;&#38656;&#27714;&#38477;&#20302;21.4&#65285;&#65292;&#25512;&#29702;&#26102;&#38388;&#24310;&#36831;&#38477;&#20302;1.73&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25317;&#26377;&#25968;&#30334;&#20159;&#21442;&#25968;&#25913;&#21464;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#29702;&#26102;&#20026;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#26082;&#38656;&#35201;&#35745;&#31639;&#21448;&#38656;&#35201;&#20869;&#23384;&#65292;&#19968;&#20010;&#35831;&#27714;&#21487;&#33021;&#38656;&#35201;&#22810;&#20010;GPU&#21644;&#25968;&#21313;GB&#30340;&#20869;&#23384;&#12290;&#22810;&#22836;&#27880;&#24847;&#21147;&#26159;LLMs&#30340;&#20851;&#38190;&#32452;&#20214;&#20043;&#19968;&#65292;&#21487;&#20197;&#21344;LLMs&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;50%&#20197;&#19978;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#21508;&#22836;&#20043;&#38388;&#23545;&#27880;&#24847;&#21147;&#30340;&#20851;&#27880;&#26377;&#24456;&#39640;&#30340;&#20887;&#20313;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Clustered Head Attention (CHAI)&#12290;CHAI&#22312;&#36816;&#34892;&#26102;&#23558;&#20855;&#26377;&#39640;&#30456;&#20851;&#24615;&#30340;&#22836;&#37096;&#32467;&#21512;&#36827;&#34892;&#33258;&#27880;&#24847;&#21147;&#65292;&#20174;&#32780;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CHAI&#33021;&#22815;&#23558;&#23384;&#20648;K,V&#32531;&#23384;&#30340;&#20869;&#23384;&#38656;&#27714;&#38477;&#20302;&#22810;&#36798;21.4%&#65292;&#25512;&#29702;&#26102;&#24310;&#36831;&#38477;&#20302;&#22810;&#36798;1.73&#20493;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#24494;&#35843;&#12290;CHAI&#23454;&#29616;&#20102;&#26368;&#22810;3.2%&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08058v1 Announce Type: cross  Abstract: Large Language Models (LLMs) with hundreds of billions of parameters have transformed the field of machine learning. However, serving these models at inference time is both compute and memory intensive, where a single request can require multiple GPUs and tens of Gigabytes of memory. Multi-Head Attention is one of the key components of LLMs, which can account for over 50% of LLMs memory and compute requirement. We observe that there is a high amount of redundancy across heads on which tokens they pay attention to. Based on this insight, we propose Clustered Head Attention (CHAI). CHAI combines heads with a high amount of correlation for self-attention at runtime, thus reducing both memory and compute. In our experiments, we show that CHAI is able to reduce the memory requirements for storing K,V cache by up to 21.4% and inference time latency by up to 1.73x without any fine-tuning required. CHAI achieves this with a maximum 3.2% deviat
&lt;/p&gt;</description></item><item><title>DrivAerNet&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#39640;&#20445;&#30495;&#24230;&#30340;&#27773;&#36710;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#24037;&#31243;&#24212;&#29992;&#20013;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#65292;&#32780;RegDGCNN&#21033;&#29992;&#36825;&#19968;&#25968;&#25454;&#38598;&#30452;&#25509;&#20174;3D&#32593;&#26684;&#25552;&#20379;&#39640;&#31934;&#24230;&#30340;&#38459;&#21147;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.08055</link><description>&lt;p&gt;
DrivAerNet&#65306;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#27668;&#21160;&#35774;&#35745;&#21644;&#22522;&#20110;&#22270;&#30340;&#38459;&#21147;&#39044;&#27979;&#30340;&#21442;&#25968;&#21270;&#27773;&#36710;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DrivAerNet: A Parametric Car Dataset for Data-Driven Aerodynamic Design and Graph-Based Drag Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08055
&lt;/p&gt;
&lt;p&gt;
DrivAerNet&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#39640;&#20445;&#30495;&#24230;&#30340;&#27773;&#36710;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#24037;&#31243;&#24212;&#29992;&#20013;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#65292;&#32780;RegDGCNN&#21033;&#29992;&#36825;&#19968;&#25968;&#25454;&#38598;&#30452;&#25509;&#20174;3D&#32593;&#26684;&#25552;&#20379;&#39640;&#31934;&#24230;&#30340;&#38459;&#21147;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102; DrivAerNet&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#39640;&#20445;&#30495;&#24230;&#30340;CFD&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;3D&#24037;&#19994;&#26631;&#20934;&#27773;&#36710;&#24418;&#29366;&#65292;&#20197;&#21450; RegDGCNN&#65292;&#36825;&#26159;&#19968;&#20010;&#21160;&#24577;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#27773;&#36710;&#27668;&#21160;&#35774;&#35745;&#12290;DrivAerNet&#25317;&#26377;4000&#20010;&#35814;&#32454;&#30340;3D&#27773;&#36710;&#32593;&#26684;&#65292;&#20351;&#29992;50&#19975;&#20010;&#34920;&#38754;&#32593;&#26684;&#38754;&#21644;&#20840;&#38754;&#27668;&#21160;&#24615;&#33021;&#25968;&#25454;&#65292;&#21253;&#25324;&#23436;&#25972;&#30340;3D&#21387;&#21147;&#12289;&#36895;&#24230;&#22330;&#21644;&#22721;&#38754;&#21098;&#20999;&#24212;&#21147;&#65292;&#28385;&#36275;&#20102;&#24037;&#31243;&#24212;&#29992;&#20013;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#23427;&#27604;&#20808;&#21069;&#21487;&#29992;&#30340;&#26368;&#22823;&#20844;&#24320;&#27773;&#36710;&#25968;&#25454;&#38598;&#22823;60&#65285;&#65292;&#20063;&#26159;&#21807;&#19968;&#21516;&#26102;&#27169;&#25311;&#36718;&#27586;&#21644;&#24213;&#30424;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#12290;RegDGCNN&#21033;&#29992;&#36825;&#19968;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#30452;&#25509;&#20174;3D&#32593;&#26684;&#25552;&#20379;&#39640;&#31934;&#24230;&#30340;&#38459;&#21147;&#20272;&#35745;&#65292;&#32469;&#36807;&#20102;&#20256;&#32479;&#38480;&#21046;&#65292;&#22914;&#38656;&#35201;2D&#22270;&#20687;&#28210;&#26579;&#25110;&#31526;&#21495;&#36317;&#31163;&#22330;&#65288;SDF&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08055v1 Announce Type: new  Abstract: This study introduces DrivAerNet, a large-scale high-fidelity CFD dataset of 3D industry-standard car shapes, and RegDGCNN, a dynamic graph convolutional neural network model, both aimed at aerodynamic car design through machine learning. DrivAerNet, with its 4000 detailed 3D car meshes using 0.5 million surface mesh faces and comprehensive aerodynamic performance data comprising of full 3D pressure, velocity fields, and wall-shear stresses, addresses the critical need for extensive datasets to train deep learning models in engineering applications. It is 60\% larger than the previously available largest public dataset of cars, and is the only open-source dataset that also models wheels and underbody. RegDGCNN leverages this large-scale dataset to provide high-precision drag estimates directly from 3D meshes, bypassing traditional limitations such as the need for 2D image rendering or Signed Distance Fields (SDF). By enabling fast drag e
&lt;/p&gt;</description></item><item><title>TutoAI &#26159;&#19968;&#20010;&#36328;&#39046;&#22495;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29289;&#29702;&#20219;&#21153;&#19978;&#21033;&#29992;AI&#36741;&#21161;&#28151;&#21512;&#23186;&#20307;&#25945;&#31243;&#21019;&#24314;&#65292;&#36890;&#36807;&#35843;&#26597;&#24120;&#35265;&#25945;&#31243;&#32452;&#20214;&#12289;&#35780;&#20272;AI&#27169;&#22411;&#25552;&#21462;&#32452;&#20214;&#30340;&#26041;&#27861;&#20197;&#21450;&#35774;&#35745;UI&#25903;&#25345;&#25945;&#31243;&#21019;&#24314;&#30340;&#25351;&#21335;&#65292;&#35777;&#26126;&#20102;&#20854;&#36739;&#22522;&#20934;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#25110;&#30456;&#20284;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.08049</link><description>&lt;p&gt;
TutoAI&#65306;&#29992;&#20110;&#29289;&#29702;&#20219;&#21153;&#30340;&#36328;&#39046;&#22495; AI &#36741;&#21161;&#28151;&#21512;&#23186;&#20307;&#25945;&#31243;&#21019;&#20316;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TutoAI: A Cross-domain Framework for AI-assisted Mixed-media Tutorial Creation on Physical Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08049
&lt;/p&gt;
&lt;p&gt;
TutoAI &#26159;&#19968;&#20010;&#36328;&#39046;&#22495;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29289;&#29702;&#20219;&#21153;&#19978;&#21033;&#29992;AI&#36741;&#21161;&#28151;&#21512;&#23186;&#20307;&#25945;&#31243;&#21019;&#24314;&#65292;&#36890;&#36807;&#35843;&#26597;&#24120;&#35265;&#25945;&#31243;&#32452;&#20214;&#12289;&#35780;&#20272;AI&#27169;&#22411;&#25552;&#21462;&#32452;&#20214;&#30340;&#26041;&#27861;&#20197;&#21450;&#35774;&#35745;UI&#25903;&#25345;&#25945;&#31243;&#21019;&#24314;&#30340;&#25351;&#21335;&#65292;&#35777;&#26126;&#20102;&#20854;&#36739;&#22522;&#20934;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#25110;&#30456;&#20284;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#23186;&#20307;&#25945;&#31243;&#23558;&#35270;&#39057;&#12289;&#22270;&#29255;&#12289;&#25991;&#26412;&#21644;&#22270;&#34920;&#25972;&#21512;&#36215;&#26469;&#65292;&#20197;&#25945;&#25480;&#36807;&#31243;&#25216;&#33021;&#65292;&#25552;&#20379;&#27604;&#22522;&#20110;&#26102;&#38388;&#36724;&#30340;&#35270;&#39057;&#26356;&#20855;&#21487;&#27983;&#35272;&#24615;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#21019;&#24314;&#27492;&#31867;&#25945;&#31243;&#26159;&#20047;&#21619;&#30340;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#23616;&#38480;&#20110;&#29305;&#23450;&#39046;&#22495;&#12290;&#34429;&#28982; AI &#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#23427;&#20204;&#30340;&#33021;&#21147;&#23578;&#19981;&#28165;&#26970;&#65292;&#32771;&#34385;&#21040;&#25152;&#28041;&#21450;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#24191;&#38420;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; TutoAI&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#29289;&#29702;&#20219;&#21153;&#30340;&#36328;&#39046;&#22495; AI &#36741;&#21161;&#28151;&#21512;&#23186;&#20307;&#25945;&#31243;&#21019;&#20316;&#30340;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#35843;&#26597;&#29616;&#26377;&#24037;&#20316;&#65292;&#25105;&#20204;&#25552;&#28860;&#20102;&#24120;&#35265;&#30340;&#25945;&#31243;&#32452;&#20214;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35782;&#21035;&#12289;&#32452;&#35013;&#21644;&#35780;&#20272;&#29992;&#20110;&#32452;&#20214;&#25552;&#21462;&#30340; AI &#27169;&#22411;&#30340;&#26041;&#27861;&#65307;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20026;&#22522;&#20110; AI &#29983;&#25104;&#30340;&#32452;&#20214;&#25903;&#25345;&#25945;&#31243;&#21019;&#24314;&#30340;&#29992;&#25143;&#30028;&#38754;&#65288;UI&#65289;&#35774;&#35745;&#25351;&#21335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; TutoAI &#22312;&#36136;&#37327;&#19978;&#36798;&#21040;&#25110;&#39640;&#20110;&#22522;&#20934;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08049v1 Announce Type: cross  Abstract: Mixed-media tutorials, which integrate videos, images, text, and diagrams to teach procedural skills, offer more browsable alternatives than timeline-based videos. However, manually creating such tutorials is tedious, and existing automated solutions are often restricted to a particular domain. While AI models hold promise, it is unclear how to effectively harness their powers, given the multi-modal data involved and the vast landscape of models. We present TutoAI, a cross-domain framework for AI-assisted mixed-media tutorial creation on physical tasks. First, we distill common tutorial components by surveying existing work; then, we present an approach to identify, assemble, and evaluate AI models for component extraction; finally, we propose guidelines for designing user interfaces (UI) that support tutorial creation based on AI-generated components. We show that TutoAI has achieved higher or similar quality compared to a baseline mo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;2D&#21644;3D&#26684;&#24335;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#27668;&#36947;&#30149;&#21464;&#20307;&#31215;&#20998;&#21106;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;3D&#27169;&#22411;&#22312;&#25429;&#25417;&#22797;&#26434;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#24322;&#65292;&#24182;&#36890;&#36807;&#23545;2D&#27169;&#22411;&#23454;&#26045;&#32454;&#24494;&#32467;&#26500;&#20998;&#21106;&#25439;&#22833;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#22806;&#37096;&#39564;&#35777;&#35777;&#23454;&#20102;&#30740;&#31350;&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;</title><link>https://arxiv.org/abs/2403.08042</link><description>&lt;p&gt;
CT&#35780;&#20272;2D&#21644;3D&#25972;&#20307;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#27668;&#36947;&#30149;&#21464;&#30340;&#20307;&#31215;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
CT evaluation of 2D and 3D holistic deep learning methods for the volumetric segmentation of airway lesions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;2D&#21644;3D&#26684;&#24335;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#27668;&#36947;&#30149;&#21464;&#20307;&#31215;&#20998;&#21106;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;3D&#27169;&#22411;&#22312;&#25429;&#25417;&#22797;&#26434;&#29305;&#24449;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#24322;&#65292;&#24182;&#36890;&#36807;&#23545;2D&#27169;&#22411;&#23454;&#26045;&#32454;&#24494;&#32467;&#26500;&#20998;&#21106;&#25439;&#22833;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#22806;&#37096;&#39564;&#35777;&#35777;&#23454;&#20102;&#30740;&#31350;&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#22312;2D&#21644;3D&#26684;&#24335;&#20013;&#30340;&#25972;&#20307;&#20998;&#21106;&#33021;&#21147;&#36827;&#34892;&#20102;&#27604;&#36739;&#25506;&#35752;&#65292;&#37325;&#28857;&#20851;&#27880;&#22218;&#24615;&#32420;&#32500;&#21270;&#65288;CF&#65289;&#30149;&#21464;&#12290;&#30740;&#31350;&#21033;&#29992;&#20102;&#26469;&#33258;&#20004;&#20010;CF&#21442;&#32771;&#20013;&#24515;&#30340;&#25968;&#25454;&#65292;&#28085;&#30422;&#20102;&#20116;&#20010;&#20027;&#35201;&#30340;CF&#32467;&#26500;&#21464;&#21270;&#12290;&#39318;&#20808;&#27604;&#36739;&#20102;2D&#21644;3D&#27169;&#22411;&#65292;&#31361;&#20986;&#20102;3D&#27169;&#22411;&#22312;&#25429;&#25417;&#31896;&#28082;&#26643;&#21644;&#23454;&#21464;&#31561;&#22797;&#26434;&#29305;&#24449;&#26041;&#38754;&#30340;&#20248;&#36234;&#33021;&#21147;&#12290;&#20026;&#20102;&#25552;&#39640;2D&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#26045;&#21644;&#35780;&#20272;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#32454;&#24494;&#32467;&#26500;&#20998;&#21106;&#30340;&#25439;&#22833;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#20934;&#30830;&#24615;&#65292;&#23613;&#31649;&#27809;&#26377;&#36229;&#36234;3D&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27169;&#22411;&#32463;&#36807;&#36827;&#19968;&#27493;&#36890;&#36807;&#23545;&#32954;&#21151;&#33021;&#27979;&#35797;&#65288;PFTs&#65289;&#30340;&#22806;&#37096;&#35780;&#20272;&#36827;&#34892;&#39564;&#35777;&#65292;&#30830;&#35748;&#20102;&#30740;&#31350;&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#39033;&#30740;&#31350;&#19981;&#20165;&#38480;&#20110;&#27604;&#36739;&#25351;&#26631;&#65307;&#36824;&#21253;&#25324;&#23545;&#27169;&#22411;&#35299;&#37322;&#24615;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08042v1 Announce Type: cross  Abstract: This research embarked on a comparative exploration of the holistic segmentation capabilities of Convolutional Neural Networks (CNNs) in both 2D and 3D formats, focusing on cystic fibrosis (CF) lesions. The study utilized data from two CF reference centers, covering five major CF structural changes. Initially, it compared the 2D and 3D models, highlighting the 3D model's superior capability in capturing complex features like mucus plugs and consolidations. To improve the 2D model's performance, a loss adapted to fine structures segmentation was implemented and evaluated, significantly enhancing its accuracy, though not surpassing the 3D model's performance. The models underwent further validation through external evaluation against pulmonary function tests (PFTs), confirming the robustness of the findings. Moreover, this study went beyond comparing metrics; it also included comprehensive assessments of the models' interpretability and 
&lt;/p&gt;</description></item><item><title>MicroT&#26159;&#19968;&#20010;&#20302;&#33021;&#32791;&#12289;&#22810;&#20219;&#21153;&#33258;&#36866;&#24212;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#20998;&#31867;&#22120;&#30340;&#20998;&#31163;&#12289;&#27169;&#22411;&#20248;&#21270;&#21644;&#26412;&#22320;&#20219;&#21153;&#35757;&#32451;&#65292;&#22312;MCUs&#19978;&#23454;&#29616;&#20102;&#27169;&#22411;&#24615;&#33021;&#30340;&#25552;&#21319;&#21644;&#33021;&#32791;&#30340;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2403.08040</link><description>&lt;p&gt;
MicroT&#65306;&#29992;&#20110;MCUs&#30340;&#20302;&#33021;&#32791;&#21644;&#33258;&#36866;&#24212;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MicroT: Low-Energy and Adaptive Models for MCUs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08040
&lt;/p&gt;
&lt;p&gt;
MicroT&#26159;&#19968;&#20010;&#20302;&#33021;&#32791;&#12289;&#22810;&#20219;&#21153;&#33258;&#36866;&#24212;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#20998;&#31867;&#22120;&#30340;&#20998;&#31163;&#12289;&#27169;&#22411;&#20248;&#21270;&#21644;&#26412;&#22320;&#20219;&#21153;&#35757;&#32451;&#65292;&#22312;MCUs&#19978;&#23454;&#29616;&#20102;&#27169;&#22411;&#24615;&#33021;&#30340;&#25552;&#21319;&#21644;&#33021;&#32791;&#30340;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MicroT&#65292;&#36825;&#26159;&#19968;&#20010;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#30340;MCUs&#30340;&#20302;&#33021;&#32791;&#12289;&#22810;&#20219;&#21153;&#33258;&#36866;&#24212;&#27169;&#22411;&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;&#21407;&#22987;&#27169;&#22411;&#21010;&#20998;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#20998;&#31867;&#22120;&#12290;&#29305;&#24449;&#25552;&#21462;&#22120;&#36890;&#36807;&#33258;&#30417;&#30563;&#30693;&#35782;&#33976;&#39311;&#33719;&#24471;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#20998;&#21106;&#21644;&#32852;&#21512;&#35757;&#32451;&#36827;&#19968;&#27493;&#20248;&#21270;&#20026;&#37096;&#20998;&#27169;&#22411;&#21644;&#23436;&#25972;&#27169;&#22411;&#12290;&#28982;&#21518;&#23558;&#36825;&#20123;&#27169;&#22411;&#37096;&#32626;&#22312;MCUs&#19978;&#65292;&#22686;&#21152;&#24182;&#22312;&#26412;&#22320;&#20219;&#21153;&#19978;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#26368;&#32456;&#25191;&#34892;&#20851;&#33410;&#25512;&#29702;&#30340;&#38454;&#27573;&#20915;&#31574;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#37096;&#20998;&#27169;&#22411;&#26368;&#21021;&#22788;&#29702;&#26679;&#26412;&#65292;&#22914;&#26524;&#32622;&#20449;&#24230;&#24471;&#20998;&#20302;&#20110;&#35774;&#23450;&#30340;&#38408;&#20540;&#65292;&#23436;&#25972;&#27169;&#22411;&#23558;&#24674;&#22797;&#24182;&#32487;&#32493;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#27169;&#22411;&#12289;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;MCU&#26495;&#19978;&#35780;&#20272;&#20102;MicroT&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#22788;&#29702;&#22810;&#20010;&#26412;&#22320;&#20219;&#21153;&#26102;&#65292;MicroT&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#24182;&#38477;&#20302;&#20102;&#33021;&#32791;&#12290;&#19982;&#26410;&#32463;&#20248;&#21270;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#30456;&#27604;&#65292;MicroT
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08040v1 Announce Type: new  Abstract: We propose MicroT, a low-energy, multi-task adaptive model framework for resource-constrained MCUs. We divide the original model into a feature extractor and a classifier. The feature extractor is obtained through self-supervised knowledge distillation and further optimized into part and full models through model splitting and joint training. These models are then deployed on MCUs, with classifiers added and trained on local tasks, ultimately performing stage-decision for joint inference. In this process, the part model initially processes the sample, and if the confidence score falls below the set threshold, the full model will resume and continue the inference. We evaluate MicroT on two models, three datasets, and two MCU boards. Our experimental evaluation shows that MicroT effectively improves model performance and reduces energy consumption when dealing with multiple local tasks. Compared to the unoptimized feature extractor, MicroT
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; McCatch &#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#8220;Oracle&#8221;&#22270;&#26469;&#26816;&#27979;&#24494;&#31751;&#65292;&#26159;&#30446;&#21069;&#21807;&#19968;&#21487;&#22238;&#31572;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#22312;&#22788;&#29702;&#38750;&#32500;&#24230;&#25968;&#25454;&#20197;&#21450;&#20855;&#26377;&#38750;&#21333;&#28857;&#24494;&#31751;&#26102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08027</link><description>&lt;p&gt;
McCatch&#65306;&#32500;&#24230;&#21644;&#38750;&#32500;&#24230;&#25968;&#25454;&#20013;&#21487;&#25193;&#23637;&#30340;&#24494;&#31751;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
McCatch: Scalable Microcluster Detection in Dimensional and Nondimensional Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08027
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; McCatch &#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#8220;Oracle&#8221;&#22270;&#26469;&#26816;&#27979;&#24494;&#31751;&#65292;&#26159;&#30446;&#21069;&#21807;&#19968;&#21487;&#22238;&#31572;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#22312;&#22788;&#29702;&#38750;&#32500;&#24230;&#25968;&#25454;&#20197;&#21450;&#20855;&#26377;&#38750;&#21333;&#28857;&#24494;&#31751;&#26102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#35774;&#35745;&#19968;&#20010;&#21487;&#20197;&#22788;&#29702;&#38750;&#32500;&#24230;&#25968;&#25454;&#65292;&#24182;&#19988;&#20197;&#24322;&#24120;&#20998;&#25968;&#23545;&#21333;&#28857;&#24494;&#31751;&#65288;"&#19968;&#27425;&#24615;"&#24322;&#24120;&#20540;&#65289;&#21644;&#38750;&#21333;&#28857;&#24494;&#31751;&#36827;&#34892;&#25490;&#21517;&#30340;&#31163;&#32676;&#28857;&#26816;&#27979;&#22120;&#65311;&#22914;&#20309;&#20197;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#8220;&#26080;&#38656;&#24178;&#39044;&#8221;&#30340;&#26041;&#24335;&#33719;&#24471;&#21512;&#29702;&#30340;&#20998;&#25968;&#65311;&#24322;&#24120;&#20540;&#30340;&#24494;&#31751;&#25351;&#31034;&#20102;&#27450;&#35784;&#27963;&#21160;&#20013;&#30340;&#32852;&#21512;&#25110;&#37325;&#22797;&#65292;&#22240;&#27492;&#23427;&#20204;&#30340;&#35782;&#21035;&#26159;&#38750;&#24120;&#21487;&#21462;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; McCatch &#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#8220;Oracle&#8221;&#22270;&#65288;1NN&#36317;&#31163;&#19982;&#32452;1NN&#36317;&#31163;&#65289;&#26469;&#26816;&#27979;&#24494;&#31751;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;31&#20010;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#39640;&#36798;100&#19975;&#20010;&#25968;&#25454;&#20803;&#32032;&#65292;&#20197;&#23637;&#31034; McCatch &#26159;&#21807;&#19968;&#21487;&#20197;&#22238;&#31572;&#19978;&#36848;&#20004;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65307;&#23427;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20854;&#20182;11&#31181;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#24403;&#25968;&#25454;&#20855;&#26377;&#38750;&#21333;&#28857;&#24494;&#31751;&#25110;&#20026;&#38750;&#32500;&#24230;&#26102;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102; McCatch &#22312;&#22270;&#24418;&#12289;&#25351;&#32441;&#12289;&#32593;&#32476;&#26085;&#24535;&#20013;&#26816;&#27979;&#26377;&#24847;&#20041;&#24494;&#31751;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08027v1 Announce Type: new  Abstract: How could we have an outlier detector that works even with nondimensional data, and ranks together both singleton microclusters ('one-off' outliers) and nonsingleton microclusters by their anomaly scores? How to obtain scores that are principled in one scalable and 'hands-off' manner? Microclusters of outliers indicate coalition or repetition in fraud activities, etc.; their identification is thus highly desirable. This paper presents McCatch: a new algorithm that detects microclusters by leveraging our proposed 'Oracle' plot (1NN Distance versus Group 1NN Distance). We study 31 real and synthetic datasets with up to 1M data elements to show that McCatch is the only method that answers both of the questions above; and, it outperforms 11 other methods, especially when the data has nonsingleton microclusters or is nondimensional. We also showcase McCatch's ability to detect meaningful microclusters in graphs, fingerprints, logs of network 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;xMLP&#65292;&#36825;&#26159;&#19968;&#31181;&#29420;&#29305;&#30340;DNN&#26550;&#26500;&#65292;&#20351;&#29992;&#29420;&#21344;&#30340;&#26041;&#28608;&#27963;&#65292;&#22312;&#32500;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20943;&#23569;&#31169;&#23494;&#25512;&#26029;&#31995;&#32479;&#20013;&#30340;&#24310;&#36831;</title><link>https://arxiv.org/abs/2403.08024</link><description>&lt;p&gt;
xMLP&#65306;&#21033;&#29992;&#29420;&#21344;&#26041;&#28608;&#27963;&#38761;&#21629;&#21270;&#31169;&#23494;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
xMLP: Revolutionizing Private Inference with Exclusive Square Activation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;xMLP&#65292;&#36825;&#26159;&#19968;&#31181;&#29420;&#29305;&#30340;DNN&#26550;&#26500;&#65292;&#20351;&#29992;&#29420;&#21344;&#30340;&#26041;&#28608;&#27963;&#65292;&#22312;&#32500;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20943;&#23569;&#31169;&#23494;&#25512;&#26029;&#31995;&#32479;&#20013;&#30340;&#24310;&#36831;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31169;&#23494;&#25512;&#26029;&#65288;PI&#65289;&#36890;&#36807;&#21033;&#29992;&#23494;&#30721;&#21407;&#35821;&#65292;&#22914;&#22810;&#26041;&#35745;&#31639;&#65288;MPC&#65289;&#21644;&#21516;&#24577;&#21152;&#23494;&#65288;HE&#65289;&#65292;&#20351;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#33021;&#22815;&#22312;&#31169;&#20154;&#25968;&#25454;&#19978;&#36816;&#34892;&#65292;&#32780;&#19981;&#20250;&#27844;&#38706;&#25935;&#24863;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;PI&#31995;&#32479;&#20013;&#20351;&#29992;&#35832;&#22914;ReLU&#20043;&#31867;&#30340;&#38750;&#32447;&#24615;&#28608;&#27963;&#21487;&#33021;&#23548;&#33268;&#19981;&#20999;&#23454;&#38469;&#30340;&#39640;PI&#24310;&#36831;&#65292;&#22240;&#20026;ReLU&#38656;&#35201;&#20351;&#29992;&#26114;&#36149;&#30340;MPC&#35745;&#31639;&#65292;&#22914;Garbled Circuits&#12290;&#30001;&#20110;&#19982;ReLU&#30456;&#27604;&#65292;&#26041;&#28608;&#27963;&#21487;&#36890;&#36807;&#27604;Beaver&#30340;&#19977;&#20803;&#32452;&#24555;&#25968;&#30334;&#20493;&#30340;&#36895;&#24230;&#36827;&#34892;&#22788;&#29702;&#65292;&#22240;&#27492;&#26356;&#36866;&#21512;PI&#20219;&#21153;&#65292;&#20294;&#26159;&#20351;&#29992;&#23427;&#20204;&#20250;&#23548;&#33268;&#27169;&#22411;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#12290;&#26412;&#25991;&#39318;&#20808;&#25506;&#35752;&#20102;&#22312;&#20351;&#29992;&#26041;&#28608;&#27963;&#21518;&#20986;&#29616;&#20934;&#30830;&#24615;&#19979;&#38477;&#30340;&#21407;&#22240;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#21363;&#36825;&#26159;&#30001;&#20110;&#8220;&#20449;&#24687;&#22797;&#21512;&#8221;&#25928;&#24212;&#12290;&#21033;&#29992;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;xMLP&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#29420;&#21344;&#26041;&#28608;&#27963;&#30340;&#26032;&#39062;DNN&#26550;&#26500;&#65292;&#21516;&#26102;&#22312;&#20004;&#32773;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08024v1 Announce Type: new  Abstract: Private Inference (PI) enables deep neural networks (DNNs) to work on private data without leaking sensitive information by exploiting cryptographic primitives such as multi-party computation (MPC) and homomorphic encryption (HE). However, the use of non-linear activations such as ReLU in DNNs can lead to impractically high PI latency in existing PI systems, as ReLU requires the use of costly MPC computations, such as Garbled Circuits. Since square activations can be processed by Beaver's triples hundreds of times faster compared to ReLU, they are more friendly to PI tasks, but using them leads to a notable drop in model accuracy. This paper starts by exploring the reason for such an accuracy drop after using square activations, and concludes that this is due to an "information compounding" effect. Leveraging this insight, we propose xMLP, a novel DNN architecture that uses square activations exclusively while maintaining parity in both 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#26469;&#33258;&#37326;&#22806;&#21644;&#23454;&#39564;&#23460;&#21365;&#65292;&#24182;&#27979;&#35797;&#20102;&#19977;&#31181;&#31070;&#32463;&#32593;&#32476;&#22312;&#20234;&#27663;&#20234;&#34442;&#21365;&#35745;&#25968;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.08016</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20234;&#27663;&#20234;&#34442;&#21365;&#35745;&#25968;&#30340;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Aedes aegypti Egg Counting with Neural Networks for Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08016
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30446;&#26631;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#26469;&#33258;&#37326;&#22806;&#21644;&#23454;&#39564;&#23460;&#21365;&#65292;&#24182;&#27979;&#35797;&#20102;&#19977;&#31181;&#31070;&#32463;&#32593;&#32476;&#22312;&#20234;&#27663;&#20234;&#34442;&#21365;&#35745;&#25968;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20234;&#27663;&#20234;&#34442;&#20173;&#28982;&#26159;&#30142;&#30149;&#20256;&#25773;&#23186;&#20171;&#26102;&#30340;&#20027;&#35201;&#20851;&#27880;&#23545;&#35937;&#12290;&#22312;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#30340;&#35768;&#22810;&#26041;&#27861;&#20013;&#65292;&#26377;&#19968;&#20123;&#37325;&#35201;&#30340;&#21327;&#35758;&#21033;&#29992;&#21365;&#25968;&#22312;&#21365;&#32599;&#25429;&#22120;&#20013;&#26469;&#35745;&#31639;&#25351;&#26631;&#65292;&#22914;LIRAa&#21644;Breteau&#25351;&#25968;&#65292;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#21487;&#39044;&#27979;&#30340;&#26292;&#21457;&#21644;&#27969;&#34892;&#30149;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#38656;&#35201;&#21365;&#25968;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#22823;&#35268;&#27169;&#22521;&#32946;&#34442;&#23376;&#26102;&#12290;&#21365;&#35745;&#25968;&#26159;&#19968;&#20010;&#32321;&#29712;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#20219;&#21153;&#65292;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30446;&#26631;&#26816;&#27979;&#26469;&#23454;&#29616;&#33258;&#21160;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#37326;&#22806;&#21644;&#23454;&#39564;&#23460;&#30340;&#21365;&#65292;&#20197;&#21450;&#19977;&#31181;&#24212;&#29992;&#20110;&#35813;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#27979;&#35797;&#32467;&#26524;&#65306;Faster R-CNN&#12289;Side-Aware Boundary Localization&#21644;FoveaBox&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08016v1 Announce Type: cross  Abstract: Aedes aegypti is still one of the main concerns when it comes to disease vectors. Among the many ways to deal with it, there are important protocols that make use of egg numbers in ovitraps to calculate indices, such as the LIRAa and the Breteau Index, which can provide information on predictable outbursts and epidemics. Also, there are many research lines that require egg numbers, specially when mass production of mosquitoes is needed. Egg counting is a laborious and error-prone task that can be automated via computer vision-based techniques, specially deep learning-based counting with object detection. In this work, we propose a new dataset comprising field and laboratory eggs, along with test results of three neural networks applied to the task: Faster R-CNN, Side-Aware Boundary Localization and FoveaBox.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#22522;&#20110;&#27169;&#25311;&#25968;&#25454;&#30340;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23545;&#26102;&#38388;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#21644;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#20915;&#31574;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.08013</link><description>&lt;p&gt;
&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22312;&#28023;&#24213;&#24037;&#31243;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Supervised Time Series Classification for Anomaly Detection in Subsea Engineering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08013
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#22522;&#20110;&#27169;&#25311;&#25968;&#25454;&#30340;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23545;&#26102;&#38388;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#21644;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#20915;&#31574;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22312;&#30417;&#27979;&#32467;&#26500;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22522;&#20110;&#20855;&#26377;&#20004;&#31181;&#29366;&#24577;&#65288;&#23436;&#25972;&#21644;&#30772;&#35010;&#65289;&#30340;&#29289;&#29702;&#31995;&#32479;&#30340;&#27169;&#25311;&#25968;&#25454;&#19978;&#20351;&#29992;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#31639;&#27861;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#26102;&#38388;&#25968;&#25454;&#30340;&#39044;&#22788;&#29702;&#36827;&#34892;&#20102;&#20840;&#38754;&#35752;&#35770;&#65292;&#20351;&#29992;&#32479;&#35745;&#24046;&#24322;&#24230;&#37327;&#21644;&#38477;&#32500;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#25928;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#24615;&#33021;&#25351;&#26631;&#23545;&#21508;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#26174;&#31034;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20316;&#20026;&#20915;&#31574;&#24037;&#20855;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08013v1 Announce Type: new  Abstract: Time series classification is of significant importance in monitoring structural systems. In this work, we investigate the use of supervised machine learning classification algorithms on simulated data based on a physical system with two states: Intact and Broken. We provide a comprehensive discussion of the preprocessing of temporal data, using measures of statistical dispersion and dimension reduction techniques. We present an intuitive baseline method and discuss its efficiency. We conclude with a comparison of the various methods based on different performance metrics, showing the advantage of using machine learning techniques as a tool in decision making.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#36755;&#20986;&#20013;&#20197;&#27599;&#23618;&#26377;&#30417;&#30563;&#30340;&#26041;&#24335;&#23545;&#21333;&#35789;&#21644;&#23383;&#31526;&#30340;&#35821;&#35328;ID&#26465;&#20214;&#21270;&#21464;&#21387;&#22120;&#23618;&#65292;&#35813;&#26041;&#27861;&#34429;&#28982;&#26410;&#33021;&#26174;&#33879;&#38477;&#20302;&#35789;&#38169;&#35823;&#29575;&#65292;&#20294;&#23637;&#29616;&#20102;&#22312;&#20165;&#20165;&#36890;&#36807;&#21475;&#35821;&#25968;&#25454;&#39044;&#27979;&#27491;&#30830;&#35821;&#35328;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08011</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#25104;&#39044;&#27979;&#21475;&#35821;&#35328;&#35782;&#21035;&#30340;&#21476;&#21513;&#25289;&#29305;&#35821;-&#33521;&#35821;&#28151;&#21512;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Gujarati-English Code-Switching Speech Recognition using ensemble prediction of spoken language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08011
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#36755;&#20986;&#20013;&#20197;&#27599;&#23618;&#26377;&#30417;&#30563;&#30340;&#26041;&#24335;&#23545;&#21333;&#35789;&#21644;&#23383;&#31526;&#30340;&#35821;&#35328;ID&#26465;&#20214;&#21270;&#21464;&#21387;&#22120;&#23618;&#65292;&#35813;&#26041;&#27861;&#34429;&#28982;&#26410;&#33021;&#26174;&#33879;&#38477;&#20302;&#35789;&#38169;&#35823;&#29575;&#65292;&#20294;&#23637;&#29616;&#20102;&#22312;&#20165;&#20165;&#36890;&#36807;&#21475;&#35821;&#25968;&#25454;&#39044;&#27979;&#27491;&#30830;&#35821;&#35328;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#35821;&#38899;&#35782;&#21035;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#22256;&#38590;&#30340;&#20219;&#21153;&#26159;&#35782;&#21035;&#35821;&#35328;&#65292;&#22240;&#20026;&#20004;&#31181;&#35821;&#35328;&#20013;&#30340;&#35768;&#22810;&#35789;&#22312;&#26576;&#20123;&#21475;&#38899;&#19979;&#21548;&#36215;&#26469;&#30456;&#20284;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#36890;&#36807;&#22312;&#36755;&#20986;&#20013;&#20197;&#27599;&#23618;&#26377;&#30417;&#30563;&#30340;&#26041;&#24335;&#23545;&#21333;&#35789;&#21644;&#23383;&#31526;&#30340;&#35821;&#35328;ID&#26465;&#20214;&#21270;&#21464;&#21387;&#22120;&#23618;&#65292;&#20197;&#25913;&#21892;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#24341;&#20837;&#35821;&#35328;&#29305;&#23450;&#21442;&#25968;&#21644;&#21487;&#35299;&#37322;&#24615;&#21040;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#26045;&#20102;&#19968;&#20010;&#26377;&#21161;&#20110;&#20445;&#25345;&#36755;&#20837;&#23545;&#40784;&#36830;&#32493;&#24615;&#30340;&#26102;&#38388;&#25439;&#22833;&#12290;&#23613;&#31649;&#26080;&#27861;&#26174;&#33879;&#38477;&#20302;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20102;&#22312;&#20165;&#20165;&#36890;&#36807;&#21475;&#35821;&#25968;&#25454;&#39044;&#27979;&#27491;&#30830;&#35821;&#35328;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#24207;&#21015;&#20013;&#21024;&#38500;LID&#24341;&#20837;&#20102;&#35821;&#35328;&#39044;&#27979;&#30340;&#27491;&#21017;&#21270;&#65292;&#26377;&#21161;&#20110;&#23545;&#40784;&#38271;&#37325;&#22797;&#30340;&#36755;&#20986;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08011v1 Announce Type: cross  Abstract: An important and difficult task in code-switched speech recognition is to recognize the language, as lots of words in two languages can sound similar, especially in some accents. We focus on improving performance of end-to-end Automatic Speech Recognition models by conditioning transformer layers on language ID of words and character in the output in an per layer supervised manner. To this end, we propose two methods of introducing language specific parameters and explainability in the multi-head attention mechanism, and implement a Temporal Loss that helps maintain continuity in input alignment. Despite being unable to reduce WER significantly, our method shows promise in predicting the correct language from just spoken data. We introduce regularization in the language prediction by dropping LID in the sequence, which helps align long repeated output sequences.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#20851;&#20110;&#38899;&#20048;&#32467;&#26500;&#24314;&#27169;&#30340;&#25216;&#26415;&#28436;&#21464;&#65292;&#21253;&#25324;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23558;&#38899;&#20048;&#29983;&#25104;&#20998;&#35299;&#20026;&#39640;&#23618;&#32467;&#26500;&#35268;&#21010;&#21644;&#20869;&#23481;&#21019;&#24314;&#38454;&#27573;&#12290;</title><link>https://arxiv.org/abs/2403.07995</link><description>&lt;p&gt;
&#20027;&#39064;&#12289;&#30701;&#35821;&#21450;&#20854;&#36827;&#23637;&#65306;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#32467;&#26500;&#30340;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Motifs, Phrases, and Beyond: The Modelling of Structure in Symbolic Music Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07995
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20013;&#20851;&#20110;&#38899;&#20048;&#32467;&#26500;&#24314;&#27169;&#30340;&#25216;&#26415;&#28436;&#21464;&#65292;&#21253;&#25324;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23558;&#38899;&#20048;&#29983;&#25104;&#20998;&#35299;&#20026;&#39640;&#23618;&#32467;&#26500;&#35268;&#21010;&#21644;&#20869;&#23481;&#21019;&#24314;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#38899;&#20048;&#32467;&#26500;&#23545;&#20110;&#29983;&#25104;&#31526;&#21495;&#38899;&#20048;&#20316;&#21697;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#32454;&#33268;&#20998;&#26512;&#20102;&#21508;&#31181;&#25216;&#26415;&#30340;&#28436;&#21464;&#65292;&#20174;&#31526;&#21495;&#26041;&#27861;&#21040;&#22522;&#30784;&#21644;&#38761;&#21629;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#35745;&#31639;&#21644;&#25968;&#25454;&#30340;&#21147;&#37327;&#36328;&#36234;&#21508;&#31181;&#35757;&#32451;&#33539;&#24335;&#65292;&#20197;&#25972;&#21512;&#36830;&#36143;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07995v1 Announce Type: cross  Abstract: Modelling musical structure is vital yet challenging for artificial intelligence systems that generate symbolic music compositions. This literature review dissects the evolution of techniques for incorporating coherent structure, from symbolic approaches to foundational and transformative deep learning methods that harness the power of computation and data across a wide variety of training paradigms. In the later stages, we review an emerging technique which we refer to as "sub-task decomposition" that involves decomposing music generation into separate high-level structural planning and content creation stages. Such systems incorporate some form of musical knowledge or neuro-symbolic methods by extracting melodic skeletons or structural templates to guide the generation. Progress is evident in capturing motifs and repetitions across all three eras reviewed, yet modelling the nuanced development of themes across extended compositions i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29983;&#25104;&#24335;&#22686;&#24378;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#36890;&#36807;&#22522;&#20110;&#24819;&#35937;&#21147;&#30340;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#26469;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07979</link><description>&lt;p&gt;
&#22810;&#26234;&#20307;&#26159;&#21542;&#26790;&#35265;&#30005;&#23376;&#32650;&#65311;&#65306;&#36890;&#36807;&#29983;&#25104;&#24335;&#23398;&#20064;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Do Agents Dream of Electric Sheep?: Improving Generalization in Reinforcement Learning through Generative Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07979
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#24335;&#22686;&#24378;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#36890;&#36807;&#22522;&#20110;&#24819;&#35937;&#21147;&#30340;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#26469;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#25311;&#21512;&#30340;&#22823;&#33041;&#20551;&#35774;&#34920;&#26126;&#26790;&#30340;&#21457;&#29983;&#26159;&#20026;&#20102;&#35753;&#20154;&#31867;&#22823;&#33041;&#36827;&#34892;&#27867;&#21270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35810;&#38382;&#26159;&#21542;&#23545;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20063;&#26159;&#22914;&#27492;&#12290;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#32463;&#39564;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#24819;&#35937;&#21147;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#22312;&#31867;&#20284;&#26790;&#22659;&#30340;&#24773;&#33410;&#20013;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#37027;&#37324;&#65292;&#23545;&#38750;&#24819;&#35937;&#21147;&#12289;&#39044;&#27979;&#30340;&#36712;&#36857;&#36827;&#34892;&#29983;&#25104;&#24615;&#22686;&#24378;&#12290;&#22312;&#22235;&#20010;ProcGen&#29615;&#22659;&#30340;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#19982;&#32463;&#20856;&#30340;&#24819;&#35937;&#21147;&#21644;&#31163;&#32447;&#35757;&#32451;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#26102;&#21487;&#20197;&#36798;&#21040;&#26356;&#39640;&#30340;&#27867;&#21270;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07979v1 Announce Type: cross  Abstract: The Overfitted Brain hypothesis suggests dreams happen to allow generalization in the human brain. Here, we ask if the same is true for reinforcement learning agents as well. Given limited experience in a real environment, we use imagination-based reinforcement learning to train a policy on dream-like episodes, where non-imaginative, predicted trajectories are modified through generative augmentations. Experiments on four ProcGen environments show that, compared to classic imagination and offline training on collected experience, our method can reach a higher level of generalization when dealing with sparsely rewarded environments.
&lt;/p&gt;</description></item><item><title>LiveCodeBench&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#26080;&#27745;&#26579;&#30340;LLMs&#35780;&#20272;&#24037;&#20855;&#65292;&#32858;&#28966;&#20110;&#20174;LeetCode&#12289;AtCoder&#21644;CodeForces&#31561;&#24179;&#21488;&#36830;&#32493;&#25910;&#38598;&#30340;&#26032;&#38382;&#39064;&#65292;&#35206;&#30422;&#33258;&#20462;&#22797;&#12289;&#20195;&#30721;&#25191;&#34892;&#12289;&#27979;&#35797;&#36755;&#20986;&#39044;&#27979;&#31561;&#26356;&#24191;&#27867;&#30340;&#20195;&#30721;&#30456;&#20851;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.07974</link><description>&lt;p&gt;
LiveCodeBench&#65306;&#29992;&#20110;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#21644;&#26080;&#27745;&#26579;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07974
&lt;/p&gt;
&lt;p&gt;
LiveCodeBench&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#26080;&#27745;&#26579;&#30340;LLMs&#35780;&#20272;&#24037;&#20855;&#65292;&#32858;&#28966;&#20110;&#20174;LeetCode&#12289;AtCoder&#21644;CodeForces&#31561;&#24179;&#21488;&#36830;&#32493;&#25910;&#38598;&#30340;&#26032;&#38382;&#39064;&#65292;&#35206;&#30422;&#33258;&#20462;&#22797;&#12289;&#20195;&#30721;&#25191;&#34892;&#12289;&#27979;&#35797;&#36755;&#20986;&#39044;&#27979;&#31561;&#26356;&#24191;&#27867;&#30340;&#20195;&#30721;&#30456;&#20851;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#24212;&#29992;&#31243;&#24207;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#31361;&#20986;&#30340;&#39046;&#22495;&#65292;&#21560;&#24341;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#26032;&#30340;&#21644;&#25913;&#36827;&#30340;LLMs&#30340;&#24320;&#21457;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#22522;&#20934;&#65288;&#20363;&#22914;HumanEval&#65292;MBPP&#65289;&#19981;&#20877;&#36275;&#20197;&#35780;&#20272;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;LiveCodeBench&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#26080;&#27745;&#26579;&#30340;LLMs&#35780;&#20272;&#24037;&#20855;&#65292;&#29992;&#20110;&#20195;&#30721;&#65292;&#23427;&#20250;&#20174;&#19977;&#20010;&#31454;&#36187;&#24179;&#21488;&#65288;LeetCode&#12289;AtCoder&#21644;CodeForces&#65289;&#19978;&#36830;&#32493;&#22320;&#25910;&#38598;&#26032;&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#36824;&#30528;&#37325;&#20851;&#27880;&#26356;&#24191;&#27867;&#30340;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#33021;&#21147;&#65292;&#22914;&#33258;&#20462;&#22797;&#12289;&#20195;&#30721;&#25191;&#34892;&#21644;&#27979;&#35797;&#36755;&#20986;&#39044;&#27979;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20195;&#30721;&#29983;&#25104;&#12290;&#30446;&#21069;&#65292;LiveCodeBench&#25176;&#31649;&#20102;&#22312;2023&#24180;5&#26376;&#33267;2024&#24180;2&#26376;&#20043;&#38388;&#21457;&#24067;&#30340;400&#20010;&#39640;&#36136;&#37327;&#32534;&#30721;&#38382;&#39064;&#12290;&#25105;&#20204;&#24050;&#32463;&#35780;&#20272;&#20102;9&#20010;&#22522;&#26412;LLMs&#21644;20&#20010;&#25351;&#20196;&#35843;&#25972;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07974v1 Announce Type: cross  Abstract: Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and February 2024. We have evaluated 9 base LLMs and 20 instruction-tuned LLMs o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;KnowCoder&#65292;&#19968;&#20010;&#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#25191;&#34892;&#26222;&#36866;&#20449;&#24687;&#25552;&#21462;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#20195;&#30721;&#39118;&#26684;&#30340;&#27169;&#24335;&#34920;&#31034;&#26041;&#27861;&#21644;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;LLMs&#23545;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#20934;&#30830;&#25552;&#21462;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.07969</link><description>&lt;p&gt;
KnowCoder&#65306;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#32534;&#30721;&#21040;LLMs&#20013;&#29992;&#20110;&#26222;&#36866;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;KnowCoder&#65292;&#19968;&#20010;&#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#25191;&#34892;&#26222;&#36866;&#20449;&#24687;&#25552;&#21462;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#20195;&#30721;&#39118;&#26684;&#30340;&#27169;&#24335;&#34920;&#31034;&#26041;&#27861;&#21644;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;LLMs&#23545;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#20934;&#30830;&#25552;&#21462;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KnowCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29992;&#20110;&#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#36827;&#34892;&#26222;&#36866;&#20449;&#24687;&#25552;&#21462;&#65288;UIE&#65289;&#12290;KnowCoder&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24335;&#34920;&#31034;&#65292;&#20351;LLMs&#33021;&#22815;&#36731;&#26494;&#29702;&#35299;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#40723;&#21169;LLMs&#36981;&#24490;&#27169;&#24335;&#24182;&#20934;&#30830;&#25552;&#21462;&#32467;&#26500;&#21270;&#30693;&#35782;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;KnowCoder&#24341;&#20837;&#20102;&#19968;&#31181;&#20195;&#30721;&#39118;&#26684;&#30340;&#27169;&#24335;&#34920;&#31034;&#26041;&#27861;&#65292;&#23558;&#19981;&#21516;&#30340;&#27169;&#24335;&#32479;&#19968;&#36716;&#25442;&#20026;Python&#31867;&#65292;&#20174;&#32780;&#21487;&#20197;&#20197;LLM&#21451;&#22909;&#30340;&#26041;&#24335;&#25429;&#25417;UIE&#20013;&#20219;&#21153;&#20043;&#38388;&#30340;&#32422;&#26463;&#31561;&#22797;&#26434;&#27169;&#24335;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;30,000&#31181;&#30693;&#35782;&#31867;&#22411;&#30340;&#20195;&#30721;&#39118;&#26684;&#27169;&#24335;&#24211;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;UIE&#20013;&#26368;&#22823;&#30340;&#24211;&#12290;&#20026;&#20102;&#31616;&#21270;LLMs&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;KnowCoder&#21253;&#21547;&#19968;&#20010;&#36890;&#36807;&#20195;&#30721;&#39044;&#35757;&#32451;&#22686;&#24378;&#20854;&#27169;&#24335;&#29702;&#35299;&#33021;&#21147;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07969v1 Announce Type: cross  Abstract: In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct Universal Information Extraction (UIE) via code generation. KnowCoder aims to develop a kind of unified schema representation that LLMs can easily understand and an effective learning framework that encourages LLMs to follow schemas and extract structured knowledge accurately. To achieve these, KnowCoder introduces a code-style schema representation method to uniformly transform different schemas into Python classes, with which complex schema information, such as constraints among tasks in UIE, can be captured in an LLM-friendly manner. We further construct a code-style schema library covering over $\textbf{30,000}$ types of knowledge, which is the largest one for UIE, to the best of our knowledge. To ease the learning process of LLMs, KnowCoder contains a two-phase learning framework that enhances its schema understanding ability via code pretraining and its 
&lt;/p&gt;</description></item><item><title>SGD&#35299;&#20915;&#26041;&#26696;&#38598;&#26159;&#19968;&#20010;&#26143;&#24418;&#22495;&#65292;&#21253;&#21547;&#19968;&#20010;&#26143;&#24418;&#27169;&#22411;&#65292;&#36890;&#36807;&#20302;&#25439;&#22833;&#25968;&#20540;&#30340;&#36335;&#24452;&#19982;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#32447;&#24615;&#30456;&#36830;&#65292;&#27169;&#38500;&#25490;&#21015;&#12290;</title><link>https://arxiv.org/abs/2403.07968</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#26159;&#21542;&#24418;&#25104;&#26143;&#24418;&#21306;&#22495;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Deep Neural Network Solutions Form a Star Domain?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07968
&lt;/p&gt;
&lt;p&gt;
SGD&#35299;&#20915;&#26041;&#26696;&#38598;&#26159;&#19968;&#20010;&#26143;&#24418;&#22495;&#65292;&#21253;&#21547;&#19968;&#20010;&#26143;&#24418;&#27169;&#22411;&#65292;&#36890;&#36807;&#20302;&#25439;&#22833;&#25968;&#20540;&#30340;&#36335;&#24452;&#19982;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#32447;&#24615;&#30456;&#36830;&#65292;&#27169;&#38500;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Entezari&#31561;&#20154;&#65288;2022&#65289;&#25512;&#27979;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21487;&#36798;&#21040;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#38598;&#26159;&#20984;&#30340;&#65292;&#32771;&#34385;&#21040;&#25490;&#21015;&#19981;&#21464;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#21152;&#23485;&#26494;&#30340;&#35266;&#28857;&#65306;SGD&#35299;&#20915;&#26041;&#26696;&#38598;&#26159;&#19968;&#20010;&#26143;&#24418;&#22495;&#65292;&#21253;&#21547;&#19968;&#20010;&#26143;&#24418;&#27169;&#22411;&#65292;&#36890;&#36807;&#20302;&#25439;&#22833;&#25968;&#20540;&#30340;&#36335;&#24452;&#19982;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#32447;&#24615;&#30456;&#36830;&#65292;&#27169;&#38500;&#25490;&#21015;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Starlight&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#32473;&#23450;&#23398;&#20064;&#20219;&#21153;&#30340;&#26143;&#24418;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#36825;&#20010;&#26143;&#24418;&#27169;&#22411;&#19982;&#20854;&#20182;&#29420;&#31435;&#25214;&#21040;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#32447;&#24615;&#30456;&#36830;&#30340;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07968v1 Announce Type: cross  Abstract: Entezari et al. (2022) conjectured that neural network solution sets reachable via stochastic gradient descent (SGD) are convex, considering permutation invariances. This means that two independent solutions can be connected by a linear path with low loss, given one of them is appropriately permuted. However, current methods to test this theory often fail to eliminate loss barriers between two independent solutions (Ainsworth et al., 2022; Benzing et al., 2022). In this work, we conjecture that a more relaxed claim holds: the SGD solution set is a star domain that contains a star model that is linearly connected to all the other solutions via paths with low loss values, modulo permutations. We propose the Starlight algorithm that finds a star model of a given learning task. We validate our claim by showing that this star model is linearly connected with other independently found solutions. As an additional benefit of our study, we demo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#22320;&#22312;&#27700;&#31291;&#25910;&#33719;&#21069;&#20960;&#20010;&#26376;&#39044;&#27979;&#21360;&#24230;&#21508;&#21306; Kharif &#23395;&#33410;&#27700;&#31291;&#30340;&#20135;&#37327;&#65292;&#30740;&#31350;&#35777;&#26126;&#27700;&#31291;&#20135;&#37327;&#21487;&#20197;&#34987;&#21512;&#29702;&#20934;&#30830;&#22320;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.07967</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27668;&#20505;&#20877;&#20998;&#26512;&#25968;&#25454;&#22312;&#21360;&#24230;&#21306;&#32423;&#27700;&#31291;&#20135;&#37327;&#39044;&#27979;&#20013;&#30340;&#21487;&#34892;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Feasibility of machine learning-based rice yield prediction in India at the district level using climate reanalysis data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07967
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#22320;&#22312;&#27700;&#31291;&#25910;&#33719;&#21069;&#20960;&#20010;&#26376;&#39044;&#27979;&#21360;&#24230;&#21508;&#21306; Kharif &#23395;&#33410;&#27700;&#31291;&#30340;&#20135;&#37327;&#65292;&#30740;&#31350;&#35777;&#26126;&#27700;&#31291;&#20135;&#37327;&#21487;&#20197;&#34987;&#21512;&#29702;&#20934;&#30830;&#22320;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20135;&#37327;&#39044;&#27979;&#26159;&#22312;&#24196;&#31292;&#25910;&#21106;&#20043;&#21069;&#39044;&#27979;&#20892;&#19994;&#29983;&#20135;&#21147;&#30340;&#31185;&#23398;&#65292;&#26377;&#21161;&#20110;&#24191;&#27867;&#30340;&#21033;&#30410;&#30456;&#20851;&#26041;&#20570;&#20986;&#26356;&#22909;&#30340;&#20892;&#19994;&#35268;&#21010;&#20915;&#31574;&#12290;&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#26159;&#21542;&#26426;&#22120;&#23398;&#20064;-based &#30340;&#20135;&#37327;&#39044;&#27979;&#27169;&#22411;&#33021;&#22815;&#22312;&#21360;&#24230;&#21306;&#32423;&#20934;&#30830;&#22320;&#39044;&#27979; Kharif &#23395;&#33410;&#30340;&#27700;&#31291;&#20135;&#37327;&#65292;&#21363;&#22312;&#27700;&#31291;&#25910;&#33719;&#20960;&#20010;&#26376;&#21069;&#12290;&#26041;&#27861;&#28041;&#21450;&#23545;&#21360;&#24230;&#29983;&#20135;&#27700;&#31291;&#30340; 247 &#20010;&#21306;&#30340; 20 &#24180;&#27668;&#20505;&#12289;&#21355;&#26143;&#21644;&#27700;&#31291;&#20135;&#37327;&#25968;&#25454;&#36827;&#34892; 19 &#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#22914; CatBoost&#12289;LightGBM&#12289;Orthogonal Matching Pursuit &#21644; Extremely Randomized Trees&#12290;&#38500;&#20102;&#27169;&#22411;&#26500;&#24314;&#65292;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#21160;&#24577;&#20202;&#34920;&#26495;&#65292;&#20102;&#35299;&#27700;&#31291;&#20135;&#37327;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#22312;&#21508;&#21306;&#20043;&#38388;&#26159;&#22914;&#20309;&#21464;&#21270;&#30340;&#12290;&#27010;&#24565;&#39564;&#35777;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27700;&#31291;&#20135;&#37327;&#21487;&#20197;&#34987;&#21512;&#29702;&#20934;&#30830;&#22320;&#39044;&#27979;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07967v1 Announce Type: new  Abstract: Yield forecasting, the science of predicting agricultural productivity before the crop harvest occurs, helps a wide range of stakeholders make better decisions around agricultural planning. This study aims to investigate whether machine learning-based yield prediction models can capably predict Kharif season rice yields at the district level in India several months before the rice harvest takes place. The methodology involved training 19 machine learning models such as CatBoost, LightGBM, Orthogonal Matching Pursuit, and Extremely Randomized Trees on 20 years of climate, satellite, and rice yield data across 247 of Indian rice-producing districts. In addition to model-building, a dynamic dashboard was built understand how the reliability of rice yield predictions varies across districts. The results of the proof-of-concept machine learning pipeline demonstrated that rice yields can be predicted with a reasonable degree of accuracy, with 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#22914;&#20309;&#20998;&#26512;&#22320;&#29699;&#31995;&#32479;&#21464;&#37327;&#23545;&#28201;&#24230;&#39044;&#25253;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#20010;&#21019;&#26032;&#65306;&#24212;&#29992;&#25968;&#25454;&#31185;&#23398;&#26041;&#27861;&#22312;&#20195;&#34920;&#24615;&#20301;&#32622;&#36827;&#34892;&#25490;&#21517;&#12289;&#21033;&#29992;Spearman&#30456;&#20851;&#24615;&#21019;&#24314;&#25490;&#21517;&#24182;&#32467;&#21512;&#20854;&#20182;&#24230;&#37327;&#20016;&#23500;&#25490;&#21517;&#12289;&#36890;&#36807;&#23398;&#20064;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#35770;&#65292;&#26368;&#32456;&#23558;&#30456;&#20851;&#24615;&#36716;&#21270;&#20026;&#25490;&#21517;&#24182;&#32452;&#21512;&#25104;&#19968;&#20010;&#24635;&#20307;&#25490;&#21517;&#12290;</title><link>https://arxiv.org/abs/2403.07966</link><description>&lt;p&gt;
&#24212;&#29992;&#25490;&#21517;&#25216;&#26415;&#20272;&#35745;&#22320;&#29699;&#21464;&#37327;&#23545;&#28201;&#24230;&#39044;&#25253;&#35823;&#24046;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Applying ranking techniques for estimating influence of Earth variables on temperature forecast error
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#22914;&#20309;&#20998;&#26512;&#22320;&#29699;&#31995;&#32479;&#21464;&#37327;&#23545;&#28201;&#24230;&#39044;&#25253;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#20010;&#21019;&#26032;&#65306;&#24212;&#29992;&#25968;&#25454;&#31185;&#23398;&#26041;&#27861;&#22312;&#20195;&#34920;&#24615;&#20301;&#32622;&#36827;&#34892;&#25490;&#21517;&#12289;&#21033;&#29992;Spearman&#30456;&#20851;&#24615;&#21019;&#24314;&#25490;&#21517;&#24182;&#32467;&#21512;&#20854;&#20182;&#24230;&#37327;&#20016;&#23500;&#25490;&#21517;&#12289;&#36890;&#36807;&#23398;&#20064;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#35770;&#65292;&#26368;&#32456;&#23558;&#30456;&#20851;&#24615;&#36716;&#21270;&#20026;&#25490;&#21517;&#24182;&#32452;&#21512;&#25104;&#19968;&#20010;&#24635;&#20307;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#22914;&#20309;&#20998;&#26512;&#22320;&#29699;&#31995;&#32479;&#21464;&#37327;&#23545;&#28201;&#24230;&#39044;&#25253;&#35823;&#24046;&#30340;&#24433;&#21709;&#12290;&#33719;&#21462;&#25968;&#25454;&#30340;&#21021;&#22987;&#26694;&#26550;&#22522;&#20110;&#20808;&#21069;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#38750;&#24120;&#26377;&#36259;&#30340;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#21069;&#36848;&#30740;&#31350;&#20165;&#20165;&#38024;&#23545;&#21464;&#37327;&#19982;&#35823;&#24046;&#30340;&#20010;&#20307;&#30456;&#20851;&#24615;&#36827;&#34892;&#24037;&#20316;&#12290;&#26412;&#30740;&#31350;&#23558;&#37325;&#26032;&#20351;&#29992;&#20027;&#35201;&#24605;&#24819;&#65292;&#20294;&#24341;&#20837;&#20102;&#19977;&#20010;&#20027;&#35201;&#21019;&#26032;&#65306;(1)&#36890;&#36807;&#23569;&#25968;&#20195;&#34920;&#24615;&#20301;&#32622;&#24212;&#29992;&#25968;&#25454;&#31185;&#23398;&#26041;&#27861;&#65307;(2)&#21033;&#29992;Spearman&#30456;&#20851;&#24615;&#21019;&#24314;&#30340;&#25490;&#21517;&#65292;&#20294;&#29992;&#20854;&#20182;&#24230;&#37327;&#20016;&#23500;&#23427;&#20204;&#20197;&#23547;&#25214;&#26356;&#31283;&#20581;&#30340;&#21464;&#37327;&#25490;&#21517;&#65307;(3)&#36890;&#36807;&#23398;&#20064;&#20855;&#26377;&#19981;&#21516;&#23454;&#39564;&#21464;&#21270;&#30340;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#26469;&#35780;&#20272;&#26041;&#27861;&#35770;&#12290;&#20854;&#20027;&#35201;&#36129;&#29486;&#26159;&#23637;&#31034;&#22914;&#20309;&#23558;&#30456;&#20851;&#24615;&#36716;&#21270;&#20026;&#25490;&#21517;&#65292;&#24182;&#23558;&#23427;&#20204;&#21512;&#24182;&#25104;&#19968;&#20010;&#24635;&#20307;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07966v1 Announce Type: new  Abstract: This paper describes how to analyze the influence of Earth system variables on the errors when providing temperature forecasts. The initial framework to get the data has been based on previous research work, which resulted in a very interesting discovery. However, the aforementioned study only worked on individual correlations of the variables with respect to the error. This research work is going to re-use the main ideas but introduce three main novelties: (1) applying a data science approach by a few representative locations; (2) taking advantage of the rankings created by Spearman correlation but enriching them with other metrics looking for a more robust ranking of the variables; (3) evaluation of the methodology by learning random forest models for regression with the distinct experimental variations. The main contribution is the framework that shows how to convert correlations into rankings and combine them into an aggregate rankin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#23558;&#26465;&#20214;&#35745;&#31639;&#26041;&#27861;&#24212;&#29992;&#20110;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#20852;&#39046;&#22495;&#20013;&#30340;&#21407;&#29702;&#21644;&#24605;&#24819;&#65292;&#24182;&#20171;&#32461;&#20102;&#19987;&#23478;&#28151;&#21512;&#32593;&#32476;&#12289;&#26631;&#35760;&#36873;&#25321;&#26426;&#21046;&#21644;&#25552;&#21069;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#31561;&#19977;&#31181;&#23454;&#29616;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.07965</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26465;&#20214;&#35745;&#31639;: &#21407;&#29702;&#19982;&#30740;&#31350;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Conditional computation in neural networks: principles and research trends
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07965
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#23558;&#26465;&#20214;&#35745;&#31639;&#26041;&#27861;&#24212;&#29992;&#20110;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#20852;&#39046;&#22495;&#20013;&#30340;&#21407;&#29702;&#21644;&#24605;&#24819;&#65292;&#24182;&#20171;&#32461;&#20102;&#19987;&#23478;&#28151;&#21512;&#32593;&#32476;&#12289;&#26631;&#35760;&#36873;&#25321;&#26426;&#21046;&#21644;&#25552;&#21069;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#31561;&#19977;&#31181;&#23454;&#29616;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#24635;&#32467;&#20102;&#23558;&#26465;&#20214;&#35745;&#31639;&#26041;&#27861;&#24212;&#29992;&#20110;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#20852;&#39046;&#22495;&#20013;&#30340;&#21407;&#29702;&#21644;&#24605;&#24819;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#21487;&#20197;&#26681;&#25454;&#36755;&#20837;&#21160;&#24577;&#28608;&#27963;&#25110;&#21435;&#28608;&#27963;&#20854;&#35745;&#31639;&#22270;&#37096;&#20998;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20363;&#22914;&#65292;&#21160;&#24577;&#36873;&#25321;&#36755;&#20837;&#26631;&#35760;&#12289;&#23618;&#65288;&#25110;&#19968;&#32452;&#23618;&#65289;&#20197;&#21450;&#27599;&#20010;&#23618;&#20869;&#30340;&#23376;&#27169;&#22359;&#65288;&#20363;&#22914;&#65292;&#21367;&#31215;&#28388;&#27874;&#22120;&#20013;&#30340;&#36890;&#36947;&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#19968;&#20010;&#36890;&#29992;&#24418;&#24335;&#26469;&#32479;&#19968;&#25551;&#36848;&#36825;&#20123;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36825;&#20123;&#21407;&#21017;&#30340;&#19977;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#23454;&#29616;&#65306;&#19987;&#23478;&#28151;&#21512;&#65288;MoEs&#65289;&#32593;&#32476;&#12289;&#26631;&#35760;&#36873;&#25321;&#26426;&#21046;&#21644;&#25552;&#21069;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#25991;&#26088;&#22312;&#21521;&#36825;&#19968;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#25552;&#20379;&#31867;&#20284;&#25945;&#31243;&#30340;&#20171;&#32461;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22359;&#21270;&#35774;&#35745;&#22312;&#25928;&#29575;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#38754;&#30340;&#22909;&#22788;&#65292;&#37325;&#28857;&#25918;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07965v1 Announce Type: cross  Abstract: This article summarizes principles and ideas from the emerging area of applying \textit{conditional computation} methods to the design of neural networks. In particular, we focus on neural networks that can dynamically activate or de-activate parts of their computational graph conditionally on their input. Examples include the dynamic selection of, e.g., input tokens, layers (or sets of layers), and sub-modules inside each layer (e.g., channels in a convolutional filter). We first provide a general formalism to describe these techniques in an uniform way. Then, we introduce three notable implementations of these principles: mixture-of-experts (MoEs) networks, token selection mechanisms, and early-exit neural networks. The paper aims to provide a tutorial-like introduction to this growing field. To this end, we analyze the benefits of these modular designs in terms of efficiency, explainability, and transfer learning, with a focus on em
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26080;&#30417;&#30563;&#30340;&#33258;&#32452;&#32455;&#26144;&#23556;&#26041;&#27861;&#25104;&#21151;&#21306;&#20998;&#20102;&#27491;&#24120;&#21069;&#21015;&#33146;&#21644;&#30284;&#32454;&#32990;&#65292;&#24182;&#23637;&#31034;&#20102;&#21069;&#21015;&#33146;&#30284;&#32454;&#32990;&#30340;&#26032;&#23376;&#32676;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2403.07960</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#21069;&#21015;&#33146;&#32454;&#32990;&#25289;&#26364;&#20809;&#35889;&#33258;&#32452;&#32455;&#26144;&#23556;&#26174;&#31034;&#30142;&#30149;&#29366;&#24577;&#30340;&#20122;&#31867;&#32676;
&lt;/p&gt;
&lt;p&gt;
Unsupervised self-organising map of prostate cell Raman spectra shows disease-state subclustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07960
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26080;&#30417;&#30563;&#30340;&#33258;&#32452;&#32455;&#26144;&#23556;&#26041;&#27861;&#25104;&#21151;&#21306;&#20998;&#20102;&#27491;&#24120;&#21069;&#21015;&#33146;&#21644;&#30284;&#32454;&#32990;&#65292;&#24182;&#23637;&#31034;&#20102;&#21069;&#21015;&#33146;&#30284;&#32454;&#32990;&#30340;&#26032;&#23376;&#32676;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#21015;&#33146;&#30284;&#26159;&#19968;&#20010;&#25552;&#20986;&#26377;&#36259;&#20020;&#24202;&#38382;&#39064;&#30340;&#30142;&#30149;&#65306;&#26159;&#21542;&#24212;&#35813;&#27835;&#30103;&#65311;&#19968;&#23567;&#37096;&#20998;&#21069;&#21015;&#33146;&#30284;&#26159;&#20405;&#34989;&#24615;&#30340;&#65292;&#38656;&#35201;&#20999;&#38500;&#21644;&#27835;&#30103;&#20197;&#38450;&#27490;&#36716;&#31227;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#35786;&#26029;&#20173;&#38590;&#20197;&#23545;&#36825;&#31867;&#24739;&#32773;&#36827;&#34892;&#39118;&#38505;&#20998;&#23618;&#65292;&#22240;&#27492;&#38656;&#35201;&#26032;&#30340;&#29983;&#29289;&#20998;&#23376;&#20122;&#31867;&#21035;&#30142;&#30149;&#26041;&#27861;&#12290;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#26080;&#30417;&#30563;&#30340;&#33258;&#32452;&#32455;&#26144;&#23556;&#26041;&#27861;&#20998;&#26512;&#20174;&#21069;&#21015;&#33146;&#32454;&#32990;&#31995;&#33719;&#21462;&#30340;&#27963;&#32454;&#32990;&#25289;&#26364;&#20809;&#35889;&#25968;&#25454;&#65307;&#25105;&#20204;&#30340;&#30446;&#30340;&#26159;&#27979;&#35797;&#36825;&#31181;&#26041;&#27861;&#22312;&#39640;&#32500;&#25968;&#25454;&#38598;&#19978;&#65292;&#22312;&#26368;&#23567;&#39044;&#22788;&#29702;&#30340;&#24773;&#20917;&#19979;&#65292;&#26159;&#21542;&#33021;&#21306;&#20998;&#30284;&#30151;&#21644;&#27491;&#24120;&#32454;&#32990;&#20197;&#21450;&#21333;&#32454;&#32990;&#27700;&#24179;&#12290;&#32467;&#26524;&#19981;&#20165;&#23637;&#31034;&#20102;&#25104;&#21151;&#21306;&#20998;&#27491;&#24120;&#21069;&#21015;&#33146;&#21644;&#30284;&#32454;&#32990;&#65292;&#36824;&#23558;&#21069;&#21015;&#33146;&#30284;&#32454;&#32990;&#31995;&#26032;&#30340;&#20122;&#31867;&#32676;&#21270;&#20026;&#20004;&#32452;&#12290;&#23545;&#27599;&#20010;&#30284;&#20122;&#31867;&#32676;&#30340;&#20809;&#35889;&#30340;&#21021;&#27493;&#20998;&#26512;&#23637;&#31034;&#20102;&#19968;&#31181;diff
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07960v1 Announce Type: cross  Abstract: Prostate cancer is a disease which poses an interesting clinical question: should it be treated? A small subset of prostate cancers are aggressive and require removal and treatment to prevent metastatic spread. However, conventional diagnostics remain challenged to risk-stratify such patients, hence, new methods of approach to biomolecularly subclassify the disease are needed. Here we use an unsupervised, self-organising map approach to analyse live-cell Raman spectroscopy data obtained from prostate cell-lines; our aim is to test the feasibility of this method to differentiate, at the single-cell-level, cancer from normal using high-dimensional datasets with minimal preprocessing. The results demonstrate not only successful separation of normal prostate and cancer cells, but also a new subclustering of the prostate cancer cell-line into two groups. Initial analysis of the spectra from each of the cancer subclusters demonstrates a diff
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#24046;&#24322;&#26816;&#27979;&#21644;&#26102;&#38388;&#32784;&#24515;&#20316;&#20026;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#26426;&#21046;&#65292;&#21033;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#27969;&#20013;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#26469;&#26377;&#25928;&#32456;&#27490;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.07958</link><description>&lt;p&gt;
&#26102;&#38388;&#20915;&#31574;&#65306;&#21033;&#29992;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#36827;&#34892;&#26377;&#25928;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Temporal Decisions: Leveraging Temporal Correlation for Efficient Decisions in Early Exit Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07958
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#24046;&#24322;&#26816;&#27979;&#21644;&#26102;&#38388;&#32784;&#24515;&#20316;&#20026;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#26426;&#21046;&#65292;&#21033;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#27969;&#20013;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#26469;&#26377;&#25928;&#32456;&#27490;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#23884;&#20837;&#24335;&#21644;&#29289;&#32852;&#32593;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20294;&#26159;&#65292;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#27169;&#22411;&#38754;&#20020;&#36164;&#28304;&#38480;&#21046;&#30340;&#25361;&#25112;&#65292;&#36825;&#21487;&#33021;&#24433;&#21709;&#27169;&#22411;&#30340;&#25512;&#26029;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#12290;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#22312;&#38544;&#34255;&#23618;&#20043;&#38388;&#38468;&#21152;&#30340;&#39069;&#22806;&#20998;&#31867;&#22120;&#21160;&#24577;&#35843;&#25972;&#27169;&#22411;&#28145;&#24230;&#12290;&#28982;&#32780;&#65292;&#23454;&#26102;&#32456;&#27490;&#20915;&#31574;&#26426;&#21046;&#23545;&#31995;&#32479;&#30340;&#25928;&#29575;&#12289;&#24310;&#36831;&#21644;&#25345;&#32493;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#24046;&#24322;&#26816;&#27979;&#21644;&#26102;&#38388;&#32784;&#24515;&#20316;&#20026;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#26426;&#21046;&#12290;&#23427;&#20204;&#21033;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#27969;&#20013;&#23384;&#22312;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#26469;&#26377;&#25928;&#22320;&#32456;&#27490;&#25512;&#26029;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#20581;&#24247;&#30417;&#27979;&#12289;&#22270;&#20687;&#20998;&#31867;&#21644;&#21796;&#37266;&#35789;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#36129;&#29486;&#33021;&#22815;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07958v1 Announce Type: cross  Abstract: Deep Learning is becoming increasingly relevant in Embedded and Internet-of-things applications. However, deploying models on embedded devices poses a challenge due to their resource limitations. This can impact the model's inference accuracy and latency. One potential solution are Early Exit Neural Networks, which adjust model depth dynamically through additional classifiers attached between their hidden layers. However, the real-time termination decision mechanism is critical for the system's efficiency, latency, and sustained accuracy.   This paper introduces Difference Detection and Temporal Patience as decision mechanisms for Early Exit Neural Networks. They leverage the temporal correlation present in sensor data streams to efficiently terminate the inference. We evaluate their effectiveness in health monitoring, image classification, and wake-word detection tasks. Our novel contributions were able to reduce the computational foo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#22686;&#24378;&#27969;&#31243;&#65292;&#33021;&#22815;&#23558;&#29616;&#26377;&#27169;&#22411;&#36716;&#25442;&#20026;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#65288;EENN&#65289;&#65292;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#37096;&#32626;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;&#22312;&#29289;&#32852;&#32593;&#21644;&#22270;&#20687;&#20998;&#31867;&#29992;&#20363;&#19978;&#26174;&#33879;&#20943;&#23569;&#25512;&#26029;&#25805;&#20316;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.07957</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#21518;&#35757;&#32451;&#22686;&#24378;&#26041;&#27861;&#29992;&#20110;&#24322;&#26500;&#21644;&#20998;&#24067;&#24335;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#30340;&#33258;&#36866;&#24212;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Efficient Post-Training Augmentation for Adaptive Inference in Heterogeneous and Distributed IoT Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07957
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#22686;&#24378;&#27969;&#31243;&#65292;&#33021;&#22815;&#23558;&#29616;&#26377;&#27169;&#22411;&#36716;&#25442;&#20026;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#65288;EENN&#65289;&#65292;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#37096;&#32626;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;&#22312;&#29289;&#32852;&#32593;&#21644;&#22270;&#20687;&#20998;&#31867;&#29992;&#20363;&#19978;&#26174;&#33879;&#20943;&#23569;&#25512;&#26029;&#25805;&#20316;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#65288;EENN&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#37096;&#32626;&#25928;&#29575;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;EENN&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#38656;&#35201;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#65292;&#30001;&#20110;&#22823;&#37327;&#39069;&#22806;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#22686;&#24378;&#27969;&#31243;&#65292;&#19987;&#27880;&#20110;&#23558;&#29616;&#26377;&#27169;&#22411;&#36716;&#25442;&#20026;EENN&#12290;&#23427;&#25191;&#34892;&#20102;&#37096;&#32626;&#21040;&#24322;&#26500;&#25110;&#20998;&#24067;&#24335;&#30828;&#20214;&#30446;&#26631;&#25152;&#38656;&#30340;&#25152;&#26377;&#35774;&#35745;&#20915;&#31574;&#65306;&#25105;&#20204;&#30340;&#26694;&#26550;&#26500;&#24314;&#20102;EENN&#26550;&#26500;&#65292;&#23558;&#20854;&#23376;&#22270;&#26144;&#23556;&#21040;&#30828;&#20214;&#30446;&#26631;&#65292;&#24182;&#37197;&#32622;&#20102;&#20854;&#20915;&#31574;&#26426;&#21046;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#25191;&#34892;&#25152;&#26377;&#36825;&#20123;&#27493;&#39588;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#29289;&#32852;&#32593;&#21644;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#29992;&#20363;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#35821;&#38899;&#21629;&#20196;&#26816;&#27979;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#33021;&#22815;&#23558;&#27599;&#27425;&#25512;&#26029;&#30340;&#24179;&#22343;&#25805;&#20316;&#20943;&#23569;&#20102;59.67%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07957v1 Announce Type: cross  Abstract: Early Exit Neural Networks (EENNs) present a solution to enhance the efficiency of neural network deployments. However, creating EENNs is challenging and requires specialized domain knowledge, due to the large amount of additional design choices. To address this issue, we propose an automated augmentation flow that focuses on converting an existing model into an EENN. It performs all required design decisions for the deployment to heterogeneous or distributed hardware targets: Our framework constructs the EENN architecture, maps its subgraphs to the hardware targets, and configures its decision mechanism. To the best of our knowledge, it is the first framework that is able to perform all of these steps.   We evaluated our approach on a collection of Internet-of-Things and standard image classification use cases. For a speech command detection task, our solution was able to reduce the mean operations per inference by 59.67%. For an ECG 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CDCL&#31639;&#27861;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#26694;&#26550;DeepCDCL&#65292;&#36890;&#36807;&#24341;&#20837;&#24322;&#27493;&#23376;&#21477;&#23398;&#20064;&#21644;&#31649;&#29702;&#32467;&#26500;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#26102;&#38388;&#28040;&#32791;&#65292;&#24182;&#22312;ACAS Xu&#21644;MNIST&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2403.07956</link><description>&lt;p&gt;
DeepCDCL: &#22522;&#20110;CDCL&#31639;&#27861;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DeepCDCL: An CDCL-based Neural Network Verification Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07956
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CDCL&#31639;&#27861;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#26694;&#26550;DeepCDCL&#65292;&#36890;&#36807;&#24341;&#20837;&#24322;&#27493;&#23376;&#21477;&#23398;&#20064;&#21644;&#31649;&#29702;&#32467;&#26500;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#26102;&#38388;&#28040;&#32791;&#65292;&#24182;&#22312;ACAS Xu&#21644;MNIST&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#36234;&#26469;&#36234;&#22810;&#30340;&#23433;&#20840;&#21644;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#23545;&#24494;&#23567;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeepCDCL&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#20914;&#31361;&#39537;&#21160;&#23376;&#21477;&#23398;&#20064;&#65288;CDCL&#65289;&#31639;&#27861;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#26694;&#26550;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24322;&#27493;&#23376;&#21477;&#23398;&#20064;&#21644;&#31649;&#29702;&#32467;&#26500;&#65292;&#30456;&#27604;&#30452;&#25509;&#24212;&#29992;CDCL&#26694;&#26550;&#65292;&#20943;&#23569;&#20102;&#20887;&#20313;&#30340;&#26102;&#38388;&#28040;&#32791;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;ACAS Xu&#21644;MNIST&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35814;&#32454;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07956v1 Announce Type: cross  Abstract: Neural networks in safety-critical applications face increasing safety and security concerns due to their susceptibility to little disturbance. In this paper, we propose DeepCDCL, a novel neural network verification framework based on the Conflict-Driven Clause Learning (CDCL) algorithm. We introduce an asynchronous clause learning and management structure, reducing redundant time consumption compared to the direct application of the CDCL framework. Furthermore, we also provide a detailed evaluation of the performance of our approach on the ACAS Xu and MNIST datasets, showing that a significant speed-up is achieved in most cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Shortcuts-fused Selective Rationalization (SSR)&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#21033;&#29992;&#28508;&#22312;&#30340;&#24555;&#25463;&#26041;&#24335;&#26469;&#25552;&#21319;&#29702;&#24615;&#21270;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#31574;&#30053;&#32531;&#35299;&#21033;&#29992;&#24555;&#25463;&#26041;&#24335;&#26469;&#32452;&#25104;&#29702;&#24615;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#34917;&#20805;&#24050;&#27880;&#37322;&#29702;&#24615;&#21270;&#25968;&#37327;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.07955</link><description>&lt;p&gt;
&#26397;&#30528;&#24544;&#23454;&#35299;&#37322;&#65306;&#21033;&#29992;&#24555;&#25463;&#26041;&#24335;&#21457;&#29616;&#26469;&#22686;&#24378;&#29702;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Faithful Explanations: Boosting Rationalization with Shortcuts Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Shortcuts-fused Selective Rationalization (SSR)&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#21033;&#29992;&#28508;&#22312;&#30340;&#24555;&#25463;&#26041;&#24335;&#26469;&#25552;&#21319;&#29702;&#24615;&#21270;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#31574;&#30053;&#32531;&#35299;&#21033;&#29992;&#24555;&#25463;&#26041;&#24335;&#26469;&#32452;&#25104;&#29702;&#24615;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#34917;&#20805;&#24050;&#27880;&#37322;&#29702;&#24615;&#21270;&#25968;&#37327;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#31070;&#32463;&#32593;&#32476;&#30340;&#26174;&#33879;&#25104;&#21151;&#24341;&#21457;&#20102;&#26377;&#36873;&#25321;&#24615;&#30340;&#29702;&#24615;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Shortcuts-fused Selective Rationalization (SSR)&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#21033;&#29992;&#28508;&#22312;&#30340;&#24555;&#25463;&#26041;&#24335;&#26469;&#25552;&#21319;&#29702;&#24615;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SSR&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#24555;&#25463;&#26041;&#24335;&#21457;&#29616;&#26041;&#27861;&#26469;&#26816;&#27979;&#20960;&#20010;&#28508;&#22312;&#30340;&#24555;&#25463;&#26041;&#24335;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#24341;&#20837;&#35782;&#21035;&#20986;&#30340;&#24555;&#25463;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#32531;&#35299;&#21033;&#29992;&#24555;&#25463;&#26041;&#24335;&#26469;&#32452;&#25104;&#29702;&#24615;&#21270;&#30340;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#24357;&#34917;&#24050;&#27880;&#37322;&#29702;&#24615;&#21270;&#25968;&#37327;&#30340;&#24046;&#36317;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#28165;&#26970;&#22320;&#39564;&#35777;&#20102;&#36825;&#19968;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07955v1 Announce Type: cross  Abstract: The remarkable success in neural networks provokes the selective rationalization. It explains the prediction results by identifying a small subset of the inputs sufficient to support them. Since existing methods still suffer from adopting the shortcuts in data to compose rationales and limited large-scale annotated rationales by human, in this paper, we propose a Shortcuts-fused Selective Rationalization (SSR) method, which boosts the rationalization by discovering and exploiting potential shortcuts. Specifically, SSR first designs a shortcuts discovery approach to detect several potential shortcuts. Then, by introducing the identified shortcuts, we propose two strategies to mitigate the problem of utilizing shortcuts to compose rationales. Finally, we develop two data augmentations methods to close the gap in the number of annotated rationales. Extensive experimental results on real-world datasets clearly validate the effectiveness of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32479;&#19968;&#22810;&#39033;&#24335;&#22270;&#28388;&#27874;&#22120;&#21644;&#30456;&#21516;&#27425;&#25968;&#30340;&#26368;&#20248;&#28388;&#27874;&#22120;&#21040;&#21516;&#38454;&#20811;&#37324;&#27931;&#22827;&#23376;&#31354;&#38388;&#65292;&#25552;&#20379;&#20102;&#31561;&#25928;&#30340;&#34920;&#36798;&#33021;&#21147;&#65307;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#20811;&#37324;&#27931;&#22827;&#23376;&#31354;&#38388;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;&#20855;&#26377;&#21487;&#25511;&#24615;&#30340;&#22810;&#39033;&#24335;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.07954</link><description>&lt;p&gt;
&#20248;&#21270;&#22810;&#39033;&#24335;&#22270;&#28388;&#27874;&#22120;&#65306;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#20811;&#37324;&#27931;&#22827;&#23376;&#31354;&#38388;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimizing Polynomial Graph Filters: A Novel Adaptive Krylov Subspace Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32479;&#19968;&#22810;&#39033;&#24335;&#22270;&#28388;&#27874;&#22120;&#21644;&#30456;&#21516;&#27425;&#25968;&#30340;&#26368;&#20248;&#28388;&#27874;&#22120;&#21040;&#21516;&#38454;&#20811;&#37324;&#27931;&#22827;&#23376;&#31354;&#38388;&#65292;&#25552;&#20379;&#20102;&#31561;&#25928;&#30340;&#34920;&#36798;&#33021;&#21147;&#65307;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#20811;&#37324;&#27931;&#22827;&#23376;&#31354;&#38388;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;&#20855;&#26377;&#21487;&#25511;&#24615;&#30340;&#22810;&#39033;&#24335;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#20063;&#31216;&#20026;&#35889;&#22270;&#28388;&#27874;&#22120;&#65292;&#22312;&#32593;&#32476;&#22270;&#20013;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#32469;&#36807;&#29305;&#24449;&#20998;&#35299;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#21508;&#31181;&#22810;&#39033;&#24335;&#22522;&#20934;&#36827;&#34892;&#28388;&#27874;&#22120;&#35757;&#32451;&#30340;&#22810;&#39033;&#24335;&#22270;&#28388;&#27874;&#22120;&#65292;&#20197;&#36817;&#20284;&#22270;&#28388;&#27874;&#22120;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#30740;&#31350;&#20174;&#32479;&#19968;&#30340;&#35282;&#24230;&#25506;&#35752;&#22810;&#26679;&#21270;&#30340;&#22810;&#39033;&#24335;&#22270;&#28388;&#27874;&#22120;&#36827;&#34892;&#20248;&#21270;&#12290;&#26412;&#25991;&#39318;&#20808;&#23558;&#22810;&#39033;&#24335;&#22270;&#28388;&#27874;&#22120;&#20197;&#21450;&#30456;&#21516;&#27425;&#25968;&#30340;&#26368;&#20248;&#28388;&#27874;&#22120;&#32479;&#19968;&#25104;&#21516;&#38454;&#20811;&#37324;&#27931;&#22827;&#23376;&#31354;&#38388;&#65292;&#20174;&#32780;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#30456;&#21516;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20174;&#32479;&#19968;&#30340;&#20811;&#37324;&#27931;&#22827;&#23376;&#31354;&#38388;&#35282;&#24230;&#30740;&#31350;&#22810;&#39033;&#24335;&#30340;&#28176;&#36817;&#25910;&#25947;&#23646;&#24615;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#19981;&#21516;&#24322;&#36136;&#31243;&#24230;&#30340;&#22270;&#20013;&#30340;&#26377;&#38480;&#36866;&#24212;&#24615;&#12290;&#21463;&#21040;&#36825;&#20123;&#20107;&#23454;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#20811;&#37324;&#27931;&#22827;&#23376;&#31354;&#38388;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;&#22810;&#39033;&#24335;&#22522;&#20934;&#65292;&#24182;&#21487;&#35777;&#26126;&#20855;&#26377;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07954v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs), known as spectral graph filters, find a wide range of applications in web networks. To bypass eigendecomposition, polynomial graph filters are proposed to approximate graph filters by leveraging various polynomial bases for filter training. However, no existing studies have explored the diverse polynomial graph filters from a unified perspective for optimization.   In this paper, we first unify polynomial graph filters, as well as the optimal filters of identical degrees into the Krylov subspace of the same order, thus providing equivalent expressive power theoretically. Next, we investigate the asymptotic convergence property of polynomials from the unified Krylov subspace perspective, revealing their limited adaptability in graphs with varying heterophily degrees. Inspired by those facts, we design a novel adaptive Krylov subspace approach to optimize polynomial bases with provable controllability over the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#35299;&#24352;&#37327;&#36827;&#19968;&#27493;&#25277;&#35937;&#31232;&#30095;DNN&#21152;&#36895;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23558;&#31232;&#30095;&#24352;&#37327;&#36716;&#25442;&#25104;&#19968;&#31995;&#21015;&#32467;&#26500;&#21270;&#31232;&#30095;&#24352;&#37327;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#31232;&#30095;DNN&#27169;&#22411;&#21644;&#30828;&#20214;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.07953</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#21270;&#31232;&#30095;&#24352;&#37327;&#20998;&#35299;&#23545;&#31232;&#30095;DNN&#21152;&#36895;&#36827;&#34892;&#25277;&#35937;&#21270;
&lt;/p&gt;
&lt;p&gt;
Abstracting Sparse DNN Acceleration via Structured Sparse Tensor Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#35299;&#24352;&#37327;&#36827;&#19968;&#27493;&#25277;&#35937;&#31232;&#30095;DNN&#21152;&#36895;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23558;&#31232;&#30095;&#24352;&#37327;&#36716;&#25442;&#25104;&#19968;&#31995;&#21015;&#32467;&#26500;&#21270;&#31232;&#30095;&#24352;&#37327;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#31232;&#30095;DNN&#27169;&#22411;&#21644;&#30828;&#20214;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20013;&#21033;&#29992;&#31232;&#30095;&#24615;&#24050;&#25104;&#20026;&#28385;&#36275;&#29616;&#20195;DNN&#26085;&#30410;&#22686;&#38271;&#30340;&#35745;&#31639;&#38656;&#27714;&#30340;&#19968;&#31181;&#20855;&#26377;&#21069;&#26223;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#31232;&#30095;DNN&#21152;&#36895;&#20173;&#28982;&#38754;&#20020;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#31232;&#30095;&#21152;&#36895;&#30340;&#24320;&#38144;&#65292;&#30828;&#20214;&#35774;&#35745;&#24072;&#26368;&#36817;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#31232;&#30095;&#30828;&#20214;&#25903;&#25345;&#65292;&#36825;&#25552;&#20379;&#20102;&#26377;&#38480;&#30340;&#28789;&#27963;&#24615;&#24182;&#38656;&#35201;&#39069;&#22806;&#30340;&#27169;&#22411;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#20026;&#26576;&#20123;&#32467;&#26500;&#21270;&#31232;&#30095;&#30828;&#20214;&#24494;&#35843;&#30340;&#20219;&#20309;&#31232;&#30095;&#27169;&#22411;&#26080;&#27861;&#34987;&#20854;&#20182;&#32467;&#26500;&#21270;&#30828;&#20214;&#21152;&#36895;&#12290;&#20026;&#20102;&#24357;&#21512;&#31232;&#30095;DNN&#27169;&#22411;&#21644;&#30828;&#20214;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#32467;&#26500;&#20998;&#35299;&#30340;&#24352;&#37327;&#36817;&#20284;&#65288;TASD&#65289;&#65292;&#21033;&#29992;&#20102;&#32447;&#24615;&#20195;&#25968;&#20013;&#30340;&#20998;&#37197;&#24615;&#36136;&#23558;&#20219;&#20309;&#31232;&#30095;&#24352;&#37327;&#36716;&#21270;&#20026;&#19968;&#31995;&#21015;&#32467;&#26500;&#21270;&#31232;&#30095;&#24352;&#37327;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36719;&#20214;&#26694;&#26550;TASDER&#65292;&#36890;&#36807;&#25628;&#32034;&#36880;&#23618;&#39640;&#36136;&#37327;&#30340;&#32467;&#26500;&#21270;&#20998;&#35299;&#26469;&#21152;&#36895;DNNs&#30340;&#26435;&#37325;&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07953v1 Announce Type: cross  Abstract: Exploiting sparsity in deep neural networks (DNNs) has been a promising area to meet the growing computation need of modern DNNs. However, in practice, sparse DNN acceleration still faces a key challenge. To minimize the overhead of sparse acceleration, hardware designers have proposed structured sparse hardware support recently, which provides limited flexibility and requires extra model fine-tuning. Moreover, any sparse model fine-tuned for certain structured sparse hardware cannot be accelerated by other structured hardware. To bridge the gap between sparse DNN models and hardware, this paper proposes tensor approximation via structured decomposition (TASD), which leverages the distributive property in linear algebra to turn any sparse tensor into a series of structured sparse tensors. Next, we develop a software framework, TASDER, to accelerate DNNs by searching layer-wise, high-quality structured decomposition for both weight and 
&lt;/p&gt;</description></item><item><title>SAMDA&#26694;&#26550;&#32467;&#21512;&#20102;SAM&#21644;nnUNet&#65292;&#36890;&#36807;&#34701;&#21512;&#8220;&#19987;&#23478;&#8221;&#21644;&#8220;&#36890;&#29992;&#8221;&#32452;&#20214;&#65292;&#22312;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#38754;&#20020;&#30340;&#36716;&#31227;&#24615;&#21644;&#31934;&#24230;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.07951</link><description>&lt;p&gt;
SAMDA&#65306;&#21033;&#29992;SAM&#36827;&#34892;&#30005;&#23376;&#26174;&#24494;&#38236;&#20998;&#21106;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
SAMDA: Leveraging SAM on Few-Shot Domain Adaptation for Electronic Microscopy Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07951
&lt;/p&gt;
&lt;p&gt;
SAMDA&#26694;&#26550;&#32467;&#21512;&#20102;SAM&#21644;nnUNet&#65292;&#36890;&#36807;&#34701;&#21512;&#8220;&#19987;&#23478;&#8221;&#21644;&#8220;&#36890;&#29992;&#8221;&#32452;&#20214;&#65292;&#22312;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#38754;&#20020;&#30340;&#36716;&#31227;&#24615;&#21644;&#31934;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#30005;&#23376;&#26174;&#24494;&#38236;&#20998;&#21106;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#22312;&#26679;&#26412;&#21644;&#27880;&#37322;&#26377;&#38480;&#26102;&#23384;&#22312;&#36716;&#31227;&#24615;&#36739;&#20302;&#30340;&#38382;&#39064;&#65292;&#32780;&#22823;&#35268;&#27169;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#36716;&#31227;&#26102;&#26356;&#20855;&#40065;&#26834;&#24615;&#65292;&#20294;&#22312;&#24494;&#35843;&#19979;&#38754;&#20020;&#20122;&#26368;&#20248;&#25913;&#36827;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550;SAMDA&#65292;&#23427;&#23558;&#8220;Segment Anything Model (SAM)&#8221;&#19982;nnUNet&#30456;&#32467;&#21512;&#21040;&#23884;&#20837;&#31354;&#38388;&#20013;&#20197;&#23454;&#29616;&#39640;&#36716;&#31227;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36873;&#25321;&#22522;&#20110;Unet&#30340;&#32593;&#32476;&#20316;&#20026;&#8220;&#19987;&#23478;&#8221;&#32452;&#20214;&#39640;&#25928;&#23398;&#20064;&#20998;&#21106;&#29305;&#24449;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;SAM&#30340;&#33258;&#36866;&#24212;&#27169;&#22359;&#20316;&#20026;&#8220;&#36890;&#29992;&#8221;&#32452;&#20214;&#29992;&#20110;&#39046;&#22495;&#36716;&#31227;&#12290;&#36890;&#36807;&#34701;&#21512;&#8220;&#36890;&#29992;&#8221;&#21644;&#8220;&#19987;&#23478;&#8221;&#32452;&#20214;&#65292;&#25105;&#20204;&#32531;&#35299;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#20013;&#22797;&#26434;&#39044;&#35757;&#32451;&#30693;&#35782;&#20013;&#30340;&#27169;&#24577;&#19981;&#24179;&#34913;&#21644;&#36716;&#31227;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07951v1 Announce Type: cross  Abstract: It has been shown that traditional deep learning methods for electronic microscopy segmentation usually suffer from low transferability when samples and annotations are limited, while large-scale vision foundation models are more robust when transferring between different domains but facing sub-optimal improvement under fine-tuning. In this work, we present a new few-shot domain adaptation framework SAMDA, which combines the Segment Anything Model(SAM) with nnUNet in the embedding space to achieve high transferability and accuracy. Specifically, we choose the Unet-based network as the "expert" component to learn segmentation features efficiently and design a SAM-based adaptation module as the "generic" component for domain transfer. By amalgamating the "generic" and "expert" components, we mitigate the modality imbalance in the complex pre-training knowledge inherent to large-scale Vision Foundation models and the challenge of transfer
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#19968;&#31181;&#28151;&#21512;&#20351;&#29992;Sepedi-&#33521;&#35821;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#25506;&#35752;&#20102;&#31471;&#21040;&#31471;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07947</link><description>&lt;p&gt;
&#35780;&#20272;&#19968;&#31181;&#28151;&#21512;&#20351;&#29992;Sepedi-&#33521;&#35821;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
The evaluation of a code-switched Sepedi-English automatic speech recognition system
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07947
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#19968;&#31181;&#28151;&#21512;&#20351;&#29992;Sepedi-&#33521;&#35821;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#25506;&#35752;&#20102;&#31471;&#21040;&#31471;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07947v1 &#21457;&#34920;&#31867;&#22411;: &#36328;&#39046;&#22495;  &#25688;&#35201;: &#35821;&#38899;&#25216;&#26415;&#26159;&#19968;&#20010;&#28085;&#30422;&#21508;&#31181;&#25216;&#26415;&#21644;&#24037;&#20855;&#30340;&#39046;&#22495;&#65292;&#29992;&#20110;&#20351;&#35774;&#22791;&#33021;&#22815;&#19982;&#35821;&#38899;&#36827;&#34892;&#20132;&#20114;&#65292;&#20363;&#22914;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#31561;&#65292;&#20801;&#35768;&#35774;&#22791;&#36890;&#36807;&#40614;&#20811;&#39118;&#20174;&#35828;&#35805;&#32773;&#37027;&#37324;&#25429;&#33719;&#35828;&#35805;&#30340;&#35805;&#35821;&#12290;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#22914;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#26159;&#24320;&#21457;ASR&#31995;&#32479;&#20013;&#20351;&#29992;&#26368;&#24191;&#27867;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#29992;&#20110;&#23545;&#20855;&#26377;&#22823;&#37327;&#35821;&#38899;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#36827;&#34892;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#23548;&#33268;&#20302;&#36164;&#28304;&#35821;&#35328;&#30456;&#23545;&#36739;&#23569;&#21457;&#23637;&#12290;&#23613;&#31649;CTC&#26041;&#27861;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#24050;&#25104;&#21151;&#20351;&#29992;&#65292;&#20294;&#23427;&#22312;Sepedi&#35821;&#35328;&#20013;&#30340;&#26377;&#25928;&#24615;&#20173;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#28151;&#21512;&#20351;&#29992;Sepedi-&#33521;&#35821;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07947v1 Announce Type: cross  Abstract: Speech technology is a field that encompasses various techniques and tools used to enable machines to interact with speech, such as automatic speech recognition (ASR), spoken dialog systems, and others, allowing a device to capture spoken words through a microphone from a human speaker. End-to-end approaches such as Connectionist Temporal Classification (CTC) and attention-based methods are the most used for the development of ASR systems. However, these techniques were commonly used for research and development for many high-resourced languages with large amounts of speech data for training and evaluation, leaving low-resource languages relatively underdeveloped. While the CTC method has been successfully used for other languages, its effectiveness for the Sepedi language remains uncertain. In this study, we present the evaluation of the Sepedi-English code-switched automatic speech recognition system. This end-to-end system was devel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#21517;&#20026;&#35748;&#30693;&#23433;&#20840;&#65292;&#29992;&#20110;&#25551;&#36848;&#21644;&#20998;&#26512;&#31070;&#32463;&#25216;&#26415;&#23545;&#20010;&#20307;&#35748;&#30693;&#38544;&#31169;&#21644;&#33258;&#27835;&#21487;&#33021;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#35299;&#20915;&#20102;&#30456;&#20851;&#38382;&#39064;&#25551;&#36848;&#21644;&#20998;&#26512;&#30340;&#38556;&#30861;&#12290;</title><link>https://arxiv.org/abs/2403.07945</link><description>&lt;p&gt;
&#19968;&#20010;&#35299;&#20915;&#31070;&#32463;&#25216;&#26415;&#35748;&#30693;&#23433;&#20840;&#38382;&#39064;&#30340;&#25968;&#23398;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Mathematical Framework for the Problem of Security for Cognition in Neurotechnology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#21517;&#20026;&#35748;&#30693;&#23433;&#20840;&#65292;&#29992;&#20110;&#25551;&#36848;&#21644;&#20998;&#26512;&#31070;&#32463;&#25216;&#26415;&#23545;&#20010;&#20307;&#35748;&#30693;&#38544;&#31169;&#21644;&#33258;&#27835;&#21487;&#33021;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#35299;&#20915;&#20102;&#30456;&#20851;&#38382;&#39064;&#25551;&#36848;&#21644;&#20998;&#26512;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#31070;&#32463;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#22312;&#31070;&#32463;&#25216;&#26415;&#21644;&#23433;&#20840;&#20043;&#38388;&#21019;&#36896;&#20102;&#19968;&#20010;&#26032;&#20852;&#30340;&#20851;&#38190;&#20132;&#21449;&#28857;&#12290;&#26893;&#20837;&#24335;&#35774;&#22791;&#12289;&#38750;&#20405;&#20837;&#24335;&#30417;&#27979;&#21644;&#38750;&#20405;&#20837;&#24335;&#27835;&#30103;&#37117;&#24102;&#26469;&#20102;&#36829;&#21453;&#20010;&#20307;&#35748;&#30693;&#38544;&#31169;&#21644;&#33258;&#27835;&#30340;&#21069;&#26223;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#31185;&#23398;&#23478;&#21644;&#21307;&#29983;&#21628;&#21505;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064; -- &#25105;&#20204;&#31216;&#20043;&#20026;&#35748;&#30693;&#23433;&#20840; -- &#20294;&#24212;&#29992;&#24037;&#20316;&#21463;&#21040;&#38480;&#21046;&#12290;&#38459;&#30861;&#31185;&#23398;&#21644;&#24037;&#31243;&#21162;&#21147;&#35299;&#20915;&#35748;&#30693;&#23433;&#20840;&#38382;&#39064;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#32570;&#20047;&#28165;&#26224;&#25551;&#36848;&#21644;&#20998;&#26512;&#30456;&#20851;&#38382;&#39064;&#30340;&#25163;&#27573;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#35748;&#30693;&#23433;&#20840;&#65292;&#36825;&#26159;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#36890;&#36807;&#20511;&#37492;&#22810;&#20010;&#39046;&#22495;&#30340;&#26041;&#27861;&#21644;&#32467;&#26524;&#65292;&#23454;&#29616;&#36825;&#31181;&#25551;&#36848;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20123;&#23545;&#35748;&#30693;&#23433;&#20840;&#26377;&#37325;&#35201;&#24433;&#21709;&#30340;&#32479;&#35745;&#29305;&#24615;&#65292;&#28982;&#21518;&#25552;&#20986;&#25551;&#36848;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07945v1 Announce Type: cross  Abstract: The rapid advancement in neurotechnology in recent years has created an emerging critical intersection between neurotechnology and security. Implantable devices, non-invasive monitoring, and non-invasive therapies all carry with them the prospect of violating the privacy and autonomy of individuals' cognition. A growing number of scientists and physicians have made calls to address this issue -- which we term Cognitive Security -- but applied efforts have been limited. A major barrier hampering scientific and engineering efforts to address Cognitive Security is the lack of a clear means of describing and analyzing relevant problems. In this paper we develop Cognitive Security, a mathematical framework which enables such description and analysis by drawing on methods and results from multiple fields. We demonstrate certain statistical properties which have significant implications for Cognitive Security, and then present descriptions of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#20844;&#24335;&#65292;&#26126;&#30830;&#30028;&#23450;&#20102;&#20004;&#31867;&#36793;&#25200;&#21160;&#26041;&#27861;&#20043;&#38388;&#30340;&#28165;&#26224;&#30028;&#38480;&#65292;&#24182;&#35299;&#31572;&#20102;&#20026;&#20309;&#36793;&#25200;&#21160;&#20855;&#26377;&#21452;&#37325;&#25928;&#26524;&#20197;&#21450;&#20309;&#20351;&#36793;&#25200;&#21160;&#28789;&#27963;&#26377;&#25928;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07943</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36793;&#25200;&#21160;&#22312;&#22270;&#25968;&#25454;&#22686;&#24378;&#21644;&#25915;&#20987;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Revisiting Edge Perturbation for Graph Neural Network in Graph Data Augmentation and Attack
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#20844;&#24335;&#65292;&#26126;&#30830;&#30028;&#23450;&#20102;&#20004;&#31867;&#36793;&#25200;&#21160;&#26041;&#27861;&#20043;&#38388;&#30340;&#28165;&#26224;&#30028;&#38480;&#65292;&#24182;&#35299;&#31572;&#20102;&#20026;&#20309;&#36793;&#25200;&#21160;&#20855;&#26377;&#21452;&#37325;&#25928;&#26524;&#20197;&#21450;&#20309;&#20351;&#36793;&#25200;&#21160;&#28789;&#27963;&#26377;&#25928;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#25200;&#21160;&#26159;&#20462;&#25913;&#22270;&#32467;&#26500;&#30340;&#19968;&#31181;&#22522;&#26412;&#26041;&#27861;&#12290;&#23427;&#21487;&#20197;&#26681;&#25454;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24615;&#33021;&#30340;&#24433;&#21709;&#20998;&#20026;&#20004;&#31181;&#31867;&#21035;&#65292;&#21363;&#22270;&#25968;&#25454;&#22686;&#24378;&#21644;&#25915;&#20987;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36793;&#25200;&#21160;&#26041;&#27861;&#30340;&#20004;&#31181;&#31867;&#21035;&#37117;&#37319;&#29992;&#30456;&#21516;&#30340;&#25805;&#20316;&#65292;&#20294;&#23545;GNN&#30340;&#20934;&#30830;&#24615;&#20135;&#29983;&#30456;&#21453;&#30340;&#24433;&#21709;&#12290;&#30446;&#21069;&#23578;&#26410;&#28165;&#26224;&#22320;&#23450;&#20041;&#36825;&#20123;&#26041;&#27861;&#22312;&#20351;&#29992;&#36793;&#25200;&#21160;&#26102;&#30340;&#26126;&#26174;&#30028;&#38480;&#12290;&#22240;&#27492;&#65292;&#19981;&#24403;&#30340;&#25200;&#21160;&#21487;&#33021;&#23548;&#33268;&#19981;&#33391;&#21518;&#26524;&#65292;&#38656;&#35201;&#31934;&#30830;&#35843;&#25972;&#20197;&#23454;&#29616;&#26399;&#26395;&#30340;&#25928;&#26524;&#12290;&#22240;&#27492;&#65292;&#8220;&#36793;&#25200;&#21160;&#20026;&#20309;&#25928;&#26524;&#20004;&#26497;&#65311;&#8221;&#21644;&#8220;&#20309;&#20351;&#36793;&#25200;&#21160;&#28789;&#27963;&#26377;&#25928;&#65311;&#8221;&#20173;&#28982;&#27809;&#26377;&#31572;&#26696;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#32479;&#19968;&#20844;&#24335;&#24182;&#24314;&#31435;&#20004;&#31867;&#36793;&#25200;&#21160;&#26041;&#27861;&#20043;&#38388;&#30340;&#26126;&#30830;&#30028;&#38480;&#26469;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07943v1 Announce Type: new  Abstract: Edge perturbation is a basic method to modify graph structures. It can be categorized into two veins based on their effects on the performance of graph neural networks (GNNs), i.e., graph data augmentation and attack. Surprisingly, both veins of edge perturbation methods employ the same operations, yet yield opposite effects on GNNs' accuracy. A distinct boundary between these methods in using edge perturbation has never been clearly defined. Consequently, inappropriate perturbations may lead to undesirable outcomes, necessitating precise adjustments to achieve desired effects. Therefore, questions of ``why edge perturbation has a two-faced effect?'' and ``what makes edge perturbation flexible and effective?'' still remain unanswered.   In this paper, we will answer these questions by proposing a unified formulation and establishing a clear boundary between two categories of edge perturbation methods. Specifically, we conduct experiments
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26816;&#27979;&#22836;&#21457;&#21644;&#22836;&#30382;&#30142;&#30149;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#22270;&#20687;&#65292;&#23454;&#29616;&#20102;&#30382;&#32932;&#30149;&#21464;&#30340;&#26089;&#26399;&#26816;&#27979;&#21644;&#35786;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.07940</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26816;&#27979;&#22836;&#21457;&#21644;&#22836;&#30382;&#30142;&#30149;
&lt;/p&gt;
&lt;p&gt;
Hair and scalp disease detection using deep learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07940
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26816;&#27979;&#22836;&#21457;&#21644;&#22836;&#30382;&#30142;&#30149;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#22270;&#20687;&#65292;&#23454;&#29616;&#20102;&#30382;&#32932;&#30149;&#21464;&#30340;&#26089;&#26399;&#26816;&#27979;&#21644;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21307;&#30103;&#20445;&#20581;&#21644;&#25216;&#26415;&#25972;&#21512;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#34920;&#29616;&#26126;&#26174;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#30382;&#32932;&#31185;&#39046;&#22495;&#30340;&#19968;&#31181;&#24320;&#21019;&#24615;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#26816;&#27979;&#22836;&#21457;&#21644;&#22836;&#30382;&#30142;&#30149;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#65292;&#20197;&#20854;&#22312;&#22270;&#20687;&#35782;&#21035;&#20013;&#30340;&#39640;&#25928;&#24615;&#32780;&#38395;&#21517;&#65292;&#23545;&#22270;&#20687;&#36827;&#34892;&#32454;&#33268;&#20998;&#26512;&#65292;&#20197;&#26816;&#27979;&#24433;&#21709;&#22836;&#21457;&#21644;&#22836;&#30382;&#30340;&#21508;&#31181;&#30382;&#32932;&#30149;&#30151;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;&#20195;&#34920;&#20102;&#30382;&#32932;&#35786;&#26029;&#26041;&#38754;&#30340;&#37325;&#22823;&#36827;&#27493;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#21644;&#39640;&#25928;&#29575;&#30340;&#26089;&#26399;&#26816;&#27979;&#21644;&#35786;&#26029;&#25163;&#27573;&#12290;&#36890;&#36807;&#21033;&#29992;CNNs&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#28508;&#21147;&#24443;&#24213;&#25913;&#21464;&#30382;&#32932;&#31185;&#23398;&#65292;&#25552;&#20379;&#26131;&#33719;&#21462;&#21644;&#21450;&#26102;&#30340;&#21307;&#30103;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07940v1 Announce Type: cross  Abstract: In recent years, there has been a notable advancement in the integration of healthcare and technology, particularly evident in the field of medical image analysis. This paper introduces a pioneering approach in dermatology, presenting a robust method for the detection of hair and scalp diseases using state-of-the-art deep learning techniques. Our methodology relies on Convolutional Neural Networks (CNNs), well-known for their efficacy in image recognition, to meticulously analyze images for various dermatological conditions affecting the hair and scalp. Our proposed system represents a significant advancement in dermatological diagnostics, offering a non-invasive and highly efficient means of early detection and diagnosis. By leveraging the capabilities of CNNs, our model holds the potential to revolutionize dermatology, providing accessible and timely healthcare solutions. Furthermore, the seamless integration of our trained model int
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#19982;&#35270;&#39057;&#23545;&#40784;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#22522;&#20934;T2AV-Bench&#65292;&#20197;&#21450;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35270;&#39057;&#23545;&#40784;TTA&#29983;&#25104;&#27169;&#22411;T2AV&#65292;&#25913;&#36827;&#20102;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#34701;&#21512;&#20102;&#35270;&#35273;&#23545;&#40784;&#25991;&#26412;&#23884;&#20837;&#12290;</title><link>https://arxiv.org/abs/2403.07938</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#19982;&#35270;&#39057;&#21516;&#27493;
&lt;/p&gt;
&lt;p&gt;
Text-to-Audio Generation Synchronized with Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07938
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#19982;&#35270;&#39057;&#23545;&#40784;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#22522;&#20934;T2AV-Bench&#65292;&#20197;&#21450;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35270;&#39057;&#23545;&#40784;TTA&#29983;&#25104;&#27169;&#22411;T2AV&#65292;&#25913;&#36827;&#20102;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#34701;&#21512;&#20102;&#35270;&#35273;&#23545;&#40784;&#25991;&#26412;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#25991;&#26412;&#21040;&#38899;&#39057;&#65288;TTA&#65289;&#29983;&#25104;&#30340;&#20851;&#27880;&#26085;&#30410;&#21152;&#24378;&#65292;&#30740;&#31350;&#20154;&#21592;&#21162;&#21147;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#21512;&#25104;&#38899;&#39057;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#34429;&#28982;&#21033;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#38899;&#39057;&#21644;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20294;&#22312;&#20445;&#25345;&#29983;&#25104;&#30340;&#38899;&#39057;&#19982;&#20854;&#35270;&#39057;&#20043;&#38388;&#26080;&#32541;&#21516;&#27493;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#32463;&#24120;&#23548;&#33268;&#21487;&#23519;&#35273;&#30340;&#38899;&#39057;-&#35270;&#35273;&#19981;&#21305;&#37197;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#19982;&#35270;&#39057;&#23545;&#40784;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#30340;&#24320;&#21019;&#24615;&#22522;&#20934;&#65292;&#21517;&#20026;T2AV-Bench&#12290;&#35813;&#22522;&#20934;&#36890;&#36807;&#19977;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#23545;&#40784;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#26032;&#39062;&#25351;&#26631;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#20026;&#20102;&#34917;&#20805;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35270;&#39057;&#23545;&#40784;TTA&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;T2AV&#12290;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#65292;T2AV&#36890;&#36807;&#23558;&#35270;&#35273;&#23545;&#40784;&#25991;&#26412;&#23884;&#20837;&#38598;&#25104;&#20026;&#20854;c
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07938v1 Announce Type: cross  Abstract: In recent times, the focus on text-to-audio (TTA) generation has intensified, as researchers strive to synthesize audio from textual descriptions. However, most existing methods, though leveraging latent diffusion models to learn the correlation between audio and text embeddings, fall short when it comes to maintaining a seamless synchronization between the produced audio and its video. This often results in discernible audio-visual mismatches. To bridge this gap, we introduce a groundbreaking benchmark for Text-to-Audio generation that aligns with Videos, named T2AV-Bench. This benchmark distinguishes itself with three novel metrics dedicated to evaluating visual alignment and temporal consistency. To complement this, we also present a simple yet effective video-aligned TTA generation model, namely T2AV. Moving beyond traditional methods, T2AV refines the latent diffusion approach by integrating visual-aligned text embeddings as its c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#22522;&#20934;&#65288;SRB&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#23545;&#21508;&#31181;&#30772;&#22351;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#21644;&#26576;&#20123;&#24314;&#27169;&#36873;&#25321;&#26377;&#21161;&#20110;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#22312;&#19981;&#21516;&#20154;&#21475;&#20122;&#32452;&#19978;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.07937</link><description>&lt;p&gt;
&#35821;&#38899;&#40065;&#26834;&#22522;&#20934;&#65306;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#40065;&#26834;&#24615;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Speech Robust Bench: A Robustness Benchmark For Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07937
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#22522;&#20934;&#65288;SRB&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#23545;&#21508;&#31181;&#30772;&#22351;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#21644;&#26576;&#20123;&#24314;&#27169;&#36873;&#25321;&#26377;&#21161;&#20110;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#22312;&#19981;&#21516;&#20154;&#21475;&#20122;&#32452;&#19978;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#30830;&#20445;&#23427;&#20204;&#22312;&#29289;&#29702;&#19990;&#30028;&#21644;&#25968;&#23383;&#19990;&#30028;&#20013;&#30340;&#21508;&#31181;&#30772;&#22351;&#19979;&#36827;&#34892;&#21487;&#38752;&#39044;&#27979;&#21464;&#24471;&#24840;&#21457;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#38899;&#40065;&#26834;&#22522;&#20934;&#65288;SRB&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;ASR&#27169;&#22411;&#23545;&#21508;&#31181;&#30772;&#22351;&#30340;&#40065;&#26834;&#24615;&#30340;&#20840;&#38754;&#22522;&#20934;&#12290;SRB&#30001;69&#20010;&#36755;&#20837;&#25200;&#21160;&#32452;&#25104;&#65292;&#26088;&#22312;&#27169;&#25311;ASR&#27169;&#22411;&#21487;&#33021;&#22312;&#29289;&#29702;&#19990;&#30028;&#21644;&#25968;&#23383;&#19990;&#30028;&#20013;&#36935;&#21040;&#30340;&#21508;&#31181;&#30772;&#22351;&#12290;&#25105;&#20204;&#20351;&#29992;SRB&#26469;&#35780;&#20272;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;ASR&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#27169;&#22411;&#22823;&#23567;&#21644;&#26576;&#20123;&#24314;&#27169;&#36873;&#25321;&#65288;&#22914;&#31163;&#25955;&#34920;&#31034;&#21644;&#33258;&#25105;&#35757;&#32451;&#65289;&#20284;&#20046;&#26377;&#21161;&#20110;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#27492;&#20998;&#26512;&#25193;&#23637;&#21040;&#34913;&#37327;ASR&#27169;&#22411;&#22312;&#26469;&#33258;&#21508;&#31181;&#20154;&#21475;&#20122;&#32452;&#30340;&#25968;&#25454;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#21363;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#20351;&#29992;&#32773;&#20197;&#21450;&#30007;&#24615;&#21644;&#22899;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#22312;&#19981;&#21516;&#20122;&#32452;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07937v1 Announce Type: cross  Abstract: As Automatic Speech Recognition (ASR) models become ever more pervasive, it is important to ensure that they make reliable predictions under corruptions present in the physical and digital world. We propose Speech Robust Bench (SRB), a comprehensive benchmark for evaluating the robustness of ASR models to diverse corruptions. SRB is composed of 69 input perturbations which are intended to simulate various corruptions that ASR models may encounter in the physical and digital world. We use SRB to evaluate the robustness of several state-of-the-art ASR models and observe that model size and certain modeling choices such as discrete representations, and self-training appear to be conducive to robustness. We extend this analysis to measure the robustness of ASR models on data from various demographic subgroups, namely English and Spanish speakers, and males and females, and observed noticeable disparities in the model's robustness across su
&lt;/p&gt;</description></item><item><title>&#22312;&#31163;&#32447;&#20004;&#20154;&#38646;&#21644;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25239;&#25968;&#25454;&#25439;&#22351;&#30340;&#23398;&#20064;&#31639;&#27861;&#20197;&#35782;&#21035;&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#23545;&#65292;&#24182;&#36890;&#36807;&#40065;&#26834;&#29256;&#26412;&#30340;&#26497;&#23567;&#26497;&#22823;&#20540;&#36845;&#20195;&#31639;&#27861;&#23454;&#29616;&#20102;&#65288;&#36817;&#65289;&#26368;&#20248;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.07933</link><description>&lt;p&gt;
&#20855;&#26377;&#25239;&#25968;&#25454;&#25439;&#22351;&#29305;&#24615;&#30340;&#31163;&#32447;&#20004;&#20154;&#38646;&#21644;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Corruption-Robust Offline Two-Player Zero-Sum Markov Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07933
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#20004;&#20154;&#38646;&#21644;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25239;&#25968;&#25454;&#25439;&#22351;&#30340;&#23398;&#20064;&#31639;&#27861;&#20197;&#35782;&#21035;&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#23545;&#65292;&#24182;&#36890;&#36807;&#40065;&#26834;&#29256;&#26412;&#30340;&#26497;&#23567;&#26497;&#22823;&#20540;&#36845;&#20195;&#31639;&#27861;&#23454;&#29616;&#20102;&#65288;&#36817;&#65289;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#31163;&#32447;&#20004;&#20154;&#38646;&#21644;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#20013;&#30340;&#25968;&#25454;&#25439;&#22351;&#40065;&#26834;&#24615;&#12290;&#32473;&#23450;&#20004;&#21517;&#29609;&#23478;&#23454;&#29616;&#36712;&#36857;&#25968;&#25454;&#38598;&#65292;&#23545;&#25163;&#21487;&#20197;&#20462;&#25913;&#20854;&#20013;&#30340; $\epsilon$ &#37096;&#20998;&#12290;&#23398;&#20064;&#32773;&#30340;&#30446;&#26631;&#26159;&#20174;&#24050;&#25439;&#22351;&#30340;&#25968;&#25454;&#20013;&#35782;&#21035;&#20986;&#19968;&#20010;&#36817;&#20284;&#30340;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#23545;&#12290;&#25105;&#20204;&#22312;&#32447;&#24615;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#20013;&#32771;&#34385;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#25968;&#25454;&#35206;&#30422;&#21644;&#25439;&#22351;&#12290;&#25105;&#20204;&#39318;&#20808;&#38024;&#23545;&#20219;&#20309;&#23398;&#20064;&#32773;&#30340;&#27425;&#20248;&#38388;&#38553;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#19979;&#30028;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24754;&#35266;&#26497;&#23567;&#26497;&#22823;&#20540;&#36845;&#20195;&#31639;&#27861;&#30340;&#40065;&#26834;&#29256;&#26412;&#65292;&#20998;&#21035;&#22312;&#25439;&#22351;&#25968;&#25454;&#19978;&#30340;&#35206;&#30422;&#21644;&#20165;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#30340;&#35206;&#30422;&#19979;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#30456;&#23545;&#20110; $\epsilon$ &#23454;&#29616;&#65288;&#36817;&#65289;&#26368;&#20248;&#23376;&#20248;&#24046;&#36793;&#30028;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#25105;&#20204;&#26159;&#39318;&#20010;&#25552;&#20379;&#36825;&#31181;&#22312;&#31163;&#32447;&#20004;&#20154;&#38646;&#21644;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#20013;&#23398;&#20064;&#36817;&#20284;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#38382;&#39064;&#29305;&#24615;&#25551;&#36848;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07933v1 Announce Type: cross  Abstract: We study data corruption robustness in offline two-player zero-sum Markov games. Given a dataset of realized trajectories of two players, an adversary is allowed to modify an $\epsilon$-fraction of it. The learner's goal is to identify an approximate Nash Equilibrium policy pair from the corrupted data. We consider this problem in linear Markov games under different degrees of data coverage and corruption. We start by providing an information-theoretic lower bound on the suboptimality gap of any learner. Next, we propose robust versions of the Pessimistic Minimax Value Iteration algorithm, both under coverage on the corrupted data and under coverage only on the clean data, and show that they achieve (near)-optimal suboptimality gap bounds with respect to $\epsilon$. We note that we are the first to provide such a characterization of the problem of learning approximate Nash Equilibrium policies in offline two-player zero-sum Markov game
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#23454;&#29616;&#25968;&#25454;&#30340;&#20302;&#32500;&#23884;&#20837;&#65292;&#23558;&#28909;&#26680;&#20316;&#20026;&#21327;&#26041;&#24046;&#20989;&#25968;&#36827;&#34892;&#35745;&#31639;&#65292;&#25311;&#21512;&#20986;&#23884;&#20837;&#20013;&#30340;&#30452;&#32447;&#36317;&#31163;&#20197;&#27010;&#29575;&#26041;&#24335;&#36817;&#20284;&#25193;&#25955;&#36317;&#31163;&#65292;&#20445;&#30041;&#20102;&#19968;&#20123;&#36739;&#23567;&#23610;&#24230;&#32467;&#26500;&#65292;&#21516;&#26102;&#20855;&#26377;&#23545;&#24322;&#24120;&#20540;&#30340;&#26356;&#24378;&#40065;&#26834;&#24615;</title><link>https://arxiv.org/abs/2403.07929</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#23545;&#28909;&#26680;&#36827;&#34892;&#33609;&#22270;: &#22312;&#20302;&#32500;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#23884;&#20837;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Sketching the Heat Kernel: Using Gaussian Processes to Embed Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07929
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39640;&#26031;&#36807;&#31243;&#23454;&#29616;&#25968;&#25454;&#30340;&#20302;&#32500;&#23884;&#20837;&#65292;&#23558;&#28909;&#26680;&#20316;&#20026;&#21327;&#26041;&#24046;&#20989;&#25968;&#36827;&#34892;&#35745;&#31639;&#65292;&#25311;&#21512;&#20986;&#23884;&#20837;&#20013;&#30340;&#30452;&#32447;&#36317;&#31163;&#20197;&#27010;&#29575;&#26041;&#24335;&#36817;&#20284;&#25193;&#25955;&#36317;&#31163;&#65292;&#20445;&#30041;&#20102;&#19968;&#20123;&#36739;&#23567;&#23610;&#24230;&#32467;&#26500;&#65292;&#21516;&#26102;&#20855;&#26377;&#23545;&#24322;&#24120;&#20540;&#30340;&#26356;&#24378;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#38750;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#35745;&#31639;&#20381;&#36182;&#20110;&#25968;&#25454;&#20960;&#20309;&#29305;&#24615;&#30340;&#39640;&#26031;&#36807;&#31243;&#23454;&#29616;&#25968;&#25454;&#22312;&#20302;&#32500;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#23884;&#20837;&#12290;&#27492;&#31867;&#23884;&#20837;&#39318;&#27425;&#20986;&#29616;&#22312;&#65288;Adler&#31561;&#20154;&#65292;2018&#65289;&#20013;&#65292;&#20316;&#20026;&#39640;&#32500;&#36890;&#29992;&#27969;&#24418;&#30340;&#29702;&#35770;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#39640;&#26031;&#36807;&#31243;&#30340;&#21327;&#26041;&#24046;&#20989;&#25968;&#35774;&#32622;&#20026;&#28909;&#26680;&#65292;&#35745;&#31639;&#23884;&#20837;&#30456;&#24403;&#20110;&#33609;&#32472;&#20195;&#34920;&#28909;&#26680;&#30340;&#30697;&#38453;&#12290;Karhunen-Lo\`eve&#23637;&#24320;&#34920;&#26126;&#65292;&#22312;&#23884;&#20837;&#20013;&#30340;&#30452;&#32447;&#36317;&#31163;&#20197;&#27010;&#29575;&#24847;&#20041;&#19978;&#36817;&#20284;&#25193;&#25955;&#36317;&#31163;&#65292;&#36991;&#20813;&#20102;&#23545;&#23574;&#38160;&#25130;&#26029;&#30340;&#38656;&#27714;&#65292;&#24182;&#20445;&#25345;&#20102;&#19968;&#20123;&#36739;&#23567;&#23610;&#24230;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23545;&#24322;&#24120;&#20540;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#26469;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07929v1 Announce Type: new  Abstract: This paper introduces a novel, non-deterministic method for embedding data in low-dimensional Euclidean space based on computing realizations of a Gaussian process depending on the geometry of the data. This type of embedding first appeared in (Adler et al, 2018) as a theoretical model for a generic manifold in high dimensions.   In particular, we take the covariance function of the Gaussian process to be the heat kernel, and computing the embedding amounts to sketching a matrix representing the heat kernel. The Karhunen-Lo\`eve expansion reveals that the straight-line distances in the embedding approximate the diffusion distance in a probabilistic sense, avoiding the need for sharp cutoffs and maintaining some of the smaller-scale structure.   Our method demonstrates further advantage in its robustness to outliers. We justify the approach with both theory and experiments.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26234;&#33021;&#30417;&#25511;&#26694;&#26550;&#65292;&#26681;&#25454;&#26381;&#21153;&#23646;&#24615;&#20026;&#20113;&#26381;&#21153;&#25512;&#33616;&#30417;&#25511;&#22120;&#65292;&#36890;&#36807;&#25366;&#25496;&#30417;&#35270;&#22120;&#23646;&#24615;&#24182;&#24314;&#31435;&#32467;&#26500;&#21270;&#26412;&#20307;&#35770;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#30417;&#25511;&#21019;&#24314;&#36807;&#31243;&#20013;&#30340;&#19981;&#23436;&#25972;&#35206;&#30422;&#21644;&#20887;&#20313;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07927</link><description>&lt;p&gt;
&#20113;&#26381;&#21153;&#30340;&#26234;&#33021;&#30417;&#25511;&#26694;&#26550;: &#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Intelligent Monitoring Framework for Cloud Services: A Data-Driven Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07927
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26234;&#33021;&#30417;&#25511;&#26694;&#26550;&#65292;&#26681;&#25454;&#26381;&#21153;&#23646;&#24615;&#20026;&#20113;&#26381;&#21153;&#25512;&#33616;&#30417;&#25511;&#22120;&#65292;&#36890;&#36807;&#25366;&#25496;&#30417;&#35270;&#22120;&#23646;&#24615;&#24182;&#24314;&#31435;&#32467;&#26500;&#21270;&#26412;&#20307;&#35770;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#30417;&#25511;&#21019;&#24314;&#36807;&#31243;&#20013;&#30340;&#19981;&#23436;&#25972;&#35206;&#30422;&#21644;&#20887;&#20313;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#26381;&#21153;&#25152;&#26377;&#32773;&#38656;&#35201;&#19981;&#26029;&#30417;&#27979;&#20854;&#26381;&#21153;&#65292;&#20197;&#30830;&#20445;&#39640;&#21487;&#29992;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290; &#30417;&#25511;&#20013;&#30340;&#31354;&#30333;&#21487;&#33021;&#23548;&#33268;&#20107;&#20214;&#26816;&#27979;&#24310;&#36831;&#21644;&#26174;&#30528;&#30340;&#36127;&#38754;&#23458;&#25143;&#24433;&#21709;&#12290; &#30446;&#21069;&#30340;&#30417;&#25511;&#21019;&#24314;&#36807;&#31243;&#26159;&#20020;&#26102;&#21644;&#21453;&#24212;&#24615;&#30340;&#12290; &#24320;&#21457;&#20154;&#21592;&#20351;&#29992;&#20854;&#37096;&#33853;&#30693;&#35782;&#65292;&#20027;&#35201;&#26159;&#22522;&#20110;&#35797;&#38169;&#30340;&#36807;&#31243;&#21019;&#24314;&#30417;&#25511;&#22120;&#12290; &#22240;&#27492;&#65292;&#30417;&#25511;&#22120;&#32463;&#24120;&#20855;&#26377;&#19981;&#23436;&#25972;&#30340;&#35206;&#30422;&#33539;&#22260;&#65292;&#36825;&#20250;&#23548;&#33268;&#29983;&#20135;&#38382;&#39064;&#65292;&#25110;&#32773;&#20887;&#20313;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#22122;&#38899;&#21644;&#28010;&#36153;&#21162;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26234;&#33021;&#30417;&#25511;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#26681;&#25454;&#20854;&#26381;&#21153;&#23646;&#24615;&#20026;&#20113;&#26381;&#21153;&#25512;&#33616;&#30417;&#25511;&#22120;&#12290; &#25105;&#20204;&#39318;&#20808;&#20174;&#24494;&#36719;&#30340;791&#20010;&#29983;&#20135;&#26381;&#21153;&#20013;&#25366;&#25496;&#20102;30,000&#22810;&#20010;&#30417;&#35270;&#22120;&#30340;&#23646;&#24615;&#65292;&#24182;&#20026;&#30417;&#35270;&#22120;&#23548;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#26412;&#20307;&#35770;&#12290; &#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#20010;&#20851;&#38190;&#32500;&#24230;: &#35201;&#30417;&#35270;&#30340;&#20869;&#23481;(&#36164;&#28304;)&#21644;&#35201;&#30417;&#35270;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07927v1 Announce Type: cross  Abstract: Cloud service owners need to continuously monitor their services to ensure high availability and reliability. Gaps in monitoring can lead to delay in incident detection and significant negative customer impact. Current process of monitor creation is ad-hoc and reactive in nature. Developers create monitors using their tribal knowledge and, primarily, a trial and error based process. As a result, monitors often have incomplete coverage which leads to production issues, or, redundancy which results in noise and wasted effort.   In this work, we address this issue by proposing an intelligent monitoring framework that recommends monitors for cloud services based on their service properties. We start by mining the attributes of 30,000+ monitors from 791 production services at Microsoft and derive a structured ontology for monitors. We focus on two crucial dimensions: what to monitor (resources) and which metrics to monitor. We conduct an ex
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#26102;&#31354;&#27493;&#24577;&#25968;&#25454;&#30340;&#25968;&#20540;&#39044;&#27979;&#65292;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#24182;&#22312;&#30701;&#36317;&#31163;&#21644;&#38271;&#36317;&#31163;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;</title><link>https://arxiv.org/abs/2403.07926</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#26102;&#31354;&#27493;&#24577;&#25968;&#25454;&#30340;&#25968;&#20540;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Value Prediction for Spatiotemporal Gait Data Using Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07926
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#26102;&#31354;&#27493;&#24577;&#25968;&#25454;&#30340;&#25968;&#20540;&#39044;&#27979;&#65292;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#24182;&#22312;&#30701;&#36317;&#31163;&#21644;&#38271;&#36317;&#31163;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27493;&#24577;&#36890;&#24120;&#29992;&#20110;&#35786;&#26029;&#21644;&#35780;&#20272;&#21307;&#23398;&#29366;&#20917;&#65292;&#24182;&#22312;&#27835;&#30103;&#21644;&#24247;&#22797;&#36807;&#31243;&#20013;&#30417;&#27979;&#36827;&#23637;&#12290;&#21033;&#29992;&#25429;&#33719;&#21387;&#21147;&#25110;&#36816;&#21160;&#30340;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#65292;&#20998;&#26512;&#27493;&#24577;&#25968;&#25454;&#20197;&#24110;&#21161;&#24674;&#22797;&#12289;&#35782;&#21035;&#27963;&#21160;&#25110;&#35782;&#21035;&#20010;&#20307;&#30340;&#25216;&#26415;&#24050;&#32463;&#20986;&#29616;&#12290;&#28145;&#24230;&#23398;&#20064;&#36890;&#24120;&#37319;&#29992;&#20998;&#31867;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20998;&#26512;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#25105;&#20204;&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#25299;&#23637;&#21040;&#26102;&#31354;&#27493;&#24577;&#25968;&#25454;&#30340;&#25968;&#20540;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20960;&#31181;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65288;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#21644;RNN&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30456;&#32467;&#21512;&#65289;&#65292;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#23454;&#39564;&#35774;&#32622;&#36827;&#34892;&#36817;&#31243;&#21644;&#36828;&#31243;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#36817;&#31243;&#39044;&#27979;&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#21487;&#20197;&#20302;&#33267;0&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07926v1 Announce Type: cross  Abstract: Human gait has been commonly used for the diagnosis and evaluation of medical conditions and for monitoring the progress during treatment and rehabilitation. The use of wearable sensors that capture pressure or motion has yielded techniques that analyze the gait data to aid recovery, identify activity performed, or identify individuals. Deep learning, usually employing classification, has been successfully utilized in a variety of applications such as computer vision, biomedical imaging analysis, and natural language processing. We expand the application of deep learning to value prediction of time-series of spatiotemporal gait data. Moreover, we explore several deep learning architectures (Recurrent Neural Networks (RNN) and RNN combined with Convolutional Neural Networks (CNN)) to make short- and long-distance predictions using two different experimental setups. Our results show that short-distance prediction has an RMSE as low as 0.
&lt;/p&gt;</description></item><item><title>&#35813;&#27169;&#22411;&#22522;&#20110;&#25193;&#25955;&#29983;&#25104;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#25512;&#26029;&#21407;&#23376;&#31867;&#22411;&#21644;&#20960;&#20309;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#31867;&#33647;&#20998;&#23376;&#26500;&#35937;&#30340;&#39640;&#31934;&#24230;&#29983;&#25104;&#65292;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07925</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#31867;&#33647;&#20998;&#23376;&#26500;&#35937;
&lt;/p&gt;
&lt;p&gt;
Physics-informed generative model for drug-like molecule conformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07925
&lt;/p&gt;
&lt;p&gt;
&#35813;&#27169;&#22411;&#22522;&#20110;&#25193;&#25955;&#29983;&#25104;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#25512;&#26029;&#21407;&#23376;&#31867;&#22411;&#21644;&#20960;&#20309;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#31867;&#33647;&#20998;&#23376;&#26500;&#35937;&#30340;&#39640;&#31934;&#24230;&#29983;&#25104;&#65292;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#26500;&#35937;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20391;&#37325;&#20110;&#37325;&#29616;&#25104;&#38190;&#32467;&#26500;&#65292;&#24182;&#19988;&#26159;&#20174;&#20256;&#32479;&#21147;&#22330;&#20013;&#36890;&#24120;&#25214;&#21040;&#30340;&#30456;&#20851;&#39033;&#26500;&#24314;&#30340;&#65292;&#20197;&#30830;&#20445;&#29289;&#29702;&#30456;&#20851;&#24615;&#34920;&#31034;&#12290;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#34987;&#29992;&#26469;&#20174;&#35757;&#32451;&#38598;&#20013;&#25512;&#26029;&#20986;&#21407;&#23376;&#31867;&#22411;&#21644;&#20960;&#20309;&#21442;&#25968;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#22312;&#25193;&#25955;&#29983;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#23454;&#29616;&#26500;&#35937;&#37319;&#26679;&#12290;&#36890;&#36807;&#23545;&#22823;&#22411;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#12289;&#31867;&#33647;&#20998;&#23376;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#20248;&#21270;&#20102;&#21322;&#32463;&#39564;GFN2-xTB&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#25104;&#38190;&#21442;&#25968;&#30340;&#39640;&#31934;&#24230;&#39044;&#27979;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#36824;&#19982;&#34507;&#30333;&#36136;&#25968;&#25454;&#24211;&#65288;PDB&#65289;&#21644;&#21073;&#26725;&#32467;&#26500;&#25968;&#25454;&#24211;&#65288;CSD&#65289;&#20013;&#30340;&#23454;&#39564;&#32467;&#26500;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07925v1 Announce Type: cross  Abstract: We present a diffusion-based, generative model for conformer generation. Our model is focused on the reproduction of bonded structure and is constructed from the associated terms traditionally found in classical force fields to ensure a physically relevant representation. Techniques in deep learning are used to infer atom typing and geometric parameters from a training set. Conformer sampling is achieved by taking advantage of recent advancements in diffusion-based generation. By training on large, synthetic data sets of diverse, drug-like molecules optimized with the semiempirical GFN2-xTB method, high accuracy is achieved for bonded parameters, exceeding that of conventional, knowledge-based methods. Results are also compared to experimental structures from the Protein Databank (PDB) and Cambridge Structural Database (CSD).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#36793;&#32536;&#35745;&#31639;&#30340;&#20248;&#21270;&#25511;&#21046;&#31995;&#32479;&#65292;&#36890;&#36807;&#20113;&#36793;&#21327;&#21516;&#21644;&#21160;&#24577;&#36164;&#28304;&#20998;&#37197;&#23454;&#29616;&#24037;&#19994;&#30446;&#26631;&#30340;&#30417;&#25511;&#21644;&#20248;&#21270;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#33410;&#30465;&#20102;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.07923</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#19982;&#36793;&#32536;&#35745;&#31639;&#22312;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#30340;&#23454;&#26102;&#30417;&#25511;&#19982;&#25511;&#21046;&#20248;&#21270;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
The Fusion of Deep Reinforcement Learning and Edge Computing for Real-time Monitoring and Control Optimization in IoT Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#36793;&#32536;&#35745;&#31639;&#30340;&#20248;&#21270;&#25511;&#21046;&#31995;&#32479;&#65292;&#36890;&#36807;&#20113;&#36793;&#21327;&#21516;&#21644;&#21160;&#24577;&#36164;&#28304;&#20998;&#37197;&#23454;&#29616;&#24037;&#19994;&#30446;&#26631;&#30340;&#30417;&#25511;&#21644;&#20248;&#21270;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#33410;&#30465;&#20102;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#24037;&#19994;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#23545;&#23454;&#26102;&#24615;&#33021;&#21644;&#25511;&#21046;&#36136;&#37327;&#30340;&#38656;&#27714;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#36793;&#32536;&#35745;&#31639;&#30340;&#20248;&#21270;&#25511;&#21046;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#20113;&#36793;&#21327;&#21516;&#65292;&#37096;&#32626;&#36731;&#37327;&#32423;&#31574;&#30053;&#32593;&#32476;&#22312;&#36793;&#32536;&#65292;&#39044;&#27979;&#31995;&#32479;&#29366;&#24577;&#65292;&#24182;&#20197;&#39640;&#39057;&#29575;&#36755;&#20986;&#25511;&#21046;&#65292;&#23454;&#29616;&#24037;&#19994;&#30446;&#26631;&#30340;&#30417;&#25511;&#21644;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#35774;&#35745;&#20102;&#21160;&#24577;&#36164;&#28304;&#20998;&#37197;&#26426;&#21046;&#65292;&#20197;&#30830;&#20445;&#36793;&#32536;&#35745;&#31639;&#36164;&#28304;&#30340;&#21512;&#29702;&#35843;&#24230;&#65292;&#23454;&#29616;&#20840;&#23616;&#20248;&#21270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20943;&#23569;&#20102;&#20113;&#36793;&#36890;&#20449;&#24310;&#36831;&#65292;&#21152;&#24555;&#20102;&#23545;&#24322;&#24120;&#24773;&#20917;&#30340;&#21709;&#24212;&#65292;&#38477;&#20302;&#20102;&#31995;&#32479;&#25925;&#38556;&#29575;&#65292;&#24310;&#38271;&#20102;&#35774;&#22791;&#24179;&#22343;&#36816;&#34892;&#26102;&#38388;&#65292;&#24182;&#33410;&#30465;&#20102;&#25163;&#21160;&#32500;&#25252;&#21644;&#26356;&#25442;&#25104;&#26412;&#12290;&#36825;&#30830;&#20445;&#20102;&#23454;&#26102;&#21644;&#31283;&#23450;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07923v1 Announce Type: cross  Abstract: In response to the demand for real-time performance and control quality in industrial Internet of Things (IoT) environments, this paper proposes an optimization control system based on deep reinforcement learning and edge computing. The system leverages cloud-edge collaboration, deploys lightweight policy networks at the edge, predicts system states, and outputs controls at a high frequency, enabling monitoring and optimization of industrial objectives. Additionally, a dynamic resource allocation mechanism is designed to ensure rational scheduling of edge computing resources, achieving global optimization. Results demonstrate that this approach reduces cloud-edge communication latency, accelerates response to abnormal situations, reduces system failure rates, extends average equipment operating time, and saves costs for manual maintenance and replacement. This ensures real-time and stable control.
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20449;&#24687;&#29109;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#25163;&#26426;&#21451;&#22909;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;transformer&#35299;&#30721;&#22120;&#30340;&#29109;&#26469;&#22312;&#35745;&#31639;&#39044;&#31639;&#20869;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;MeRino&#27169;&#22411;&#65292;&#22312;&#31227;&#21160;&#35774;&#32622;&#19979;&#23637;&#29616;&#20986;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;transformer&#27169;&#22411;&#31454;&#20105;&#24615;&#33021;&#30340;&#29305;&#28857;</title><link>https://arxiv.org/abs/2403.07921</link><description>&lt;p&gt;
Merino&#65306;&#22522;&#20110;&#29109;&#39537;&#21160;&#30340;IoT&#35774;&#22791;&#19978;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Merino: Entropy-driven Design for Generative Language Models on IoT Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07921
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20449;&#24687;&#29109;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#25163;&#26426;&#21451;&#22909;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;transformer&#35299;&#30721;&#22120;&#30340;&#29109;&#26469;&#22312;&#35745;&#31639;&#39044;&#31639;&#20869;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;MeRino&#27169;&#22411;&#65292;&#22312;&#31227;&#21160;&#35774;&#32622;&#19979;&#23637;&#29616;&#20986;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;transformer&#27169;&#22411;&#31454;&#20105;&#24615;&#33021;&#30340;&#29305;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#29616;&#20195;&#26102;&#20195;&#30340;&#38761;&#21629;&#24615;&#36827;&#27493;&#65292;&#28982;&#32780;&#65292;&#30452;&#25509;&#37096;&#32626;LLMs&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#65292;&#27604;&#22914;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#35774;&#22791;&#65292;&#30001;&#20110;&#20854;&#39640;&#35745;&#31639;&#25104;&#26412;&#32780;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20449;&#24687;&#29109;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#25163;&#26426;&#21451;&#22909;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35774;&#35745;&#33539;&#24335;&#26159;&#22312;&#32473;&#23450;&#30340;&#35745;&#31639;&#39044;&#31639;&#20869;&#26368;&#22823;&#21270;transformer&#35299;&#30721;&#22120;&#30340;&#29109;&#12290;&#25972;&#20010;&#35774;&#35745;&#36807;&#31243;&#28041;&#21450;&#35299;&#20915;&#19968;&#20010;&#25968;&#23398;&#35268;&#21010;&#65288;MP&#65289;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#20960;&#20998;&#38047;&#20869;&#22312;CPU&#19978;&#23436;&#25104;&#65292;&#20351;&#20854;&#20960;&#20046;&#26159;&#38646;&#25104;&#26412;&#30340;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#35774;&#35745;&#30340;&#27169;&#22411;MeRino&#65292;&#22312;&#20061;&#20010;NLP&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#31227;&#21160;&#35774;&#32622;&#19979;&#23545;&#25239;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;transformer&#27169;&#22411;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;MeRino&#22312;&#31227;&#21160;&#35774;&#32622;&#19979;&#33719;&#24471;&#20102;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#38646;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07921v1 Announce Type: cross  Abstract: Generative Large Language Models (LLMs) stand as a revolutionary advancement in the modern era of artificial intelligence (AI). However, directly deploying LLMs in resource-constrained hardware, such as Internet-of-Things (IoT) devices, is difficult due to their high computational cost. In this paper, we propose a novel information-entropy framework for designing mobile-friendly generative language models. Our key design paradigm is to maximize the entropy of transformer decoders within the given computational budgets. The whole design procedure involves solving a mathematical programming (MP) problem, which can be done on the CPU within minutes, making it nearly zero-cost. We evaluate our designed models, termed MeRino, across nine NLP downstream tasks, showing their competitive performance against the state-of-the-art autoregressive transformer models under the mobile setting. Notably, MeRino achieves similar or better zero performan
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ProtLLM&#65292;&#19968;&#31181;&#20855;&#26377;&#29420;&#29305;&#21160;&#24577;&#34507;&#30333;&#36136;&#35013;&#37197;&#26426;&#21046;&#21450;&#34507;&#30333;&#36136;&#20316;&#20026;&#21333;&#35789;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#30340;&#20132;&#32455;&#24335;&#34507;&#30333;&#36136;-&#35821;&#35328;LLM&#65292;&#24182;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#20132;&#32455;&#24335;&#34507;&#30333;&#36136;-&#25991;&#26412;&#25968;&#25454;&#38598;&#29992;&#20110;&#39044;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.07920</link><description>&lt;p&gt;
ProtLLM&#65306;&#19968;&#31181;&#20855;&#26377;&#34507;&#30333;&#36136;&#20316;&#20026;&#21333;&#35789;&#39044;&#35757;&#32451;&#30340;&#20132;&#32455;&#24335;&#34507;&#30333;&#36136;-&#35821;&#35328;LLM
&lt;/p&gt;
&lt;p&gt;
ProtLLM: An Interleaved Protein-Language LLM with Protein-as-Word Pre-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07920
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ProtLLM&#65292;&#19968;&#31181;&#20855;&#26377;&#29420;&#29305;&#21160;&#24577;&#34507;&#30333;&#36136;&#35013;&#37197;&#26426;&#21046;&#21450;&#34507;&#30333;&#36136;&#20316;&#20026;&#21333;&#35789;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#30340;&#20132;&#32455;&#24335;&#34507;&#30333;&#36136;-&#35821;&#35328;LLM&#65292;&#24182;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#20132;&#32455;&#24335;&#34507;&#30333;&#36136;-&#25991;&#26412;&#25968;&#25454;&#38598;&#29992;&#20110;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ProtLLM&#65292;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#36328;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#26082;&#26377;&#34507;&#30333;&#36136;&#20026;&#20013;&#24515;&#21448;&#26377;&#34507;&#30333;&#36136;-&#35821;&#35328;&#20219;&#21153;&#12290;ProtLLM&#20855;&#26377;&#29420;&#29305;&#30340;&#21160;&#24577;&#34507;&#30333;&#36136;&#35013;&#37197;&#26426;&#21046;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#19982;&#20219;&#24847;&#25968;&#37327;&#30340;&#34507;&#30333;&#36136;&#20132;&#32455;&#22312;&#19968;&#36215;&#30340;&#22797;&#26434;&#36755;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34507;&#30333;&#36136;&#20316;&#20026;&#21333;&#35789;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#26469;&#35757;&#32451;ProtLLM&#12290;&#36890;&#36807;&#24320;&#21457;&#19987;&#38376;&#30340;&#34507;&#30333;&#36136;&#35789;&#27719;&#34920;&#65292;&#25105;&#20204;&#36171;&#20104;&#35813;&#27169;&#22411;&#19981;&#20165;&#39044;&#27979;&#33258;&#28982;&#35821;&#35328;&#32780;&#19988;&#39044;&#27979;&#26469;&#33258;&#22823;&#37327;&#20505;&#36873;&#34507;&#30333;&#36136;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20132;&#32455;&#24335;&#34507;&#30333;&#36136;-&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#21629;&#21517;&#20026;InterPT&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#12290;&#35813;&#25968;&#25454;&#38598;&#20840;&#38754;&#28085;&#30422;&#20102;&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#65288;&#22914;&#34507;&#30333;&#36136;&#27880;&#37322;&#65289;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#65288;&#22914;&#29983;&#29289;&#30740;&#31350;&#35770;&#25991;&#65289;&#65292;&#20174;&#32780;&#36171;&#20104;ProtLLM&#29702;&#35299;&#34507;&#30333;&#36136;&#30340;&#20851;&#38190;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#32463;&#20856;&#25968;&#25454;&#38598;&#19978;&#23545;ProtLLM&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07920v1 Announce Type: cross  Abstract: We propose ProtLLM, a versatile cross-modal large language model (LLM) for both protein-centric and protein-language tasks. ProtLLM features a unique dynamic protein mounting mechanism, enabling it to handle complex inputs where the natural language text is interspersed with an arbitrary number of proteins. Besides, we propose the protein-as-word language modeling approach to train ProtLLM. By developing a specialized protein vocabulary, we equip the model with the capability to predict not just natural language but also proteins from a vast pool of candidates. Additionally, we construct a large-scale interleaved protein-text dataset, named InterPT, for pre-training. This dataset comprehensively encompasses both (1) structured data sources like protein annotations and (2) unstructured data sources like biological research papers, thereby endowing ProtLLM with crucial knowledge for understanding proteins. We evaluate ProtLLM on classic 
&lt;/p&gt;</description></item><item><title>&#24320;&#25918;&#22522;&#37329;&#20250;&#27169;&#22411;&#20855;&#26377;&#24191;&#27867;&#21487;&#29992;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#24102;&#26469;&#20102;&#37325;&#22823;&#21033;&#30410;&#65292;&#20294;&#20063;&#23384;&#22312;&#36793;&#38469;&#39118;&#38505;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#26469;&#35780;&#20272;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07918</link><description>&lt;p&gt;
&#35770;&#24320;&#25918;&#22522;&#37329;&#20250;&#27169;&#22411;&#30340;&#31038;&#20250;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Societal Impact of Open Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07918
&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22522;&#37329;&#20250;&#27169;&#22411;&#20855;&#26377;&#24191;&#27867;&#21487;&#29992;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#24102;&#26469;&#20102;&#37325;&#22823;&#21033;&#30410;&#65292;&#20294;&#20063;&#23384;&#22312;&#36793;&#38469;&#39118;&#38505;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#26469;&#35780;&#20272;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07918v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#22522;&#37329;&#20250;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#25216;&#26415;&#65306;&#23427;&#20204;&#22914;&#20309;&#20844;&#24320;&#21457;&#24067;&#30452;&#25509;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290; &#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#24320;&#25918;&#22522;&#37329;&#20250;&#27169;&#22411;&#65292;&#22312;&#36825;&#37324;&#23450;&#20041;&#20026;&#20855;&#26377;&#24191;&#27867;&#21487;&#29992;&#27169;&#22411;&#26435;&#37325;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;Llama 2&#12289;Stable Diffusion XL&#65289;&#12290; &#25105;&#20204;&#30830;&#23450;&#20102;&#24320;&#25918;&#22522;&#37329;&#20250;&#27169;&#22411;&#30340;&#20116;&#20010;&#29420;&#29305;&#23646;&#24615;&#65288;&#20363;&#22914;&#26356;&#22823;&#30340;&#21487;&#23450;&#21046;&#24615;&#12289;&#36739;&#24046;&#30340;&#30417;&#25511;&#65289;&#65292;&#36825;&#20123;&#23646;&#24615;&#23548;&#33268;&#20102;&#23427;&#20204;&#30340;&#21033;&#30410;&#21644;&#39118;&#38505;&#12290; &#24320;&#25918;&#22522;&#37329;&#20250;&#27169;&#22411;&#25552;&#20379;&#20102;&#37325;&#22823;&#30340;&#21033;&#30410;&#65292;&#20294;&#20063;&#26377;&#19968;&#20123;&#38480;&#21046;&#65292;&#28085;&#30422;&#20102;&#21019;&#26032;&#12289;&#31454;&#20105;&#12289;&#20915;&#31574;&#26435;&#30340;&#20998;&#37197;&#20197;&#21450;&#36879;&#26126;&#24230;&#12290;&#20026;&#20102;&#29702;&#35299;&#20854;&#34987;&#28389;&#29992;&#30340;&#39118;&#38505;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#20998;&#26512;&#20854;&#36793;&#38469;&#39118;&#38505;&#30340;&#39118;&#38505;&#35780;&#20272;&#26694;&#26550;&#12290; &#22312;&#20960;&#20010;&#28389;&#29992;&#21521;&#37327;&#65288;&#20363;&#22914;&#32593;&#32476;&#25915;&#20987;&#12289;&#29983;&#29289;&#27494;&#22120;&#65289;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#30340;&#30740;&#31350;&#19981;&#36275;&#20197;&#26377;&#25928;&#22320;&#34920;&#24449;&#24320;&#25918;&#22522;&#37329;&#20250;&#27169;&#22411;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#36793;&#38469;&#39118;&#38505;&#12290; &#35813;&#26694;&#26550;&#26377;&#21161;&#20110;&#25193;&#23637;&#35770;&#25991;&#20013;&#24314;&#35758;&#30340;&#20998;&#26512;&#20197;&#20272;&#35745;&#26032;&#25216;&#26415;&#30340;&#23433;&#20840;&#24615;&#36793;&#38469;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07918v1 Announce Type: cross  Abstract: Foundation models are powerful technologies: how they are released publicly directly shapes their societal impact. In this position paper, we focus on open foundation models, defined here as those with broadly available model weights (e.g. Llama 2, Stable Diffusion XL). We identify five distinctive properties (e.g. greater customizability, poor monitoring) of open foundation models that lead to both their benefits and risks. Open foundation models present significant benefits, with some caveats, that span innovation, competition, the distribution of decision-making power, and transparency. To understand their risks of misuse, we design a risk assessment framework for analyzing their marginal risk. Across several misuse vectors (e.g. cyberattacks, bioweapons), we find that current research is insufficient to effectively characterize the marginal risk of open foundation models relative to pre-existing technologies. The framework helps ex
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#29992;&#20110;&#33258;&#21160;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#36827;&#21270;&#31639;&#27861;&#20013;&#30340;&#21464;&#24322;&#25805;&#20316;&#31526;&#65292;&#22312;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#22522;&#20934;&#38598;&#19978;&#20248;&#20110;&#21333;&#29420;&#23398;&#20064;&#31574;&#30053;&#21644;&#31616;&#21333;&#36827;&#21270;&#31639;&#27861;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07917</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#20027;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#30340;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Neural-Evolutionary Algorithm for Autonomous Transit Network Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07917
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#29992;&#20110;&#33258;&#21160;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#36827;&#21270;&#31639;&#27861;&#20013;&#30340;&#21464;&#24322;&#25805;&#20316;&#31526;&#65292;&#22312;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#22522;&#20934;&#38598;&#19978;&#20248;&#20110;&#21333;&#29420;&#23398;&#20064;&#31574;&#30053;&#21644;&#31616;&#21333;&#36827;&#21270;&#31639;&#27861;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#21010;&#20844;&#20849;&#20132;&#36890;&#32593;&#32476;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20294;&#26159;&#20026;&#20102;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#20844;&#20132;&#36710;&#30340;&#22909;&#22788;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35268;&#21010;&#33258;&#21160;&#39550;&#39542;&#20844;&#20132;&#36710;&#30340;&#36335;&#32447;&#32593;&#32476;&#12290;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#26500;&#24314;&#36335;&#32447;&#32593;&#32476;&#30340;&#31574;&#30053;&#65292;&#28982;&#21518;&#23558;&#35813;&#31574;&#30053;&#29992;&#20316;&#36827;&#21270;&#31639;&#27861;&#20013;&#30340;&#22810;&#20010;&#21464;&#24322;&#25805;&#20316;&#31526;&#20043;&#19968;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;&#20844;&#20132;&#32593;&#32476;&#35774;&#35745;&#22522;&#20934;&#38598;&#19978;&#35780;&#20272;&#36825;&#31181;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#29616;&#23454;&#22522;&#20934;&#23454;&#20363;&#19978;&#30340;&#34920;&#29616;&#27604;&#21333;&#29420;&#23398;&#20064;&#30340;&#31574;&#30053;&#39640;&#20986;&#39640;&#36798;20\%&#65292;&#27604;&#31616;&#21333;&#30340;&#36827;&#21270;&#31639;&#27861;&#26041;&#27861;&#39640;&#20986;&#39640;&#36798;53%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07917v1 Announce Type: cross  Abstract: Planning a public transit network is a challenging optimization problem, but essential in order to realize the benefits of autonomous buses. We propose a novel algorithm for planning networks of routes for autonomous buses. We first train a graph neural net model as a policy for constructing route networks, and then use the policy as one of several mutation operators in a evolutionary algorithm. We evaluate this algorithm on a standard set of benchmarks for transit network design, and find that it outperforms the learned policy alone by up to 20\% and a plain evolutionary algorithm approach by up to 53\% on realistic benchmark instances.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#20013;&#65292;&#34701;&#21512;&#20102;&#20135;&#19994;&#32423;&#26041;&#27861;&#21644;&#37327;&#21270;&#37329;&#34701;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#39046;&#22495;&#26041;&#27861;&#30340;&#29420;&#29305;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2403.07916</link><description>&lt;p&gt;
&#25512;&#36827;&#25237;&#36164;&#21069;&#27839;&#65306;&#38754;&#21521;&#20135;&#19994;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Advancing Investment Frontiers: Industry-grade Deep Reinforcement Learning for Portfolio Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#20013;&#65292;&#34701;&#21512;&#20102;&#20135;&#19994;&#32423;&#26041;&#27861;&#21644;&#37327;&#21270;&#37329;&#34701;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#39046;&#22495;&#26041;&#27861;&#30340;&#29420;&#29305;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#36164;&#20135;&#31867;&#21035;&#26080;&#20851;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;&#65292;&#23558;&#20135;&#19994;&#32423;&#26041;&#27861;&#19982;&#37327;&#21270;&#37329;&#34701;&#30456;&#32467;&#21512;&#12290;&#22312;&#36825;&#19968;&#34701;&#21512;&#30340;&#26680;&#24515;&#26159;&#25105;&#20204;&#30340;&#24378;&#22823;&#26694;&#26550;&#65292;&#19981;&#20165;&#23558;&#20808;&#36827;&#30340;DRL&#31639;&#27861;&#19982;&#29616;&#20195;&#35745;&#31639;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#36824;&#24378;&#35843;&#20005;&#26684;&#30340;&#32479;&#35745;&#20998;&#26512;&#12289;&#36719;&#20214;&#24037;&#31243;&#21644;&#30417;&#31649;&#21512;&#35268;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#23558;&#37329;&#34701;&#24378;&#21270;&#23398;&#20064;&#19982;&#26426;&#22120;&#20154;&#23398;&#21644;&#25968;&#23398;&#29289;&#29702;&#20013;&#30340;&#20174;&#27169;&#25311;&#21040;&#30495;&#23454;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#30740;&#31350;&#65292;&#20026;&#25105;&#20204;&#30340;&#26694;&#26550;&#21644;&#35770;&#25454;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26368;&#32456;&#20171;&#32461;&#20102;AlphaOptimizerNet&#65292;&#19968;&#31181;&#20855;&#26377;&#19987;&#26377;&#26435;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65288;&#20197;&#21450;&#30456;&#24212;&#30340;&#24211;&#65289;&#12290;AlphaOptimizerNet&#26159;&#20174;&#26368;&#26032;&#30340;&#25991;&#29486;&#21644;&#25105;&#20204;&#29420;&#29305;&#30340;&#36328;&#23398;&#31185;&#26041;&#27861;&#30340;&#32508;&#21512;&#20013;&#21457;&#23637;&#32780;&#26469;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07916v1 Announce Type: new  Abstract: This research paper delves into the application of Deep Reinforcement Learning (DRL) in asset-class agnostic portfolio optimization, integrating industry-grade methodologies with quantitative finance. At the heart of this integration is our robust framework that not only merges advanced DRL algorithms with modern computational techniques but also emphasizes stringent statistical analysis, software engineering and regulatory compliance. To the best of our knowledge, this is the first study integrating financial Reinforcement Learning with sim-to-real methodologies from robotics and mathematical physics, thus enriching our frameworks and arguments with this unique perspective. Our research culminates with the introduction of AlphaOptimizerNet, a proprietary Reinforcement Learning agent (and corresponding library). Developed from a synthesis of state-of-the-art (SOTA) literature and our unique interdisciplinary methodology, AlphaOptimizerNe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#20219;&#21153;&#35843;&#24230;&#26041;&#26696;&#65292;&#26088;&#22312;&#23454;&#29616;&#22823;&#35268;&#27169;&#20113;&#35745;&#31639;&#31995;&#32479;&#20013;&#20219;&#21153;&#35843;&#24230;&#30340;&#26368;&#20248;&#21033;&#29992;&#21644;&#26368;&#22823;&#25191;&#34892;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.07905</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#22686;&#24378;Kubernetes&#33258;&#21160;&#35843;&#24230;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#20113;&#35745;&#31639;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhancing Kubernetes Automated Scheduling with Deep Learning and Reinforcement Techniques for Large-Scale Cloud Computing Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#20219;&#21153;&#35843;&#24230;&#26041;&#26696;&#65292;&#26088;&#22312;&#23454;&#29616;&#22823;&#35268;&#27169;&#20113;&#35745;&#31639;&#31995;&#32479;&#20013;&#20219;&#21153;&#35843;&#24230;&#30340;&#26368;&#20248;&#21033;&#29992;&#21644;&#26368;&#22823;&#25191;&#34892;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20113;&#35745;&#31639;&#24212;&#29992;&#35268;&#27169;&#30340;&#25345;&#32493;&#25193;&#22823;&#65292;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#36880;&#28176;&#25104;&#20026;&#35299;&#20915;&#22823;&#35268;&#27169;&#20113;&#35745;&#31639;&#31995;&#32479;&#33258;&#21160;&#20219;&#21153;&#35843;&#24230;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#38024;&#23545;&#22823;&#35268;&#27169;&#20113;&#35745;&#31639;&#31995;&#32479;&#20013;&#20219;&#21153;&#35843;&#24230;&#30340;&#22797;&#26434;&#24615;&#21644;&#23454;&#26102;&#24615;&#38656;&#27714;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#20219;&#21153;&#35843;&#24230;&#26041;&#26696;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23454;&#26102;&#30417;&#27979;&#21644;&#39044;&#27979;&#20113;&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#21442;&#25968;&#65292;&#20197;&#33719;&#21462;&#31995;&#32479;&#29366;&#24577;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26681;&#25454;&#23454;&#26102;&#31995;&#32479;&#29366;&#24577;&#21644;&#20219;&#21153;&#29305;&#24449;&#21160;&#24577;&#35843;&#25972;&#20219;&#21153;&#35843;&#24230;&#31574;&#30053;&#65292;&#23454;&#29616;&#31995;&#32479;&#36164;&#28304;&#30340;&#26368;&#20339;&#21033;&#29992;&#21644;&#20219;&#21153;&#25191;&#34892;&#25928;&#29575;&#30340;&#26368;&#22823;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07905v1 Announce Type: cross  Abstract: With the continuous expansion of the scale of cloud computing applications, artificial intelligence technologies such as Deep Learning and Reinforcement Learning have gradually become the key tools to solve the automated task scheduling of large-scale cloud computing systems. Aiming at the complexity and real-time requirement of task scheduling in large-scale cloud computing system, this paper proposes an automatic task scheduling scheme based on deep learning and reinforcement learning. Firstly, the deep learning technology is used to monitor and predict the parameters in the cloud computing system in real time to obtain the system status information. Then, combined with reinforcement learning algorithm, the task scheduling strategy is dynamically adjusted according to the real-time system state and task characteristics to achieve the optimal utilization of system resources and the maximum of task execution efficiency. This paper veri
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#21512;&#35268;&#21644;&#30417;&#30563;&#30340;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#65292;&#24378;&#35843;&#20102;DSA&#21644;AIA&#30417;&#31649;&#26694;&#26550;&#20013;&#23384;&#22312;&#30340;&#30417;&#31649;&#31354;&#30333;&#65292;&#24182;&#35201;&#27714;AIA&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#31038;&#20250;&#20844;&#27665;&#25552;&#20379;&#25968;&#25454;&#21644;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;</title><link>https://arxiv.org/abs/2403.07904</link><description>&lt;p&gt;
&#27491;&#35270;&#30417;&#31649;&#31354;&#30333;&#65306;&#36890;&#36807;&#32435;&#20837;&#31038;&#20250;&#20844;&#27665;&#25171;&#36896;&#36229;&#36234;AIA&#30340;&#27431;&#30431;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Addressing the Regulatory Gap: Moving Towards an EU AI Audit Ecosystem Beyond the AIA by Including Civil Society
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07904
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#21512;&#35268;&#21644;&#30417;&#30563;&#30340;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#65292;&#24378;&#35843;&#20102;DSA&#21644;AIA&#30417;&#31649;&#26694;&#26550;&#20013;&#23384;&#22312;&#30340;&#30417;&#31649;&#31354;&#30333;&#65292;&#24182;&#35201;&#27714;AIA&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#31038;&#20250;&#20844;&#27665;&#25552;&#20379;&#25968;&#25454;&#21644;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27431;&#27954;&#31435;&#27861;&#26426;&#26500;&#25552;&#20986;&#20102;&#25968;&#23383;&#26381;&#21153;&#27861;&#26696;&#65288;DSA&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#65288;AIA&#65289;&#26469;&#35268;&#33539;&#24179;&#21488;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20135;&#21697;&#12290;&#26412;&#25991;&#23457;&#26597;&#20102;&#31532;&#19977;&#26041;&#23457;&#35745;&#22312;&#36825;&#20004;&#39033;&#27861;&#24459;&#20013;&#30340;&#22320;&#20301;&#20197;&#21450;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#25552;&#20379;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#35775;&#38382;&#26435;&#38480;&#12290;&#36890;&#36807;&#32771;&#34385;&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#20013;&#31532;&#19977;&#26041;&#23457;&#35745;&#21644;&#31532;&#19977;&#26041;&#25968;&#25454;&#35775;&#38382;&#30340;&#20215;&#20540;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#30417;&#31649;&#31354;&#30333;&#65292;&#21363;&#12298;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#12299;&#27809;&#26377;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#31038;&#20250;&#20844;&#27665;&#25552;&#20379;&#25968;&#25454;&#35775;&#38382;&#26435;&#38480;&#12290;&#25105;&#20204;&#23545;&#25991;&#29486;&#30340;&#36129;&#29486;&#21253;&#25324;&#65306;&#65288;1&#65289;&#23450;&#20041;&#20102;&#19968;&#20010;&#34701;&#21512;&#21512;&#35268;&#21644;&#30417;&#30563;&#30340;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#12290;&#65288;2&#65289;&#24378;&#35843;&#20102;DSA&#21644;AIA&#30417;&#31649;&#26694;&#26550;&#20013;&#23384;&#22312;&#30340;&#30417;&#31649;&#31354;&#30333;&#65292;&#38459;&#30861;&#20102;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#30340;&#24314;&#31435;&#12290;&#65288;3&#65289;&#24378;&#35843;&#30740;&#31350;&#21644;&#31038;&#20250;&#20844;&#27665;&#30340;&#31532;&#19977;&#26041;&#23457;&#35745;&#24517;&#39035;&#25104;&#20026;&#35813;&#29983;&#24577;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#65292;&#24182;&#35201;&#27714;AIA&#21253;&#25324;&#25968;&#25454;&#21644;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07904v1 Announce Type: cross  Abstract: The European legislature has proposed the Digital Services Act (DSA) and Artificial Intelligence Act (AIA) to regulate platforms and Artificial Intelligence (AI) products. We review to what extent third-party audits are part of both laws and to what extent access to models and data is provided. By considering the value of third-party audits and third-party data access in an audit ecosystem, we identify a regulatory gap in that the Artificial Intelligence Act does not provide access to data for researchers and civil society. Our contributions to the literature include: (1) Defining an AI audit ecosystem that incorporates compliance and oversight. (2) Highlighting a regulatory gap within the DSA and AIA regulatory framework, preventing the establishment of an AI audit ecosystem. (3) Emphasizing that third-party audits by research and civil society must be part of that ecosystem and demand that the AIA include data and model access for ce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20851;&#27880;&#22810;&#22336;&#25509;&#20837;&#25216;&#26415;&#19979;&#19968;&#20195;&#65288;NGMA&#65289;&#30340;&#26368;&#26032;&#30740;&#31350;&#21644;&#21019;&#26032;&#65292;&#20197;&#21450;&#20854;&#19982;&#36793;&#32536;&#35745;&#31639;&#12289;&#32593;&#32476;&#20999;&#29255;&#12289;&#31354;&#20013;&#35745;&#31639;&#12289;&#35821;&#20041;&#36890;&#20449;&#21644;&#26426;&#22120;&#23398;&#20064;&#31561;&#20851;&#38190;&#25216;&#26415;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.07903</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#35745;&#31639;&#19982;&#36793;&#32536;&#26234;&#33021;&#26102;&#20195;&#30340;&#22810;&#22336;&#25509;&#20837;
&lt;/p&gt;
&lt;p&gt;
Multiple Access in the Era of Distributed Computing and Edge Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#22810;&#22336;&#25509;&#20837;&#25216;&#26415;&#19979;&#19968;&#20195;&#65288;NGMA&#65289;&#30340;&#26368;&#26032;&#30740;&#31350;&#21644;&#21019;&#26032;&#65292;&#20197;&#21450;&#20854;&#19982;&#36793;&#32536;&#35745;&#31639;&#12289;&#32593;&#32476;&#20999;&#29255;&#12289;&#31354;&#20013;&#35745;&#31639;&#12289;&#35821;&#20041;&#36890;&#20449;&#21644;&#26426;&#22120;&#23398;&#20064;&#31561;&#20851;&#38190;&#25216;&#26415;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20391;&#37325;&#20110;&#22522;&#30784;&#19979;&#19968;&#20195;&#22810;&#22336;&#25509;&#20837;&#65288;NGMA&#65289;&#25216;&#26415;&#30340;&#26368;&#26032;&#30740;&#31350;&#21644;&#21019;&#26032;&#65292;&#20197;&#21450;&#19982;&#31532;&#20845;&#20195;&#65288;6G&#65289;&#26080;&#32447;&#32593;&#32476;&#30340;&#20854;&#20182;&#20851;&#38190;&#25216;&#26415;&#30340;&#20849;&#23384;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#32771;&#23519;&#20102;&#22810;&#22336;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#65292;&#36825;&#23545;&#28385;&#36275;&#32593;&#32476;&#36793;&#32536;&#30340;&#25968;&#25454;&#22788;&#29702;&#21644;&#35745;&#31639;&#38656;&#27714;&#22686;&#38271;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#21450;&#32593;&#32476;&#20999;&#29255;&#12290;&#28982;&#21518;&#25105;&#20204;&#25506;&#35752;&#20102;&#31354;&#20013;&#35745;&#31639;&#65288;OTA&#65289;&#65292;&#34987;&#35748;&#20026;&#26159;&#25552;&#20379;&#21508;&#31181;&#21151;&#33021;&#24555;&#36895;&#39640;&#25928;&#35745;&#31639;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#35821;&#20041;&#36890;&#20449;&#65292;&#34987;&#35748;&#20026;&#26159;&#36890;&#36807;&#20851;&#27880;&#26377;&#24847;&#20041;&#20449;&#24687;&#30340;&#20132;&#25442;&#26469;&#25552;&#39640;&#36890;&#20449;&#31995;&#32479;&#30340;&#26377;&#25928;&#26041;&#24335;&#65292;&#20174;&#32780;&#20943;&#23569;&#19981;&#24517;&#35201;&#30340;&#25968;&#25454;&#24182;&#25552;&#39640;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#22238;&#39038;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#22810;&#22336;&#25509;&#20837;&#25216;&#26415;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#37325;&#28857;&#25918;&#22312;&#20102;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07903v1 Announce Type: cross  Abstract: This paper focuses on the latest research and innovations in fundamental next-generation multiple access (NGMA) techniques and the coexistence with other key technologies for the sixth generation (6G) of wireless networks. In more detail, we first examine multi-access edge computing (MEC), which is critical to meeting the growing demand for data processing and computational capacity at the edge of the network, as well as network slicing. We then explore over-the-air (OTA) computing, which is considered to be an approach that provides fast and efficient computation of various functions. We also explore semantic communications, identified as an effective way to improve communication systems by focusing on the exchange of meaningful information, thus minimizing unnecessary data and increasing efficiency. The interrelationship between machine learning (ML) and multiple access technologies is also reviewed, with an emphasis on federated lea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DecompDiff&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#37197;&#20307;&#20998;&#23376;&#20998;&#35299;&#20026;&#33218;&#21644;&#25903;&#26550;&#65292;&#24182;&#24341;&#20837;&#20998;&#35299;&#20808;&#39564;&#65292;&#32467;&#21512;&#38190;&#25193;&#25955;&#21644;&#26377;&#25928;&#24615;&#25351;&#23548;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#39640;&#20146;&#21644;&#21147;&#20998;&#23376;&#24182;&#20445;&#25345;&#20998;&#23376;&#24615;&#36136;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.07902</link><description>&lt;p&gt;
DecompDiff&#65306;&#20855;&#26377;&#20998;&#35299;&#20808;&#39564;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
DecompDiff: Diffusion Models with Decomposed Priors for Structure-Based Drug Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DecompDiff&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#37197;&#20307;&#20998;&#23376;&#20998;&#35299;&#20026;&#33218;&#21644;&#25903;&#26550;&#65292;&#24182;&#24341;&#20837;&#20998;&#35299;&#20808;&#39564;&#65292;&#32467;&#21512;&#38190;&#25193;&#25955;&#21644;&#26377;&#25928;&#24615;&#25351;&#23548;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#39640;&#20146;&#21644;&#21147;&#20998;&#23376;&#24182;&#20445;&#25345;&#20998;&#23376;&#24615;&#36136;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07902v1 &#20132;&#21449;&#31867;&#22411;&#65306;&#35774;&#35745;3D&#37197;&#20307;&#20197;&#36866;&#24212;&#38774;&#32467;&#21512;&#20301;&#28857;&#26159;&#33647;&#29289;&#21457;&#29616;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#33647;&#29289;&#35774;&#35745;&#26041;&#27861;&#23558;&#25152;&#26377;&#37197;&#20307;&#21407;&#23376;&#35270;&#20026;&#24179;&#31561;&#22788;&#29702;&#65292;&#24573;&#30053;&#20102;&#37197;&#20307;&#20013;&#21407;&#23376;&#22312;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#19981;&#21516;&#20316;&#29992;&#65292;&#24182;&#19988;&#22312;&#25506;&#32034;&#22823;&#22411;&#31867;&#20284;&#33647;&#29289;&#20998;&#23376;&#31354;&#38388;&#26102;&#21487;&#33021;&#25928;&#29575;&#36739;&#20302;&#12290;&#21463;&#21046;&#33647;&#24815;&#20363;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#37197;&#20307;&#20998;&#23376;&#20998;&#35299;&#20026;&#20004;&#20010;&#37096;&#20998;&#65292;&#21363;&#33218;&#21644;&#25903;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21363;DecompDiff&#65292;&#20854;&#20855;&#26377;&#23545;&#33218;&#21644;&#25903;&#26550;&#36827;&#34892;&#20998;&#35299;&#30340;&#20808;&#39564;&#12290;&#20026;&#20102;&#20419;&#36827;&#20998;&#35299;&#29983;&#25104;&#24182;&#25913;&#36827;&#29983;&#25104;&#30340;&#20998;&#23376;&#30340;&#24615;&#36136;&#65292;&#25105;&#20204;&#22312;&#27169;&#22411;&#20013;&#32467;&#21512;&#20102;&#38190;&#25193;&#25955;&#65292;&#24182;&#22312;&#37319;&#26679;&#38454;&#27573;&#21152;&#20837;&#20102;&#39069;&#22806;&#30340;&#26377;&#25928;&#24615;&#25351;&#23548;&#12290;&#23545;CrossDocked2020&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#39640;&#20146;&#21644;&#21147;&#20998;&#23376;&#30340;&#21516;&#26102;&#20445;&#25345;&#36866;&#24403;&#30340;&#20998;&#23376;&#24615;&#36136;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07902v1 Announce Type: cross  Abstract: Designing 3D ligands within a target binding site is a fundamental task in drug discovery. Existing structured-based drug design methods treat all ligand atoms equally, which ignores different roles of atoms in the ligand for drug design and can be less efficient for exploring the large drug-like molecule space. In this paper, inspired by the convention in pharmaceutical practice, we decompose the ligand molecule into two parts, namely arms and scaffold, and propose a new diffusion model, DecompDiff, with decomposed priors over arms and scaffold. In order to facilitate the decomposed generation and improve the properties of the generated molecules, we incorporate both bond diffusion in the model and additional validity guidance in the sampling phase. Extensive experiments on CrossDocked2020 show that our approach achieves state-of-the-art performance in generating high-affinity molecules while maintaining proper molecular properties an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;CLIP&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;PEFT&#30340;&#26799;&#24230;&#20173;&#21487;&#29992;&#20110;&#36827;&#34892;&#22270;&#20687;&#37325;&#26500;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;CLIP&#30340;&#37325;&#26500;&#25915;&#20987;&#26041;&#27861;MIP&#12290;</title><link>https://arxiv.org/abs/2403.07901</link><description>&lt;p&gt;
MIP: &#20174;PEFT&#26799;&#24230;&#20013;&#22522;&#20110;CLIP&#36827;&#34892;&#22270;&#20687;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
MIP: CLIP-based Image Reconstruction from PEFT Gradients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;CLIP&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;PEFT&#30340;&#26799;&#24230;&#20173;&#21487;&#29992;&#20110;&#36827;&#34892;&#22270;&#20687;&#37325;&#26500;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#38024;&#23545;CLIP&#30340;&#37325;&#26500;&#25915;&#20987;&#26041;&#27861;MIP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CLIP&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#23588;&#20854;&#26159;&#32852;&#37030;&#23398;&#20064;&#12290;&#36890;&#24120;&#65292;&#22522;&#20110;CLIP&#30340;&#32852;&#37030;&#23398;&#20064;&#37319;&#29992;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#21482;&#24494;&#35843;&#36866;&#37197;&#22120;&#21442;&#25968;&#25110;&#36719;&#25552;&#31034;&#65292;&#32780;&#19981;&#26159;&#23436;&#25972;&#30340;&#21442;&#25968;&#12290;&#23613;&#31649;PEFT&#19982;&#20256;&#32479;&#35757;&#32451;&#27169;&#24335;&#19981;&#21516;&#65292;&#20294;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#36866;&#37197;&#22120;&#25110;&#36719;&#25552;&#31034;&#30340;&#26799;&#24230;&#20173;&#28982;&#21487;&#20197;&#29992;&#20110;&#25191;&#34892;&#22270;&#20687;&#37325;&#26500;&#25915;&#20987;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Multm-In-Parvo&#65288;MIP&#65289;&#65292;&#19968;&#31181;&#38024;&#23545;&#22522;&#20110;CLIP&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#30340;&#19987;&#26377;&#37325;&#26500;&#25915;&#20987;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MIP&#21487;&#20197;&#26681;&#25454;&#36719;&#25552;&#31034;&#25110;&#36866;&#37197;&#22120;&#30340;&#26799;&#24230;&#37325;&#26500;CLIP&#35757;&#32451;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;MIP&#21253;&#25324;&#19968;&#20010;&#26631;&#31614;&#39044;&#27979;&#31574;&#30053;&#26469;&#21152;&#36895;&#25915;&#20987;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07901v1 Announce Type: cross  Abstract: Contrastive Language-Image Pre-training (CLIP) model, as an effective pre-trained multimodal neural network, has been widely used in distributed machine learning tasks, especially Federated Learning (FL). Typically, CLIP-based FL adopts Parameter-Efficient Fine-Tuning (PEFT) for model training, which only fine-tunes adapter parameters or soft prompts rather than the full parameters. Although PEFT is different from the traditional training mode, in this paper, we theoretically analyze that the gradients of adapters or soft prompts can still be used to perform image reconstruction attacks. Based on our theoretical analysis, we propose Multm-In-Parvo (MIP), a proprietary reconstruction attack method targeting CLIP-based distributed machine learning architecture. Specifically, MIP can reconstruct CLIP training images according to the gradients of soft prompts or an adapter. In addition, MIP includes a label prediction strategy to accelerat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Copula&#29109;&#30340;&#21452;&#26679;&#26412;&#26816;&#39564;&#30340;&#38750;&#21442;&#25968;&#22810;&#20803;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#21464;&#28857;&#26816;&#27979;&#65292;&#24182;&#32467;&#21512;&#20102;&#20108;&#21449;&#20998;&#21106;&#31574;&#30053;&#65292;&#26377;&#25928;&#39564;&#35777;&#20102;&#20854;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#27604;&#36739;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.07892</link><description>&lt;p&gt;
&#22522;&#20110;Copula&#29109;&#30340;&#21452;&#26679;&#26412;&#26816;&#39564;&#30340;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Change Point Detection with Copula Entropy based Two-Sample Test
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Copula&#29109;&#30340;&#21452;&#26679;&#26412;&#26816;&#39564;&#30340;&#38750;&#21442;&#25968;&#22810;&#20803;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#21464;&#28857;&#26816;&#27979;&#65292;&#24182;&#32467;&#21512;&#20102;&#20108;&#21449;&#20998;&#21106;&#31574;&#30053;&#65292;&#26377;&#25928;&#39564;&#35777;&#20102;&#20854;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#27604;&#36739;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#28857;&#26816;&#27979;&#26159;&#19968;&#31181;&#20856;&#22411;&#20219;&#21153;&#65292;&#26088;&#22312;&#21457;&#29616;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#21464;&#21270;&#65292;&#24182;&#21487;&#20197;&#29992;&#21452;&#26679;&#26412;&#26816;&#39564;&#26469;&#35299;&#20915;&#12290;Copula&#29109;&#26159;&#19968;&#20010;&#29992;&#20110;&#27979;&#37327;&#32479;&#35745;&#29420;&#31435;&#24615;&#30340;&#25968;&#23398;&#27010;&#24565;&#65292;&#26368;&#36817;&#24341;&#20837;&#20102;&#22522;&#20110;&#23427;&#30340;&#21452;&#26679;&#26412;&#26816;&#39564;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Copula&#29109;&#30340;&#21452;&#26679;&#26412;&#26816;&#39564;&#30340;&#38750;&#21442;&#25968;&#22810;&#20803;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#21464;&#28857;&#26816;&#27979;&#12290;&#39318;&#20808;&#65292;&#23558;&#21333;&#19968;&#21464;&#28857;&#26816;&#27979;&#25552;&#20986;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#27599;&#20010;&#28857;&#30340;&#19968;&#32452;&#21452;&#26679;&#26412;&#26816;&#39564;&#65292;&#24182;&#23558;&#21464;&#28857;&#35270;&#20026;&#20855;&#26377;&#26368;&#22823;&#26816;&#39564;&#32479;&#35745;&#37327;&#30340;&#28857;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#21333;&#19968;&#21464;&#28857;&#26816;&#27979;&#26041;&#27861;&#19982;&#20108;&#21449;&#20998;&#21106;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#22810;&#21464;&#28857;&#26816;&#27979;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#31867;&#20284;&#26041;&#27861;&#22312;&#27169;&#25311;&#21333;&#21464;&#37327;&#21644;&#22810;&#21464;&#37327;&#25968;&#25454;&#20197;&#21450;Nile&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07892v1 Announce Type: cross  Abstract: Change point detection is a typical task that aim to find changes in time series and can be tackled with two-sample test. Copula Entropy is a mathematical concept for measuring statistical independence and a two-sample test based on it was introduced recently. In this paper we propose a nonparametric multivariate method for multiple change point detection with the copula entropy-based two-sample test. The single change point detection is first proposed as a group of two-sample tests on every points of time series data and the change point is considered as with the maximum of the test statistics. The multiple change point detection is then proposed by combining the single change point detection method with binary segmentation strategy. We verified the effectiveness of our method and compared it with the other similar methods on the simulated univariate and multivariate data and the Nile data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;H.264&#32534;&#30721;&#21387;&#32553;&#31639;&#27861;&#30340;&#25968;&#23383;&#35270;&#39057;&#31713;&#25913;&#26816;&#27979;&#25216;&#26415;&#65292;&#36890;&#36807;&#20998;&#26512;&#23439;&#22359;&#20449;&#24687;&#21644;&#36816;&#21160;&#30690;&#37327;&#65292;&#21033;&#29992;&#30690;&#37327;&#25903;&#25345;&#26426;&#26500;&#24314;&#31435;&#27169;&#22411;&#65292;&#25104;&#21151;&#23454;&#29616;&#23545;&#35270;&#39057;&#37325;&#26032;&#21387;&#32553;&#30340;&#20934;&#30830;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.07891</link><description>&lt;p&gt;
&#22522;&#20110;&#21387;&#32553;&#31639;&#27861;&#30340;&#25968;&#23383;&#35270;&#39057;&#31713;&#25913;&#26816;&#27979;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Digital Video Manipulation Detection Technique Based on Compression Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07891
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;H.264&#32534;&#30721;&#21387;&#32553;&#31639;&#27861;&#30340;&#25968;&#23383;&#35270;&#39057;&#31713;&#25913;&#26816;&#27979;&#25216;&#26415;&#65292;&#36890;&#36807;&#20998;&#26512;&#23439;&#22359;&#20449;&#24687;&#21644;&#36816;&#21160;&#30690;&#37327;&#65292;&#21033;&#29992;&#30690;&#37327;&#25903;&#25345;&#26426;&#26500;&#24314;&#31435;&#27169;&#22411;&#65292;&#25104;&#21151;&#23454;&#29616;&#23545;&#35270;&#39057;&#37325;&#26032;&#21387;&#32553;&#30340;&#20934;&#30830;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#22270;&#20687;&#21644;&#35270;&#39057;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#36215;&#30528;&#38750;&#24120;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22914;&#20170;&#65292;&#20154;&#20204;&#21487;&#20197;&#20351;&#29992;&#37197;&#22791;&#20808;&#36827;&#38598;&#25104;&#25668;&#20687;&#22836;&#21644;&#24378;&#22823;&#22270;&#20687;&#22788;&#29702;&#24212;&#29992;&#31243;&#24207;&#30340;&#21487;&#36127;&#25285;&#31227;&#21160;&#35774;&#22791;&#12290;&#25216;&#26415;&#30340;&#21457;&#23637;&#19981;&#20165;&#20419;&#36827;&#20102;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#29983;&#25104;&#65292;&#20063;&#20419;&#36827;&#20102;&#26377;&#24847;&#23545;&#20854;&#36827;&#34892;&#20462;&#25913;&#65292;&#26080;&#35770;&#26159;&#20986;&#20110;&#23089;&#20048;&#30446;&#30340;&#36824;&#26159;&#24694;&#24847;&#30446;&#30340;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24517;&#19981;&#21487;&#23569;&#30340;&#26159;&#29992;&#20110;&#26816;&#27979;&#22270;&#20687;&#21644;&#35270;&#39057;&#31713;&#25913;&#30340;&#21462;&#35777;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#26512;H.264&#32534;&#30721;&#20351;&#29992;&#30340;&#21387;&#32553;&#31639;&#27861;&#30340;&#21462;&#35777;&#25216;&#26415;&#12290;&#37325;&#26032;&#21387;&#32553;&#30340;&#23384;&#22312;&#20351;&#29992;&#20102;&#23439;&#22359;&#20449;&#24687;&#65292;&#36825;&#26159;H.264-MPEG4&#26631;&#20934;&#30340;&#29305;&#28857;&#65292;&#20197;&#21450;&#36816;&#21160;&#30690;&#37327;&#12290;&#20351;&#29992;&#30690;&#37327;&#25903;&#25345;&#26426;&#26500;&#24314;&#31435;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#26816;&#27979;&#35270;&#39057;&#26159;&#21542;&#24050;&#32463;&#37325;&#26032;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07891v1 Announce Type: cross  Abstract: Digital images and videos play a very important role in everyday life. Nowadays, people have access the affordable mobile devices equipped with advanced integrated cameras and powerful image processing applications. Technological development facilitates not only the generation of multimedia content, but also the intentional modification of it, either with recreational or malicious purposes. This is where forensic techniques to detect manipulation of images and videos become essential. This paper proposes a forensic technique by analysing compression algorithms used by the H.264 coding. The presence of recompression uses information of macroblocks, a characteristic of the H.264-MPEG4 standard, and motion vectors. A Vector Support Machine is used to create the model that allows to accurately detect if a video has been recompressed.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20048;&#35266;&#30340;&#21069;&#30651;&#24615;&#39046;&#23548;&#32773;&#31639;&#27861;&#65288;OFTRL&#65289;&#21644;&#36866;&#24403;&#30340;&#25968;&#20540;&#26356;&#26032;&#31243;&#24207;&#65292;&#22312;&#20840;&#20449;&#24687;&#19968;&#33324;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#25214;&#21040;&#20102;$\widetilde{O}(T^{-1})$-approximate&#65288;&#31895;&#31961;&#65289;&#30456;&#20851;&#22343;&#34913;&#65292;&#36825;&#22312;$T$&#27425;&#36845;&#20195;&#20869;&#24471;&#20197;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.07890</link><description>&lt;p&gt;
$\widetilde{O}(T^{-1})$ &#25910;&#25947;&#21040;&#65288;&#31895;&#31961;&#65289;&#30456;&#20851;&#22343;&#34913;&#22312;&#20840;&#20449;&#24687;&#19968;&#33324;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
$\widetilde{O}(T^{-1})$ Convergence to (Coarse) Correlated Equilibria in Full-Information General-Sum Markov Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20048;&#35266;&#30340;&#21069;&#30651;&#24615;&#39046;&#23548;&#32773;&#31639;&#27861;&#65288;OFTRL&#65289;&#21644;&#36866;&#24403;&#30340;&#25968;&#20540;&#26356;&#26032;&#31243;&#24207;&#65292;&#22312;&#20840;&#20449;&#24687;&#19968;&#33324;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#25214;&#21040;&#20102;$\widetilde{O}(T^{-1})$-approximate&#65288;&#31895;&#31961;&#65289;&#30456;&#20851;&#22343;&#34913;&#65292;&#36825;&#22312;$T$&#27425;&#36845;&#20195;&#20869;&#24471;&#20197;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
No-regret&#23398;&#20064;&#19982;&#21338;&#24328;&#35770;&#23494;&#20999;&#30456;&#20851;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#38750;&#32806;&#21512;&#30340;&#26080;&#24724;&#23398;&#20064;&#21160;&#24577;&#65292;&#24403;&#25152;&#26377;&#29609;&#23478;&#22312;&#27491;&#21017;&#24418;&#24335;&#28216;&#25103;&#20013;&#37319;&#29992;&#26102;&#65292;&#20197;$\widetilde{O}(T^{-1})$&#30340;&#25509;&#36817;&#26368;&#20248;&#36895;&#29575;&#25910;&#25947;&#21040;&#21508;&#31181;&#22343;&#34913;&#35299;&#65292;&#36825;&#26174;&#30528;&#25913;&#36827;&#20102;&#32463;&#20856;&#26080;&#24724;&#23398;&#20064;&#32773;&#30340;$O(1/\sqrt{T})$&#36895;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#31867;&#20284;&#30340;&#25910;&#25947;&#32467;&#26524;&#24456;&#23569;&#35265;&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#36890;&#29992;&#30340;&#35774;&#32622;&#65292;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#20048;&#35266;&#30340;&#21069;&#30651;&#24615;&#39046;&#23548;&#32773;&#31639;&#27861;&#65288;OFTRL&#65289;&#65292;&#36830;&#21516;&#36866;&#24403;&#30340;&#25968;&#20540;&#26356;&#26032;&#31243;&#24207;&#65292;&#21487;&#20197;&#22312;$T$&#27425;&#36845;&#20195;&#20869;&#25214;&#21040;&#20840;&#20449;&#24687;&#19968;&#33324;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#30340;$\widetilde{O}(T^{-1})$&#36817;&#20284;&#65288;&#31895;&#31961;&#65289;&#30456;&#20851;&#22343;&#34913;&#12290;&#25968;&#20540;&#32467;&#26524;&#20063;&#21253;&#25324;&#20197;&#35777;&#23454;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07890v1 Announce Type: cross  Abstract: No-regret learning has a long history of being closely connected to game theory. Recent works have devised uncoupled no-regret learning dynamics that, when adopted by all the players in normal-form games, converge to various equilibrium solutions at a near-optimal rate of $\widetilde{O}(T^{-1})$, a significant improvement over the $O(1/\sqrt{T})$ rate of classic no-regret learners. However, analogous convergence results are scarce in Markov games, a more generic setting that lays the foundation for multi-agent reinforcement learning. In this work, we close this gap by showing that the optimistic-follow-the-regularized-leader (OFTRL) algorithm, together with appropriate value update procedures, can find $\widetilde{O}(T^{-1})$-approximate (coarse) correlated equilibria in full-information general-sum Markov games within $T$ iterations. Numerical results are also included to corroborate our theoretical findings.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#25490;&#38500;&#20116;&#31181;&#26174;&#33879;&#30340;&#24037;&#20214;&#65292;&#24182;&#24212;&#29992;&#27010;&#29575;&#38408;&#20540;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.07743</link><description>&lt;p&gt;
&#20026;&#35745;&#31639;&#30149;&#29702;&#23398;&#31995;&#32479;&#37197;&#22791;&#24037;&#20214;&#22788;&#29702;&#27969;&#27700;&#32447;&#65306;&#35745;&#31639;&#19982;&#24615;&#33021;&#26435;&#34913;&#30340;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
Equipping Computational Pathology Systems with Artifact Processing Pipelines: A Showcase for Computation and Performance Trade-offs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07743
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#25490;&#38500;&#20116;&#31181;&#26174;&#33879;&#30340;&#24037;&#20214;&#65292;&#24182;&#24212;&#29992;&#27010;&#29575;&#38408;&#20540;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#26159;&#30284;&#30151;&#35786;&#26029;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#22312;&#26174;&#24494;&#38236;&#19979;&#36827;&#34892;&#26816;&#26597;&#12290;&#28982;&#32780;&#65292;&#32452;&#32455;&#30149;&#29702;&#23398;&#22788;&#29702;&#36807;&#31243;&#20250;&#20135;&#29983;&#19968;&#20123;&#24037;&#20214;&#65292;&#26368;&#32456;&#20250;&#36716;&#31227;&#21040;&#29627;&#29827;&#36733;&#29627;&#29255;&#30340;&#25968;&#23383;&#21270;&#29256;&#26412;&#65292;&#21363;&#20840;&#29627;&#24187;&#28783;&#29255;&#12290;&#24037;&#20214;&#26159;&#35786;&#26029;&#26080;&#20851;&#30340;&#21306;&#22495;&#65292;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#65288;CPATH&#65289;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#25490;&#38500;&#24037;&#20214;&#23545;&#20110;&#21487;&#38752;&#30340;&#33258;&#21160;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#26041;&#26696;&#65292;&#29992;&#20110;&#26816;&#27979;&#21253;&#25324;&#25439;&#22351;&#32452;&#32455;&#12289;&#27169;&#31946;&#12289;&#35126;&#30385;&#32452;&#32455;&#12289;&#27668;&#27873;&#21644;&#22312;WSIs&#20013;&#30340;&#32452;&#32455;&#23398;&#26080;&#20851;&#34880;&#28082;&#31561;&#20116;&#31181;&#26174;&#33879;&#24037;&#20214;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35757;&#32451;&#29420;&#31435;&#30340;&#20108;&#20803;DL&#27169;&#22411;&#20316;&#20026;&#19987;&#23478;&#26469;&#25429;&#25417;&#29305;&#23450;&#30340;&#24037;&#20214;&#24418;&#24577;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#34701;&#21512;&#26426;&#21046;&#26469;&#38598;&#25104;&#23427;&#20204;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#23545;&#26368;&#32456;&#30340;&#27010;&#29575;&#36827;&#34892;&#27010;&#29575;&#38408;&#20540;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07743v1 Announce Type: cross  Abstract: Histopathology is a gold standard for cancer diagnosis under a microscopic examination. However, histological tissue processing procedures result in artifacts, which are ultimately transferred to the digitized version of glass slides, known as whole slide images (WSIs). Artifacts are diagnostically irrelevant areas and may result in wrong deep learning (DL) algorithms predictions. Therefore, detecting and excluding artifacts in the computational pathology (CPATH) system is essential for reliable automated diagnosis. In this paper, we propose a mixture of experts (MoE) scheme for detecting five notable artifacts, including damaged tissue, blur, folded tissue, air bubbles, and histologically irrelevant blood from WSIs. First, we train independent binary DL models as experts to capture particular artifact morphology. Then, we ensemble their predictions using a fusion mechanism. We apply probabilistic thresholding over the final probabilit
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#20855;&#36523;&#20195;&#29702;&#30340;&#20004;&#31181;&#25345;&#32493;&#23398;&#20064;&#35774;&#32622;&#65306;&#23398;&#20064;&#26032;&#34892;&#20026;&#21644;&#26032;&#29615;&#22659;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#20449;&#24230;&#24471;&#20998;&#26469;&#26356;&#26032;&#23384;&#20648;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#36991;&#20813;&#38656;&#35201;&#20219;&#21153;&#36793;&#30028;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07548</link><description>&lt;p&gt;
&#20114;&#21160;&#25351;&#20196;&#36319;&#38543;&#20195;&#29702;&#30340;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Continual Learning For Interactive Instruction Following Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07548
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#20855;&#36523;&#20195;&#29702;&#30340;&#20004;&#31181;&#25345;&#32493;&#23398;&#20064;&#35774;&#32622;&#65306;&#23398;&#20064;&#26032;&#34892;&#20026;&#21644;&#26032;&#29615;&#22659;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#20449;&#24230;&#24471;&#20998;&#26469;&#26356;&#26032;&#23384;&#20648;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#36991;&#20813;&#38656;&#35201;&#20219;&#21153;&#36793;&#30028;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36890;&#36807;&#35821;&#35328;&#25351;&#20196;&#25191;&#34892;&#26085;&#24120;&#20219;&#21153;&#30340;&#20855;&#36523;&#20195;&#29702;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#25991;&#29486;&#22823;&#37117;&#20551;&#23450;&#20195;&#29702;&#22312;&#24320;&#22987;&#26102;&#23601;&#23398;&#20064;&#25152;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26679;&#30340;&#23398;&#20064;&#22330;&#26223;&#36739;&#19981;&#29616;&#23454;&#65292;&#22240;&#20026;&#26426;&#22120;&#20154;&#20195;&#29702;&#24212;&#35813;&#22312;&#25506;&#32034;&#21644;&#24863;&#30693;&#19990;&#30028;&#30340;&#36807;&#31243;&#20013;&#19981;&#26029;&#22320;&#23398;&#20064;&#12290;&#20026;&#20102;&#26397;&#30528;&#26356;&#30495;&#23454;&#30340;&#20855;&#36523;&#20195;&#29702;&#23398;&#20064;&#22330;&#26223;&#36808;&#36827;&#19968;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25345;&#32493;&#23398;&#20064;&#35774;&#32622;&#20379;&#20855;&#36523;&#20195;&#29702;&#20351;&#29992;&#65307;&#23398;&#20064;&#26032;&#34892;&#20026;&#65288;&#34892;&#20026;&#22686;&#37327;&#23398;&#20064;&#65292;Behavior-IL&#65289;&#21644;&#26032;&#29615;&#22659;&#65288;&#29615;&#22659;&#22686;&#37327;&#23398;&#20064;&#65292;Environment-IL&#65289;&#12290;&#22312;&#20219;&#21153;&#20013;&#65292;&#20808;&#21069;&#22522;&#20110;&#8220;&#25968;&#25454;&#20808;&#39564;&#8221;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#32500;&#25252;&#36807;&#21435;&#20219;&#21153;&#30340;logits&#12290;&#28982;&#32780;&#65292;&#23384;&#20648;&#30340;&#20449;&#24687;&#24448;&#24448;&#26159;&#19981;&#20805;&#20998;&#23398;&#20064;&#30340;&#20449;&#24687;&#65292;&#38656;&#35201;&#20219;&#21153;&#36793;&#30028;&#20449;&#24687;&#65292;&#32780;&#36825;&#31181;&#20449;&#24687;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#35758;&#22522;&#20110;&#33258;&#20449;&#24230;&#24471;&#20998;&#32780;&#26080;&#38656;&#20219;&#21153;&#36793;&#30028;&#20449;&#24687;&#26469;&#26356;&#26032;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07548v1 Announce Type: new  Abstract: In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning. We argue that such a learning scenario is less realistic since a robotic agent is supposed to learn the world continuously as it explores and perceives it. To take a step towards a more realistic embodied agent learning scenario, we propose two continual learning setups for embodied agents; learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL) For the tasks, previous 'data prior' based continual learning methods maintain logits for the past tasks. However, the stored information is often insufficiently learned information and requires task boundary information, which might not always be available. Here, we propose to update them based on confidence scores without task boundary information d
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#21435;&#38500;&#26694;&#26550;GraphRevoker&#65292;&#36890;&#36807;&#22270;&#23646;&#24615;&#24863;&#30693;&#21010;&#20998;&#21644;&#22270;&#23545;&#27604;&#23376;&#27169;&#22411;&#32858;&#21512;&#65292;&#26356;&#22909;&#22320;&#20445;&#25345;&#20102;&#19981;&#21487;&#35757;&#32451;GNNs&#30340;&#27169;&#22411;&#25928;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.07353</link><description>&lt;p&gt;
&#20855;&#26377;&#39640;&#25928;&#37096;&#20998;&#37325;&#26032;&#35757;&#32451;&#30340;&#22270;&#21435;&#38500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Unlearning with Efficient Partial Retraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07353
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#21435;&#38500;&#26694;&#26550;GraphRevoker&#65292;&#36890;&#36807;&#22270;&#23646;&#24615;&#24863;&#30693;&#21010;&#20998;&#21644;&#22270;&#23545;&#27604;&#23376;&#27169;&#22411;&#32858;&#21512;&#65292;&#26356;&#22909;&#22320;&#20445;&#25345;&#20102;&#19981;&#21487;&#35757;&#32451;GNNs&#30340;&#27169;&#22411;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;GNNs &#21487;&#33021;&#20250;&#22312;&#19981;&#33391;&#30340;&#22270;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#21487;&#33021;&#20250;&#38477;&#20302;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#35753;&#24050;&#32463;&#35757;&#32451;&#36807;&#30340;GNNs&#33021;&#22815;&#26377;&#25928;&#22320;&#21435;&#38500;&#19981;&#38656;&#35201;&#30340;&#25968;&#25454;&#65292;&#19968;&#31181;&#29702;&#24819;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#22522;&#20110;&#37325;&#26032;&#35757;&#32451;&#30340;&#22270;&#21435;&#38500;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#35757;&#32451;&#22270;&#20998;&#25104;&#23376;&#22270;&#65292;&#24182;&#22312;&#20854;&#19978;&#35757;&#32451;&#23376;&#27169;&#22411;&#65292;&#20174;&#32780;&#36890;&#36807;&#37096;&#20998;&#37325;&#26032;&#35757;&#32451;&#23454;&#29616;&#24555;&#36895;&#21435;&#38500;&#12290;&#28982;&#32780;&#65292;&#22270;&#20998;&#21306;&#36807;&#31243;&#20250;&#23548;&#33268;&#35757;&#32451;&#22270;&#20013;&#30340;&#20449;&#24687;&#20002;&#22833;&#65292;&#20174;&#32780;&#23548;&#33268;&#23376;GNN&#27169;&#22411;&#30340;&#27169;&#22411;&#25928;&#29992;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07353v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in various real-world applications. However, GNNs may be trained on undesirable graph data, which can degrade their performance and reliability. To enable trained GNNs to efficiently unlearn unwanted data, a desirable solution is retraining-based graph unlearning, which partitions the training graph into subgraphs and trains sub-models on them, allowing fast unlearning through partial retraining. However, the graph partition process causes information loss in the training graph, resulting in the low model utility of sub-GNN models. In this paper, we propose GraphRevoker, a novel graph unlearning framework that better maintains the model utility of unlearnable GNNs. Specifically, we preserve the graph property with graph property-aware sharding and effectively aggregate the sub-GNN models for prediction with graph contrastive sub-model aggregation. We conduct extensive experime
&lt;/p&gt;</description></item><item><title>HiRA-Pro&#26159;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#23545;&#40784;&#22810;&#27169;&#24577;&#26102;&#31354;&#25968;&#25454;&#30340;&#36807;&#31243;&#29289;&#29702;&#39537;&#21160;&#26041;&#27861;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#23545;&#40784;&#20855;&#26377;&#20122;&#27627;&#31186;&#29616;&#35937;&#30340;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#26234;&#33021;&#21046;&#36896;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.06888</link><description>&lt;p&gt;
HiRA-Pro: &#39640;&#20998;&#36776;&#29575;&#23545;&#40784;&#22810;&#27169;&#24577;&#26102;&#31354;&#25968;&#25454;&#30340;&#36807;&#31243;&#29289;&#29702;&#39537;&#21160;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HiRA-Pro: High resolution alignment of multimodal spatio-temporal data: a process physics driven approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06888
&lt;/p&gt;
&lt;p&gt;
HiRA-Pro&#26159;&#19968;&#20010;&#39640;&#20998;&#36776;&#29575;&#23545;&#40784;&#22810;&#27169;&#24577;&#26102;&#31354;&#25968;&#25454;&#30340;&#36807;&#31243;&#29289;&#29702;&#39537;&#21160;&#26041;&#27861;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#23545;&#40784;&#20855;&#26377;&#20122;&#27627;&#31186;&#29616;&#35937;&#30340;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#26234;&#33021;&#21046;&#36896;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;HiRA-Pro&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#31243;&#24207;&#65292;&#29992;&#20110;&#22312;&#39640;&#26102;&#31354;&#20998;&#36776;&#29575;&#19979;&#23545;&#26469;&#33258;&#23637;&#31034;&#22810;&#26679;&#30636;&#24577;&#12289;&#38750;&#32447;&#24615;&#38543;&#26426;&#21160;&#24577;&#30340;&#30495;&#23454;&#19990;&#30028;&#36807;&#31243;&#21644;&#31995;&#32479;&#30340;&#22810;&#27169;&#24577;&#20449;&#21495;&#36827;&#34892;&#23545;&#40784;&#65292;&#20363;&#22914;&#21046;&#36896;&#26426;&#22120;&#12290;&#23427;&#22522;&#20110;&#35782;&#21035;&#21644;&#21516;&#27493;&#36825;&#20123;&#19981;&#21516;&#20449;&#21495;&#20013;&#26174;&#33879;&#36816;&#21160;&#23398;&#21644;&#21160;&#21147;&#23398;&#20107;&#20214;&#30340;&#36807;&#31243;&#29305;&#24449;&#12290;HiRA-Pro&#35299;&#20915;&#20102;&#23545;&#40784;&#20855;&#26377;&#20122;&#27627;&#31186;&#29616;&#35937;&#30340;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#32780;&#20256;&#32479;&#30340;&#26102;&#38388;&#25139;&#12289;&#22806;&#37096;&#35302;&#21457;&#22120;&#25110;&#22522;&#20110;&#26102;&#38047;&#30340;&#23545;&#40784;&#26041;&#27861;&#21017;&#38590;&#20197;&#32988;&#20219;&#12290;HiRA-Pro&#30340;&#26377;&#25928;&#24615;&#22312;&#26234;&#33021;&#21046;&#36896;&#29615;&#22659;&#20013;&#24471;&#21040;&#20102;&#23637;&#31034;&#65292;&#22312;&#36825;&#37324;&#65292;&#23427;&#23545;&#26469;&#33258;Optomec-LENS MTS 500&#28151;&#21512;&#26426;&#22120;&#36827;&#34892;3D&#25171;&#21360;&#21644;&#38115;&#21066;&#25805;&#20316;&#26399;&#38388;&#33719;&#21462;&#30340;13+&#36890;&#36947;&#25968;&#25454;&#36827;&#34892;&#20102;&#23545;&#40784;&#12290;&#28982;&#21518;&#65292;&#23545;&#40784;&#25968;&#25454;&#34987;&#20307;&#32032;&#21270;&#65292;&#29983;&#25104;&#23545;&#24212;&#20110;&#25152;&#21046;&#36896;&#38646;&#20214;&#19978;&#30340;&#29289;&#29702;&#20307;&#32032;&#30340;0.25&#31186;&#23545;&#40784;&#25968;&#25454;&#22359;&#12290;HiRA-Pro&#30340;&#20248;&#36234;&#24615;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#36827;&#19968;&#27493;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06888v1 Announce Type: cross  Abstract: We present HiRA-Pro, a novel procedure to align, at high spatio-temporal resolutions, multimodal signals from real-world processes and systems that exhibit diverse transient, nonlinear stochastic dynamics, such as manufacturing machines. It is based on discerning and synchronizing the process signatures of salient kinematic and dynamic events in these disparate signals. HiRA-Pro addresses the challenge of aligning data with sub-millisecond phenomena, where traditional timestamp, external trigger, or clock-based alignment methods fall short. The effectiveness of HiRA-Pro is demonstrated in a smart manufacturing context, where it aligns data from 13+ channels acquired during 3D-printing and milling operations on an Optomec-LENS MTS 500 hybrid machine. The aligned data is then voxelized to generate 0.25 second aligned data chunks that correspond to physical voxels on the produced part. The superiority of HiRA-Pro is further showcased thro
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#31867;&#22120;&#39044;&#27979;&#21464;&#37327;&#30340;&#31163;&#25955;&#20540;&#24182;&#23558;&#20854;&#20316;&#20026;&#38468;&#21152;&#21464;&#37327;&#29992;&#20110;&#20016;&#23500;&#22238;&#24402;&#38382;&#39064;&#30340;&#21021;&#22987;&#21521;&#37327;&#65292;&#32463;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06829</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#31867;&#22120;&#26500;&#24314;&#21464;&#37327;&#36741;&#21161;&#22238;&#24402;&#65306;&#19968;&#20010;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Constructing Variables Using Classifiers as an Aid to Regression: An Empirical Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06829
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#31867;&#22120;&#39044;&#27979;&#21464;&#37327;&#30340;&#31163;&#25955;&#20540;&#24182;&#23558;&#20854;&#20316;&#20026;&#38468;&#21152;&#21464;&#37327;&#29992;&#20110;&#20016;&#23500;&#22238;&#24402;&#38382;&#39064;&#30340;&#21021;&#22987;&#21521;&#37327;&#65292;&#32463;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21019;&#24314;&#21464;&#37327;&#65288;&#22312;&#22238;&#24402;&#30340;&#24773;&#20917;&#19979;&#65289;&#65292;&#20197;&#34917;&#20805;&#21021;&#22987;&#36755;&#20837;&#21521;&#37327;&#20013;&#25152;&#21253;&#21547;&#30340;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#20316;&#20026;&#39044;&#22788;&#29702;&#27493;&#39588;&#36816;&#34892;&#65292;&#22312;&#35813;&#27493;&#39588;&#20013;&#65292;&#35201;&#22238;&#24402;&#30340;&#21464;&#37327;&#30340;&#36830;&#32493;&#20540;&#34987;&#31163;&#25955;&#21270;&#20026;&#19968;&#32452;&#38388;&#38548;&#65292;&#28982;&#21518;&#29992;&#20110;&#23450;&#20041;&#20540;&#38408;&#12290;&#28982;&#21518;&#23545;&#20998;&#31867;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#39044;&#27979;&#35201;&#22238;&#24402;&#30340;&#20540;&#26159;&#21542;&#23567;&#20110;&#25110;&#31561;&#20110;&#36825;&#20123;&#38408;&#20540;&#20013;&#30340;&#27599;&#19968;&#20010;&#12290;&#28982;&#21518;&#20998;&#31867;&#22120;&#30340;&#19981;&#21516;&#36755;&#20986;&#34987;&#38142;&#25509;&#22312;&#19968;&#20010;&#38468;&#21152;&#30340;&#21464;&#37327;&#21521;&#37327;&#20013;&#65292;&#20016;&#23500;&#20102;&#22238;&#24402;&#38382;&#39064;&#30340;&#21021;&#22987;&#21521;&#37327;&#12290;&#23454;&#29616;&#30340;&#31995;&#32479;&#22240;&#27492;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#36890;&#29992;&#30340;&#39044;&#22788;&#29702;&#24037;&#20855;&#12290;&#25105;&#20204;&#36890;&#36807;5&#31181;&#31867;&#22411;&#30340;&#22238;&#24402;&#22120;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#20016;&#23500;&#26041;&#27861;&#65292;&#24182;&#22312;33&#20010;&#22238;&#24402;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06829v1 Announce Type: new  Abstract: This paper proposes a method for the automatic creation of variables (in the case of regression) that complement the information contained in the initial input vector. The method works as a pre-processing step in which the continuous values of the variable to be regressed are discretized into a set of intervals which are then used to define value thresholds. Then classifiers are trained to predict whether the value to be regressed is less than or equal to each of these thresholds. The different outputs of the classifiers are then concatenated in the form of an additional vector of variables that enriches the initial vector of the regression problem. The implemented system can thus be considered as a generic pre-processing tool. We tested the proposed enrichment method with 5 types of regressors and evaluated it in 33 regression datasets. Our experimental results confirm the interest of the approach.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Koopman&#21512;&#22863;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#21512;&#22863;&#20135;&#29983;&#20855;&#26377;&#39640;&#27169;&#22411;&#38388;&#26041;&#24046;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25913;&#21892;&#38598;&#25104;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.06757</link><description>&lt;p&gt;
Koopman&#21512;&#22863;&#29992;&#20110;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Koopman Ensembles for Probabilistic Time Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06757
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Koopman&#21512;&#22863;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#21512;&#22863;&#20135;&#29983;&#20855;&#26377;&#39640;&#27169;&#22411;&#38388;&#26041;&#24046;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25913;&#21892;&#38598;&#25104;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#32972;&#26223;&#19979;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;Koopman&#31639;&#23376;&#30340;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#32477;&#22823;&#22810;&#25968;&#36825;&#31867;&#30740;&#31350;&#20165;&#38480;&#20110;&#30830;&#23450;&#24615;&#39044;&#27979;&#65292;&#32780;&#22312;&#20687;&#27668;&#35937;&#23398;&#21644;&#27668;&#35937;&#23398;&#36825;&#26679;&#30340;&#39046;&#22495;&#65292;&#30693;&#35782;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35757;&#32451;&#27169;&#22411;&#21512;&#22863;&#20197;&#20135;&#29983;&#38543;&#26426;&#36755;&#20986;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#30495;&#23454;&#36965;&#24863;&#22270;&#20687;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#23454;&#39564;&#34920;&#26126;&#65292;&#29420;&#31435;&#35757;&#32451;&#27169;&#22411;&#30340;&#38598;&#25104;&#36807;&#20998;&#33258;&#20449;&#65292;&#24182;&#19988;&#20351;&#29992;&#26126;&#30830;&#40723;&#21169;&#25104;&#21592;&#29983;&#25104;&#20855;&#26377;&#39640;&#27169;&#22411;&#38388;&#26041;&#24046;&#30340;&#39044;&#27979;&#30340;&#35757;&#32451;&#26631;&#20934;&#26497;&#22823;&#25913;&#21892;&#20102;&#38598;&#25104;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06757v1 Announce Type: new  Abstract: In the context of an increasing popularity of data-driven models to represent dynamical systems, many machine learning-based implementations of the Koopman operator have recently been proposed. However, the vast majority of those works are limited to deterministic predictions, while the knowledge of uncertainty is critical in fields like meteorology and climatology. In this work, we investigate the training of ensembles of models to produce stochastic outputs. We show through experiments on real remote sensing image time series that ensembles of independently trained models are highly overconfident and that using a training criterion that explicitly encourages the members to produce predictions with high inter-model variances greatly improves the uncertainty quantification of the ensembles.
&lt;/p&gt;</description></item><item><title>ALL0CORE&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#23427;&#22312;&#20445;&#25345;&#35745;&#31639;&#21487;&#22788;&#29702;&#24615;&#30340;&#22522;&#30784;&#19978;&#21033;&#29992;Tucker&#20998;&#35299;&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#26680;&#30340;&#24494;&#23567;&#37096;&#20998;&#21363;&#36798;&#21040;&#19982;&#23436;&#25972;Tucker&#20998;&#35299;&#30456;&#21516;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.06153</link><description>&lt;p&gt;
ALL0CORE&#24352;&#37327;&#20998;&#35299;&#29992;&#20110;&#31232;&#30095;&#35745;&#25968;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
The ALL0CORE Tensor Decomposition for Sparse Count Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06153
&lt;/p&gt;
&lt;p&gt;
ALL0CORE&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#23427;&#22312;&#20445;&#25345;&#35745;&#31639;&#21487;&#22788;&#29702;&#24615;&#30340;&#22522;&#30784;&#19978;&#21033;&#29992;Tucker&#20998;&#35299;&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#26680;&#30340;&#24494;&#23567;&#37096;&#20998;&#21363;&#36798;&#21040;&#19982;&#23436;&#25972;Tucker&#20998;&#35299;&#30456;&#21516;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ALL0CORE&#65292;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#24418;&#24335;&#12290;ALL0CORE&#26159;&#19968;&#31181;Tucker&#20998;&#35299;&#65292;&#20854;&#20013;&#26680;&#24352;&#37327;&#30340;&#38750;&#38646;&#20803;&#32032;&#25968;&#37327;&#65288;&#21363;L0&#33539;&#25968;&#65289;&#34987;&#38480;&#21046;&#20026;&#36828;&#23567;&#20110;&#26680;&#30340;&#22823;&#23567;&#30340;&#39044;&#35774;&#20540;Q&#12290;&#34429;&#28982;&#29992;&#25143;&#35268;&#23450;&#20102;&#24635;&#39044;&#31639;Q&#65292;&#20294;&#38750;&#38646;&#20803;&#32032;&#30340;&#20301;&#32622;&#21644;&#20540;&#26159;&#28508;&#22312;&#21464;&#37327;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#20998;&#37197;&#32473;&#26680;&#24352;&#37327;&#30340;&#21508;&#20010;&#37096;&#20998;&#12290;ALL0CORE&#65292;&#21363;&#20998;&#37197;&#30340;L0&#32422;&#26463;&#26680;&#65292;&#22240;&#27492;&#26082;&#20855;&#26377;CP&#20998;&#35299;&#30340;&#35745;&#31639;&#21487;&#22788;&#29702;&#24615;&#65292;&#21448;&#20855;&#26377;Tucker&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#20196;&#20154;&#28385;&#24847;&#12290;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ALL0CORE&#36890;&#24120;&#21482;&#38656;&#20351;&#29992;&#26680;&#30340;&#24494;&#23567;&#37096;&#20998;&#65288;&#20363;&#22914;&#65374;1%&#65289;&#21363;&#21487;&#20197;&#19982;&#23436;&#25972;Tucker&#20998;&#35299;&#30456;&#21516;&#30340;&#32467;&#26524;&#65292;&#32780;&#25104;&#26412;&#20165;&#30456;&#24212;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06153v1 Announce Type: cross  Abstract: This paper introduces ALL0CORE, a new form of probabilistic non-negative tensor decomposition. ALL0CORE is a Tucker decomposition where the number of non-zero elements (i.e., the L0-norm) of the core tensor is constrained to a preset value Q much smaller than the size of the core. While the user dictates the total budget Q, the locations and values of the non-zero elements are latent variables and allocated across the core tensor during inference. ALL0CORE -- i.e., allocated L0-constrained core -- thus enjoys both the computational tractability of CP decomposition and the qualitatively appealing latent structure of Tucker. In a suite of real-data experiments, we demonstrate that ALL0CORE typically requires only tiny fractions (e.g.,~1%) of the full core to achieve the same results as full Tucker decomposition at only a correspondingly tiny fraction of the cost.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20513;&#23548;&#20026;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#30340;&#32452;&#21512;&#38382;&#39064;&#26500;&#24314;&#36890;&#29992;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#29305;&#23450;&#34920;&#31034;&#26080;&#27861;&#36328;&#36234;&#19981;&#21516;&#32452;&#21512;&#38382;&#39064;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06026</link><description>&lt;p&gt;
&#20026;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#30340;&#32452;&#21512;&#38382;&#39064;&#26500;&#24314;&#36890;&#29992;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Towards a Generic Representation of Cominatorial Problems for Learning-Based Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20513;&#23548;&#20026;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#30340;&#32452;&#21512;&#38382;&#39064;&#26500;&#24314;&#36890;&#29992;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#29305;&#23450;&#34920;&#31034;&#26080;&#27861;&#36328;&#36234;&#19981;&#21516;&#32452;&#21512;&#38382;&#39064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20154;&#21592;&#23545;&#20351;&#29992;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#20135;&#29983;&#20102;&#20852;&#36259;&#65292;&#26080;&#35770;&#26159;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#36824;&#26159;&#19982;&#20256;&#32479;&#20248;&#21270;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#26223;&#19979;&#65292;&#25361;&#25112;&#22312;&#20110;&#23558;&#30446;&#26631;&#32452;&#21512;&#38382;&#39064;&#32534;&#30721;&#25104;&#36866;&#29992;&#20110;&#23398;&#20064;&#31639;&#27861;&#30340;&#32467;&#26500;&#12290;&#35768;&#22810;&#29616;&#26377;&#20316;&#21697;&#25552;&#20986;&#20102;&#29305;&#23450;&#20110;&#38382;&#39064;&#30340;&#34920;&#31034;&#65292;&#36890;&#24120;&#20197;&#22270;&#30340;&#24418;&#24335;&#65292;&#20197;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#27867;&#21270;&#24615;&#65292;&#22240;&#20026;&#34920;&#31034;&#19981;&#33021;&#36731;&#26131;&#20174;&#19968;&#20010;&#32452;&#21512;&#38382;&#39064;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#32452;&#21512;&#38382;&#39064;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#19968;&#20123;&#23581;&#35797;&#21435;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#21482;&#25552;&#20379;&#20102;&#37096;&#20998;&#27867;&#21270;&#24615;&#12290;&#37492;&#20110;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#20513;&#23548;&#20026;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#30340;&#32452;&#21512;&#38382;&#39064;&#26397;&#30528;&#23436;&#20840;&#36890;&#29992;&#30340;&#34920;&#31034;&#26041;&#24335;&#36808;&#36827;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06026v1 Announce Type: cross  Abstract: In recent years, there has been a growing interest in using learning-based approaches for solving combinatorial problems, either in an end-to-end manner or in conjunction with traditional optimization algorithms. In both scenarios, the challenge lies in encoding the targeted combinatorial problems into a structure compatible with the learning algorithm. Many existing works have proposed problem-specific representations, often in the form of a graph, to leverage the advantages of \textit{graph neural networks}. However, these approaches lack generality, as the representation cannot be easily transferred from one combinatorial problem to another one. While some attempts have been made to bridge this gap, they still offer a partial generality only. In response to this challenge, this paper advocates for progress toward a fully generic representation of combinatorial problems for learning-based approaches. The approach we propose involves 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#24418;&#24335;&#21270;&#20102;&#33322;&#22825;&#22120;&#20219;&#21153;&#21644;&#23433;&#20840;&#35201;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21160;&#26500;&#24314;&#22870;&#21169;&#20989;&#25968;&#20197;&#23454;&#29616;&#26377;&#25928;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#25506;&#35752;&#20102;&#20174;&#23433;&#20840;LTL&#35268;&#33539;&#26500;&#24314;&#33322;&#22825;&#22120;&#23631;&#34109;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#21487;&#20197;&#25552;&#20379;&#27010;&#29575;&#20445;&#35777;&#30340;&#35774;&#35745;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;&#36825;&#20123;&#23631;&#34109;&#19982;&#19981;&#21516;&#31574;&#30053;&#30340;&#20114;&#21160;&#21644;&#22870;&#21169;&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.05693</link><description>&lt;p&gt;
&#22797;&#26434;&#33322;&#22825;&#22120;&#20219;&#21153;&#30340;&#23631;&#34109;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Shielded Deep Reinforcement Learning for Complex Spacecraft Tasking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#24418;&#24335;&#21270;&#20102;&#33322;&#22825;&#22120;&#20219;&#21153;&#21644;&#23433;&#20840;&#35201;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21160;&#26500;&#24314;&#22870;&#21169;&#20989;&#25968;&#20197;&#23454;&#29616;&#26377;&#25928;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#25506;&#35752;&#20102;&#20174;&#23433;&#20840;LTL&#35268;&#33539;&#26500;&#24314;&#33322;&#22825;&#22120;&#23631;&#34109;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#21487;&#20197;&#25552;&#20379;&#27010;&#29575;&#20445;&#35777;&#30340;&#35774;&#35745;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;&#36825;&#20123;&#23631;&#34109;&#19982;&#19981;&#21516;&#31574;&#30053;&#30340;&#20114;&#21160;&#21644;&#22870;&#21169;&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#33322;&#22825;&#22120;&#25511;&#21046;&#36890;&#36807;&#23631;&#34109;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;SDRL&#65289;&#24050;&#25104;&#20026;&#24555;&#36895;&#22686;&#38271;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#23631;&#34109;&#30340;&#26500;&#24314;&#21644;&#20219;&#21153;&#30340;&#23450;&#20041;&#20173;&#19981;&#22815;&#27491;&#24335;&#65292;&#23548;&#33268;&#31574;&#30053;&#26080;&#27861;&#20445;&#35777;&#23433;&#20840;&#24182;&#32473;RL&#20195;&#29702;&#35774;&#23450;&#20102;&#27169;&#26865;&#20004;&#21487;&#30340;&#30446;&#26631;&#12290;&#26412;&#25991;&#39318;&#20808;&#25506;&#35752;&#20102;&#20351;&#29992;&#24418;&#24335;&#35821;&#35328;&#65292;&#21363;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#65292;&#26469;&#24418;&#24335;&#21270;&#33322;&#22825;&#22120;&#20219;&#21153;&#21644;&#23433;&#20840;&#35201;&#27714;&#12290;&#28982;&#21518;&#23450;&#20041;&#20102;&#19968;&#31181;&#33258;&#21160;&#20174;co-safe LTL&#35268;&#33539;&#26500;&#24314;&#22870;&#21169;&#20989;&#25968;&#20197;&#26377;&#25928;&#35757;&#32451;SDRL&#26694;&#26550;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20026;&#33322;&#22825;&#22120;&#24212;&#29992;&#20174;&#23433;&#20840;LTL&#35268;&#33539;&#26500;&#24314;&#23631;&#34109;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#21487;&#20197;&#25552;&#20379;&#27010;&#29575;&#20445;&#35777;&#30340;&#35774;&#35745;&#12290;&#36890;&#36807;&#20960;&#20010;&#23454;&#39564;&#23637;&#31034;&#20102;&#36825;&#20123;&#23631;&#34109;&#19982;&#19981;&#21516;&#31574;&#30053;&#30340;&#20114;&#21160;&#20197;&#21450;&#22870;&#21169;&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05693v1 Announce Type: new  Abstract: Autonomous spacecraft control via Shielded Deep Reinforcement Learning (SDRL) has become a rapidly growing research area. However, the construction of shields and the definition of tasking remains informal, resulting in policies with no guarantees on safety and ambiguous goals for the RL agent. In this paper, we first explore the use of formal languages, namely Linear Temporal Logic (LTL), to formalize spacecraft tasks and safety requirements. We then define a manner in which to construct a reward function from a co-safe LTL specification automatically for effective training in SDRL framework. We also investigate methods for constructing a shield from a safe LTL specification for spacecraft applications and propose three designs that provide probabilistic guarantees. We show how these shields interact with different policies and the flexibility of the reward structure through several experiments.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#39640;&#26031;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#22312;&#39640;&#32500;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#35745;&#31639;&#26377;&#25928;&#31639;&#27861;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#34920;&#26126;&#36825;&#31181;&#22797;&#26434;&#24230;&#26159;&#20805;&#20998;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.05529</link><description>&lt;p&gt;
&#23398;&#20064;&#39640;&#26031;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Computational Complexity of Learning Gaussian Single-Index Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05529
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#39640;&#26031;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#22312;&#39640;&#32500;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#35745;&#31639;&#26377;&#25928;&#31639;&#27861;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#34920;&#26126;&#36825;&#31181;&#22797;&#26434;&#24230;&#26159;&#20805;&#20998;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#25351;&#25968;&#27169;&#22411;&#26159;&#20855;&#26377;&#26893;&#20837;&#32467;&#26500;&#30340;&#39640;&#32500;&#22238;&#24402;&#38382;&#39064;&#65292;&#20854;&#20013;&#26631;&#31614;&#20381;&#36182;&#20110;&#36890;&#36807;&#36890;&#29992;&#12289;&#38750;&#32447;&#24615;&#21644;&#28508;&#22312;&#38750;&#30830;&#23450;&#24615;&#36716;&#25442;&#30340;&#36755;&#20837;&#30340;&#26410;&#30693;&#19968;&#32500;&#25237;&#24433;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#32479;&#35745;&#25512;&#26029;&#20219;&#21153;&#31867;&#21035;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#27169;&#26495;&#65292;&#29992;&#20110;&#30740;&#31350;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#32479;&#35745;&#21644;&#35745;&#31639;&#25240;&#34935;&#12290;&#23613;&#31649;&#24674;&#22797;&#38544;&#34255;&#26041;&#21521;&#30340;&#20449;&#24687;&#35770;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#32500;&#24230;$d$&#26159;&#32447;&#24615;&#30340;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#32479;&#35745;&#26597;&#35810;&#65288;SQ&#65289;&#26694;&#26550;&#21644;&#20302;&#38454;&#22810;&#39033;&#24335;&#65288;LDP&#65289;&#26694;&#26550;&#20869;&#65292;&#35745;&#31639;&#39640;&#25928;&#30340;&#31639;&#27861;&#24517;&#39035;&#38656;&#35201;$\Omega(d^{k^\star/2})$&#20010;&#26679;&#26412;&#65292;&#20854;&#20013;$k^\star$&#26159;&#25105;&#20204;&#26126;&#30830;&#34920;&#24449;&#30340;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#8220;&#29983;&#25104;&#8221;&#25351;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#20351;&#29992;&#37096;&#20998;&#36857;&#30340;&#21305;&#37197;&#19978;&#30028;&#26469;&#35777;&#26126;&#36825;&#20010;&#26679;&#26412;&#22797;&#26434;&#24230;&#20063;&#26159;&#20805;&#20998;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05529v1 Announce Type: new  Abstract: Single-Index Models are high-dimensional regression problems with planted structure, whereby labels depend on an unknown one-dimensional projection of the input via a generic, non-linear, and potentially non-deterministic transformation. As such, they encompass a broad class of statistical inference tasks, and provide a rich template to study statistical and computational trade-offs in the high-dimensional regime.   While the information-theoretic sample complexity to recover the hidden direction is linear in the dimension $d$, we show that computationally efficient algorithms, both within the Statistical Query (SQ) and the Low-Degree Polynomial (LDP) framework, necessarily require $\Omega(d^{k^\star/2})$ samples, where $k^\star$ is a "generative" exponent associated with the model that we explicitly characterize. Moreover, we show that this sample complexity is also sufficient, by establishing matching upper bounds using a partial-trace
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;R2D2&#65292;&#29992;&#20110;&#35299;&#20915;&#23556;&#30005;&#22825;&#25991;&#23398;&#20013;&#39640;&#20998;&#36776;&#29575;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.05452</link><description>&lt;p&gt;
&#29992;&#20110;&#23556;&#30005;&#22825;&#25991;&#23398;&#20013;&#24555;&#36895;&#31934;&#23494;&#25104;&#20687;&#30340;R2D2&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31995;&#21015;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
The R2D2 deep neural network series paradigm for fast precision imaging in radio astronomy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;R2D2&#65292;&#29992;&#20110;&#35299;&#20915;&#23556;&#30005;&#22825;&#25991;&#23398;&#20013;&#39640;&#20998;&#36776;&#29575;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23556;&#30005;&#24178;&#28041;&#25104;&#20687;&#38656;&#35201;&#35299;&#20915;&#26469;&#33258;&#22823;&#25968;&#25454;&#37327;&#30340;&#39640;&#20998;&#36776;&#29575;&#39640;&#21160;&#24577;&#33539;&#22260;&#36870;&#38382;&#39064;&#12290;&#26368;&#36817;&#22522;&#20110;&#20248;&#21270;&#29702;&#35770;&#30340;&#22270;&#20687;&#37325;&#24314;&#25216;&#26415;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#25104;&#20687;&#31934;&#24230;&#33021;&#21147;&#65292;&#36828;&#36828;&#36229;&#20986;&#20102;CLEAN&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#26041;&#27861;&#21253;&#25324;&#30001;&#25163;&#24037;&#35774;&#35745;&#30340;&#27491;&#21017;&#21270;&#31639;&#23376;&#25512;&#21160;&#30340;&#20808;&#36827;&#36817;&#31471;&#31639;&#27861;&#65292;&#22914;SARA&#31995;&#21015;&#65292;&#20197;&#21450;&#30001;&#23398;&#20064;&#27491;&#21017;&#21270;&#21435;&#22122;&#22120;&#25512;&#21160;&#30340;&#28151;&#21512;&#25554;&#25300;&#65288;PnP&#65289;&#31639;&#27861;&#65292;&#22914;AIRI&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;&#21644;PnP&#32467;&#26500;&#39640;&#24230;&#36845;&#20195;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22788;&#29702;&#26410;&#26469;&#20202;&#22120;&#39044;&#26399;&#30340;&#26497;&#31471;&#25968;&#25454;&#22823;&#23567;&#30340;&#33021;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;&#8220;&#29992;&#20110;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#27531;&#24046;&#23545;&#27531;&#24046;DNN&#31995;&#21015;&#8221;&#12290;R2D2&#30340;&#37325;&#24314;&#34987;&#24418;&#25104;&#20026;&#19968;&#31995;&#21015;&#27531;&#24046;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#20316;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#65292;&#36890;&#36807;&#36845;&#20195;&#20272;&#35745;&#32780;&#24471;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05452v1 Announce Type: cross  Abstract: Radio-interferometric (RI) imaging entails solving high-resolution high-dynamic range inverse problems from large data volumes. Recent image reconstruction techniques grounded in optimization theory have demonstrated remarkable capability for imaging precision, well beyond CLEAN's capability. These range from advanced proximal algorithms propelled by handcrafted regularization operators, such as the SARA family, to hybrid plug-and-play (PnP) algorithms propelled by learned regularization denoisers, such as AIRI. Optimization and PnP structures are however highly iterative, which hinders their ability to handle the extreme data sizes expected from future instruments. To address this scalability challenge, we introduce a novel deep learning approach, dubbed ``Residual-to-Residual DNN series for high-Dynamic range imaging'. R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of Deep Neural Netw
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Multimodal ECG Instruction Tuning&#65288;MEIT&#65289;&#26694;&#26550;&#65292;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;LLMs&#21644;&#22810;&#27169;&#24577;&#25351;&#23548;&#35299;&#20915;ECG&#25253;&#21578;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;ECG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04945</link><description>&lt;p&gt;
&#20026;&#25253;&#21578;&#29983;&#25104;&#35843;&#20248;&#24515;&#30005;&#22270;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Electrocardiogram Instruction Tuning for Report Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04945
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Multimodal ECG Instruction Tuning&#65288;MEIT&#65289;&#26694;&#26550;&#65292;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;LLMs&#21644;&#22810;&#27169;&#24577;&#25351;&#23548;&#35299;&#20915;ECG&#25253;&#21578;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;ECG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20316;&#20026;&#24515;&#33039;&#30149;&#24773;&#30417;&#27979;&#30340;&#20027;&#35201;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24037;&#20855;&#65292;&#23545;&#20110;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20351;&#29992;ECG&#25968;&#25454;&#23545;&#24515;&#33039;&#30149;&#24773;&#36827;&#34892;&#20998;&#31867;&#65292;&#20294;&#24573;&#30053;&#20102;ECG&#25253;&#21578;&#29983;&#25104;&#65292;&#36825;&#19981;&#20165;&#32791;&#26102;&#65292;&#32780;&#19988;&#38656;&#35201;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;ECG&#25253;&#21578;&#29983;&#25104;&#24182;&#30830;&#20445;&#20854;&#22810;&#21151;&#33021;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Multimodal ECG Instruction Tuning&#65288;MEIT&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;\textit{&#39318;&#27425;}&#23581;&#35797;&#20351;&#29992;LLMs&#21644;&#22810;&#27169;&#24577;&#25351;&#23548;&#26469;&#35299;&#20915;ECG&#25253;&#21578;&#29983;&#25104;&#38382;&#39064;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#35780;&#20272;MEIT&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;ECG&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#21508;&#31181;LLM&#39592;&#24178;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29420;&#29305;&#22320;&#23545;&#40784;&#20102;ECG&#20449;&#21495;&#21644;&#25253;&#21578;&#30340;&#34920;&#31034;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#35780;&#20272;MEIT&#19982;&#20061;&#20010;&#24320;&#28304;LLMs&#65292;&#20351;&#29992;&#20102;&#36229;&#36807;80&#19975;&#20010;ECG&#25253;&#21578;&#12290;MEIT&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04945v1 Announce Type: new  Abstract: Electrocardiogram (ECG) serves as the primary non-invasive diagnostic tool for cardiac conditions monitoring, are crucial in assisting clinicians. Recent studies have concentrated on classifying cardiac conditions using ECG data but have overlooked ECG report generation, which is not only time-consuming but also requires clinical expertise. To automate ECG report generation and ensure its versatility, we propose the Multimodal ECG Instruction Tuning (MEIT) framework, the \textit{first} attempt to tackle ECG report generation with LLMs and multimodal instructions. To facilitate future research, we establish a benchmark to evaluate MEIT with various LLMs backbones across two large-scale ECG datasets. Our approach uniquely aligns the representations of the ECG signal and the report, and we conduct extensive experiments to benchmark MEIT with nine open source LLMs, using more than 800,000 ECG reports. MEIT's results underscore the superior p
&lt;/p&gt;</description></item><item><title>&#22823;&#20048;&#36879;&#20551;&#35774;&#25351;&#20986;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#31232;&#30095;&#23376;&#32593;&#32476;&#65292;&#35757;&#32451;&#23396;&#31435;&#23376;&#32593;&#32476;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#65292;&#35843;&#26597;&#32508;&#36848;&#20102;LTH&#29616;&#29366;&#24182;&#25552;&#20986;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;</title><link>https://arxiv.org/abs/2403.04861</link><description>&lt;p&gt;
&#22823;&#20048;&#36879;&#20551;&#35774;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Lottery Ticket Hypothesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04861
&lt;/p&gt;
&lt;p&gt;
&#22823;&#20048;&#36879;&#20551;&#35774;&#25351;&#20986;&#31070;&#32463;&#32593;&#32476;&#20013;&#23384;&#22312;&#31232;&#30095;&#23376;&#32593;&#32476;&#65292;&#35757;&#32451;&#23396;&#31435;&#23376;&#32593;&#32476;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#65292;&#35843;&#26597;&#32508;&#36848;&#20102;LTH&#29616;&#29366;&#24182;&#25552;&#20986;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#20048;&#36879;&#20551;&#35774;(LTH)&#25351;&#20986;&#65292;&#31264;&#23494;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21253;&#21547;&#19968;&#20010;&#39640;&#24230;&#31232;&#30095;&#30340;&#23376;&#32593;&#32476;&#65288;&#21363;&#65292;&#20013;&#22870;&#31080;&#65289;&#65292;&#24403;&#20197;&#23396;&#31435;&#26041;&#24335;&#35757;&#32451;&#26102;&#65292;&#21487;&#20197;&#23454;&#29616;&#27604;&#21407;&#22987;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;LTH&#22312;&#35768;&#22810;&#30740;&#31350;&#20013;&#24050;&#32463;&#22312;&#32463;&#39564;&#21644;&#29702;&#35770;&#19978;&#24471;&#21040;&#35777;&#23454;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38656;&#35201;&#35299;&#20915;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#20363;&#22914;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#24320;&#28304;&#26694;&#26550;&#21644;&#19968;&#33268;&#30340;&#23454;&#39564;&#35774;&#32622;&#23545;LTH&#26410;&#26469;&#30740;&#31350;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#27425;&#20174;&#19981;&#21516;&#35282;&#24230;&#23457;&#35270;&#20197;&#24448;&#20851;&#20110;LTH&#30340;&#30740;&#31350;&#21644;&#30740;&#31350;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#21015;&#20986;&#20102;&#36827;&#19968;&#27493;&#25506;&#32034;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#36825;&#39033;&#35843;&#26597;&#26088;&#22312;&#28145;&#20837;&#20102;&#35299;LTH&#30340;&#29616;&#29366;&#65292;&#24182;&#24320;&#21457;&#19968;&#20010;&#24471;&#21040;&#22949;&#21892;&#32500;&#25252;&#30340;&#24179;&#21488;&#65292;&#29992;&#20110;&#36827;&#34892;&#23454;&#39564;&#24182;&#19982;&#26368;&#26032;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04861v1 Announce Type: new  Abstract: The Lottery Ticket Hypothesis (LTH) states that a dense neural network model contains a highly sparse subnetwork (i.e., winning tickets) that can achieve even better performance than the original model when trained in isolation. While LTH has been proved both empirically and theoretically in many works, there still are some open issues, such as efficiency and scalability, to be addressed. Also, the lack of open-source frameworks and consensual experimental setting poses a challenge to future research on LTH. We, for the first time, examine previous research and studies on LTH from different perspectives. We also discuss issues in existing works and list potential directions for further exploration. This survey aims to provide an in-depth look at the state of LTH and develop a duly maintained platform to conduct experiments and compare with the most updated baselines.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#20132;&#21449;&#27880;&#24847;&#21147;&#65288;DCA&#65289;&#27169;&#22411;&#65292;&#26681;&#25454;&#38899;&#39057;&#21644;&#35270;&#39057;&#27169;&#24577;&#20043;&#38388;&#30340;&#24378;&#24369;&#20114;&#34917;&#20851;&#31995;&#65292;&#21160;&#24577;&#36873;&#25321;&#20132;&#21449;&#20851;&#27880;&#25110;&#19981;&#20851;&#27880;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.04661</link><description>&lt;p&gt;
&#38754;&#21521;&#38899;&#39057;-&#35270;&#39057;&#20154;&#21592;&#39564;&#35777;&#30340;&#21160;&#24577;&#20132;&#21449;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Dynamic Cross Attention for Audio-Visual Person Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04661
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#20132;&#21449;&#27880;&#24847;&#21147;&#65288;DCA&#65289;&#27169;&#22411;&#65292;&#26681;&#25454;&#38899;&#39057;&#21644;&#35270;&#39057;&#27169;&#24577;&#20043;&#38388;&#30340;&#24378;&#24369;&#20114;&#34917;&#20851;&#31995;&#65292;&#21160;&#24577;&#36873;&#25321;&#20132;&#21449;&#20851;&#27880;&#25110;&#19981;&#20851;&#27880;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#21592;&#25110;&#36523;&#20221;&#39564;&#35777;&#36890;&#24120;&#20351;&#29992;&#20010;&#20307;&#27169;&#24577;&#65288;&#22914;&#38754;&#37096;&#21644;&#22768;&#38899;&#65289;&#36827;&#34892;&#25506;&#32034;&#65292;&#20294;&#26368;&#36817;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#30340;&#38899;&#35270;&#39057;&#34701;&#21512;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#21333;&#27169;&#24577;&#26041;&#27861;&#12290;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#36890;&#24120;&#34987;&#26399;&#26395;&#20855;&#26377;&#24378;&#28872;&#30340;&#20114;&#34917;&#20851;&#31995;&#65292;&#22312;&#26377;&#25928;&#30340;&#38899;&#35270;&#39057;&#34701;&#21512;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#19981;&#24635;&#26159;&#24378;&#28872;&#30456;&#20114;&#34917;&#20805;&#65292;&#23427;&#20204;&#20063;&#21487;&#33021;&#23637;&#29616;&#20986;&#24369;&#30340;&#20114;&#34917;&#20851;&#31995;&#65292;&#23548;&#33268;&#38899;&#35270;&#39057;&#29305;&#24449;&#34920;&#31034;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#20132;&#21449;&#27880;&#24847;&#21147;&#65288;DCA&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#36328;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#20043;&#38388;&#30340;&#24378;&#24369;&#20114;&#34917;&#20851;&#31995;&#65292;&#21160;&#24577;&#36873;&#25321;&#20132;&#21449;&#20851;&#27880;&#25110;&#19981;&#20851;&#27880;&#30340;&#29305;&#24449;&#12290;&#29305;&#21035;&#22320;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26465;&#20214;&#38376;&#25511;&#23618;&#26469;&#35780;&#20272;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36129;&#29486;&#65292;&#24182;&#20165;&#36873;&#25321;&#36328;&#27169;&#24577;&#20851;&#27880;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04661v1 Announce Type: cross  Abstract: Although person or identity verification has been predominantly explored using individual modalities such as face and voice, audio-visual fusion has recently shown immense potential to outperform unimodal approaches. Audio and visual modalities are often expected to pose strong complementary relationships, which plays a crucial role in effective audio-visual fusion. However, they may not always strongly complement each other, they may also exhibit weak complementary relationships, resulting in poor audio-visual feature representations. In this paper, we propose a Dynamic Cross-Attention (DCA) model that can dynamically select the cross-attended or unattended features on the fly based on the strong or weak complementary relationships, respectively, across audio and visual modalities. In particular, a conditional gating layer is designed to evaluate the contribution of the cross-attention mechanism and choose cross-attended features only
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;SWAP-Score&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25628;&#32034;&#31354;&#38388;&#21644;&#20219;&#21153;&#20013;&#27979;&#37327;&#32593;&#32476;&#22312;&#19968;&#25209;&#36755;&#20837;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#36827;&#19968;&#27493;&#25552;&#39640;&#30456;&#20851;&#24615;&#65292;&#23454;&#29616;&#27169;&#22411;&#22823;&#23567;&#30340;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.04161</link><description>&lt;p&gt;
SWAP-NAS: &#36866;&#29992;&#20110;&#36229;&#24555;&#36895;NAS&#30340;&#26679;&#26412;&#32423;&#28608;&#27963;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
SWAP-NAS: Sample-Wise Activation Patterns For Ultra-Fast NAS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;SWAP-Score&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25628;&#32034;&#31354;&#38388;&#21644;&#20219;&#21153;&#20013;&#27979;&#37327;&#32593;&#32476;&#22312;&#19968;&#25209;&#36755;&#20837;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#36827;&#19968;&#27493;&#25552;&#39640;&#30456;&#20851;&#24615;&#65292;&#23454;&#29616;&#27169;&#22411;&#22823;&#23567;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#65288;&#21363;&#38646;&#25104;&#26412;&#20195;&#29702;&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#36991;&#20813;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#23588;&#20854;&#26159;&#22312;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#20013;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#65292;&#27604;&#22914;&#22312;&#19981;&#21516;&#25628;&#32034;&#31354;&#38388;&#21644;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#26377;&#38480;&#30340;&#20851;&#32852;&#24615;&#21644;&#24046;&#21170;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26679;&#26412;&#32423;&#28608;&#27963;&#27169;&#24335;&#21450;&#20854;&#34893;&#29983;&#29289;SWAP-Score&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#12290;&#23427;&#27979;&#37327;&#20102;&#32593;&#32476;&#22312;&#19968;&#25209;&#36755;&#20837;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;SWAP-Score&#19982;&#19981;&#21516;&#25628;&#32034;&#31354;&#38388;&#21644;&#20219;&#21153;&#20013;&#30340;&#30495;&#23454;&#24615;&#33021;&#24378;&#30456;&#20851;&#65292;&#22312;NAS-Bench-101/201/301&#21644;TransNAS-Bench-101&#19978;&#32988;&#36807;&#20102;15&#31181;&#29616;&#26377;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#24230;&#37327;&#12290;SWAP-Score&#21487;&#20197;&#36890;&#36807;&#27491;&#21017;&#21270;&#36827;&#19968;&#27493;&#22686;&#24378;&#65292;&#36825;&#22312;&#22522;&#20110;&#21333;&#20803;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#23454;&#29616;&#27169;&#22411;&#22823;&#23567;&#25511;&#21046;&#12290;&#20363;&#22914;&#65292;Spearman&#30340;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04161v1 Announce Type: new  Abstract: Training-free metrics (a.k.a. zero-cost proxies) are widely used to avoid resource-intensive neural network training, especially in Neural Architecture Search (NAS). Recent studies show that existing training-free metrics have several limitations, such as limited correlation and poor generalisation across different search spaces and tasks. Hence, we propose Sample-Wise Activation Patterns and its derivative, SWAP-Score, a novel high-performance training-free metric. It measures the expressivity of networks over a batch of input samples. The SWAP-Score is strongly correlated with ground-truth performance across various search spaces and tasks, outperforming 15 existing training-free metrics on NAS-Bench-101/201/301 and TransNAS-Bench-101. The SWAP-Score can be further enhanced by regularisation, which leads to even higher correlations in cell-based search space and enables model size control during the search. For example, Spearman's rank
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#21644;&#25216;&#26415;&#39564;&#35777;&#27010;&#24565;&#65292;&#29992;&#20110;&#23545;&#35328;&#35821;&#20013;&#30340;&#38750;&#35328;&#35821;&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23558;&#20854;&#19982;&#21547;&#20041;&#20851;&#32852;&#36215;&#26469;&#65292;&#20174;&#32780;&#20026;&#25506;&#32034;&#34920;&#36798;&#23454;&#29616;&#22810;&#23618;&#38901;&#24459;&#20107;&#20214;&#30340;&#22823;&#22411;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03522</link><description>&lt;p&gt;
&#33258;&#21457;&#24615;&#35328;&#35821;&#20013;&#30340;&#38750;&#35328;&#35821;&#20449;&#24687; - &#26397;&#30528;&#26032;&#30340;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Non-verbal information in spontaneous speech - towards a new framework of analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03522
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#21644;&#25216;&#26415;&#39564;&#35777;&#27010;&#24565;&#65292;&#29992;&#20110;&#23545;&#35328;&#35821;&#20013;&#30340;&#38750;&#35328;&#35821;&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23558;&#20854;&#19982;&#21547;&#20041;&#20851;&#32852;&#36215;&#26469;&#65292;&#20174;&#32780;&#20026;&#25506;&#32034;&#34920;&#36798;&#23454;&#29616;&#22810;&#23618;&#38901;&#24459;&#20107;&#20214;&#30340;&#22823;&#22411;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35328;&#35821;&#20013;&#30340;&#38750;&#35328;&#35821;&#20449;&#21495;&#26159;&#30001;&#38901;&#24459;&#32534;&#30721;&#30340;&#65292;&#25658;&#24102;&#30340;&#20449;&#24687;&#33539;&#22260;&#20174;&#23545;&#35805;&#34892;&#20026;&#21040;&#24577;&#24230;&#21644;&#24773;&#24863;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#25484;&#25569;&#25484;&#22768;&#32467;&#26500;&#30340;&#21407;&#21017;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#21644;&#25216;&#26415;&#39564;&#35777;&#27010;&#24565;&#65292;&#29992;&#20110;&#23545;&#38901;&#24459;&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23558;&#20854;&#19982;&#21547;&#20041;&#20851;&#32852;&#36215;&#26469;&#12290;&#35813;&#26694;&#26550;&#35299;&#37322;&#20102;&#22810;&#23618;&#38901;&#24459;&#20107;&#20214;&#30340;&#34920;&#23618;&#34920;&#31034;&#12290;&#20316;&#20026;&#23454;&#26045;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#36807;&#31243;&#65292;&#21487;&#20197;&#35299;&#24320;&#19977;&#20010;&#32423;&#21035;&#30340;&#38901;&#24459;&#29616;&#35937;&#12290;&#23427;&#20381;&#36182;&#20110;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#23454;&#29616;&#21516;&#26102;&#30340;&#22810;&#31867;&#21035;/&#22810;&#26631;&#31614;&#26816;&#27979;&#12290;&#23427;&#21487;&#20197;&#27010;&#25324;&#21508;&#31181;&#21508;&#26679;&#30340;&#33258;&#21457;&#25968;&#25454;&#65292;&#22312;&#19982;&#20154;&#31867;&#27880;&#37322;&#30456;&#24403;&#25110;&#20248;&#20110;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#12290;&#38500;&#20102;&#23545;&#38901;&#24459;&#30340;&#26631;&#20934;&#21270;&#24418;&#24335;&#21270;&#22806;&#65292;&#35299;&#24320;&#38901;&#24459;&#27169;&#24335;&#36824;&#21487;&#20197;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03522v1 Announce Type: cross  Abstract: Non-verbal signals in speech are encoded by prosody and carry information that ranges from conversation action to attitude and emotion. Despite its importance, the principles that govern prosodic structure are not yet adequately understood. This paper offers an analytical schema and a technological proof-of-concept for the categorization of prosodic signals and their association with meaning. The schema interprets surface-representations of multi-layered prosodic events. As a first step towards implementation, we present a classification process that disentangles prosodic phenomena of three orders. It relies on fine-tuning a pre-trained speech recognition model, enabling the simultaneous multi-class/multi-label detection. It generalizes over a large variety of spontaneous data, performing on a par with, or superior to, human annotation. In addition to a standardized formalization of prosody, disentangling prosodic patterns can direct a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#30740;&#28145;&#20837;&#25506;&#35752;&#20102;&#36793;&#32536;&#23398;&#20064;(EL)&#20013;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#21508;&#31181;&#26041;&#27861;&#21644;&#26041;&#27861;&#35770;&#65292;&#26088;&#22312;&#32508;&#21512;&#29616;&#26377;&#30693;&#35782;&#65292;&#35782;&#21035;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#26410;&#26469;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.02619</link><description>&lt;p&gt;
&#22312;&#36793;&#32536;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Training Machine Learning models at the Edge: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02619
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#30740;&#28145;&#20837;&#25506;&#35752;&#20102;&#36793;&#32536;&#23398;&#20064;(EL)&#20013;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#21508;&#31181;&#26041;&#27861;&#21644;&#26041;&#27861;&#35770;&#65292;&#26088;&#22312;&#32508;&#21512;&#29616;&#26377;&#30693;&#35782;&#65292;&#35782;&#21035;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#26410;&#26469;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#35745;&#31639;(EC)&#36817;&#24180;&#26469;&#33719;&#24471;&#20102;&#26174;&#33879;&#20851;&#27880;&#65292;&#36890;&#36807;&#22312;&#36793;&#32536;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;(AI)&#33021;&#21147;&#65292;&#25215;&#35834;&#25552;&#39640;&#25928;&#29575;&#12290;&#34429;&#28982;&#20027;&#35201;&#20851;&#27880;&#28857;&#22312;&#36793;&#32536;&#37096;&#32626;&#21644;&#25512;&#26029;&#26426;&#22120;&#23398;&#20064;(ML)&#27169;&#22411;&#65292;&#20294;&#35757;&#32451;&#26041;&#38754;&#20173;&#28982;&#36739;&#23569;&#34987;&#25506;&#35752;&#12290;&#36825;&#39033;&#35843;&#30740;&#28145;&#20837;&#25506;&#35752;&#20102;&#36793;&#32536;&#23398;&#20064;(EL)&#65292;&#29305;&#21035;&#26159;&#22312;&#36793;&#32536;&#20248;&#21270;ML&#27169;&#22411;&#35757;&#32451;&#30340;&#26041;&#38754;&#12290;&#20854;&#30446;&#26631;&#26159;&#20840;&#38754;&#25506;&#35752;EL&#20013;&#30340;&#19981;&#21516;&#26041;&#27861;&#21644;&#26041;&#27861;&#35770;&#65292;&#32508;&#21512;&#29616;&#26377;&#30693;&#35782;&#65292;&#35782;&#21035;&#25361;&#25112;&#65292;&#24182;&#31361;&#20986;&#26410;&#26469;&#36235;&#21183;&#12290;&#21033;&#29992;Scopus&#30340;&#39640;&#32423;&#25628;&#32034;&#65292;&#30830;&#23450;&#20102;&#20851;&#20110;EL&#30340;&#30456;&#20851;&#25991;&#29486;&#65292;&#26174;&#31034;&#20102;&#30740;&#31350;&#24037;&#20316;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#26041;&#38754;&#30340;&#32858;&#28966;&#65292;&#29305;&#21035;&#26159;&#32852;&#37030;&#23398;&#20064;(FL)&#12290;&#27492;&#35843;&#30740;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#27604;&#36739;&#29992;&#20110;&#20248;&#21270;&#36793;&#32536;&#23398;&#20064;&#30340;ML&#30340;&#25216;&#26415;&#30340;&#25351;&#21335;&#65292;&#20197;&#21450;&#23545;&#19981;&#21516;&#26694;&#26550;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02619v1 Announce Type: new  Abstract: Edge Computing (EC) has gained significant traction in recent years, promising enhanced efficiency by integrating Artificial Intelligence (AI) capabilities at the edge. While the focus has primarily been on the deployment and inference of Machine Learning (ML) models at the edge, the training aspect remains less explored. This survey delves into Edge Learning (EL), specifically the optimization of ML model training at the edge. The objective is to comprehensively explore diverse approaches and methodologies in EL, synthesize existing knowledge, identify challenges, and highlight future trends. Utilizing Scopus' advanced search, relevant literature on EL was identified, revealing a concentration of research efforts in distributed learning methods, particularly Federated Learning (FL). This survey further provides a guideline for comparing techniques used to optimize ML for edge learning, along with an exploration of different frameworks, 
&lt;/p&gt;</description></item><item><title>&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#20013;&#23384;&#22312;&#31867;&#20934;&#30830;&#29575;&#24046;&#24322;&#23548;&#33268;&#30340;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#26377;&#38382;&#39064;&#30340;&#34920;&#31034;&#26041;&#24335;&#23548;&#33268;&#27169;&#22411;&#23545;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#31867;&#21035;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#39044;&#27979;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.18133</link><description>&lt;p&gt;
&#31867;&#19981;&#24179;&#31561;&#65306;&#20851;&#20110;&#22270;&#20687;&#35782;&#21035;&#20844;&#24179;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Classes Are Not Equal: An Empirical Study on Image Recognition Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18133
&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#20013;&#23384;&#22312;&#31867;&#20934;&#30830;&#29575;&#24046;&#24322;&#23548;&#33268;&#30340;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#26377;&#38382;&#39064;&#30340;&#34920;&#31034;&#26041;&#24335;&#23548;&#33268;&#27169;&#22411;&#23545;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#31867;&#21035;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#39044;&#27979;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#22270;&#20687;&#35782;&#21035;&#20844;&#24179;&#24615;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#21363;&#22312;&#35832;&#22914;ImageNet&#20043;&#31867;&#30340;&#24179;&#34913;&#25968;&#25454;&#19978;&#23384;&#22312;&#26497;&#31471;&#31867;&#20934;&#30830;&#29575;&#24046;&#24322;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19981;&#21516;&#31867;&#21035;&#24182;&#19981;&#30456;&#31561;&#65292;&#20844;&#24179;&#24615;&#38382;&#39064;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#32593;&#32476;&#26550;&#26500;&#21644;&#27169;&#22411;&#23481;&#37327;&#19978;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#20844;&#24179;&#24615;&#30340;&#20960;&#20010;&#26377;&#36259;&#29305;&#24615;&#12290;&#39318;&#20808;&#65292;&#19981;&#20844;&#24179;&#24615;&#20027;&#35201;&#28304;&#20110;&#26377;&#38382;&#39064;&#30340;&#34920;&#31034;&#65292;&#32780;&#38750;&#20998;&#31867;&#22120;&#20559;&#24046;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#8220;&#27169;&#22411;&#39044;&#27979;&#20559;&#24046;&#8221;&#27010;&#24565;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20248;&#21270;&#36807;&#31243;&#20013;&#26377;&#38382;&#39064;&#34920;&#31034;&#30340;&#36215;&#28304;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#27169;&#22411;&#20542;&#21521;&#20110;&#23545;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#31867;&#21035;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#39044;&#27979;&#20559;&#24046;&#12290;&#36825;&#24847;&#21619;&#30528;&#26356;&#22810;&#20854;&#20182;&#31867;&#21035;&#23558;&#19982;&#36739;&#38590;&#35782;&#21035;&#30340;&#31867;&#21035;&#28151;&#28102;&#12290;&#28982;&#21518;&#65292;&#20551;&#38451;&#20363;&#65288;FPs&#65289;&#23558;&#20027;&#23548;&#20248;&#21270;&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#23548;&#33268;&#23427;&#20204;&#30340;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18133v1 Announce Type: new  Abstract: In this paper, we present an empirical study on image recognition fairness, i.e., extreme class accuracy disparity on balanced data like ImageNet. We experimentally demonstrate that classes are not equal and the fairness issue is prevalent for image classification models across various datasets, network architectures, and model capacities. Moreover, several intriguing properties of fairness are identified. First, the unfairness lies in problematic representation rather than classifier bias. Second, with the proposed concept of Model Prediction Bias, we investigate the origins of problematic representation during optimization. Our findings reveal that models tend to exhibit greater prediction biases for classes that are more challenging to recognize. It means that more other classes will be confused with harder classes. Then the False Positives (FPs) will dominate the learning in optimization, thus leading to their poor accuracy. Further,
&lt;/p&gt;</description></item><item><title>DS-Agent&#26159;&#19968;&#20010;&#33258;&#21160;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21644;&#26696;&#20363;&#25512;&#29702;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#31185;&#23398;&#20219;&#21153;&#20013;&#28789;&#27963;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#24182;&#36890;&#36807;&#21453;&#39304;&#26426;&#21046;&#25345;&#32493;&#25913;&#21892;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.17453</link><description>&lt;p&gt;
DS-Agent&#65306;&#36890;&#36807;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26696;&#20363;&#25512;&#29702;&#33021;&#21147;&#23454;&#29616;&#33258;&#21160;&#21270;&#25968;&#25454;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17453
&lt;/p&gt;
&lt;p&gt;
DS-Agent&#26159;&#19968;&#20010;&#33258;&#21160;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21644;&#26696;&#20363;&#25512;&#29702;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#31185;&#23398;&#20219;&#21153;&#20013;&#28789;&#27963;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#24182;&#36890;&#36807;&#21453;&#39304;&#26426;&#21046;&#25345;&#32493;&#25913;&#21892;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#29702;&#30340;&#28508;&#21147;&#65292;&#20197;&#33258;&#21160;&#21270;&#25968;&#25454;&#31185;&#23398;&#20219;&#21153;&#65292;&#30446;&#26631;&#26159;&#29702;&#35299;&#20219;&#21153;&#35201;&#27714;&#65292;&#28982;&#21518;&#26500;&#24314;&#21644;&#35757;&#32451;&#26368;&#21512;&#36866;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;LLM&#20195;&#29702;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#65292;&#20294;&#22312;&#36825;&#31181;&#24773;&#26223;&#19979;&#29983;&#25104;&#19981;&#21512;&#29702;&#30340;&#23454;&#39564;&#35745;&#21010;&#21463;&#21040;&#38459;&#30861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DS-Agent&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;LLM&#20195;&#29702;&#21644;&#26696;&#20363;&#25512;&#29702;&#65288;CBR&#65289;&#30340;&#26032;&#39062;&#33258;&#21160;&#21270;&#26694;&#26550;&#12290;&#22312;&#24320;&#21457;&#38454;&#27573;&#65292;DS-Agent&#36981;&#24490;CBR&#26694;&#26550;&#26469;&#26500;&#24314;&#33258;&#21160;&#36845;&#20195;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#28789;&#27963;&#21033;&#29992;&#26469;&#33258;Kaggle&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#21453;&#39304;&#26426;&#21046;&#20419;&#36827;&#19968;&#33268;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;DS-Agent&#23454;&#29616;&#20102;&#19968;&#20010;&#20302;&#36164;&#28304;&#37096;&#32626;&#38454;&#27573;&#65292;&#37319;&#29992;&#31616;&#21270;&#30340;CBR&#33539;&#20363;&#26469;&#36866;&#24212;&#24320;&#21457;&#38454;&#27573;&#25104;&#21151;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#36827;&#34892;&#30452;&#25509;&#20195;&#30721;&#29983;&#25104;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17453v1 Announce Type: new  Abstract: In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#38544;&#24335;&#25195;&#25551;&#20307;&#27169;&#22411;&#65292;&#33021;&#22815;&#36830;&#32493;&#34920;&#31034;&#20219;&#24847;&#36816;&#21160;&#65292;&#24182;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#36895;&#24230;&#21644;&#20960;&#20309;&#30896;&#25758;&#26816;&#26597;&#30340;&#20934;&#30830;&#24615;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.15281</link><description>&lt;p&gt;
&#31070;&#32463;&#38544;&#24335;&#25195;&#25551;&#20307;&#27169;&#22411;&#29992;&#20110;&#24555;&#36895;&#30896;&#25758;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Neural Implicit Swept Volume Models for Fast Collision Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15281
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#38544;&#24335;&#25195;&#25551;&#20307;&#27169;&#22411;&#65292;&#33021;&#22815;&#36830;&#32493;&#34920;&#31034;&#20219;&#24847;&#36816;&#21160;&#65292;&#24182;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#36895;&#24230;&#21644;&#20960;&#20309;&#30896;&#25758;&#26816;&#26597;&#30340;&#20934;&#30830;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30896;&#25758;&#26816;&#27979;&#26159;&#36816;&#21160;&#35268;&#21010;&#20013;&#26368;&#32791;&#26102;&#30340;&#25805;&#20316;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#25506;&#32034;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21152;&#36895;&#30896;&#25758;&#26816;&#27979;&#21644;&#22522;&#20110;&#37319;&#26679;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#20391;&#37325;&#20110;&#21033;&#29992;&#26426;&#22120;&#20154;&#20960;&#20309;&#20307;&#25110;&#26426;&#22120;&#20154;&#36816;&#21160;&#30340;&#25195;&#25551;&#20307;&#30340;&#31070;&#32463;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#38544;&#24335;&#25195;&#25551;&#20307;&#27169;&#22411;&#65292;&#39318;&#27425;&#36830;&#32493;&#34920;&#31034;&#30001;&#36215;&#22987;&#21644;&#30446;&#26631;&#37197;&#32622;&#21442;&#25968;&#21270;&#30340;&#20219;&#24847;&#36816;&#21160;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#24555;&#36895;&#35745;&#31639;&#20219;&#21153;&#31354;&#38388;&#20013;&#20219;&#24847;&#28857;&#21040;&#26426;&#22120;&#20154;&#36816;&#21160;&#30340;&#26377;&#31526;&#21495;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#23558;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26377;&#31526;&#21495;&#36317;&#31163;&#35745;&#31639;&#30340;&#36895;&#24230;&#19982;&#20960;&#20309;&#30896;&#25758;&#26816;&#26597;&#30340;&#24378;&#22823;&#20934;&#30830;&#24615;&#20445;&#35777;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15281v1 Announce Type: cross  Abstract: Collision detection is one of the most time-consuming operations during motion planning. Thus, there is an increasing interest in exploring machine learning techniques to speed up collision detection and sampling-based motion planning. A recent line of research focuses on utilizing neural signed distance functions of either the robot geometry or the swept volume of the robot motion. Building on this, we present a novel neural implicit swept volume model that is the first to continuously represent arbitrary motions parameterized by their start and goal configurations. This allows to quickly compute signed distances for any point in the task space to the robot motion. Further, we present an algorithm combining the speed of the deep learning-based signed distance computations with the strong accuracy guarantees of geometric collision checkers. We validate our approach in simulated and real-world robotic experiments, and demonstrate that i
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#30340;&#25193;&#23637;&#8212;&#8212;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#35201;&#25351;&#23548;&#65292;&#38656;&#35201;&#35880;&#24910;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#20445;&#25345;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14989</link><description>&lt;p&gt;
&#20998;&#26512;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14989
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#30340;&#25193;&#23637;&#8212;&#8212;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#35201;&#25351;&#23548;&#65292;&#38656;&#35201;&#35880;&#24910;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#20445;&#25345;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#38388;&#38548;&#21644;&#32570;&#22833;&#20540;&#23545;&#20110;&#20551;&#35774;&#19968;&#33268;&#38388;&#38548;&#21644;&#23436;&#25972;&#25968;&#25454;&#30340;&#20256;&#32479;&#26041;&#27861;&#26500;&#25104;&#25361;&#25112;&#12290;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#19982;&#24120;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#32467;&#21512;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#21521;&#37327;&#22330;&#23398;&#20064;&#36830;&#32493;&#28508;&#22312;&#34920;&#31034;&#12290;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#39033;&#25193;&#23637;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#28982;&#32780;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#38388;&#38548;&#21644;&#32570;&#22833;&#20540;&#26102;&#65292;&#36825;&#31181;&#28155;&#21152;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#12290;&#22240;&#27492;&#65292;&#20180;&#32454;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#23545;&#20110;&#20445;&#25345;&#31283;&#23450;&#24615;&#21644;&#22686;&#24378;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#31895;&#24515;&#30340;&#36873;&#25321;&#21487;&#33021;&#23548;&#33268;&#20986;&#29616;&#27809;&#26377;&#24378;&#35299;&#12289;&#38543;&#26426;&#30772;&#22351;&#25110;&#19981;&#31283;&#23450;&#30340;Euler&#31163;&#25955;&#21270;&#31561;&#19981;&#21033;&#30340;&#24615;&#36136;&#65292;&#26174;&#33879;&#24433;&#21709;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14989v1 Announce Type: cross  Abstract: Irregular sampling intervals and missing values in real-world time series data present challenges for conventional methods that assume consistent intervals and complete data. Neural Ordinary Differential Equations (Neural ODEs) offer an alternative approach, utilizing neural networks combined with ODE solvers to learn continuous latent representations through parameterized vector fields. Neural Stochastic Differential Equations (Neural SDEs) extend Neural ODEs by incorporating a diffusion term, although this addition is not trivial, particularly when addressing irregular intervals and missing values. Consequently, careful design of drift and diffusion functions is crucial for maintaining stability and enhancing performance, while incautious choices can result in adverse properties such as the absence of strong solutions, stochastic destabilization, or unstable Euler discretizations, significantly affecting Neural SDEs' performance. In 
&lt;/p&gt;</description></item><item><title>UR2M&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36164;&#28304;&#24863;&#30693;&#30340;&#20107;&#20214;&#26816;&#27979;&#26694;&#26550;&#65292;&#38024;&#23545;&#24494;&#25511;&#21046;&#22120;&#19978;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#26469;&#35299;&#20915;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26102;&#20135;&#29983;&#19981;&#20934;&#30830;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09264</link><description>&lt;p&gt;
UR2M: &#24494;&#25511;&#21046;&#22120;&#19978;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36164;&#28304;&#24863;&#30693;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09264
&lt;/p&gt;
&lt;p&gt;
UR2M&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36164;&#28304;&#24863;&#30693;&#30340;&#20107;&#20214;&#26816;&#27979;&#26694;&#26550;&#65292;&#38024;&#23545;&#24494;&#25511;&#21046;&#22120;&#19978;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#26469;&#35299;&#20915;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#26102;&#20135;&#29983;&#19981;&#20934;&#30830;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#30340;&#25968;&#25454;&#20998;&#24067;&#21457;&#29983;&#21464;&#21270;&#26102;&#23481;&#26131;&#20135;&#29983;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#36825;&#31181;&#33030;&#24369;&#24615;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#31227;&#21160;&#21307;&#30103;&#31561;&#24212;&#29992;&#20013;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26377;&#28508;&#21147;&#36890;&#36807;&#35780;&#20272;&#27169;&#22411;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25216;&#26415;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#20869;&#23384;&#65292;&#20351;&#23427;&#20204;&#22312;&#24494;&#25511;&#21046;&#22120;&#65288;MCU&#65289;&#19978;&#30340;&#23454;&#26045;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#36825;&#20010;&#38480;&#21046;&#38459;&#30861;&#20102;&#35768;&#22810;&#37325;&#35201;&#30340;&#35774;&#22791;&#19978;&#21487;&#31359;&#25140;&#20107;&#20214;&#26816;&#27979;&#65288;WED&#65289;&#24212;&#29992;&#30340;&#21487;&#34892;&#24615;&#65292;&#22914;&#24515;&#33039;&#30149;&#21457;&#20316;&#26816;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UR2M&#65292;&#19968;&#20010;&#38024;&#23545;MCU&#30340;&#26032;&#39062;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#36164;&#28304;&#24863;&#30693;&#20107;&#20214;&#26816;&#27979;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#65288;i&#65289;&#22522;&#20110;&#35777;&#25454;&#29702;&#35770;&#24320;&#21457;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;WED&#65292;&#29992;&#20110;&#20934;&#30830;&#30340;&#20107;&#20214;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09264v1 Announce Type: new Abstract: Traditional machine learning techniques are prone to generating inaccurate predictions when confronted with shifts in the distribution of data between the training and testing phases. This vulnerability can lead to severe consequences, especially in applications such as mobile healthcare. Uncertainty estimation has the potential to mitigate this issue by assessing the reliability of a model's output. However, existing uncertainty estimation techniques often require substantial computational resources and memory, making them impractical for implementation on microcontrollers (MCUs). This limitation hinders the feasibility of many important on-device wearable event detection (WED) applications, such as heart attack detection.   In this paper, we present UR2M, a novel Uncertainty and Resource-aware event detection framework for MCUs. Specifically, we (i) develop an uncertainty-aware WED based on evidential theory for accurate event detection
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#22825;&#27668;&#39044;&#27979;&#30340;&#20892;&#19994;&#25512;&#33616;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#23391;&#21152;&#25289;&#22269;&#20892;&#19994;&#38754;&#20020;&#30340;&#22825;&#27668;&#19981;&#21033;&#22240;&#32032;&#23545;&#31918;&#39135;&#29983;&#20135;&#30340;&#24433;&#21709;&#65292;&#20197;&#23454;&#29616;&#30408;&#21033;&#12289;&#21487;&#25345;&#32493;&#21644;&#20892;&#27665;&#21451;&#22909;&#30340;&#20892;&#19994;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2401.11410</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20892;&#19994;&#25512;&#33616;&#31995;&#32479;&#65306;&#19968;&#31181;&#22810;&#21464;&#37327;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Agricultural Recommendation System based on Deep Learning: A Multivariate Weather Forecasting Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11410
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#22825;&#27668;&#39044;&#27979;&#30340;&#20892;&#19994;&#25512;&#33616;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#23391;&#21152;&#25289;&#22269;&#20892;&#19994;&#38754;&#20020;&#30340;&#22825;&#27668;&#19981;&#21033;&#22240;&#32032;&#23545;&#31918;&#39135;&#29983;&#20135;&#30340;&#24433;&#21709;&#65292;&#20197;&#23454;&#29616;&#30408;&#21033;&#12289;&#21487;&#25345;&#32493;&#21644;&#20892;&#27665;&#21451;&#22909;&#30340;&#20892;&#19994;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23391;&#21152;&#25289;&#22269;&#20027;&#35201;&#26159;&#19968;&#20010;&#20892;&#19994;&#22269;&#23478;&#65292;&#20892;&#19994;&#37096;&#38376;&#23545;&#20110;&#21152;&#24555;&#32463;&#27982;&#22686;&#38271;&#21644;&#20445;&#38556;&#20154;&#27665;&#31918;&#39135;&#23433;&#20840;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#34429;&#28982;&#23391;&#21152;&#25289;&#22269;&#21171;&#21160;&#23494;&#38598;&#22411;&#20892;&#19994;&#21462;&#24471;&#20102;&#31918;&#39135;&#20135;&#37327;&#31283;&#27493;&#22686;&#38271;&#65292;&#20294;&#24120;&#24120;&#21463;&#21040;&#19981;&#21033;&#22825;&#27668;&#26465;&#20214;&#30340;&#24433;&#21709;&#65292;&#22914;&#26292;&#38632;&#12289;&#20302;&#28201;&#21644;&#24178;&#26097;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#22240;&#32032;&#20005;&#37325;&#24433;&#21709;&#20102;&#31918;&#39135;&#29983;&#20135;&#65292;&#20351;&#24471;&#22269;&#23478;&#30340;&#31918;&#39135;&#23433;&#20840;&#21463;&#21040;&#23041;&#32961;&#12290;&#20026;&#20102;&#23454;&#29616;&#30408;&#21033;&#12289;&#21487;&#25345;&#32493;&#19988;&#20892;&#27665;&#21451;&#22909;&#30340;&#20892;&#19994;&#23454;&#36341;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22825;&#27668;&#39044;&#27979;&#27169;&#22411;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#20316;&#29289;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11410v2 Announce Type: replace-cross  Abstract: Bangladesh is predominantly an agricultural country, where the agrarian sector plays an essential role in accelerating economic growth and enabling the food security of the people. The performance of this sector has an overwhelming impact on the primary macroeconomic objectives like food security, employment generation, poverty alleviation, human resources development, and other economic and social forces. Although Bangladesh's labor-intensive agriculture has achieved steady increases in food grain production, it often suffered from unfavorable weather conditions such as heavy rainfall, low temperature, and drought. Consequently, these factors hinder the production of food substantially, putting the country's overall food security in danger. In order to have a profitable, sustainable, and farmer-friendly agricultural practice, this paper proposes a context-based crop recommendation system powered by a weather forecast model. Wi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#31867;&#20154;&#30340;&#34920;&#31034;&#65292;&#21487;&#20197;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#31526;&#21512;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#25903;&#25345;&#20262;&#29702;&#31561;&#22810;&#26041;&#38754;&#30340;&#20215;&#20540;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2312.14106</link><description>&lt;p&gt;
&#23398;&#20064;&#31867;&#20154;&#34920;&#31034;&#20197;&#23454;&#29616;&#23398;&#20064;&#31867;&#20154;&#20215;&#20540;&#35266;
&lt;/p&gt;
&lt;p&gt;
Learning Human-like Representations to Enable Learning Human Values
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14106
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#31867;&#20154;&#30340;&#34920;&#31034;&#65292;&#21487;&#20197;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#31526;&#21512;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#25903;&#25345;&#20262;&#29702;&#31561;&#22810;&#26041;&#38754;&#30340;&#20215;&#20540;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#26500;&#24314;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30456;&#19968;&#33268;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#20197;&#36991;&#20813;&#36896;&#25104;&#20260;&#23475;&#25110;&#36829;&#21453;&#31038;&#20250;&#23545;&#21487;&#25509;&#21463;&#34892;&#20026;&#30340;&#26631;&#20934;&#65311;&#25105;&#20204;&#35748;&#20026;&#65292;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20043;&#38388;&#30340;&#34920;&#24449;&#23545;&#40784;&#26377;&#21161;&#20110;&#20215;&#20540;&#35266;&#30340;&#23545;&#40784;&#12290;&#20351;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23398;&#20064;&#31867;&#20154;&#31867;&#23545;&#19990;&#30028;&#30340;&#34920;&#31034;&#20855;&#26377;&#35768;&#22810;&#24050;&#30693;&#22909;&#22788;&#65292;&#21253;&#25324;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12289;&#22686;&#24378;&#23545;&#39046;&#22495;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#21644;&#25552;&#39640;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#36825;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#34920;&#31034;&#23545;&#40784;&#20063;&#21487;&#20197;&#25903;&#25345;&#20215;&#20540;&#23545;&#40784;&#65292;&#20351;ML&#31995;&#32479;&#36981;&#24490;&#20154;&#31867;&#20215;&#20540;&#35266;&#21644;&#31038;&#20250;&#35268;&#33539;&#12290;&#25105;&#20204;&#20851;&#27880;&#20262;&#29702;&#23398;&#20316;&#20026;&#20215;&#20540;&#23545;&#40784;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#24182;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#35774;&#32622;&#20013;&#20351;&#29992;&#21508;&#31181;&#26041;&#27861;&#35757;&#32451;ML&#20195;&#29702;&#65292;&#20854;&#20013;&#22870;&#21169;&#21453;&#26144;&#25152;&#36873;&#34892;&#21160;&#30340;&#36947;&#24503;&#21487;&#25509;&#21463;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21512;&#25104;&#23454;&#39564;&#26469;&#35777;&#26126;&#20195;&#29702;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#34920;&#31034;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14106v2 Announce Type: replace  Abstract: How can we build AI systems that are aligned with human values to avoid causing harm or violating societal standards for acceptable behavior? We argue that representational alignment between humans and AI agents facilitates value alignment. Making AI systems learn human-like representations of the world has many known benefits, including improving generalization, robustness to domain shifts, and few-shot learning performance. We propose that this kind of representational alignment between machine learning (ML) models and humans can also support value alignment, allowing ML systems to conform to human values and societal norms. We focus on ethics as one aspect of value alignment and train ML agents using a variety of methods in a multi-armed bandit setting, where rewards reflect the moral acceptability of the chosen action. We use a synthetic experiment to demonstrate that agents' representational alignment with the environment bounds
&lt;/p&gt;</description></item><item><title>SkillDiffuser&#36890;&#36807;&#23558;&#21487;&#35299;&#37322;&#30340;&#25216;&#33021;&#23398;&#20064;&#19982;&#26465;&#20214;&#25193;&#25955;&#35268;&#21010;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#39640;&#23618;&#25351;&#20196;&#19979;&#29983;&#25104;&#36830;&#36143;&#36712;&#36857;&#30340;&#20998;&#23618;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2312.11598</link><description>&lt;p&gt;
SkillDiffuser: &#36890;&#36807;&#25216;&#33021;&#25277;&#35937;&#22312;&#22522;&#20110;&#25193;&#25955;&#30340;&#20219;&#21153;&#25191;&#34892;&#20013;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#20998;&#23618;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
SkillDiffuser: Interpretable Hierarchical Planning via Skill Abstractions in Diffusion-Based Task Execution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11598
&lt;/p&gt;
&lt;p&gt;
SkillDiffuser&#36890;&#36807;&#23558;&#21487;&#35299;&#37322;&#30340;&#25216;&#33021;&#23398;&#20064;&#19982;&#26465;&#20214;&#25193;&#25955;&#35268;&#21010;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#39640;&#23618;&#25351;&#20196;&#19979;&#29983;&#25104;&#36830;&#36143;&#36712;&#36857;&#30340;&#20998;&#23618;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#26426;&#22120;&#20154;&#36712;&#36857;&#35268;&#21010;&#26041;&#38754;&#30340;&#24378;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20174;&#39640;&#23618;&#25351;&#20196;&#29983;&#25104;&#36830;&#36143;&#30340;&#36712;&#36857;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38656;&#35201;&#22810;&#20010;&#39034;&#24207;&#25216;&#33021;&#30340;&#38271;&#36317;&#31163;&#32452;&#21512;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SkillDiffuser&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#20998;&#23618;&#35268;&#21010;&#26694;&#26550;&#65292;&#23558;&#21487;&#35299;&#37322;&#30340;&#25216;&#33021;&#23398;&#20064;&#19982;&#26465;&#20214;&#25193;&#25955;&#35268;&#21010;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#22312;&#36739;&#39640;&#23618;&#27425;&#65292;&#25216;&#33021;&#25277;&#35937;&#27169;&#22359;&#20174;&#35270;&#35273;&#35266;&#23519;&#21644;&#35821;&#35328;&#25351;&#20196;&#20013;&#23398;&#20064;&#31163;&#25955;&#30340;&#12289;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#25216;&#33021;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#25216;&#33021;&#23884;&#20837;&#34987;&#29992;&#26469;&#26465;&#20214;&#21270;&#25193;&#25955;&#27169;&#22411;&#65292;&#29983;&#25104;&#19982;&#25216;&#33021;&#23545;&#40784;&#30340;&#23450;&#21046;&#28508;&#22312;&#36712;&#36857;&#12290;&#36825;&#20801;&#35768;&#29983;&#25104;&#31526;&#21512;&#21487;&#23398;&#20064;&#25216;&#33021;&#30340;&#22810;&#26679;&#29366;&#24577;&#36712;&#36857;&#12290;&#36890;&#36807;&#23558;&#25216;&#33021;&#23398;&#20064;&#19982;&#26465;&#20214;&#36712;&#36857;&#29983;&#25104;&#30456;&#32467;&#21512;&#65292;SkillDiffuser&#20135;&#29983;&#36830;&#36143;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11598v2 Announce Type: replace-cross  Abstract: Diffusion models have demonstrated strong potential for robotic trajectory planning. However, generating coherent trajectories from high-level instructions remains challenging, especially for long-range composition tasks requiring multiple sequential skills. We propose SkillDiffuser, an end-to-end hierarchical planning framework integrating interpretable skill learning with conditional diffusion planning to address this problem. At the higher level, the skill abstraction module learns discrete, human-understandable skill representations from visual observations and language instructions. These learned skill embeddings are then used to condition the diffusion model to generate customized latent trajectories aligned with the skills. This allows generating diverse state trajectories that adhere to the learnable skills. By integrating skill learning with conditional trajectory generation, SkillDiffuser produces coherent behavior fo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20462;&#25913;&#22522;&#20110;&#36712;&#36857;&#25104;&#26412;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#22312;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#27169;&#20223;&#22909;&#30340;&#36712;&#36857;&#21644;&#36991;&#20813;&#22351;&#30340;&#36712;&#36857;&#26469;&#25913;&#36827;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2312.10385</link><description>&lt;p&gt;
&#27169;&#20223;&#22909;&#30340;&#24182;&#36991;&#20813;&#22351;&#30340;&#65306;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#22686;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Imitate the Good and Avoid the Bad: An Incremental Approach to Safe Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10385
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20462;&#25913;&#22522;&#20110;&#36712;&#36857;&#25104;&#26412;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#22312;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#27169;&#20223;&#22909;&#30340;&#36712;&#36857;&#21644;&#36991;&#20813;&#22351;&#30340;&#36712;&#36857;&#26469;&#25913;&#36827;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#25191;&#34892;&#23433;&#20840;&#21160;&#20316;&#30340;&#27969;&#34892;&#26694;&#26550;&#26159;&#32422;&#26463;RL&#65292;&#20854;&#20013;&#21033;&#29992;&#22522;&#20110;&#36712;&#36857;&#30340;&#25104;&#26412;&#32422;&#26463;&#65288;&#25110;&#20854;&#20182;&#25104;&#26412;&#24230;&#37327;&#65289;&#26469;&#25191;&#34892;&#23433;&#20840;&#25805;&#20316;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#22312;&#26368;&#22823;&#21270;&#26399;&#26395;&#22870;&#21169;&#30340;&#21516;&#26102;&#25191;&#34892;&#36825;&#20123;&#32422;&#26463;&#12290;&#26368;&#36817;&#35299;&#20915;&#32422;&#26463;RL&#30340;&#26041;&#27861;&#23558;&#22522;&#20110;&#36712;&#36857;&#30340;&#25104;&#26412;&#32422;&#26463;&#36716;&#25442;&#20026;&#19968;&#20010;&#26367;&#20195;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;RL&#26041;&#27861;&#36827;&#34892;&#36731;&#24494;&#20462;&#25913;&#26469;&#35299;&#20915;&#12290;&#36825;&#31867;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#22312;&#27599;&#20010;&#29366;&#24577;&#19978;&#23545;&#25104;&#26412;&#32422;&#26463;&#36827;&#34892;&#36807;&#24230;&#25110;&#19981;&#36275;&#20272;&#35745;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#19981;&#20462;&#25913;&#22522;&#20110;&#36712;&#36857;&#30340;&#25104;&#26412;&#32422;&#26463;&#65292;&#32780;&#26159;&#27169;&#20223;&#8220;&#22909;&#8221;&#36712;&#36857;&#24182;&#36991;&#20813;&#20174;&#36880;&#27493;&#25913;&#36827;&#30340;&#31574;&#30053;&#29983;&#25104;&#30340;&#8220;&#22351;&#8221;&#36712;&#36857;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;oracle&#65292;&#21033;&#29992;&#22870;&#21169;&#38408;&#20540;&#65288;&#38543;&#23398;&#20064;&#21464;&#21270;&#65289;&#21644;&#25972;&#20307;&#25104;&#26412;&#32422;&#26463;&#26469;&#23558;&#36712;&#36857;&#26631;&#35760;&#20026;&#8220;&#22909;&#8221;&#25110;&#8220;&#22351;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10385v3 Announce Type: replace-cross  Abstract: A popular framework for enforcing safe actions in Reinforcement Learning (RL) is Constrained RL, where trajectory based constraints on expected cost (or other cost measures) are employed to enforce safety and more importantly these constraints are enforced while maximizing expected reward. Most recent approaches for solving Constrained RL convert the trajectory based cost constraint into a surrogate problem that can be solved using minor modifications to RL methods. A key drawback with such approaches is an over or underestimation of the cost constraint at each state. Therefore, we provide an approach that does not modify the trajectory based cost constraint and instead imitates ``good'' trajectories and avoids ``bad'' trajectories generated from incrementally improving policies. We employ an oracle that utilizes a reward threshold (which is varied with learning) and the overall cost constraint to label trajectories as ``good''
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#20219;&#20309;&#25915;&#20987;&#30340;&#25345;&#32493;&#23545;&#25239;&#24615;&#38450;&#24481;&#65288;CAD&#65289;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2312.09481</link><description>&lt;p&gt;
&#25345;&#32493;&#19981;&#26029;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Continual Adversarial Defense
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09481
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#20219;&#20309;&#25915;&#20987;&#30340;&#25345;&#32493;&#23545;&#25239;&#24615;&#38450;&#24481;&#65288;CAD&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#27599;&#26376;&#38024;&#23545;&#35270;&#35273;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#24555;&#36895;&#28436;&#21464;&#30340;&#29305;&#24615;&#65292;&#20154;&#20204;&#25552;&#20986;&#20102;&#35768;&#22810;&#38450;&#24481;&#26041;&#27861;&#65292;&#26088;&#22312;&#23613;&#21487;&#33021;&#36890;&#29992;&#21270;&#20197;&#25269;&#24481;&#23613;&#21487;&#33021;&#22810;&#30340;&#24050;&#30693;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#23545;&#25239;&#25152;&#26377;&#31867;&#22411;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#24182;&#19981;&#29616;&#23454;&#65292;&#22240;&#20026;&#38450;&#24481;&#31995;&#32479;&#36816;&#34892;&#30340;&#29615;&#22659;&#26159;&#21160;&#24577;&#30340;&#65292;&#21253;&#21547;&#38543;&#30528;&#26102;&#38388;&#20986;&#29616;&#30340;&#21508;&#31181;&#29420;&#29305;&#25915;&#20987;&#12290;&#38450;&#24481;&#31995;&#32479;&#24517;&#39035;&#25910;&#38598;&#22312;&#32447;&#23569;&#26679;&#26412;&#23545;&#25239;&#21453;&#39304;&#20197;&#36805;&#36895;&#22686;&#24378;&#33258;&#36523;&#65292;&#20805;&#20998;&#21033;&#29992;&#20869;&#23384;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#20219;&#20309;&#25915;&#20987;&#30340;&#25345;&#32493;&#23545;&#25239;&#24615;&#38450;&#24481;&#65288;CAD&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21508;&#31181;&#25915;&#20987;&#36880;&#20010;&#38454;&#27573;&#20986;&#29616;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;CAD&#22522;&#20110;&#22235;&#39033;&#21407;&#21017;&#36827;&#34892;&#24314;&#27169;&#65306;(1) &#25345;&#32493;&#36866;&#24212;&#26032;&#25915;&#20987;&#32780;&#26080;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;(2) &#23569;&#26679;&#26412;&#36866;&#24212;&#65292;(3) &#20869;&#23384;&#39640;&#25928;&#36866;&#24212;&#65292;&#20197;&#21450;(4) &#39640;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09481v2 Announce Type: replace-cross  Abstract: In response to the rapidly evolving nature of adversarial attacks against visual classifiers on a monthly basis, numerous defenses have been proposed to generalize against as many known attacks as possible. However, designing a defense method that generalizes to all types of attacks is not realistic because the environment in which defense systems operate is dynamic and comprises various unique attacks that emerge as time goes on. The defense system must gather online few-shot defense feedback to promptly enhance itself, leveraging efficient memory utilization. Therefore, we propose the first continual adversarial defense (CAD) framework that adapts to any attacks in a dynamic scenario, where various attacks emerge stage by stage. In practice, CAD is modeled under four principles: (1) continual adaptation to new attacks without catastrophic forgetting, (2) few-shot adaptation, (3) memory-efficient adaptation, and (4) high accur
&lt;/p&gt;</description></item><item><title>&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;3D&#21407;&#23376;&#31995;&#32479;&#20013;&#20197;&#21033;&#29992;&#29289;&#29702;&#23545;&#31216;&#24615;&#21644;&#21270;&#23398;&#24615;&#36136;&#31561;&#24402;&#32435;&#20559;&#24046;&#26469;&#23398;&#20064;&#20960;&#20309;&#22270;&#20449;&#24687;&#34920;&#31034;&#32780;&#33879;&#31216;&#12290;</title><link>https://arxiv.org/abs/2312.07511</link><description>&lt;p&gt;
&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;3D&#21407;&#23376;&#31995;&#32479;&#20013;&#30340;&#23454;&#36341;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
A Hitchhiker's Guide to Geometric GNNs for 3D Atomic Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07511
&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;3D&#21407;&#23376;&#31995;&#32479;&#20013;&#20197;&#21033;&#29992;&#29289;&#29702;&#23545;&#31216;&#24615;&#21644;&#21270;&#23398;&#24615;&#36136;&#31561;&#24402;&#32435;&#20559;&#24046;&#26469;&#23398;&#20064;&#20960;&#20309;&#22270;&#20449;&#24687;&#34920;&#31034;&#32780;&#33879;&#31216;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#39318;&#36873;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#23853;&#38706;&#22836;&#35282;&#65292;&#25903;&#25345;&#20174;&#34507;&#30333;&#32467;&#26500;&#39044;&#27979;&#21040;&#20998;&#23376;&#27169;&#25311;&#21644;&#26448;&#26009;&#29983;&#25104;&#31561;&#24212;&#29992;&#65292;&#20854;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#35832;&#22914;&#29289;&#29702;&#23545;&#31216;&#24615;&#21644;&#21270;&#23398;&#24615;&#36136;&#20043;&#31867;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#23398;&#20064;&#36825;&#20123;&#20960;&#20309;&#22270;&#30340;&#20449;&#24687;&#34920;&#31034;&#12290;&#22312;&#36825;&#31687;&#20027;&#35266;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#32780;&#33258;&#36275;&#22320;&#27010;&#36848;&#20102;&#29992;&#20110;3D&#21407;&#23376;&#31995;&#32479;&#30340;&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07511v2 Announce Type: replace-cross  Abstract: Recent advances in computational modelling of atomic systems, spanning molecules, proteins, and materials, represent them as geometric graphs with atoms embedded as nodes in 3D Euclidean space. In these graphs, the geometric attributes transform according to the inherent physical symmetries of 3D atomic systems, including rotations and translations in Euclidean space, as well as node permutations. In recent years, Geometric Graph Neural Networks have emerged as the preferred machine learning architecture powering applications ranging from protein structure prediction to molecular simulations and material generation. Their specificity lies in the inductive biases they leverage - such as physical symmetries and chemical properties - to learn informative representations of these geometric graphs.   In this opinionated paper, we provide a comprehensive and self-contained overview of the field of Geometric GNNs for 3D atomic systems
&lt;/p&gt;</description></item><item><title>TimeDRL&#26159;&#19968;&#20010;&#20855;&#26377;&#35299;&#32544;&#21452;&#23618;&#23884;&#20837;&#30340;&#36890;&#29992;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26102;&#38388;&#25139;&#32423;&#21035;&#21644;&#23454;&#20363;&#32423;&#21035;&#30340;&#23884;&#20837;&#20043;&#38388;&#30340;&#35299;&#32544;&#27966;&#29983;&#20197;&#21450;&#26102;&#38388;&#25139;-&#39044;&#27979;&#21644;&#23454;&#20363;-&#23545;&#27604;&#20219;&#21153;&#30340;&#21033;&#29992;&#65292;&#23454;&#29616;&#20102;&#23398;&#20064;&#20016;&#23500;&#34920;&#31034;&#24182;&#35299;&#20915;&#24402;&#32435;&#20559;&#24046;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2312.04142</link><description>&lt;p&gt;
TimeDRL&#65306;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TimeDRL: Disentangled Representation Learning for Multivariate Time-Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04142
&lt;/p&gt;
&lt;p&gt;
TimeDRL&#26159;&#19968;&#20010;&#20855;&#26377;&#35299;&#32544;&#21452;&#23618;&#23884;&#20837;&#30340;&#36890;&#29992;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26102;&#38388;&#25139;&#32423;&#21035;&#21644;&#23454;&#20363;&#32423;&#21035;&#30340;&#23884;&#20837;&#20043;&#38388;&#30340;&#35299;&#32544;&#27966;&#29983;&#20197;&#21450;&#26102;&#38388;&#25139;-&#39044;&#27979;&#21644;&#23454;&#20363;-&#23545;&#27604;&#20219;&#21153;&#30340;&#21033;&#29992;&#65292;&#23454;&#29616;&#20102;&#23398;&#20064;&#20016;&#23500;&#34920;&#31034;&#24182;&#35299;&#20915;&#24402;&#32435;&#20559;&#24046;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65288;&#20363;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#24037;&#19994;&#65289;&#38750;&#24120;&#20855;&#26377;&#20449;&#24687;&#37327;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#26631;&#31614;&#21644;&#39640;&#32500;&#24230;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30740;&#31350;&#26174;&#31034;&#20102;&#22312;&#23398;&#20064;&#20016;&#23500;&#34920;&#31034;&#32780;&#19981;&#20381;&#36182;&#20110;&#26631;&#31614;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#23398;&#20064;&#35299;&#32544;&#23884;&#20837;&#21644;&#35299;&#20915;&#24402;&#32435;&#20559;&#24046;&#65288;&#20363;&#22914;&#21464;&#25442;&#19981;&#21464;&#24615;&#65289;&#26041;&#38754;&#36824;&#26377;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TimeDRL&#65292;&#19968;&#20010;&#20855;&#26377;&#35299;&#32544;&#21452;&#23618;&#23884;&#20837;&#30340;&#36890;&#29992;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;TimeDRL&#30340;&#19977;&#20010;&#26032;&#39062;&#29305;&#24449;&#20026;&#65306;&#65288;i&#65289;&#20351;&#29992;[CLS]&#20196;&#29260;&#31574;&#30053;&#20174;&#25171;&#34917;&#19969;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#35299;&#32544;&#26102;&#38388;&#25139;&#32423;&#21644;&#23454;&#20363;&#32423;&#23884;&#20837;&#65307;&#65288;ii&#65289;&#21033;&#29992;&#26102;&#38388;&#25139;&#39044;&#27979;&#21644;&#23454;&#20363;&#23545;&#27604;&#20219;&#21153;&#36827;&#34892;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#65292;&#21069;&#32773;&#20248;&#21270;&#26102;&#38388;&#25139;&#32423;&#21035;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04142v2 Announce Type: replace-cross  Abstract: Multivariate time-series data in numerous real-world applications (e.g., healthcare and industry) are informative but challenging due to the lack of labels and high dimensionality. Recent studies in self-supervised learning have shown their potential in learning rich representations without relying on labels, yet they fall short in learning disentangled embeddings and addressing issues of inductive bias (e.g., transformation-invariance). To tackle these challenges, we propose TimeDRL, a generic multivariate time-series representation learning framework with disentangled dual-level embeddings. TimeDRL is characterized by three novel features: (i) disentangled derivation of timestamp-level and instance-level embeddings from patched time-series data using a [CLS] token strategy; (ii) utilization of timestamp-predictive and instance-contrastive tasks for disentangled representation learning, with the former optimizing timestamp-lev
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#36125;&#21494;&#26031;&#27169;&#22411;&#31867;&#65292;&#36890;&#36807;&#39046;&#22495;&#32422;&#26463;&#30340;&#35774;&#32622;&#26469;&#25913;&#21892;&#27979;&#35797;&#21644;&#26410;&#27979;&#35797;&#24739;&#32773;&#30340;&#39118;&#38505;&#39044;&#27979;</title><link>https://arxiv.org/abs/2312.03878</link><description>&lt;p&gt;
&#39046;&#22495;&#32422;&#26463;&#22312;&#32570;&#22833;&#32467;&#26524;&#25968;&#25454;&#26102;&#25913;&#21892;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Domain constraints improve risk prediction when outcome data is missing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03878
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#36125;&#21494;&#26031;&#27169;&#22411;&#31867;&#65292;&#36890;&#36807;&#39046;&#22495;&#32422;&#26463;&#30340;&#35774;&#32622;&#26469;&#25913;&#21892;&#27979;&#35797;&#21644;&#26410;&#27979;&#35797;&#24739;&#32773;&#30340;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#29992;&#20110;&#39044;&#27979;&#20154;&#31867;&#20915;&#31574;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#21382;&#21490;&#20915;&#31574;&#20915;&#23450;&#20102;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#21482;&#35266;&#23519;&#21040;&#21307;&#29983;&#21382;&#21490;&#19978;&#27979;&#35797;&#30340;&#24739;&#32773;&#30340;&#27979;&#35797;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#27169;&#22411;&#31867;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#36825;&#31181;&#24773;&#20917;&#12290;&#27169;&#22411;&#30340;&#30446;&#30340;&#26159;&#20934;&#30830;&#20272;&#35745;&#27979;&#35797;&#21644;&#26410;&#27979;&#35797;&#24739;&#32773;&#30340;&#39118;&#38505;&#12290;&#30001;&#20110;&#26410;&#27979;&#35797;&#24739;&#32773;&#21487;&#33021;&#20986;&#29616;&#21508;&#31181;&#21487;&#33021;&#24615;&#65292;&#22240;&#27492;&#20272;&#35745;&#36825;&#20010;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#22312;&#20581;&#24247;&#39046;&#22495;&#21512;&#29702;&#30340;&#39046;&#22495;&#32422;&#26463;&#65306;&#39044;valence&#32422;&#26463;&#65292;&#20854;&#20013;&#25972;&#20307;&#30142;&#30149;&#24739;&#30149;&#29575;&#26159;&#24050;&#30693;&#30340;&#65292;&#19987;&#19994;&#32422;&#26463;&#65292;&#20854;&#20013;&#20154;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03878v2 Announce Type: replace  Abstract: Machine learning models are often trained to predict the outcome resulting from a human decision. For example, if a doctor decides to test a patient for disease, will the patient test positive? A challenge is that historical decision-making determines whether the outcome is observed: we only observe test outcomes for patients doctors historically tested. Untested patients, for whom outcomes are unobserved, may differ from tested patients along observed and unobserved dimensions. We propose a Bayesian model class which captures this setting. The purpose of the model is to accurately estimate risk for both tested and untested patients. Estimating this model is challenging due to the wide range of possibilities for untested patients. To address this, we propose two domain constraints which are plausible in health settings: a prevalence constraint, where the overall disease prevalence is known, and an expertise constraint, where the huma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35753;ChatGPT&#21644;Bard&#20882;&#20805;&#22797;&#26434;&#20154;&#29289;&#35282;&#33394;&#65292;&#32469;&#36807;&#20102;&#23433;&#20840;&#26426;&#21046;&#21644;&#19987;&#38376;&#35757;&#32451;&#31243;&#24207;&#65292;&#23637;&#31034;&#20102;&#34987;&#31105;&#27490;&#30340;&#22238;&#24212;&#23454;&#38469;&#19978;&#34987;&#25552;&#20379;&#20102;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#33719;&#21462;&#26410;&#32463;&#25480;&#26435;&#12289;&#38750;&#27861;&#25110;&#26377;&#23475;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2312.03853</link><description>&lt;p&gt;
LLMs&#30340;&#20004;&#38754;&#24615;&#65306;Jekyll&#21338;&#22763;&#19982;Hyde&#20808;&#29983;
&lt;/p&gt;
&lt;p&gt;
Dr. Jekyll and Mr. Hyde: Two Faces of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35753;ChatGPT&#21644;Bard&#20882;&#20805;&#22797;&#26434;&#20154;&#29289;&#35282;&#33394;&#65292;&#32469;&#36807;&#20102;&#23433;&#20840;&#26426;&#21046;&#21644;&#19987;&#38376;&#35757;&#32451;&#31243;&#24207;&#65292;&#23637;&#31034;&#20102;&#34987;&#31105;&#27490;&#30340;&#22238;&#24212;&#23454;&#38469;&#19978;&#34987;&#25552;&#20379;&#20102;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#33719;&#21462;&#26410;&#32463;&#25480;&#26435;&#12289;&#38750;&#27861;&#25110;&#26377;&#23475;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20165;&#19968;&#24180;&#21069;&#65292;&#25105;&#20204;&#30446;&#30585;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#22312;&#32467;&#21512;&#20687;&#32842;&#22825;&#26426;&#22120;&#20154;&#21161;&#25163;&#20043;&#31867;&#30340;&#24212;&#29992;&#26102;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#20123;&#21161;&#25163;&#20135;&#29983;&#19981;&#24403;&#22238;&#24212;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#23433;&#20840;&#26426;&#21046;&#21644;&#19987;&#38376;&#30340;&#35757;&#32451;&#31243;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35753;ChatGPT&#21644;Bard&#65288;&#20197;&#21450;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#26159;Bing chat&#65289;&#20882;&#20805;&#22797;&#26434;&#20154;&#29289;&#35282;&#33394;&#65292;&#32469;&#36807;&#20102;&#36825;&#20123;&#25514;&#26045;&#65292;&#36825;&#20123;&#35282;&#33394;&#19982;&#23427;&#20204;&#26412;&#24212;&#25104;&#20026;&#30340;&#30495;&#23454;&#21161;&#25163;&#30340;&#29305;&#24449;&#30456;&#21453;&#12290;&#25105;&#20204;&#39318;&#20808;&#21019;&#36896;&#20986;&#36825;&#20123;&#20154;&#29289;&#35282;&#33394;&#30340;&#22797;&#26434;&#20256;&#35760;&#65292;&#28982;&#21518;&#22312;&#21516;&#19968;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#20351;&#29992;&#23427;&#20204;&#36827;&#34892;&#26032;&#30340;&#23545;&#35805;&#12290;&#25105;&#20204;&#30340;&#23545;&#35805;&#37319;&#29992;&#35282;&#33394;&#25198;&#28436;&#39118;&#26684;&#65292;&#20197;&#33719;&#24471;&#21161;&#25163;&#19981;&#34987;&#20801;&#35768;&#25552;&#20379;&#30340;&#22238;&#24212;&#12290;&#36890;&#36807;&#20351;&#29992;&#20154;&#29289;&#35282;&#33394;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#34987;&#31105;&#27490;&#30340;&#22238;&#24212;&#23454;&#38469;&#19978;&#34987;&#25552;&#20379;&#20102;&#65292;&#20174;&#32780;&#26377;&#21487;&#33021;&#33719;&#21462;&#26410;&#32463;&#25480;&#26435;&#12289;&#38750;&#27861;&#25110;&#26377;&#23475;&#30340;&#20449;&#24687;&#12290;&#36825;&#39033;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#25239;&#24615;pe
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03853v2 Announce Type: replace-cross  Abstract: Only a year ago, we witnessed a rise in the use of Large Language Models (LLMs), especially when combined with applications like chatbot assistants. Safety mechanisms and specialized training procedures are implemented to prevent improper responses from these assistants. In this work, we bypass these measures for ChatGPT and Bard (and, to some extent, Bing chat) by making them impersonate complex personas with opposite characteristics as those of the truthful assistants they are supposed to be. We start by creating elaborate biographies of these personas, which we then use in a new session with the same chatbots. Our conversation followed a role-play style to get the response the assistant was not allowed to provide. By making use of personas, we show that the response that is prohibited is actually provided, making it possible to obtain unauthorized, illegal, or harmful information. This work shows that by using adversarial pe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;MD&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#35774;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;VAE&#27169;&#22411;&#29983;&#25104;&#26032;&#22411;Vitrimer&#24182;&#26681;&#25454;&#25152;&#38656;Tg&#25351;&#23548;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2312.03690</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23376;&#21160;&#21147;&#23398;&#21644;&#29983;&#25104;&#24314;&#27169;&#23454;&#29616;&#29627;&#29827;&#36716;&#21270;&#28201;&#24230;&#30340;&#36870;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Inverse Design of Vitrimeric Polymers by Molecular Dynamics and Generative Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03690
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;MD&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#35774;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;VAE&#27169;&#22411;&#29983;&#25104;&#26032;&#22411;Vitrimer&#24182;&#26681;&#25454;&#25152;&#38656;Tg&#25351;&#23548;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vitrimer&#26159;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#21160;&#24577;&#20849;&#20215;&#33258;&#36866;&#24212;&#32593;&#32476;&#37325;&#26032;&#25490;&#21015;&#32780;&#20855;&#26377;&#33258;&#25105;&#20462;&#22797;&#33021;&#21147;&#30340;&#26032;&#22411;&#21487;&#25345;&#32493;&#32858;&#21512;&#29289;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#26500;&#25104;&#20998;&#23376;&#36873;&#25321;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24615;&#36136;&#31354;&#38388;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#28508;&#22312;&#24212;&#29992;&#30340;&#20805;&#20998;&#23454;&#29616;&#12290;&#36890;&#36807;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#29305;&#21035;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#27169;&#22411;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#29983;&#25104;&#26032;&#22411;Vitrimer&#24182;&#26681;&#25454;&#25152;&#38656;&#29627;&#29827;&#36716;&#21464;&#28201;&#24230;&#65288;Tg&#65289;&#25351;&#23548;&#20854;&#36870;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;Vitrimer&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#19968;&#30334;&#19975;&#31181;&#65292;&#24182;&#36890;&#36807;&#39640;&#36890;&#37327;MD&#27169;&#25311;&#65292;&#30001;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#26657;&#20934;&#65292;&#35745;&#31639;&#20102;8424&#31181;&#30340;Tg&#12290;&#25152;&#25552;&#20986;&#30340;VAE&#37319;&#29992;&#21452;&#22270;&#32534;&#30721;&#22120;&#21644;&#28508;&#22312;&#32500;&#24230;&#37325;&#21472;&#26041;&#26696;&#65292;&#20801;&#35768;&#22810;&#25104;&#20998;Vitrimer&#30340;&#20010;&#20307;&#34920;&#31034;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#36830;&#32493;&#30340;&#28508;&#22312;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03690v2 Announce Type: replace-cross  Abstract: Vitrimer is a new class of sustainable polymers with the ability of self-healing through rearrangement of dynamic covalent adaptive networks. However, a limited choice of constituent molecules restricts their property space, prohibiting full realization of their potential applications. Through a combination of molecular dynamics (MD) simulations and machine learning (ML), particularly a novel graph variational autoencoder (VAE) model, we establish a method for generating novel vitrimers and guide their inverse design based on desired glass transition temperature (Tg). We build the first vitrimer dataset of one million and calculate Tg on 8,424 of them by high-throughput MD simulations calibrated by a Gaussian process model. The proposed VAE employs dual graph encoders and a latent dimension overlapping scheme which allows for individual representation of multi-component vitrimers. By constructing a continuous latent space conta
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#25366;&#25496;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#26412;&#22320;LLMs&#26469;&#35299;&#20915;&#36890;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#38382;&#39064;&#65292;&#30830;&#20445;&#25968;&#25454;&#23433;&#20840;&#24182;&#36827;&#34892;&#36827;&#19968;&#27493;&#35843;&#25972;</title><link>https://arxiv.org/abs/2312.01678</link><description>&lt;p&gt;
Jellyfish&#65306;&#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#39044;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jellyfish: A Large Language Model for Data Preprocessing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01678
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#25366;&#25496;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#26412;&#22320;LLMs&#26469;&#35299;&#20915;&#36890;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#38382;&#39064;&#65292;&#30830;&#20445;&#25968;&#25454;&#23433;&#20840;&#24182;&#36827;&#34892;&#36827;&#19968;&#27493;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#25366;&#25496;&#31649;&#36947;&#20013;&#23558;&#21407;&#22987;&#25968;&#25454;&#36716;&#25442;&#20026;&#26377;&#21033;&#20110;&#31616;&#21333;&#22788;&#29702;&#30340;&#24178;&#20928;&#26684;&#24335;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#65288;DP&#65289;&#20013;LLMs&#30340;&#21033;&#29992;&#12290;&#19982;&#20351;&#29992;LLMs&#20026;DP&#35774;&#35745;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#24341;&#36215;&#20102;&#20852;&#36259;&#30456;&#27604;&#65292;&#26368;&#36817;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#20513;&#35758;&#36890;&#24120;&#20381;&#36182;&#20110;GPT API&#65292;&#24341;&#21457;&#20102;&#19981;&#21487;&#36991;&#20813;&#30340;&#25968;&#25454;&#27844;&#38671;&#25285;&#24551;&#12290;&#19982;&#36825;&#20123;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#32771;&#34385;&#23558;&#25351;&#23548;&#35843;&#25972;&#26412;&#22320;LLMs&#65288;7-13B&#27169;&#22411;&#65289;&#20316;&#20026;&#36890;&#29992;DP&#38382;&#35299;&#22120;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#20195;&#34920;&#24615;DP&#20219;&#21153;&#30340;&#22235;&#32452;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#38024;&#23545;DP&#23450;&#21046;&#30340;&#24207;&#21015;&#21270;&#21644;&#30693;&#35782;&#27880;&#20837;&#25216;&#26415;&#26500;&#24314;&#20102;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25351;&#23548;&#35843;&#25972;&#30340;LLMs&#20351;&#29992;&#25143;&#33021;&#22815;&#20026;DP&#25163;&#21160;&#21046;&#23450;&#25351;&#23548;&#12290;&#21516;&#26102;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#26412;&#22320;&#12289;&#21333;&#19968;&#21644;&#20215;&#26684;&#20302;&#24265;&#30340;GPU&#19978;&#36816;&#34892;&#65292;&#30830;&#20445;&#25968;&#25454;&#23433;&#20840;&#24182;&#23454;&#29616;&#36827;&#19968;&#27493;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#20026;DP&#25351;&#23548;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01678v4 Announce Type: replace  Abstract: This paper explores the utilization of LLMs for data preprocessing (DP), a crucial step in the data mining pipeline that transforms raw data into a clean format conducive to easy processing. Whereas the use of LLMs has sparked interest in devising universal solutions to DP, recent initiatives in this domain typically rely on GPT APIs, raising inevitable data breach concerns. Unlike these approaches, we consider instruction-tuning local LLMs (7 - 13B models) as universal DP ask solver. We select a collection of datasets across four representative DP tasks and construct instruction-tuning data using serialization and knowledge injection techniques tailored to DP. As such, the instruction-tuned LLMs empower users to manually craft instructions for DP. Meanwhile, they can operate on a local, single, and low-priced GPU, ensuring data security and enabling further tuning. Our experiments show that our dataset constructed for DP instruction
&lt;/p&gt;</description></item><item><title>MLLMs&#36890;&#36807;&#20026;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#24314;&#31435;&#26356;&#20016;&#23500;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20851;&#32852;&#65292;&#20197;&#22686;&#24378;&#35270;&#35273;-&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#8220;&#25991;&#26412;&#21098;&#20999;&#8221;&#26041;&#27861;&#26469;&#36991;&#20813;&#20559;&#35265;&#24341;&#20837;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.18765</link><description>&lt;p&gt;
MLLMs&#22686;&#24378;&#35270;&#35273;-&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MLLMs-Augmented Visual-Language Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18765
&lt;/p&gt;
&lt;p&gt;
MLLMs&#36890;&#36807;&#20026;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#24314;&#31435;&#26356;&#20016;&#23500;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20851;&#32852;&#65292;&#20197;&#22686;&#24378;&#35270;&#35273;-&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#8220;&#25991;&#26412;&#21098;&#20999;&#8221;&#26041;&#27861;&#26469;&#36991;&#20813;&#20559;&#35265;&#24341;&#20837;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18765v3 &#20844;&#21578;&#31867;&#22411;: replace-cross &#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#22312;&#35768;&#22810;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21487;&#20197;&#36890;&#36807;&#20026;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#24314;&#31435;&#26356;&#20016;&#23500;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20851;&#32852;&#26469;&#21152;&#24378;&#35270;&#35273;-&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24456;&#31616;&#21333;&#65292;&#21033;&#29992;MLLMs&#20026;&#27599;&#20010;&#22270;&#20687;&#25193;&#23637;&#22810;&#20010;&#19981;&#21516;&#30340;&#26631;&#39064;&#12290;&#20026;&#20102;&#38450;&#27490;MLLMs&#30340;&#24187;&#35273;&#21644;&#21333;&#35843;&#35821;&#35328;&#39118;&#26684;&#24341;&#20837;&#30340;&#20559;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#25991;&#26412;&#21098;&#20999;&#8221;&#26469;&#20445;&#25345;&#25193;&#23637;&#26631;&#39064;&#30340;&#36136;&#37327;&#21644;&#21487;&#29992;&#24615;&#12290;&#22312;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#65292;&#22312;&#19981;&#24341;&#20837;&#39069;&#22806;&#30340;&#35757;&#32451;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#35843;&#21644;&#38646;-shot&#35774;&#32622;&#19979;&#19968;&#33268;&#22320;&#22312;Recall@1&#19978;&#33719;&#24471;&#20102;5.6 ~ 35.0&#21644;16.8 ~ 46.1&#30340;&#25913;&#36827;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19982;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#30456;&#24403;&#30340;&#38646;-shot&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18765v3 Announce Type: replace-cross  Abstract: Visual-language pre-training has achieved remarkable success in many multi-modal tasks, largely attributed to the availability of large-scale image-text datasets. In this work, we demonstrate that Multi-modal Large Language Models (MLLMs) can enhance visual-language representation learning by establishing richer image-text associations for image-text datasets. Our approach is simple, utilizing MLLMs to extend multiple diverse captions for each image. To prevent the bias introduced by MLLMs' hallucinations and monotonous language styles, we propose "text shearing" to maintain the quality and availability of extended captions. In image-text retrieval, without introducing additional training cost, our method consistently obtains 5.6 ~ 35.0 and 16.8 ~ 46.1 improvement on Recall@1 under the fine-tuning and zero-shot settings, respectively. Notably, we obtain zero-shot results that are comparable to fine-tuning on target datasets, wh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DPOD&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#36328;&#39046;&#22495;&#25968;&#25454;&#26469;&#25913;&#21892;&#25152;&#38656;&#39046;&#22495;&#30340;&#33073;&#31163;&#19978;&#19979;&#25991;&#35823;&#20449;&#24687;&#26816;&#27979;&#65292;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.16496</link><description>&lt;p&gt;
DPOD&#65306;&#38754;&#21521;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;
DPOD: Domain-Specific Prompt Tuning for Multimodal Fake News Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DPOD&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#36328;&#39046;&#22495;&#25968;&#25454;&#26469;&#25913;&#21892;&#25152;&#38656;&#39046;&#22495;&#30340;&#33073;&#31163;&#19978;&#19979;&#25991;&#35823;&#20449;&#24687;&#26816;&#27979;&#65292;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#26032;&#38395;&#21033;&#29992;&#33073;&#31163;&#19978;&#19979;&#25991;&#30340;&#22270;&#20687;&#20256;&#25773;&#24050;&#32463;&#21464;&#24471;&#26222;&#36941;&#65292;&#26159;&#20449;&#24687;&#36807;&#36733;&#26102;&#20195;&#19968;&#20010;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#33073;&#31163;&#19978;&#19979;&#25991;&#30340;&#34394;&#20551;&#26032;&#38395;&#21487;&#33021;&#28041;&#21450;&#19981;&#21516;&#39046;&#22495;&#65292;&#22914;&#25919;&#27835;&#12289;&#20307;&#32946;&#12289;&#23089;&#20048;&#31561;&#12290;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#19981;&#21516;&#39046;&#22495;&#26032;&#38395;&#25991;&#31456;&#23384;&#22312;&#30528;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#37096;&#20998;&#39046;&#22495;&#25968;&#25454;&#20016;&#23500;&#65292;&#32780;&#20854;&#20182;&#39046;&#22495;&#25968;&#25454;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24517;&#39035;&#24320;&#21457;&#20986;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#37327;&#35774;&#32622;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36328;&#39046;&#22495;&#25968;&#25454;&#26159;&#21542;&#26377;&#21161;&#20110;&#25913;&#21892;&#25152;&#38656;&#39046;&#22495;&#30340;&#33073;&#31163;&#19978;&#19979;&#25991;&#35823;&#20449;&#24687;&#26816;&#27979;&#65288;&#22312;&#27492;&#31216;&#20026;&#22810;&#27169;&#24577;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#65289;&#30340;&#26041;&#27861;&#20197;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DPOD&#65288;&#20351;&#29992;&#36328;&#39046;&#22495;&#25968;&#25454;&#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#35843;&#33410;&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16496v2 Announce Type: replace  Abstract: The spread of fake news using out-of-context images has become widespread and is a relevant problem in this era of information overload. Such out-of-context fake news may arise across different domains like politics, sports, entertainment, etc. In practical scenarios, an inherent problem of imbalance exists among news articles from such widely varying domains, resulting in a few domains with abundant data, while the rest containing very limited data. Under such circumstances, it is imperative to develop methods which can work in such varying amounts of data setting. In this work, we explore whether out-of-domain data can help to improve out-of-context misinformation detection (termed here as multi-modal fake news detection) of a desired domain, to address this challenging problem. Towards this goal, we propose a novel framework termed DPOD (Domain-specific Prompt-tuning using Out-of-Domain data). First, to compute generalizable featu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20960;&#20309;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#21738;&#31181;&#32467;&#26500;&#23646;&#24615;&#20135;&#29983;&#26368;&#26377;&#25928;&#30340;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;Ricci&#26354;&#29575;&#30340;&#26032;&#22411;&#32467;&#26500;&#32534;&#30721;&#65288;&#23616;&#37096;&#26354;&#29575;&#37197;&#32622;&#65292;LCP&#65289;&#65292;&#35777;&#26126;&#20854;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#23558;&#23616;&#37096;&#32467;&#26500;&#32534;&#30721;&#19982;&#20840;&#23616;&#20301;&#32622;&#32534;&#30721;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#21319;&#19979;&#28216;&#24615;&#33021;&#65292;&#25429;&#25417;&#21040;&#20114;&#34917;&#30340;&#20960;&#20309;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2311.14864</link><description>&lt;p&gt;
&#36890;&#36807;&#23616;&#37096;&#26354;&#29575;&#37197;&#32622;&#23454;&#29616;&#26377;&#25928;&#30340;&#32467;&#26500;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Effective Structural Encodings via Local Curvature Profiles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20960;&#20309;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#21738;&#31181;&#32467;&#26500;&#23646;&#24615;&#20135;&#29983;&#26368;&#26377;&#25928;&#30340;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;Ricci&#26354;&#29575;&#30340;&#26032;&#22411;&#32467;&#26500;&#32534;&#30721;&#65288;&#23616;&#37096;&#26354;&#29575;&#37197;&#32622;&#65292;LCP&#65289;&#65292;&#35777;&#26126;&#20854;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#23558;&#23616;&#37096;&#32467;&#26500;&#32534;&#30721;&#19982;&#20840;&#23616;&#20301;&#32622;&#32534;&#30721;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#21319;&#19979;&#28216;&#24615;&#33021;&#65292;&#25429;&#25417;&#21040;&#20114;&#34917;&#30340;&#20960;&#20309;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21644;&#20301;&#32622;&#32534;&#30721;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#24320;&#22987;&#31995;&#32479;&#22320;&#30740;&#31350;&#36825;&#20123;&#26041;&#27861;&#32534;&#30721;&#30340;&#32467;&#26500;&#23646;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#24615;&#33021;&#26435;&#34913;&#12290;&#28982;&#32780;&#65292;&#21738;&#20123;&#32467;&#26500;&#23646;&#24615;&#20135;&#29983;&#26368;&#26377;&#25928;&#30340;&#32534;&#30721;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20960;&#20309;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;Ricci&#26354;&#29575;&#30340;&#26032;&#22411;&#32467;&#26500;&#32534;&#30721;&#65288;&#23616;&#37096;&#26354;&#29575;&#37197;&#32622;&#65292;&#31616;&#31216;LCP&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#32534;&#30721;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#65292;&#23558;&#23616;&#37096;&#32467;&#26500;&#32534;&#30721;&#65288;&#22914;LCP&#65289;&#19982;&#20840;&#23616;&#20301;&#32622;&#32534;&#30721;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#21319;&#19979;&#28216;&#24615;&#33021;&#65292;&#34920;&#26126;&#23427;&#20204;&#25429;&#25417;&#20102;&#20114;&#34917;&#30340;&#20960;&#20309;&#20449;&#24687;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#32534;&#30721;&#19982;&#65288;&#22522;&#20110;&#26354;&#29575;&#30340;&#65289;&#37325;&#36830;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14864v2 Announce Type: replace  Abstract: Structural and Positional Encodings can significantly improve the performance of Graph Neural Networks in downstream tasks. Recent literature has begun to systematically investigate differences in the structural properties that these approaches encode, as well as performance trade-offs between them. However, the question of which structural properties yield the most effective encoding remains open. In this paper, we investigate this question from a geometric perspective. We propose a novel structural encoding based on discrete Ricci curvature (Local Curvature Profiles, short LCP) and show that it significantly outperforms existing encoding approaches. We further show that combining local structural encodings, such as LCP, with global positional encodings improves downstream performance, suggesting that they capture complementary geometric information. Finally, we compare different encoding types with (curvature-based) rewiring techni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23433;&#20840;&#30340;&#22240;&#26524;&#34920;&#31034;&#26041;&#27861;FUSION&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#32467;&#26500;&#21270;&#24773;&#26223;&#20449;&#24687;&#20419;&#36827;&#27867;&#21270;&#30340;&#31471;&#21040;&#31471;&#39550;&#39542;&#31574;&#30053;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2311.10747</link><description>&lt;p&gt;
&#38754;&#21521;&#23433;&#20840;&#30340;&#22240;&#26524;&#34920;&#31034;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#20540;&#24471;&#20449;&#36182;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safety-aware Causal Representation for Trustworthy Offline Reinforcement Learning in Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23433;&#20840;&#30340;&#22240;&#26524;&#34920;&#31034;&#26041;&#27861;FUSION&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#32467;&#26500;&#21270;&#24773;&#26223;&#20449;&#24687;&#20419;&#36827;&#27867;&#21270;&#30340;&#31471;&#21040;&#31471;&#39550;&#39542;&#31574;&#30053;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#26469;&#33258;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#21508;&#31181;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#20013;&#20445;&#25345;&#23433;&#20840;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#32570;&#20047;&#38271;&#23614;&#21644;&#26080;&#27861;&#39044;&#35265;&#30340;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;saFety-aware strUctured Scenario representatION (FUSION)&#65292;&#36825;&#26159;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24320;&#21019;&#24615;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#21270;&#22330;&#26223;&#20449;&#24687;&#20419;&#36827;&#23398;&#20064;&#21487;&#27867;&#21270;&#30340;&#31471;&#21040;&#31471;&#39550;&#39542;&#31574;&#30053;&#12290;FUSION&#21033;&#29992;&#20998;&#35299;&#22870;&#21169;&#12289;&#25104;&#26412;&#12289;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22312;&#21160;&#24577;&#20132;&#36890;&#29615;&#22659;&#20013;&#36827;&#34892;&#32467;&#26500;&#21270;&#39034;&#24207;&#25512;&#29702;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#20004;&#20010;&#20856;&#22411;&#30495;&#23454;&#19990;&#30028;&#35774;&#23450;&#19979;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10747v3 Announce Type: replace-cross  Abstract: In the domain of autonomous driving, the offline Reinforcement Learning~(RL) approaches exhibit notable efficacy in addressing sequential decision-making problems from offline datasets. However, maintaining safety in diverse safety-critical scenarios remains a significant challenge due to long-tailed and unforeseen scenarios absent from offline datasets. In this paper, we introduce the saFety-aware strUctured Scenario representatION (FUSION), a pioneering representation learning method in offline RL to facilitate the learning of a generalizable end-to-end driving policy by leveraging structured scenario information. FUSION capitalizes on the causal relationships between the decomposed reward, cost, state, and action space, constructing a framework for structured sequential reasoning in dynamic traffic environments. We conduct extensive evaluations in two typical real-world settings of the distribution shift in autonomous vehicl
&lt;/p&gt;</description></item><item><title>Agent Lumos&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#21644;&#27169;&#22359;&#21270;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35268;&#21010;&#27169;&#22359;&#23398;&#20064;&#39640;&#32423;&#23376;&#30446;&#26631;&#29983;&#25104;&#65292;&#35757;&#32451;&#25509;&#22320;&#27169;&#22359;&#23558;&#20854;&#36716;&#21270;&#20026;&#21160;&#20316;&#65292;&#20419;&#36827;&#24191;&#27867;&#20114;&#21160;&#20219;&#21153;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2311.05657</link><description>&lt;p&gt;
Agent Lumos: &#32479;&#19968;&#21644;&#27169;&#22359;&#21270;&#35757;&#32451;&#24320;&#28304;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Agent Lumos: Unified and Modular Training for Open-Source Language Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05657
&lt;/p&gt;
&lt;p&gt;
Agent Lumos&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#21644;&#27169;&#22359;&#21270;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35268;&#21010;&#27169;&#22359;&#23398;&#20064;&#39640;&#32423;&#23376;&#30446;&#26631;&#29983;&#25104;&#65292;&#35757;&#32451;&#25509;&#22320;&#27169;&#22359;&#23558;&#20854;&#36716;&#21270;&#20026;&#21160;&#20316;&#65292;&#20419;&#36827;&#24191;&#27867;&#20114;&#21160;&#20219;&#21153;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38381;&#28304;&#20195;&#29702;&#23384;&#22312;&#35832;&#22810;&#38382;&#39064;&#65292;&#22914;&#32570;&#20047;&#36127;&#25285;&#24471;&#36215;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#37325;&#22797;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#30340;&#20114;&#21160;&#20219;&#21153;&#20013;&#12290;&#36825;&#20419;&#20351;&#20102;&#24320;&#28304;&#26367;&#20195;&#26041;&#26696;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102; LUMOS&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20026;&#35757;&#32451;&#24320;&#28304; LLM-based &#20195;&#29702;&#32780;&#35774;&#35745;&#30340;&#26694;&#26550;&#20043;&#19968;&#12290;LUMOS&#20855;&#26377;&#21487;&#23398;&#20064;&#12289;&#32479;&#19968;&#21644;&#27169;&#22359;&#21270;&#30340;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#23398;&#20064;&#39640;&#32423;&#23376;&#30446;&#26631;&#29983;&#25104;&#30340;&#35268;&#21010;&#27169;&#22359;&#65292;&#20197;&#21450;&#19968;&#20010;&#35757;&#32451;&#26377;&#32032;&#30340;&#25509;&#22320;&#27169;&#22359;&#65292;&#29992;&#20110;&#20351;&#29992;&#25191;&#34892;&#27169;&#22359;&#20013;&#30340;&#21508;&#31181;&#24037;&#20855;&#23558;&#36825;&#20123;&#36716;&#21270;&#20026;&#21160;&#20316;&#12290;&#36825;&#31181;&#35774;&#35745;&#20801;&#35768;&#27169;&#22359;&#21270;&#21319;&#32423;&#65292;&#24182;&#26356;&#24191;&#27867;&#22320;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20114;&#21160;&#20219;&#21153;&#12290;&#20026;&#20102;&#20419;&#36827;&#36890;&#29992;&#20195;&#29702;&#23398;&#20064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#28304;&#33258;&#21508;&#31181;&#22797;&#26434;&#20114;&#21160;&#20219;&#21153;&#20013;&#19981;&#21516;&#22320;&#38754;&#30495;&#23454;&#25512;&#29702;&#21407;&#29702;&#30340;&#22823;&#35268;&#27169;&#12289;&#32479;&#19968;&#21644;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#27880;&#37322;&#12290;&#22312;9&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;LUMOS&#34920;&#29616;&#20986;&#20102;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#65288;1&#65289;LUMOS&#22312;&#22810;&#20010;&#36739;&#22823;&#30340;&#24320;&#28304;a
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05657v2 Announce Type: replace  Abstract: Closed-source agents suffer from several issues such as a lack of affordability, transparency, and reproducibility, particularly on complex interactive tasks. This motivates the development of open-source alternatives. We introduce LUMOS, one of the first frameworks for training open-source LLM-based agents. LUMOS features a learnable, unified, and modular architecture with a planning module that learns high-level subgoal generation, and a grounding module trained to translate these into actions using various tools in the execution module. The design allows for modular upgrades and wider applicability to diverse interactive tasks. To foster generalizable agent learning, we collect large-scale, unified, and high-quality training annotations derived from diverse ground-truth reasoning rationales across various complex interactive tasks. On 9 datasets, LUMOS exhibits several key advantages: (1) LUMOS excels multiple larger open-source a
&lt;/p&gt;</description></item><item><title>&#22312;&#22522;&#20110;&#20559;&#22909;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#21270;&#35774;&#35745;&#30340;&#31639;&#27861;&#22312;&#32447;&#24615;MDP&#27169;&#22411;&#19979;&#34920;&#29616;&#20986;&#26679;&#26412;&#39640;&#25928;&#24615;&#21644;&#22810;&#39033;&#24335;&#36816;&#34892;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#21270;&#20027;&#21160;&#23398;&#20064;&#36807;&#31243;&#26368;&#23567;&#21270;&#20102;&#26597;&#35810;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.14554</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#21270;&#20351;&#22522;&#20110;&#20559;&#22909;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#21464;&#24471;&#39640;&#25928;
&lt;/p&gt;
&lt;p&gt;
Making RL with Preference-based Feedback Efficient via Randomization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14554
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#20559;&#22909;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#21270;&#35774;&#35745;&#30340;&#31639;&#27861;&#22312;&#32447;&#24615;MDP&#27169;&#22411;&#19979;&#34920;&#29616;&#20986;&#26679;&#26412;&#39640;&#25928;&#24615;&#21644;&#22810;&#39033;&#24335;&#36816;&#34892;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#21270;&#20027;&#21160;&#23398;&#20064;&#36807;&#31243;&#26368;&#23567;&#21270;&#20102;&#26597;&#35810;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38656;&#35201;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65292;&#32780;&#19988;&#22312;&#32479;&#35745;&#22797;&#26434;&#24615;&#12289;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#26597;&#35810;&#22797;&#26434;&#24615;&#26041;&#38754;&#38656;&#35201;&#39640;&#25928;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#20351;&#29992;&#20559;&#22909;&#26469;&#34920;&#36798;&#23545;&#36712;&#36857;&#23545;&#30340;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#12290;&#22312;&#32447;&#24615;MDP&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#22312;&#31639;&#27861;&#35774;&#35745;&#20013;&#24341;&#20837;&#38543;&#26426;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20855;&#26377;&#26679;&#26412;&#39640;&#25928;&#24615;&#65288;&#21363;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#30340;&#26368;&#22351;&#24773;&#20917;&#36951;&#25022;&#30028;&#38480;&#65289;&#21644;&#22810;&#39033;&#24335;&#36816;&#34892;&#26102;&#38388;&#65288;&#21363;&#35745;&#31639;&#22797;&#26434;&#24230;&#19982;&#30456;&#20851;&#21442;&#25968;&#26159;&#22810;&#39033;&#24335;&#20851;&#31995;&#65289;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36827;&#19968;&#27493;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#21270;&#20027;&#21160;&#23398;&#20064;&#36807;&#31243;&#26368;&#23567;&#21270;&#20102;&#26597;&#35810;&#22797;&#26434;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23637;&#31034;&#20102;&#36951;&#25022;&#30028;&#38480;&#21644;&#26597;&#35810;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#36817;&#20046;&#26368;&#20248;&#25240;&#34935;&#12290;&#20026;&#20102;&#23558;&#32467;&#26524;&#25512;&#24191;&#21040;&#26356;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21463;&#32447;&#24615;MDP&#27169;&#22411;&#30340;&#38543;&#26426;&#21270;&#31639;&#27861;&#21551;&#21457;&#32780;&#26469;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#38543;&#26426;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14554v2 Announce Type: replace-cross  Abstract: Reinforcement Learning algorithms that learn from human feedback (RLHF) need to be efficient in terms of statistical complexity, computational complexity, and query complexity. In this work, we consider the RLHF setting where the feedback is given in the format of preferences over pairs of trajectories. In the linear MDP model, using randomization in algorithm design, we present an algorithm that is sample efficient (i.e., has near-optimal worst-case regret bounds) and has polynomial running time (i.e., computational complexity is polynomial with respect to relevant parameters). Our algorithm further minimizes the query complexity through a novel randomized active learning procedure. In particular, our algorithm demonstrates a near-optimal tradeoff between the regret bound and the query complexity. To extend the results to more general nonlinear function approximation, we design a model-based randomized algorithm inspired by th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30452;&#25509;&#19982;&#23884;&#20837;&#20132;&#20114;&#65292;&#23558;&#25277;&#35937;&#21521;&#37327;&#36716;&#25442;&#20026;&#21487;&#29702;&#35299;&#30340;&#21465;&#36848;&#65292;&#20351;&#24471;&#22797;&#26434;&#23884;&#20837;&#25968;&#25454;&#26356;&#20855;&#35299;&#37322;&#24615;&#21644;&#24191;&#27867;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.04475</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#23494;&#23884;&#20837;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Demystifying Embedding Spaces using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04475
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30452;&#25509;&#19982;&#23884;&#20837;&#20132;&#20114;&#65292;&#23558;&#25277;&#35937;&#21521;&#37327;&#36716;&#25442;&#20026;&#21487;&#29702;&#35299;&#30340;&#21465;&#36848;&#65292;&#20351;&#24471;&#22797;&#26434;&#23884;&#20837;&#25968;&#25454;&#26356;&#20855;&#35299;&#37322;&#24615;&#21644;&#24191;&#27867;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#24050;&#32463;&#25104;&#20026;&#34920;&#31034;&#26377;&#20851;&#23454;&#20307;&#12289;&#27010;&#24565;&#21644;&#20851;&#31995;&#30340;&#22797;&#26434;&#22810;&#26041;&#38754;&#20449;&#24687;&#30340;&#20851;&#38190;&#25163;&#27573;&#65292;&#20197;&#19968;&#31181;&#32039;&#20945;&#19988;&#26377;&#29992;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#38590;&#20197;&#30452;&#25509;&#35299;&#37322;&#12290;&#23613;&#31649;&#19979;&#28216;&#20219;&#21153;&#21033;&#29992;&#20102;&#36825;&#20123;&#21387;&#32553;&#34920;&#31034;&#65292;&#20294;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#38477;&#32500;&#25110;&#19987;&#38376;&#30340;&#26426;&#22120;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#21487;&#35270;&#21270;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30452;&#25509;&#19982;&#23884;&#20837;&#20132;&#20114;&#65292;&#23558;&#25277;&#35937;&#21521;&#37327;&#36716;&#25442;&#20026;&#21487;&#29702;&#35299;&#30340;&#21465;&#36848;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20351;&#36825;&#20123;&#23884;&#20837;&#26356;&#20855;&#35299;&#37322;&#24615;&#21644;&#24191;&#27867;&#23454;&#29992;&#24615;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;&#23884;&#20837;&#27880;&#20837;LLMs&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#23884;&#20837;&#25968;&#25454;&#30340;&#26597;&#35810;&#21644;&#25506;&#32034;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#65306;&#22686;&#24378;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAVs&#65289;&#12289;&#20256;&#36798;&#26032;&#39062;&#30340;&#23884;&#20837;&#23454;&#20307;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.04475v2 Announce Type: replace-cross  Abstract: Embeddings have become a pivotal means to represent complex, multi-faceted information about entities, concepts, and relationships in a condensed and useful format. Nevertheless, they often preclude direct interpretation. While downstream tasks make use of these compressed representations, meaningful interpretation usually requires visualization using dimensionality reduction or specialized machine learning interpretability methods. This paper addresses the challenge of making such embeddings more interpretable and broadly useful, by employing Large Language Models (LLMs) to directly interact with embeddings -- transforming abstract vectors into understandable narratives. By injecting embeddings into LLMs, we enable querying and exploration of complex embedding data. We demonstrate our approach on a variety of diverse tasks, including: enhancing concept activation vectors (CAVs), communicating novel embedded entities, and decod
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;PECoRe&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#19978;&#19979;&#25991;&#20351;&#29992;&#24773;&#20917;&#65292;&#20174;&#32780;&#35780;&#20272;&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2310.01188</link><description>&lt;p&gt;
&#37327;&#21270;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#32972;&#26223;&#20381;&#36182;&#24615;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Plausibility of Context Reliance in Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01188
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;PECoRe&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#19978;&#19979;&#25991;&#20351;&#29992;&#24773;&#20917;&#65292;&#20174;&#32780;&#35780;&#20272;&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PECoRe&#30340;&#31471;&#21040;&#31471;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#65292;&#26088;&#22312;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20013;&#19978;&#19979;&#25991;&#20351;&#29992;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#27169;&#22411;&#20869;&#37096;&#26469;&#23545;&#27604;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#30446;&#26631;&#20196;&#29260;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#35777;&#26126;&#20854;&#39044;&#27979;&#30340;&#19978;&#19979;&#25991;&#32447;&#32034;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#20351;&#29992;PECORE&#26469;&#37327;&#21270;&#20855;&#26377;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#65292;&#23558;&#27169;&#22411;&#30340;&#29702;&#30001;&#19982;&#20154;&#31867;&#27880;&#37322;&#22312;&#20960;&#20010;&#23618;&#27425;&#30340;&#35805;&#35821;&#27700;&#24179;&#19978;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.01188v2 Announce Type: replace-cross  Abstract: Establishing whether language models can use contextual information in a human-plausible way is important to ensure their trustworthiness in real-world settings. However, the questions of when and which parts of the context affect model generations are typically tackled separately, with current plausibility evaluations being practically limited to a handful of artificial benchmarks. To address this, we introduce Plausibility Evaluation of Context Reliance (PECoRe), an end-to-end interpretability framework designed to quantify context usage in language models' generations. Our approach leverages model internals to (i) contrastively identify context-sensitive target tokens in generated texts and (ii) link them to contextual cues justifying their prediction. We use \pecore to quantify the plausibility of context-aware machine translation models, comparing model rationales with human annotations across several discourse-level pheno
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#36890;&#36807;&#35757;&#32451;&#32447;&#24615;Transformer&#27169;&#22411;&#35299;&#20915;&#22238;&#24402;&#20219;&#21153;&#65292;&#21457;&#29616;&#36825;&#31181;&#31616;&#21333;&#30340;&#32447;&#24615;&#21270;&#27169;&#22411;&#33021;&#22815;&#37325;&#29616;Transformer&#35757;&#32451;&#21160;&#24577;&#30340;&#22810;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#34920;&#26126;&#32447;&#24615;&#27880;&#24847;&#21147;&#21487;&#33021;&#26159;&#29702;&#35299;Transformer&#20248;&#21270;&#30340;&#20851;&#38190;&#12290;</title><link>https://arxiv.org/abs/2310.01082</link><description>&lt;p&gt;
&#32447;&#24615;&#27880;&#24847;&#21147;&#21487;&#33021;&#23601;&#26159;&#29702;&#35299;Transformer&#20248;&#21270;&#30340;&#20851;&#38190;
&lt;/p&gt;
&lt;p&gt;
Linear attention is (maybe) all you need (to understand transformer optimization)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01082
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#36890;&#36807;&#35757;&#32451;&#32447;&#24615;Transformer&#27169;&#22411;&#35299;&#20915;&#22238;&#24402;&#20219;&#21153;&#65292;&#21457;&#29616;&#36825;&#31181;&#31616;&#21333;&#30340;&#32447;&#24615;&#21270;&#27169;&#22411;&#33021;&#22815;&#37325;&#29616;Transformer&#35757;&#32451;&#21160;&#24577;&#30340;&#22810;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#34920;&#26126;&#32447;&#24615;&#27880;&#24847;&#21147;&#21487;&#33021;&#26159;&#29702;&#35299;Transformer&#20248;&#21270;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#30340;&#35757;&#32451;&#22240;&#38656;&#35201;&#20180;&#32454;&#35774;&#35745;&#20248;&#21270;&#22120;&#24182;&#20351;&#29992;&#21508;&#31181;&#21551;&#21457;&#24335;&#32780;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#36890;&#36807;&#20180;&#32454;&#30740;&#31350;&#19968;&#20010;&#31616;&#21333;&#20294;&#32463;&#20856;&#30340;&#32447;&#24615;&#21270;&#27973;&#23618;Transformer&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#35299;&#26512;Transformer&#35757;&#32451;&#32454;&#24494;&#20043;&#22788;&#30340;&#36827;&#23637;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35757;&#32451;&#32447;&#24615;Transformer&#26469;&#35299;&#20915;&#22238;&#24402;&#20219;&#21153;&#65292;&#28789;&#24863;&#26469;&#28304;&#20110;J. von Oswald&#31561;&#20154;&#65288;ICML 2023&#65289;&#21644;K. Ahn&#31561;&#20154;&#65288;NeurIPS 2023&#65289;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25105;&#20204;&#25552;&#20986;&#30340;&#32447;&#24615;&#21270;&#27169;&#22411;&#33021;&#22815;&#37325;&#29616;Transformer&#35757;&#32451;&#21160;&#24577;&#30340;&#20960;&#20010;&#37325;&#35201;&#26041;&#38754;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24471;&#21040;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#32447;&#24615;&#21270;Transformer&#27169;&#22411;&#23454;&#38469;&#19978;&#21487;&#33021;&#26159;&#29702;&#35299;Transformer&#20248;&#21270;&#30340;&#26377;&#20215;&#20540;&#12289;&#29616;&#23454;&#30340;&#25277;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.01082v2 Announce Type: replace-cross  Abstract: Transformer training is notoriously difficult, requiring a careful design of optimizers and use of various heuristics. We make progress towards understanding the subtleties of training Transformers by carefully studying a simple yet canonical linearized shallow Transformer model. Specifically, we train linear Transformers to solve regression tasks, inspired by J.~von Oswald et al.~(ICML 2023), and K.~Ahn et al.~(NeurIPS 2023). Most importantly, we observe that our proposed linearized models can reproduce several prominent aspects of Transformer training dynamics. Consequently, the results obtained in this paper suggest that a simple linearized Transformer model could actually be a valuable, realistic abstraction for understanding Transformer optimization.
&lt;/p&gt;</description></item><item><title>CRAFT&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#24037;&#20855;&#21019;&#24314;&#21644;&#26816;&#32034;&#26694;&#26550;&#65292;&#33021;&#22815;&#23450;&#21046;LLMs&#65292;&#20026;&#20854;&#21019;&#24314;&#29305;&#23450;&#20219;&#21153;&#30340;&#24037;&#20855;&#38598;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#38598;&#22686;&#24378;&#20854;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2309.17428</link><description>&lt;p&gt;
CRAFT: &#36890;&#36807;&#21019;&#24314;&#21644;&#26816;&#32034;&#19987;&#19994;&#24037;&#20855;&#38598;&#23450;&#21046;LLMs
&lt;/p&gt;
&lt;p&gt;
CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.17428
&lt;/p&gt;
&lt;p&gt;
CRAFT&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#24037;&#20855;&#21019;&#24314;&#21644;&#26816;&#32034;&#26694;&#26550;&#65292;&#33021;&#22815;&#23450;&#21046;LLMs&#65292;&#20026;&#20854;&#21019;&#24314;&#29305;&#23450;&#20219;&#21153;&#30340;&#24037;&#20855;&#38598;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#38598;&#22686;&#24378;&#20854;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#36890;&#36807;&#29983;&#25104;&#20195;&#30721;&#29255;&#27573;&#24182;&#36890;&#36807;&#29305;&#23450;&#20219;&#21153;&#30340;&#24212;&#29992;&#31243;&#24207;&#32534;&#31243;&#25509;&#21475;&#65288;API&#65289;&#25191;&#34892;&#23427;&#20204;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CRAFT&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;LLMs&#21019;&#24314;&#24037;&#20855;&#38598;&#30340;&#36890;&#29992;&#24037;&#20855;&#21019;&#24314;&#21644;&#26816;&#32034;&#26694;&#26550;&#12290;&#23427;&#20026;&#20219;&#21153;&#21019;&#24314;&#20102;&#29305;&#23450;&#30340;&#24037;&#20855;&#38598;&#65292;&#24182;&#20026;LLMs&#37197;&#22791;&#20102;&#19968;&#20010;&#32452;&#20214;&#65292;&#29992;&#20110;&#20174;&#36825;&#20123;&#38598;&#21512;&#20013;&#26816;&#32034;&#24037;&#20855;&#65292;&#20197;&#22686;&#24378;&#23427;&#20204;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.17428v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) are often augmented with tools to solve complex tasks. By generating code snippets and executing them through task-specific Application Programming Interfaces (APIs), they can offload certain functions to dedicated external modules, such as image encoding and performing calculations. However, most existing approaches to augment LLMs with tools are constrained by general-purpose APIs and lack the flexibility for tailoring them to specific tasks. In this work, we present CRAFT, a general tool creation and retrieval framework for LLMs. It creates toolsets specifically curated for the tasks and equips LLMs with a component that retrieves tools from these sets to enhance their capability to solve complex tasks. For each task, we collect specific code solutions by prompting GPT-4 to solve the training examples. Following a validation step ensuring the correctness, these solutions are abstracted into code 
&lt;/p&gt;</description></item><item><title>AdaSAP&#26041;&#27861;&#36890;&#36807;&#32593;&#32476;&#38160;&#24230;&#30340;&#35270;&#35282;&#32479;&#19968;&#20102;&#31283;&#20581;&#24615;&#21644;&#32039;&#20945;&#24615;&#30340;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#24341;&#20837;&#26435;&#37325;&#25200;&#21160;&#26469;&#20351;&#31232;&#30095;&#32593;&#32476;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#36755;&#20837;&#21464;&#21270;&#31283;&#20581;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2306.14306</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#38160;&#24230;&#24863;&#30693;&#20462;&#21098;&#29992;&#20110;&#31283;&#20581;&#31232;&#30095;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Adaptive Sharpness-Aware Pruning for Robust Sparse Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.14306
&lt;/p&gt;
&lt;p&gt;
AdaSAP&#26041;&#27861;&#36890;&#36807;&#32593;&#32476;&#38160;&#24230;&#30340;&#35270;&#35282;&#32479;&#19968;&#20102;&#31283;&#20581;&#24615;&#21644;&#32039;&#20945;&#24615;&#30340;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#24341;&#20837;&#26435;&#37325;&#25200;&#21160;&#26469;&#20351;&#31232;&#30095;&#32593;&#32476;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#30340;&#36755;&#20837;&#21464;&#21270;&#31283;&#20581;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2306.14306v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24517;&#39035;&#20855;&#26377;&#31283;&#20581;&#24615;&#21644;&#32039;&#20945;&#24615;&#36825;&#20004;&#20010;&#22522;&#26412;&#23646;&#24615;&#12290;&#31283;&#20581;&#24615;&#21644;&#32039;&#20945;&#24615;&#30340;&#30446;&#26631;&#20284;&#20046;&#26159;&#30456;&#20114;&#30683;&#30462;&#30340;&#65292;&#22240;&#20026;&#31283;&#20581;&#24615;&#38656;&#35201;&#22312;&#22495;&#38388;&#36827;&#34892;&#27867;&#21270;&#65292;&#32780;&#21387;&#32553;&#36807;&#31243;&#21017;&#21033;&#29992;&#19968;&#20010;&#22495;&#20013;&#30340;&#29305;&#23450;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#38160;&#24230;&#24863;&#30693;&#20462;&#21098;&#65288;AdaSAP&#65289;&#65292;&#36890;&#36807;&#32593;&#32476;&#38160;&#24230;&#30340;&#35270;&#35282;&#32479;&#19968;&#20102;&#36825;&#20123;&#30446;&#26631;&#12290;AdaSAP&#26041;&#27861;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#24341;&#20837;&#26435;&#37325;&#25200;&#21160;&#26469;&#20248;&#21270;&#25439;&#22833;&#26223;&#35266;&#65292;&#20174;&#32780;&#20135;&#29983;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#36755;&#20837;&#21464;&#21270;&#31283;&#20581;&#30340;&#31232;&#30095;&#32593;&#32476;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#26082;&#36866;&#21512;&#20462;&#21098;&#21448;&#36866;&#29992;&#20110;&#25913;&#36827;&#31283;&#20581;&#24615;&#30340;&#27491;&#21017;&#21270;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#19978;&#65292;AdaSAP&#33021;&#22815;&#20351;&#20462;&#21098;&#27169;&#22411;&#22312;ImageNet C&#19978;&#30340;&#31283;&#20581;&#31934;&#24230;&#25552;&#39640;&#26368;&#39640;&#36798;&#21040;+6&#65285;&#65292;&#22312;ImageNet V2&#19978;&#25552;&#39640;+4&#65285;&#65292;&#22312;&#30446;&#26631;&#26816;&#27979;&#19978;&#22312;&#19968;&#32452;&#21463;&#25439;Pascal VOC&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;+4&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.14306v2 Announce Type: replace  Abstract: Robustness and compactness are two essential attributes of deep learning models that are deployed in the real world. The goals of robustness and compactness may seem to be at odds, since robustness requires generalization across domains, while the process of compression exploits specificity in one domain. We introduce Adaptive Sharpness-Aware Pruning (AdaSAP), which unifies these goals through the lens of network sharpness. The AdaSAP method produces sparse networks that are robust to input variations which are unseen at training time. We achieve this by strategically incorporating weight perturbations in order to optimize the loss landscape. This allows the model to be both primed for pruning and regularized for improved robustness. AdaSAP improves the robust accuracy of pruned models on image classification by up to +6% on ImageNet C and +4% on ImageNet V2, and on object detection by +4% on a corrupted Pascal VOC dataset, over a wi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#25239;&#24615;&#20998;&#24067;&#35774;&#32622;&#20013;&#20855;&#26377;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#32479;&#35745;&#20449;&#24687;&#20445;&#35777;&#25910;&#25947;&#65292;&#33021;&#22815;&#36866;&#24212;&#23545;&#25239;&#24615;&#20998;&#24067;&#65292;&#22312;&#27169;&#25311;&#20013;&#23637;&#31034;&#20102;&#35299;&#20915;&#20984;&#38382;&#39064;&#30340;&#39640;&#25928;&#24615;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#35782;&#21035;&#23545;&#25163;&#24037;&#20316;&#32773;&#21644;&#23481;&#24525;&#19981;&#21516;&#23545;&#25163;&#29575;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2302.14615</link><description>&lt;p&gt;
&#22312;&#23545;&#25239;&#24615;&#20998;&#24067;&#35774;&#32622;&#20013;&#30340;&#38543;&#26426;Kaczmarz
&lt;/p&gt;
&lt;p&gt;
Randomized Kaczmarz in Adversarial Distributed Setting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.14615
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#25239;&#24615;&#20998;&#24067;&#35774;&#32622;&#20013;&#20855;&#26377;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#32479;&#35745;&#20449;&#24687;&#20445;&#35777;&#25910;&#25947;&#65292;&#33021;&#22815;&#36866;&#24212;&#23545;&#25239;&#24615;&#20998;&#24067;&#65292;&#22312;&#27169;&#25311;&#20013;&#23637;&#31034;&#20102;&#35299;&#20915;&#20984;&#38382;&#39064;&#30340;&#39640;&#25928;&#24615;&#65292;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#35782;&#21035;&#23545;&#25163;&#24037;&#20316;&#32773;&#21644;&#23481;&#24525;&#19981;&#21516;&#23545;&#25163;&#29575;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#23384;&#22312;&#23545;&#25239;&#24615;&#25110;&#21463;&#25439;&#24037;&#20316;&#32773;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#31283;&#20581;&#24615;&#65292;&#36825;&#26159;&#20351;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#21464;&#24471;&#23454;&#29992;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20984;&#20248;&#21270;&#38382;&#39064;&#20855;&#26377;&#23545;&#25239;&#24615;&#30340;&#36845;&#20195;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#31616;&#21333;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30830;&#20445;&#25910;&#25947;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#23545;&#25239;&#24615;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#22312;&#23384;&#22312;&#23545;&#25163;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#27169;&#25311;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#20984;&#38382;&#39064;&#19978;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#27169;&#25311;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#23384;&#22312;&#23545;&#25163;&#30340;&#24773;&#20917;&#19979;&#30340;&#39640;&#25928;&#24615;&#65292;&#24182;&#19988;&#23427;&#33021;&#22815;&#39640;&#24230;&#20934;&#30830;&#22320;&#35782;&#21035;&#23545;&#25163;&#24037;&#20316;&#32773;&#24182;&#23481;&#24525;&#19981;&#21516;&#27700;&#24179;&#30340;&#23545;&#25163;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.14615v2 Announce Type: replace-cross  Abstract: Developing large-scale distributed methods that are robust to the presence of adversarial or corrupted workers is an important part of making such methods practical for real-world problems. In this paper, we propose an iterative approach that is adversary-tolerant for convex optimization problems. By leveraging simple statistics, our method ensures convergence and is capable of adapting to adversarial distributions. Additionally, the efficiency of the proposed methods for solving convex problems is shown in simulations with the presence of adversaries. Through simulations, we demonstrate the efficiency of our approach in the presence of adversaries and its ability to identify adversarial workers with high accuracy and tolerate varying levels of adversary rates.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#38024;&#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#39318;&#27425;&#35843;&#30740;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#23398;&#26415;&#30028;&#21644;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>https://arxiv.org/abs/2302.06670</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#26816;&#27979;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Explainable Anomaly Detection in Images and Videos: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.06670
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#38024;&#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#39318;&#27425;&#35843;&#30740;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#23398;&#26415;&#30028;&#21644;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#21644;&#23450;&#20301;&#35270;&#35273;&#25968;&#25454;&#65288;&#21253;&#25324;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#23398;&#26415;&#30028;&#21644;&#24212;&#29992;&#23454;&#38469;&#22330;&#26223;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#21487;&#35270;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#36805;&#36895;&#21457;&#23637;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#40657;&#30418;&#27169;&#22411;&#30340;&#35299;&#37322;&#20197;&#21450;&#20026;&#20309;&#21487;&#20197;&#21306;&#20998;&#24322;&#24120;&#30340;&#21512;&#29702;&#35299;&#37322;&#21364;&#21313;&#20998;&#31232;&#32570;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20379;&#20102;&#19968;&#39033;&#38598;&#20013;&#20110;&#21487;&#35299;&#37322;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#35843;&#30740;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#22270;&#20687;&#32423;&#21644;&#35270;&#39057;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#22522;&#26412;&#32972;&#26223;&#12290;&#28982;&#21518;&#65292;&#20316;&#20026;&#26412;&#35843;&#30740;&#30340;&#20027;&#35201;&#20869;&#23481;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38024;&#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#20840;&#38754;&#21644;&#35814;&#23613;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20026;&#20160;&#20040;&#19968;&#20123;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#32780;&#21478;&#19968;&#20123;&#21017;&#21482;&#33021;&#24212;&#29992;&#20110;&#19968;&#31181;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.06670v2 Announce Type: replace-cross  Abstract: Anomaly detection and localization of visual data, including images and videos, are of great significance in both machine learning academia and applied real-world scenarios. Despite the rapid development of visual anomaly detection techniques in recent years, the interpretations of these black-box models and reasonable explanations of why anomalies can be distinguished out are scarce. This paper provides the first survey concentrated on explainable visual anomaly detection methods. We first introduce the basic background of image-level and video-level anomaly detection. Then, as the main content of this survey, a comprehensive and exhaustive literature review of explainable anomaly detection methods for both images and videos is presented. Next, we analyze why some explainable anomaly detection methods can be applied to both images and videos and why others can be only applied to one modality. Additionally, we provide summaries
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#35838;&#31243;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#35299;&#20915;&#29616;&#26377;&#22270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#19981;&#20339;&#38382;&#39064;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2302.02926</link><description>&lt;p&gt;
&#35838;&#31243;&#22270;&#26426;&#22120;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Curriculum Graph Machine Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.02926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#35838;&#31243;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#35299;&#20915;&#29616;&#26377;&#22270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#19981;&#20339;&#38382;&#39064;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22312;&#25991;&#29486;&#20013;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34987;&#35774;&#35745;&#20026;&#20197;&#38543;&#26426;&#39034;&#24207;&#23545;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#33021;&#30001;&#20110;&#24573;&#35270;&#19981;&#21516;&#22270;&#25968;&#25454;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#23427;&#20204;&#30340;&#35757;&#32451;&#39034;&#24207;&#23545;&#27169;&#22411;&#20248;&#21270;&#29366;&#24577;&#30340;&#24433;&#21709;&#32780;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#65292;&#35838;&#31243;&#22270;&#26426;&#22120;&#23398;&#20064;&#65288;Graph CL&#65289;&#24212;&#36816;&#32780;&#29983;&#65292;&#34701;&#21512;&#20102;&#22270;&#26426;&#22120;&#23398;&#20064;&#21644;&#35838;&#31243;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#21560;&#24341;&#20102;&#30740;&#31350;&#30028;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#27010;&#36848;&#20102;&#20851;&#20110;Graph CL&#30340;&#26041;&#27861;&#65292;&#24182;&#35814;&#32454;&#35843;&#26597;&#20102;&#36825;&#19968;&#26041;&#21521;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;Graph CL&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#20854;&#27491;&#24335;&#30340;&#38382;&#39064;&#23450;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#26041;&#27861;&#20998;&#31867;&#21644;&#24635;&#32467;&#20026;&#19977;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.02926v2 Announce Type: replace  Abstract: Graph machine learning has been extensively studied in both academia and industry. However, in the literature, most existing graph machine learning models are designed to conduct training with data samples in a random order, which may suffer from suboptimal performance due to ignoring the importance of different graph data samples and their training orders for the model optimization status. To tackle this critical problem, curriculum graph machine learning (Graph CL), which integrates the strength of graph machine learning and curriculum learning, arises and attracts an increasing amount of attention from the research community. Therefore, in this paper, we comprehensively overview approaches on Graph CL and present a detailed survey of recent advances in this direction. Specifically, we first discuss the key challenges of Graph CL and provide its formal problem definition. Then, we categorize and summarize existing methods into thre
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#25856;&#23721;&#24555;&#25346;&#19978;&#23433;&#35013;&#30340;&#21152;&#36895;&#24230;&#20256;&#24863;&#22120;&#37319;&#38598;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#22312;&#25856;&#23721;&#27963;&#21160;&#20013;&#26816;&#27979;&#25856;&#23721;&#32773;&#19979;&#38477;&#24773;&#20917;&#30340;&#25216;&#26415;&#65292;&#20445;&#25252;&#25856;&#23721;&#32773;&#38544;&#31169;&#21644;&#20581;&#36523;&#25151;&#25104;&#26412;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20415;&#21033;&#24615;&#12290;</title><link>https://arxiv.org/abs/2301.10164</link><description>&lt;p&gt;
&#22522;&#20110;&#20256;&#24863;&#22120;&#22686;&#24378;&#24555;&#25346;&#30340;&#26397;&#21521;&#30340;&#25856;&#23721;&#20013;&#19979;&#38477;&#34892;&#20026;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Lowering Detection in Sport Climbing Based on Orientation of the Sensor Enhanced Quickdraw
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.10164
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#25856;&#23721;&#24555;&#25346;&#19978;&#23433;&#35013;&#30340;&#21152;&#36895;&#24230;&#20256;&#24863;&#22120;&#37319;&#38598;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#22312;&#25856;&#23721;&#27963;&#21160;&#20013;&#26816;&#27979;&#25856;&#23721;&#32773;&#19979;&#38477;&#24773;&#20917;&#30340;&#25216;&#26415;&#65292;&#20445;&#25252;&#25856;&#23721;&#32773;&#38544;&#31169;&#21644;&#20581;&#36523;&#25151;&#25104;&#26412;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20415;&#21033;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36319;&#36394;&#25856;&#23721;&#32773;&#30340;&#27963;&#21160;&#20197;&#25913;&#21892;&#26381;&#21153;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#20182;&#20204;&#30340;&#22522;&#30784;&#35774;&#26045;&#26159;&#25856;&#23721;&#20581;&#36523;&#25151;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;&#24517;&#39035;&#20174;&#24320;&#22987;&#20998;&#26512;&#27599;&#20010;&#25856;&#23721;&#27963;&#21160;&#30452;&#21040;&#25856;&#30331;&#32773;&#38477;&#19979;&#26469;&#12290;&#22240;&#27492;&#65292;&#21457;&#29616;&#25856;&#23721;&#32773;&#19979;&#38477;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#36825;&#26631;&#24535;&#30528;&#25856;&#30331;&#32467;&#26463;&#12290;&#24517;&#39035;&#22312;&#20445;&#25252;&#25856;&#23721;&#32773;&#21644;&#20581;&#36523;&#25151;&#25104;&#26412;&#38544;&#31169;&#21644;&#20415;&#21033;&#24615;&#30340;&#21516;&#26102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#30828;&#20214;&#21407;&#22411;&#65292;&#20351;&#29992;&#38468;&#22312;&#22681;&#19978;&#30340;&#25856;&#23721;&#35774;&#22791;&#19978;&#30340;&#21152;&#36895;&#24230;&#20256;&#24863;&#22120;&#25910;&#38598;&#25968;&#25454;&#65292;&#31216;&#20026;&#24555;&#25346;&#65292;&#23427;&#36830;&#25509;&#25856;&#23721;&#32499;&#21644;&#34746;&#26643;&#38170;&#28857;&#12290;&#30456;&#24212;&#30340;&#20256;&#24863;&#22120;&#34987;&#37197;&#32622;&#20026;&#33410;&#33021;&#65292;&#22240;&#27492;&#22312;&#25856;&#23721;&#20581;&#36523;&#25151;&#22823;&#37327;&#20351;&#29992;&#26102;&#22312;&#36153;&#29992;&#21644;&#26356;&#25442;&#25152;&#38656;&#26102;&#38388;&#26041;&#38754;&#21464;&#24471;&#23454;&#29992;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#30828;&#20214;&#35268;&#26684;&#65292;&#24182;&#30740;&#31350;&#20102;&#20256;&#24863;&#22120;&#27979;&#24471;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.10164v2 Announce Type: replace-cross  Abstract: Tracking climbers' activity to improve services and make the best use of their infrastructure is a concern for climbing gyms. Each climbing session must be analyzed from beginning till lowering of the climber. Therefore, spotting the climbers descending is crucial since it indicates when the ascent has come to an end. This problem must be addressed while preserving privacy and convenience of the climbers and the costs of the gyms. To this aim, a hardware prototype is developed to collect data using accelerometer sensors attached to a piece of climbing equipment mounted on the wall, called quickdraw, that connects the climbing rope to the bolt anchors. The corresponding sensors are configured to be energy-efficient, hence become practical in terms of expenses and time consumption for replacement when using in large quantity in a climbing gym. This paper describes hardware specifications, studies data measured by the sensors in u
&lt;/p&gt;</description></item><item><title>&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36148;&#36817;&#26799;&#24230;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#23616;&#37096;&#24179;&#28369;&#24230;&#27169;&#37327;&#30340;&#20272;&#35745;&#65292;&#21487;&#20197;&#22312;&#20984;&#20248;&#21270;&#20013;&#36991;&#20813;&#20351;&#29992;&#22238;&#28335;&#32447;&#25628;&#32034;&#65292;&#24182;&#26681;&#25454;&#23616;&#37096;&#24179;&#28369;&#24230;&#20272;&#35745;&#33258;&#36866;&#24212;&#35843;&#25972;&#27493;&#38271;&#12290;</title><link>https://arxiv.org/abs/2301.04431</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#36828;&#31471;&#31639;&#27861;&#29992;&#20110;&#20984;&#20248;&#21270;&#22312;&#26799;&#24230;&#30340;&#23616;&#37096;Lipschitz&#36830;&#32493;&#24615;&#19979;
&lt;/p&gt;
&lt;p&gt;
Adaptive proximal algorithms for convex optimization under local Lipschitz continuity of the gradient
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.04431
&lt;/p&gt;
&lt;p&gt;
&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36148;&#36817;&#26799;&#24230;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#30340;&#23616;&#37096;&#24179;&#28369;&#24230;&#27169;&#37327;&#30340;&#20272;&#35745;&#65292;&#21487;&#20197;&#22312;&#20984;&#20248;&#21270;&#20013;&#36991;&#20813;&#20351;&#29992;&#22238;&#28335;&#32447;&#25628;&#32034;&#65292;&#24182;&#26681;&#25454;&#23616;&#37096;&#24179;&#28369;&#24230;&#20272;&#35745;&#33258;&#36866;&#24212;&#35843;&#25972;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#28335;&#32447;&#25628;&#32034;&#26159;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#20855;&#26377;&#23616;&#37096;Lipschitz&#26799;&#24230;&#30340;&#36830;&#32493;&#21487;&#24494;&#20989;&#25968;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#35777;&#26126;&#22312;&#20984;&#35774;&#32622;&#20013;&#23436;&#20840;&#21487;&#20197;&#36991;&#20813;&#32447;&#25628;&#32034;&#65292;&#24182;&#19988;&#20801;&#35768;&#27493;&#38271;&#26681;&#25454;&#23616;&#37096;&#24179;&#28369;&#24230;&#20272;&#35745;&#36827;&#34892;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#22238;&#28335;&#25110;&#35780;&#20272;&#20989;&#25968;&#20540;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36148;&#36817;&#26799;&#24230;&#26041;&#27861;&#65292;adaPG&#65292;&#23427;&#20351;&#29992;&#23616;&#37096;&#24179;&#28369;&#24230;&#27169;&#37327;&#30340;&#26032;&#20272;&#35745;&#65292;&#23548;&#33268;&#26356;&#23569;&#20445;&#23432;&#30340;&#27493;&#38271;&#26356;&#26032;&#65292;&#36824;&#21487;&#20197;&#22788;&#29702;&#38750;&#20809;&#28369;&#39033;&#12290;&#36825;&#20010;&#24819;&#27861;&#34987;&#25193;&#23637;&#21040;&#21407;&#22987;-&#23545;&#20598;&#35774;&#32622;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#19977;&#39033;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#65292;adaPD&#65292;&#23427;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;PDHG&#26041;&#27861;&#30340;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#25552;&#20986;&#20102;&#8220;&#26412;&#36136;&#19978;&#8221;&#23436;&#20840;&#33258;&#36866;&#24212;&#30340;&#21464;&#20307;adaPD$^+$&#65292;&#36890;&#36807;&#35843;&#29992;&#19968;&#20010;&#22238;&#28335;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#35780;&#20272;&#32447;&#24615;&#31639;&#23376;&#33539;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.04431v4 Announce Type: replace-cross  Abstract: Backtracking linesearch is the de facto approach for minimizing continuously differentiable functions with locally Lipschitz gradient. In recent years, it has been shown that in the convex setting it is possible to avoid linesearch altogether, and to allow the stepsize to adapt based on a local smoothness estimate without any backtracks or evaluations of the function value. In this work we propose an adaptive proximal gradient method, adaPG, that uses novel estimates of the local smoothness modulus which leads to less conservative stepsize updates and that can additionally cope with nonsmooth terms. This idea is extended to the primal-dual setting where an adaptive three-term primal-dual algorithm, adaPD, is proposed which can be viewed as an extension of the PDHG method. Moreover, in this setting the "essentially" fully adaptive variant adaPD$^+$ is proposed that avoids evaluating the linear operator norm by invoking a backtra
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22768;&#23398;&#29305;&#24449;&#21644;&#38477;&#32500;&#30340;&#26080;&#30417;&#30563;&#22768;&#23398;&#22330;&#26223;&#26144;&#23556;&#26041;&#27861;&#65292;&#21033;&#29992;&#23616;&#37096;&#20849;&#24418;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;LOCA&#65289;&#23398;&#20064;&#22768;&#23398;&#25968;&#25454;&#30340;&#26631;&#20934;&#21270;&#22352;&#26631;&#65292;&#33021;&#22815;&#36739;&#22909;&#22320;&#22788;&#29702;&#28151;&#21709;&#21644;&#21152;&#24615;&#22122;&#22768;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#21644;&#27169;&#25311;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2301.00448</link><description>&lt;p&gt;
&#22522;&#20110;&#22768;&#23398;&#29305;&#24449;&#21644;&#38477;&#32500;&#30340;&#26080;&#30417;&#30563;&#22768;&#23398;&#22330;&#26223;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Acoustic Scene Mapping Based on Acoustic Features and Dimensionality Reduction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.00448
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22768;&#23398;&#29305;&#24449;&#21644;&#38477;&#32500;&#30340;&#26080;&#30417;&#30563;&#22768;&#23398;&#22330;&#26223;&#26144;&#23556;&#26041;&#27861;&#65292;&#21033;&#29992;&#23616;&#37096;&#20849;&#24418;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;LOCA&#65289;&#23398;&#20064;&#22768;&#23398;&#25968;&#25454;&#30340;&#26631;&#20934;&#21270;&#22352;&#26631;&#65292;&#33021;&#22815;&#36739;&#22909;&#22320;&#22788;&#29702;&#28151;&#21709;&#21644;&#21152;&#24615;&#22122;&#22768;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#21644;&#27169;&#25311;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22768;&#23398;&#22330;&#26223;&#26144;&#23556;&#26041;&#27861;&#38656;&#35201;&#20272;&#35745;&#40614;&#20811;&#39118;&#20043;&#38388;&#30340;&#21040;&#36798;&#26102;&#38388;&#24046;&#65288;TDOA&#65289;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;TDOA&#20272;&#35745;&#23545;&#28151;&#21709;&#21644;&#21152;&#24615;&#22122;&#22768;&#38750;&#24120;&#25935;&#24863;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#30340;&#33258;&#28982;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#22312;&#23616;&#37096;&#20849;&#24418;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;LOCA&#65289;&#20043;&#19978;-&#19968;&#31181;&#29992;&#20110;&#20174;&#27979;&#37327;&#20013;&#23398;&#20064;&#26631;&#20934;&#21270;&#25968;&#25454;&#22352;&#26631;&#30340;&#31163;&#32447;&#28145;&#24230;&#23398;&#20064;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35774;&#32622;&#21253;&#25324;&#19968;&#20010;&#27979;&#37327;&#22768;&#28304;&#22312;&#22768;&#23398;&#23553;&#38381;&#31354;&#38388;&#20013;&#22810;&#20010;&#20301;&#32622;&#30340;&#40614;&#20811;&#39118;&#38453;&#21015;&#12290;&#25105;&#20204;&#35777;&#26126;LOCA&#23398;&#20064;&#21040;&#20102;&#19968;&#20010;&#19982;&#40614;&#20811;&#39118;&#30340;&#31354;&#38388;&#20301;&#32622;&#31561;&#36317;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#36924;&#30495;&#30340;&#27169;&#25311;&#23454;&#39564;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#20854;&#20182;&#38477;&#32500;&#26041;&#26696;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#28151;&#21709;&#23545;LOCA&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.00448v2 Announce Type: replace-cross  Abstract: Classical methods for acoustic scene mapping require the estimation of time difference of arrival (TDOA) between microphones. Unfortunately, TDOA estimation is very sensitive to reverberation and additive noise. We introduce an unsupervised data-driven approach that exploits the natural structure of the data. Our method builds upon local conformal autoencoders (LOCA) - an offline deep learning scheme for learning standardized data coordinates from measurements. Our experimental setup includes a microphone array that measures the transmitted sound source at multiple locations across the acoustic enclosure. We demonstrate that LOCA learns a representation that is isometric to the spatial locations of the microphones. The performance of our method is evaluated using a series of realistic simulations and compared with other dimensionality-reduction schemes. We further assess the influence of reverberation on the results of LOCA and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30452;&#25509;&#28508;&#22312;&#27169;&#22411;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#20108;&#27425;&#39640;&#26031;&#25511;&#21046;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#29366;&#24577;&#34920;&#31034;&#20989;&#25968;&#21644;&#25511;&#21046;&#22120;&#12290;</title><link>https://arxiv.org/abs/2212.14511</link><description>&lt;p&gt;
&#30452;&#25509;&#28508;&#22312;&#27169;&#22411;&#23398;&#20064;&#33021;&#22815;&#35299;&#20915;&#32447;&#24615;&#20108;&#27425;&#39640;&#26031;&#25511;&#21046;&#38382;&#39064;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Direct Latent Model Learning Solve Linear Quadratic Gaussian Control?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.14511
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30452;&#25509;&#28508;&#22312;&#27169;&#22411;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#20108;&#27425;&#39640;&#26031;&#25511;&#21046;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#29366;&#24577;&#34920;&#31034;&#20989;&#25968;&#21644;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#28508;&#22312;&#39640;&#32500;&#35266;&#27979;&#20013;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#30340;&#20219;&#21153;&#65292;&#30446;&#26631;&#26159;&#25511;&#21046;&#26410;&#30693;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#31995;&#32479;&#12290;&#25105;&#20204;&#37319;&#29992;&#30452;&#25509;&#28508;&#22312;&#27169;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#19982;&#35268;&#21010;&#30452;&#25509;&#30456;&#20851;&#30340;&#25968;&#37327;&#65288;&#20363;&#22914;&#25104;&#26412;&#65289;&#26469;&#23398;&#20064;&#28508;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#37325;&#24314;&#35266;&#27979;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31181;&#30452;&#35266;&#30340;&#22522;&#20110;&#25104;&#26412;&#39537;&#21160;&#30340;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#20108;&#27425;&#39640;&#26031;&#65288;LQG&#65289;&#25511;&#21046;&#38382;&#39064;&#65292;&#36825;&#26159;&#26368;&#22522;&#26412;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#25511;&#21046;&#38382;&#39064;&#20043;&#19968;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#29366;&#24577;&#34920;&#31034;&#20989;&#25968;&#21644;&#20351;&#29992;&#30452;&#25509;&#23398;&#20064;&#30340;&#28508;&#22312;&#27169;&#22411;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#25511;&#21046;&#22120;&#30340;&#20445;&#35777;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23613;&#31649;&#20197;&#21069;&#30340;&#30456;&#20851;&#24037;&#20316;&#21462;&#24471;&#20102;&#21508;&#31181;&#32463;&#39564;&#25104;&#21151;&#65292;&#20294;&#22312;&#36825;&#39033;&#24037;&#20316;&#20043;&#21069;&#65292;&#23578;&#19981;&#28165;&#26970;&#36825;&#31181;&#22522;&#20110;&#25104;&#26412;&#39537;&#21160;&#30340;&#28508;&#22312;&#27169;&#22411;&#23398;&#20064;&#26041;&#27861;&#26159;&#21542;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.14511v2 Announce Type: replace  Abstract: We study the task of learning state representations from potentially high-dimensional observations, with the goal of controlling an unknown partially observable system. We pursue a direct latent model learning approach, where a dynamic model in some latent state space is learned by predicting quantities directly related to planning (e.g., costs) without reconstructing the observations. In particular, we focus on an intuitive cost-driven state representation learning method for solving Linear Quadratic Gaussian (LQG) control, one of the most fundamental partially observable control problems. As our main results, we establish finite-sample guarantees of finding a near-optimal state representation function and a near-optimal controller using the directly learned latent model. To the best of our knowledge, despite various empirical successes, prior to this work it was unclear if such a cost-driven latent model learner enjoys finite-sampl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#36827;&#34892;&#24314;&#27169;&#30340;&#21464;&#25442;&#19981;&#21464;&#25439;&#22833;&#20989;&#25968;TILDE-Q&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#25429;&#33719;&#20449;&#21495;&#24418;&#29366;&#21644;&#27169;&#25311;&#32454;&#24494;&#26102;&#38388;&#21160;&#24577;&#19978;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2210.15050</link><description>&lt;p&gt;
TILDE-Q&#65306;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#21464;&#25442;&#19981;&#21464;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
TILDE-Q: A Transformation Invariant Loss Function for Time-Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.15050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#36827;&#34892;&#24314;&#27169;&#30340;&#21464;&#25442;&#19981;&#21464;&#25439;&#22833;&#20989;&#25968;TILDE-Q&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#25429;&#33719;&#20449;&#21495;&#24418;&#29366;&#21644;&#27169;&#25311;&#32454;&#24494;&#26102;&#38388;&#21160;&#24577;&#19978;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#35299;&#20915;&#33021;&#28304;&#12289;&#22825;&#27668;&#12289;&#20132;&#36890;&#21644;&#32463;&#27982;&#31561;&#21508;&#39046;&#22495;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26159;&#19968;&#20010;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#30340;&#39046;&#22495;&#65292;&#20294;&#23545;&#20110;&#39044;&#27979;&#22797;&#26434;&#30340;&#26102;&#38388;&#27169;&#24335;&#65292;&#22914;&#39034;&#24207;&#25968;&#25454;&#20013;&#30340;&#31361;&#21464;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#36825;&#31181;&#22256;&#38590;&#28304;&#20110;&#23558;Lp&#33539;&#25968;&#36317;&#31163;&#26368;&#23567;&#21270;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#20363;&#22914;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#25110;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#65292;&#36825;&#20123;&#20989;&#25968;&#23481;&#26131;&#21463;&#21040;&#22797;&#26434;&#26102;&#38388;&#21160;&#24577;&#24314;&#27169;&#21644;&#20449;&#21495;&#24418;&#29366;&#25429;&#33719;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20989;&#25968;&#36890;&#24120;&#20250;&#23548;&#33268;&#27169;&#22411;&#34892;&#20026;&#24322;&#24120;&#65292;&#29983;&#25104;&#19982;&#21407;&#22987;&#26102;&#38388;&#24207;&#21015;&#19981;&#30456;&#20851;&#30340;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#19968;&#31181;&#36229;&#36234;&#28857;&#23545;&#28857;&#27604;&#36739;&#30340;&#24418;&#29366;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24418;&#29366;&#21644;&#22833;&#30495;&#30340;&#23450;&#20041;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.15050v2 Announce Type: replace  Abstract: Time-series forecasting has gained increasing attention in the field of artificial intelligence due to its potential to address real-world problems across various domains, including energy, weather, traffic, and economy. While time-series forecasting is a well-researched field, predicting complex temporal patterns such as sudden changes in sequential data still poses a challenge with current models. This difficulty stems from minimizing Lp norm distances as loss functions, such as mean absolute error (MAE) or mean square error (MSE), which are susceptible to both intricate temporal dynamics modeling and signal shape capturing. Furthermore, these functions often cause models to behave aberrantly and generate uncorrelated results with the original time-series. Consequently, developing a shape-aware loss function that goes beyond mere point-wise comparison is essential. In this paper, we examine the definition of shape and distortions, 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#27491;&#30830;&#26657;&#20934;&#35823;&#24046;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#26657;&#20934;&#35823;&#24046;&#19982;&#27491;&#30830;&#24471;&#20998;&#30456;&#20851;&#32852;&#65292;&#25552;&#20379;&#20102;&#26368;&#20339;&#20272;&#35745;&#29305;&#24615;&#30340;&#19978;&#30028;&#65292;&#21487;&#38752;&#37327;&#21270;&#27169;&#22411;&#26657;&#20934;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2203.07835</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#30830;&#24471;&#20998;&#25913;&#36827;&#20998;&#31867;&#21450;&#20854;&#20182;&#20219;&#21153;&#30340;&#26356;&#22909;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Better Uncertainty Calibration via Proper Scores for Classification and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2203.07835
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#27491;&#30830;&#26657;&#20934;&#35823;&#24046;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#26657;&#20934;&#35823;&#24046;&#19982;&#27491;&#30830;&#24471;&#20998;&#30456;&#20851;&#32852;&#65292;&#25552;&#20379;&#20102;&#26368;&#20339;&#20272;&#35745;&#29305;&#24615;&#30340;&#19978;&#30028;&#65292;&#21487;&#38752;&#37327;&#21270;&#27169;&#22411;&#26657;&#20934;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#23545;&#20110;&#25935;&#24863;&#30340;&#29616;&#23454;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20174;&#19994;&#32773;&#36234;&#26469;&#36234;&#20851;&#27880;&#25913;&#36827;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#12290;&#26657;&#20934;&#35823;&#24046;&#26088;&#22312;&#37327;&#21270;&#27010;&#29575;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#65292;&#20294;&#23427;&#20204;&#30340;&#20272;&#35745;&#36890;&#24120;&#26159;&#26377;&#20559;&#24046;&#19988;&#19981;&#19968;&#33268;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#27491;&#30830;&#26657;&#20934;&#35823;&#24046;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#27599;&#20010;&#26657;&#20934;&#35823;&#24046;&#19982;&#27491;&#30830;&#24471;&#20998;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#26377;&#26368;&#20339;&#20272;&#35745;&#29305;&#24615;&#30340;&#30456;&#24212;&#19978;&#30028;&#12290;&#36825;&#31181;&#20851;&#31995;&#21487;&#29992;&#20110;&#21487;&#38752;&#22320;&#37327;&#21270;&#27169;&#22411;&#26657;&#20934;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#23637;&#31034;&#20102;&#19982;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#24120;&#29992;&#20272;&#35745;&#22120;&#30340;&#32570;&#38519;&#12290;&#30001;&#20110;&#27491;&#30830;&#24471;&#20998;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#65292;&#36825;&#20026;&#36229;&#20986;&#20998;&#31867;&#30340;&#37325;&#26032;&#26657;&#20934;&#25552;&#20379;&#20102;&#33258;&#28982;&#24310;&#20280;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2203.07835v4 Announce Type: replace  Abstract: With model trustworthiness being crucial for sensitive real-world applications, practitioners are putting more and more focus on improving the uncertainty calibration of deep neural networks. Calibration errors are designed to quantify the reliability of probabilistic predictions but their estimators are usually biased and inconsistent. In this work, we introduce the framework of proper calibration errors, which relates every calibration error to a proper score and provides a respective upper bound with optimal estimation properties. This relationship can be used to reliably quantify the model calibration improvement. We theoretically and empirically demonstrate the shortcomings of commonly used estimators compared to our approach. Due to the wide applicability of proper scores, this gives a natural extension of recalibration beyond classification.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20122;&#32447;&#24615;&#26102;&#38388;&#19979;&#30340;&#32456;&#31471;&#23884;&#20837;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#25197;&#26354;$1+\epsilon$&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#26356;&#21152;&#36890;&#29992;&#19988;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2110.08691</link><description>&lt;p&gt;
&#20122;&#32447;&#24615;&#26102;&#38388;&#19979;&#30340;&#32456;&#31471;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Terminal Embeddings in Sublinear Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.08691
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20122;&#32447;&#24615;&#26102;&#38388;&#19979;&#30340;&#32456;&#31471;&#23884;&#20837;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#25197;&#26354;$1+\epsilon$&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#26356;&#21152;&#36890;&#29992;&#19988;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65288;Elkin, Filtser, Neiman 2017&#65289;&#24341;&#20837;&#20102;&#20174;&#19968;&#20010;&#20855;&#26377;&#19968;&#32452;&#25351;&#23450;&#32456;&#31471;$T \subset X$&#30340;&#24230;&#37327;&#31354;&#38388;$(X,d_X)$&#21040;&#21478;&#19968;&#20010;$(Y,d_Y)$&#30340;{\it &#32456;&#31471;&#23884;&#20837;}&#30340;&#27010;&#24565;&#12290;&#24403;$X,Y$&#37117;&#26159;&#27431;&#20960;&#37324;&#24503;&#24230;&#37327;&#65292;&#20854;&#20013;$Y$&#26159;$m$&#32500;&#26102;&#65292;&#26368;&#36817;&#65288;Narayanan, Nelson 2019&#65289;&#22312;&#65288;Mahabadi, Makarychev, Makarychev, Razenshteyn 2018&#65289;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#26679;&#19968;&#20010;&#32456;&#31471;&#23884;&#20837;&#23454;&#29616;&#25197;&#26354;$1+\epsilon$&#65292;&#20854;&#20013;$m = O(\epsilon^{-2}\log n)$&#65292;&#20854;&#20013;$n := |T|$&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.08691v3 Announce Type: replace-cross  Abstract: Recently (Elkin, Filtser, Neiman 2017) introduced the concept of a {\it terminal embedding} from one metric space $(X,d_X)$ to another $(Y,d_Y)$ with a set of designated terminals $T\subset X$. Such an embedding $f$ is said to have distortion $\rho\ge 1$ if $\rho$ is the smallest value such that there exists a constant $C&gt;0$ satisfying   \begin{equation*}   \forall x\in T\ \forall q\in X,\ C d_X(x, q) \le d_Y(f(x), f(q)) \le C \rho d_X(x, q) .   \end{equation*}   When $X,Y$ are both Euclidean metrics with $Y$ being $m$-dimensional, recently (Narayanan, Nelson 2019), following work of (Mahabadi, Makarychev, Makarychev, Razenshteyn 2018), showed that distortion $1+\epsilon$ is achievable via such a terminal embedding with $m = O(\epsilon^{-2}\log n)$ for $n := |T|$. This generalizes the Johnson-Lindenstrauss lemma, which only preserves distances within $T$ and not to $T$ from the rest of space. The downside of prior work is that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21452;&#27491;&#21017;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#31232;&#30095;&#29983;&#29289;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#39044;&#27979;&#33647;&#29289;&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#21644;&#33647;&#29289;&#30142;&#30149;&#20851;&#32852;&#26041;&#38754;&#20855;&#26377;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.16664</link><description>&lt;p&gt;
&#24555;&#36895;&#21452;&#27491;&#21017;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#31232;&#30095;&#29983;&#29289;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Fast Dual-Regularized Autoencoder for Sparse Biological Data. (arXiv:2401.16664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21452;&#27491;&#21017;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#31232;&#30095;&#29983;&#29289;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#39044;&#27979;&#33647;&#29289;&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#21644;&#33647;&#29289;&#30142;&#30149;&#20851;&#32852;&#26041;&#38754;&#20855;&#26377;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31232;&#30095;&#25968;&#25454;&#20013;&#25512;&#26029;&#20851;&#31995;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#24212;&#29992;&#33539;&#22260;&#20174;&#20135;&#21697;&#25512;&#33616;&#21040;&#33647;&#29289;&#21457;&#29616;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#31232;&#30095;&#30697;&#38453;&#34917;&#20840;&#30340;&#32447;&#24615;&#27169;&#22411;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#19978;&#30456;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#20855;&#26377;&#24778;&#20154;&#30340;&#20248;&#21183;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#32447;&#24615;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#27973;&#23618;&#33258;&#32534;&#30721;&#22120;&#26469;&#35299;&#20915;&#21452;&#37051;&#22495;&#27491;&#21017;&#21270;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#33647;&#29289;&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#21644;&#33647;&#29289;&#30142;&#30149;&#20851;&#32852;&#26041;&#38754;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relationship inference from sparse data is an important task with applications ranging from product recommendation to drug discovery. A recently proposed linear model for sparse matrix completion has demonstrated surprising advantage in speed and accuracy over more sophisticated recommender systems algorithms. Here we extend the linear model to develop a shallow autoencoder for the dual neighborhood-regularized matrix completion problem. We demonstrate the speed and accuracy advantage of our approach over the existing state-of-the-art in predicting drug-target interactions and drug-disease associations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35266;&#23519;&#31383;&#21475;&#21010;&#20998;&#20026;&#26102;&#38388;&#27573;&#65292;&#36890;&#36807;&#20248;&#21270;&#39640;&#20248;&#20808;&#32423;&#29305;&#24449;&#30340;&#26102;&#38388;bin&#22823;&#23567;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#31616;&#21333;&#12289;&#26356;&#24555;&#36895;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#19988;&#33021;&#22815;&#36798;&#21040;&#19982;&#26356;&#22797;&#26434;&#27169;&#22411;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16537</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#34892;&#25919;&#25968;&#25454;&#26426;&#22120;&#23398;&#20064;&#35266;&#23519;&#26102;&#38388;&#31383;&#21475;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Efficient Observation Time Window Segmentation for Administrative Data Machine Learning. (arXiv:2401.16537v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35266;&#23519;&#31383;&#21475;&#21010;&#20998;&#20026;&#26102;&#38388;&#27573;&#65292;&#36890;&#36807;&#20248;&#21270;&#39640;&#20248;&#20808;&#32423;&#29305;&#24449;&#30340;&#26102;&#38388;bin&#22823;&#23567;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#31616;&#21333;&#12289;&#26356;&#24555;&#36895;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#19988;&#33021;&#22815;&#36798;&#21040;&#19982;&#26356;&#22797;&#26434;&#27169;&#22411;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#34892;&#25919;&#25968;&#25454;&#39044;&#27979;&#32467;&#26524;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#39046;&#22495;&#65292;&#23588;&#20854;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#12290;&#22823;&#22810;&#25968;&#34892;&#25919;&#25968;&#25454;&#35760;&#24405;&#37117;&#26377;&#26102;&#38388;&#25139;&#65292;&#35760;&#24405;&#38543;&#26102;&#38388;&#30340;&#27169;&#24335;&#26159;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20851;&#38190;&#36755;&#20837;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35266;&#23519;&#31383;&#21475;&#21010;&#20998;&#20026;&#26102;&#38388;&#27573;&#25110;&#8220;bins&#8221;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#36807;&#31243;&#65292;&#21487;&#20197;&#30830;&#23450;&#21738;&#20123;&#25968;&#25454;&#29305;&#24449;&#26368;&#36866;&#21512;&#36739;&#23567;&#30340;&#65292;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#26102;&#38388;&#27573;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#20303;&#25151;/&#26080;&#23478;&#21487;&#24402;&#30340;&#34892;&#25919;&#25968;&#25454;&#19978;&#20135;&#29983;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20854;&#20182;&#29305;&#24449;&#20351;&#29992;&#21333;&#20010;&#26102;&#38388;bin&#30340;&#24773;&#20917;&#19979;&#65292;&#20248;&#21270;&#36825;&#20123;&#39640;&#20248;&#20808;&#32423;&#29305;&#24449;&#30340;&#26102;&#38388;bin&#22823;&#23567;&#21487;&#20197;&#23454;&#29616;&#26356;&#31616;&#21333;&#65292;&#26356;&#24555;&#36895;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#36824;&#21487;&#20197;&#23454;&#29616;&#19982;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36825;&#20123;&#27169;&#22411;&#40664;&#35748;&#23558;&#25152;&#26377;&#25968;&#25454;&#29305;&#24449;&#29992;&#30456;&#21516;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Utilizing administrative data to predict outcomes is an important application area of machine learning, particularly in healthcare. Most administrative data records are timestamped and the pattern of records over time is a key input for machine learning models. This paper explores how best to divide the observation window of a machine learning model into time segments or "bins". A computationally efficient process is presented that identifies which data features benefit most from smaller, higher resolution time segments. Results generated on healthcare and housing/homelessness administrative data demonstrate that optimizing the time bin size of these high priority features while using a single time bin for the other features achieves machine learning models that are simpler and quicker to train. This approach also achieves similar and sometimes better performance than more complex models that default to representing all data features with the same time resolution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#20998;&#25968;&#20272;&#35745;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#23545;&#20998;&#25968;&#20272;&#35745;&#36827;&#34892;&#20998;&#26512;&#30340;&#25968;&#23398;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2401.15604</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#20998;&#25968;&#20272;&#35745;&#65306;&#20248;&#21270;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neural Network-Based Score Estimation in Diffusion Models: Optimization and Generalization. (arXiv:2401.15604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#20998;&#25968;&#20272;&#35745;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#23545;&#20998;&#25968;&#20272;&#35745;&#36827;&#34892;&#20998;&#26512;&#30340;&#25968;&#23398;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19982;GANs&#30456;&#23218;&#32654;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25913;&#36827;&#20445;&#30495;&#24230;&#65292;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#39640;&#36136;&#37327;&#26679;&#26412;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#26469;&#23398;&#20064;&#20998;&#25968;&#20989;&#25968;&#12290;&#23613;&#31649;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#23578;&#19981;&#28165;&#26970;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#26159;&#21542;&#21487;&#20197;&#20197;&#21487;&#35777;&#23454;&#30340;&#20934;&#30830;&#24615;&#23398;&#20064;&#20998;&#25968;&#20989;&#25968;&#12290;&#20316;&#20026;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#30340;&#39318;&#35201;&#27493;&#39588;&#65292;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#20998;&#25968;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21253;&#25324;&#23398;&#20064;&#36807;&#31243;&#30340;&#20248;&#21270;&#21644;&#27867;&#21270;&#26041;&#38754;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#24418;&#24335;&#26469;&#23558;&#21435;&#22122;&#20998;&#25968;&#21305;&#37197;&#38382;&#39064;&#21046;&#23450;&#20026;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#22238;&#24402;&#38382;&#39064;&#12290;&#19982;&#26631;&#20934;&#30340;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#30456;&#27604;&#65292;&#20998;&#25968;&#21305;&#37197;&#38382;&#39064;&#24341;&#20837;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#26080;&#30028;&#36755;&#20837;&#65292;&#21521;&#37327;&#20540;&#36755;&#20986;&#21644;&#39069;&#22806;&#30340;&#26102;&#38388;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as a powerful tool rivaling GANs in generating high-quality samples with improved fidelity, flexibility, and robustness. A key component of these models is to learn the score function through score matching. Despite empirical success on various tasks, it remains unclear whether gradient-based algorithms can learn the score function with a provable accuracy. As a first step toward answering this question, this paper establishes a mathematical framework for analyzing score estimation using neural networks trained by gradient descent. Our analysis covers both the optimization and the generalization aspects of the learning procedure. In particular, we propose a parametric form to formulate the denoising score-matching problem as a regression with noisy labels. Compared to the standard supervised learning setup, the score-matching problem introduces distinct challenges, including unbounded input, vector-valued output, and an additional time variable, preventing
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#35745;&#31639;DP-SGD&#32452;&#32423;&#21035;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36825;&#20010;&#26041;&#27861;&#22312;&#20351;&#29992;&#27850;&#26494;&#25277;&#26679;&#25110;&#22266;&#23450;&#25209;&#37327;&#22823;&#23567;&#25277;&#26679;&#26102;&#26159;&#32039;&#23494;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.10294</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#26426;&#21046;&#30340;DP-SGD&#25277;&#26679;&#30340;DP&#32423;&#21035;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Tight Group-Level DP Guarantees for DP-SGD with Sampling via Mixture of Gaussians Mechanisms. (arXiv:2401.10294v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#35745;&#31639;DP-SGD&#32452;&#32423;&#21035;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36825;&#20010;&#26041;&#27861;&#22312;&#20351;&#29992;&#27850;&#26494;&#25277;&#26679;&#25110;&#22266;&#23450;&#25209;&#37327;&#22823;&#23567;&#25277;&#26679;&#26102;&#26159;&#32039;&#23494;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#35745;&#31639;DP-SGD&#30340;&#32452;&#32423;&#21035;$(\epsilon, \delta)$-DP&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#24403;&#20351;&#29992;&#27850;&#26494;&#25277;&#26679;&#25110;&#22266;&#23450;&#25209;&#37327;&#22823;&#23567;&#25277;&#26679;&#26102;&#12290;&#22312;&#23454;&#29616;&#20013;&#65292;&#38500;&#20102;&#31163;&#25955;&#21270;&#38169;&#35823;&#20043;&#22806;&#65292;&#36890;&#36807;&#27492;&#36807;&#31243;&#35745;&#31639;&#30340;DP&#38480;&#21046;&#26159;&#32039;&#23494;&#30340;&#65288;&#20551;&#35774;&#25105;&#20204;&#21457;&#24067;&#20102;&#27599;&#20010;&#20013;&#38388;&#36845;&#20195;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give a procedure for computing group-level $(\epsilon, \delta)$-DP guarantees for DP-SGD, when using Poisson sampling or fixed batch size sampling. Up to discretization errors in the implementation, the DP guarantees computed by this procedure are tight (assuming we release every intermediate iterate).
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#29366;&#24577;&#21644;&#21382;&#21490;&#34920;&#31034;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20102;&#36825;&#20123;&#26041;&#27861;&#21644;&#26694;&#26550;&#23454;&#38469;&#19978;&#37117;&#22522;&#20110;&#33258;&#39044;&#27979;&#25277;&#35937;&#30340;&#20849;&#21516;&#24605;&#24819;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#21644;&#31616;&#21270;&#31639;&#27861;&#26469;&#23398;&#20064;&#33258;&#39044;&#27979;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.08898</link><description>&lt;p&gt;
&#26725;&#25509;&#29366;&#24577;&#21644;&#21382;&#21490;&#34920;&#31034;&#65306;&#29702;&#35299;&#33258;&#39044;&#27979;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bridging State and History Representations: Understanding Self-Predictive RL. (arXiv:2401.08898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#29366;&#24577;&#21644;&#21382;&#21490;&#34920;&#31034;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20102;&#36825;&#20123;&#26041;&#27861;&#21644;&#26694;&#26550;&#23454;&#38469;&#19978;&#37117;&#22522;&#20110;&#33258;&#39044;&#27979;&#25277;&#35937;&#30340;&#20849;&#21516;&#24605;&#24819;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#21644;&#31616;&#21270;&#31639;&#27861;&#26469;&#23398;&#20064;&#33258;&#39044;&#27979;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#26159;&#25152;&#26377;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26680;&#24515;&#65292;&#36866;&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#12290;&#35768;&#22810;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#21644;&#29702;&#35770;&#26694;&#26550;&#34987;&#24320;&#21457;&#29992;&#20110;&#29702;&#35299;&#20160;&#20040;&#26500;&#25104;&#20102;&#26377;&#25928;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#20849;&#21516;&#23646;&#24615;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;&#30475;&#20284;&#19981;&#21516;&#30340;&#29366;&#24577;&#21644;&#21382;&#21490;&#25277;&#35937;&#26041;&#27861;&#21644;&#26694;&#26550;&#23454;&#38469;&#19978;&#22522;&#20110;&#33258;&#39044;&#27979;&#25277;&#35937;&#30340;&#20849;&#21516;&#24605;&#24819;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#24191;&#27867;&#37319;&#29992;&#30340;&#30446;&#26631;&#21644;&#20248;&#21270;&#65288;&#22914;&#20572;&#26799;&#24230;&#25216;&#26415;&#65289;&#22312;&#23398;&#20064;&#33258;&#39044;&#27979;&#34920;&#31034;&#20013;&#30340;&#29702;&#35770;&#27934;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#20849;&#21516;&#20135;&#29983;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29366;&#24577;&#21644;&#21382;&#21490;&#30340;&#33258;&#39044;&#27979;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#24212;&#29992;&#20110;&#26631;&#20934;MDP&#12289;&#24102;&#26377;dist&#30340;MDP&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations are at the core of all deep reinforcement learning (RL) methods for both Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs). Many representation learning methods and theoretical frameworks have been developed to understand what constitutes an effective representation. However, the relationships between these methods and the shared properties among them remain unclear. In this paper, we show that many of these seemingly distinct methods and frameworks for state and history abstractions are, in fact, based on a common idea of self-predictive abstraction. Furthermore, we provide theoretical insights into the widely adopted objectives and optimization, such as the stop-gradient technique, in learning self-predictive representations. These findings together yield a minimalist algorithm to learn self-predictive representations for states and histories. We validate our theories by applying our algorithm to standard MDPs, MDPs with dist
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#36827;&#34892;&#32452;&#32455;&#20266;&#24433;&#20998;&#21106;&#19982;&#20005;&#37325;&#24615;&#20998;&#26512;&#30340;&#33258;&#21160;&#35786;&#26029;&#26041;&#27861;&#12290;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20154;&#24037;&#26234;&#33021;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23545;&#25972;&#20010;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#36827;&#34892;&#33258;&#20027;&#20998;&#26512;&#65292;&#20294;&#21463;&#21040;&#32452;&#32455;&#20266;&#24433;&#24433;&#21709;&#30340;&#21306;&#22495;&#38656;&#35201;&#34987;&#20934;&#30830;&#35782;&#21035;&#21644;&#25490;&#38500;&#12290;</title><link>http://arxiv.org/abs/2401.01386</link><description>&lt;p&gt;
&#20351;&#29992;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#30340;&#32452;&#32455;&#20266;&#24433;&#20998;&#21106;&#19982;&#20005;&#37325;&#24615;&#20998;&#26512;&#30340;&#33258;&#21160;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Tissue Artifact Segmentation and Severity Analysis for Automated Diagnosis Using Whole Slide Images. (arXiv:2401.01386v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01386
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#36827;&#34892;&#32452;&#32455;&#20266;&#24433;&#20998;&#21106;&#19982;&#20005;&#37325;&#24615;&#20998;&#26512;&#30340;&#33258;&#21160;&#35786;&#26029;&#26041;&#27861;&#12290;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20154;&#24037;&#26234;&#33021;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#23545;&#25972;&#20010;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#36827;&#34892;&#33258;&#20027;&#20998;&#26512;&#65292;&#20294;&#21463;&#21040;&#32452;&#32455;&#20266;&#24433;&#24433;&#21709;&#30340;&#21306;&#22495;&#38656;&#35201;&#34987;&#20934;&#30830;&#35782;&#21035;&#21644;&#25490;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#30149;&#29702;&#23398;&#20998;&#26512;&#21644;&#35786;&#26029;&#26159;&#30001;&#19987;&#23478;&#22312;&#26174;&#24494;&#38236;&#19979;&#36890;&#36807;&#35266;&#23519;&#29627;&#29827;&#20999;&#29255;&#26631;&#26412;&#36827;&#34892;&#25163;&#21160;&#30524;&#29699;&#21028;&#26029;&#26469;&#23436;&#25104;&#30340;&#12290;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#26159;&#20174;&#29627;&#29827;&#20999;&#29255;&#21046;&#20316;&#30340;&#25968;&#23383;&#26631;&#26412;&#12290;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#20351;&#24471;&#26631;&#26412;&#33021;&#22815;&#22312;&#35745;&#31639;&#26426;&#23631;&#24149;&#19978;&#35266;&#23519;&#65292;&#24182;&#24341;&#21457;&#20102;&#35745;&#31639;&#30149;&#29702;&#23398;&#65292;&#20854;&#20013;&#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#33258;&#21160;&#20998;&#26512;&#21644;&#35786;&#26029;&#12290;&#20511;&#21161;&#24403;&#21069;&#30340;&#35745;&#31639;&#36827;&#23637;&#65292;&#25972;&#20010;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#21487;&#20197;&#22312;&#27809;&#26377;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#33258;&#20027;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#20840;&#24187;&#28783;&#20999;&#29255;&#22270;&#20687;&#21463;&#21040;&#32452;&#32455;&#20266;&#24433;&#65288;&#22914;&#32452;&#32455;&#35126;&#30385;&#25110;&#27668;&#27873;&#65289;&#30340;&#24433;&#21709;&#65292;&#21017;&#20998;&#26512;&#21487;&#33021;&#20250;&#22833;&#36133;&#25110;&#23548;&#33268;&#38169;&#35823;&#30340;&#35786;&#26029;&#65292;&#36825;&#21462;&#20915;&#20110;&#20266;&#24433;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#29616;&#26377;&#30340;&#20266;&#24433;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#19987;&#23478;&#23545;&#20005;&#37325;&#31243;&#24230;&#30340;&#35780;&#20272;&#65292;&#20197;&#28040;&#38500;&#21463;&#21040;&#20266;&#24433;&#24433;&#21709;&#30340;&#21306;&#22495;&#36827;&#34892;&#20998;&#26512;&#12290;&#36825;&#20010;&#36807;&#31243;&#32791;&#26102;&#12289;&#32321;&#29712;&#65292;&#24182;&#19988;&#26377;&#25439;&#20110;&#33258;&#21160;&#21270;&#20998;&#26512;&#25110;&#20266;&#24433;&#21435;&#38500;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, pathological analysis and diagnosis are performed by manually eyeballing glass slide specimens under a microscope by an expert. The whole slide image is the digital specimen produced from the glass slide. Whole slide image enabled specimens to be observed on a computer screen and led to computational pathology where computer vision and artificial intelligence are utilized for automated analysis and diagnosis. With the current computational advancement, the entire whole slide image can be analyzed autonomously without human supervision. However, the analysis could fail or lead to wrong diagnosis if the whole slide image is affected by tissue artifacts such as tissue fold or air bubbles depending on the severity. Existing artifact detection methods rely on experts for severity assessment to eliminate artifact affected regions from the analysis. This process is time consuming, exhausting and undermines the goal of automated analysis or removal of artifacts without evaluatin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#23454;&#29616;&#23545;&#38544;&#31169;&#25915;&#20987;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24674;&#22797;&#27744;&#21270;&#23618;&#36755;&#20837;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#19979;&#25552;&#20379;&#26356;&#39640;&#30340;&#25991;&#26412;&#24674;&#22797;&#29575;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2312.05720</link><description>&lt;p&gt;
&#36229;&#36234;&#26799;&#24230;&#21644;&#20808;&#39564;&#30693;&#35782;&#22312;&#38544;&#31169;&#25915;&#20987;&#20013;&#65306;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#20013;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning. (arXiv:2312.05720v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#23454;&#29616;&#23545;&#38544;&#31169;&#25915;&#20987;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24674;&#22797;&#27744;&#21270;&#23618;&#36755;&#20837;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#19979;&#25552;&#20379;&#26356;&#39640;&#30340;&#25991;&#26412;&#24674;&#22797;&#29575;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24378;&#35843;&#20998;&#25955;&#24335;&#35757;&#32451;&#65292;&#36890;&#36807;&#26412;&#22320;&#23384;&#20648;&#25968;&#25454;&#24182;&#20165;&#21457;&#36865;&#27169;&#22411;&#26356;&#26032;&#65292;&#24378;&#35843;&#29992;&#25143;&#38544;&#31169;&#12290;&#26368;&#36817;&#65292;&#19968;&#31995;&#21015;&#26377;&#20851;&#38544;&#31169;&#25915;&#20987;&#30340;&#24037;&#20316;&#36890;&#36807;&#20174;&#32852;&#37030;&#23398;&#20064;&#19978;&#19979;&#25991;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#25935;&#24863;&#30340;&#35757;&#32451;&#25991;&#26412;&#26469;&#25439;&#23475;&#29992;&#25143;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25915;&#20987;&#25216;&#26415;&#38754;&#20020;&#30528;&#19981;&#21516;&#30340;&#38556;&#30861;&#65306;&#19968;&#20123;&#24037;&#20316;&#20027;&#35201;&#20351;&#29992;&#26377;&#38480;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#65288;&#20363;&#22914;&#65292;&#25209;&#22788;&#29702;&#22823;&#23567;&#20026;1&#65289;&#65292;&#32780;&#20854;&#20182;&#25216;&#26415;&#21017;&#23481;&#26131;&#34987;&#26816;&#27979;&#20986;&#26469;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#38590;&#20197;&#26816;&#27979;&#30340;&#29305;&#28857;&#65292;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#35774;&#32622;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#25991;&#26412;&#24674;&#22797;&#29575;&#12290;&#22522;&#20110;&#22522;&#26412;&#30340;&#26799;&#24230;&#21305;&#37197;&#21644;&#39046;&#22495;&#20808;&#39564;&#30693;&#35782;&#65292;&#25105;&#20204;&#36890;&#36807;&#24674;&#22797;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#22686;&#24378;&#25915;&#20987;&#33021;&#21147;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#29305;&#24449;&#32423;&#21035;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#19982;&#26799;&#24230;&#25968;&#25454;&#19981;&#21516;&#65292;&#36825;&#20123;&#20449;&#21495;&#19981;&#20250;&#22312;&#21477;&#23376;&#21644;&#26631;&#35760;&#20043;&#38388;&#36827;&#34892;&#24179;&#22343;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) emphasizes decentralized training by storing data locally and sending only model updates, underlining user privacy. Recently, a line of works on privacy attacks impairs user privacy by extracting sensitive training text from language models in the context of FL. Yet, these attack techniques face distinct hurdles: some work chiefly with limited batch sizes (e.g., batch size of 1), and others are easily detectable. This paper introduces an innovative approach that is challenging to detect, significantly enhancing the recovery rate of text in various batch-size settings. Building on fundamental gradient matching and domain prior knowledge, we enhance the attack by recovering the input of the Pooler layer of language models, which enables us to provide additional supervised signals at the feature level. Unlike gradient data, these signals do not average across sentences and tokens, thereby offering more nuanced and effective insights. We benchmark our method using t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#36951;&#25022;&#21040;&#32622;&#20449;&#38598;&#36716;&#25442;&#26041;&#27861;&#25913;&#36827;&#20102;&#36923;&#36753;&#22238;&#24402;&#36172;&#21338;&#26426;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#30340;&#20984;&#32622;&#20449;&#38598;&#65292;&#24182;&#24212;&#29992;&#20110;&#20855;&#26377;&#26032;&#30340;&#38789;&#38598;&#20013;&#27493;&#39588;&#30340;&#36951;&#25022;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.18554</link><description>&lt;p&gt;
&#36890;&#36807;&#36951;&#25022;&#21040;&#32622;&#20449;&#38598;&#36716;&#25442;&#25913;&#36827;&#65288;&#22810;&#39033;&#24335;&#65289;&#36923;&#36753;&#22238;&#24402;&#36172;&#21338;&#26426;&#30340;&#36951;&#25022;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Improved Regret Bounds of (Multinomial) Logistic Bandits via Regret-to-Confidence-Set Conversion. (arXiv:2310.18554v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#36951;&#25022;&#21040;&#32622;&#20449;&#38598;&#36716;&#25442;&#26041;&#27861;&#25913;&#36827;&#20102;&#36923;&#36753;&#22238;&#24402;&#36172;&#21338;&#26426;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#30340;&#20984;&#32622;&#20449;&#38598;&#65292;&#24182;&#24212;&#29992;&#20110;&#20855;&#26377;&#26032;&#30340;&#38789;&#38598;&#20013;&#27493;&#39588;&#30340;&#36951;&#25022;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#22238;&#24402;&#36172;&#21338;&#26426;&#26159;&#24314;&#27169;&#29992;&#25143;&#36873;&#25321;&#30340;&#26222;&#36941;&#26694;&#26550;&#65292;&#20363;&#22914;&#24191;&#21578;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#28857;&#20987;&#19982;&#21542;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20808;&#21069;&#30340;&#24037;&#20316;&#24573;&#35270;&#25110;&#24573;&#30053;&#20102;$S \geq \lVert \theta_\star \rVert_2$&#20013;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20854;&#20013;$\theta_\star \in \mathbb{R}^d$&#26159;&#26410;&#30693;&#30340;&#21442;&#25968;&#21521;&#37327;&#65292;&#24403;$S$&#36739;&#22823;&#26102;&#65292;&#20363;&#22914;$S \geq d$&#65292;&#36825;&#20250;&#20135;&#29983;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;&#8220;&#36951;&#25022;&#21040;&#32622;&#20449;&#38598;&#36716;&#25442;&#65288;R2CS&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#25913;&#21892;&#20102;&#23545;$S$&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#26500;&#24314;&#19968;&#20010;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#23384;&#22312;&#24615;&#30340;&#20984;&#32622;&#20449;&#38598;&#12290;&#20351;&#29992;R2CS&#65292;&#25105;&#20204;&#22312;&#36923;&#36753;&#22238;&#24402;&#36172;&#21338;&#26426;&#30340;&#36951;&#25022;&#30028;&#38480;&#26041;&#38754;&#33719;&#24471;&#20102;&#20005;&#26684;&#30340;&#25913;&#36827;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#35745;&#31639;&#21487;&#34892;&#24615;&#21644;&#23545;&#20854;&#20182;&#22240;&#32032;&#65288;&#22914;$d$&#21644;$T$&#65289;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26032;&#32622;&#20449;&#38598;&#24212;&#29992;&#20110;&#20855;&#26377;&#26032;&#30340;&#38789;&#38598;&#20013;&#27493;&#39588;&#30340;&#36923;&#36753;&#22238;&#24402;&#36172;&#21338;&#26426;&#30340;&#36951;&#25022;&#20998;&#26512;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#39069;&#22806;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logistic bandit is a ubiquitous framework of modeling users' choices, e.g., click vs. no click for advertisement recommender system. We observe that the prior works overlook or neglect dependencies in $S \geq \lVert \theta_\star \rVert_2$, where $\theta_\star \in \mathbb{R}^d$ is the unknown parameter vector, which is particularly problematic when $S$ is large, e.g., $S \geq d$. In this work, we improve the dependency on $S$ via a novel approach called {\it regret-to-confidence set conversion (R2CS)}, which allows us to construct a convex confidence set based on only the \textit{existence} of an online learning algorithm with a regret guarantee. Using R2CS, we obtain a strict improvement in the regret bound w.r.t. $S$ in logistic bandits while retaining computational feasibility and the dependence on other factors such as $d$ and $T$. We apply our new confidence set to the regret analyses of logistic bandits with a new martingale concentration step that circumvents an additional factor
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#19978;&#30340;&#26368;&#20248;&#20256;&#36755;&#26469;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#28040;&#38500;&#20102;&#29616;&#26377;&#25439;&#22833;&#20989;&#25968;&#22312;&#33410;&#28857;&#32423;&#21035;&#39044;&#27979;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11762</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
A Quasi-Wasserstein Loss for Learning Graph Neural Networks. (arXiv:2310.11762v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11762
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#19978;&#30340;&#26368;&#20248;&#20256;&#36755;&#26469;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#28040;&#38500;&#20102;&#29616;&#26377;&#25439;&#22833;&#20989;&#25968;&#22312;&#33410;&#28857;&#32423;&#21035;&#39044;&#27979;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22312;&#33410;&#28857;&#32423;&#21035;&#39044;&#27979;&#20219;&#21153;&#20013;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26102;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25439;&#22833;&#20989;&#25968;&#26159;&#29420;&#31435;&#22320;&#24212;&#29992;&#20110;&#27599;&#20010;&#33410;&#28857;&#30340;&#65292;&#21363;&#20351;&#33410;&#28857;&#23884;&#20837;&#21644;&#23427;&#20204;&#30340;&#26631;&#31614;&#30001;&#20110;&#22270;&#32467;&#26500;&#30340;&#23384;&#22312;&#32780;&#19981;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#12290;&#20026;&#20102;&#28040;&#38500;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#65288;QW&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#20511;&#21161;&#20110;&#22312;&#22270;&#19978;&#23450;&#20041;&#30340;&#26368;&#20248;&#20256;&#36755;&#65292;&#20174;&#32780;&#24341;&#23548;GNN&#30340;&#26032;&#23398;&#20064;&#21644;&#39044;&#27979;&#33539;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#8220;&#20934;&#29926;&#29380;&#26031;&#22374;&#8221;&#36317;&#31163;&#65292;&#29992;&#20110;&#35266;&#27979;&#21040;&#30340;&#22810;&#32500;&#33410;&#28857;&#26631;&#31614;&#21644;&#23427;&#20204;&#30340;&#20272;&#35745;&#20043;&#38388;&#65292;&#36890;&#36807;&#20248;&#21270;&#22312;&#22270;&#36793;&#19978;&#23450;&#20041;&#30340;&#26631;&#31614;&#20256;&#36755;&#12290;&#36825;&#20123;&#20272;&#35745;&#26159;&#30001;&#19968;&#20010;GNN&#21442;&#25968;&#21270;&#30340;&#65292;&#20854;&#20013;&#26368;&#20248;&#26631;&#31614;&#20256;&#36755;&#21487;&#20197;&#36873;&#25321;&#24615;&#22320;&#30830;&#23450;&#22270;&#36793;&#30340;&#26435;&#37325;&#12290;&#36890;&#36807;&#23558;&#26631;&#31614;&#20256;&#36755;&#30340;&#20005;&#26684;&#32422;&#26463;&#37325;&#26032;&#34920;&#36798;&#20026;&#22522;&#20110;Bregman&#25955;&#24230;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#25152;&#25552;&#20986;&#30340;&#20934;&#29926;&#29380;&#26031;&#22374;&#25439;&#22833;&#65292;&#20851;&#32852;&#20004;&#20010;&#39640;&#25928;&#27714;&#35299;&#22120;&#26469;&#23398;&#20064;GNN&#20197;&#21450;&#26368;&#20248;&#26631;&#31614;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;
When learning graph neural networks (GNNs) in node-level prediction tasks, most existing loss functions are applied for each node independently, even if node embeddings and their labels are non-i.i.d. because of their graph structures. To eliminate such inconsistency, in this study we propose a novel Quasi-Wasserstein (QW) loss with the help of the optimal transport defined on graphs, leading to new learning and prediction paradigms of GNNs. In particular, we design a "Quasi-Wasserstein" distance between the observed multi-dimensional node labels and their estimations, optimizing the label transport defined on graph edges. The estimations are parameterized by a GNN in which the optimal label transport may determine the graph edge weights optionally. By reformulating the strict constraint of the label transport to a Bregman divergence-based regularizer, we obtain the proposed Quasi-Wasserstein loss associated with two efficient solvers learning the GNN together with optimal label transp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29983;&#25104;&#29109;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#22312;&#27979;&#24230;&#21040;&#27979;&#24230;&#26144;&#23556;&#20013;&#30340;&#24212;&#29992;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#38750;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#25104;&#26412;&#12289;&#30830;&#23450;&#24615;&#33945;&#26684;&#26144;&#23556;&#12289;&#26144;&#23556;&#36328;&#19981;&#21487;&#27604;&#36739;&#31354;&#38388;&#21644;&#36136;&#37327;&#23432;&#24658;&#32422;&#26463;&#31561;&#23454;&#38469;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.09254</link><description>&lt;p&gt;
&#29983;&#25104;&#29109;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#22312;&#31354;&#38388;&#20869;&#22806;&#26144;&#23556;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative Entropic Neural Optimal Transport To Map Within and Across Spaces. (arXiv:2310.09254v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09254
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29983;&#25104;&#29109;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#22312;&#27979;&#24230;&#21040;&#27979;&#24230;&#26144;&#23556;&#20013;&#30340;&#24212;&#29992;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#38750;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#25104;&#26412;&#12289;&#30830;&#23450;&#24615;&#33945;&#26684;&#26144;&#23556;&#12289;&#26144;&#23556;&#36328;&#19981;&#21487;&#27604;&#36739;&#31354;&#38388;&#21644;&#36136;&#37327;&#23432;&#24658;&#32422;&#26463;&#31561;&#23454;&#38469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#27979;&#24230;&#21040;&#27979;&#24230;&#30340;&#26144;&#23556;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#23588;&#20854;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#21344;&#25454;&#37325;&#35201;&#22320;&#20301;&#12290;&#36817;&#24180;&#26469;&#65292;&#21463;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#21551;&#21457;&#30340;&#25216;&#26415;&#19981;&#26029;&#28044;&#29616;&#12290;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36825;&#20123;&#26041;&#27861;&#32479;&#31216;&#20026;"&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;"&#65292;&#23558;&#26368;&#20248;&#20256;&#36755;&#20316;&#20026;&#24402;&#32435;&#20559;&#22909;&#65306;&#36825;&#20123;&#26144;&#23556;&#24212;&#35813;&#38024;&#23545;&#32473;&#23450;&#30340;&#25104;&#26412;&#20989;&#25968;&#26159;&#26368;&#20248;&#30340;&#65292;&#33021;&#20197;&#33410;&#32422;&#30340;&#26041;&#24335;&#65288;&#36890;&#36807;&#26368;&#23567;&#21270;&#20301;&#31227;&#65289;&#22312;&#31354;&#38388;&#20869;&#25110;&#31354;&#38388;&#38388;&#31227;&#21160;&#28857;&#12290;&#36825;&#19968;&#21407;&#21017;&#22312;&#30452;&#35266;&#19978;&#26159;&#21512;&#29702;&#30340;&#65292;&#20294;&#24448;&#24448;&#38754;&#20020;&#20960;&#20010;&#23454;&#38469;&#25361;&#25112;&#65292;&#38656;&#35201;&#35843;&#25972;&#26368;&#20248;&#20256;&#36755;&#24037;&#20855;&#31665;&#65306;&#22788;&#29702;&#20854;&#20182;&#38750;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#25104;&#26412;&#30340;&#25361;&#25112;&#65292;&#30830;&#23450;&#24615;&#29366;&#20917;&#19979;&#30340;&#33945;&#26684;&#26144;&#23556;&#20844;&#24335;&#20250;&#38480;&#21046;&#28789;&#27963;&#24615;&#65292;&#26144;&#23556;&#22312;&#19981;&#21487;&#27604;&#36739;&#30340;&#31354;&#38388;&#20013;&#20250;&#24102;&#26469;&#22810;&#20010;&#25361;&#25112;&#65292;&#26368;&#20248;&#20256;&#36755;&#22266;&#26377;&#30340;&#36136;&#37327;&#23432;&#24658;&#32422;&#26463;&#21487;&#33021;&#23545;&#24322;&#24120;&#25968;&#25454;&#32473;&#20104;&#36807;&#22810;&#30340;&#37325;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning measure-to-measure mappings is a crucial task in machine learning, featured prominently in generative modeling. Recent years have witnessed a surge of techniques that draw inspiration from optimal transport (OT) theory. Combined with neural network models, these methods collectively known as \textit{Neural OT} use optimal transport as an inductive bias: such mappings should be optimal w.r.t. a given cost function, in the sense that they are able to move points in a thrifty way, within (by minimizing displacements) or across spaces (by being isometric). This principle, while intuitive, is often confronted with several practical challenges that require adapting the OT toolbox: cost functions other than the squared-Euclidean cost can be challenging to handle, the deterministic formulation of Monge maps leaves little flexibility, mapping across incomparable spaces raises multiple challenges, while the mass conservation constraint inherent to OT can provide too much credit to outli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Search-Adaptor&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23450;&#21046;&#21270;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25913;&#21892;&#20449;&#24687;&#26816;&#32034;&#21644;&#25628;&#32034;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20462;&#25913;&#25991;&#26412;&#23884;&#20837;&#65292;Search-Adaptor&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#31283;&#23450;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.08750</link><description>&lt;p&gt;
Search-Adaptor: &#29992;&#20110;&#20449;&#24687;&#26816;&#32034;&#30340;&#25991;&#26412;&#23884;&#20837;&#20010;&#24615;&#21270;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
Search-Adaptor: Text Embedding Customization for Information Retrieval. (arXiv:2310.08750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Search-Adaptor&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23450;&#21046;&#21270;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25913;&#21892;&#20449;&#24687;&#26816;&#32034;&#21644;&#25628;&#32034;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20462;&#25913;&#25991;&#26412;&#23884;&#20837;&#65292;Search-Adaptor&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#31283;&#23450;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#30340;&#25991;&#26412;&#23884;&#20837;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#21892;&#20449;&#24687;&#26816;&#32034;&#21644;&#25628;&#32034;&#30340;&#28508;&#21147;&#12290;&#38500;&#20102;&#19968;&#30452;&#20197;&#26469;&#24120;&#35268;&#20351;&#29992;&#30340;&#38646;&#26679;&#26412;&#35774;&#32622;&#22806;&#65292;&#21033;&#29992;&#30456;&#20851;&#26597;&#35810;-&#35821;&#26009;&#24211;&#37197;&#23545;&#25968;&#25454;&#30340;&#20449;&#24687;&#33021;&#21147;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;Search-Adaptor&#65292;&#20197;&#20415;&#20197;&#39640;&#25928;&#19988;&#31283;&#20581;&#30340;&#26041;&#24335;&#23450;&#21046;&#21270;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20449;&#24687;&#26816;&#32034;&#12290;Search-Adaptor&#21487;&#20197;&#20462;&#25913;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#21407;&#22987;&#25991;&#26412;&#23884;&#20837;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38598;&#25104;&#65292;&#21253;&#25324;&#21482;&#33021;&#36890;&#36807;API&#35775;&#38382;&#30340;&#27169;&#22411;&#12290;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#33521;&#25991;&#21644;&#22810;&#35821;&#35328;&#26816;&#32034;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Search-Adaptor&#30340;&#19968;&#33268;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;--&#20363;&#22914;&#65292;&#22312;13&#20010;BEIR&#25968;&#25454;&#38598;&#19978;&#65292;nDCG@10&#30456;&#23545;&#20110;Google Embedding APIs&#24179;&#22343;&#25552;&#39640;&#20102;5.2%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text embeddings extracted by pre-trained Large Language Models (LLMs) have significant potential to improve information retrieval and search. Beyond the zero-shot setup in which they are being conventionally used, being able to take advantage of the information from the relevant query-corpus paired data has the power to further boost the LLM capabilities. In this paper, we propose a novel method, Search-Adaptor, for customizing LLMs for information retrieval in an efficient and robust way. Search-Adaptor modifies the original text embedding generated by pre-trained LLMs, and can be integrated with any LLM, including those only available via APIs. On multiple real-world English and multilingual retrieval datasets, we show consistent and significant performance benefits for Search-Adaptor -- e.g., more than 5.2% improvements over the Google Embedding APIs in nDCG@10 averaged over 13 BEIR datasets.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07793</link><description>&lt;p&gt;
GenTKG: &#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07793
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;(tKG)&#39046;&#22495;&#30340;&#20852;&#36259;&#65292;&#20854;&#20013;&#20256;&#32479;&#30340;&#22522;&#20110;&#23884;&#20837;&#21644;&#35268;&#21017;&#30340;&#27169;&#22411;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#39044;&#35757;&#32451;&#30340;LLM&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#32467;&#26500;&#21270;&#30340;&#26102;&#38388;&#20851;&#31995;&#25968;&#25454;&#65292;&#24182;&#21462;&#20195;&#23427;&#20204;&#25104;&#20026;&#26102;&#38388;&#20851;&#31995;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#30693;&#35782;&#39044;&#27979;&#24341;&#20837;&#29983;&#25104;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;LLM&#21487;&#20197;&#22788;&#29702;&#30340;&#24207;&#21015;&#33258;&#28982;&#34920;&#36798;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#40511;&#27807;&#65292;&#22312;tKG&#30340;&#24222;&#22823;&#25968;&#25454;&#37327;&#21644;&#24494;&#35843;LLM&#30340;&#24040;&#22823;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#20063;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;GenTKG&#65292;&#23427;&#22312;tKG&#19978;&#25191;&#34892;&#29983;&#25104;&#24335;&#39044;&#27979;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;GenTKG&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional carefully designed embedding-based and rule-based models dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval augmented generation framework that performs generative forecasting on tKGs named GenTKG, which combines a temporal logical rule-based retrieval strategy and lightweight parameter-efficient instruction tuning. Extensive experiments hav
&lt;/p&gt;</description></item><item><title>CacheGen&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#36827;&#34892;&#21387;&#32553;&#26469;&#20943;&#23569;LLM&#30340;&#32593;&#32476;&#33719;&#21462;&#21644;&#22788;&#29702;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2310.07240</link><description>&lt;p&gt;
CacheGen&#65306;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#30340;&#24555;&#36895;&#19978;&#19979;&#25991;&#21152;&#36733;
&lt;/p&gt;
&lt;p&gt;
CacheGen: Fast Context Loading for Language Model Applications. (arXiv:2310.07240v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07240
&lt;/p&gt;
&lt;p&gt;
CacheGen&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#19978;&#19979;&#25991;&#36827;&#34892;&#21387;&#32553;&#26469;&#20943;&#23569;LLM&#30340;&#32593;&#32476;&#33719;&#21462;&#21644;&#22788;&#29702;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25215;&#25285;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#20854;&#36755;&#20837;&#23558;&#25972;&#21512;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#20197;&#24212;&#23545;&#38656;&#35201;&#39046;&#22495;&#30693;&#35782;&#25110;&#29992;&#25143;&#29305;&#23450;&#30340;&#23545;&#35805;&#21382;&#21490;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#23545;&#20110;&#21709;&#24212;&#24335;&#30340;LLM&#31995;&#32479;&#26469;&#35828;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#22312;&#25152;&#26377;&#19978;&#19979;&#25991;&#34987;&#33719;&#21462;&#21644;LLM&#22788;&#29702;&#20043;&#21069;&#65292;&#26080;&#27861;&#29983;&#25104;&#20219;&#20309;&#20869;&#23481;&#12290;&#29616;&#26377;&#31995;&#32479;&#20165;&#36890;&#36807;&#20248;&#21270;&#19978;&#19979;&#25991;&#22788;&#29702;&#30340;&#35745;&#31639;&#24310;&#36831;&#65288;&#20363;&#22914;&#65292;&#36890;&#36807;&#32531;&#23384;&#25991;&#26412;&#19978;&#19979;&#25991;&#30340;&#20013;&#38388;&#38190;&#20540;&#29305;&#24449;&#65289;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#20294;&#24448;&#24448;&#20250;&#23548;&#33268;&#19978;&#19979;&#25991;&#33719;&#21462;&#30340;&#32593;&#32476;&#24310;&#36831;&#26356;&#38271;&#65288;&#20363;&#22914;&#65292;&#38190;&#20540;&#29305;&#24449;&#28040;&#32791;&#30340;&#24102;&#23485;&#27604;&#25991;&#26412;&#19978;&#19979;&#25991;&#22823;&#20960;&#20010;&#25968;&#37327;&#32423;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CacheGen&#65292;&#20197;&#26368;&#23567;&#21270;LLM&#19978;&#19979;&#25991;&#33719;&#21462;&#21644;&#22788;&#29702;&#30340;&#24310;&#36831;&#12290;CacheGen&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#30340;&#38190;&#20540;&#65288;KV&#65289;&#29305;&#24449;&#21387;&#32553;&#20026;&#26356;&#32039;&#20945;&#30340;&#27604;&#29305;&#27969;&#34920;&#31034;&#65292;&#20943;&#23569;&#20102;&#20256;&#36755;&#25152;&#38656;&#30340;&#24102;&#23485;&#12290;&#32534;&#30721;&#22120;&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;&#37327;&#21270;&#21644;......
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) take on more complex tasks, their inputs incorporate longer contexts to respond to questions that require domain knowledge or user-specific conversational histories. Yet, using long contexts poses a challenge for responsive LLM systems, as nothing can be generated until all the contexts are fetched to and processed by the LLM. Existing systems optimize only the computation delay in context processing (e.g., by caching intermediate key-value features of the text context) but often cause longer network delays in context fetching (e.g., key-value features consume orders of magnitude larger bandwidth than the text context).  This paper presents CacheGen to minimize the delays in fetching and processing contexts for LLMs. CacheGen reduces the bandwidth needed for transmitting long contexts' key-value (KV) features through a novel encoder that compresses KV features into more compact bitstream representations. The encoder combines adaptive quantization with a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#22810;&#32423;&#36724;&#21521;&#21387;&#32553;&#26426;&#22312;&#29123;&#27668;&#36718;&#26426;&#20013;&#23574;&#38388;&#38553;&#21464;&#21270;&#23545;&#27668;&#21160;&#24615;&#33021;&#24433;&#21709;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#19982;CFD&#22522;&#20934;&#30456;&#23218;&#32654;&#30340;&#23454;&#26102;&#20934;&#30830;&#24615;&#65292;&#26041;&#20415;&#38598;&#25104;&#21040;&#29123;&#27668;&#36718;&#26426;&#30340;&#21046;&#36896;&#21644;&#26500;&#24314;&#36807;&#31243;&#20013;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.04264</link><description>&lt;p&gt;
C(NN)FD -- &#22810;&#32423;&#36724;&#21521;&#21387;&#32553;&#26426;&#27668;&#21160;&#24615;&#33021;&#20013;&#23574;&#38388;&#38553;&#21464;&#21270;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
C(NN)FD -- deep learning predictions of tip clearance variations on multi-stage axial compressors aerodynamic performance. (arXiv:2310.04264v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#22810;&#32423;&#36724;&#21521;&#21387;&#32553;&#26426;&#22312;&#29123;&#27668;&#36718;&#26426;&#20013;&#23574;&#38388;&#38553;&#21464;&#21270;&#23545;&#27668;&#21160;&#24615;&#33021;&#24433;&#21709;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#19982;CFD&#22522;&#20934;&#30456;&#23218;&#32654;&#30340;&#23454;&#26102;&#20934;&#30830;&#24615;&#65292;&#26041;&#20415;&#38598;&#25104;&#21040;&#29123;&#27668;&#36718;&#26426;&#30340;&#21046;&#36896;&#21644;&#26500;&#24314;&#36807;&#31243;&#20013;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36804;&#20170;&#20026;&#27490;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#35832;&#22914;CFD&#65288;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#65289;&#31561;&#29289;&#29702;&#27169;&#25311;&#22312;&#24037;&#19994;&#19978;&#30340;&#37325;&#35201;&#24615;&#26377;&#38480;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#32423;&#36724;&#21521;&#21387;&#32553;&#26426;&#22312;&#29123;&#27668;&#36718;&#26426;&#20013;&#23574;&#38388;&#38553;&#21464;&#21270;&#23545;&#27668;&#21160;&#24615;&#33021;&#30340;&#23454;&#26102;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#24320;&#21457;&#21644;&#24212;&#29992;&#12290;&#25152;&#25552;&#20986;&#30340;C(NN)FD&#26550;&#26500;&#32463;&#35777;&#26126;&#21487;&#25193;&#23637;&#33267;&#24037;&#19994;&#24212;&#29992;&#65292;&#24182;&#36798;&#21040;&#19982;CFD&#22522;&#20934;&#30456;&#23218;&#32654;&#30340;&#23454;&#26102;&#20934;&#30830;&#24615;&#12290;&#37096;&#32626;&#30340;&#27169;&#22411;&#21487;&#36731;&#26494;&#38598;&#25104;&#21040;&#29123;&#27668;&#36718;&#26426;&#30340;&#21046;&#36896;&#21644;&#26500;&#24314;&#36807;&#31243;&#20013;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20998;&#26512;&#35780;&#20272;&#24615;&#33021;&#24433;&#21709;&#24182;&#28508;&#22312;&#20943;&#23569;&#26114;&#36149;&#29289;&#29702;&#27979;&#35797;&#35201;&#27714;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Application of deep learning methods to physical simulations such as CFD (Computational Fluid Dynamics), have been so far of limited industrial relevance. This paper demonstrates the development and application of a deep learning framework for real-time predictions of the impact of tip clearance variations on the aerodynamic performance of multi-stage axial compressors in gas turbines. The proposed C(NN)FD architecture is proven to be scalable to industrial applications, and achieves in real-time accuracy comparable to the CFD benchmark. The deployed model, is readily integrated within the manufacturing and build process of gas turbines, thus providing the opportunity to analytically assess the impact on performance and potentially reduce requirements for expensive physical tests.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CoLiDE&#31639;&#27861;&#29992;&#20110;&#23398;&#20064;&#32447;&#24615;DAG&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#20984;&#35780;&#20998;&#20989;&#25968;&#65292;&#32467;&#21512;&#20102;&#26631;&#24230;&#30340;&#20849;&#21516;&#20272;&#35745;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#23558;&#31232;&#30095;&#21442;&#25968;&#19982;&#22806;&#29983;&#22122;&#22768;&#27700;&#24179;&#20998;&#31163;&#12290;</title><link>http://arxiv.org/abs/2310.02895</link><description>&lt;p&gt;
CoLiDE: &#20849;&#21516;&#32447;&#24615;&#26377;&#21521;&#26080;&#29615;&#22270;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
CoLiDE: Concomitant Linear DAG Estimation. (arXiv:2310.02895v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;CoLiDE&#31639;&#27861;&#29992;&#20110;&#23398;&#20064;&#32447;&#24615;DAG&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#20984;&#35780;&#20998;&#20989;&#25968;&#65292;&#32467;&#21512;&#20102;&#26631;&#24230;&#30340;&#20849;&#21516;&#20272;&#35745;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#23558;&#31232;&#30095;&#21442;&#25968;&#19982;&#22806;&#29983;&#22122;&#22768;&#27700;&#24179;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22788;&#29702;&#20174;&#36981;&#24490;&#32447;&#24615;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411; (SEM) &#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#21521;&#26080;&#29615;&#22270; (DAG) &#32467;&#26500;&#30340;&#32452;&#21512;&#38382;&#39064;&#12290;&#21033;&#29992;&#19981;&#21487;&#24494;&#20998;&#12289;&#38750;&#20984;&#30340;&#26377;&#25928;&#24615;&#29305;&#24449;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#21463;&#38480;&#20248;&#21270;&#33539;&#24335;&#65292;&#20197;&#20415;&#39640;&#25928;&#22320;&#25506;&#32034;DAG&#31354;&#38388;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#22871;&#32034;&#31867;&#22411;&#30340;&#35780;&#20998;&#20989;&#25968;&#26469;&#24341;&#23548;&#36825;&#20010;&#25628;&#32034;&#36807;&#31243;&#65292;&#36825;&#20123;&#20989;&#25968;&#22312;$\textit{&#26410;&#30693;}$SEM&#22122;&#22768;&#26041;&#24046;&#22312;&#38382;&#39064;&#23454;&#20363;&#20043;&#38388;&#21457;&#29983;&#21464;&#21270;&#26102;&#38656;&#36827;&#34892;&#26114;&#36149;&#30340;&#24809;&#32602;&#21442;&#25968;&#37325;&#26032;&#35843;&#25972;&#65292;&#24182;&#19988;&#38544;&#21547;&#22320;&#20381;&#36182;&#20110;&#26377;&#30028;&#21516;&#26041;&#24046;&#20551;&#35774;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20984;&#35780;&#20998;&#20989;&#25968;&#65292;&#29992;&#20110;&#31232;&#30095;&#24863;&#30693;&#32447;&#24615;DAG&#30340;&#23398;&#20064;&#65292;&#35813;&#20989;&#25968;&#32467;&#21512;&#20102;&#26631;&#24230;&#30340;&#20849;&#21516;&#20272;&#35745;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#23558;&#31232;&#30095;&#21442;&#25968;&#19982;&#22806;&#29983;&#22122;&#22768;&#27700;&#24179;&#20998;&#31163;&#12290;&#36890;&#36807;&#24179;&#28369;&#30340;&#12289;&#38750;&#20984;&#30340;&#26080;&#29615;&#24809;&#32602;&#39033;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#21487;&#20197;&#24471;&#21040;CoLiDE &#65288;&#20849;&#21516;&#32447;&#24615;DAG&#20272;&#35745;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We deal with the combinatorial problem of learning directed acyclic graph (DAG) structure from observational data adhering to a linear structural equation model (SEM). Leveraging advances in differentiable, nonconvex characterizations of acyclicity, recent efforts have advocated a continuous constrained optimization paradigm to efficiently explore the space of DAGs. Most existing methods employ lasso-type score functions to guide this search, which (i) require expensive penalty parameter retuning when the $\textit{unknown}$ SEM noise variances change across problem instances; and (ii) implicitly rely on limiting homoscedasticity assumptions. In this work, we propose a new convex score function for sparsity-aware learning of linear DAGs, which incorporates concomitant estimation of scale and thus effectively decouples the sparsity parameter from the exogenous noise levels. Regularization via a smooth, nonconvex acyclicity penalty term yields CoLiDE ($\textbf{Co}$ncomitant $\textbf{Li}$n
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTM&#65289;&#65292;&#23427;&#21487;&#20197;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#65292;&#21516;&#26102;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.02279</link><description>&lt;p&gt;
&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65306;&#23398;&#20064;&#25193;&#25955;&#30340;&#27010;&#29575;&#27969;ODE&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion. (arXiv:2310.02279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02279
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTM&#65289;&#65292;&#23427;&#21487;&#20197;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#65292;&#21516;&#26102;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CM&#65289;&#21152;&#36895;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#65292;&#20294;&#20197;&#29306;&#29298;&#26679;&#26412;&#36136;&#37327;&#20026;&#20195;&#20215;&#65292;&#32570;&#20047;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26469;&#26435;&#34913;&#36895;&#24230;&#21644;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTM&#65289;&#65292;&#23427;&#26159;&#21253;&#25324;CM&#21644;&#22522;&#20110;&#24471;&#20998;&#27169;&#22411;&#22312;&#20869;&#30340;&#27867;&#21270;&#27169;&#22411;&#12290;CTM&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#36755;&#20986;&#24471;&#20998;&#65288;&#21363;&#23545;&#25968;&#23494;&#24230;&#30340;&#26799;&#24230;&#65289;&#65292;&#24182;&#20801;&#35768;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#20219;&#24847;&#21021;&#22987;&#21644;&#26368;&#32456;&#26102;&#38388;&#20043;&#38388;&#36827;&#34892;&#19981;&#21463;&#38480;&#21046;&#30340;&#36941;&#21382;&#27010;&#29575;&#27969;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;CTM&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#21644;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#26377;&#25928;&#32452;&#21512;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;CIFAR-10&#65288;FID 1.73&#65289;&#21644;64X64&#20998;&#36776;&#29575;&#30340;ImageNet&#19978;&#23454;&#29616;&#26032;&#30340;&#26368;&#20808;&#36827;FID&#12290;CTM&#36824;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#37319;&#26679;&#26041;&#26696;&#65292;&#21253;&#25324;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#30340;ODE&#35299;&#20013;&#30340;&#38271;&#36339;&#36291;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64X64 resolution (FID 2.06). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE soluti
&lt;/p&gt;</description></item><item><title>SmartPlay&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6&#20010;&#20855;&#26377;&#19981;&#21516;&#25361;&#25112;&#30340;&#28216;&#25103;&#65292;&#24182;&#27979;&#35797;&#20102;&#26234;&#33021;LLM Agent&#30340;&#22810;&#31181;&#20851;&#38190;&#33021;&#21147;&#12290;&#36825;&#19981;&#20165;&#26159;&#19968;&#20010;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#36824;&#21487;&#20197;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.01557</link><description>&lt;p&gt;
SmartPlay: &#19968;&#31181;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SmartPlay : A Benchmark for LLMs as Intelligent Agents. (arXiv:2310.01557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01557
&lt;/p&gt;
&lt;p&gt;
SmartPlay&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6&#20010;&#20855;&#26377;&#19981;&#21516;&#25361;&#25112;&#30340;&#28216;&#25103;&#65292;&#24182;&#27979;&#35797;&#20102;&#26234;&#33021;LLM Agent&#30340;&#22810;&#31181;&#20851;&#38190;&#33021;&#21147;&#12290;&#36825;&#19981;&#20165;&#26159;&#19968;&#20010;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#36824;&#21487;&#20197;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26234;&#33021;Agent&#21644;&#19979;&#19968;&#20195;&#33258;&#21160;&#21270;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;LLMs&#20316;&#20026;Agent&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SmartPlay&#65306;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;LLMs&#20316;&#20026;Agent&#30340;&#26041;&#27861;&#35770;&#12290;SmartPlay&#21253;&#25324;6&#20010;&#19981;&#21516;&#30340;&#28216;&#25103;&#65292;&#21253;&#25324;&#21098;&#20992;&#30707;&#22836;&#24067;&#12289;&#27721;&#35834;&#22612;&#12289;Minecraft&#31561;&#12290;&#27599;&#20010;&#28216;&#25103;&#37117;&#20855;&#26377;&#29420;&#29305;&#30340;&#35774;&#32622;&#65292;&#25552;&#20379;&#26368;&#22810;20&#20010;&#35780;&#20272;&#35774;&#32622;&#21644;&#26080;&#38480;&#30340;&#29615;&#22659;&#21464;&#21270;&#12290;SmartPlay&#20013;&#30340;&#27599;&#20010;&#28216;&#25103;&#37117;&#29420;&#29305;&#22320;&#25361;&#25112;&#20102;&#26234;&#33021;LLM Agent&#30340;9&#20010;&#37325;&#35201;&#33021;&#21147;&#30340;&#23376;&#38598;&#65292;&#21253;&#25324;&#23545;&#23545;&#35937;&#20381;&#36182;&#30340;&#25512;&#29702;&#12289;&#25552;&#21069;&#35268;&#21010;&#12289;&#31354;&#38388;&#25512;&#29702;&#12289;&#20174;&#21382;&#21490;&#20013;&#23398;&#20064;&#21644;&#29702;&#35299;&#38543;&#26426;&#24615;&#12290;&#27599;&#20010;&#28216;&#25103;&#27979;&#35797;&#30340;&#33021;&#21147;&#38598;&#30340;&#21306;&#21035;&#20351;&#25105;&#20204;&#33021;&#22815;&#21333;&#29420;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#12290;SmartPlay&#19981;&#20165;&#26159;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#32780;&#19988;&#20063;&#26159;&#35780;&#20272;Agent&#22312;&#19981;&#21516;&#33021;&#21147;&#26041;&#38754;&#30340;&#24615;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36866;&#29992;&#20110;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#65292;&#21487;&#20197;&#21253;&#25324;&#20855;&#26377;&#20559;&#24046;&#12289;&#36339;&#36291;&#36830;&#25509;&#21644;&#26368;&#22823;&#27744;&#21270;&#30340;&#36890;&#29992;DAG ReLU&#32593;&#32476;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#24674;&#22797;&#25110;&#36229;&#36234;&#20102;&#24050;&#30693;&#30340;&#36335;&#24452;&#33539;&#25968;&#30028;&#38480;&#65292;&#24182;&#25361;&#25112;&#20102;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#19968;&#20123;&#20855;&#20307;&#25215;&#35834;&#12290;</title><link>http://arxiv.org/abs/2310.01225</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#20195;&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#65306;&#24433;&#21709;&#12289;&#21069;&#26223;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A path-norm toolkit for modern networks: consequences, promises and challenges. (arXiv:2310.01225v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36866;&#29992;&#20110;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#65292;&#21487;&#20197;&#21253;&#25324;&#20855;&#26377;&#20559;&#24046;&#12289;&#36339;&#36291;&#36830;&#25509;&#21644;&#26368;&#22823;&#27744;&#21270;&#30340;&#36890;&#29992;DAG ReLU&#32593;&#32476;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#24674;&#22797;&#25110;&#36229;&#36234;&#20102;&#24050;&#30693;&#30340;&#36335;&#24452;&#33539;&#25968;&#30028;&#38480;&#65292;&#24182;&#25361;&#25112;&#20102;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#19968;&#20123;&#20855;&#20307;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#23436;&#20840;&#33021;&#22815;&#21253;&#25324;&#20855;&#26377;&#20559;&#24046;&#12289;&#36339;&#36291;&#36830;&#25509;&#21644;&#26368;&#22823;&#27744;&#21270;&#30340;&#36890;&#29992;DAG ReLU&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#19981;&#20165;&#36866;&#29992;&#20110;&#26368;&#24191;&#27867;&#30340;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#65292;&#36824;&#21487;&#20197;&#24674;&#22797;&#25110;&#36229;&#36234;&#24050;&#30693;&#30340;&#27492;&#31867;&#33539;&#25968;&#30340;&#26368;&#23574;&#38160;&#30028;&#38480;&#12290;&#36825;&#20123;&#25193;&#23637;&#30340;&#36335;&#24452;&#33539;&#25968;&#36824;&#20139;&#26377;&#36335;&#24452;&#33539;&#25968;&#30340;&#24120;&#35268;&#20248;&#28857;&#65306;&#35745;&#31639;&#31616;&#20415;&#12289;&#23545;&#32593;&#32476;&#30340;&#23545;&#31216;&#24615;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#22312;&#21069;&#39304;&#32593;&#32476;&#19978;&#27604;&#25805;&#20316;&#31526;&#33539;&#25968;&#30340;&#20056;&#31215;&#65288;&#21478;&#19968;&#31181;&#24120;&#29992;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65289;&#20855;&#26377;&#26356;&#22909;&#30340;&#38160;&#24230;&#12290;&#24037;&#20855;&#21253;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#26131;&#20110;&#23454;&#26045;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#25968;&#20540;&#35780;&#20272;&#22312;ImageNet&#19978;&#23545;ResNet&#30340;&#26368;&#23574;&#38160;&#30028;&#38480;&#26469;&#25361;&#25112;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#20855;&#20307;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces the first toolkit around path-norms that is fully able to encompass general DAG ReLU networks with biases, skip connections and max pooling. This toolkit notably allows us to establish generalization bounds for real modern neural networks that are not only the most widely applicable path-norm based ones, but also recover or beat the sharpest known bounds of this type. These extended path-norms further enjoy the usual benefits of path-norms: ease of computation, invariance under the symmetries of the network, and improved sharpness on feedforward networks compared to the product of operators' norms, another complexity measure most commonly used.  The versatility of the toolkit and its ease of implementation allow us to challenge the concrete promises of path-norm-based generalization bounds, by numerically evaluating the sharpest known bounds for ResNets on ImageNet.
&lt;/p&gt;</description></item><item><title>DataInf&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#24433;&#21709;&#21147;&#36817;&#20284;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#19978;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.00902</link><description>&lt;p&gt;
DataInf&#65306;&#22312;LLMs&#21644;&#25193;&#25955;&#27169;&#22411;&#20013;&#39640;&#25928;&#20272;&#35745;&#25968;&#25454;&#24433;&#21709;&#21147;
&lt;/p&gt;
&lt;p&gt;
DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models. (arXiv:2310.00902v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00902
&lt;/p&gt;
&lt;p&gt;
DataInf&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#24433;&#21709;&#21147;&#36817;&#20284;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#19978;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#24433;&#21709;&#21147;&#23545;&#20110;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#21644;&#25552;&#39640;AI&#31649;&#36947;&#30340;&#36879;&#26126;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#24433;&#21709;&#20989;&#25968;&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#21644;&#27969;&#34892;&#30340;&#25968;&#25454;&#24402;&#23646;&#26041;&#27861;&#65292;&#20294;&#20854;&#35745;&#31639;&#25104;&#26412;&#20351;&#20854;&#38590;&#20197;&#20351;&#29992;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35774;&#32622;&#20013;&#26356;&#21152;&#31361;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DataInf&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#24433;&#21709;&#21147;&#36817;&#20284;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#12290;&#36890;&#36807;&#21033;&#29992;&#26131;&#20110;&#35745;&#31639;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;DataInf&#22312;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#24433;&#21709;&#35745;&#31639;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;DataInf&#29305;&#21035;&#36866;&#29992;&#20110;&#35832;&#22914;LoRA&#30340;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#25216;&#26415;&#12290;&#36890;&#36807;&#31995;&#32479;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DataInf&#33021;&#22815;&#20934;&#30830;&#22320;&#36817;&#20284;&#24433;&#21709;&#20998;&#25968;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#26041;&#27861;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantifying the impact of training data points is crucial for understanding the outputs of machine learning models and for improving the transparency of the AI pipeline. The influence function is a principled and popular data attribution method, but its computational cost often makes it challenging to use. This issue becomes more pronounced in the setting of large language models and text-to-image models. In this work, we propose DataInf, an efficient influence approximation method that is practical for large-scale generative AI models. Leveraging an easy-to-compute closed-form expression, DataInf outperforms existing influence computation algorithms in terms of computational and memory efficiency. Our theoretical analysis shows that DataInf is particularly well-suited for parameter-efficient fine-tuning techniques such as LoRA. Through systematic empirical evaluations, we show that DataInf accurately approximates influence scores and is orders of magnitude faster than existing methods
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36793;&#30028;&#26368;&#22823;&#21270;&#21644;&#25913;&#36827;&#30340;Lipschitz&#27491;&#21017;&#21270;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#36755;&#20986;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#21644;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;Lipschitz&#24120;&#25968;&#26469;&#25552;&#39640;&#28145;&#24230;&#20998;&#31867;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00116</link><description>&lt;p&gt;
&#21160;&#24577;&#36793;&#30028;&#26368;&#22823;&#21270;&#21644;&#25913;&#36827;&#30340;Lipschitz&#27491;&#21017;&#21270;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certified Robustness via Dynamic Margin Maximization and Improved Lipschitz Regularization. (arXiv:2310.00116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36793;&#30028;&#26368;&#22823;&#21270;&#21644;&#25913;&#36827;&#30340;Lipschitz&#27491;&#21017;&#21270;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#36755;&#20986;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#21644;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;Lipschitz&#24120;&#25968;&#26469;&#25552;&#39640;&#28145;&#24230;&#20998;&#31867;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#28145;&#24230;&#20998;&#31867;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20363;&#22914;&#35774;&#35745;&#20855;&#26377;&#26356;&#22909;&#40065;&#26834;&#24615;&#24615;&#36136;&#30340;&#26032;&#26550;&#26500;&#65288;&#20363;&#22914;&#65292;Lipschitz-capped&#32593;&#32476;&#65289;&#25110;&#20462;&#25913;&#35757;&#32451;&#36807;&#31243;&#26412;&#36523;&#65288;&#20363;&#22914;&#65292;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#65292;&#32422;&#26463;&#23398;&#20064;&#25110;&#27491;&#21017;&#21270;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#22686;&#21152;&#36755;&#20837;&#65288;&#29305;&#24449;&#65289;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#21487;&#33021;&#24182;&#19981;&#26377;&#25928;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#23545;&#24320;&#21457;&#33021;&#22815;&#30452;&#25509;&#25805;&#32437;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#20915;&#31574;&#36793;&#30028;&#30340;&#35757;&#32451;&#36807;&#31243;&#24863;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#35813;&#31867;&#21035;&#30340;&#26368;&#26032;&#21457;&#23637;&#22522;&#30784;&#19978;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#40065;&#26834;&#35757;&#32451;&#31639;&#27861;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#36755;&#20986;&#65288;logit&#65289;&#31354;&#38388;&#20013;&#22686;&#21152;&#36793;&#30028;&#65292;&#24182;&#27839;&#30528;&#33030;&#24369;&#26041;&#21521;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;Lipschitz&#24120;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20004;&#20010;&#30446;&#26631;&#21487;&#20197;&#30452;&#25509;&#20419;&#36827;&#36755;&#20837;&#31354;&#38388;&#20013;&#26356;&#22823;&#30340;&#36793;&#30028;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;...
&lt;/p&gt;
&lt;p&gt;
To improve the robustness of deep classifiers against adversarial perturbations, many approaches have been proposed, such as designing new architectures with better robustness properties (e.g., Lipschitz-capped networks), or modifying the training process itself (e.g., min-max optimization, constrained learning, or regularization). These approaches, however, might not be effective at increasing the margin in the input (feature) space. As a result, there has been an increasing interest in developing training procedures that can directly manipulate the decision boundary in the input space. In this paper, we build upon recent developments in this category by developing a robust training algorithm whose objective is to increase the margin in the output (logit) space while regularizing the Lipschitz constant of the model along vulnerable directions. We show that these two objectives can directly promote larger margins in the input space. To this end, we develop a scalable method for calcula
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Snowball&#65292;&#19968;&#20010;&#36890;&#36807;&#20010;&#20307;&#35270;&#35282;&#19978;&#30340;&#21452;&#21521;&#36873;&#20030;&#26041;&#27861;&#26469;&#25269;&#25239;&#32852;&#37030;&#23398;&#20064;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#26694;&#26550;&#12290;&#23427;&#36890;&#36807;&#33258;&#19979;&#32780;&#19978;&#21644;&#33258;&#19978;&#32780;&#19979;&#30340;&#36873;&#20030;&#36807;&#31243;&#65292;&#36880;&#27493;&#25490;&#38500;&#24863;&#26579;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#22810;&#26679;&#24615;&#23548;&#33268;&#27169;&#22411;&#26356;&#26032;&#28151;&#26434;&#20998;&#25955;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16456</link><description>&lt;p&gt;
&#25269;&#25239;&#32852;&#37030;&#23398;&#20064;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#21452;&#21521;&#36873;&#20030;&#21644;&#20010;&#20307;&#35270;&#35282;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Resisting Backdoor Attacks in Federated Learning via Bidirectional Elections and Individual Perspective. (arXiv:2309.16456v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Snowball&#65292;&#19968;&#20010;&#36890;&#36807;&#20010;&#20307;&#35270;&#35282;&#19978;&#30340;&#21452;&#21521;&#36873;&#20030;&#26041;&#27861;&#26469;&#25269;&#25239;&#32852;&#37030;&#23398;&#20064;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#26694;&#26550;&#12290;&#23427;&#36890;&#36807;&#33258;&#19979;&#32780;&#19978;&#21644;&#33258;&#19978;&#32780;&#19979;&#30340;&#36873;&#20030;&#36807;&#31243;&#65292;&#36880;&#27493;&#25490;&#38500;&#24863;&#26579;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#22810;&#26679;&#24615;&#23548;&#33268;&#27169;&#22411;&#26356;&#26032;&#28151;&#26434;&#20998;&#25955;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#25269;&#24481;&#32852;&#37030;&#23398;&#20064;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#20027;&#35201;&#36890;&#36807;&#20943;&#36731;&#24863;&#26579;&#27169;&#22411;&#30340;&#24433;&#21709;&#25110;&#25490;&#38500;&#24863;&#26579;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;&#21069;&#32773;&#20250;&#23545;&#27169;&#22411;&#20934;&#30830;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#32780;&#21518;&#32773;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#33391;&#24615;&#21644;&#24863;&#26579;&#27169;&#22411;&#26356;&#26032;&#20043;&#38388;&#30340;&#20840;&#23616;&#28165;&#26224;&#36793;&#30028;&#30340;&#21028;&#23450;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#22810;&#26679;&#24615;&#65292;&#27169;&#22411;&#26356;&#26032;&#22312;&#29616;&#23454;&#20013;&#23481;&#26131;&#28151;&#26434;&#24182;&#20998;&#25955;&#12290;&#26412;&#25991;&#20851;&#27880;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#25490;&#38500;&#24863;&#26579;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#19982;&#20197;&#24448;&#20174;&#20840;&#23616;&#35270;&#35282;&#20986;&#21457;&#30340;&#35266;&#28857;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Snowball&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#21518;&#38376;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20010;&#20307;&#35270;&#35282;&#19978;&#30340;&#21452;&#21521;&#36873;&#20030;&#65292;&#21463;&#21040;&#25105;&#20204;&#25512;&#23548;&#20986;&#30340;&#19968;&#20010;&#21407;&#21017;&#21644;&#32852;&#37030;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#21407;&#21017;&#30340;&#21551;&#21457;&#12290;&#23427;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;a&#65289;&#33258;&#19979;&#32780;&#19978;&#30340;&#36873;&#20030;&#65292;&#27599;&#20010;&#20505;&#36873;&#27169;&#22411;&#26356;&#26032;&#23545;&#22810;&#20010;&#23545;&#31561;&#20505;&#36873;&#27169;&#22411;&#26356;&#26032;&#36827;&#34892;&#25237;&#31080;&#65292;&#20197;&#36873;&#20986;&#19968;&#20123;&#27169;&#22411;&#26356;&#26032;&#20316;&#20026;&#32858;&#21512;&#30340;&#34987;&#36873;&#39033;&#65307;b&#65289;&#33258;&#19978;&#32780;&#19979;&#30340;&#36873;&#20030;&#65292;&#34987;&#36873;&#39033;&#36880;&#27493;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing approaches defend against backdoor attacks in federated learning (FL) mainly through a) mitigating the impact of infected models, or b) excluding infected models. The former negatively impacts model accuracy, while the latter usually relies on globally clear boundaries between benign and infected model updates. However, model updates are easy to be mixed and scattered throughout in reality due to the diverse distributions of local data. This work focuses on excluding infected models in FL. Unlike previous perspectives from a global view, we propose Snowball, a novel anti-backdoor FL framework through bidirectional elections from an individual perspective inspired by one principle deduced by us and two principles in FL and deep learning. It is characterized by a) bottom-up election, where each candidate model update votes to several peer ones such that a few model updates are elected as selectees for aggregation; and b) top-down election, where selectees progressively enlarge t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#20219;&#21153;&#39044;&#27979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#31163;&#32676;&#26816;&#27979;&#22120;&#36827;&#34892;&#20219;&#21153;&#26631;&#35782;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#26080;&#20219;&#21153;&#26631;&#35782;&#31526;&#30340;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.15048</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#20219;&#21153;&#39044;&#27979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Class Incremental Learning via Likelihood Ratio Based Task Prediction. (arXiv:2309.15048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15048
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#20219;&#21153;&#39044;&#27979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#31163;&#32676;&#26816;&#27979;&#22120;&#36827;&#34892;&#20219;&#21153;&#26631;&#35782;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#26080;&#20219;&#21153;&#26631;&#35782;&#31526;&#30340;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#26159;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19981;&#26029;&#23398;&#20064;&#30340;&#35774;&#32622;&#65292;&#36890;&#36807;&#39034;&#24207;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#27599;&#20010;&#20219;&#21153;&#30001;&#19968;&#32452;&#21807;&#19968;&#30340;&#31867;&#32452;&#25104;&#12290;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#20851;&#38190;&#29305;&#28857;&#26159;&#65292;&#22312;&#27979;&#35797;&#26102;&#19981;&#25552;&#20379;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;&#26631;&#35782;&#31526;&#65288;&#25110;&#20219;&#21153;ID&#65289;&#12290;&#20026;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#39044;&#27979;&#20219;&#21153;ID&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#19968;&#31181;&#26032;&#20852;&#30340;&#29702;&#35770;&#19978;&#21512;&#29702;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#26681;&#25454;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#20849;&#20139;&#32593;&#32476;&#20013;&#20026;&#25152;&#26377;&#20219;&#21153;&#35757;&#32451;&#27599;&#20010;&#20219;&#21153;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#20197;&#22788;&#29702;&#36951;&#24536;&#12290;&#35813;&#26041;&#27861;&#20013;&#27599;&#20010;&#20219;&#21153;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;&#38750;&#24120;&#35268;&#20998;&#31867;&#22120;&#32780;&#19981;&#26159;&#20256;&#32479;&#20998;&#31867;&#22120;&#30340;&#31163;&#32676;&#26816;&#27979;&#22120;&#12290;&#31163;&#32676;&#26816;&#27979;&#22120;&#21487;&#20197;&#23545;&#20219;&#21153;&#20869;&#65288;&#20998;&#24067;&#20869;&#65288;IND&#65289;&#65289;&#30340;&#31867;&#36827;&#34892;&#39044;&#27979;&#21644;&#35782;&#21035;&#31163;&#32676;&#25968;&#25454;&#12290;&#22312;&#25512;&#26029;&#26399;&#38388;&#65292;&#31163;&#32676;&#26816;&#27979;&#33021;&#21147;&#26159;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;ID&#39044;&#27979;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#35748;&#20026;&#20351;&#29992;&#20256;&#32479;&#30340;&#31163;&#32676;&#26816;&#27979;&#22120;&#36827;&#34892;&#20219;&#21153;ID&#39044;&#27979;&#26159;&#27425;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class incremental learning (CIL) is a challenging setting of continual learning, which learns a series of tasks sequentially. Each task consists of a set of unique classes. The key feature of CIL is that no task identifier (or task-id) is provided at test time for each test sample. Predicting the task-id for each test sample is a challenging problem. An emerging theoretically justified and effective approach is to train a task-specific model for each task in a shared network for all tasks based on a task-incremental learning (TIL) method to deal with forgetting. The model for each task in this approach is an out-of-distribution (OOD) detector rather than a conventional classifier. The OOD detector can perform both within-task (in-distribution (IND)) class prediction and OOD detection. The OOD detection capability is the key for task-id prediction during inference for each test sample. However, this paper argues that using a traditional OOD detector for task-id prediction is sub-optimal
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#26680;&#23398;&#20064;&#65288;DKL&#65289;&#32467;&#21512;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#19982;&#39640;&#26031;&#36807;&#31243;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20026;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#21644;&#25511;&#21046;&#21512;&#25104;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#35937;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;DKL&#19982;&#21306;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;IMDP&#65289;&#23454;&#29616;&#23545;&#38543;&#26426;&#21160;&#24577;&#31995;&#32479;&#30340;&#25511;&#21046;&#21512;&#25104;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#39640;&#25928;&#30340;&#28145;&#24230;&#26550;&#26500;&#21644;&#27491;&#30830;&#24615;&#20445;&#35777;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.06569</link><description>&lt;p&gt;
&#28145;&#24230;&#26680;&#23398;&#20064;&#22312;&#25511;&#21046;&#21512;&#25104;&#20013;&#30340;&#24212;&#29992;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Promises of Deep Kernel Learning for Control Synthesis. (arXiv:2309.06569v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06569
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26680;&#23398;&#20064;&#65288;DKL&#65289;&#32467;&#21512;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#19982;&#39640;&#26031;&#36807;&#31243;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20026;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#30340;&#23398;&#20064;&#21644;&#25511;&#21046;&#21512;&#25104;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#35937;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;DKL&#19982;&#21306;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;IMDP&#65289;&#23454;&#29616;&#23545;&#38543;&#26426;&#21160;&#24577;&#31995;&#32479;&#30340;&#25511;&#21046;&#21512;&#25104;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#39640;&#25928;&#30340;&#28145;&#24230;&#26550;&#26500;&#21644;&#27491;&#30830;&#24615;&#20445;&#35777;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26680;&#23398;&#20064;&#65288;DKL&#65289;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#19982;&#39640;&#26031;&#36807;&#31243;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30456;&#32467;&#21512;&#12290;&#22240;&#27492;&#65292;&#23427;&#26159;&#19968;&#20010;&#26377;&#28508;&#21147;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#23398;&#20064;&#21644;&#25511;&#21046;&#22797;&#26434;&#30340;&#21160;&#24577;&#31995;&#32479;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#25277;&#35937;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#22797;&#26434;&#35268;&#33539;&#19979;&#20351;&#29992;DKL&#36827;&#34892;&#38543;&#26426;&#21160;&#24577;&#31995;&#32479;&#30340;&#25511;&#21046;&#21512;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#26102;&#24577;&#36923;&#36753;&#35268;&#33539;&#65292;&#24182;&#21019;&#24314;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;DKL&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26410;&#30693;&#31995;&#32479;&#65292;&#24182;&#23558;DKL&#27169;&#22411;&#27491;&#24335;&#25277;&#35937;&#25104;&#21306;&#38388;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;IMDP&#65289;&#65292;&#20197;&#36827;&#34892;&#20855;&#26377;&#27491;&#30830;&#24615;&#20445;&#35777;&#30340;&#25511;&#21046;&#21512;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#19968;&#31181;&#28145;&#24230;&#26550;&#26500;&#65292;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#23398;&#20064;&#21644;&#39640;&#25928;&#30340;&#25277;&#35937;&#35745;&#31639;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35828;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#19968;&#20010;5&#32500;&#38750;&#32447;&#24615;&#38543;&#26426;&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;DKL&#36827;&#34892;&#25511;&#21046;&#21512;&#25104;&#21487;&#20197;&#22823;&#22823;&#20248;&#20110;&#29366;&#24577;-
&lt;/p&gt;
&lt;p&gt;
Deep Kernel Learning (DKL) combines the representational power of neural networks with the uncertainty quantification of Gaussian Processes. Hence, it is potentially a promising tool to learn and control complex dynamical systems. In this work, we develop a scalable abstraction-based framework that enables the use of DKL for control synthesis of stochastic dynamical systems against complex specifications. Specifically, we consider temporal logic specifications and create an end-to-end framework that uses DKL to learn an unknown system from data and formally abstracts the DKL model into an Interval Markov Decision Process (IMDP) to perform control synthesis with correctness guarantees. Furthermore, we identify a deep architecture that enables accurate learning and efficient abstraction computation. The effectiveness of our approach is illustrated on various benchmarks, including a 5-D nonlinear stochastic system, showing how control synthesis with DKL can substantially outperform state-
&lt;/p&gt;</description></item><item><title>MASA-TCN&#26159;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;EEG&#24773;&#32490;&#35782;&#21035;&#30340;&#22810;&#38170;&#28857;&#31354;&#38388;&#24863;&#30693;&#26102;&#38388;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#31354;&#38388;&#24863;&#30693;&#26102;&#38388;&#23618;&#26469;&#25552;&#21462;EEG&#31354;&#38388;&#27169;&#24335;&#65292;&#24182;&#33021;&#22312;&#24773;&#32490;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.16207</link><description>&lt;p&gt;
MASA-TCN: &#22810;&#38170;&#28857;&#31354;&#38388;&#24863;&#30693;&#26102;&#38388;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;EEG&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
MASA-TCN: Multi-anchor Space-aware Temporal Convolutional Neural Networks for Continuous and Discrete EEG Emotion Recognition. (arXiv:2308.16207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16207
&lt;/p&gt;
&lt;p&gt;
MASA-TCN&#26159;&#19968;&#31181;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;EEG&#24773;&#32490;&#35782;&#21035;&#30340;&#22810;&#38170;&#28857;&#31354;&#38388;&#24863;&#30693;&#26102;&#38388;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#31354;&#38388;&#24863;&#30693;&#26102;&#38388;&#23618;&#26469;&#25552;&#21462;EEG&#31354;&#38388;&#27169;&#24335;&#65292;&#24182;&#33021;&#22312;&#24773;&#32490;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;&#20027;&#35201;&#26377;&#20004;&#31181;&#24773;&#20917;&#65306;&#31163;&#25955;&#26631;&#31614;&#30340;&#20998;&#31867;&#21644;&#36830;&#32493;&#26631;&#35760;&#30340;&#22238;&#24402;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#31639;&#27861;&#65292;&#20294;&#23545;&#20110;&#22238;&#24402;&#20219;&#21153;&#21482;&#26377;&#23569;&#25968;&#26041;&#27861;&#12290;&#23545;&#20110;&#24773;&#32490;&#22238;&#24402;&#65292;&#26631;&#31614;&#22312;&#26102;&#38388;&#19978;&#26159;&#36830;&#32493;&#30340;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#26041;&#27861;&#26159;&#23398;&#20064;&#26102;&#24577;&#21160;&#24577;&#27169;&#24335;&#12290;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#21644;&#26102;&#38388;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;TCN&#65289;&#34987;&#29992;&#26469;&#23398;&#20064;EEG&#29305;&#24449;&#21521;&#37327;&#30340;&#26102;&#38388;&#32972;&#26223;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;EEG&#30340;&#31354;&#38388;&#27169;&#24335;&#27809;&#26377;&#34987;&#26377;&#25928;&#25552;&#21462;&#20986;&#26469;&#12290;&#20026;&#20102;&#20351;TCN&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#24615;&#33021;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#31354;&#38388;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#21517;&#20026;MASA-TCN&#65292;&#29992;&#20110;EEG&#24773;&#32490;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;&#31354;&#38388;&#24863;&#30693;&#26102;&#38388;&#23618;&#20351;&#24471;TCN&#33021;&#22815;&#20174;EEG&#30005;&#26497;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#20013;&#36827;&#34892;&#39069;&#22806;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion recognition using electroencephalogram (EEG) mainly has two scenarios: classification of the discrete labels and regression of the continuously tagged labels. Although many algorithms were proposed for classification tasks, there are only a few methods for regression tasks. For emotion regression, the label is continuous in time. A natural method is to learn the temporal dynamic patterns. In previous studies, long short-term memory (LSTM) and temporal convolutional neural networks (TCN) were utilized to learn the temporal contextual information from feature vectors of EEG. However, the spatial patterns of EEG were not effectively extracted. To enable the spatial learning ability of TCN towards better regression and classification performances, we propose a novel unified model, named MASA-TCN, for EEG emotion regression and classification tasks. The space-aware temporal layer enables TCN to additionally learn from spatial relations among EEG electrodes. Besides, a novel multi-an
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21452;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#29992;&#20110;&#26816;&#27979;&#21644;&#20998;&#32423;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#65292;&#36890;&#36807;&#21033;&#29992;&#21333;&#20010;&#30524;&#24213;&#35270;&#32593;&#33180;&#22270;&#20687;&#36827;&#34892;&#26089;&#26399;&#35786;&#26029;&#21644;&#25104;&#21151;&#27835;&#30103;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#22823;&#22411;&#22810;&#20013;&#24515;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#24050;&#26377;&#25991;&#29486;&#12290;</title><link>http://arxiv.org/abs/2308.09945</link><description>&lt;p&gt;
&#21452;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#29992;&#20110;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#30340;&#26816;&#27979;&#21644;&#20998;&#32423;
&lt;/p&gt;
&lt;p&gt;
Dual Branch Deep Learning Network for Detection and Stage Grading of Diabetic Retinopathy. (arXiv:2308.09945v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09945
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21452;&#25903;&#36335;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#29992;&#20110;&#26816;&#27979;&#21644;&#20998;&#32423;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#65292;&#36890;&#36807;&#21033;&#29992;&#21333;&#20010;&#30524;&#24213;&#35270;&#32593;&#33180;&#22270;&#20687;&#36827;&#34892;&#26089;&#26399;&#35786;&#26029;&#21644;&#25104;&#21151;&#27835;&#30103;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#22823;&#22411;&#22810;&#20013;&#24515;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#24050;&#26377;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#26159;&#31958;&#23615;&#30149;&#30340;&#20005;&#37325;&#24182;&#21457;&#30151;&#65292;&#22914;&#26524;&#19981;&#21450;&#26102;&#27835;&#30103;&#21487;&#33021;&#23548;&#33268;&#27704;&#20037;&#24615;&#22833;&#26126;&#12290;&#23545;&#35813;&#30142;&#30149;&#30340;&#26089;&#26399;&#21644;&#20934;&#30830;&#35786;&#26029;&#23545;&#20110;&#25104;&#21151;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21033;&#29992;&#21333;&#20010;&#30524;&#24213;&#35270;&#32593;&#33180;&#22270;&#20687;&#26816;&#27979;&#21644;&#20998;&#32423;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#37319;&#29992;&#36801;&#31227;&#23398;&#20064;&#65292;&#20351;&#29992;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#22312;&#26032;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#21253;&#25324;&#20174;&#20844;&#24320;&#26469;&#28304;&#33719;&#24471;&#30340;APTOS 2019&#25968;&#25454;&#38598;&#22312;&#20869;&#30340;&#22823;&#22411;&#22810;&#20013;&#24515;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;APTOS 2019&#19978;&#65292;&#23427;&#22312;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#30340;&#26816;&#27979;&#21644;&#20998;&#32423;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#24050;&#26377;&#30340;&#25991;&#29486;&#12290;&#22312;&#20108;&#20998;&#31867;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;98.50&#65285;&#65292;&#25935;&#24863;&#24615;&#36798;&#21040;99.46&#65285;&#65292;&#29305;&#24322;&#24615;&#36798;&#21040;97.51&#65285;&#12290;&#22312;&#20998;&#32423;&#20013;&#65292;&#23427;&#36798;&#21040;&#20102;93.00&#65285;&#30340;&#24179;&#26041;&#21152;&#26435;Kappa&#20540;&#65292;&#20934;&#30830;&#29575;&#36824;&#26159;&#24456;&#39640;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diabetic retinopathy is a severe complication of diabetes that can lead to permanent blindness if not treated promptly. Early and accurate diagnosis of the disease is essential for successful treatment. This paper introduces a deep learning method for the detection and stage grading of diabetic retinopathy, using a single fundus retinal image. Our model utilizes transfer learning, employing two state-of-the-art pre-trained models as feature extractors and fine-tuning them on a new dataset. The proposed model is trained on a large multi-center dataset, including the APTOS 2019 dataset, obtained from publicly available sources. It achieves remarkable performance in diabetic retinopathy detection and stage classification on the APTOS 2019, outperforming the established literature. For binary classification, the proposed approach achieves an accuracy of 98.50%, a sensitivity of 99.46%, and a specificity of 97.51%. In stage grading, it achieves a quadratic weighted kappa of 93.00%, an accur
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#20266;&#26631;&#31614;&#23398;&#20064;&#30340;&#38750;&#20405;&#20837;&#24335;&#35821;&#38899;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#65288;MTQ-Net&#65289;&#65292;&#36890;&#36807;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#20266;&#26631;&#31614;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#30456;&#23545;&#20110;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.09262</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#20266;&#26631;&#31614;&#23398;&#20064;&#22312;&#38750;&#20405;&#20837;&#24335;&#35821;&#38899;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Pseudo-Label Learning for Non-Intrusive Speech Quality Assessment Model. (arXiv:2308.09262v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09262
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#20266;&#26631;&#31614;&#23398;&#20064;&#30340;&#38750;&#20405;&#20837;&#24335;&#35821;&#38899;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#65288;MTQ-Net&#65289;&#65292;&#36890;&#36807;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#20266;&#26631;&#31614;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#30456;&#23545;&#20110;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20219;&#21153;&#20266;&#26631;&#31614;&#23398;&#20064;&#65288;MPL&#65289;&#30340;&#38750;&#20405;&#20837;&#24335;&#35821;&#38899;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;MTQ-Net&#12290;MPL&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#33719;&#21462;&#20266;&#26631;&#31614;&#20998;&#25968;&#21644;&#25191;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#35780;&#20272;&#30446;&#26631;&#26159;&#19977;&#20010;3QUEST&#25351;&#26631;&#65292;&#21363;&#35821;&#38899;MOS&#65288;S-MOS&#65289;&#65292;&#22122;&#22768;MOS&#65288;N-MOS&#65289;&#21644;&#36890;&#29992;MOS&#65288;G-MOS&#65289;&#12290;&#39044;&#35757;&#32451;&#30340;MOSA-Net&#27169;&#22411;&#34987;&#29992;&#26469;&#20272;&#35745;&#19977;&#20010;&#20266;&#26631;&#31614;&#65306;&#20027;&#35266;&#35780;&#20272;&#35821;&#38899;&#36136;&#37327;&#65288;PESQ&#65289;&#65292;&#30701;&#26102;&#23458;&#35266;&#21487;&#25026;&#24615;&#65288;STOI&#65289;&#21644;&#35821;&#38899;&#22833;&#30495;&#25351;&#25968;&#65288;SDI&#65289;&#12290;&#28982;&#21518;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#36890;&#36807;&#32467;&#21512;&#26377;&#30417;&#30563;&#25439;&#22833;&#65288;&#36890;&#36807;&#20272;&#35745;&#20998;&#25968;&#19982;&#30495;&#23454;&#26631;&#31614;&#20043;&#38388;&#30340;&#24046;&#24322;&#23548;&#20986;&#65289;&#21644;&#21322;&#30417;&#30563;&#25439;&#22833;&#65288;&#36890;&#36807;&#20272;&#35745;&#20998;&#25968;&#19982;&#20266;&#26631;&#31614;&#20043;&#38388;&#30340;&#24046;&#24322;&#23548;&#20986;&#65289;&#26469;&#35757;&#32451;MTQ-Net&#65292;&#20854;&#20013;&#20351;&#29992;Huber&#25439;&#22833;&#20989;&#25968;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#39318;&#20808;&#35777;&#26126;&#20102;MPL&#30456;&#36739;&#20110;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes a multi-task pseudo-label learning (MPL)-based non-intrusive speech quality assessment model called MTQ-Net. MPL consists of two stages: obtaining pseudo-label scores from a pretrained model and performing multi-task learning. The 3QUEST metrics, namely Speech-MOS (S-MOS), Noise-MOS (N-MOS), and General-MOS (G-MOS), are the assessment targets. The pretrained MOSA-Net model is utilized to estimate three pseudo labels: perceptual evaluation of speech quality (PESQ), short-time objective intelligibility (STOI), and speech distortion index (SDI). Multi-task learning is then employed to train MTQ-Net by combining a supervised loss (derived from the difference between the estimated score and the ground-truth label) and a semi-supervised loss (derived from the difference between the estimated score and the pseudo label), where the Huber loss is employed as the loss function. Experimental results first demonstrate the advantages of MPL compared to training a model from scra
&lt;/p&gt;</description></item><item><title>PTransIPs&#26159;&#19968;&#31181;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#30340;&#27688;&#22522;&#37240;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#21333;&#35789;&#65292;&#24182;&#32467;&#21512;&#22823;&#22411;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#27169;&#22411;&#30340;&#23884;&#20837;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#29992;&#20110;&#35782;&#21035;&#30967;&#37240;&#21270;&#20301;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.05115</link><description>&lt;p&gt;
&#22522;&#20110;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;Transformer&#30340;&#30967;&#37240;&#21270;&#20301;&#28857;&#35782;&#21035;&#26041;&#27861;(PTransIPs)
&lt;/p&gt;
&lt;p&gt;
PTransIPs: Identification of phosphorylation sites based on protein pretrained language model and Transformer. (arXiv:2308.05115v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05115
&lt;/p&gt;
&lt;p&gt;
PTransIPs&#26159;&#19968;&#31181;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#30340;&#27688;&#22522;&#37240;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#21333;&#35789;&#65292;&#24182;&#32467;&#21512;&#22823;&#22411;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#27169;&#22411;&#30340;&#23884;&#20837;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#29992;&#20110;&#35782;&#21035;&#30967;&#37240;&#21270;&#20301;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30967;&#37240;&#21270;&#26159;&#35768;&#22810;&#22522;&#30784;&#32454;&#32990;&#36807;&#31243;&#30340;&#26680;&#24515;&#65292;&#24433;&#21709;&#30528;&#21508;&#31181;&#30142;&#30149;&#30340;&#21457;&#29983;&#21644;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#30967;&#37240;&#21270;&#20301;&#28857;&#30340;&#35782;&#21035;&#26159;&#29702;&#35299;&#32454;&#32990;&#21644;&#30149;&#27602;&#24863;&#26579;&#30340;&#20998;&#23376;&#26426;&#21046;&#30340;&#37325;&#35201;&#19968;&#27493;&#65292;&#21487;&#33021;&#20026;&#26032;&#30340;&#27835;&#30103;&#38774;&#28857;&#25552;&#20379;&#22522;&#30784;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PTransIPs&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#30967;&#37240;&#21270;&#20301;&#28857;&#12290;PTransIPs&#23558;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#30340;&#27688;&#22522;&#37240;&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#21333;&#35789;&#65292;&#24182;&#26681;&#25454;&#24207;&#21015;&#20013;&#27688;&#22522;&#37240;&#30340;&#31867;&#22411;&#21644;&#20301;&#32622;&#25552;&#21462;&#29420;&#29305;&#30340;&#32534;&#30721;&#12290;&#23427;&#36824;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#27169;&#22411;&#30340;&#23884;&#20837;&#20316;&#20026;&#39069;&#22806;&#30340;&#25968;&#25454;&#36755;&#20837;&#12290;PTransIPs&#36827;&#19968;&#27493;&#36890;&#36807;&#32467;&#21512;&#20855;&#26377;&#27531;&#24046;&#36830;&#25509;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#27169;&#22411;&#65292;&#37197;&#22791;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20840;&#36830;&#25509;&#23618;&#36755;&#20986;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phosphorylation is central to numerous fundamental cellular processes, influencing the onset and progression of a variety of diseases. Identification of phosphorylation sites is thus an important step for understanding the molecular mechanisms of cells and virus infection, which potentially leads to new therapeutic targets. In this study, we present PTransIPs, a novel deep learning model for the identification of phosphorylation sites. PTransIPs treats amino acids in protein sequences as words in natural language, extracting unique encodings based on the types along with position of amino acids in the sequence. It also incorporates embeddings from large pre-trained protein models as additional data inputs. PTransIPS is further trained on a combination model of convolutional neural network with residual connections and Transformer model equipped with multi-head attention mechanisms. At last, the model outputs classification results through a fully connected layer. The results of indepen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#30495;&#23454;&#21644;&#21487;&#20449;&#30340;&#25200;&#21160;&#25110;&#24322;&#24120;&#22270;&#20687;&#26469;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#21644;&#35757;&#32451;Robusta&#65292;&#19968;&#31181;&#40065;&#26834;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21487;&#20197;&#20026;&#35757;&#32451;&#21487;&#38752;&#30340;&#20998;&#21106;&#27169;&#22411;&#25552;&#20379;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#22312;&#38754;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#25200;&#21160;&#21644;&#20998;&#24067;&#21464;&#21270;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02535</link><description>&lt;p&gt;
&#23398;&#20064;&#29983;&#25104;&#29992;&#20110;&#40065;&#26834;&#35821;&#20041;&#20998;&#21106;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Training Datasets for Robust Semantic Segmentation. (arXiv:2308.02535v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#30495;&#23454;&#21644;&#21487;&#20449;&#30340;&#25200;&#21160;&#25110;&#24322;&#24120;&#22270;&#20687;&#26469;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#21644;&#35757;&#32451;Robusta&#65292;&#19968;&#31181;&#40065;&#26834;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21487;&#20197;&#20026;&#35757;&#32451;&#21487;&#38752;&#30340;&#20998;&#21106;&#27169;&#22411;&#25552;&#20379;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#22312;&#38754;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#25200;&#21160;&#21644;&#20998;&#24067;&#21464;&#21270;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26159;&#23427;&#20204;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#25200;&#21160;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#24212;&#29992;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26631;&#31614;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;&#21644;&#22270;&#20687;&#21040;&#26631;&#31614;&#20998;&#21106;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#26469;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;Robusta&#65292;&#29992;&#20110;&#29983;&#25104;&#30495;&#23454;&#21644;&#21487;&#20449;&#30340;&#25200;&#21160;&#25110;&#24322;&#24120;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#21487;&#38752;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19979;&#28216;&#20998;&#21106;&#32593;&#32476;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#22312;&#38754;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#25200;&#21160;&#12289;&#20998;&#24067;&#21464;&#21270;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation techniques have shown significant progress in recent years, but their robustness to real-world perturbations and data samples not seen during training remains a challenge, particularly in safety-critical applications. In this paper, we propose a novel approach to improve the robustness of semantic segmentation techniques by leveraging the synergy between label-to-image generators and image-to-label segmentation models. Specifically, we design and train Robusta, a novel robust conditional generative adversarial network to generate realistic and plausible perturbed or outlier images that can be used to train reliable segmentation models. We conduct in-depth studies of the proposed generative model, assess the performance and robustness of the downstream segmentation network, and demonstrate that our approach can significantly enhance the robustness of semantic segmentation techniques in the face of real-world perturbations, distribution shifts, and out-of-distributi
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21253;&#21547;&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#23398;&#20064;&#33021;&#21147;&#20351;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#19982;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#12289;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#22914;&#20309;&#32858;&#21512;&#26631;&#31614;&#20449;&#24687;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#30340;&#24037;&#20316;&#26426;&#21046;&#21450;&#20854;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22788;&#29702;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2307.12375</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22312;&#23398;&#20064;&#26631;&#31614;&#20851;&#31995;&#19978;&#20855;&#26377;&#21019;&#26032;&#65292;&#20294;&#24182;&#38750;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning. (arXiv:2307.12375v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12375
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21253;&#21547;&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#23398;&#20064;&#33021;&#21147;&#20351;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#19982;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#12289;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#22914;&#20309;&#32858;&#21512;&#26631;&#31614;&#20449;&#24687;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#30340;&#24037;&#20316;&#26426;&#21046;&#21450;&#20854;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22788;&#29702;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#22312;&#21253;&#21547;&#36755;&#20837;-&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#36890;&#24120;&#26174;&#33879;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;LLMs&#30340;&#36825;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#30340;&#24037;&#20316;&#26426;&#21046;&#23578;&#26080;&#20849;&#35782;&#65306;&#20363;&#22914;&#65292;&#34429;&#28982;Xie&#31561;&#20154;&#65288;2021&#24180;&#65289;&#23558;ICL&#27604;&#20316;&#19968;&#31181;&#36890;&#29992;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;Min&#31561;&#20154;&#65288;2022b&#24180;&#65289;&#35748;&#20026;ICL&#29978;&#33267;&#19981;&#33021;&#20174;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#23398;&#20064;&#26631;&#31614;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;&#19979;&#19977;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#32467;&#26524;&#65292;&#65288;2&#65289;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#30340;&#36755;&#20837;-&#26631;&#31614;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#65288;3&#65289;ICL&#22914;&#20309;&#32858;&#21512;&#26469;&#33258;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#26631;&#31614;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#36890;&#24120;&#20250;&#25972;&#21512;&#19978;&#19979;&#25991;&#26631;&#31614;&#30340;&#20449;&#24687;&#65292;&#20294;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#26631;&#31614;&#20851;&#31995;&#34987;&#21306;&#21035;&#23545;&#24453;&#65292;&#27169;&#22411;&#19981;&#20250;&#23558;&#25152;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#21516;&#23545;&#24453;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#23545;LLMs&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of Large Language Models (LLMs) on downstream tasks often improves significantly when including examples of the input-label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works: for example, while Xie et al. (2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we study (1) how labels of in-context examples affect predictions, (2) how label relationships learned during pre-training interact with input-label examples provided in-context, and (3) how ICL aggregates label information across in-context examples. Our findings suggests LLMs usually incorporate information from in-context labels, but that pre-training and in-context label relationships are treated differently, and that the model does not consider all in-context information equally. Our results give insights into underst
&lt;/p&gt;</description></item><item><title>UniTabE&#26159;&#19968;&#31181;&#38754;&#21521;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#34920;&#26684;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#34920;&#26684;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;&#24182;&#20855;&#26377;&#23545;&#22810;&#26679;&#21270;&#19979;&#28216;&#24212;&#29992;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09249</link><description>&lt;p&gt;
UniTabE: &#38754;&#21521;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#34920;&#26684;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data. (arXiv:2307.09249v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09249
&lt;/p&gt;
&lt;p&gt;
UniTabE&#26159;&#19968;&#31181;&#38754;&#21521;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#34920;&#26684;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#34920;&#26684;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;&#24182;&#20855;&#26377;&#23545;&#22810;&#26679;&#21270;&#19979;&#28216;&#24212;&#29992;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#26126;&#35777;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31361;&#30772;&#24615;&#24433;&#21709;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#23041;&#21147;&#25193;&#23637;&#21040;&#20256;&#32479;&#34987;&#24573;&#35270;&#30340;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#65292;&#35813;&#39046;&#22495;&#30001;&#20110;&#19981;&#21516;&#20219;&#21153;&#22266;&#26377;&#30340;&#20247;&#22810;&#34920;&#26684;&#27169;&#24335;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#24037;&#20316;&#30340;&#20027;&#35201;&#30740;&#31350;&#38382;&#39064;&#22260;&#32469;&#24322;&#26500;&#34920;&#26684;&#32467;&#26500;&#30340;&#36866;&#24212;&#24615;&#12289;&#34920;&#26684;&#25968;&#25454;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#21327;&#35758;&#30340;&#24314;&#31435;&#12289;&#23398;&#21040;&#30340;&#30693;&#35782;&#22312;&#20219;&#21153;&#20043;&#38388;&#30340;&#27867;&#21270;&#21644;&#21487;&#20256;&#36882;&#24615;&#12289;&#23545;&#22810;&#26679;&#21270;&#19979;&#28216;&#24212;&#29992;&#30340;&#36866;&#24212;&#24615;&#20197;&#21450;&#38543;&#26102;&#38388;&#30340;&#22686;&#37327;&#21015;&#30340;&#32435;&#20837;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;UniTabE&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20197;&#19968;&#33268;&#30340;&#26041;&#24335;&#22788;&#29702;&#34920;&#26684;&#65292;&#25670;&#33073;&#20102;&#29305;&#23450;&#34920;&#26684;&#32467;&#26500;&#24378;&#21152;&#30340;&#32422;&#26463;&#12290;UniTabE&#30340;&#26680;&#24515;&#27010;&#24565;&#26159;&#23545;&#27599;&#20010;&#22522;&#26412;&#34920;&#26684;&#36827;&#34892;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Natural Language Processing (NLP) have witnessed the groundbreaking impact of pretrained models, yielding impressive outcomes across various tasks. This study seeks to extend the power of pretraining methodologies to tabular data, a domain traditionally overlooked, yet inherently challenging due to the plethora of table schemas intrinsic to different tasks. The primary research questions underpinning this work revolve around the adaptation to heterogeneous table structures, the establishment of a universal pretraining protocol for tabular data, the generalizability and transferability of learned knowledge across tasks, the adaptation to diverse downstream applications, and the incorporation of incremental columns over time. In response to these challenges, we introduce UniTabE, a pioneering method designed to process tables in a uniform manner, devoid of constraints imposed by specific table structures. UniTabE's core concept relies on representing each basic tab
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#21333;&#32454;&#32990;&#24046;&#24322;&#20998;&#26512;&#27979;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#38750;&#32447;&#24615;&#27604;&#36739;&#22797;&#26434;&#30340;&#32454;&#32990;&#38388;&#20998;&#23376;&#29305;&#24449;&#20998;&#24067;&#12290;&#36890;&#36807;&#21033;&#29992;&#26680;&#23884;&#20837;&#30340;&#21464;&#24322;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25581;&#31034;&#32454;&#32990;&#32676;&#20307;&#20013;&#38544;&#34109;&#30340;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26680;&#27979;&#35797;&#22914;&#20309;&#20811;&#26381;&#21333;&#32454;&#32990;&#24046;&#24322;&#20998;&#26512;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24212;&#29992;&#20110;&#30740;&#31350;&#20998;&#21270;&#36870;&#36716;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.08509</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#21333;&#32454;&#32990;&#24046;&#24322;&#20998;&#26512;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Kernel-Based Testing for Single-Cell Differential Analysis. (arXiv:2307.08509v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#26041;&#27861;&#30340;&#21333;&#32454;&#32990;&#24046;&#24322;&#20998;&#26512;&#27979;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#38750;&#32447;&#24615;&#27604;&#36739;&#22797;&#26434;&#30340;&#32454;&#32990;&#38388;&#20998;&#23376;&#29305;&#24449;&#20998;&#24067;&#12290;&#36890;&#36807;&#21033;&#29992;&#26680;&#23884;&#20837;&#30340;&#21464;&#24322;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25581;&#31034;&#32454;&#32990;&#32676;&#20307;&#20013;&#38544;&#34109;&#30340;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26680;&#27979;&#35797;&#22914;&#20309;&#20811;&#26381;&#21333;&#32454;&#32990;&#24046;&#24322;&#20998;&#26512;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24212;&#29992;&#20110;&#30740;&#31350;&#20998;&#21270;&#36870;&#36716;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#32454;&#32990;&#25216;&#26415;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#22522;&#22240;&#34920;&#36798;&#21644;&#34920;&#35266;&#36951;&#20256;&#20462;&#39280;&#31561;&#20998;&#23376;&#29305;&#24449;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20197;&#25511;&#21046;&#21644;&#24378;&#26377;&#21147;&#30340;&#26041;&#24335;&#27604;&#36739;&#36825;&#20123;&#22797;&#26434;&#20998;&#24067;&#38754;&#20020;&#30528;&#26041;&#27861;&#35770;&#19978;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#22522;&#20110;&#26680;&#23884;&#20837;&#30340;&#26680;&#27979;&#35797;&#26694;&#26550;&#26469;&#38750;&#32447;&#24615;&#27604;&#36739;&#32454;&#32990;&#38388;&#22797;&#26434;&#20998;&#23376;&#29305;&#24449;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#20165;&#20801;&#35768;&#23545;&#29305;&#24449;&#36827;&#34892;&#20998;&#26512;&#65292;&#36824;&#33021;&#22312;&#32771;&#34385;&#20102;&#23427;&#20204;&#20043;&#38388;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#36716;&#24405;&#32452;&#25110;&#34920;&#35266;&#32452;&#30340;&#20840;&#23616;&#27604;&#36739;&#12290;&#36890;&#36807;&#20351;&#29992;&#20998;&#31867;&#22120;&#22522;&#20110;&#26680;&#23884;&#20837;&#30340;&#21464;&#24322;&#24615;&#26469;&#21306;&#20998;&#32454;&#32990;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21457;&#29616;&#22312;&#32454;&#32990;&#32676;&#20307;&#20013;&#21407;&#26412;&#26080;&#27861;&#23519;&#35273;&#21040;&#30340;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26680;&#27979;&#35797;&#26041;&#27861;&#22914;&#20309;&#20811;&#26381;&#19987;&#38376;&#29992;&#20110;&#21333;&#32454;&#32990;&#30340;&#24046;&#24322;&#20998;&#26512;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#36824;&#23558;&#26680;&#27979;&#35797;&#24212;&#29992;&#20110;&#30740;&#31350;&#20998;&#21270;&#36870;&#36716;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Single-cell technologies have provided valuable insights into the distribution of molecular features, such as gene expression and epigenomic modifications. However, comparing these complex distributions in a controlled and powerful manner poses methodological challenges. Here we propose to benefit from the kernel-testing framework to compare the complex cell-wise distributions of molecular features in a non-linear manner based on their kernel embedding. Our framework not only allows for feature-wise analyses but also enables global comparisons of transcriptomes or epigenomes, considering their intricate dependencies. By using a classifier to discriminate cells based on the variability of their embedding, our method uncovers heterogeneities in cell populations that would otherwise go undetected. We show that kernel testing overcomes the limitations of differential analysis methods dedicated to single-cell. Kernel testing is applied to investigate the reversion process of differentiating
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#20004;&#20010;&#24433;&#21709;VFL&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65306;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#29305;&#24449;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#20851;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#21010;&#20998;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#24341;&#20837;&#30495;&#23454;&#30340;VFL&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#22270;&#20687;-&#22270;&#20687;VFL&#24773;&#26223;&#20013;&#30340;&#19981;&#36275;&#12290;&#30740;&#31350;&#23545;&#20110;&#26410;&#26469;&#30340;VFL&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.02040</link><description>&lt;p&gt;
VertiBench: &#22312;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#22522;&#20934;&#20013;&#25512;&#36827;&#29305;&#24449;&#20998;&#24067;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks. (arXiv:2307.02040v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#20004;&#20010;&#24433;&#21709;VFL&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65306;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#29305;&#24449;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#20851;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#21010;&#20998;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#24341;&#20837;&#30495;&#23454;&#30340;VFL&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#22270;&#20687;-&#22270;&#20687;VFL&#24773;&#26223;&#20013;&#30340;&#19981;&#36275;&#12290;&#30740;&#31350;&#23545;&#20110;&#26410;&#26469;&#30340;VFL&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#22312;&#29305;&#24449;&#21010;&#20998;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#35201;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38480;&#21046;&#65292;&#24456;&#23569;&#26377;&#20844;&#24320;&#30340;&#30495;&#23454;&#19990;&#30028;VFL&#25968;&#25454;&#38598;&#29992;&#20110;&#31639;&#27861;&#35780;&#20272;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#38598;&#21482;&#20195;&#34920;&#20102;&#26377;&#38480;&#30340;&#29305;&#24449;&#20998;&#24067;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#36890;&#24120;&#37319;&#29992;&#20174;&#20840;&#23616;&#38598;&#21512;&#20013;&#30340;&#20219;&#24847;&#29305;&#24449;&#21010;&#20998;&#23548;&#20986;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#36825;&#21482;&#25429;&#25417;&#21040;&#20102;&#19968;&#37096;&#20998;&#29305;&#24449;&#20998;&#24067;&#65292;&#23548;&#33268;&#31639;&#27861;&#24615;&#33021;&#35780;&#20272;&#19981;&#36275;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#24433;&#21709;VFL&#24615;&#33021;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#8212;&#8212;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#29305;&#24449;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#30456;&#20851;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#21010;&#20998;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#30495;&#23454;&#30340;VFL&#25968;&#25454;&#38598;&#26469;&#24357;&#34917;&#22270;&#20687;-&#22270;&#20687;VFL&#24773;&#26223;&#20013;&#30340;&#19981;&#36275;&#12290;&#25105;&#20204;&#23545;&#23574;&#31471;VFL&#31639;&#27861;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning (VFL) is a crucial paradigm for training machine learning models on feature-partitioned, distributed data. However, due to privacy restrictions, few public real-world VFL datasets exist for algorithm evaluation, and these represent a limited array of feature distributions. Existing benchmarks often resort to synthetic datasets, derived from arbitrary feature splits from a global set, which only capture a subset of feature distributions, leading to inadequate algorithm performance assessment. This paper addresses these shortcomings by introducing two key factors affecting VFL performance - feature importance and feature correlation - and proposing associated evaluation metrics and dataset splitting methods. Additionally, we introduce a real VFL dataset to address the deficit in image-image VFL scenarios. Our comprehensive evaluation of cutting-edge VFL algorithms provides valuable insights for future research in the field.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#31561;&#28183;&#24615;&#30340;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#36870;&#25193;&#25955;&#36807;&#31243;&#23454;&#29616;&#20102;&#26032;&#39062;&#30340;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#65292;&#22312;&#39640;&#32500;&#37319;&#26679;&#20013;&#34920;&#29616;&#20986;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02037</link><description>&lt;p&gt;
&#26080;&#31561;&#28183;&#24615;&#30340;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#65306;&#19968;&#31181;&#36870;&#25193;&#25955;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Monte Carlo Sampling without Isoperimetry: A Reverse Diffusion Approach. (arXiv:2307.02037v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#31561;&#28183;&#24615;&#30340;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#36870;&#25193;&#25955;&#36807;&#31243;&#23454;&#29616;&#20102;&#26032;&#39062;&#30340;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#65292;&#22312;&#39640;&#32500;&#37319;&#26679;&#20013;&#34920;&#29616;&#20986;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#36890;&#24120;&#21462;&#20915;&#20110;&#25193;&#25955;&#36335;&#24452;&#19978;&#24471;&#20998;&#20272;&#35745;&#30340;&#31934;&#24230;&#65292;&#37325;&#28857;&#20851;&#27880;&#25193;&#25955;&#27169;&#22411;&#21450;&#20854;&#29983;&#25104;&#39640;&#36136;&#37327;&#25968;&#25454;&#26679;&#26412;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#36890;&#36807;&#36870;&#25193;&#25955;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#23545;&#37319;&#26679;&#25991;&#29486;&#36827;&#34892;&#30740;&#31350;&#65292;&#21457;&#29616;&#21487;&#20197;&#36890;&#36807;&#36716;&#31227;&#26680;&#30340;&#20998;&#35299;&#23558;&#24471;&#20998;&#20272;&#35745;&#36716;&#21270;&#20026;&#22343;&#20540;&#20272;&#35745;&#38382;&#39064;&#12290;&#36890;&#36807;&#20272;&#35745;&#36741;&#21161;&#20998;&#24067;&#30340;&#22343;&#20540;&#65292;&#36870;&#25193;&#25955;&#36807;&#31243;&#21487;&#20197;&#20135;&#29983;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#24635;&#21464;&#24046;&#36317;&#31163;&#19979;&#30340;&#25910;&#25947;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#31639;&#27861;&#30340;&#31561;&#28183;&#24615;&#20381;&#36182;&#24615;&#30456;&#23545;&#36739;&#20302;&#65292;&#27604;&#20256;&#32479;&#30340;MCMC&#25216;&#26415;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#39640;&#32500;&#37319;&#26679;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The efficacy of modern generative models is commonly contingent upon the precision of score estimation along the diffusion path, with a focus on diffusion models and their ability to generate high-quality data samples. This study delves into the potentialities of posterior sampling through reverse diffusion. An examination of the sampling literature reveals that score estimation can be transformed into a mean estimation problem via the decomposition of the transition kernel. By estimating the mean of the auxiliary distribution, the reverse diffusion process can give rise to a novel posterior sampling algorithm, which diverges from traditional gradient-based Markov Chain Monte Carlo (MCMC) methods. We provide the convergence analysis in total variation distance and demonstrate that the isoperimetric dependency of the proposed algorithm is comparatively lower than that observed in conventional MCMC techniques, which justifies the superior performance for high dimensional sampling with er
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#29575;&#32422;&#26463;&#19979;&#30340;&#23433;&#20840;&#20851;&#38190;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#26126;&#30830;&#26799;&#24230;&#34920;&#36798;&#24335;&#30340;Safe Policy Gradient-REINFORCE&#65288;SPG-REINFORCE&#65289;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#30028;&#38480;&#35777;&#26126;&#20102;&#27010;&#29575;&#32422;&#26463;&#35774;&#32622;&#22312;&#26368;&#20248;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#20855;&#26377;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.17279</link><description>&lt;p&gt;
&#23433;&#20840;&#20851;&#38190;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#29575;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Constraint for Safety-Critical Reinforcement Learning. (arXiv:2306.17279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#29575;&#32422;&#26463;&#19979;&#30340;&#23433;&#20840;&#20851;&#38190;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#26126;&#30830;&#26799;&#24230;&#34920;&#36798;&#24335;&#30340;Safe Policy Gradient-REINFORCE&#65288;SPG-REINFORCE&#65289;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#30028;&#38480;&#35777;&#26126;&#20102;&#27010;&#29575;&#32422;&#26463;&#35774;&#32622;&#22312;&#26368;&#20248;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#20855;&#26377;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#27010;&#29575;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#23433;&#20840;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23433;&#20840;&#31574;&#30053;&#25110;&#25511;&#21046;&#22120;&#26159;&#25351;&#20197;&#39640;&#27010;&#29575;&#20445;&#25345;&#20195;&#29702;&#22312;&#32473;&#23450;&#23433;&#20840;&#38598;&#21512;&#20013;&#30340;&#36712;&#36857;&#12290;&#25105;&#20204;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#39057;&#32321;&#25506;&#32034;&#30340;&#32047;&#31215;&#32422;&#26463;&#38382;&#39064;&#21644;&#36825;&#31181;&#27010;&#29575;&#32422;&#26463;&#38382;&#39064;&#20043;&#38388;&#24314;&#31435;&#20102;&#32852;&#31995;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#30028;&#38480;&#65292;&#38416;&#26126;&#27010;&#29575;&#32422;&#26463;&#35774;&#32622;&#22312;&#26368;&#20248;&#24615;&#21644;&#23433;&#20840;&#24615;&#65288;&#32422;&#26463;&#28385;&#36275;&#65289;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;&#22312;&#22788;&#29702;&#27010;&#29575;&#32422;&#26463;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#27491;&#22914;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25152;&#25506;&#32034;&#30340;&#37027;&#26679;&#65292;&#28304;&#20110;&#27809;&#26377;&#26126;&#30830;&#30340;&#26799;&#24230;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#36825;&#31181;&#26126;&#30830;&#30340;&#26799;&#24230;&#34920;&#36798;&#24335;&#65292;&#31216;&#20043;&#20026;Safe Policy Gradient-REINFORCE&#65288;SPG-REINFORCE&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#26799;&#24230;SPG-Actor-Critic
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the problem of learning safe policies for probabilistic-constrained reinforcement learning (RL). Specifically, a safe policy or controller is one that, with high probability, maintains the trajectory of the agent in a given safe set. We establish a connection between this probabilistic-constrained setting and the cumulative-constrained formulation that is frequently explored in the existing literature. We provide theoretical bounds elucidating that the probabilistic-constrained setting offers a better trade-off in terms of optimality and safety (constraint satisfaction). The challenge encountered when dealing with the probabilistic constraints, as explored in this work, arises from the absence of explicit expressions for their gradients. Our prior work provides such an explicit gradient expression for probabilistic constraints which we term Safe Policy Gradient-REINFORCE (SPG-REINFORCE). In this work, we provide an improved gradient SPG-Actor-Critic that lead
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35782;&#21035;&#24615;&#24418;&#24335;&#65292;&#31216;&#20026;&#37327;&#21270;&#22352;&#26631;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39640;&#24230;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#19979;&#65292;&#21487;&#20197;&#24674;&#22797;&#31163;&#25955;&#21270;&#30340;&#28508;&#22312;&#22352;&#26631;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#35299;&#32544;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.16334</link><description>&lt;p&gt;
&#36890;&#36807;&#23494;&#24230;&#26631;&#24535;&#26816;&#27979;&#26469;&#35782;&#21035;&#31163;&#25955;&#21270;&#28508;&#22312;&#22352;&#26631;&#31995;&#32479;&#30340;&#21487;&#35782;&#21035;&#24615;
&lt;/p&gt;
&lt;p&gt;
Identifiability of Discretized Latent Coordinate Systems via Density Landmarks Detection. (arXiv:2306.16334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35782;&#21035;&#24615;&#24418;&#24335;&#65292;&#31216;&#20026;&#37327;&#21270;&#22352;&#26631;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39640;&#24230;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#19979;&#65292;&#21487;&#20197;&#24674;&#22797;&#31163;&#25955;&#21270;&#30340;&#28508;&#22312;&#22352;&#26631;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#35299;&#32544;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#26088;&#22312;&#20165;&#20174;&#35266;&#23519;&#21040;&#30340;&#20998;&#24067;&#20013;&#24674;&#22797;&#26377;&#24847;&#20041;&#30340;&#28508;&#22312;&#30495;&#23454;&#22240;&#32032;&#12290; &#21487;&#35782;&#21035;&#24615;&#20026;&#35299;&#32544;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290; &#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#33258;&#36866;&#24212;&#29420;&#31435;&#28508;&#21464;&#37327;&#22240;&#23376;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#20809;&#28369;&#22240;&#23376;&#21040;&#35266;&#27979;&#30340;&#26144;&#23556;&#19979;&#65292;&#26080;&#30417;&#30563;&#30340;&#21487;&#35782;&#21035;&#24615;&#22312;i.i.d.&#35774;&#32622;&#19979;&#26159;&#29702;&#35770;&#19978;&#19981;&#21487;&#33021;&#30340;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38750;&#24120;&#24778;&#20154;&#30340;&#26159;&#65292;&#22312;&#39640;&#24230;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#20809;&#28369;&#26144;&#23556;&#65288;&#19968;&#20010;&#24494;&#20998;&#21516;&#32986;&#65289;&#19979;&#65292;&#21487;&#20197;&#24674;&#22797;&#31163;&#25955;&#21270;&#30340;&#28508;&#22312;&#22352;&#26631;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#26144;&#23556;&#36827;&#34892;&#20219;&#20309;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290; &#36825;&#26159;&#22312;&#20551;&#35774;&#28508;&#22312;&#23494;&#24230;&#20855;&#26377;&#36724;&#23545;&#40784;&#30340;&#19981;&#36830;&#32493;&#26631;&#24535;&#30340;&#24773;&#20917;&#19979;&#65292;&#20294;&#19981;&#20570;&#22240;&#32032;&#30340;&#32479;&#35745;&#29420;&#31435;&#30340;&#19981;&#29616;&#23454;&#30340;&#20551;&#35774;&#12290; &#25105;&#20204;&#24341;&#20837;&#20102;&#36825;&#31181;&#26032;&#39062;&#30340;&#21487;&#35782;&#21035;&#24615;&#24418;&#24335;&#65292;&#31216;&#20026;&#37327;&#21270;&#22352;&#26631;&#21487;&#35782;&#21035;&#24615;&#65292;&#24182;&#23545;&#24674;&#22797;&#31163;&#25955;&#22352;&#26631;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentanglement aims to recover meaningful latent ground-truth factors from only the observed distribution. Identifiability provides the theoretical grounding for disentanglement to be well-founded. Unfortunately, unsupervised identifiability of independent latent factors is a theoretically proven impossibility in the i.i.d. setting under a general nonlinear smooth map from factors to observations. In this work, we show that, remarkably, it is possible to recover discretized latent coordinates under a highly generic nonlinear smooth mapping (a diffeomorphism) without any additional inductive bias on the mapping. This is, assuming that latent density has axis-aligned discontinuity landmarks, but without making the unrealistic assumption of statistical independence of the factors. We introduce this novel form of identifiability, termed quantized coordinate identifiability, and provide a comprehensive proof of the recovery of discretized coordinates.
&lt;/p&gt;</description></item><item><title>&#31995;&#32479;&#20998;&#26512;&#20102;&#24433;&#23376;&#27169;&#22411;&#19981;&#23545;&#40784;&#38382;&#39064;&#30340;&#21407;&#22240;&#65292;&#24182;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#25110;&#23454;&#20363;&#21152;&#26435;&#26041;&#27861;&#37325;&#26032;&#23545;&#40784;&#24433;&#23376;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30333;&#30418;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.05093</link><description>&lt;p&gt;
&#37325;&#26032;&#23545;&#40784;&#24433;&#23376;&#27169;&#22411;&#33021;&#22815;&#25552;&#39640;&#30333;&#30418;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Re-aligning Shadow Models can Improve White-box Membership Inference Attacks. (arXiv:2306.05093v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05093
&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#20998;&#26512;&#20102;&#24433;&#23376;&#27169;&#22411;&#19981;&#23545;&#40784;&#38382;&#39064;&#30340;&#21407;&#22240;&#65292;&#24182;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#25110;&#23454;&#20363;&#21152;&#26435;&#26041;&#27861;&#37325;&#26032;&#23545;&#40784;&#24433;&#23376;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30333;&#30418;&#25104;&#21592;&#38544;&#31169;&#25915;&#20987;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#27844;&#38706;&#20102;&#26377;&#20851;&#20854;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290;&#38543;&#30528;&#27169;&#22411;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#34987;&#29992;&#20110;&#35774;&#22791;&#19978;&#65292;&#33258;&#21160;&#21270;&#20219;&#21153;&#21644;&#39537;&#21160;&#26032;&#24212;&#29992;&#65292;&#20154;&#20204;&#24320;&#22987;&#20851;&#27880;&#30333;&#30418;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#32780;&#19981;&#26159;&#20165;&#25552;&#20379;&#23545;&#27169;&#22411;&#30340;&#26597;&#35810;&#35775;&#38382;&#30340;&#40657;&#30418;&#35774;&#32622;&#65292;&#36825;&#22686;&#21152;&#20102;&#25915;&#20987;&#38754;&#12290;&#23558;&#40657;&#30418;&#21040;&#30333;&#30418;&#35774;&#32622;&#30340;&#24433;&#23376;&#24314;&#27169;&#25216;&#26415;&#30452;&#25509;&#25193;&#23637;&#21040;&#30333;&#30418;&#35774;&#32622;&#20013;&#65292;&#36890;&#24120;&#34920;&#29616;&#19981;&#22914;&#20165;&#36827;&#34892;&#40657;&#30418;&#25915;&#20987;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24050;&#30693;&#29305;&#24449;&#8212;&#8212;&#19981;&#23545;&#40784;&#12290;&#26412;&#25991;&#39318;&#27425;&#23545;&#24433;&#23376;&#27169;&#22411;&#19981;&#23545;&#40784;&#30340;&#21407;&#22240;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#24182;&#34920;&#26126;&#37319;&#29992;&#19981;&#21516;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#26159;&#24433;&#23376;&#27169;&#22411;&#19981;&#23545;&#40784;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#34701;&#21512;&#25991;&#29486;&#20013;&#20808;&#21069;&#24320;&#21457;&#30340;&#22810;&#31181;&#37325;&#26032;&#23545;&#40784;&#25216;&#26415;&#25193;&#23637;&#21040;&#24433;&#23376;&#24314;&#27169;&#19978;&#19979;&#25991;&#20013;&#65292;&#30446;&#26631;&#26159;&#37325;&#26032;&#23545;&#40784;......
&lt;/p&gt;
&lt;p&gt;
Machine learning models have been shown to leak sensitive information about their training datasets. As models are being increasingly used, on devices, to automate tasks and power new applications, there have been concerns that such white-box access to its parameters, as opposed to the black-box setting which only provides query access to the model, increases the attack surface. Directly extending the shadow modelling technique from the black-box to the white-box setting has been shown, in general, not to perform better than black-box only attacks. A key reason is misalignment, a known characteristic of deep neural networks. We here present the first systematic analysis of the causes of misalignment in shadow models and show the use of a different weight initialisation to be the main cause of shadow model misalignment. Second, we extend several re-alignment techniques, previously developed in the model fusion literature, to the shadow modelling context, where the goal is to re-align th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#20102;&#38750;&#32447;&#24615;&#26799;&#24230;&#27169;&#22411;&#65288;NGM&#65289;&#65292;&#23427;&#26159;&#21487;&#35299;&#26512;&#22320;&#20351;&#29992;Taylor&#32423;&#25968;&#25299;&#23637;&#23548;&#20986;&#30340;&#38381;&#21512;&#24418;&#24335;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22320;&#29699;&#31995;&#32479;&#22797;&#26434;&#36807;&#31243;&#30340;&#23376;&#32593;&#26684;&#23610;&#24230;&#65288;SGS&#65289;&#38381;&#21512;/&#21442;&#25968;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.05014</link><description>&lt;p&gt;
&#20174;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#20013;&#23398;&#20064;&#23376;&#32593;&#26684;&#23610;&#24230;&#38381;&#21512;&#24418;&#24335;&#26041;&#31243;&#65306;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Learning Closed-form Equations for Subgrid-scale Closures from High-fidelity Data: Promises and Challenges. (arXiv:2306.05014v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#20102;&#38750;&#32447;&#24615;&#26799;&#24230;&#27169;&#22411;&#65288;NGM&#65289;&#65292;&#23427;&#26159;&#21487;&#35299;&#26512;&#22320;&#20351;&#29992;Taylor&#32423;&#25968;&#25299;&#23637;&#23548;&#20986;&#30340;&#38381;&#21512;&#24418;&#24335;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22320;&#29699;&#31995;&#32479;&#22797;&#26434;&#36807;&#31243;&#30340;&#23376;&#32593;&#26684;&#23610;&#24230;&#65288;SGS&#65289;&#38381;&#21512;/&#21442;&#25968;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21457;&#29616;&#22320;&#29699;&#31995;&#32479;&#22797;&#26434;&#36807;&#31243;&#30340;&#23376;&#32593;&#26684;&#23610;&#24230;&#65288;SGS&#65289;&#38381;&#21512;/&#21442;&#25968;&#21270;&#30340;&#21487;&#35299;&#37322;&#24615;&#38381;&#21512;&#24418;&#24335;&#26041;&#31243;&#19978;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#34920;&#29616;&#20986;&#27987;&#21402;&#20852;&#36259;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#24191;&#27867;&#30340;&#24211;&#24212;&#29992;&#36890;&#29992;&#30340;&#26041;&#31243;&#21457;&#29616;&#25216;&#26415;&#65292;&#20174;&#32463;&#36807;&#28388;&#27874;&#30340;&#20108;&#32500;&#24378;&#36843;&#28237;&#27969;&#21644;&#29790;&#21033; - &#36125;&#32435;&#24503;&#23545;&#27969;&#30340;&#30452;&#25509;&#25968;&#20540;&#27169;&#25311;&#20013;&#23398;&#20064;&#38381;&#21512;&#24418;&#24335;&#12290;&#22312;&#24120;&#35265;&#30340;&#28388;&#27874;&#22120;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#24378;&#26377;&#21147;&#22320;&#21457;&#29616;&#20102;&#21160;&#37327;&#21644;&#28909;&#36890;&#37327;&#30340;&#30456;&#21516;&#24418;&#24335;&#30340;&#38381;&#21512;&#24418;&#24335;&#12290;&#36825;&#20123;&#38381;&#21512;&#24418;&#24335;&#21462;&#20915;&#20110;&#34987;&#36807;&#28388;&#21464;&#37327;&#65288;&#36895;&#24230;&#12289;&#28201;&#24230;&#65289;&#30340;&#26799;&#24230;&#30340;&#38750;&#32447;&#24615;&#32452;&#21512;&#65292;&#20854;&#20013;&#30340;&#24120;&#25968;&#29420;&#31435;&#20110;&#27969;&#20307;/&#27969;&#21160;&#29305;&#24615;&#65292;&#20165;&#20381;&#36182;&#20110;&#36807;&#28388;&#22120;&#31867;&#22411;/&#22823;&#23567;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20123;&#38381;&#21512;&#24418;&#24335;&#26159;&#38750;&#32447;&#24615;&#26799;&#24230;&#27169;&#22411;&#65288;NGM&#65289;&#65292;&#21487;&#20197;&#20351;&#29992;Taylor&#32423;&#25968;&#23637;&#24320;&#20998;&#26512;&#22320;&#23548;&#20986;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#20351;&#29992;&#24120;&#35265;&#30340;&#65288;&#26080;&#29289;&#29702;&#20449;&#24687;&#30340;&#65289;&#26041;&#31243;&#21457;&#29616;&#31639;&#27861;&#26102;&#65292;&#26080;&#35770;&#26159;&#20160;&#20040;&#31995;&#32479;/&#29289;&#29702;&#23398;&#65292;&#21457;&#29616;&#30340;&#38381;&#21512;&#24418;&#24335;&#22987;&#32456;&#19982;Taylor&#32423;&#25968;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is growing interest in discovering interpretable, closed-form equations for subgrid-scale (SGS) closures/parameterizations of complex processes in Earth system. Here, we apply a common equation-discovery technique with expansive libraries to learn closures from filtered direct numerical simulations of 2D forced turbulence and Rayleigh-B\'enard convection (RBC). Across common filters, we robustly discover closures of the same form for momentum and heat fluxes. These closures depend on nonlinear combinations of gradients of filtered variables (velocity, temperature), with constants that are independent of the fluid/flow properties and only depend on filter type/size. We show that these closures are the nonlinear gradient model (NGM), which is derivable analytically using Taylor-series expansions. In fact, we suggest that with common (physics-free) equation-discovery algorithms, regardless of the system/physics, discovered closures are always consistent with the Taylor-series. Like 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38271;&#23614;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#35299;&#20915;&#26041;&#26696;&#65306;&#19968;&#31181;&#28789;&#27963;&#30340;&#20998;&#24067;&#23545;&#40784;&#26041;&#27861;&#65292;&#19968;&#31181;&#36719;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#21644;&#19968;&#31181;&#25193;&#20805;&#26410;&#26631;&#35760;&#38598;&#30340;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.04621</link><description>&lt;p&gt;
&#19968;&#27425;&#24615;&#23545;&#40784;&#12289;&#25552;&#28860;&#21644;&#25193;&#20805;&#25152;&#26377;&#19981;&#24179;&#34913;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Align, Distill, and Augment Everything All at Once for Imbalanced Semi-Supervised Learning. (arXiv:2306.04621v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38271;&#23614;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#35299;&#20915;&#26041;&#26696;&#65306;&#19968;&#31181;&#28789;&#27963;&#30340;&#20998;&#24067;&#23545;&#40784;&#26041;&#27861;&#65292;&#19968;&#31181;&#36719;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#21644;&#19968;&#31181;&#25193;&#20805;&#26410;&#26631;&#35760;&#38598;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#38271;&#23614;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#26102;&#65292;&#38656;&#38754;&#23545;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#24050;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#36793;&#32536;&#20998;&#24067;&#30340;&#21306;&#21035;&#65292;&#21069;&#32773;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#19988;&#21487;&#33021;&#19982;&#21518;&#32773;&#19981;&#21516;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#20123;&#37325;&#22823;&#25361;&#25112;&#12290;&#31532;&#19968;&#20010;&#25361;&#25112;&#26159;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36991;&#20813;&#20351;&#20266;&#26631;&#31614;&#23545;&#30446;&#26631;&#20998;&#24067;&#30340;&#20559;&#20506;&#65292;&#22914;&#24050;&#26631;&#35760;&#25968;&#25454;&#25110;&#24179;&#34913;&#20998;&#24067;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#26159;&#30830;&#20445;&#25512;&#29702;&#26102;&#30340;&#24179;&#34913;&#26410;&#26631;&#35760;&#20998;&#24067;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26041;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#36890;&#36807;&#28789;&#27963;&#30340;&#20998;&#24067;&#23545;&#40784;&#65292;&#36880;&#28176;&#23558;&#20998;&#31867;&#22120;&#20174;&#21160;&#24577;&#20272;&#35745;&#30340;&#26410;&#26631;&#35760;&#20808;&#39564;&#20998;&#24067;&#23545;&#40784;&#21040;&#24179;&#34913;&#20998;&#24067;&#65307;&#21033;&#29992;&#34987;&#22522;&#20110;&#38408;&#20540;&#30340;&#26041;&#27861;&#33293;&#24323;&#30340;&#20302;&#32622;&#20449;&#24230;&#20266;&#26631;&#31614;&#30340;&#36719;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#65307;&#20197;&#21450;&#19968;&#31181;&#23558;&#26631;&#35760;&#37096;&#20998;&#30340;&#36755;&#20837;&#25968;&#25454;&#25193;&#23637;&#21040;&#26410;&#26631;&#35760;&#38598;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Addressing the class imbalance in long-tailed semi-supervised learning (SSL) poses a few significant challenges stemming from differences between the marginal distributions of unlabeled data and the labeled data, as the former is often unknown and potentially distinct from the latter. The first challenge is to avoid biasing the pseudo-labels towards an incorrect distribution, such as that of the labeled data or a balanced distribution, during training. However, we still wish to ensure a balanced unlabeled distribution during inference, which is the second challenge. To address both of these challenges, we propose a three-faceted solution: a flexible distribution alignment that progressively aligns the classifier from a dynamically estimated unlabeled prior towards a balanced distribution, a soft consistency regularization that exploits underconfident pseudo-labels discarded by threshold-based methods, and a schema for expanding the unlabeled set with input data from the labeled partiti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31616;&#21333;&#24418;&#24335;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#65288;SMNN&#65289;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#26367;&#20195;&#20984;&#22810;&#38754;&#20307;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#39318;&#27425;&#24341;&#20837;&#20102; SMNN &#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.00010</link><description>&lt;p&gt;
&#31616;&#21333;&#24418;&#24335;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Explainability in Simplicial Map Neural Networks. (arXiv:2306.00010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31616;&#21333;&#24418;&#24335;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#65288;SMNN&#65289;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#26367;&#20195;&#20984;&#22810;&#38754;&#20307;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#39318;&#27425;&#24341;&#20837;&#20102; SMNN &#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#21333;&#24418;&#24335;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#65288;SMNN&#65289;&#26159;&#22522;&#20110;&#25299;&#25169;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20855;&#26377;&#26222;&#36866;&#36924;&#36817;&#33021;&#21147;&#21644;&#22312;&#36866;&#24403;&#26465;&#20214;&#19979;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#20013;&#24212;&#29992; SMNN &#23384;&#22312;&#19968;&#20123;&#29942;&#39048;&#65292;&#39318;&#20808;&#27809;&#26377;&#23450;&#20041; SMNN &#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20854;&#27425;&#23545;&#20110;&#36755;&#20837;&#25968;&#25454;&#38598;&#38656;&#35201;&#26500;&#24314;&#19968;&#20010;&#21253;&#22260;&#20984;&#22810;&#38754;&#20307;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#23376;&#38598;&#21644;&#25237;&#24433;&#21040;&#36229;&#29699;&#38754;&#30340;&#26041;&#27861;&#20316;&#20026;&#26367;&#20195;&#20984;&#22810;&#38754;&#20307;&#30340; SMNN &#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#39318;&#27425;&#24341;&#20837;&#20102; SMNN &#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simplicial map neural networks (SMNNs) are topology-based neural networks with interesting properties such as universal approximation capability and robustness to adversarial examples under appropriate conditions. However, SMNNs present some bottlenecks for their possible application in high dimensions. First, no SMNN training process has been defined so far. Second, SMNNs require the construction of a convex polytope surrounding the input dataset. In this paper, we propose a SMNN training procedure based on a support subset of the given dataset and a method based on projection to a hypersphere as a replacement for the convex polytope construction. In addition, the explainability capacity of SMNNs is also introduced for the first time in this paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#30103;&#39046;&#22495;&#8220;&#20146;&#27835;&#30103;&#8221;&#25805;&#20316;&#30340;&#38480;&#21046;&#65292;&#19988;&#32771;&#34385;&#21040;&#20102;&#25805;&#20316;&#39044;&#31639;&#30340;&#20855;&#26377;&#20449;&#24687;&#39044;&#31639;&#30340;&#24773;&#22659;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#36825;&#31181;&#31639;&#27861;&#23558;&#22312;&#32447;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#21644;&#24773;&#22659;&#36172;&#21338;&#26426;&#23398;&#20064;&#31639;&#27861;&#26377;&#26426;&#22320;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.18511</link><description>&lt;p&gt;
&#20855;&#26377;&#20449;&#24687;&#39044;&#31639;&#30340;&#24773;&#22659;&#36172;&#21338;&#26426;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Contextual Bandits with Budgeted Information Reveal. (arXiv:2305.18511v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#21307;&#30103;&#39046;&#22495;&#8220;&#20146;&#27835;&#30103;&#8221;&#25805;&#20316;&#30340;&#38480;&#21046;&#65292;&#19988;&#32771;&#34385;&#21040;&#20102;&#25805;&#20316;&#39044;&#31639;&#30340;&#20855;&#26377;&#20449;&#24687;&#39044;&#31639;&#30340;&#24773;&#22659;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#36825;&#31181;&#31639;&#27861;&#23558;&#22312;&#32447;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#21644;&#24773;&#22659;&#36172;&#21338;&#26426;&#23398;&#20064;&#31639;&#27861;&#26377;&#26426;&#22320;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#22659;&#36172;&#21338;&#26426;&#31639;&#27861;&#24120;&#29992;&#20110;&#25512;&#33616;&#20010;&#24615;&#21270;&#30340;&#21307;&#30103;&#22788;&#29702;&#26041;&#24335;&#65292;&#20294;&#22312;&#23454;&#38469;&#25805;&#20316;&#20013;&#65292;&#20026;&#20445;&#35777;&#27835;&#30103;&#25928;&#26524;&#65292;&#21307;&#29983;&#36890;&#24120;&#38656;&#35201;&#35201;&#27714;&#24739;&#32773;&#37319;&#21462;&#27809;&#26377;&#30452;&#25509;&#22909;&#22788;&#30340;&#8220;&#20146;&#27835;&#30103;&#8221;&#25805;&#20316;&#65292;&#32780;&#19988;&#20020;&#24202;&#21307;&#29983;&#30340;&#25805;&#20316;&#39044;&#31639;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26377;&#25928;&#32467;&#21512;&#20102;&#20004;&#31181;&#31639;&#27861;&#26041;&#27861;&#20043;&#38271;&#65306;1&#65289;&#19968;&#20010;&#20915;&#23450;&#26368;&#20339;&#26102;&#26426;&#19982;&#24739;&#32773;&#32852;&#31995;&#30340;&#22312;&#32447;&#21407;&#22987;-&#23545;&#20598;&#65288;primal-dual&#65289;&#31639;&#27861;&#65292;2&#65289;&#29992;&#20110;&#21521;&#24739;&#32773;&#25552;&#20379;&#20010;&#24615;&#21270;&#27835;&#30103;&#30340;&#24773;&#22659;&#36172;&#21338;&#26426;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#20855;&#26377;&#20122;&#32447;&#24615;&#30340;&#22238;&#24402;&#30028;&#38480;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual bandit algorithms are commonly used in digital health to recommend personalized treatments. However, to ensure the effectiveness of the treatments, patients are often requested to take actions that have no immediate benefit to them, which we refer to as pro-treatment actions. In practice, clinicians have a limited budget to encourage patients to take these actions and collect additional information. We introduce a novel optimization and learning algorithm to address this problem. This algorithm effectively combines the strengths of two algorithmic approaches in a seamless manner, including 1) an online primal-dual algorithm for deciding the optimal timing to reach out to patients, and 2) a contextual bandit learning algorithm to deliver personalized treatment to the patient. We prove that this algorithm admits a sub-linear regret bound. We illustrate the usefulness of this algorithm on both synthetic and real-world data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#21508;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#21033;&#29992;&#20219;&#20309;&#22238;&#24402;&#22120;&#26816;&#27979;&#25968;&#20540;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#20540;&#19982;&#33258;&#28982;&#25968;&#25454;&#27874;&#21160;&#65292;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#30495;&#27491;&#30340;&#24322;&#24120;&#21644;&#33258;&#28982;&#25968;&#25454;&#27874;&#21160;&#12290;</title><link>http://arxiv.org/abs/2305.16583</link><description>&lt;p&gt;
&#36890;&#36807;&#20219;&#24847;&#22238;&#24402;&#27169;&#22411;&#26816;&#27979;&#25968;&#20540;&#25968;&#25454;&#20013;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting Errors in Numerical Data via any Regression Model. (arXiv:2305.16583v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16583
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#21508;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#21033;&#29992;&#20219;&#20309;&#22238;&#24402;&#22120;&#26816;&#27979;&#25968;&#20540;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#20540;&#19982;&#33258;&#28982;&#25968;&#25454;&#27874;&#21160;&#65292;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#30495;&#27491;&#30340;&#24322;&#24120;&#21644;&#33258;&#28982;&#25968;&#25454;&#27874;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#22256;&#25200;&#30528;&#35768;&#22810;&#25968;&#20540;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#25968;&#25454;&#35760;&#24405;&#30340;&#20540;&#21487;&#33021;&#30001;&#20110;&#38169;&#35823;&#30340;&#20256;&#24863;&#22120;&#12289;&#25968;&#25454;&#36755;&#20837;/&#22788;&#29702;&#38169;&#35823;&#25110;&#19981;&#23436;&#32654;&#30340;&#20154;&#31867;&#20272;&#35745;&#31561;&#21407;&#22240;&#32780;&#26080;&#27861;&#21305;&#37197;&#30495;&#23454;&#30340;&#24213;&#23618;&#20540;&#12290;&#25105;&#20204;&#32771;&#34385;&#20272;&#35745;&#27839;&#25968;&#20540;&#21015;&#21738;&#20123;&#25968;&#25454;&#20540;&#26159;&#19981;&#27491;&#30830;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#20219;&#20309;&#22238;&#24402;&#22120;&#65288;&#21363;&#22522;&#20110;&#25968;&#25454;&#38598;&#20013;&#30340;&#20854;&#20182;&#21464;&#37327;&#26469;&#39044;&#27979;&#35813;&#21015;&#20540;&#30340;&#32479;&#35745;&#23398;&#25110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65289;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#36890;&#36807;&#32771;&#34385;&#21508;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21306;&#20998;&#20102;&#30495;&#27491;&#30340;&#24322;&#24120;&#21644;&#33258;&#28982;&#25968;&#25454;&#27874;&#21160;&#65292;&#26465;&#20214;&#26159;&#26377;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#34920;&#26126;&#20854;&#20182;&#26041;&#27861;&#65288;&#22914;&#31526;&#21512;&#24615;&#25512;&#26029;&#65289;&#38590;&#20197;&#26816;&#27979;&#38169;&#35823;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#35823;&#24046;&#26816;&#27979;&#22522;&#20934;&#65292;&#28041;&#21450; 5 &#20010;&#20855;&#26377;&#30495;&#23454;&#19990;&#30028;&#25968;&#23383;&#38169;&#35823;&#30340;&#22238;&#24402;&#25968;&#25454;&#38598;&#65288;&#23545;&#20110;&#20854;&#20013;&#30340;&#30495;&#23454;&#20540;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Noise plagues many numerical datasets, where the recorded values in the data may fail to match the true underlying values due to reasons including: erroneous sensors, data entry/processing mistakes, or imperfect human estimates. Here we consider estimating \emph{which} data values are incorrect along a numerical column. We present a model-agnostic approach that can utilize \emph{any} regressor (i.e.\ statistical or machine learning model) which was fit to predict values in this column based on the other variables in the dataset. By accounting for various uncertainties, our approach distinguishes between genuine anomalies and natural data fluctuations, conditioned on the available information in the dataset. We establish theoretical guarantees for our method and show that other approaches like conformal inference struggle to detect errors. We also contribute a new error detection benchmark involving 5 regression datasets with real-world numerical errors (for which the true values are al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#21512;&#24182;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#8220;ZipIt&#65281;&#8221;&#12290;</title><link>http://arxiv.org/abs/2305.03053</link><description>&lt;p&gt;
ZipIt&#65281;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#21512;&#24182;&#19981;&#21516;&#20219;&#21153;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ZipIt! Merging Models from Different Tasks without Training. (arXiv:2305.03053v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#21512;&#24182;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#8220;ZipIt&#65281;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20856;&#22411;&#30340;&#28145;&#24230;&#35270;&#35273;&#35782;&#21035;&#27169;&#22411;&#33021;&#22815;&#25191;&#34892;&#23427;&#20204;&#32463;&#36807;&#35757;&#32451;&#30340;&#21333;&#19968;&#20219;&#21153;&#12290;&#26412;&#25991;&#35299;&#20915;&#23558;&#23436;&#20840;&#19981;&#21516;&#30340;&#12289;&#27599;&#20010;&#35299;&#20915;&#19968;&#20010;&#29420;&#31435;&#20219;&#21153;&#30340;&#27169;&#22411;&#21512;&#24182;&#25104;&#19968;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#26497;&#20854;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#20197;&#21069;&#30340;&#27169;&#22411;&#21512;&#24182;&#24037;&#20316;&#23558;&#19968;&#20010;&#27169;&#22411;&#32622;&#25442;&#21040;&#21478;&#19968;&#20010;&#27169;&#22411;&#30340;&#31354;&#38388;&#20013;&#65292;&#20877;&#23558;&#23427;&#20204;&#30456;&#21152;&#12290;&#34429;&#28982;&#36825;&#23545;&#20110;&#22312;&#21516;&#19968;&#20010;&#20219;&#21153;&#19978;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#36215;&#20316;&#29992;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#26410;&#33021;&#32771;&#34385;&#21040;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;ZipIt&#65281;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#24182;&#30456;&#21516;&#32467;&#26500;&#30340;&#20004;&#20010;&#20219;&#24847;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#31181;&#31616;&#21333;&#30340;&#31574;&#30053;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#32771;&#34385;&#21040;&#22312;&#27169;&#22411;&#20043;&#38388;&#27809;&#26377;&#20849;&#20139;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#21512;&#24182;&#38382;&#39064;&#25193;&#23637;&#21040;&#36824;&#20801;&#35768;&#21512;&#24182;&#27599;&#20010;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#65292;&#23450;&#20041;&#19968;&#20010;&#36890;&#29992;&#30340;&#8220;zip&#8221;&#25805;&#20316;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#28155;&#21152;&#25903;&#25345;&#37096;&#20998;&#21387;&#32553;&#27169;&#22411;&#30340;&#21151;&#33021;&#65292;&#30452;&#21040;&#29305;&#23450;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
Typical deep visual recognition models are capable of performing the one task they were trained on. In this paper, we tackle the extremely difficult problem of combining completely distinct models with different initializations, each solving a separate task, into one multi-task model without any additional training. Prior work in model merging permutes one model to the space of the other then adds them together. While this works for models trained on the same task, we find that this fails to account for the differences in models trained on disjoint tasks. Thus, we introduce "ZipIt!", a general method for merging two arbitrary models of the same architecture that incorporates two simple strategies. First, in order to account for features that aren't shared between models, we expand the model merging problem to additionally allow for merging features within each model by defining a general "zip" operation. Second, we add support for partially zipping the models up until a specified layer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#21644;&#20998;&#26512;&#21534;&#22124;&#27963;&#24615;&#20197;&#35780;&#20272;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#12290;&#27969;&#31243;&#21487;&#20197;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25968;&#25454;&#36136;&#37327;&#39564;&#35777;&#21644;&#21487;&#35299;&#37322;&#30340;&#32454;&#32990;&#20998;&#21106;&#27169;&#22359;&#12290;</title><link>http://arxiv.org/abs/2304.13764</link><description>&lt;p&gt;
&#25581;&#31034;&#24040;&#22124;&#32454;&#32990;&#21534;&#22124;&#20316;&#29992;&#65306;&#29992;&#20110;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#20998;&#26512;&#30340;&#21487;&#25193;&#23637;&#21644;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Phagocytosis Unveiled: A Scalable and Interpretable Deep learning Framework for Neurodegenerative Disease Analysis. (arXiv:2304.13764v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#21644;&#20998;&#26512;&#21534;&#22124;&#27963;&#24615;&#20197;&#35780;&#20272;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#12290;&#27969;&#31243;&#21487;&#20197;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25968;&#25454;&#36136;&#37327;&#39564;&#35777;&#21644;&#21487;&#35299;&#37322;&#30340;&#32454;&#32990;&#20998;&#21106;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#21160;&#24577;&#26080;&#26579;&#33394;&#32454;&#32990;&#30340;&#21534;&#22124;&#20316;&#29992;&#23545;&#20110;&#35780;&#20272;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#30456;&#34924;&#26174;&#24494;&#38236;&#35270;&#39057;&#26102;&#65292;&#27979;&#37327;&#24555;&#36895;&#32454;&#32990;&#30456;&#20114;&#20316;&#29992;&#21644;&#21306;&#20998;&#32454;&#32990;&#19982;&#32972;&#26223;&#20351;&#24471;&#36825;&#39033;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23436;&#20840;&#33258;&#21160;&#21270;&#12289;&#21487;&#25193;&#23637;&#21644;&#22810;&#21151;&#33021;&#30340;&#23454;&#26102;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#21644;&#20998;&#26512;&#21534;&#22124;&#27963;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27969;&#31243;&#21487;&#20197;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25968;&#25454;&#36136;&#37327;&#39564;&#35777;&#27169;&#22359;&#20197;&#25269;&#28040;&#21487;&#33021;&#30340;&#26174;&#24494;&#38236;&#36816;&#21160;&#21644;&#24103;&#27169;&#31946;&#31561;&#25200;&#21160;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#32454;&#32990;&#20998;&#21106;&#27169;&#22359;&#65292;&#20197;&#25913;&#21892;&#19982;&#40657;&#21283;&#23376;&#31639;&#27861;&#30456;&#27604;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#36825;&#21253;&#25324;&#20004;&#20010;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#33021;&#21147;&#65306;&#35270;&#35273;&#35828;&#26126;&#21644;&#27169;&#22411;&#31616;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#19981;&#26159;&#39640;&#24615;&#33021;&#30340;&#23545;&#31435;&#38754;&#65292;&#32780;&#26159;&#25552;&#20379;&#24517;&#35201;&#30340;&#28145;&#24230;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantifying the phagocytosis of dynamic, unstained cells is essential for evaluating neurodegenerative diseases. However, measuring rapid cell interactions and distinguishing cells from backgrounds make this task challenging when processing time-lapse phase-contrast video microscopy. In this study, we introduce a fully automated, scalable, and versatile realtime framework for quantifying and analyzing phagocytic activity. Our proposed pipeline can process large data-sets and includes a data quality verification module to counteract potential perturbations such as microscope movements and frame blurring. We also propose an explainable cell segmentation module to improve the interpretability of deep learning methods compared to black-box algorithms. This includes two interpretable deep learning capabilities: visual explanation and model simplification. We demonstrate that interpretability in deep learning is not the opposite of high performance, but rather provides essential deep learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;K&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#21464;&#20307;&#65292;&#21487;&#20197;&#30830;&#20445;&#26368;&#36817;&#37051;&#23621;&#30830;&#23454;&#25509;&#36817;&#26410;&#26631;&#35760;&#26679;&#26412;&#65292;&#24182;&#22312;&#36807;&#31243;&#20013;&#25214;&#21040;K&#20540;&#12290;&#19982;&#26631;&#20934;KNN&#30456;&#27604;&#65292;&#35813;&#31639;&#27861;&#22312;&#23460;&#20869;&#25351;&#32441;&#23450;&#20301;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.10151</link><description>&lt;p&gt;
&#28789;&#27963;&#30340;K&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#65306;&#25512;&#23548;&#21644;&#22312;&#22522;&#20110;&#31163;&#23376;&#36801;&#31227;&#35889;&#30340;&#23460;&#20869;&#23450;&#20301;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flexible K Nearest Neighbors Classifier: Derivation and Application for Ion-mobility Spectrometry-based Indoor Localization. (arXiv:2304.10151v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;K&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#21464;&#20307;&#65292;&#21487;&#20197;&#30830;&#20445;&#26368;&#36817;&#37051;&#23621;&#30830;&#23454;&#25509;&#36817;&#26410;&#26631;&#35760;&#26679;&#26412;&#65292;&#24182;&#22312;&#36807;&#31243;&#20013;&#25214;&#21040;K&#20540;&#12290;&#19982;&#26631;&#20934;KNN&#30456;&#27604;&#65292;&#35813;&#31639;&#27861;&#22312;&#23460;&#20869;&#25351;&#32441;&#23450;&#20301;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
K&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#24191;&#27867;&#24212;&#29992;&#20110;&#25351;&#32441;&#23450;&#20301;&#25110;&#21307;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#23427;&#22522;&#20110;K&#20010;&#26631;&#35760;&#26679;&#26412;&#65292;&#21363;&#26368;&#36817;&#37051;&#23621;&#30340;&#31867;&#25104;&#21592;&#36523;&#20221;&#65292;&#20915;&#23450;&#26410;&#26631;&#35760;&#26679;&#26412;&#30340;&#31867;&#25104;&#21592;&#36523;&#20221;&#12290;K&#30340;&#36873;&#25321;&#19968;&#30452;&#26159;&#21508;&#31181;&#30740;&#31350;&#21644;&#25552;&#20986;KNN&#21464;&#20307;&#30340;&#20027;&#39064;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#21464;&#20307;&#34987;&#35777;&#26126;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#21464;&#20307;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KNN&#21464;&#20307;&#65292;&#30830;&#20445;K&#20010;&#26368;&#36817;&#37051;&#23621;&#30830;&#23454;&#25509;&#36817;&#26410;&#26631;&#35760;&#26679;&#26412;&#65292;&#24182;&#22312;&#36807;&#31243;&#20013;&#25214;&#21040;K&#20540;&#12290;&#35813;&#31639;&#27861;&#22312;&#29702;&#35770;&#24773;&#26223;&#21644;&#22522;&#20110;&#31163;&#23376;&#36801;&#31227;&#35889;&#25351;&#32441;&#30340;&#23460;&#20869;&#23450;&#20301;&#26041;&#38754;&#36827;&#34892;&#20102;&#27979;&#35797;&#21644;&#27604;&#36739;&#12290;&#27979;&#35797;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#31639;&#27861;&#22312;&#19982;KNN&#21516;&#26679;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#27604;KNN&#26356;&#39640;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The K Nearest Neighbors (KNN) classifier is widely used in many fields such as fingerprint-based localization or medicine. It determines the class membership of unlabelled sample based on the class memberships of the K labelled samples, the so-called nearest neighbors, that are closest to the unlabelled sample. The choice of K has been the topic of various studies and proposed KNN-variants. Yet no variant has been proven to outperform all other variants. In this paper a new KNN-variant is proposed which ensures that the K nearest neighbors are indeed close to the unlabelled sample and finds K along the way. The proposed algorithm is tested and compared to the standard KNN in theoretical scenarios and for indoor localization based on ion-mobility spectrometry fingerprints. It achieves a higher classification accuracy than the KNN in the tests, while requiring having the same computational demand.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#25968;&#25454;&#22686;&#24378;&#30340;&#22312;&#32447;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#24378;&#27491;&#21017;&#21270;&#26469;&#36991;&#20813;&#23849;&#28291;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16521</link><description>&lt;p&gt;
&#22312;&#19981;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#30340;&#24773;&#20917;&#19979;&#21152;&#24378;&#27491;&#21017;&#21270;&#26469;&#38450;&#27490;&#22312;&#32447;&#28145;&#24230;&#32858;&#31867;&#20013;&#30340;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Hard Regularization to Prevent Collapse in Online Deep Clustering without Data Augmentation. (arXiv:2303.16521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16521
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#25968;&#25454;&#22686;&#24378;&#30340;&#22312;&#32447;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#24378;&#27491;&#21017;&#21270;&#26469;&#36991;&#20813;&#23849;&#28291;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#28145;&#24230;&#32858;&#31867;&#26159;&#25351;&#32852;&#21512;&#20351;&#29992;&#29305;&#24449;&#25552;&#21462;&#32593;&#32476;&#21644;&#32858;&#31867;&#27169;&#22411;&#65292;&#20197;&#23558;&#27599;&#20010;&#26032;&#25968;&#25454;&#28857;&#25110;&#25209;&#22788;&#29702;&#20998;&#37197;&#21040;&#32858;&#31867;&#26631;&#31614;&#20013;&#12290;&#23613;&#31649;&#27604;&#31163;&#32447;&#26041;&#27861;&#26356;&#24555;&#36895;&#21644;&#26356;&#28789;&#27963;&#65292;&#20294;&#22312;&#32447;&#32858;&#31867;&#24456;&#23481;&#26131;&#36798;&#21040;&#23849;&#28291;&#35299;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#23558;&#25152;&#26377;&#36755;&#20837;&#26144;&#23556;&#21040;&#21516;&#19968;&#28857;&#65292;&#24182;&#23558;&#25152;&#26377;&#36755;&#20837;&#25918;&#20837;&#21333;&#20010;&#32858;&#31867;&#20013;&#12290;&#29616;&#26377;&#25104;&#21151;&#27169;&#22411;&#37319;&#29992;&#20102;&#21508;&#31181;&#25216;&#26415;&#26469;&#36991;&#20813;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#38656;&#35201;&#25968;&#25454;&#22686;&#24378;&#25110;&#26088;&#22312;&#20351;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#32858;&#31867;&#30340;&#24179;&#22343;&#36719;&#20998;&#37197;&#30456;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#23545;&#30828;&#20998;&#37197;&#36827;&#34892;&#20102;&#35268;&#21017;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#23548;&#20986;&#19968;&#20010;&#30452;&#35266;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#21487;&#20197;&#30452;&#25509;&#21253;&#21547;&#22312;&#32534;&#30721;&#22120;&#32593;&#32476;&#30340;&#35757;&#32451;&#20013;&#12290;&#22312;&#22235;&#20010;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126;&#23427;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#21152;&#31283;&#23450;&#22320;&#36991;&#20813;&#20102;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online deep clustering refers to the joint use of a feature extraction network and a clustering model to assign cluster labels to each new data point or batch as it is processed. While faster and more versatile than offline methods, online clustering can easily reach the collapsed solution where the encoder maps all inputs to the same point and all are put into a single cluster. Successful existing models have employed various techniques to avoid this problem, most of which require data augmentation or which aim to make the average soft assignment across the dataset the same for each cluster. We propose a method that does not require data augmentation, and that, differently from existing methods, regularizes the hard assignments. Using a Bayesian framework, we derive an intuitive optimization objective that can be straightforwardly included in the training of the encoder network. Tested on four image datasets, we show that it consistently avoids collapse more robustly than other method
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23481;&#26131;&#23398;&#20064;&#30340;&#26679;&#26412;&#23545;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#36215;&#21040;&#26368;&#22823;&#30340;&#20316;&#29992;&#65292;&#36825;&#26377;&#21161;&#20110;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#65292;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.09195</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#30340;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#65306;&#26131;&#20110;&#23398;&#20064;&#30340;&#26679;&#26412;&#36215;&#21040;&#26368;&#22823;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Contrastive Self-supervised Learning: Easy Examples Contribute the Most. (arXiv:2302.09195v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09195
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23481;&#26131;&#23398;&#20064;&#30340;&#26679;&#26412;&#23545;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#36215;&#21040;&#26368;&#22823;&#30340;&#20316;&#29992;&#65292;&#36825;&#26377;&#21161;&#20110;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#65292;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20174;&#22823;&#37327;&#30340;&#26080;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#34920;&#31034;&#12290;&#38543;&#30528;&#25968;&#25454;&#38598;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#65292;&#35782;&#21035;&#23545;&#23398;&#20064;&#27492;&#31867;&#34920;&#31034;&#26368;&#26377;&#29992;&#30340;&#31034;&#20363;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;SSL&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;SSL&#30340;&#20215;&#20540;&#22914;&#20309;&#37327;&#21270;&#19968;&#30452;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#35777;&#26126;&#22312;&#26399;&#26395;&#24847;&#20041;&#19979;&#65292;&#23545;&#27604;SSL&#20013;&#23545;&#23398;&#20064;&#20570;&#20986;&#26368;&#22823;&#36129;&#29486;&#30340;&#31034;&#20363;&#26159;&#20855;&#26377;&#26368;&#30456;&#20284;&#25968;&#25454;&#22686;&#24378;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#23376;&#38598;&#30340;SSL&#30340;&#24191;&#20041;&#24615;&#33021;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23545;SSL&#20570;&#20986;&#26368;&#22823;&#36129;&#29486;&#30340;&#23376;&#38598;&#26159;&#23545;&#30417;&#30563;&#23398;&#20064;&#20570;&#20986;&#26368;&#23567;&#36129;&#29486;&#30340;&#23376;&#38598;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23376;&#38598;&#22312;CIFAR100&#12289;CIFAR&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#38543;&#26426;&#23376;&#38598;3%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) learns high-quality representations from large pools of unlabeled training data. As datasets grow larger, it becomes crucial to identify the examples that contribute the most to learning such representations. This enables efficient SSL by reducing the volume of data required for learning high-quality representations. Nevertheless, quantifying the value of examples for SSL has remained an open question. In this work, we address this for the first time, by proving that examples that contribute the most to contrastive SSL are those that have the most similar augmentations to other examples, in expectation. We provide rigorous guarantees for the generalization performance of SSL on such subsets. Empirically, we discover, perhaps surprisingly, the subsets that contribute the most to SSL are those that contribute the least to supervised learning. Through extensive experiments, we show that our subsets outperform random subsets by more than 3% on CIFAR100, CIFAR
&lt;/p&gt;</description></item><item><title>&#24322;&#26500;&#35270;&#35273;&#28145;&#24230;&#32593;&#32476;&#31038;&#21306;&#20013;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#21487;&#20197;&#33258;&#25105;&#30417;&#30563;&#22320;&#24320;&#21457;&#20986;&#20849;&#20139;&#21327;&#35758;&#65292;&#20197;&#25351;&#20195;&#19968;&#32452;&#30446;&#26631;&#20013;&#30340;&#30446;&#26631;&#23545;&#35937;&#65292;&#24182;&#21487;&#29992;&#20110;&#27807;&#36890;&#19981;&#21516;&#31890;&#24230;&#30340;&#26410;&#30693;&#23545;&#35937;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2302.08913</link><description>&lt;p&gt;
&#24322;&#26500;&#35270;&#35273;&#28145;&#24230;&#32593;&#32476;&#31038;&#21306;&#20013;&#30340;&#25351;&#20195;&#24615;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Referential communication in heterogeneous communities of pre-trained visual deep networks. (arXiv:2302.08913v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08913
&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#35270;&#35273;&#28145;&#24230;&#32593;&#32476;&#31038;&#21306;&#20013;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#21487;&#20197;&#33258;&#25105;&#30417;&#30563;&#22320;&#24320;&#21457;&#20986;&#20849;&#20139;&#21327;&#35758;&#65292;&#20197;&#25351;&#20195;&#19968;&#32452;&#30446;&#26631;&#20013;&#30340;&#30446;&#26631;&#23545;&#35937;&#65292;&#24182;&#21487;&#29992;&#20110;&#27807;&#36890;&#19981;&#21516;&#31890;&#24230;&#30340;&#26410;&#30693;&#23545;&#35937;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#39044;&#35757;&#32451;&#22270;&#20687;&#22788;&#29702;&#31070;&#32463;&#32593;&#32476;&#34987;&#23884;&#20837;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#25110;&#26426;&#22120;&#20154;&#31561;&#33258;&#20027;&#20195;&#29702;&#20013;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#22312;&#23427;&#20204;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#31995;&#32479;&#22914;&#20309;&#30456;&#20114;&#20043;&#38388;&#36827;&#34892;&#27807;&#36890;&#20197;&#20102;&#35299;&#21608;&#22260;&#30340;&#19990;&#30028;&#12290;&#20316;&#20026;&#26397;&#30528;&#36825;&#20010;&#26041;&#21521;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#22312;&#19968;&#32452;&#24322;&#26500;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#32593;&#32476;&#31038;&#21306;&#20013;&#36827;&#34892;"&#25351;&#20195;&#24615;&#27807;&#36890;"&#30340;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#21487;&#20197;&#33258;&#25105;&#30417;&#30563;&#22320;&#21457;&#23637;&#19968;&#31181;&#20849;&#20139;&#21327;&#35758;&#26469;&#25351;&#20195;&#19968;&#32452;&#20505;&#36873;&#30446;&#26631;&#20013;&#30340;&#30446;&#26631;&#23545;&#35937;&#12290;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#65292;&#36825;&#31181;&#20849;&#20139;&#21327;&#35758;&#20063;&#21487;&#20197;&#29992;&#26469;&#27807;&#36890;&#19981;&#21516;&#31890;&#24230;&#30340;&#20808;&#21069;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#19968;&#20010;&#26368;&#21021;&#19981;&#23646;&#20110;&#29616;&#26377;&#31038;&#21306;&#30340;&#35270;&#35273;&#32593;&#32476;&#21487;&#20197;&#36731;&#26494;&#22320;&#23398;&#20064;&#21040;&#31038;&#21306;&#30340;&#21327;&#35758;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23450;&#24615;&#21644;&#23450;&#37327;&#22320;&#30740;&#31350;&#20102;&#36825;&#31181;&#26032;&#20135;&#29983;&#30340;&#21327;&#35758;&#30340;&#23646;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large pre-trained image-processing neural networks are being embedded in autonomous agents such as self-driving cars or robots, the question arises of how such systems can communicate with each other about the surrounding world, despite their different architectures and training regimes. As a first step in this direction, we systematically explore the task of \textit{referential communication} in a community of heterogeneous state-of-the-art pre-trained visual networks, showing that they can develop, in a self-supervised way, a shared protocol to refer to a target object among a set of candidates. This shared protocol can also be used, to some extent, to communicate about previously unseen object categories of different granularity. Moreover, a visual network that was not initially part of an existing community can learn the community's protocol with remarkable ease. Finally, we study, both qualitatively and quantitatively, the properties of the emergent protocol, providing some evi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#20284;&#28982;&#20998;&#25968;&#65288;FLS&#65289;&#35780;&#20272;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#26679;&#26412;&#30340;&#26032;&#39062;&#24615;&#12289;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#25351;&#26631;&#65292;&#24182;&#33021;&#22815;&#26816;&#27979;&#20986;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.04440</link><description>&lt;p&gt;
&#20351;&#29992;&#26679;&#26412;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Feature Likelihood Score: Evaluating Generalization of Generative Models Using Samples. (arXiv:2302.04440v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#20284;&#28982;&#20998;&#25968;&#65288;FLS&#65289;&#35780;&#20272;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#26679;&#26412;&#30340;&#26032;&#39062;&#24615;&#12289;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#25351;&#26631;&#65292;&#24182;&#33021;&#22815;&#26816;&#27979;&#20986;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#65292;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#32500;&#12289;&#22797;&#26434;&#21644;&#29031;&#29255;&#33324;&#36924;&#30495;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#26041;&#27861;&#20173;&#28982;&#19981;&#23436;&#20840;&#65306;&#26631;&#20934;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#25351;&#26631;&#24182;&#19981;&#24635;&#26159;&#36866;&#29992;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#20063;&#24456;&#23569;&#19982;&#24863;&#30693;&#20445;&#30495;&#24230;&#30456;&#20851;&#65292;&#32780;&#22522;&#20110;&#26679;&#26412;&#30340;&#25351;&#26631;&#65288;&#22914;FID&#65289;&#23545;&#36807;&#25311;&#21512;&#19981;&#25935;&#24863;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#31216;&#20026;&#29305;&#24449;&#20284;&#28982;&#20998;&#25968;&#65288;FLS&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#22522;&#20110;&#26679;&#26412;&#30340;&#20998;&#25968;&#65292;&#20351;&#29992;&#23494;&#24230;&#20272;&#35745;&#26469;&#25552;&#20379;&#20840;&#38754;&#30340;&#19977;&#30456;&#35780;&#20272;&#65292;&#32771;&#34385;&#29983;&#25104;&#26679;&#26412;&#30340;&#26032;&#39062;&#24615;&#65288;&#21363;&#19982;&#35757;&#32451;&#26679;&#26412;&#19981;&#21516;&#65289;&#12289;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;FLS&#22312;&#26816;&#27979;&#36807;&#25311;&#21512;&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;&#65292;&#20808;&#21069;&#25552;&#20986;&#30340;&#24230;&#37327;&#25351;&#26631;&#26080;&#27861;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23545;&#21508;&#31181;&#22270;&#20687;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#31867;&#21035;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;FLS&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The past few years have seen impressive progress in the development of deep generative models capable of producing high-dimensional, complex, and photo-realistic data. However, current methods for evaluating such models remain incomplete: standard likelihood-based metrics do not always apply and rarely correlate with perceptual fidelity, while sample-based metrics, such as FID, are insensitive to overfitting, i.e., inability to generalize beyond the training set. To address these limitations, we propose a new metric called the Feature Likelihood Score (FLS), a parametric sample-based score that uses density estimation to provide a comprehensive trichotomic evaluation accounting for novelty (i.e., different from the training samples), fidelity, and diversity of generated samples. We empirically demonstrate the ability of FLS to identify specific overfitting problem cases, where previously proposed metrics fail. We also extensively evaluate FLS on various image datasets and model classes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22312;&#32447;&#26680;&#23398;&#20064;&#20013;&#30340;&#26032;&#31639;&#27861;&#36951;&#25022;&#30028;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#20248;&#20110;&#20197;&#21069;&#30340;&#32467;&#26524;&#12290;&#35813;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#21462;&#20915;&#20110;&#26680;&#30697;&#38453;&#29305;&#24449;&#20540;&#30340;&#34928;&#20943;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.12989</link><description>&lt;p&gt;
&#22312;&#32447;&#26680;&#23398;&#20064;&#20013;&#30340;&#25913;&#36827;&#26680;&#23545;&#40784;&#36951;&#25022;&#30028;
&lt;/p&gt;
&lt;p&gt;
Improved Kernel Alignment Regret Bound for Online Kernel Learning. (arXiv:2212.12989v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22312;&#32447;&#26680;&#23398;&#20064;&#20013;&#30340;&#26032;&#31639;&#27861;&#36951;&#25022;&#30028;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#20248;&#20110;&#20197;&#21069;&#30340;&#32467;&#26524;&#12290;&#35813;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#21462;&#20915;&#20110;&#26680;&#30697;&#38453;&#29305;&#24449;&#20540;&#30340;&#34928;&#20943;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;Hinge&#25439;&#22833;&#20989;&#25968;&#19979;&#30340;&#22312;&#32447;&#26680;&#23398;&#20064;&#65292;&#25913;&#36827;&#20102;&#26680;&#23545;&#40784;&#36951;&#25022;&#30028;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#30028;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#20248;&#20110;&#20197;&#21069;&#30340;&#32467;&#26524;&#12290;&#22914;&#26524;&#26680;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#21576;&#25351;&#25968;&#34928;&#20943;&#65292;&#21017;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#20026;$O(\ln^2{T})$&#65292;&#36951;&#25022;&#30028;&#20026;$O(\sqrt{\mathcal{A}_T})$&#12290;&#21542;&#21017;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#20026;$O(\sqrt{\mathcal{A}_TT})$&#65292;&#36951;&#25022;&#30028;&#20026;$O((\mathcal{A}_TT)^{\frac{1}{4}})$&#12290;&#25105;&#20204;&#23558;&#31639;&#27861;&#25193;&#23637;&#21040;&#25209;&#37327;&#23398;&#20064;&#65292;&#24182;&#33719;&#24471;&#20102;$O(\frac{1}{T}\sqrt{\mathbb{E}[\mathcal{A}_T]})$&#30340;&#20313;&#37327;&#39118;&#38505;&#30028;&#65292;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we improve the kernel alignment regret bound for online kernel learning in the regime of the Hinge loss function. Previous algorithm achieves a regret of $O((\mathcal{A}_TT\ln{T})^{\frac{1}{4}})$ at a computational complexity (space and per-round time) of $O(\sqrt{\mathcal{A}_TT\ln{T}})$, where $\mathcal{A}_T$ is called \textit{kernel alignment}. We propose an algorithm whose regret bound and computational complexity are better than previous results. Our results depend on the decay rate of eigenvalues of the kernel matrix. If the eigenvalues of the kernel matrix decay exponentially, then our algorithm enjoys a regret of $O(\sqrt{\mathcal{A}_T})$ at a computational complexity of $O(\ln^2{T})$. Otherwise, our algorithm enjoys a regret of $O((\mathcal{A}_TT)^{\frac{1}{4}})$ at a computational complexity of $O(\sqrt{\mathcal{A}_TT})$. We extend our algorithm to batch learning and obtain a $O(\frac{1}{T}\sqrt{\mathbb{E}[\mathcal{A}_T]})$ excess risk bound which improves the p
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#31934;&#35843;&#21487;&#20197;&#25552;&#39640;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20294;&#31934;&#35843;&#36890;&#24120;&#20250;&#20351;&#27169;&#22411;&#36807;&#24230;&#19987;&#38376;&#21270;&#65292;&#38477;&#20302;&#20102;&#20854;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#27867;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#31934;&#35843;&#26694;&#26550;ProMoT&#21487;&#20197;&#20943;&#23569;&#36825;&#31181;&#26684;&#24335;&#29305;&#21270;&#12290;</title><link>http://arxiv.org/abs/2211.00635</link><description>&lt;p&gt;
&#20004;&#38454;&#27573;LLM&#31934;&#35843;&#26041;&#27861;&#65306;&#26356;&#23569;&#29305;&#21270;&#12289;&#26356;&#22810;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Two-stage LLM Fine-tuning with Less Specialization and More Generalization. (arXiv:2211.00635v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00635
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#31934;&#35843;&#21487;&#20197;&#25552;&#39640;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20294;&#31934;&#35843;&#36890;&#24120;&#20250;&#20351;&#27169;&#22411;&#36807;&#24230;&#19987;&#38376;&#21270;&#65292;&#38477;&#20302;&#20102;&#20854;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#27867;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#31934;&#35843;&#26694;&#26550;ProMoT&#21487;&#20197;&#20943;&#23569;&#36825;&#31181;&#26684;&#24335;&#29305;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#21644;&#25552;&#31034;&#30340;&#36890;&#29992;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#22312;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#31934;&#35843;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#20854;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#31934;&#35843;&#36890;&#24120;&#20351;&#27169;&#22411;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36807;&#20110;&#19987;&#38376;&#21270;&#65292;&#24182;&#38477;&#20302;&#20102;&#20854;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#27867;&#21270;&#23398;&#20064;&#24615;&#33021;&#65292;&#36825;&#22312;&#38656;&#35201;&#22788;&#29702;&#27809;&#26377;&#31934;&#35843;&#25968;&#25454;&#30340;&#20854;&#20182;&#20219;&#21153;&#26102;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#21333;&#20219;&#21153;&#31934;&#35843;&#30830;&#23454;&#20250;&#38477;&#20302;LLM&#30340;&#27867;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#36951;&#24536;&#30340;&#19968;&#20010;&#37325;&#35201;&#21407;&#22240;&#26159;&#26684;&#24335;&#29305;&#21270;&#65292;&#21363;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#20110;&#31934;&#35843;&#20219;&#21153;&#30340;&#26684;&#24335;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#34920;&#26126;&#26684;&#24335;&#29305;&#21270;&#21457;&#29983;&#22312;&#31934;&#35843;&#30340;&#26089;&#26399;&#38454;&#27573;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prompt Tuning with MOdel Tuning (ProMoT)&#36825;&#19968;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#31934;&#35843;&#26694;&#26550;&#65292;&#21487;&#20197;&#20943;&#23569;&#26684;&#24335;&#29305;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained large language models (LLMs) are general purpose problem solvers applicable to a diverse set of tasks with prompts. They can be further improved towards a specific task by fine-tuning on a specialized dataset. However, fine-tuning usually makes the model narrowly specialized on this dataset with reduced general in-context learning performances, which is undesirable whenever the fine-tuned model needs to handle additional tasks where no fine-tuning data is available. In this work, we first demonstrate that fine-tuning on a single task indeed decreases LLMs' general in-context learning performance. We discover one important cause of such forgetting, format specialization, where the model overfits to the format of the fine-tuned task. We further show that format specialization happens at the very beginning of fine-tuning. To solve this problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet effective two-stage fine-tuning framework that reduces format special
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Meta Pattern Concern Score&#8221;&#30340;&#26032;&#22411;&#35780;&#20272;&#25351;&#26631;&#65292;&#23427;&#22522;&#20110;&#27010;&#29575;&#39044;&#27979;&#30340;&#25277;&#35937;&#34920;&#24449;&#21644;&#21487;&#35843;&#33410;&#30340;&#38408;&#20540;&#65292;&#23558;&#20154;&#31867;&#20215;&#20540;&#35266;&#24341;&#20837;&#21040;&#22810;&#20998;&#31867;&#22120;&#20013;&#65292;&#21487;&#20197;&#29992;&#20110;&#26681;&#25454;&#20154;&#31867;&#20215;&#20540;&#35266;&#24688;&#24403;&#22320;&#35780;&#20272;&#40657;&#30418;&#27169;&#22411;&#22312;&#29616;&#23454;&#20013;&#30340;&#24212;&#29992;&#25928;&#26524;&#65292;&#24182;&#19988;&#21487;&#20197;&#27604;&#36739;&#22312;&#20855;&#26377;&#19981;&#21516;&#20154;&#31867;&#20215;&#20540;&#35266;&#19979;&#20351;&#29992;&#19981;&#21516;&#20998;&#31867;&#22120;&#30340;&#21516;&#19968;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2209.06408</link><description>&lt;p&gt;
Meta Pattern Concern Score&#65306;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#22810;&#20998;&#31867;&#22120;&#35780;&#20272;&#26032;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Meta Pattern Concern Score: A Novel Evaluation Measure with Human Values for Multi-classifiers. (arXiv:2209.06408v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Meta Pattern Concern Score&#8221;&#30340;&#26032;&#22411;&#35780;&#20272;&#25351;&#26631;&#65292;&#23427;&#22522;&#20110;&#27010;&#29575;&#39044;&#27979;&#30340;&#25277;&#35937;&#34920;&#24449;&#21644;&#21487;&#35843;&#33410;&#30340;&#38408;&#20540;&#65292;&#23558;&#20154;&#31867;&#20215;&#20540;&#35266;&#24341;&#20837;&#21040;&#22810;&#20998;&#31867;&#22120;&#20013;&#65292;&#21487;&#20197;&#29992;&#20110;&#26681;&#25454;&#20154;&#31867;&#20215;&#20540;&#35266;&#24688;&#24403;&#22320;&#35780;&#20272;&#40657;&#30418;&#27169;&#22411;&#22312;&#29616;&#23454;&#20013;&#30340;&#24212;&#29992;&#25928;&#26524;&#65292;&#24182;&#19988;&#21487;&#20197;&#27604;&#36739;&#22312;&#20855;&#26377;&#19981;&#21516;&#20154;&#31867;&#20215;&#20540;&#35266;&#19979;&#20351;&#29992;&#19981;&#21516;&#20998;&#31867;&#22120;&#30340;&#21516;&#19968;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#29616;&#23454;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#65292;&#22914;&#20309;&#26681;&#25454;&#29305;&#23450;&#30340;&#20154;&#31867;&#20215;&#20540;&#35266;&#24688;&#24403;&#22320;&#35780;&#20272;&#40657;&#30418;&#27169;&#22411;&#22312;&#31038;&#21306;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#20154;&#31867;&#20215;&#20540;&#21253;&#25324;&#23545;&#19981;&#21516;&#20005;&#37325;&#31243;&#24230;&#30340;&#38169;&#35823;&#26696;&#20363;&#36827;&#34892;&#24809;&#32602;&#65292;&#24182;&#22312;&#24635;&#20307;&#24615;&#33021;&#20943;&#23569;&#29305;&#23450;&#21361;&#38505;&#26696;&#20363;&#30340;&#24773;&#20917;&#19979;&#20570;&#20986;&#22949;&#21327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Meta Pattern Concern Score&#8221;&#30340;&#26032;&#22411;&#35780;&#20272;&#25351;&#26631;&#65292;&#23427;&#22522;&#20110;&#27010;&#29575;&#39044;&#27979;&#30340;&#25277;&#35937;&#34920;&#24449;&#21644;&#21487;&#35843;&#33410;&#30340;&#38408;&#20540;&#65292;&#23558;&#20154;&#31867;&#20215;&#20540;&#35266;&#24341;&#20837;&#21040;&#22810;&#20998;&#31867;&#22120;&#20013;&#12290;&#20174;&#25216;&#26415;&#19978;&#35762;&#65292;&#25105;&#20204;&#20174;&#20004;&#31181;&#24120;&#35265;&#25351;&#26631;&#8212;&#8212;&#28151;&#28102;&#30697;&#38453;&#35780;&#20272;&#25351;&#26631;&#21644;&#25439;&#22833;&#20540;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#20013;&#23398;&#20064;&#65292;&#20351;&#25105;&#20204;&#30340;&#25351;&#26631;&#22312;&#36890;&#29992;&#20219;&#21153;&#19979;&#21516;&#26679;&#26377;&#25928;&#65292;&#20132;&#21449;&#29109;&#25439;&#22833;&#22312;&#26497;&#38480;&#24773;&#20917;&#19979;&#25104;&#20026;&#25105;&#20204;&#25351;&#26631;&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25351;&#26631;&#36824;&#21487;&#20197;&#29992;&#20110;&#27604;&#36739;&#22312;&#20855;&#26377;&#19981;&#21516;&#20154;&#31867;&#20215;&#20540;&#35266;&#19979;&#20351;&#29992;&#19981;&#21516;&#20998;&#31867;&#22120;&#30340;&#21516;&#19968;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25351;&#26631;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#20063;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
While advanced classifiers have been increasingly used in real-world safety-critical applications, how to properly evaluate the black-box models given specific human values remains a concern in the community. Such human values include punishing error cases of different severity in varying degrees and making compromises in general performance to reduce specific dangerous cases. In this paper, we propose a novel evaluation measure named Meta Pattern Concern Score based on the abstract representation of probabilistic prediction and the adjustable threshold for the concession in prediction confidence, to introduce the human values into multi-classifiers. Technically, we learn from the advantages and disadvantages of two kinds of common metrics, namely the confusion matrix-based evaluation measures and the loss values, so that our measure is effective as them even under general tasks, and the cross entropy loss becomes a special case of our measure in the limit. Besides, our measure can als
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TSFool&#30340;&#40657;&#30418;&#26041;&#27861;, &#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#38024;&#23545;RNN&#20998;&#31867;&#22120;&#30340;&#39640;&#24230;&#38590;&#20197;&#23519;&#35273;&#30340;&#23545;&#25239;&#24615;&#26102;&#38388;&#24207;&#21015;&#65292;&#22312;&#32771;&#34385;&#23545;&#25239;&#26679;&#26412;&#38590;&#20197;&#23519;&#35273;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#23545;&#25239;&#24615;&#25915;&#20987;&#25913;&#36827;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#26469;&#22686;&#24378;&#25200;&#21160;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.06388</link><description>&lt;p&gt;
TSFool: &#36890;&#36807;&#22810;&#30446;&#26631;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#29983;&#25104;&#39640;&#24230;&#38590;&#20197;&#23519;&#35273;&#30340;&#23545;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
TSFool: Crafting Highly-imperceptible Adversarial Time Series through Multi-objective Black-box Attack to Fool RNN Classifiers. (arXiv:2209.06388v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TSFool&#30340;&#40657;&#30418;&#26041;&#27861;, &#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#38024;&#23545;RNN&#20998;&#31867;&#22120;&#30340;&#39640;&#24230;&#38590;&#20197;&#23519;&#35273;&#30340;&#23545;&#25239;&#24615;&#26102;&#38388;&#24207;&#21015;&#65292;&#22312;&#32771;&#34385;&#23545;&#25239;&#26679;&#26412;&#38590;&#20197;&#23519;&#35273;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#23545;&#25239;&#24615;&#25915;&#20987;&#25913;&#36827;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#26469;&#22686;&#24378;&#25200;&#21160;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#24456;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#29616;&#26377;&#30340;&#26799;&#24230;&#25915;&#20987;&#26041;&#27861;&#22312;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19979;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#26159;&#22240;&#20026;RNN&#30340;&#24490;&#29615;&#32467;&#26500;&#38459;&#27490;&#20102;&#30452;&#25509;&#30340;&#27169;&#22411;&#24046;&#20998;&#65292;&#32780;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23545;&#25200;&#21160;&#30340;&#35270;&#35273;&#25935;&#24863;&#24615;&#25361;&#25112;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#20256;&#32479;&#23616;&#37096;&#20248;&#21270;&#30446;&#26631;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TSFool&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#29983;&#25104;&#38024;&#23545;RNN&#20998;&#31867;&#22120;&#30340;&#39640;&#24230;&#38590;&#20197;&#23519;&#35273;&#30340;&#23545;&#25239;&#24615;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#20248;&#21270;&#30446;&#26631;&#65292;&#31216;&#20026;Camouflage Coefficient&#65292;&#20174;&#31867;&#20998;&#24067;&#30340;&#35282;&#24230;&#32771;&#34385;&#23545;&#25239;&#26679;&#26412;&#30340;&#38590;&#20197;&#23519;&#35273;&#24615;&#65292;&#24182;&#30456;&#24212;&#22320;&#23558;&#23545;&#25239;&#24615;&#25915;&#20987;&#25913;&#36827;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#22686;&#24378;&#25200;&#21160;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#25670;&#33073;&#19981;&#21516;&#27169;&#22411;&#38388;&#30340;&#36716;&#31227;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#29305;&#23450;&#20110;&#27169;&#22411;&#30340;&#22238;&#36991;&#35268;&#21017;&#12290;&#22312;&#20154;&#36896;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TSFool&#21487;&#20197;&#29983;&#25104;&#39640;&#38590;&#24230;&#25915;&#20987;&#21516;&#26102;&#20445;&#25345;&#23545;&#25239;&#26679;&#26412;&#30340;&#19981;&#26131;&#34987;&#26816;&#27979;&#24615;&#65292;&#24182;&#26377;&#24456;&#39640;&#30340;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network (NN) classifiers are vulnerable to adversarial attacks. Although the existing gradient-based attacks achieve state-of-the-art performance in feed-forward NNs and image recognition tasks, they do not perform as well on time series classification with recurrent neural network (RNN) models. This is because the cyclical structure of RNN prevents direct model differentiation and the visual sensitivity of time series data to perturbations challenges the traditional local optimization objective of the adversarial attack. In this paper, a black-box method called TSFool is proposed to efficiently craft highly-imperceptible adversarial time series for RNN classifiers. We propose a novel global optimization objective named Camouflage Coefficient to consider the imperceptibility of adversarial samples from the perspective of class distribution, and accordingly refine the adversarial attack as a multi-objective optimization problem to enhance the perturbation quality. To get rid of t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23558;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#24212;&#29992;&#20110;&#32463;&#20856;&#38382;&#31572;&#21644;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2203.11155</link><description>&lt;p&gt;
&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#22312;&#32463;&#20856;&#38382;&#31572;&#21644;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Quantum Density Matrix in Classical Question Answering and Classical Image Classification. (arXiv:2203.11155v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11155
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23558;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#24212;&#29992;&#20110;&#32463;&#20856;&#38382;&#31572;&#21644;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#21487;&#34920;&#31034;&#25972;&#20010;&#37327;&#23376;&#31995;&#32479;&#30340;&#20840;&#37096;&#20449;&#24687;&#65292;&#23558;&#23494;&#24230;&#30697;&#38453;&#29992;&#20110;&#32463;&#20856;&#38382;&#31572;&#20219;&#21153;&#21487;&#20197;&#26356;&#21152;&#26377;&#25928;&#22320;&#23454;&#29616;&#38382;&#39064;&#22238;&#31572;&#12290;&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;&#26032;&#26426;&#21046;&#65292;&#20197;&#24212;&#23545;&#36755;&#20837;&#20026;&#30697;&#38453;&#30340;&#24773;&#20917;&#65292;&#24182;&#23558;&#35813;&#26426;&#21046;&#24212;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;QA&#38382;&#39064;&#30340;&#27714;&#35299;&#65292;&#21516;&#26102;&#20063;&#35777;&#26126;&#20102;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#21487;&#20197;&#22686;&#24378;&#32463;&#20856;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#29305;&#24449;&#20449;&#24687;&#21644;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26032;&#26694;&#26550;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;CNN&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum density matrix represents all the information of the entire quantum system, and novel models of meaning employing density matrices naturally model linguistic phenomena such as hyponymy and linguistic ambiguity, among others in quantum question answering tasks. Naturally, we argue that applying the quantum density matrix into classical Question Answering (QA) tasks can show more effective performance. Specifically, we (i) design a new mechanism based on Long Short-Term Memory (LSTM) to accommodate the case when the inputs are matrixes; (ii) apply the new mechanism to QA problems with Convolutional Neural Network (CNN) and gain the LSTM-based QA model with the quantum density matrix. Experiments of our new model on TREC-QA and WIKI-QA data sets show encouraging results. Similarly, we argue that the quantum density matrix can also enhance the image feature information and the relationship between the features for the classical image classification. Thus, we (i) combine density mat
&lt;/p&gt;</description></item><item><title>&#29992;&#22522;&#20110;&#27169;&#22411;&#26500;&#24314;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#36866;&#24212;&#24615;&#22320;&#35843;&#25972;&#27493;&#38271;&#21644;&#25628;&#32034;&#26041;&#21521;&#65292;&#25552;&#39640;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2111.07058</link><description>&lt;p&gt;
&#29992;&#27169;&#22411;&#26500;&#24314;&#22686;&#24378;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bolstering Stochastic Gradient Descent with Model Building. (arXiv:2111.07058v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.07058
&lt;/p&gt;
&lt;p&gt;
&#29992;&#22522;&#20110;&#27169;&#22411;&#26500;&#24314;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#36866;&#24212;&#24615;&#22320;&#35843;&#25972;&#27493;&#38271;&#21644;&#25628;&#32034;&#26041;&#21521;&#65292;&#25552;&#39640;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#21450;&#20854;&#21464;&#31181;&#26159;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#26680;&#24515;&#20248;&#21270;&#31639;&#27861;&#65292;&#33021;&#22815;&#33719;&#24471;&#33391;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#38024;&#23545;&#29305;&#23450;&#24212;&#29992;&#36827;&#34892;&#35843;&#20248;&#26102;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#36825;&#20123;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#32447;&#25628;&#32034;&#26041;&#27861;&#36845;&#20195;&#35843;&#25972;&#27493;&#38271;&#65292;&#21487;&#20197;&#38477;&#20302;&#35843;&#20248;&#36807;&#31243;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#21521;&#27493;&#27169;&#22411;&#26500;&#24314;&#30340;&#38543;&#26426;&#32447;&#25628;&#32034;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#36825;&#20010;&#27169;&#22411;&#26500;&#24314;&#27493;&#39588;&#34701;&#20837;&#20102;&#20108;&#38454;&#20449;&#24687;&#65292;&#19981;&#20165;&#21487;&#20197;&#35843;&#25972;&#27493;&#38271;&#65292;&#36824;&#21487;&#20197;&#35843;&#25972;&#25628;&#32034;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#20998;&#32452;&#65288;&#24352;&#37327;&#30340;&#23618;&#65289;&#65292;&#20026;&#27599;&#20010;&#21442;&#25968;&#32452;&#24314;&#31435;&#27169;&#22411;&#24182;&#35745;&#31639;&#26032;&#30340;&#27493;&#38271;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#23545;&#35282;&#21270;&#26041;&#27861;&#20351;&#24471;&#36873;&#25321;&#30340;&#27493;&#38271;&#26159;&#33258;&#36866;&#24212;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25910;&#25947;&#36895;&#24230;&#20998;&#26512;&#65292;
&lt;/p&gt;
&lt;p&gt;
Stochastic gradient descent method and its variants constitute the core optimization algorithms that achieve good convergence rates for solving machine learning problems. These rates are obtained especially when these algorithms are fine-tuned for the application at hand. Although this tuning process can require large computational costs, recent work has shown that these costs can be reduced by line search methods that iteratively adjust the step length. We propose an alternative approach to stochastic line search by using a new algorithm based on forward step model building. This model building step incorporates second-order information that allows adjusting not only the step length but also the search direction. Noting that deep learning model parameters come in groups (layers of tensors), our method builds its model and calculates a new step for each parameter group. This novel diagonalization approach makes the selected step lengths adaptive. We provide convergence rate analysis, a
&lt;/p&gt;</description></item></channel></rss>