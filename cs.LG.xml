<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;Python&#24211;Frame Semantic Transformer&#65292;&#35813;&#24211;&#29992;&#20110;&#26694;&#26550;&#35821;&#20041;&#35299;&#26512;&#65292;&#24182;&#22312;&#26131;&#29992;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#23454;&#29616;&#25509;&#36817;&#26368;&#26032;&#27700;&#24179;&#65292;&#21487;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;FrameNet&#35789;&#27719;&#21333;&#20803;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26469;&#25552;&#39640;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12788</link><description>&lt;p&gt;
&#24320;&#28304;&#26694;&#26550;&#35821;&#20041;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Open-source Frame Semantic Parsing. (arXiv:2303.12788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;Python&#24211;Frame Semantic Transformer&#65292;&#35813;&#24211;&#29992;&#20110;&#26694;&#26550;&#35821;&#20041;&#35299;&#26512;&#65292;&#24182;&#22312;&#26131;&#29992;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#23454;&#29616;&#25509;&#36817;&#26368;&#26032;&#27700;&#24179;&#65292;&#21487;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;FrameNet&#35789;&#27719;&#21333;&#20803;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26469;&#25552;&#39640;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26368;&#36817;&#20960;&#24180;&#26469;&#26694;&#26550;&#35821;&#20041;&#35299;&#26512;&#30340;&#26368;&#26032;&#25216;&#26415;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#32456;&#31471;&#29992;&#25143;&#26469;&#35828;&#65292;&#23558;&#26368;&#26032;&#27169;&#22411;&#24212;&#29992;&#20110;&#23454;&#36341;&#20173;&#28982;&#24456;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Frame Semantic Transformer&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;Python&#24211;&#65292;&#21487;&#20197;&#22312;&#20851;&#27880;&#26131;&#29992;&#24615;&#30340;&#21516;&#26102;&#65292;&#22312;FrameNet 1.7&#19978;&#23454;&#29616;&#25509;&#36817;&#26368;&#26032;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#22312;Propbank&#21644;FrameNet&#31034;&#20363;&#19978;&#24494;&#35843;&#30340;T5&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#65292;&#24182;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;FrameNet&#35789;&#27719;&#21333;&#20803;&#20026;T5&#25552;&#20379;&#25552;&#31034;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26469;&#25552;&#39640;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the state-of-the-art for frame semantic parsing has progressed dramatically in recent years, it is still difficult for end-users to apply state-of-the-art models in practice. To address this, we present Frame Semantic Transformer, an open-source Python library which achieves near state-of-the-art performance on FrameNet 1.7, while focusing on ease-of-use. We use a T5 model fine-tuned on Propbank and FrameNet exemplars as a base, and improve performance by using FrameNet lexical units to provide hints to T5 at inference time. We enhance robustness to real-world data by using textual data augmentations during training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#8212;&#8212;&#33707;&#29305;&#37324;&#21345;&#22810;&#31574;&#30053;&#26799;&#24230;(MPG)&#65292;&#35813;&#31639;&#27861;&#23588;&#20854;&#22312;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#31361;&#20986;&#65292;&#33021;&#22815;&#23454;&#29616;&#19968;&#31995;&#21015;&#31574;&#30053;&#30340;&#35757;&#32451;&#21644;&#23398;&#20064;&#20197;&#36798;&#21040;&#20219;&#21153;&#30340;&#26368;&#20248;&#21270;&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#25910;&#25947;&#24615;&#21644;&#20840;&#23616;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12785</link><description>&lt;p&gt;
&#29109;&#27491;&#21017;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#33707;&#29305;&#37324;&#21345;&#22810;&#31574;&#30053;&#26799;&#24230;&#65306;&#25910;&#25947;&#24615;&#19982;&#20840;&#23616;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Matryoshka Policy Gradient for Entropy-Regularized RL: Convergence and Global Optimality. (arXiv:2303.12785v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#8212;&#8212;&#33707;&#29305;&#37324;&#21345;&#22810;&#31574;&#30053;&#26799;&#24230;(MPG)&#65292;&#35813;&#31639;&#27861;&#23588;&#20854;&#22312;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#31361;&#20986;&#65292;&#33021;&#22815;&#23454;&#29616;&#19968;&#31995;&#21015;&#31574;&#30053;&#30340;&#35757;&#32451;&#21644;&#23398;&#20064;&#20197;&#36798;&#21040;&#20219;&#21153;&#30340;&#26368;&#20248;&#21270;&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#25910;&#25947;&#24615;&#21644;&#20840;&#23616;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#8212;&#8212;&#33707;&#29305;&#37324;&#21345;&#22810;&#31574;&#30053;&#26799;&#24230;(MPG)&#65292;&#22312;&#26368;&#22823;&#29109;&#24378;&#21270;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#20195;&#29702;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#38500;&#20102;&#32047;&#35745;&#22870;&#21169;&#22806;&#30340;&#29109;&#22870;&#21169;&#12290;MPG&#19982;&#26631;&#20934;PG&#30340;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#23427;&#35757;&#32451;&#19968;&#31995;&#21015;&#31574;&#30053;&#21516;&#26102;&#23398;&#20064;&#26377;&#38480;&#30340;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#38024;&#23545;&#21333;&#19968;&#30340;&#26631;&#20934;&#30446;&#26631;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#31574;&#30053;&#12290;&#23545;&#20110;softmax&#31574;&#30053;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;MPG&#30340;&#25910;&#25947;&#24615;&#21644;&#26497;&#38480;&#30340;&#20840;&#23616;&#26368;&#20248;&#24615;&#65292;&#36890;&#36807;&#35777;&#26126;MPG&#30446;&#26631;&#30340;&#21807;&#19968;&#20020;&#30028;&#28857;&#26159;&#26368;&#20248;&#31574;&#30053;&#65307;&#21363;&#20351;&#22312;&#36830;&#32493;&#32039;&#33268;&#29366;&#24577;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#32467;&#26524;&#20173;&#28982;&#25104;&#31435;&#12290;MPG&#30452;&#35266;&#12289;&#29702;&#35770;&#19978;Sound&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#26631;&#20934;&#26368;&#22823;&#29109;&#30446;&#26631;&#30340;&#26368;&#20248;&#31574;&#30053;&#21487;&#20197;&#36890;&#36807;MPG&#26694;&#26550;&#30340;&#26368;&#20248;&#31574;&#30053;&#36827;&#34892;&#20219;&#24847;&#31934;&#24230;&#30340;&#36924;&#36817;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#31574;&#30053;&#29992;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;MPG&#38750;&#24120;&#36866;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
A novel Policy Gradient (PG) algorithm, called Matryoshka Policy Gradient (MPG), is introduced and studied, in the context of max-entropy reinforcement learning, where an agent aims at maximising entropy bonuses additional to its cumulative rewards. MPG differs from standard PG in that it trains a sequence of policies to learn finite horizon tasks simultaneously, instead of a single policy for the single standard objective. For softmax policies, we prove convergence of MPG and global optimality of the limit by showing that the only critical point of the MPG objective is the optimal policy; these results hold true even in the case of continuous compact state space. MPG is intuitive, theoretically sound and we furthermore show that the optimal policy of the standard max-entropy objective can be approximated arbitrarily well by the optimal policy of the MPG framework. Finally, we justify that MPG is well suited when the policies are parametrized with neural networks and we provide an simp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; HopCPT &#30340;&#26032;&#19968;&#33268;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#19981;&#20165;&#33021;&#22815;&#22788;&#29702;&#26102;&#38388;&#32467;&#26500;&#65292;&#32780;&#19988;&#33021;&#22815;&#21033;&#29992;&#20854;&#20248;&#21183;&#65292;&#24050;&#22312;&#22810;&#31181;&#30495;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12783</link><description>&lt;p&gt;
&#22522;&#20110;&#29616;&#20195; Hopfield &#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#19968;&#33268;&#24615;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction for Time Series with Modern Hopfield Networks. (arXiv:2303.12783v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12783
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; HopCPT &#30340;&#26032;&#19968;&#33268;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#19981;&#20165;&#33021;&#22815;&#22788;&#29702;&#26102;&#38388;&#32467;&#26500;&#65292;&#32780;&#19988;&#33021;&#22815;&#21033;&#29992;&#20854;&#20248;&#21183;&#65292;&#24050;&#22312;&#22810;&#31181;&#30495;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#19968;&#33268;&#24615;&#39044;&#27979;&#26041;&#27861;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38590;&#20197;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#65292;&#22240;&#20026;&#26102;&#38388;&#24207;&#21015;&#30340;&#33258;&#30456;&#20851;&#32467;&#26500;&#36829;&#21453;&#20102;&#19968;&#33268;&#24615;&#39044;&#27979;&#25152;&#38656;&#30340;&#22522;&#26412;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; HopCPT&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110; Hopfield &#32593;&#32476;&#30340;&#26102;&#38388;&#24207;&#21015;&#19968;&#33268;&#24615;&#39044;&#27979;&#26041;&#27861;&#65292;&#19981;&#20165;&#33021;&#22815;&#24212;&#23545;&#26102;&#38388;&#32467;&#26500;&#65292;&#32780;&#19988;&#33021;&#22815;&#21033;&#29992;&#23427;&#20204;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23384;&#22312;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#20013;&#22312;&#29702;&#35770;&#19978;&#26159;&#26377;&#24456;&#22909;&#30340;&#29702;&#35770;&#22522;&#30784;&#30340;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#22312;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
To quantify uncertainty, conformal prediction methods are gaining continuously more interest and have already been successfully applied to various domains. However, they are difficult to apply to time series as the autocorrelative structure of time series violates basic assumptions required by conformal prediction. We propose HopCPT, a novel conformal prediction approach for time series that not only copes with temporal structures but leverages them. We show that our approach is theoretically well justified for time series where temporal dependencies are present. In experiments, we demonstrate that our new approach outperforms state-of-the-art conformal prediction methods on multiple real-world time series datasets from four different domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;ChatGPT&#35780;&#20272;&#20013;&#38754;&#20020;&#30340;&#25968;&#25454;&#27745;&#26579;&#25361;&#25112;&#65292;&#36890;&#36807;&#20542;&#21521;&#24615;&#26816;&#27979;&#20219;&#21153;&#38416;&#36848;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#38381;&#21512;&#19988;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#30340;&#26102;&#20195;&#30830;&#20445;&#27169;&#22411;&#35780;&#20272;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12767</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#30456;&#20449;ChatGPT&#30340;&#35780;&#20272;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can we trust the evaluation on ChatGPT?. (arXiv:2303.12767v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;ChatGPT&#35780;&#20272;&#20013;&#38754;&#20020;&#30340;&#25968;&#25454;&#27745;&#26579;&#25361;&#25112;&#65292;&#36890;&#36807;&#20542;&#21521;&#24615;&#26816;&#27979;&#20219;&#21153;&#38416;&#36848;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#38381;&#21512;&#19988;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#30340;&#26102;&#20195;&#30830;&#20445;&#27169;&#22411;&#35780;&#20272;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#31532;&#19968;&#20010;&#34987;&#24191;&#27867;&#37319;&#32435;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23637;&#31034;&#20986;&#22312;&#22810;&#39033;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#27169;&#22411;&#30340;&#38381;&#21512;&#24615;&#20197;&#21450;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#31867;&#21453;&#39304;&#19981;&#26029;&#26356;&#26032;&#65292;&#35780;&#20272;ChatGPT&#22312;&#19981;&#21516;&#38382;&#39064;&#39046;&#22495;&#30340;&#34920;&#29616;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#37325;&#28857;&#35752;&#35770;&#20102;&#22312;ChatGPT&#30340;&#35780;&#20272;&#20013;&#23384;&#22312;&#30340;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20542;&#21521;&#24615;&#26816;&#27979;&#20219;&#21153;&#20316;&#20026;&#26696;&#20363;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22914;&#20309;&#22312;&#38381;&#21512;&#21644;&#25345;&#32493;&#35757;&#32451;&#27169;&#22411;&#30340;&#26102;&#20195;&#65292;&#36991;&#20813;&#25968;&#25454;&#27745;&#26579;&#21644;&#30830;&#20445;&#20844;&#24179;&#30340;&#27169;&#22411;&#35780;&#20272;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, the first large language model (LLM) with mass adoption, has demonstrated remarkable performance in numerous natural language tasks. Despite its evident usefulness, evaluating ChatGPT's performance in diverse problem domains remains challenging due to the closed nature of the model and its continuous updates via Reinforcement Learning from Human Feedback (RLHF). We highlight the issue of data contamination in ChatGPT evaluations, with a case study of the task of stance detection. We discuss the challenge of preventing data contamination and ensuring fair model evaluation in the age of closed and continuously trained models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;LSTM&#27169;&#22411;&#23454;&#29616;&#20102;&#23545;&#35270;&#39057;&#20250;&#35758;&#36890;&#35805;&#20013;&#20986;&#29616;&#30340;&#26102;&#38388;&#22833;&#30495;&#30340;&#24314;&#27169;&#65292;&#24182;&#25104;&#21151;&#39044;&#27979;&#20102;&#35270;&#39057;&#36136;&#37327;&#65292;&#20854;&#20013;&#27169;&#22411;&#30340;&#27599;&#24103;&#36755;&#20986;&#21487;&#20197;&#35814;&#32454;&#20998;&#26512;&#35270;&#39057;&#36136;&#37327;&#25439;&#22833;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2303.12761</link><description>&lt;p&gt;
&#32771;&#34385;&#35270;&#39057;&#20250;&#35758;&#36890;&#35805;&#20013;&#30340;&#26102;&#38388;&#22833;&#30495;&#30340;&#22522;&#20110;LSTM&#30340;&#35270;&#39057;&#36136;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
LSTM-based Video Quality Prediction Accounting for Temporal Distortions in Videoconferencing Calls. (arXiv:2303.12761v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;LSTM&#27169;&#22411;&#23454;&#29616;&#20102;&#23545;&#35270;&#39057;&#20250;&#35758;&#36890;&#35805;&#20013;&#20986;&#29616;&#30340;&#26102;&#38388;&#22833;&#30495;&#30340;&#24314;&#27169;&#65292;&#24182;&#25104;&#21151;&#39044;&#27979;&#20102;&#35270;&#39057;&#36136;&#37327;&#65292;&#20854;&#20013;&#27169;&#22411;&#30340;&#27599;&#24103;&#36755;&#20986;&#21487;&#20197;&#35814;&#32454;&#20998;&#26512;&#35270;&#39057;&#36136;&#37327;&#25439;&#22833;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35270;&#39057;&#36136;&#37327;&#27169;&#22411;&#65288;&#20363;&#22914;VMAF&#65289;&#33021;&#22815;&#36890;&#36807;&#23558;&#38477;&#36136;&#35270;&#39057;&#19982;&#21442;&#32771;&#35270;&#39057;&#36827;&#34892;&#27604;&#36739;&#65292;&#33719;&#24471;&#20986;&#33394;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;&#35270;&#39057;&#20250;&#35758;&#36890;&#35805;&#26399;&#38388;&#21457;&#29983;&#30340;&#26102;&#38388;&#22833;&#30495;&#65288;&#20363;&#22914;&#24103;&#20923;&#32467;&#25110;&#36339;&#36291;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32463;&#30001;&#32676;&#20247;&#26234;&#24935;&#26631;&#35760;&#30340;&#20027;&#35266;&#36136;&#37327;&#35780;&#20998;&#35757;&#32451;LSTM&#33258;&#21160;&#24314;&#27169;&#36825;&#31181;&#22833;&#30495;&#12290;&#25105;&#20204;&#20174;83&#31181;&#19981;&#21516;&#30340;&#32593;&#32476;&#24773;&#20917;&#19979;&#30340;&#23454;&#26102;&#35270;&#39057;&#20250;&#35758;&#20013;&#25910;&#38598;&#20102;&#35270;&#39057;&#12290;&#25105;&#20204;&#22312;&#28304;&#35270;&#39057;&#19978;&#24212;&#29992;QR&#30721;&#20316;&#20026;&#26631;&#35760;&#26469;&#21019;&#24314;&#23545;&#40784;&#30340;&#21442;&#32771;&#35270;&#39057;&#65292;&#24182;&#26681;&#25454;&#23545;&#40784;&#21521;&#37327;&#35745;&#31639;&#26102;&#38388;&#29305;&#24449;&#12290;&#32467;&#21512;VMAF&#26680;&#24515;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#23454;&#29616;&#20102;&#39564;&#35777;&#38598;&#19978;0.99&#30340;PCC&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36755;&#20986;&#27599;&#24103;&#36136;&#37327;&#65292;&#21487;&#20197;&#35814;&#32454;&#35299;&#26512;&#23548;&#33268;&#35270;&#39057;&#36136;&#37327;&#25439;&#22833;&#30340;&#21407;&#22240;&#12290;VCM&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#22312;https://github.com/micr&#19978;&#24320;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current state-of-the-art video quality models, such as VMAF, give excellent prediction results by comparing the degraded video with its reference video. However, they do not consider temporal distortions (e.g., frame freezes or skips) that occur during videoconferencing calls. In this paper, we present a data-driven approach for modeling such distortions automatically by training an LSTM with subjective quality ratings labeled via crowdsourcing. The videos were collected from live videoconferencing calls in 83 different network conditions. We applied QR codes as markers on the source videos to create aligned references and compute temporal features based on the alignment vectors. Using these features together with VMAF core features, our proposed model achieves a PCC of 0.99 on the validation set. Furthermore, our model outputs per-frame quality that gives detailed insight into the cause of video quality impairments. The VCM model and dataset are open-sourced at https://github.com/micr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#35299;&#20915;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#21363;&#24320;&#21457;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#25191;&#34892;&#30340;&#39640;&#25928;&#26080;&#30417;&#30563;&#20998;&#21106;&#65292;&#22522;&#20110;&#20316;&#32773;&#30340;&#35266;&#23519;&#65292;&#24403;&#20687;&#32032;&#26144;&#23556;&#21040;&#39640;&#32500;&#31354;&#38388;&#26102;&#65292;&#20998;&#21106;&#21487;&#20197;&#33719;&#24471;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12753</link><description>&lt;p&gt;
&#20851;&#20110;&#35774;&#22791;&#31471;&#26080;&#30417;&#30563;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
On-Device Unsupervised Image Segmentation. (arXiv:2303.12753v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#35299;&#20915;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#21363;&#24320;&#21457;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#25191;&#34892;&#30340;&#39640;&#25928;&#26080;&#30417;&#30563;&#20998;&#21106;&#65292;&#22522;&#20110;&#20316;&#32773;&#30340;&#35266;&#23519;&#65292;&#24403;&#20687;&#32032;&#26144;&#23556;&#21040;&#39640;&#32500;&#31354;&#38388;&#26102;&#65292;&#20998;&#21106;&#21487;&#20197;&#33719;&#24471;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#31361;&#30772;&#65292;&#20197;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#20998;&#21106;&#24050;&#32463;&#22312;&#35768;&#22810;&#30740;&#31350;&#20013;&#20986;&#29616;&#12290;&#23427;&#20204;&#20013;&#30340;&#22823;&#22810;&#25968;&#37117;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#65307;&#28982;&#32780;&#65292;&#20026;&#25903;&#25345;&#20998;&#21106;&#65292;&#38656;&#35201;&#27599;&#20010;&#20687;&#32032;&#30340;&#26631;&#31614;&#65292;&#36825;&#26174;&#28982;&#26159;&#26114;&#36149;&#30340;&#12290;&#22240;&#27492;&#65292;&#32570;&#20047;&#27880;&#37322;&#20998;&#21106;&#25968;&#25454;&#30340;&#38382;&#39064;&#36890;&#24120;&#23384;&#22312;&#12290;&#36830;&#32493;&#23398;&#20064;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#24335;&#65292;&#20294;&#23427;&#20173;&#28982;&#23545;&#20154;&#24037;&#26631;&#27880;&#26377;&#24456;&#39640;&#30340;&#35201;&#27714;&#12290;&#32780;&#19988;&#65292;&#38544;&#31169;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20998;&#21106;&#25968;&#25454;&#38750;&#24120;&#37325;&#35201;&#65292;&#36825;&#36827;&#19968;&#27493;&#35201;&#27714;&#36827;&#34892;&#35774;&#22791;&#31471;&#30340;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20197;&#19968;&#31181;&#26367;&#20195;&#30340;&#26041;&#24335;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65306;&#25105;&#20204;&#25552;&#20986;&#24320;&#21457;&#39640;&#25928;&#30340;&#26080;&#30417;&#30563;&#20998;&#21106;&#65292;&#21487;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#25191;&#34892;&#65292;&#32780;&#19981;&#26159;&#30417;&#30563;&#24335;&#20998;&#21106;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#24403;&#20687;&#32032;&#26144;&#23556;&#21040;&#39640;&#32500;&#31354;&#38388;&#26102;&#65292;&#20998;&#21106;&#21487;&#20197;&#33719;&#24471;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Along with the breakthrough of convolutional neural networks, learning-based segmentation has emerged in many research works. Most of them are based on supervised learning, requiring plenty of annotated data; however, to support segmentation, a label for each pixel is required, which is obviously expensive. As a result, the issue of lacking annotated segmentation data commonly exists. Continuous learning is a promising way to deal with this issue; however, it still has high demands on human labor for annotation. What's more, privacy is highly required in segmentation data for real-world applications, which further calls for on-device learning. In this paper, we aim to resolve the above issue in an alternative way: Instead of supervised segmentation, we propose to develop efficient unsupervised segmentation that can be executed on edge devices. Based on our observation that segmentation can obtain high performance when pixels are mapped to a high-dimension space, we for the first time b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#25512;&#29702;&#20013;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#38382;&#39064;&#65292;&#21457;&#29616;CLIP&#23384;&#22312;&#35823;&#26657;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#29256;&#30340;&#28201;&#24230;&#32553;&#25918;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#27599;&#20010;&#29305;&#23450;&#30340;CLIP&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.12748</link><description>&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#38646;&#26679;&#26412;&#25512;&#29702;&#20013;&#30340;&#26657;&#20934;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enabling Calibration In The Zero-Shot Inference of Large Vision-Language Models. (arXiv:2303.12748v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#25512;&#29702;&#20013;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#38382;&#39064;&#65292;&#21457;&#29616;CLIP&#23384;&#22312;&#35823;&#26657;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#29256;&#30340;&#28201;&#24230;&#32553;&#25918;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#27599;&#20010;&#29305;&#23450;&#30340;CLIP&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26657;&#20934;&#23545;&#20110;&#20445;&#35777;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#20351;&#29992;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#27492;&#22312;&#30417;&#30563;&#20998;&#31867;&#27169;&#22411;&#20013;&#23545;&#20854;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#38477;&#20302;&#35823;&#26657;&#20934;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#36827;&#34892;&#38646;&#26679;&#26412;&#25512;&#29702;&#26102;&#30340;&#26657;&#20934;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#20363;&#22914;CLIP&#12290;&#26412;&#30740;&#31350;&#34913;&#37327;&#20102;&#36328;&#30456;&#20851;&#21464;&#37327;&#65288;&#22914;&#25552;&#31034;&#65292;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#65289;&#30340;&#26657;&#20934;&#24773;&#20917;&#65292;&#24182;&#21457;&#29616;CLIP&#30340;&#38646;&#26679;&#26412;&#25512;&#29702;&#23384;&#22312;&#35823;&#26657;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#29256;&#30340;&#28201;&#24230;&#32553;&#25918;&#26041;&#27861;&#65292;&#19982;CLIP&#20316;&#20026;&#38646;&#26679;&#26412;&#25512;&#29702;&#27169;&#22411;&#30340;&#24120;&#35265;&#29992;&#20363;&#30456;&#19968;&#33268;&#65292;&#24182;&#23637;&#31034;&#20986;&#21333;&#20010;&#23398;&#20064;&#30340;&#28201;&#24230;&#20540;&#21487;&#20197;&#24191;&#27867;&#36866;&#29992;&#20110;&#27599;&#20010;&#29305;&#23450;&#30340;CLIP&#27169;&#22411;&#65288;&#30001;&#36873;&#23450;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#23450;&#20041;&#65289;&#65292;&#36328;&#19981;&#21516;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#21644;&#25552;&#31034;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calibration of deep learning models is crucial to their trustworthiness and safe usage, and as such, has been extensively studied in supervised classification models, with methods crafted to decrease miscalibration. However, there has yet to be a comprehensive study of the calibration of vision-language models that are used for zero-shot inference, like CLIP. We measure calibration across relevant variables like prompt, dataset, and architecture, and find that zero-shot inference with CLIP is miscalibrated. Furthermore, we propose a modified version of temperature scaling that is aligned with the common use cases of CLIP as a zero-shot inference model, and show that a single learned temperature generalizes for each specific CLIP model (defined by a chosen pre-training dataset and architecture) across inference dataset and prompt choice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#31574;&#30053;&#8212;&#8212;&#26080;&#30417;&#30563;&#25513;&#27169;&#24341;&#23548;&#21512;&#25104;&#65292;&#33021;&#22815;&#22312;&#26368;&#23567;&#25163;&#21160;&#20998;&#21106;&#30340;&#24773;&#20917;&#19979;&#65292;&#33719;&#24471;&#21512;&#25104;&#22270;&#20687;&#21644;&#20998;&#21106;&#65292;&#26377;&#25928;&#22320;&#36991;&#20813;&#20102;&#20998;&#21106;&#25513;&#27169;&#25968;&#37327;&#19981;&#36275;&#12289;&#26631;&#31614;&#19981;&#20934;&#30830;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12747</link><description>&lt;p&gt;
&#23569;&#21363;&#26159;&#22810;&#65306;&#26368;&#23569;&#30340;&#25163;&#21160;&#20998;&#21106;&#19979;&#65292;&#26080;&#30417;&#30563;&#25513;&#27169;&#24341;&#23548;CT&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Less is More: Unsupervised Mask-guided Annotated CT Image Synthesis with Minimum Manual Segmentations. (arXiv:2303.12747v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#31574;&#30053;&#8212;&#8212;&#26080;&#30417;&#30563;&#25513;&#27169;&#24341;&#23548;&#21512;&#25104;&#65292;&#33021;&#22815;&#22312;&#26368;&#23567;&#25163;&#21160;&#20998;&#21106;&#30340;&#24773;&#20917;&#19979;&#65292;&#33719;&#24471;&#21512;&#25104;&#22270;&#20687;&#21644;&#20998;&#21106;&#65292;&#26377;&#25928;&#22320;&#36991;&#20813;&#20102;&#20998;&#21106;&#25513;&#27169;&#25968;&#37327;&#19981;&#36275;&#12289;&#26631;&#31614;&#19981;&#20934;&#30830;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#23454;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#24037;&#20855;&#65292;&#25968;&#25454;&#21512;&#25104;&#36890;&#24120;&#20250;&#20026;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#24102;&#26469;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20026;&#21512;&#25104;&#30340;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#30456;&#24212;&#30340;&#20998;&#21106;&#25513;&#27169;&#26159;&#36153;&#21147;&#21644;&#20027;&#35266;&#30340;&#12290;&#20026;&#20102;&#33719;&#24471;&#37197;&#23545;&#30340;&#21512;&#25104;&#21307;&#23398;&#22270;&#20687;&#21644;&#20998;&#21106;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#20998;&#21106;&#25513;&#27169;&#20316;&#20026;&#21512;&#25104;&#26465;&#20214;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20998;&#21106;&#25513;&#27169;&#26465;&#20214;&#19979;&#30340;&#29983;&#25104;&#27169;&#22411;&#20173;&#28982;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#12289;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#21482;&#33021;&#25552;&#20379;&#26377;&#38480;&#30340;&#20154;&#20307;&#35299;&#21078;&#32467;&#26500;&#32422;&#26463;&#65292;&#23548;&#33268;&#22270;&#20687;&#29305;&#24449;&#19981;&#30495;&#23454;&#12290;&#27492;&#22806;&#65292;&#19981;&#21464;&#30340;&#20687;&#32032;&#32423;&#26465;&#20214;&#21487;&#33021;&#20250;&#20943;&#23569;&#21512;&#25104;&#30149;&#21464;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#25968;&#25454;&#22686;&#24378;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#31574;&#30053;&#65292;&#21363;&#26080;&#30417;&#30563;&#25513;&#27169;&#24341;&#23548;&#21512;&#25104;&#65292;&#20197;&#33719;&#24471;&#21512;&#25104;&#22270;&#20687;&#21644;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a pragmatic data augmentation tool, data synthesis has generally returned dividends in performance for deep learning based medical image analysis. However, generating corresponding segmentation masks for synthetic medical images is laborious and subjective. To obtain paired synthetic medical images and segmentations, conditional generative models that use segmentation masks as synthesis conditions were proposed. However, these segmentation mask-conditioned generative models still relied on large, varied, and labeled training datasets, and they could only provide limited constraints on human anatomical structures, leading to unrealistic image features. Moreover, the invariant pixel-level conditions could reduce the variety of synthetic lesions and thus reduce the efficacy of data augmentation. To address these issues, in this work, we propose a novel strategy for medical image synthesis, namely Unsupervised Mask (UM)-guided synthesis, to obtain both synthetic images and segmentations
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#25972;&#20307;&#23545;&#35937;&#24182;&#28789;&#27963;&#22320;&#23450;&#20301;&#21644;&#26059;&#36716;&#23545;&#35937;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#33258;&#36974;&#25377;&#21644;&#22806;&#36974;&#25377;&#12290;&#36890;&#36807;&#36845;&#20195;&#26500;&#24314;&#22810;&#20010;&#23545;&#35937;&#26469;&#25552;&#39640;&#25972;&#20307;&#23545;&#35937;&#26500;&#36896;&#30340;&#22810;&#26679;&#24615;&#65292;&#26500;&#36896;&#30340;&#23545;&#35937;&#21487;&#20197;&#22312;&#35757;&#32451;&#24103;&#20013;&#38543;&#26426;&#25918;&#32622;&#21644;&#26059;&#36716;&#12290;</title><link>http://arxiv.org/abs/2303.12743</link><description>&lt;p&gt;
DR.CPO&#65306;&#36890;&#36807;&#36845;&#20195;&#26500;&#24314;&#12289;&#38543;&#26426;&#25918;&#32622;&#21644; HPR &#36974;&#34109;&#23454;&#29616;&#30340;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#19977;&#32500;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
DR.CPO: Diversified and Realistic 3D Augmentation via Iterative Construction, Random Placement, and HPR Occlusion. (arXiv:2303.12743v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12743
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#25972;&#20307;&#23545;&#35937;&#24182;&#28789;&#27963;&#22320;&#23450;&#20301;&#21644;&#26059;&#36716;&#23545;&#35937;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#33258;&#36974;&#25377;&#21644;&#22806;&#36974;&#25377;&#12290;&#36890;&#36807;&#36845;&#20195;&#26500;&#24314;&#22810;&#20010;&#23545;&#35937;&#26469;&#25552;&#39640;&#25972;&#20307;&#23545;&#35937;&#26500;&#36896;&#30340;&#22810;&#26679;&#24615;&#65292;&#26500;&#36896;&#30340;&#23545;&#35937;&#21487;&#20197;&#22312;&#35757;&#32451;&#24103;&#20013;&#38543;&#26426;&#25918;&#32622;&#21644;&#26059;&#36716;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#24120;&#29992;&#20110;&#25913;&#36827;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#12290;&#26368;&#22522;&#26412;&#30340;&#26041;&#27861;&#21253;&#25324;&#25554;&#20837;&#22797;&#21046;&#23545;&#35937;&#21644;&#26059;&#36716;&#21644;&#32553;&#25918;&#25972;&#20010;&#35757;&#32451;&#24103;&#12290;&#20063;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#21464;&#20307;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#21487;&#33021;&#24615;&#30456;&#27604;&#30456;&#24403;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#26500;&#36896;&#25972;&#20307;&#23545;&#35937;&#65292;&#33258;&#30001;&#22320;&#23450;&#20301;&#21644;&#26059;&#36716;&#23545;&#35937;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#33258;&#36974;&#25377;&#21644;&#22806;&#36974;&#25377;&#12290;&#20026;&#20102;&#25552;&#39640;&#25972;&#20307;&#23545;&#35937;&#26500;&#36896;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#65292;&#23558;&#20174;&#29616;&#23454;&#19990;&#30028;&#35266;&#23519;&#21040;&#30340;&#22810;&#20010;&#23545;&#35937;&#38543;&#26426;&#32452;&#21512;&#25104;&#21333;&#20010;&#23545;&#35937;&#12290;&#19982;&#29616;&#26377;&#22686;&#24378;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#26500;&#36896;&#30340;&#23545;&#35937;&#21487;&#20197;&#38543;&#26426;&#25918;&#32622;&#21644;&#26059;&#36716;&#22312;&#35757;&#32451;&#24103;&#20013;&#65292;&#22240;&#20026;&#36866;&#24403;&#30340;&#36974;&#25377;&#21487;&#20197;&#21453;&#26144;&#22312;&#26368;&#32456;&#25972;&#20307;&#23545;&#35937;&#20013;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#38450;&#27490;&#36807;&#24230;&#22686;&#24378;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#23618;&#36974;&#25377;&#27010;&#29575;&#35774;&#32622;&#65292;&#36890;&#36807;&#23545;&#35937;&#30340;&#20301;&#32622;&#21644;&#22823;&#23567;&#35843;&#25972;&#36974;&#25377;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In autonomous driving, data augmentation is commonly used for improving 3D object detection. The most basic methods include insertion of copied objects and rotation and scaling of the entire training frame. Numerous variants have been developed as well. The existing methods, however, are considerably limited when compared to the variety of the real world possibilities. In this work, we develop a diversified and realistic augmentation method that can flexibly construct a whole-body object, freely locate and rotate the object, and apply self-occlusion and external-occlusion accordingly. To improve the diversity of the whole-body object construction, we develop an iterative method that stochastically combines multiple objects observed from the real world into a single object. Unlike the existing augmentation methods, the constructed objects can be randomly located and rotated in the training frame because proper occlusions can be reflected to the whole-body objects in the final step. Fina
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23581;&#35797;&#23558;&#26426;&#22120;&#23398;&#20064;&#21644;&#23439;&#35266;&#24494;&#20998;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20511;&#20197;&#25552;&#39640;&#20132;&#36890;&#27969;&#37327;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;LSTM&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24471;&#20986;&#25317;&#22581;&#24773;&#20917;&#21644;&#26410;&#26469;30&#20998;&#38047;&#20869;&#36890;&#36807;&#20256;&#24863;&#22120;&#30340;&#36710;&#36742;&#24635;&#25968;&#65292;&#36827;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#31934;&#24230;&#65292;&#24182;&#20498;&#36716;&#22522;&#30784;&#22270;&#20687;&#20197;&#24674;&#22797;&#19982;&#27969;&#37327;-&#23494;&#24230;&#20851;&#31995;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.12740</link><description>&lt;p&gt;
&#20511;&#21161;&#26426;&#22120;&#23398;&#20064;&#25552;&#39640;&#23439;&#35266;&#20132;&#36890;&#27969;&#37327;&#27169;&#22411;&#65306;&#22522;&#30784;&#22270;&#20687;&#30340;&#20498;&#36716;&#21644;&#36793;&#30028;&#26465;&#20214;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Inverting the Fundamental Diagram and Forecasting Boundary Conditions: How Machine Learning Can Improve Macroscopic Models for Traffic Flow. (arXiv:2303.12740v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23581;&#35797;&#23558;&#26426;&#22120;&#23398;&#20064;&#21644;&#23439;&#35266;&#24494;&#20998;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20511;&#20197;&#25552;&#39640;&#20132;&#36890;&#27969;&#37327;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;LSTM&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24471;&#20986;&#25317;&#22581;&#24773;&#20917;&#21644;&#26410;&#26469;30&#20998;&#38047;&#20869;&#36890;&#36807;&#20256;&#24863;&#22120;&#30340;&#36710;&#36742;&#24635;&#25968;&#65292;&#36827;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#31934;&#24230;&#65292;&#24182;&#20498;&#36716;&#22522;&#30784;&#22270;&#20687;&#20197;&#24674;&#22797;&#19982;&#27969;&#37327;-&#23494;&#24230;&#20851;&#31995;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#26032;&#26041;&#27861;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#19982;&#23439;&#35266;&#24494;&#20998;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#36710;&#36742;&#20132;&#36890;&#30340;&#20272;&#35745;&#21644;&#39044;&#27979;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#32452;&#25968;&#25454;&#65292;&#21253;&#25324;&#36890;&#36807;&#23450;&#28857;&#20256;&#24863;&#22120;&#25910;&#38598;&#30340;&#36710;&#36742;&#36890;&#37327;&#21644;&#36895;&#24230;&#25968;&#25454;&#65292;&#24182;&#25353;&#36710;&#36947;&#21644;&#36710;&#36742;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;&#21033;&#29992;&#22522;&#20110;LSTM&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#25512;&#26029;&#20986;&#20004;&#20010;&#37325;&#35201;&#20449;&#24687;&#65306;1&#65289;&#26159;&#21542;&#22312;&#20256;&#24863;&#22120;&#19979;&#20986;&#29616;&#25317;&#22581;&#65292;2&#65289;&#22312;&#26410;&#26469;30&#20998;&#38047;&#20869;&#23558;&#36890;&#36807;&#20256;&#24863;&#22120;&#30340;&#36710;&#36742;&#24635;&#25968;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#25552;&#39640;&#22522;&#20110;LWR&#30340;&#19968;&#38454;&#22810;&#31867;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#35813;&#27169;&#22411;&#25551;&#36848;&#20102;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#20132;&#36890;&#27969;&#21160;&#12290;&#31532;&#19968;&#20010;&#20449;&#24687;&#34987;&#29992;&#26469;&#20498;&#36716;&#65288;&#20985;&#24418;&#65289;&#22522;&#30784;&#22270;&#20687;&#65292;&#20174;&#32780;&#24674;&#22797;&#19982;&#35813;&#27573;&#36335;&#27573;&#30456;&#20851;&#30340;&#27969;&#37327;-&#23494;&#24230;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we aim at developing new methods to join machine learning techniques and macroscopic differential models for vehicular traffic estimation and forecast. It is well known that data-driven and model-driven approaches have (sometimes complementary) advantages and drawbacks. We consider here a dataset with flux and velocity data of vehicles moving on a highway, collected by fixed sensors and classified by lane and by class of vehicle. By means of a machine learning model based on an LSTM recursive neural network, we extrapolate two important pieces of information: 1) if congestion is appearing under the sensor, and 2) the total amount of vehicles which is going to pass under the sensor in the next future (30 min). These pieces of information are then used to improve the accuracy of an LWR-based first-order multi-class model describing the dynamics of traffic flow between sensors. The first piece of information is used to invert the (concave) fundamental diagram, thus recoveri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28508;&#31354;&#38388;&#25805;&#20316;&#20248;&#21270;CAD&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;StyleCLIP&#26469;&#36866;&#29992;&#20110;&#20307;&#32032;&#27169;&#22411;&#24418;&#24335;&#30340;CAD&#27169;&#22411;&#65292;&#33021;&#22815;&#20248;&#21270;&#23454;&#38469;CAD&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.12739</link><description>&lt;p&gt;
&#37319;&#29992;&#28508;&#31354;&#38388;&#25805;&#20316;&#26469;&#20248;&#21270;CAD&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Optimizing CAD Models with Latent Space Manipulation. (arXiv:2303.12739v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28508;&#31354;&#38388;&#25805;&#20316;&#20248;&#21270;CAD&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;StyleCLIP&#26469;&#36866;&#29992;&#20110;&#20307;&#32032;&#27169;&#22411;&#24418;&#24335;&#30340;CAD&#27169;&#22411;&#65292;&#33021;&#22815;&#20248;&#21270;&#23454;&#38469;CAD&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#28041;&#21450;&#21040;&#33258;&#21160;&#21270;&#39046;&#22495;&#20013;CAD&#27169;&#22411;&#30340;&#20248;&#21270;&#26102;&#65292;&#31070;&#32463;&#32593;&#32476;&#30446;&#21069;&#21482;&#36215;&#21040;&#20102;&#36739;&#23567;&#30340;&#20316;&#29992;&#12290;&#20248;&#21270;&#25277;&#35937;&#29305;&#24615;&#22914;&#33258;&#21160;&#21270;&#33021;&#21147;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#38590;&#34987;&#27169;&#25311;&#65292;&#23545;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#26469;&#35828;&#36807;&#20110;&#22797;&#26434;&#65292;&#32780;&#19988;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#32570;&#20047;&#25968;&#25454;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20687;StyleCLIP&#36825;&#26679;&#30340;&#21487;&#25805;&#32437;&#22270;&#20687;&#20013;&#30340;&#25277;&#35937;&#29305;&#24449;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#12290;&#23427;&#20204;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#24615;&#32593;&#32476;&#30340;&#28508;&#31354;&#38388;&#65292;&#24182;&#19988;&#22240;&#27492;&#20063;&#21487;&#20197;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;CAD&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#20063;&#36866;&#29992;&#20110;&#20248;&#21270;CAD&#38646;&#20214;&#30340;&#25277;&#35937;&#33258;&#21160;&#21270;&#30456;&#20851;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#25193;&#23637;StyleCLIP&#20197;&#36866;&#29992;&#20110;&#20307;&#32032;&#27169;&#22411;&#24418;&#24335;&#30340;CAD&#27169;&#22411;&#23454;&#29616;&#20102;&#36825;&#19968;&#28857;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;3D StyleGAN&#21644;&#33258;&#23450;&#20041;&#20998;&#31867;&#22120;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#36890;&#36807;&#20248;&#21270;&#23454;&#38469;CAD&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#33021;&#21147;&#32780;&#20855;&#22791;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
When it comes to the optimization of CAD models in the automation domain, neural networks currently play only a minor role. Optimizing abstract features such as automation capability is challenging, since they can be very difficult to simulate, are too complex for rule-based systems, and also have little to no data available for machine-learning methods. On the other hand, image manipulation methods that can manipulate abstract features in images such as StyleCLIP have seen much success. They rely on the latent space of pretrained generative adversarial networks, and could therefore also make use of the vast amount of unlabeled CAD data. In this paper, we show that such an approach is also suitable for optimizing abstract automation-related features of CAD parts. We achieved this by extending StyleCLIP to work with CAD models in the form of voxel models, which includes using a 3D StyleGAN and a custom classifier. Finally, we demonstrate the ability of our system for the optimiziation o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#36974;&#30422;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;DPPMask&#65292;&#29992;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPPs&#65289;&#26367;&#25442;&#20102;&#38543;&#26426;&#36807;&#31243;&#20197;&#20943;&#23569;&#36974;&#30422;&#21518;&#22270;&#20687;&#30340;&#35821;&#20041;&#21464;&#21270;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25913;&#21892;&#20102;&#20195;&#34920;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12736</link><description>&lt;p&gt;
DPPMask&#65306;&#22522;&#20110;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#30340;&#36974;&#30422;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DPPMask: Masked Image Modeling with Determinantal Point Processes. (arXiv:2303.12736v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#36974;&#30422;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;DPPMask&#65292;&#29992;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPPs&#65289;&#26367;&#25442;&#20102;&#38543;&#26426;&#36807;&#31243;&#20197;&#20943;&#23569;&#36974;&#30422;&#21518;&#22270;&#20687;&#30340;&#35821;&#20041;&#21464;&#21270;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25913;&#21892;&#20102;&#20195;&#34920;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36974;&#30422;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#26088;&#22312;&#37325;&#24314;&#38543;&#26426;&#36974;&#30422;&#30340;&#22270;&#20687;&#65292;&#24182;&#24050;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#20195;&#34920;&#24615;&#33021;&#12290;&#23613;&#31649;&#20855;&#26377;&#23454;&#35777;&#25928;&#26524;&#65292;&#20294;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#24573;&#30053;&#20102;&#19968;&#20010;&#37325;&#35201;&#20107;&#23454;&#65292;&#21363;&#24378;&#21046;&#27169;&#22411;&#37325;&#24314;&#36229;&#20986;&#24674;&#22797;&#33539;&#22260;&#30340;&#29289;&#20307;&#65288;&#22914;&#37027;&#20123;&#34987;&#36974;&#30422;&#30340;&#29289;&#20307;&#65289;&#26159;&#19981;&#21512;&#29702;&#30340;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#22343;&#21248;&#38543;&#26426;&#36974;&#30422;&#19981;&#21487;&#36991;&#20813;&#22320;&#20002;&#22833;&#19968;&#20123;&#20851;&#38190;&#29289;&#20307;&#24182;&#26356;&#25913;&#21407;&#22987;&#35821;&#20041;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#19968;&#31181;&#19981;&#23545;&#40784;&#38382;&#39064;&#24182;&#26368;&#32456;&#20260;&#23475;&#20102;&#20195;&#34920;&#24615;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#30830;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPPs&#65289;&#26367;&#25442;&#20026;&#38543;&#26426;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36974;&#30422;&#31574;&#30053;&#65292;&#21363;DPPMask&#65292;&#20197;&#20943;&#23569;&#36974;&#30422;&#21518;&#22270;&#20687;&#30340;&#35821;&#20041;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#23454;&#29616;&#26102;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#25105;&#20204;&#29305;&#21035;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;MIM&#26694;&#26550;&#65292;MASK&#21644;GMS&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25913;&#21892;&#20102;&#20195;&#34920;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Image Modeling (MIM) has achieved impressive representative performance with the aim of reconstructing randomly masked images. Despite the empirical success, most previous works have neglected the important fact that it is unreasonable to force the model to reconstruct something beyond recovery, such as those masked objects. In this work, we show that uniformly random masking widely used in previous works unavoidably loses some key objects and changes original semantic information, resulting in a misalignment problem and hurting the representative learning eventually. To address this issue, we augment MIM with a new masking strategy namely the DPPMask by substituting the random process with Determinantal Point Process (DPPs) to reduce the semantic change of the image after masking. Our method is simple yet effective and requires no extra learnable parameters when implemented within various frameworks. In particular, we evaluate our method on two representative MIM frameworks, MA
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;&#21644;&#25991;&#26412;&#20559;&#32622;&#22522;&#20934;MMBias&#65292;&#28085;&#30422;14&#20010;&#20154;&#21475;&#23376;&#32676;&#65292;&#24182;&#21033;&#29992;&#35813;&#22522;&#20934;&#35780;&#20272;&#20102;&#22810;&#20010;&#33258;&#25105;&#30417;&#30563;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#21253;&#25324;CLIP&#12289;ALBEF&#21644;ViLT&#65289;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#20559;&#21521;&#26576;&#20123;&#32676;&#20307;&#30340;&#26377;&#24847;&#20041;&#30340;&#20559;&#35265;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#35774;&#35745;&#30340;&#21435;&#20559;&#32622;&#26041;&#27861;&#20316;&#20026;&#21518;&#22788;&#29702;&#27493;&#39588;&#65292;&#21487;&#20197;&#20943;&#36731;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#20445;&#35777;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12734</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#20559;&#24046;&#65306;&#24341;&#20837;&#19968;&#31181;&#26694;&#26550;&#20197;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21051;&#26495;&#21360;&#35937;&#20559;&#24046;&#65292;&#36229;&#36234;&#24615;&#21035;&#21644;&#31181;&#26063;&#12290;
&lt;/p&gt;
&lt;p&gt;
MultiModal Bias: Introducing a Framework for Stereotypical Bias Assessment beyond Gender and Race in Vision Language Models. (arXiv:2303.12734v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;&#21644;&#25991;&#26412;&#20559;&#32622;&#22522;&#20934;MMBias&#65292;&#28085;&#30422;14&#20010;&#20154;&#21475;&#23376;&#32676;&#65292;&#24182;&#21033;&#29992;&#35813;&#22522;&#20934;&#35780;&#20272;&#20102;&#22810;&#20010;&#33258;&#25105;&#30417;&#30563;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#21253;&#25324;CLIP&#12289;ALBEF&#21644;ViLT&#65289;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#20559;&#21521;&#26576;&#20123;&#32676;&#20307;&#30340;&#26377;&#24847;&#20041;&#30340;&#20559;&#35265;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#35774;&#35745;&#30340;&#21435;&#20559;&#32622;&#26041;&#27861;&#20316;&#20026;&#21518;&#22788;&#29702;&#27493;&#39588;&#65292;&#21487;&#20197;&#20943;&#36731;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#20445;&#35777;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#20027;&#35757;&#32451;&#30340;&#31361;&#30772;&#20026;&#19968;&#31867;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#34429;&#28982;&#23545;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#36827;&#34892;&#20102;&#19968;&#20123;&#35843;&#26597;&#65292;&#20294;&#20027;&#35201;&#38598;&#20013;&#22312;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#19978;&#65292;&#23545;&#20110;&#20854;&#20182;&#30456;&#20851;&#32676;&#20307;&#65292;&#22914;&#23447;&#25945;&#12289;&#22269;&#31821;&#12289;&#24615;&#21462;&#21521;&#25110;&#27531;&#30142;&#20154;&#32676;&#65292;&#32473;&#20104;&#30340;&#20851;&#27880;&#36739;&#23569;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#31216;&#20026;MMBias&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#20559;&#24046;&#22522;&#20934;&#26469;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#21253;&#25324;&#32422;3800&#20010;&#22270;&#20687;&#21644;&#30701;&#35821;&#65292;&#28085;&#30422;14&#20010;&#20154;&#21475;&#23376;&#32676;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#20960;&#20010;&#33879;&#21517;&#30340;&#33258;&#25105;&#30417;&#30563;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21253;&#25324;CLIP&#12289;ALBEF&#21644;ViLT&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#26377;&#24847;&#20041;&#30340;&#20559;&#24046;&#65292;&#20559;&#21521;&#26576;&#20123;&#32676;&#20307;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#36825;&#31181;&#22823;&#35268;&#27169;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#35774;&#35745;&#30340;&#21435;&#20559;&#32622;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#21518;&#22788;&#29702;&#27493;&#39588;&#24212;&#29992;&#20110;&#20943;&#36731;&#20559;&#24046;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in self supervised training have led to a new class of pretrained vision language models. While there have been investigations of bias in multimodal models, they have mostly focused on gender and racial bias, giving much less attention to other relevant groups, such as minorities with regard to religion, nationality, sexual orientation, or disabilities. This is mainly due to lack of suitable benchmarks for such groups. We seek to address this gap by providing a visual and textual bias benchmark called MMBias, consisting of around 3,800 images and phrases covering 14 population subgroups. We utilize this dataset to assess bias in several prominent self supervised multimodal models, including CLIP, ALBEF, and ViLT. Our results show that these models demonstrate meaningful bias favoring certain groups. Finally, we introduce a debiasing method designed specifically for such large pre-trained models that can be applied as a post-processing step to mitigate bias, while p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#31526;&#21495;&#23398;&#23545;&#22270;&#20687;&#36827;&#34892;&#20462;&#25913;&#65292;&#20197;&#23637;&#29616;&#38750;&#29289;&#29702;&#12289;&#25277;&#35937;&#23646;&#24615;&#30340;&#21464;&#21270;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#31526;&#21495;&#23398;&#23646;&#24615;&#30456;&#20851;&#30340;&#28508;&#22312;&#35270;&#35273;&#35937;&#24449;&#65292;&#26377;&#21161;&#20110;&#26032;&#30340;&#35270;&#35273;&#27010;&#24565;&#20135;&#29983;&#12290;</title><link>http://arxiv.org/abs/2303.12731</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#31526;&#21495;&#23398;&#30340;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Visualizing Semiotics in Generative Adversarial Networks. (arXiv:2303.12731v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12731
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#31526;&#21495;&#23398;&#23545;&#22270;&#20687;&#36827;&#34892;&#20462;&#25913;&#65292;&#20197;&#23637;&#29616;&#38750;&#29289;&#29702;&#12289;&#25277;&#35937;&#23646;&#24615;&#30340;&#21464;&#21270;&#65292;&#24182;&#25581;&#31034;&#20102;&#19982;&#31526;&#21495;&#23398;&#23646;&#24615;&#30456;&#20851;&#30340;&#28508;&#22312;&#35270;&#35273;&#35937;&#24449;&#65292;&#26377;&#21161;&#20110;&#26032;&#30340;&#35270;&#35273;&#27010;&#24565;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#30340;&#22270;&#20687;&#21487;&#20197;&#36890;&#36807;&#8220;&#31526;&#21495;&#23398;&#8221;&#36827;&#34892;&#20462;&#25913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31867;&#20284;&#20110;&#20687;&#32032;&#12289;&#33394;&#35843;&#20043;&#31867;&#30340;&#29289;&#29702;&#23646;&#24615;&#21487;&#20197;&#34987;&#20462;&#25913;&#19968;&#26679;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20063;&#21487;&#20197;&#20462;&#25913;&#38750;&#29289;&#29702;&#12289;&#25277;&#35937;&#30340;&#23646;&#24615;&#12290;&#20363;&#22914;&#65292;&#26426;&#33329;&#20056;&#21153;&#21592;&#30340;&#21046;&#26381;&#35774;&#35745;&#21487;&#20197;&#34987;&#20462;&#25913;&#20026;&#26356;&#21152;&#8220;&#35686;&#35273;&#8221;&#65292;&#19981;&#37027;&#20040;&#8220;&#20005;&#32899;&#8221;&#65292;&#25110;&#32773;&#26356;&#21152;&#8220;&#23454;&#29992;&#8221;&#12290;&#19968;&#20010;&#25151;&#23376;&#30340;&#24418;&#24335;&#21487;&#20197;&#34987;&#20462;&#25913;&#20026;&#26356;&#21152;&#8220;&#26410;&#26469;&#24863;&#8221;&#65292;&#19968;&#36742;&#36710;&#26356;&#21152;&#8220;&#21451;&#22909;&#8221;&#65292;&#19968;&#21452;&#29699;&#38795;&#21017;&#21487;&#20197;&#34987;&#26356;&#21152;&#8220;&#37034;&#24694;&#8221;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25581;&#31034;&#20102;&#19982;&#24863;&#20852;&#36259;&#30340;&#31526;&#21495;&#23398;&#23646;&#24615;&#30456;&#20851;&#30340;&#28508;&#22312;&#35270;&#35273;&#35937;&#24449;&#65292;&#20351;&#24471;&#36825;&#19968;&#36807;&#31243;&#21487;&#20197;&#20351;&#29992;&#25277;&#35937;&#27010;&#24565;&#36827;&#34892;&#35270;&#35273;&#24418;&#24335;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36845;&#20195;&#24335;&#30340;&#65292;&#20801;&#35768;&#23545;&#23646;&#24615;&#23384;&#22312;&#31243;&#24230;&#36827;&#34892;&#25511;&#21046;&#65292;&#21487;&#20197;&#29992;&#20110;&#36741;&#21161;&#35774;&#35745;&#36807;&#31243;&#65292;&#20135;&#29983;&#26032;&#30340;&#35270;&#35273;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
We perform a set of experiments to demonstrate that images generated using a Generative Adversarial Network can be modified using 'semiotics.' We show that just as physical attributes such as the hue and saturation of an image can be modified, so too can its non-physical, abstract properties using our method. For example, the design of a flight attendant's uniform may be modified to look more 'alert,' less 'austere,' or more 'practical.' The form of a house can be modified to appear more 'futuristic,' a car more 'friendly' a pair of sneakers, 'evil.' Our method uncovers latent visual iconography associated with the semiotic property of interest, enabling a process of visual form-finding using abstract concepts. Our approach is iterative and allows control over the degree of attribute presence and can be used to aid the design process to yield emergent visual concepts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#25968;&#25454;&#39537;&#21160;&#30340;&#32768;&#26001;&#24314;&#27169;&#31995;&#32479;&#36827;&#34892;&#20102;&#22522;&#30784;&#24314;&#35774;&#65292;&#20801;&#35768;&#35843;&#26597;&#21592;&#39044;&#20808;&#26368;&#23567;&#21270;&#32768;&#26001;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#26377;&#29992;&#30340;&#22270;&#20687;&#26469;&#25910;&#38598;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.12730</link><description>&lt;p&gt;
&#38754;&#21521;&#25968;&#25454;&#39537;&#21160;&#30340;&#28023;&#27915;&#22823;&#22411;&#21160;&#29289;&#35843;&#26597;&#20013;&#30340;&#32768;&#26001;&#20998;&#31867;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Toward Data-Driven Glare Classification and Prediction for Marine Megafauna Survey. (arXiv:2303.12730v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#25968;&#25454;&#39537;&#21160;&#30340;&#32768;&#26001;&#24314;&#27169;&#31995;&#32479;&#36827;&#34892;&#20102;&#22522;&#30784;&#24314;&#35774;&#65292;&#20801;&#35768;&#35843;&#26597;&#21592;&#39044;&#20808;&#26368;&#23567;&#21270;&#32768;&#26001;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#26377;&#29992;&#30340;&#22270;&#20687;&#26469;&#25910;&#38598;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20272;&#35745;&#29289;&#31181;&#25968;&#37327;&#65292;&#21152;&#25343;&#22823;&#21271;&#22823;&#35199;&#27915;&#27700;&#22495;&#30340;&#28626;&#21361;&#29289;&#31181;&#36827;&#34892;&#31995;&#32479;&#35843;&#26597;&#65292;&#24182;&#24433;&#21709;&#30528;&#25919;&#31574;&#12290;&#26412;&#25991;&#38024;&#23545;&#25968;&#25454;&#39537;&#21160;&#30340;&#32768;&#26001;&#24314;&#27169;&#31995;&#32479;&#36827;&#34892;&#20102;&#22522;&#30784;&#24314;&#35774;&#65292;&#36825;&#23558;&#20801;&#35768;&#35843;&#26597;&#21592;&#39044;&#20808;&#26368;&#23567;&#21270;&#32768;&#26001;&#12290;&#35843;&#26597;&#21592;&#20351;&#29992;&#26816;&#27979;&#20989;&#25968;&#20272;&#35745;&#26410;&#26126;&#26174;&#30475;&#21040;&#30340;&#24040;&#22411;&#21160;&#29289;&#31181;&#32676;&#12290;&#30740;&#31350;&#30340;&#19968;&#20010;&#30446;&#26631;&#26159;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#26377;&#29992;&#30340;&#22270;&#20687;&#26469;&#25910;&#38598;&#25968;&#25454;&#65292;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#20351;&#29992;&#32768;&#26001;&#27169;&#22411;&#39044;&#27979;&#32768;&#26001;&#24182;&#20248;&#21270;&#26080;&#32768;&#26001;&#25968;&#25454;&#30340;&#25910;&#38598;&#12290;&#20026;&#26500;&#24314;&#27492;&#27169;&#22411;&#65292;&#25105;&#20204;&#21033;&#29992;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#22823;&#22411;&#25968;&#25454;&#38598;&#20351;&#29992;&#33258;&#28982;&#20266;&#26631;&#31614;&#26041;&#27861;&#20351;&#29992;&#32423;&#32852;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#36827;&#34892;&#26631;&#35760;&#12290;&#20351;&#29992;&#21453;&#23556;&#29575;&#27169;&#22411;&#65292;&#20197;&#30830;&#23450;&#24863;&#20852;&#36259;&#30340;&#29305;&#24449;&#65292;&#22635;&#20805;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#21487;&#20197;&#36827;&#34892;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Critically endangered species in Canadian North Atlantic waters are systematically surveyed to estimate species populations which influence governing policies. Due to its impact on policy, population accuracy is important. This paper lays the foundation towards a data-driven glare modelling system, which will allow surveyors to preemptively minimize glare. Surveyors use a detection function to estimate megafauna populations which are not explicitly seen. A goal of the research is to maximize useful imagery collected, to that end we will use our glare model to predict glare and optimize for glare-free data collection. To build this model, we leverage a small labelled dataset to perform semi-supervised learning. The large dataset is labelled with a Cascading Random Forest Model using a na\"ive pseudo-labelling approach. A reflectance model is used, which pinpoints features of interest, to populate our datasets which allows for context-aware machine learning models. The pseudo-labelled da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LocalEyenet&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#20165;&#23450;&#20301;&#30524;&#37096;&#21306;&#22495;&#65292;&#21487;&#20197;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#22521;&#35757;&#12290;&#35813;&#26550;&#26500;&#22312;&#22534;&#21472;&#30340;&#27801;&#28431;&#39592;&#24178;&#19978;&#23398;&#20064;&#33258;&#25105;&#27880;&#24847;&#21147;&#65292;&#24182;&#22312;&#27599;&#20010;&#27801;&#28431;&#20013;&#21512;&#24182;&#20102;&#28145;&#23618;&#32858;&#21512;&#65292;&#20197;&#26368;&#23567;&#21270;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2303.12728</link><description>&lt;p&gt;
LocalEyenet: &#29992;&#20110;&#30524;&#37096;&#23450;&#20301;&#30340;&#28145;&#24230;&#27880;&#24847;&#21147;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LocalEyenet: Deep Attention framework for Localization of Eyes. (arXiv:2303.12728v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LocalEyenet&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#29992;&#20110;&#20165;&#23450;&#20301;&#30524;&#37096;&#21306;&#22495;&#65292;&#21487;&#20197;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#22521;&#35757;&#12290;&#35813;&#26550;&#26500;&#22312;&#22534;&#21472;&#30340;&#27801;&#28431;&#39592;&#24178;&#19978;&#23398;&#20064;&#33258;&#25105;&#27880;&#24847;&#21147;&#65292;&#24182;&#22312;&#27599;&#20010;&#27801;&#28431;&#20013;&#21512;&#24182;&#20102;&#28145;&#23618;&#32858;&#21512;&#65292;&#20197;&#26368;&#23567;&#21270;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#20154;&#26426;&#30028;&#38754;&#23545;&#20110;&#29616;&#20195;&#26426;&#22120;&#30340;&#33258;&#20027;&#24615;&#21644;&#25928;&#29575;&#25552;&#39640;&#24050;&#32463;&#25104;&#20026;&#24517;&#38656;&#12290;&#22522;&#20110;&#20957;&#35270;&#30340;&#20154;&#31867;&#24178;&#39044;&#26159;&#21019;&#24314;&#30028;&#38754;&#20197;&#20943;&#23569;&#20154;&#20026;&#35823;&#24046;&#30340;&#26377;&#25928;&#21644;&#26041;&#20415;&#30340;&#36873;&#39033;&#12290;&#38754;&#37096;&#26631;&#24535;&#26816;&#27979;&#23545;&#20110;&#35774;&#35745;&#24378;&#22823;&#30340;&#20957;&#35270;&#26816;&#27979;&#31995;&#32479;&#38750;&#24120;&#20851;&#38190;&#12290;&#22522;&#20110;&#22238;&#24402;&#30340;&#26041;&#27861;&#20351;&#24471;&#23545;&#24212;&#20110;&#38754;&#37096;&#19981;&#21516;&#21306;&#22495;&#30340;&#26631;&#24535;&#29289;&#20855;&#26377;&#33391;&#22909;&#30340;&#31354;&#38388;&#23450;&#20301;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#36890;&#36807;&#24341;&#20837;&#27880;&#24847;&#21147;&#20173;&#28982;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LocalEyenet&#30340;&#28145;&#24230;&#31895;&#21040;&#32454;&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#20165;&#23450;&#20301;&#30524;&#37096;&#21306;&#22495;&#65292;&#21487;&#20197;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#22521;&#35757;&#12290; &#35813;&#27169;&#22411;&#26550;&#26500;&#22522;&#20110;&#22534;&#21472;&#30340;&#27801;&#28431;&#39592;&#24178;&#65292;&#23398;&#20064;&#29305;&#24449;&#22270;&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#65292;&#20197;&#24110;&#21161;&#20445;&#30041;&#38754;&#37096;&#22270;&#20687;&#20013;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#31354;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#27599;&#20010;&#27801;&#28431;&#20013;&#37117;&#21512;&#24182;&#20102;&#28145;&#23618;&#32858;&#21512;&#65292;&#20197;&#26368;&#23567;&#21270;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Development of human machine interface has become a necessity for modern day machines to catalyze more autonomy and more efficiency. Gaze driven human intervention is an effective and convenient option for creating an interface to alleviate human errors. Facial landmark detection is very crucial for designing a robust gaze detection system. Regression based methods capacitate good spatial localization of the landmarks corresponding to different parts of the faces. But there are still scope of improvements which have been addressed by incorporating attention.  In this paper, we have proposed a deep coarse-to-fine architecture called LocalEyenet for localization of only the eye regions that can be trained end-to-end. The model architecture, build on stacked hourglass backbone, learns the self-attention in feature maps which aids in preserving global as well as local spatial dependencies in face image. We have incorporated deep layer aggregation in each hourglass to minimize the loss of a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20998;&#24418;&#22270;&#20687;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#35270;&#35273;&#25928;&#26524;&#30340;&#20998;&#24418;&#22270;&#20687;&#65292;&#24182;&#19988;&#20860;&#23481;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26377;&#26395;&#20026;&#19979;&#28216;&#20219;&#21153;&#21644;&#31185;&#23398;&#29702;&#35299;&#25552;&#20379;&#26032;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.12722</link><description>&lt;p&gt;
&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20998;&#24418;
&lt;/p&gt;
&lt;p&gt;
Learning Fractals by Gradient Descent. (arXiv:2303.12722v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20998;&#24418;&#22270;&#20687;&#21442;&#25968;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#35270;&#35273;&#25928;&#26524;&#30340;&#20998;&#24418;&#22270;&#20687;&#65292;&#24182;&#19988;&#20860;&#23481;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26377;&#26395;&#20026;&#19979;&#28216;&#20219;&#21153;&#21644;&#31185;&#23398;&#29702;&#35299;&#25552;&#20379;&#26032;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24418;&#26159;&#19968;&#31181;&#21487;&#20197;&#23637;&#31034;&#33258;&#28982;&#30028;&#20013;&#22797;&#26434;&#19988;&#33258;&#30456;&#20284;&#30340;&#20960;&#20309;&#24418;&#24577;&#30340;&#22270;&#26696;&#65288;&#20363;&#22914;&#20113;&#21644;&#26893;&#29289;&#65289;&#12290;&#26368;&#36817;&#22312;&#35270;&#35273;&#35782;&#21035;&#39046;&#22495;&#20013;&#65292;&#20511;&#21161;&#36825;&#31181;&#23646;&#24615;&#29983;&#25104;&#38543;&#26426;&#20998;&#24418;&#22270;&#20687;&#26469;&#36827;&#34892;&#27169;&#22411;&#39044;&#35757;&#32451;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#30456;&#21453;&#30340;&#38382;&#39064;--&#32473;&#23450;&#19968;&#20010;&#30446;&#26631;&#22270;&#20687;&#65288;&#19981;&#19968;&#23450;&#26159;&#20998;&#24418;&#65289;&#65292;&#25105;&#20204;&#26088;&#22312;&#29983;&#25104;&#19968;&#20010;&#31867;&#20284;&#30340;&#20998;&#24418;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20998;&#24418;&#22270;&#20687;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25214;&#21040;&#39640;&#36136;&#37327;&#35270;&#35273;&#25928;&#26524;&#30340;&#20998;&#24418;&#21442;&#25968;&#65292;&#24182;&#19988;&#20860;&#23481;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24320;&#21551;&#20102;&#22810;&#31181;&#21487;&#33021;&#24615;&#65292;&#20363;&#22914;&#20026;&#19979;&#28216;&#20219;&#21153;&#12289;&#31185;&#23398;&#29702;&#35299;&#31561;&#23398;&#20064;&#20998;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fractals are geometric shapes that can display complex and self-similar patterns found in nature (e.g., clouds and plants). Recent works in visual recognition have leveraged this property to create random fractal images for model pre-training. In this paper, we study the inverse problem -- given a target image (not necessarily a fractal), we aim to generate a fractal image that looks like it. We propose a novel approach that learns the parameters underlying a fractal image via gradient descent. We show that our approach can find fractal parameters of high visual quality and be compatible with different loss functions, opening up several potentials, e.g., learning fractals for downstream tasks, scientific understanding, etc.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26131;&#20110;&#23454;&#29616;&#30340;&#38750;&#20984;&#24352;&#37327;&#23436;&#25104;&#26694;&#26550;&#65288;TL12&#21644;TCCUR&#65289;&#65292;&#38024;&#23545;&#31649;&#29366;&#37319;&#26679;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#31350;&#65292;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#23427;&#20204;&#22312;&#20302;&#37319;&#26679;&#29575;&#19979;&#22312;&#26102;&#38388;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#25240;&#34935;&#65292;&#19988;&#22312;&#33267;&#23569;&#26576;&#20010;&#26041;&#38754;&#20248;&#20110;&#32463;&#20856;&#23436;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.12721</link><description>&lt;p&gt;
&#38754;&#21521;&#31649;&#29366;&#37319;&#26679;&#30340;&#20302;&#31209;&#24352;&#37327;&#23436;&#25104;&#30340;&#38750;&#20984;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Non-convex approaches for low-rank tensor completion under tubal sampling. (arXiv:2303.12721v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26131;&#20110;&#23454;&#29616;&#30340;&#38750;&#20984;&#24352;&#37327;&#23436;&#25104;&#26694;&#26550;&#65288;TL12&#21644;TCCUR&#65289;&#65292;&#38024;&#23545;&#31649;&#29366;&#37319;&#26679;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#31350;&#65292;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#23427;&#20204;&#22312;&#20302;&#37319;&#26679;&#29575;&#19979;&#22312;&#26102;&#38388;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#25240;&#34935;&#65292;&#19988;&#22312;&#33267;&#23569;&#26576;&#20010;&#26041;&#38754;&#20248;&#20110;&#32463;&#20856;&#23436;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#23436;&#25104;&#26159;&#29616;&#20195;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20855;&#20307;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#31216;&#20026;&#31649;&#29366;&#37319;&#26679;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#26131;&#20110;&#23454;&#29616;&#30340;&#38750;&#20984;&#24352;&#37327;&#23436;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;&#24352;&#37327; $L_1$-$L_2$ (TL12) &#21644;&#24352;&#37327;&#23436;&#25104; via CUR (TCCUR)&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#24425;&#33394;&#22270;&#20687;&#20462;&#22797;&#38382;&#39064;&#19978;&#27979;&#35797;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#25581;&#31034;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#20302;&#37319;&#26679;&#29575;&#19979;&#20934;&#30830;&#24615;&#21644;&#26102;&#38388;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#23427;&#20204;&#20013;&#30340;&#27599;&#19968;&#20010;&#22312;&#33267;&#23569;&#26576;&#20010;&#26041;&#38754;&#20248;&#20110;&#19968;&#20123;&#32463;&#20856;&#23436;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor completion is an important problem in modern data analysis. In this work, we investigate a specific sampling strategy, referred to as tubal sampling. We propose two novel non-convex tensor completion frameworks that are easy to implement, named tensor $L_1$-$L_2$ (TL12) and tensor completion via CUR (TCCUR). We test the efficiency of both methods on synthetic data and a color image inpainting problem. Empirical results reveal a trade-off between the accuracy and time efficiency of these two methods in a low sampling ratio. Each of them outperforms some classical completion methods in at least one aspect.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;Sentinel-2&#22270;&#20687;&#30340;&#39068;&#33394;&#20998;&#21106;&#21644;&#33258;&#21160;&#26631;&#35760;&#30340;&#26497;&#22320;&#28023;&#20912;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;U-Net&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#31934;&#20934;&#20998;&#31867;&#65292;&#20026;&#23545;&#20840;&#29699;&#21464;&#26262;&#36827;&#34892;&#26377;&#25928;&#30417;&#27979;&#25552;&#20379;&#20102;&#25216;&#26415;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2303.12719</link><description>&lt;p&gt;
&#22522;&#20110;&#39068;&#33394;&#20998;&#21106;&#21644;&#33258;&#21160;&#26631;&#35760;Sentinel-2&#22270;&#20687;&#30340;&#26497;&#22320;&#28023;&#20912;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Toward Polar Sea-Ice Classification using Color-based Segmentation and Auto-labeling of Sentinel-2 Imagery to Train an Efficient Deep Learning Model. (arXiv:2303.12719v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;Sentinel-2&#22270;&#20687;&#30340;&#39068;&#33394;&#20998;&#21106;&#21644;&#33258;&#21160;&#26631;&#35760;&#30340;&#26497;&#22320;&#28023;&#20912;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;U-Net&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#31934;&#20934;&#20998;&#31867;&#65292;&#20026;&#23545;&#20840;&#29699;&#21464;&#26262;&#36827;&#34892;&#26377;&#25928;&#30417;&#27979;&#25552;&#20379;&#20102;&#25216;&#26415;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#21464;&#26262;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#20102;&#26497;&#22320;&#21306;&#22495;&#28023;&#20912;&#21644;&#20912;&#24029;&#30340;&#34701;&#21270;&#31561;&#29615;&#22659;&#28798;&#38590;&#12290;&#26497;&#22320;&#28023;&#20912;&#30340;&#34701;&#21270;&#27169;&#24335;&#21644;&#36864;&#32553;&#26159;&#20840;&#29699;&#21464;&#26262;&#30340;&#37325;&#35201;&#25351;&#26631;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#24378;&#22823;&#26377;&#25928;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;Sentinel-2&#21355;&#26143;&#22270;&#20687;&#23545;&#26497;&#22320;&#28023;&#20912;&#36827;&#34892;&#20998;&#31867;&#65292;&#21253;&#25324;&#21402;&#20912;&#12289;&#35206;&#38634;&#34180;&#20912;&#21644;&#24320;&#38420;&#27700;&#22495;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#32570;&#20047;S2&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#22522;&#30784;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37319;&#29992;&#21512;&#36866;&#30340;&#39068;&#33394;&#38408;&#20540;&#30830;&#23450;&#39068;&#33394;&#20998;&#21106;&#26041;&#27861;&#65292;&#32467;&#21512;&#33258;&#21160;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#20102;&#19968;&#20010;U-Net&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#23436;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#26469;&#23454;&#29616;&#22270;&#20687;&#20998;&#31867;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global warming is an urgent issue that is generating catastrophic environmental changes, such as the melting of sea ice and glaciers, particularly in the polar regions. The melting pattern and retreat of polar sea ice cover is an essential indicator of global warming. The Sentinel-2 satellite (S2) captures high-resolution optical imagery over the polar regions. This research aims at developing a robust and effective system for classifying polar sea ice as thick or snow-covered, young or thin, or open water using S2 images. A key challenge is the lack of labeled S2 training data to serve as the ground truth. We demonstrate a method with high precision to segment and automatically label the S2 images based on suitably determined color thresholds and employ these auto-labeled data to train a U-Net machine model (a fully convolutional neural network), yielding good classification accuracy. Evaluation results over S2 data from the polar summer season in the Ross Sea region of the Antarctic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28784;&#31665; MDP &#30340;&#31574;&#30053;&#32508;&#21512;&#31639;&#27861;&#65292;&#20351;&#29992;&#21306;&#38388; MDP &#20316;&#20026;&#20869;&#37096;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65292;&#32467;&#21512;&#19979;&#32622;&#20449;&#21306;&#38388;&#25506;&#32034;&#21644;&#34892;&#21160;&#21010;&#20998;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#26377;&#38480;&#37319;&#26679;&#19979;&#30340;&#38382;&#39064;&#65292;&#29992;&#20110;&#21512;&#25104;&#26368;&#22823;&#21270;&#22238;&#25253;&#30340;&#23454;&#29992;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2303.12718</link><description>&lt;p&gt;
&#26377;&#38480;&#37319;&#26679;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#31574;&#30053;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
Strategy Synthesis in Markov Decision Processes Under Limited Sampling Access. (arXiv:2303.12718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28784;&#31665; MDP &#30340;&#31574;&#30053;&#32508;&#21512;&#31639;&#27861;&#65292;&#20351;&#29992;&#21306;&#38388; MDP &#20316;&#20026;&#20869;&#37096;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65292;&#32467;&#21512;&#19979;&#32622;&#20449;&#21306;&#38388;&#25506;&#32034;&#21644;&#34892;&#21160;&#21010;&#20998;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#26377;&#38480;&#37319;&#26679;&#19979;&#30340;&#38382;&#39064;&#65292;&#29992;&#20110;&#21512;&#25104;&#26368;&#22823;&#21270;&#22238;&#25253;&#30340;&#23454;&#29992;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25511;&#21046;&#29702;&#35770;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#24418;&#24335;&#26041;&#27861;&#20013;&#65292;&#19968;&#20010;&#26680;&#24515;&#30340;&#20219;&#21153;&#26159;&#20026;&#22312;&#37096;&#20998;&#26410;&#30693;&#29615;&#22659;&#19979;&#25805;&#20316;&#30340;&#20195;&#29702;&#21512;&#25104;&#26368;&#22823;&#21270;&#22238;&#25253;&#30340;&#31574;&#30053;&#12290;&#22312;&#28784;&#31665;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243; (MDPs) &#27169;&#22411;&#20013;&#65292;&#20195;&#29702;&#30340;&#34892;&#20026;&#24433;&#21709;&#20197;&#21518;&#30340;&#29366;&#24577;&#32780;&#19981;&#26159;&#28041;&#21450;&#21040;&#30340;&#27010;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20026;&#28784;&#31665; MDP &#21512;&#25104;&#31574;&#30053;&#35774;&#35745;&#20102;&#19968;&#20010;&#20869;&#37096;&#27169;&#22411;&#20026;&#21306;&#38388; MDP &#30340;&#31574;&#30053;&#32508;&#21512;&#31639;&#27861;&#12290;&#20026;&#20102;&#24212;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26377;&#38480;&#37319;&#26679;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20004;&#20010;&#26032;&#30340;&#27010;&#24565;&#24341;&#20837;&#21040;&#31639;&#27861;&#20013;&#65292;&#19987;&#27880;&#20110;&#24555;&#36895;&#25104;&#21151;&#30340;&#23398;&#20064;&#32780;&#19981;&#26159;&#38543;&#26426;&#20445;&#35777;&#21644;&#26368;&#20248;&#24615;&#65306;&#19979;&#32622;&#20449;&#21306;&#38388;&#25506;&#32034;&#21152;&#24378;&#24050;&#32463;&#23398;&#20064;&#30340;&#21487;&#34892;&#31574;&#30053;&#30340;&#21464;&#20307;&#65292;&#34892;&#21160;&#21010;&#20998;&#23558;&#23398;&#20064;&#34892;&#21160;&#31354;&#38388;&#32553;&#23567;&#21040;&#26377;&#21069;&#36884;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#36890;&#36807;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#23454;&#20363;&#35828;&#26126;&#20102;&#31639;&#27861;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central task in control theory, artificial intelligence, and formal methods is to synthesize reward-maximizing strategies for agents that operate in partially unknown environments. In environments modeled by gray-box Markov decision processes (MDPs), the impact of the agents' actions are known in terms of successor states but not the stochastics involved. In this paper, we devise a strategy synthesis algorithm for gray-box MDPs via reinforcement learning that utilizes interval MDPs as internal model. To compete with limited sampling access in reinforcement learning, we incorporate two novel concepts into our algorithm, focusing on rapid and successful learning rather than on stochastic guarantees and optimality: lower confidence bound exploration reinforces variants of already learned practical strategies and action scoping reduces the learning action space to promising actions. We illustrate benefits of our algorithms by means of a prototypical implementation applied on examples fro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#24605;&#24819;&#30340;&#28508;&#22312;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;Barrett&#39135;&#31649;&#30142;&#30149;&#36827;&#31243;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#37325;&#24314;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2303.12711</link><description>&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#24863;&#30693;&#30340;&#28508;&#22312;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#24314;&#27169;Barrett&#39135;&#31649;&#30142;&#30149;&#36827;&#31243;
&lt;/p&gt;
&lt;p&gt;
Geometry-Aware Latent Representation Learning for Modeling Disease Progression of Barrett's Esophagus. (arXiv:2303.12711v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#24605;&#24819;&#30340;&#28508;&#22312;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;Barrett&#39135;&#31649;&#30142;&#30149;&#36827;&#31243;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#37325;&#24314;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Barrett&#39135;&#31649;&#26159;&#39135;&#31649;&#33146;&#30284;&#30340;&#21807;&#19968;&#20808;&#39537;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#35786;&#26029;&#26102;&#39044;&#21518;&#19981;&#33391;&#30340;&#39135;&#31649;&#30284;&#30151;&#12290;&#22240;&#27492;&#65292;&#35786;&#26029;Barrett&#39135;&#31649;&#23545;&#20110;&#39044;&#38450;&#21644;&#27835;&#30103;&#39135;&#31649;&#30284;&#33267;&#20851;&#37325;&#35201;&#12290;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25903;&#25345;Barrett&#39135;&#31649;&#35786;&#26029;&#65292;&#20294;&#32452;&#32455;&#30149;&#29702;&#23398;&#35757;&#32451;&#25968;&#25454;&#30340;&#39640;&#35266;&#23519;&#32773;&#21464;&#24322;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#12290;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAEs)&#36827;&#34892;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26174;&#31034;&#20986;&#28508;&#22312;&#20248;&#21183;&#65292;&#22240;&#20026;&#23427;&#20204;&#23558;&#36755;&#20837;&#25968;&#25454;&#26144;&#23556;&#21040;&#20855;&#26377;&#20165;&#26377;&#29992;&#29305;&#24449;&#30340;&#20302;&#32500;&#27969;&#24418;&#65292;&#20026;&#25913;&#36827;&#19979;&#28216;&#20219;&#21153;&#21644;&#35265;&#35299;&#23558;Barrett&#39135;&#31649;&#30149;&#31243;&#34920;&#24449;&#12290;&#28982;&#32780;&#65292;VAE&#30340;&#27431;&#20960;&#37324;&#24471;&#28508;&#22312;&#31354;&#38388;&#25197;&#26354;&#20102;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#30142;&#30149;&#36827;&#23637;&#24314;&#27169;&#12290;&#20960;&#20309;VAEs&#20026;&#28508;&#22312;&#31354;&#38388;&#25552;&#20379;&#38468;&#21152;&#20960;&#20309;&#32467;&#26500;&#65292;RHVAE&#20551;&#35774;&#20026;&#40654;&#26364;&#27969;&#24418;&#65292;$\mathcal{S}$-VAE&#20551;&#35774;&#20026;&#36229;&#29699;&#38754;&#27969;&#24418;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;$\mathcal{S}$-VAE&#20248;&#20110;&#24120;&#35268;VAE&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#37325;&#24314;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Barrett's Esophagus (BE) is the only precursor known to Esophageal Adenocarcinoma (EAC), a type of esophageal cancer with poor prognosis upon diagnosis. Therefore, diagnosing BE is crucial in preventing and treating esophageal cancer. While supervised machine learning supports BE diagnosis, high interobserver variability in histopathological training data limits these methods. Unsupervised representation learning via Variational Autoencoders (VAEs) shows promise, as they map input data to a lower-dimensional manifold with only useful features, characterizing BE progression for improved downstream tasks and insights. However, the VAE's Euclidean latent space distorts point relationships, hindering disease progression modeling. Geometric VAEs provide additional geometric structure to the latent space, with RHVAE assuming a Riemannian manifold and $\mathcal{S}$-VAE a hyperspherical manifold. Our study shows that $\mathcal{S}$-VAE outperforms vanilla VAE with better reconstruction losses, 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#38381;&#30151;&#30340;&#26816;&#27979;&#21644;&#35786;&#26029;&#65292;&#26088;&#22312;&#37327;&#21270;&#26041;&#27861;&#20381;&#36182;&#20110;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#23458;&#35266;&#30340;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#34892;&#20026;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.12707</link><description>&lt;p&gt;
&#27604;&#36739;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#38381;&#30151;&#26816;&#27979;&#20013;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Comparison of Probabilistic Deep Learning Methods for Autism Detection. (arXiv:2303.12707v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12707
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#38381;&#30151;&#30340;&#26816;&#27979;&#21644;&#35786;&#26029;&#65292;&#26088;&#22312;&#37327;&#21270;&#26041;&#27861;&#20381;&#36182;&#20110;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#23458;&#35266;&#30340;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#34892;&#20026;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#26159;&#19968;&#31181;&#30446;&#21069;&#22312;&#19990;&#30028;&#33539;&#22260;&#20869;&#26222;&#36941;&#23384;&#22312;&#30340;&#31070;&#32463;&#21457;&#32946;&#38556;&#30861;&#12290;ASD &#20250;&#22312;&#20010;&#20307;&#30340;&#25972;&#20010;&#29983;&#21629;&#26399;&#20869;&#23384;&#22312;&#65292;&#24433;&#21709;&#20182;&#20204;&#30340;&#34892;&#20026;&#21644;&#20132;&#27969;&#26041;&#24335;&#65292;&#23548;&#33268;&#26126;&#26174;&#30340;&#31038;&#20132;&#38556;&#30861;&#12289;&#37325;&#22797;&#30340;&#34892;&#20026;&#29305;&#24449;&#20197;&#21450;&#20852;&#36259;&#21463;&#38480;&#12290;&#26089;&#26399;&#21457;&#29616;&#35813;&#30142;&#30149;&#26377;&#21161;&#20110;&#21551;&#21160;&#27835;&#30103;&#24182;&#24110;&#21161;&#24739;&#32773;&#36807;&#19978;&#27491;&#24120;&#30340;&#29983;&#27963;&#12290;&#30446;&#21069;&#24050;&#32463;&#30740;&#31350;&#21644;&#24320;&#21457;&#20102;&#19968;&#20123;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23450;&#37327;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#20020;&#24202;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#37327;&#21270;&#26041;&#27861;&#20381;&#36182;&#20110;&#26426;&#22120;&#23398;&#20064;&#65292;&#19968;&#20123;&#22797;&#26434;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#24320;&#21457;&#29992;&#20110;&#21152;&#36895;&#33258;&#38381;&#30151;&#30340;&#26816;&#27979;&#21644;&#35786;&#26029;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#26041;&#27861;&#65292;&#20197;&#20854;&#25152;&#24212;&#29992;&#30340;&#25968;&#25454;&#38598;&#31867;&#22411;&#20026;&#29305;&#24449;&#36827;&#34892;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autism Spectrum Disorder (ASD) is one neuro developmental disorder that is now widespread in the world. ASD persists throughout the life of an individual, impacting the way they behave and communicate, resulting to notable deficits consisting of social life retardation, repeated behavioural traits and a restriction in their interests. Early detection of the disorder helps in the onset treatment and helps one to lead a normal life. There are clinical approaches used in detection of autism, relying on behavioural data and in worst cases, neuroimaging. Quantitative methods involving machine learning have been studied and developed to overcome issues with clinical approaches. These quantitative methods rely on machine learning, with some complex methods based on deep learning developed to accelerate detection and diagnosis of ASD. These literature is aimed at exploring most state-of-the-art probabilistic methods in use today, characterizing them with the type of dataset they're most applie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#35268;&#33539;&#24314;&#27169;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#26816;&#27979;&#20986;&#22810;&#31181;&#25104;&#20687;&#21644;&#29983;&#29289;&#21464;&#37327;&#20013;&#30340;&#24322;&#24120;&#24615;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#30740;&#31350;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#30142;&#30149;&#12290;</title><link>http://arxiv.org/abs/2303.12706</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#36328;&#22810;&#31181;&#25104;&#20687;&#27169;&#24577;&#36827;&#34892;&#35268;&#33539;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Variational Autoencoders for normative modelling across multiple imaging modalities. (arXiv:2303.12706v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#35268;&#33539;&#24314;&#27169;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#26816;&#27979;&#20986;&#22810;&#31181;&#25104;&#20687;&#21644;&#29983;&#29289;&#21464;&#37327;&#20013;&#30340;&#24322;&#24120;&#24615;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#30740;&#31350;&#24102;&#26377;&#24322;&#36136;&#24615;&#30340;&#30142;&#30149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24120;&#35265;&#31070;&#32463;&#30142;&#30149;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#30142;&#30149;&#24322;&#36136;&#24615;&#65292;&#21253;&#25324;&#30149;&#22240;&#12289;&#31070;&#32463;&#25104;&#20687;&#29305;&#24449;&#12289;&#21512;&#24182;&#30151;&#25110;&#22522;&#22240;&#21464;&#24322;&#30340;&#24046;&#24322;&#12290;&#35268;&#33539;&#24314;&#27169;&#24050;&#25104;&#20026;&#30740;&#31350;&#36825;&#31181;&#20154;&#32676;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#20854;&#20013;&#23545;&#29983;&#29702;&#31995;&#32479;&#30340;&#8220;&#27491;&#24120;&#8221;&#34892;&#20026;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#20010;&#20307;&#23618;&#38754;&#19978;&#26816;&#27979;&#19982;&#30142;&#30149;&#30149;&#29702;&#30456;&#20851;&#30340;&#20559;&#24046;&#12290;&#23545;&#20110;&#35768;&#22810;&#24322;&#36136;&#24615;&#30142;&#30149;&#65292;&#25105;&#20204;&#39044;&#35745;&#20250;&#35266;&#23519;&#21040;&#22810;&#31181;&#31070;&#32463;&#25104;&#20687;&#21644;&#29983;&#29289;&#21464;&#37327;&#30340;&#24322;&#24120;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#35268;&#33539;&#27169;&#22411;&#20027;&#35201;&#26159;&#20026;&#20102;&#30740;&#31350;&#21333;&#19968;&#25104;&#20687;&#27169;&#24577;&#32780;&#24320;&#21457;&#30340;&#12290;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#22810;&#27169;&#24577;&#35268;&#33539;&#24314;&#27169;&#26694;&#26550;&#65292;&#22312;&#22810;&#20010;&#27169;&#24577;&#30340;&#21464;&#37327;&#20013;&#32858;&#21512;&#24322;&#24120;&#24615;&#65292;&#24182;&#19988;&#27604;&#21333;&#27169;&#24335;&#22522;&#32447;&#26356;&#33021;&#26816;&#27979;&#21040;&#20559;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#26816;&#27979;T1&#21644;DTI&#25968;&#25454;&#20013;&#30340;&#20010;&#20307;&#23618;&#38754;&#20559;&#24046;&#30340;&#22810;&#27169;&#24577;VAE&#35268;&#33539;&#27169;&#22411;&#12290;&#19982;&#21333;&#27169;&#24335;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#26816;&#27979;&#21040;&#36731;&#24230;&#35748;&#30693;&#21463;&#25439;&#30340;&#21463;&#35797;&#32773;&#20013;&#30340;&#20559;&#24046;&#65292;&#35777;&#26126;&#20102;&#22810;&#27169;&#24577;&#35268;&#33539;&#24314;&#27169;&#29992;&#20110;&#30142;&#30149;&#24322;&#36136;&#24615;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the challenges of studying common neurological disorders is disease heterogeneity including differences in causes, neuroimaging characteristics, comorbidities, or genetic variation. Normative modelling has become a popular method for studying such cohorts where the 'normal' behaviour of a physiological system is modelled and can be used at subject level to detect deviations relating to disease pathology. For many heterogeneous diseases, we expect to observe abnormalities across a range of neuroimaging and biological variables. However, thus far, normative models have largely been developed for studying a single imaging modality. We aim to develop a multi-modal normative modelling framework where abnormality is aggregated across variables of multiple modalities and is better able to detect deviations than uni-modal baselines. We propose two multi-modal VAE normative models to detect subject level deviations across T1 and DTI data. Our proposed models were better able to detect di
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26631;&#31614;&#35777;&#25454;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24320;&#25918;&#38598;&#21160;&#20316;&#35782;&#21035;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#38382;&#39064;&#21644;&#21516;&#19968;&#22330;&#26223;&#20013;&#21333;&#20010;&#25110;&#22810;&#20010;&#28436;&#21592;&#20197;&#21450;&#20219;&#20309;&#28436;&#21592;&#30340;&#21516;&#26102;&#21160;&#20316;&#30340;&#26356;&#19968;&#33324;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12698</link><description>&lt;p&gt;
&#22810;&#26631;&#31614;&#35777;&#25454;&#23398;&#20064;&#29992;&#20110;&#24320;&#25918;&#38598;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Open Set Action Recognition via Multi-Label Evidential Learning. (arXiv:2303.12698v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26631;&#31614;&#35777;&#25454;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24320;&#25918;&#38598;&#21160;&#20316;&#35782;&#21035;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#38382;&#39064;&#21644;&#21516;&#19968;&#22330;&#26223;&#20013;&#21333;&#20010;&#25110;&#22810;&#20010;&#28436;&#21592;&#20197;&#21450;&#20219;&#20309;&#28436;&#21592;&#30340;&#21516;&#26102;&#21160;&#20316;&#30340;&#26356;&#19968;&#33324;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#24320;&#25918;&#38598;&#21160;&#20316;&#35782;&#21035;&#26041;&#27861;&#38598;&#20013;&#20110;&#26032;&#39062;&#24615;&#26816;&#27979;&#65292;&#20551;&#35774;&#35270;&#39057;&#21098;&#36753;&#26174;&#31034;&#21333;&#20010;&#21160;&#20316;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#26631;&#31614;&#35777;&#25454;&#23398;&#20064;&#65288;MULE&#65289;&#36827;&#34892;&#24320;&#25918;&#38598;&#21160;&#20316;&#35782;&#21035;&#21644;&#26032;&#39062;&#24615;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36229;&#36234;&#20102;&#20197;&#21069;&#30340;&#26032;&#21160;&#20316;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#21516;&#19968;&#22330;&#26223;&#20013;&#21333;&#20010;&#25110;&#22810;&#20010;&#28436;&#21592;&#20197;&#21450;&#20219;&#20309;&#28436;&#21592;&#30340;&#21516;&#26102;&#21160;&#20316;&#30340;&#26356;&#19968;&#33324;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;Beta&#35777;&#25454;&#31070;&#32463;&#32593;&#32476;&#22522;&#20110;&#28436;&#21592;-&#19978;&#19979;&#25991;-&#23545;&#35937;&#20851;&#31995;&#34920;&#31034;&#65292;&#20351;&#29992;Beta&#23494;&#24230;&#20272;&#35745;&#22810;&#21160;&#20316;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#28155;&#21152;&#35777;&#25454;&#21435;&#20559;&#32622;&#32422;&#26463;&#21040;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#20197;&#20943;&#23569;&#35270;&#39057;&#34920;&#31034;&#30340;&#38745;&#24577;&#20559;&#24046;&#65292;&#20174;&#32780;&#21487;&#20197;&#38169;&#35823;&#22320;&#20851;&#32852;&#39044;&#27979;&#21644;&#38745;&#24577;&#32447;&#32034;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#21407;&#22987;-&#23545;&#20598;&#24179;&#22343;&#26041;&#26696;&#26356;&#26032;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#20248;&#21270;&#25152;&#25552;&#20986;&#30340;&#38382;&#39064;&#12290;&#20248;&#21270;&#31639;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods for open-set action recognition focus on novelty detection that assumes video clips show a single action, which is unrealistic in the real world. We propose a new method for open set action recognition and novelty detection via MUlti-Label Evidential learning (MULE), that goes beyond previous novel action detection methods by addressing the more general problems of single or multiple actors in the same scene, with simultaneous action(s) by any actor. Our Beta Evidential Neural Network estimates multi-action uncertainty with Beta densities based on actor-context-object relation representations. An evidence debiasing constraint is added to the objective function for optimization to reduce the static bias of video representations, which can incorrectly correlate predictions and static cues. We develop a learning algorithm based on a primal-dual average scheme update to optimize the proposed problem. Theoretical analysis of the optimization algorithm demonstrates the conve
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#26469;&#23398;&#20064;&#38750;&#25311;&#21512;&#20998;&#25968;&#30340;&#20998;&#24067;&#65292;&#24182;&#21033;&#29992;&#20854;&#26435;&#37325;&#20998;&#37197;&#26356;&#22810;&#30340;&#37325;&#35201;&#24615;&#32473;&#27531;&#24046;&#19982;&#27979;&#35797;&#28857;&#30456;&#20284;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31526;&#21512;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.12695</link><description>&lt;p&gt;
&#38750;&#25311;&#21512;&#20998;&#25968;&#37325;&#26032;&#26435;&#37325;&#23454;&#29616;&#33258;&#36866;&#24212;&#19968;&#33268;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adaptive Conformal Prediction by Reweighting Nonconformity Score. (arXiv:2303.12695v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12695
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#26469;&#23398;&#20064;&#38750;&#25311;&#21512;&#20998;&#25968;&#30340;&#20998;&#24067;&#65292;&#24182;&#21033;&#29992;&#20854;&#26435;&#37325;&#20998;&#37197;&#26356;&#22810;&#30340;&#37325;&#35201;&#24615;&#32473;&#27531;&#24046;&#19982;&#27979;&#35797;&#28857;&#30456;&#20284;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#31526;&#21512;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#38469;&#25104;&#21151;&#65292;&#20294;&#30001;&#19968;&#33268;&#24615;&#39044;&#27979;&#65288;CP&#65289;&#32473;&#20986;&#30340;&#39044;&#27979;&#21306;&#38388;&#65288;PI&#65289;&#21487;&#33021;&#26080;&#27861;&#21453;&#26144;&#32473;&#23450;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#38480;&#21046;&#28304;&#20110;CP&#26041;&#27861;&#23545;&#25152;&#26377;&#27979;&#35797;&#28857;&#20351;&#29992;&#24120;&#25968;&#20462;&#27491;&#65292;&#26080;&#35270;&#23427;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#30830;&#20445;&#35206;&#30422;&#29305;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;&#65288;QRF&#65289;&#26469;&#23398;&#20064;&#38750;&#25311;&#21512;&#20998;&#25968;&#30340;&#20998;&#24067;&#65292;&#24182;&#21033;&#29992;QRF&#30340;&#26435;&#37325;&#23558;&#26356;&#22810;&#30340;&#37325;&#35201;&#24615;&#20998;&#37197;&#32473;&#27531;&#24046;&#19982;&#27979;&#35797;&#28857;&#30456;&#20284;&#30340;&#26679;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#30340;PI&#38271;&#24230;&#26356;&#31526;&#21512;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;QRF&#23398;&#20064;&#21040;&#30340;&#26435;&#37325;&#25552;&#20379;&#20102;&#29305;&#24449;&#31354;&#38388;&#30340;&#21010;&#20998;&#65292;&#36890;&#36807;&#32452;&#21512;&#19968;&#33268;&#21270;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25913;&#36827;PI&#30340;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20139;&#26377;&#22522;&#20110;&#26679;&#26412;&#21644;&#22522;&#20110;&#35757;&#32451;&#26465;&#20214;&#30340;&#26080;&#20551;&#35774;&#26377;&#38480;&#35206;&#30422;&#29575;&#65292;&#24182;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#65292;&#20063;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
Despite attractive theoretical guarantees and practical successes, Predictive Interval (PI) given by Conformal Prediction (CP) may not reflect the uncertainty of a given model. This limitation arises from CP methods using a constant correction for all test points, disregarding their individual uncertainties, to ensure coverage properties. To address this issue, we propose using a Quantile Regression Forest (QRF) to learn the distribution of nonconformity scores and utilizing the QRF's weights to assign more importance to samples with residuals similar to the test point. This approach results in PI lengths that are more aligned with the model's uncertainty. In addition, the weights learnt by the QRF provide a partition of the features space, allowing for more efficient computations and improved adaptiveness of the PI through groupwise conformalization. Our approach enjoys an assumption-free finite sample marginal and training-conditional coverage, and under suitable assumptions, it also
&lt;/p&gt;</description></item><item><title>&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#20542;&#21521;&#20110;&#20154;&#31867;&#24418;&#29366;&#35782;&#21035;&#65292;&#32780;&#38750;&#20165;&#20165;&#21033;&#29992;&#32441;&#29702;&#32447;&#32034;&#12290;&#22312;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#32032;&#25551;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#21464;&#25442;&#25928;&#26524;&#20173;&#28982;&#26377;&#25928;&#65292;&#24182;&#19988;&#22312;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20063;&#36866;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.12669</link><description>&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#19979;&#22522;&#20110;&#20154;&#31867;&#34892;&#20026;&#30340;&#30740;&#31350;&#25193;&#23637;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
An Extended Study of Human-like Behavior under Adversarial Training. (arXiv:2303.12669v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12669
&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#20542;&#21521;&#20110;&#20154;&#31867;&#24418;&#29366;&#35782;&#21035;&#65292;&#32780;&#38750;&#20165;&#20165;&#21033;&#29992;&#32441;&#29702;&#32447;&#32034;&#12290;&#22312;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#32032;&#25551;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#21464;&#25442;&#25928;&#26524;&#20173;&#28982;&#26377;&#25928;&#65292;&#24182;&#19988;&#22312;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20063;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23384;&#22312;&#35768;&#22810;&#32570;&#38519;&#65292;&#20854;&#20013;&#26368;&#20005;&#37325;&#30340;&#20043;&#19968;&#26159;&#23545;&#20998;&#24067;&#20559;&#24046;&#30340;&#25935;&#24863;&#24615;&#65292;&#36825;&#20801;&#35768;&#27169;&#22411;&#36731;&#26131;&#34987;&#23567;&#22411;&#25200;&#21160;&#27450;&#39575;&#24182;&#20570;&#20986;&#38169;&#35823;&#39044;&#27979;&#65292;&#32780;&#36825;&#20123;&#25200;&#21160;&#36890;&#24120;&#23545;&#20154;&#31867;&#26469;&#35828;&#19981;&#26131;&#23519;&#35273;&#24182;&#19981;&#24517;&#39035;&#20855;&#26377;&#35821;&#20041;&#21547;&#20041;&#12290; &#23545;&#25239;&#24615;&#35757;&#32451;&#36890;&#36807;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#23545;&#27169;&#22411;&#36827;&#34892;&#25200;&#21160;&#26469;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290; &#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#20063;&#25351;&#20986;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#29702;&#26041;&#24335;&#19981;&#21516;&#20110;&#20154;&#31867;&#12290; &#20154;&#31867;&#36890;&#36807;&#24418;&#29366;&#35782;&#21035;&#23545;&#35937;&#65292;&#32780;&#31070;&#32463;&#32593;&#32476;&#20027;&#35201;&#21033;&#29992;&#32441;&#29702;&#32447;&#32034;&#12290; &#20363;&#22914;&#65292;&#21463;&#36807;&#29031;&#29255;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#25512;&#24191;&#21040;&#21253;&#21547;&#32032;&#25551;&#30340;&#25968;&#25454;&#38598;&#12290; &#26377;&#36259;&#30340;&#26159;&#65292;&#36824;&#34920;&#26126;&#23545;&#25239;&#24615;&#35757;&#32451;&#20284;&#20046;&#26377;&#21033;&#20110;&#22686;&#21152;&#36716;&#21521;&#24418;&#29366;&#20559;&#24046;&#30340;&#36235;&#21183;&#12290; &#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#24182;&#23601;&#21508;&#31181;&#26550;&#26500;&#65292;&#24120;&#35265;&#30340;$\ell_2$&#21644;$\ell_\infty$-training&#65292;&#20197;&#21450;&#22522;&#20110;Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#25928;&#24212;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#12290; &#25105;&#20204;&#22312;&#29289;&#20307;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36716;&#21521;&#24418;&#29366;&#20559;&#24046;&#30340;&#21464;&#25442;&#19981;&#20165;&#38480;&#20110;&#35270;&#35273;&#39046;&#22495;&#65292;&#32780;&#19988;&#20063;&#36866;&#29992;&#20110;&#35821;&#35328;&#22788;&#29702;&#12290; &#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23545;&#25239;&#24615;&#35757;&#32451;&#23548;&#33268;&#27169;&#22411;&#26356;&#22810;&#22320;&#20381;&#36182;&#32452;&#21512;&#32467;&#26500;&#26469;&#35782;&#21035;&#23545;&#35937;&#21644;&#36827;&#34892;&#39044;&#27979;&#65292;&#20174;&#32780;&#20855;&#26377;&#26356;&#25509;&#36817;&#20154;&#31867;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have a number of shortcomings. Amongst the severest ones is the sensitivity to distribution shifts which allows models to be easily fooled into wrong predictions by small perturbations to inputs that are often imperceivable to humans and do not have to carry semantic meaning. Adversarial training poses a partial solution to address this issue by training models on worst-case perturbations. Yet, recent work has also pointed out that the reasoning in neural networks is different from humans. Humans identify objects by shape, while neural nets mainly employ texture cues. Exemplarily, a model trained on photographs will likely fail to generalize to datasets containing sketches. Interestingly, it was also shown that adversarial training seems to favorably increase the shift toward shape bias. In this work, we revisit this observation and provide an extensive analysis of this effect on various architectures, the common $\ell_2$- and $\ell_\infty$-training, and Transformer-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; PIQ&#65292;&#36890;&#36807;&#23545;&#20998;&#31867;&#22120;&#36827;&#34892;&#21521;&#37327;&#37327;&#21270;&#65292;&#23558;&#20854;&#34920;&#31034;&#36716;&#25442;&#20026;&#31163;&#25955;&#31867;&#29305;&#23450;&#30340;&#28508;&#31354;&#38388;&#65292;&#20174;&#32780;&#35299;&#37322;&#20998;&#31867;&#22120;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#65292;&#24182;&#19988;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#35813;&#26041;&#27861;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#23481;&#26131;&#35753;&#20154;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.12659</link><description>&lt;p&gt;
&#36890;&#36807;&#37327;&#21270;&#36827;&#34892;&#30340;&#20107;&#21518;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Posthoc Interpretation via Quantization. (arXiv:2303.12659v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; PIQ&#65292;&#36890;&#36807;&#23545;&#20998;&#31867;&#22120;&#36827;&#34892;&#21521;&#37327;&#37327;&#21270;&#65292;&#23558;&#20854;&#34920;&#31034;&#36716;&#25442;&#20026;&#31163;&#25955;&#31867;&#29305;&#23450;&#30340;&#28508;&#31354;&#38388;&#65292;&#20174;&#32780;&#35299;&#37322;&#20998;&#31867;&#22120;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#65292;&#24182;&#19988;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#35813;&#26041;&#27861;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#23481;&#26131;&#35753;&#20154;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#36890;&#36807;&#37327;&#21270;&#23454;&#29616;&#30340;&#20107;&#21518;&#35299;&#37322;&#65288;PIQ&#65289;&#8221;&#65292;&#29992;&#20110;&#35299;&#37322;&#35757;&#32451;&#20998;&#31867;&#22120;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#21521;&#37327;&#37327;&#21270;&#23558;&#20998;&#31867;&#22120;&#30340;&#34920;&#31034;&#36716;&#25442;&#20026;&#31163;&#25955;&#65292;&#31867;&#29305;&#23450;&#30340;&#28508;&#31354;&#38388;&#12290;&#31867;&#29305;&#23450;&#30340;&#30721;&#26412;&#20316;&#20026;&#29942;&#39048;&#65292;&#36843;&#20351;&#35299;&#37322;&#32773;&#19987;&#27880;&#20110;&#20998;&#31867;&#22120;&#35748;&#20026;&#29992;&#20110;&#36827;&#34892;&#39044;&#27979;&#30340;&#36755;&#20837;&#25968;&#25454;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#30740;&#31350;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#19982;&#25991;&#29486;&#20013;&#30340;&#20960;&#31181;&#20854;&#20182;&#35299;&#37322;&#26041;&#27861;&#30456;&#27604;&#65292;PIQ&#29983;&#25104;&#30340;&#35299;&#37322;&#26356;&#23481;&#26131;&#34987;&#21442;&#19982;&#25105;&#20204;&#29992;&#25143;&#30740;&#31350;&#30340;&#20154;&#25152;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new approach, called "Posthoc Interpretation via Quantization (PIQ)", for interpreting decisions made by trained classifiers. Our method utilizes vector quantization to transform the representations of a classifier into a discrete, class-specific latent space. The class-specific codebooks act as a bottleneck that forces the interpreter to focus on the parts of the input data deemed relevant by the classifier for making a prediction. We evaluated our method through quantitative and qualitative studies and found that PIQ generates interpretations that are more easily understood by participants to our user studies when compared to several other interpretation methods in the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Pharos-guided Attack (PgA) &#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#20195;&#34920;&#33391;&#24615;&#22270;&#20687;&#35821;&#20041;&#30340; Pharos &#20195;&#30721;&#23454;&#29616;&#24555;&#36895;&#19988;&#21487;&#38752;&#22320;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#65292;&#33021;&#22815;&#26356;&#20840;&#38754;&#22320;&#35780;&#20272;&#28145;&#24230;&#21704;&#24076;&#26816;&#32034;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12658</link><description>&lt;p&gt;
&#28145;&#24230;&#21704;&#24076;&#26816;&#32034;&#26041;&#27861;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#21487;&#38752;&#39640;&#25928;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Reliable and Efficient Evaluation of Adversarial Robustness for Deep Hashing-Based Retrieval. (arXiv:2303.12658v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Pharos-guided Attack (PgA) &#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#20195;&#34920;&#33391;&#24615;&#22270;&#20687;&#35821;&#20041;&#30340; Pharos &#20195;&#30721;&#23454;&#29616;&#24555;&#36895;&#19988;&#21487;&#38752;&#22320;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#65292;&#33021;&#22815;&#26356;&#20840;&#38754;&#22320;&#35780;&#20272;&#28145;&#24230;&#21704;&#24076;&#26816;&#32034;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#21704;&#24076;&#25216;&#26415;&#22312;&#22823;&#35268;&#27169;&#22270;&#20687;&#26816;&#32034;&#39046;&#22495;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#30001;&#20110;&#26410;&#20805;&#20998;&#21033;&#29992;&#21407;&#22987;&#26679;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#25110;&#38656;&#35201;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#36825;&#20123;&#20851;&#31995;&#65292;&#22312;&#35780;&#20272;&#28145;&#24230;&#21704;&#24076;&#27169;&#22411;&#23545;&#25239;&#26679;&#26412;&#30340;&#40065;&#26834;&#24615;&#26102;&#23384;&#22312;&#24615;&#33021;&#19979;&#38477;&#25110;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25915;&#20987;&#26041;&#27861; Pharos-guided Attack (PgA)&#65292;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#20195;&#34920;&#33391;&#24615;&#22270;&#20687;&#35821;&#20041;&#30340; Pharos &#20195;&#30721;&#65292;&#21487;&#24555;&#36895;&#19988;&#21487;&#38752;&#22320;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#20845;&#31181;&#25915;&#20987;&#26041;&#27861;&#30456;&#27604;&#65292;PgA &#22312;&#25104;&#21151;&#29575;&#21644;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#26356;&#20840;&#38754;&#22320;&#35780;&#20272;&#28145;&#24230;&#21704;&#24076;&#26816;&#32034;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep hashing has been extensively applied to massive image retrieval due to its efficiency and effectiveness. Recently, several adversarial attacks have been presented to reveal the vulnerability of deep hashing models against adversarial examples. However, existing attack methods suffer from degraded performance or inefficiency because they underutilize the semantic relations between original samples or spend a lot of time learning these relations with a deep neural network. In this paper, we propose a novel Pharos-guided Attack, dubbed PgA, to evaluate the adversarial robustness of deep hashing networks reliably and efficiently. Specifically, we design pharos code to represent the semantics of the benign image, which preserves the similarity to semantically relevant samples and dissimilarity to irrelevant ones. It is proven that we can quickly calculate the pharos code via a simple math formula. Accordingly, PgA can directly conduct a reliable and efficient attack on deep hashing-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#33258;&#30417;&#30563;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#29992;&#20110;&#20581;&#22766;&#27874;&#26463;&#25104;&#24418;&#65292;&#33021;&#22815;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#21508;&#31181;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12653</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#20581;&#22766;&#20840;&#24687;&#27627;&#31859;&#27874;&#27874;&#26463;&#25104;&#24418;
&lt;/p&gt;
&lt;p&gt;
Robust Holographic mmWave Beamforming by Self-Supervised Hybrid Deep Learning. (arXiv:2303.12653v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#33258;&#30417;&#30563;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#29992;&#20110;&#20581;&#22766;&#27874;&#26463;&#25104;&#24418;&#65292;&#33021;&#22815;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#21508;&#31181;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#22825;&#32447;&#38453;&#21015;&#30340;&#27874;&#26463;&#25104;&#24418;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;5G&#21644;&#21363;&#23558;&#25512;&#20986;&#30340;6G&#20013;&#65292;&#22240;&#27492;&#21508;&#31181;&#25216;&#26415;&#34987;&#21033;&#29992;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#20363;&#22914;&#28145;&#24230;&#23398;&#20064;&#12289;&#39640;&#32423;&#20248;&#21270;&#31639;&#27861;&#31561;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#20855;&#26377;&#28145;&#24230;&#23398;&#20064;&#30340;&#20808;&#21069;&#30740;&#31350;&#26041;&#26696;&#20013;&#20854;&#24615;&#33021;&#30456;&#24403;&#21560;&#24341;&#20154;&#65292;&#20294;&#36890;&#24120;&#24403;&#29615;&#22659;&#25110;&#25968;&#25454;&#38598;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#20854;&#24615;&#33021;&#20250;&#36805;&#36895;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#20855;&#26377;&#24378;&#22823;&#40065;&#26834;&#24615;&#30340;&#26377;&#25928;&#27874;&#26463;&#25104;&#24418;&#32593;&#32476;&#26159;&#26234;&#33021;&#26080;&#32447;&#36890;&#20449;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20581;&#22766;&#30340;&#27874;&#26463;&#25104;&#24418;&#33258;&#30417;&#30563;&#32593;&#32476;&#65292;&#24182;&#22312;&#20004;&#31181;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#21508;&#31181;&#22330;&#26223;&#19979;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#20855;&#26377;&#28151;&#21512;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#32593;&#32476;&#22312;&#32463;&#20856;&#30340;DeepMIMO&#21644;&#26032;&#30340;WAIR-D&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#29615;&#22659;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#21407;&#29702;&#26469;&#35299;&#37322;&#36825;&#20010;&#32763;&#35793;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beamforming with large-scale antenna arrays has been widely used in recent years, which is acknowledged as an important part in 5G and incoming 6G. Thus, various techniques are leveraged to improve its performance, e.g., deep learning, advanced optimization algorithms, etc. Although its performance in many previous research scenarios with deep learning is quite attractive, usually it drops rapidly when the environment or dataset is changed. Therefore, designing effective beamforming network with strong robustness is an open issue for the intelligent wireless communications. In this paper, we propose a robust beamforming self-supervised network, and verify it in two kinds of different datasets with various scenarios. Simulation results show that the proposed self-supervised network with hybrid learning performs well in both classic DeepMIMO and new WAIR-D dataset with the strong robustness under the various environments. Also, we present the principle to explain the rationality of this 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35760;&#24518;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#39640;&#24230;&#21160;&#24577;&#21644;&#24322;&#36136;&#20132;&#36890;&#29615;&#22659;&#19979;&#23454;&#26102;&#39044;&#27979;&#20132;&#36890;&#37327;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12643</link><description>&lt;p&gt;
&#22522;&#20110;&#35760;&#24518;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20132;&#36890;&#37327;&#39044;&#27979;&#65306;LSTM&#21644;GRU&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Traffic Volume Prediction using Memory-Based Recurrent Neural Networks: A comparative analysis of LSTM and GRU. (arXiv:2303.12643v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12643
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35760;&#24518;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#39640;&#24230;&#21160;&#24577;&#21644;&#24322;&#36136;&#20132;&#36890;&#29615;&#22659;&#19979;&#23454;&#26102;&#39044;&#27979;&#20132;&#36890;&#37327;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#39044;&#27979;&#20132;&#36890;&#37327;&#21487;&#20197;&#25552;&#39640;&#20132;&#36890;&#27969;&#37327;&#21644;&#36947;&#36335;&#23433;&#20840;&#24615;&#12290;&#31934;&#30830;&#30340;&#20132;&#36890;&#37327;&#39044;&#27979;&#26377;&#21161;&#20110;&#35686;&#31034;&#39550;&#39542;&#21592;&#27839;&#30528;&#20182;&#20204;&#21916;&#27426;&#30340;&#36335;&#32447;&#30340;&#23454;&#26102;&#20132;&#36890;&#27969;&#37327;&#65292;&#36991;&#20813;&#28508;&#22312;&#30340;&#27515;&#38145;&#24773;&#20917;&#12290;&#29616;&#26377;&#30340;&#21442;&#25968;&#27169;&#22411;&#26080;&#27861;&#21487;&#38752;&#22320;&#39044;&#27979;&#21160;&#24577;&#21644;&#22797;&#26434;&#20132;&#36890;&#26465;&#20214;&#19979;&#30340;&#20132;&#36890;&#37327;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#23454;&#26102;&#22320;&#35780;&#20272;&#21644;&#39044;&#27979;&#27599;&#20010;&#32473;&#23450;&#26102;&#38388;&#27493;&#30340;&#20132;&#36890;&#37327;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#38750;&#32447;&#24615;&#30340;&#22522;&#20110;&#35760;&#24518;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20844;&#20132;&#39640;&#26550;&#20132;&#36890;&#37327;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#22312;&#39640;&#24230;&#21160;&#24577;&#21644;&#24322;&#36136;&#20132;&#36890;&#29615;&#22659;&#19979;&#39044;&#27979;&#20132;&#36890;&#37327;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting traffic volume in real-time can improve both traffic flow and road safety. A precise traffic volume forecast helps alert drivers to the flow of traffic along their preferred routes, preventing potential deadlock situations. Existing parametric models cannot reliably forecast traffic volume in dynamic and complex traffic conditions. Therefore, in order to evaluate and forecast the traffic volume for every given time step in a real-time manner, we develop non-linear memory-based deep neural network models. Our extensive experiments run on the Metro Interstate Traffic Volume dataset demonstrate the effectiveness of the proposed models in predicting traffic volume in highly dynamic and heterogeneous traffic environments.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#27665;&#20027;&#21270;&#65292;&#21253;&#25324;&#22235;&#31181;&#31867;&#22411;&#30340;&#27665;&#20027;&#21270;&#65306;AI&#20351;&#29992;&#30340;&#27665;&#20027;&#21270;&#65292;AI&#24320;&#21457;&#30340;&#27665;&#20027;&#21270;&#65292;AI&#21033;&#28070;&#30340;&#27665;&#20027;&#21270;&#65292;&#21644;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#12290;&#35201;&#24819;&#23454;&#29616;&#26377;&#25928;&#30340;&#25919;&#31574;&#21644;&#26435;&#34913;&#35752;&#35770;&#65292;&#38656;&#35201;&#35748;&#35782;&#21040;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#22312;&#20915;&#31574;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;</title><link>http://arxiv.org/abs/2303.12642</link><description>&lt;p&gt;
&#26234;&#33021;&#21270;&#30340;&#27665;&#20027;&#21270;&#65306;&#22810;&#31181;&#21547;&#20041;&#12289;&#30446;&#26631;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Democratising AI: Multiple Meanings, Goals, and Methods. (arXiv:2303.12642v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12642
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;AI&#30340;&#27665;&#20027;&#21270;&#65292;&#21253;&#25324;&#22235;&#31181;&#31867;&#22411;&#30340;&#27665;&#20027;&#21270;&#65306;AI&#20351;&#29992;&#30340;&#27665;&#20027;&#21270;&#65292;AI&#24320;&#21457;&#30340;&#27665;&#20027;&#21270;&#65292;AI&#21033;&#28070;&#30340;&#27665;&#20027;&#21270;&#65292;&#21644;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#12290;&#35201;&#24819;&#23454;&#29616;&#26377;&#25928;&#30340;&#25919;&#31574;&#21644;&#26435;&#34913;&#35752;&#35770;&#65292;&#38656;&#35201;&#35748;&#35782;&#21040;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#22312;&#20915;&#31574;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20154;&#21628;&#21505;&#23454;&#29616;AI&#30340;&#27665;&#20027;&#21270;&#65292;&#20294;&#36825;&#20010;&#35789;&#35821;&#29992;&#26469;&#25351;&#20195;&#22810;&#31181;&#30446;&#26631;&#65292;&#26377;&#26102;&#20250;&#30456;&#20114;&#20914;&#31361;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#36890;&#24120;&#35752;&#35770;&#30340;&#22235;&#31181;AI&#27665;&#20027;&#21270;&#31867;&#22411;&#65306;(1) AI&#20351;&#29992;&#30340;&#27665;&#20027;&#21270;&#65292;(2) AI&#24320;&#21457;&#30340;&#27665;&#20027;&#21270;&#65292;(3) AI&#21033;&#28070;&#30340;&#27665;&#20027;&#21270;&#65292;&#21644;(4) AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#23454;&#29616;&#27599;&#31181;&#27665;&#20027;&#21270;&#24418;&#24335;&#30340;&#22810;&#20010;&#30446;&#26631;&#21644;&#26041;&#27861;&#12290;&#20174;&#26412;&#25991;&#20013;&#20027;&#35201;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#65292;AI&#30340;&#27665;&#20027;&#21270;&#26159;&#19968;&#20010;&#22810;&#20803;&#32780;&#26377;&#26102;&#20250;&#30456;&#20114;&#20914;&#31361;&#30340;&#27010;&#24565;&#65292;&#19981;&#24212;&#28151;&#28102;AI&#21487;&#35775;&#38382;&#24615;&#30340;&#25913;&#21892;&#12290;&#22914;&#26524;&#25105;&#20204;&#24819;&#35201;&#36229;&#36234;&#23545;&#26234;&#33021;&#21270;&#27665;&#20027;&#21270;&#30340;&#27169;&#31946;&#25215;&#35834;&#65292;&#36827;&#20837;&#20855;&#20307;&#25919;&#31574;&#21644;&#26435;&#34913;&#30340;&#29983;&#20135;&#24615;&#35752;&#35770;&#65292;&#25105;&#20204;&#38656;&#35201;&#35748;&#35782;&#21040;AI&#27835;&#29702;&#30340;&#27665;&#20027;&#21270;&#22312;&#36328;&#36234;&#20851;&#20110;&#20351;&#29992;&#12289;&#24320;&#21457;&#21644;&#21033;&#28070;&#30340;&#20915;&#31574;&#20013;&#23548;&#33322;&#26435;&#34913;&#21644;&#39118;&#38505;&#30340;&#20027;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous parties are calling for the democratisation of AI, but the phrase is used to refer to a variety of goals, the pursuit of which sometimes conflict. This paper identifies four kinds of AI democratisation that are commonly discussed: (1) the democratisation of AI use, (2) the democratisation of AI development, (3) the democratisation of AI profits, and (4) the democratisation of AI governance. Numerous goals and methods of achieving each form of democratisation are discussed. The main takeaway from this paper is that AI democratisation is a multifarious and sometimes conflicting concept that should not be conflated with improving AI accessibility. If we want to move beyond ambiguous commitments to democratising AI, to productive discussions of concrete policies and trade-offs, then we need to recognise the principal role of the democratisation of AI governance in navigating tradeoffs and risks across decisions around use, development, and profits.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#25239;&#24615;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23545;&#25239;&#24615;&#25628;&#32034;&#36807;&#31243;&#20013;&#32467;&#21512;&#33258;&#32534;&#30721;&#22120;&#37325;&#26500;&#25439;&#22833;&#21644;&#23558;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#34892;&#20026;&#19982;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#36830;&#25509;&#65292;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#25628;&#32034;&#36807;&#31243;&#30340;&#36895;&#24230;&#21644;&#29983;&#25104;&#32467;&#26524;&#30340;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12634</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#23545;&#25239;&#24615;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised counterfactual explanations. (arXiv:2303.12634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#25239;&#24615;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23545;&#25239;&#24615;&#25628;&#32034;&#36807;&#31243;&#20013;&#32467;&#21512;&#33258;&#32534;&#30721;&#22120;&#37325;&#26500;&#25439;&#22833;&#21644;&#23558;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#34892;&#20026;&#19982;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#36830;&#25509;&#65292;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#25628;&#32034;&#36807;&#31243;&#30340;&#36895;&#24230;&#21644;&#29983;&#25104;&#32467;&#26524;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#35299;&#37322;&#26159;&#29992;&#20110;&#26597;&#25214;&#26368;&#23567;&#24178;&#39044;&#29305;&#24449;&#20540;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#27169;&#22411;&#23558;&#39044;&#27979;&#26356;&#25913;&#20026;&#19981;&#21516;&#30340;&#36755;&#20986;&#25110;&#30446;&#26631;&#36755;&#20986;&#12290;&#26377;&#25928;&#30340;&#23545;&#25239;&#24615;&#35299;&#37322;&#24212;&#20855;&#26377;&#21487;&#33021;&#30340;&#29305;&#24449;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#29983;&#25104;&#23545;&#25239;&#24615;&#35299;&#37322;&#30340;&#25361;&#25112;&#65292;&#20351;&#20854;&#22788;&#20110;&#19982;&#35757;&#32451;&#25968;&#25454;&#30456;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#65292;&#24182;&#19988;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20204;&#23646;&#20110;&#30446;&#26631;&#31867;&#20998;&#24067;&#12290;&#36890;&#36807;&#22312;&#23545;&#25239;&#24615;&#25628;&#32034;&#36807;&#31243;&#20013;&#32467;&#21512;&#33258;&#32534;&#30721;&#22120;&#37325;&#26500;&#25439;&#22833;&#26469;&#35299;&#20915;&#36825;&#20010;&#35201;&#27714;&#12290;&#23558;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#34892;&#20026;&#19982;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#36830;&#25509;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#23545;&#25239;&#24615;&#25628;&#32034;&#36807;&#31243;&#30340;&#36895;&#24230;&#21644;&#29983;&#25104;&#32467;&#26524;&#30340;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30340;&#25345;&#32493;&#21162;&#21147;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#33258;&#32534;&#30721;&#22120;&#22312;&#21322;&#30417;&#30563;&#29366;&#24577;&#19979;&#34987;&#35757;&#32451;&#26102;&#65292;&#23545;&#25239;&#24615;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations for machine learning models are used to find minimal interventions to the feature values such that the model changes the prediction to a different output or a target output. A valid counterfactual explanation should have likely feature values. Here, we address the challenge of generating counterfactual explanations that lie in the same data distribution as that of the training data and more importantly, they belong to the target class distribution. This requirement has been addressed through the incorporation of auto-encoder reconstruction loss in the counterfactual search process. Connecting the output behavior of the classifier to the latent space of the auto-encoder has further improved the speed of the counterfactual search process and the interpretability of the resulting counterfactual explanations. Continuing this line of research, we show further improvement in the interpretability of counterfactual explanations when the auto-encoder is trained in a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#38376;&#30340;&#26032;&#22411;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#21518;&#38376;&#24182;&#19981;&#33021;&#25104;&#21151;&#36827;&#34892;&#25915;&#20987;&#65292;&#22240;&#20026;&#21518;&#38376;&#19981;&#33021;&#20998;&#31163;&#35757;&#32451;&#21644;&#38750;&#35757;&#32451;&#26679;&#26412;&#30340;&#25439;&#22833;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2303.12589</link><description>&lt;p&gt;
&#21518;&#38376;&#26159;&#21542;&#26377;&#21161;&#20110;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Backdoors Assist Membership Inference Attacks?. (arXiv:2303.12589v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#38376;&#30340;&#26032;&#22411;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#21518;&#38376;&#24182;&#19981;&#33021;&#25104;&#21151;&#36827;&#34892;&#25915;&#20987;&#65292;&#22240;&#20026;&#21518;&#38376;&#19981;&#33021;&#20998;&#31163;&#35757;&#32451;&#21644;&#38750;&#35757;&#32451;&#26679;&#26412;&#30340;&#25439;&#22833;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#27602;&#30244;&#26679;&#26412;&#26102;&#65292;&#25968;&#25454;&#38544;&#31169;&#21487;&#33021;&#20250;&#27844;&#28431;&#65292;&#20363;&#22914;&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#20250;&#25512;&#26029;&#26679;&#26412;&#26159;&#21542;&#21253;&#21547;&#22312;&#27169;&#22411;&#30340;&#35757;&#32451;&#20043;&#20013;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#23558;&#26679;&#26412;&#31227;&#21160;&#21040;&#19968;&#20010;&#24322;&#24120;&#20540;&#20043;&#20013;&#12290;&#28982;&#32780;&#65292;&#25915;&#20987;&#21487;&#33021;&#20250;&#34987;&#26816;&#27979;&#21040;&#65292;&#22240;&#20026;&#30001;&#20110;&#27602;&#30244;&#26679;&#26412;&#32780;&#23548;&#33268;&#30340;&#25512;&#26029;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#31181;&#22522;&#20110;&#21518;&#38376;&#30340;&#26032;&#22411;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65292;&#20854;&#36890;&#36807;&#21518;&#38376;&#36820;&#22238;&#25915;&#20987;&#32773;&#39044;&#26399;&#30340;&#36755;&#20986;&#26469;&#36827;&#34892;&#25915;&#20987;&#12290;&#36890;&#36807;&#23545;&#19968;&#20010;&#23398;&#26415;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19977;&#20010;&#20851;&#38190;&#30340;&#35265;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21518;&#38376;&#36741;&#21161;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26159;&#19981;&#25104;&#21151;&#30340;&#12290;&#20854;&#27425;&#65292;&#24403;&#25105;&#20204;&#20998;&#26512;&#25439;&#22833;&#20998;&#24067;&#20197;&#20102;&#35299;&#19981;&#25104;&#21151;&#32467;&#26524;&#30340;&#21407;&#22240;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#21518;&#38376;&#19981;&#33021;&#20998;&#31163;&#35757;&#32451;&#21644;&#38750;&#35757;&#32451;&#26679;&#26412;&#30340;&#25439;&#22833;&#20998;&#24067;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#21518;&#38376;&#19981;&#33021;&#24433;&#21709;&#24178;&#20928;&#26679;&#26412;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
When an adversary provides poison samples to a machine learning model, privacy leakage, such as membership inference attacks that infer whether a sample was included in the training of the model, becomes effective by moving the sample to an outlier. However, the attacks can be detected because inference accuracy deteriorates due to poison samples. In this paper, we discuss a \textit{backdoor-assisted membership inference attack}, a novel membership inference attack based on backdoors that return the adversary's expected output for a triggered sample. We found three crucial insights through experiments with an academic benchmark dataset. We first demonstrate that the backdoor-assisted membership inference attack is unsuccessful. Second, when we analyzed loss distributions to understand the reason for the unsuccessful results, we found that backdoors cannot separate loss distributions of training and non-training samples. In other words, backdoors cannot affect the distribution of clean 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#30340;&#24555;&#25463;&#26041;&#24335;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#20256;&#32479;&#30340;&#32531;&#35299;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12578</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#24555;&#25463;&#26041;&#24335;&#65306;&#32531;&#35299;&#31574;&#30053;&#21450;&#20854;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic Reasoning Shortcuts: Mitigation Strategies and their Limitations. (arXiv:2303.12578v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#30340;&#24555;&#25463;&#26041;&#24335;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#20256;&#32479;&#30340;&#32531;&#35299;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#39044;&#27979;&#22120;&#23398;&#20064;&#20174;&#23376;&#31526;&#21495;&#36755;&#20837;&#21040;&#26356;&#39640;&#23618;&#27425;&#27010;&#24565;&#30340;&#26144;&#23556;&#65292;&#28982;&#21518;&#22312;&#36825;&#20010;&#20013;&#38388;&#34920;&#31034;&#19978;&#25191;&#34892;&#65288;&#27010;&#29575;&#65289;&#36923;&#36753;&#25512;&#29702;&#12290;&#36825;&#31181;&#35774;&#32622;&#22312;&#31526;&#21495;&#20808;&#39564;&#30693;&#35782;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#36890;&#24120;&#34987;&#35748;&#20026;&#22312;&#36981;&#23432;&#30693;&#35782;&#30340;&#21069;&#25552;&#19979;&#25552;&#20379;&#20102;&#35299;&#37322;&#24615;&#30340;&#22909;&#22788;&#65292;&#22240;&#20026;&#23398;&#20064;&#30340;&#27010;&#24565;&#21487;&#20197;&#26356;&#22909;&#22320;&#34987;&#20154;&#31867;&#21033;&#30410;&#30456;&#20851;&#32773;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#35777;&#26126;&#20102;&#36825;&#31181;&#35774;&#32622;&#21463;&#21040;&#25512;&#29702;&#24555;&#25463;&#26041;&#24335;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#21033;&#29992;&#20855;&#26377;&#24847;&#22806;&#35821;&#20041;&#30340;&#27010;&#24565;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#30340;&#39044;&#27979;&#65292;&#23548;&#33268;&#20102;&#24046;&#30340;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#24182;&#25439;&#23475;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#25512;&#29702;&#24555;&#25463;&#26041;&#24335;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#24418;&#24335;&#32852;&#31995;&#65292;&#24182;&#30830;&#23450;&#20102;&#25512;&#29702;&#24555;&#25463;&#26041;&#24335;&#21487;&#33021;&#20986;&#29616;&#30340;&#24773;&#20917;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#33258;&#28982;&#32531;&#35299;&#31574;&#30053;&#65288;&#22914;&#37325;&#24314;&#21644;&#27010;&#24565;&#30417;&#30563;&#65289;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic predictors learn a mapping from sub-symbolic inputs to higher-level concepts and then carry out (probabilistic) logical inference on this intermediate representation. This setup offers clear advantages in terms of consistency to symbolic prior knowledge, and is often believed to provide interpretability benefits in that - by virtue of complying with the knowledge the learned concepts can be better understood by human stakeholders. However, it was recently shown that this setup is affected by reasoning shortcuts whereby predictions attain high accuracy by leveraging concepts with unintended semantics, yielding poor out-of-distribution performance and compromising interpretability. In this short paper, we establish a formal link between reasoning shortcuts and the optima of the loss function, and identify situations in which reasoning shortcuts can arise. Based on this, we discuss limitations of natural mitigation strategies such as reconstruction and concept supervision
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WAE-MDP&#30340;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20219;&#20309;RL&#31574;&#30053;&#20013;&#25552;&#21462;&#24418;&#24335;&#21487;&#39564;&#35777;&#25511;&#21046;&#22120;&#65292;&#24182;&#19988;&#20855;&#26377;&#24179;&#34913;&#25511;&#21046;&#24615;&#33021;&#21644;&#23433;&#20840;&#20043;&#38388;&#30340;&#25928;&#26524;&#21644;&#35299;&#20915;&#19968;&#20123;&#23398;&#20064;&#32570;&#38519;&#30340;&#22810;&#26041;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.12558</link><description>&lt;p&gt;
Wasserstein&#33258;&#32534;&#30721;MDPs&#65306;&#20855;&#26377;&#22810;&#26041;&#20445;&#35777;&#30340;&#39640;&#25928;RL&#31574;&#30053;&#27491;&#24335;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Auto-encoded MDPs: Formal Verification of Efficiently Distilled RL Policies with Many-sided Guarantees. (arXiv:2303.12558v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12558
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WAE-MDP&#30340;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20219;&#20309;RL&#31574;&#30053;&#20013;&#25552;&#21462;&#24418;&#24335;&#21487;&#39564;&#35777;&#25511;&#21046;&#22120;&#65292;&#24182;&#19988;&#20855;&#26377;&#24179;&#34913;&#25511;&#21046;&#24615;&#33021;&#21644;&#23433;&#20840;&#20043;&#38388;&#30340;&#25928;&#26524;&#21644;&#35299;&#20915;&#19968;&#20123;&#23398;&#20064;&#32570;&#38519;&#30340;&#22810;&#26041;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26377;&#35768;&#22810;&#25104;&#21151;&#26696;&#20363;&#65292;&#20294;&#36890;&#36807;&#36825;&#20123;&#20808;&#36827;&#25216;&#26415;&#23398;&#20064;&#30340;&#20915;&#31574;&#32773;&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#20013;&#30340;&#22823;&#35268;&#27169;&#37096;&#32626;&#21463;&#21040;&#27491;&#24335;&#20445;&#35777;&#19981;&#36275;&#30340;&#38459;&#30861;&#12290;&#21464;&#20998;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;VAE-MDPs&#65289;&#26159;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#20174;&#20219;&#20309;RL&#31574;&#30053;&#20013;&#25552;&#21462;&#24418;&#24335;&#21487;&#39564;&#35777;&#25511;&#21046;&#22120;&#30340;&#21487;&#38752;&#26694;&#26550;&#12290;&#34429;&#28982;&#30456;&#20851;&#20445;&#35777;&#28085;&#30422;&#20102;&#23454;&#38469;&#38382;&#39064;&#30340;&#28385;&#36275;&#24615;&#21644;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#65292;&#20294;VAE&#26041;&#27861;&#22240;&#32570;&#20047;&#25277;&#35937;&#21644;&#34920;&#31034;&#20445;&#35777;&#20197;&#25903;&#25345;&#28508;&#22312;&#26368;&#20248;&#21270;&#32780;&#36973;&#21463;&#22810;&#31181;&#23398;&#20064;&#32570;&#38519;&#65288;&#21518;&#39564;&#23849;&#22604;&#65292;&#23398;&#20064;&#36895;&#24230;&#24930;&#65292;&#21160;&#21147;&#23398;&#20272;&#35745;&#19981;&#33391;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Wasserstein&#33258;&#32534;&#30721;MDP&#65288;WAE-MDP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#25191;&#34892;&#21407;&#22987;&#31574;&#30053;&#30340;&#26234;&#33021;&#20307;&#34892;&#20026;&#21644;&#25552;&#21462;&#20986;&#30340;&#31574;&#30053;&#20043;&#38388;&#30340;&#26368;&#20248;&#36716;&#36816;&#30340;&#24809;&#32602;&#24418;&#24335;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#21033;&#20110;&#25511;&#21046;&#24615;&#33021;&#21644;&#23433;&#20840;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#19978;&#36848;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#20851;&#20110;&#24615;&#33021;&#21644;&#23433;&#20840;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;RL&#22522;&#20934;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep reinforcement learning (DRL) has many success stories, the large-scale deployment of policies learned through these advanced techniques in safety-critical scenarios is hindered by their lack of formal guarantees. Variational Markov Decision Processes (VAE-MDPs) are discrete latent space models that provide a reliable framework for distilling formally verifiable controllers from any RL policy. While the related guarantees address relevant practical aspects such as the satisfaction of performance and safety properties, the VAE approach suffers from several learning flaws (posterior collapse, slow learning speed, poor dynamics estimates), primarily due to the absence of abstraction and representation guarantees to support latent optimization. We introduce the Wasserstein auto-encoded MDP (WAE-MDP), a latent space model that fixes those issues by minimizing a penalized form of the optimal transport between the behaviors of the agent executing the original policy and the disti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#27969;&#34892;&#30149;&#21464;&#21270;&#23545;&#31639;&#27861;&#30340;&#37096;&#32626;&#25928;&#26524;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.12540</link><description>&lt;p&gt;
&#8220;&#22270;&#20687;&#20998;&#26512;&#31639;&#27861;&#22312;&#27969;&#34892;&#30149;&#21464;&#21270;&#19979;&#30340;&#37096;&#32626;&#8221;
&lt;/p&gt;
&lt;p&gt;
Deployment of Image Analysis Algorithms under Prevalence Shifts. (arXiv:2303.12540v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#27969;&#34892;&#30149;&#21464;&#21270;&#23545;&#31639;&#27861;&#30340;&#37096;&#32626;&#25928;&#26524;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#24046;&#36317;&#26159;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20020;&#24202;&#36716;&#21270;&#20013;&#26368;&#37325;&#35201;&#30340;&#38556;&#30861;&#20043;&#19968;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;&#21644;&#32593;&#32476;&#26550;&#26500;&#65292;&#20294;&#24456;&#23569;&#20851;&#27880;&#31639;&#27861;&#22312;&#23454;&#36341;&#20013;&#37096;&#32626;&#26102;&#27969;&#34892;&#30149;&#21457;&#29983;&#29575;&#21464;&#21270;&#30340;&#29305;&#23450;&#24433;&#21709;&#12290;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#27665;&#20027;&#21270;&#30340;&#32972;&#26223;&#65292;&#30001;&#20110;&#30142;&#30149;&#27969;&#34892;&#29575;&#21487;&#33021;&#22312;&#26102;&#38388;&#21644;&#22320;&#28857;&#19978;&#21464;&#21270;&#24456;&#22823;&#65292;&#22240;&#27492;&#24320;&#21457;/&#39564;&#35777;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#21644;&#37096;&#32626;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#30340;&#31867;&#21035;&#39057;&#29575;&#20043;&#38388;&#30340;&#24046;&#24322;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#65288;i&#65289;&#26657;&#20934;&#30340;&#31243;&#24230;&#65292;&#65288;ii&#65289;&#20915;&#31574;&#38408;&#20540;&#19982;&#26368;&#20248;&#20540;&#30340;&#20559;&#24046;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#39564;&#35777;&#25351;&#26631;&#21453;&#26144;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#22312;&#37096;&#32626;&#20154;&#21475;&#30340;&#33021;&#21147;&#65292;&#23454;&#35777;&#22320;&#35777;&#26126;&#20102;&#32570;&#23569;&#27969;&#34892;&#30149;&#22788;&#29702;&#30340;&#28508;&#22312;&#20005;&#37325;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain gaps are among the most relevant roadblocks in the clinical translation of machine learning (ML)-based solutions for medical image analysis. While current research focuses on new training paradigms and network architectures, little attention is given to the specific effect of prevalence shifts on an algorithm deployed in practice. Such discrepancies between class frequencies in the data used for a method's development/validation and that in its deployment environment(s) are of great importance, for example in the context of artificial intelligence (AI) democratization, as disease prevalences may vary widely across time and location. Our contribution is twofold. First, we empirically demonstrate the potentially severe consequences of missing prevalence handling by analyzing (i) the extent of miscalibration, (ii) the deviation of the decision threshold from the optimum, and (iii) the ability of validation metrics to reflect neural network performance on the deployment population a
&lt;/p&gt;</description></item><item><title>DevelSet&#26159;&#19968;&#31181;GPU&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#21152;&#36895;&#30340;&#27700;&#24179;&#38598;&#37329;&#23646;&#20809;&#21051;&#25513;&#27169;&#32508;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26354;&#29575;&#39033;&#20943;&#23569;&#25513;&#27169;&#22797;&#26434;&#24230;&#65292;&#24182;&#24212;&#29992;GPU&#21152;&#36895;&#26469;&#20811;&#26381;&#35745;&#31639;&#29942;&#39048;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#21487;&#25171;&#21360;&#24615;&#21644;&#24555;&#36895;&#36845;&#20195;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2303.12529</link><description>&lt;p&gt;
DevelSet: &#28145;&#24230;&#31070;&#32463;&#27700;&#24179;&#38598;&#20248;&#21270;&#26041;&#27861;&#29992;&#20110;&#20809;&#21051;&#25513;&#27169;&#21046;&#20316;
&lt;/p&gt;
&lt;p&gt;
DevelSet: Deep Neural Level Set for Instant Mask Optimization. (arXiv:2303.12529v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12529
&lt;/p&gt;
&lt;p&gt;
DevelSet&#26159;&#19968;&#31181;GPU&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#21152;&#36895;&#30340;&#27700;&#24179;&#38598;&#37329;&#23646;&#20809;&#21051;&#25513;&#27169;&#32508;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26354;&#29575;&#39033;&#20943;&#23569;&#25513;&#27169;&#22797;&#26434;&#24230;&#65292;&#24182;&#24212;&#29992;GPU&#21152;&#36895;&#26469;&#20811;&#26381;&#35745;&#31639;&#29942;&#39048;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#21487;&#25171;&#21360;&#24615;&#21644;&#24555;&#36895;&#36845;&#20195;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20808;&#36827;&#24037;&#33402;&#33410;&#28857;&#20013;&#29305;&#24449;&#23610;&#23544;&#30340;&#19981;&#26029;&#32553;&#23567;&#65292;&#25513;&#27169;&#20248;&#21270;&#22312;&#20256;&#32479;&#35774;&#35745;&#27969;&#31243;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#24182;&#20276;&#38543;&#30528;&#20809;&#21051;&#36817;&#20284;&#26657;&#27491;(OPC)&#26041;&#27861;&#20013;&#35745;&#31639;&#24320;&#38144;&#30340;&#29190;&#28856;&#24335;&#22686;&#38271;&#12290;&#26368;&#36817;, &#21453;&#21521;&#20809;&#21051;&#25216;&#26415;(ILT)&#24050;&#32463;&#24341;&#36215;&#20102;&#37325;&#35270;, &#24182;&#22312;&#26032;&#20852;&#30340; OPC &#35299;&#20915;&#26041;&#26696;&#20013;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;ILT&#26041;&#27861;&#35201;&#20040;&#32791;&#26102;&#65292;&#35201;&#20040;&#25513;&#27169;&#21360;&#21047;&#24615;&#33021;&#21644;&#21487;&#21046;&#36896;&#24615;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DevelSet&#65292;&#19968;&#31181;GPU&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#21152;&#36895;&#30340;&#27700;&#24179;&#38598;&#37329;&#23646;&#20809;&#21051;&#25513;&#27169;&#32508;&#21512;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#24341;&#20837;&#26354;&#29575;&#39033;&#26469;&#38477;&#20302;&#25513;&#27169;&#22797;&#26434;&#24230;&#24182;&#24212;&#29992;GPU&#21152;&#36895;&#26469;&#20811;&#26381;&#35745;&#31639;&#29942;&#39048;&#65292;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#27700;&#24179;&#38598;&#30340;ILT&#31639;&#27861;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21487;&#25171;&#21360;&#24615;&#21644;&#24555;&#36895;&#36845;&#20195;&#25910;&#25947;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#27700;&#24179;&#38598;&#22266;&#26377;&#21407;&#29702;&#24039;&#22937;&#22320;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the feature size continuously shrinking in advanced technology nodes, mask optimization is increasingly crucial in the conventional design flow, accompanied by an explosive growth in prohibitive computational overhead in optical proximity correction (OPC) methods. Recently, inverse lithography technique (ILT) has drawn significant attention and is becoming prevalent in emerging OPC solutions. However, ILT methods are either time-consuming or in weak performance of mask printability and manufacturability. In this paper, we present DevelSet, a GPU and deep neural network (DNN) accelerated level set OPC framework for metal layer. We first improve the conventional level set-based ILT algorithm by introducing the curvature term to reduce mask complexity and applying GPU acceleration to overcome computational bottlenecks. To further enhance printability and fast iterative convergence, we propose a novel deep neural network delicately designed with level set intrinsic principles to facil
&lt;/p&gt;</description></item><item><title>Split-Et-Impera&#26159;&#19968;&#31181;&#38024;&#23545;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#30340;&#26694;&#26550;&#65292;&#21487;&#26681;&#25454;&#31070;&#32463;&#32593;&#32476;&#21487;&#35299;&#37322;&#24615;&#21407;&#21017;&#30830;&#23450;&#26368;&#20339;&#30340;&#32593;&#32476;&#20998;&#21106;&#28857;&#12289;&#36890;&#36807;&#36890;&#20449;&#24863;&#30693;&#20223;&#30495;&#36827;&#34892;&#24555;&#36895;&#35780;&#20272;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#37325;&#26032;&#25490;&#21015;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#30828;&#20214;&#24179;&#21488;&#30340;&#29305;&#24615;&#36866;&#24212;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#12290;</title><link>http://arxiv.org/abs/2303.12524</link><description>&lt;p&gt;
Split-Et-Impera: &#19968;&#31181;&#29992;&#20110;&#35774;&#35745;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Split-Et-Impera: A Framework for the Design of Distributed Deep Learning Applications. (arXiv:2303.12524v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12524
&lt;/p&gt;
&lt;p&gt;
Split-Et-Impera&#26159;&#19968;&#31181;&#38024;&#23545;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#30340;&#26694;&#26550;&#65292;&#21487;&#26681;&#25454;&#31070;&#32463;&#32593;&#32476;&#21487;&#35299;&#37322;&#24615;&#21407;&#21017;&#30830;&#23450;&#26368;&#20339;&#30340;&#32593;&#32476;&#20998;&#21106;&#28857;&#12289;&#36890;&#36807;&#36890;&#20449;&#24863;&#30693;&#20223;&#30495;&#36827;&#34892;&#24555;&#36895;&#35780;&#20272;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#37325;&#26032;&#25490;&#21015;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#30828;&#20214;&#24179;&#21488;&#30340;&#29305;&#24615;&#36866;&#24212;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35768;&#22810;&#27169;&#24335;&#35782;&#21035;&#24212;&#29992;&#37117;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#20998;&#24067;&#24335;&#26550;&#26500;&#65292;&#20854;&#20013;&#24863;&#30693;&#21644;&#35745;&#31639;&#33410;&#28857;&#36890;&#36807;&#36890;&#20449;&#32593;&#32476;&#30456;&#20114;&#20132;&#20114;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#25552;&#20379;&#24378;&#22823;&#30340;&#20915;&#31574;&#26426;&#21046;&#65292;&#20294;&#20063;&#38656;&#35201;&#39640;&#35745;&#31639;&#37327;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Split-Et-Impera&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#30830;&#23450;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20339;&#20998;&#21106;&#28857;&#65292;&#24182;&#36890;&#36807;&#36890;&#20449;&#24863;&#30693;&#20223;&#30495;&#36827;&#34892;&#24555;&#36895;&#35780;&#20272;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#37325;&#26032;&#25490;&#21015;&#65292;&#21516;&#26102;&#32771;&#34385;&#30446;&#26631;&#30828;&#20214;&#24179;&#21488;&#30340;&#29305;&#24615;&#65292;&#20197;&#36866;&#24212;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#25552;&#20379;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#24037;&#20855;&#31665;&#65292;&#35774;&#35745;&#65292;&#39564;&#35777;&#21644;&#20248;&#21270;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#20998;&#24067;&#24335;&#31995;&#32479;&#25110;&#32593;&#32476;&#36890;&#20449;&#21327;&#35758;&#26377;&#28145;&#20837;&#30340;&#20102;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many recent pattern recognition applications rely on complex distributed architectures in which sensing and computational nodes interact together through a communication network. Deep neural networks (DNNs) play an important role in this scenario, furnishing powerful decision mechanisms, at the price of a high computational effort. Consequently, powerful state-of-the-art DNNs are frequently split over various computational nodes, e.g., a first part stays on an embedded device and the rest on a server. Deciding where to split a DNN is a challenge in itself, making the design of deep learning applications even more complicated. Therefore, we propose Split-Et-Impera, a novel and practical framework that i) determines the set of the best-split points of a neural network based on deep network interpretability principles without performing a tedious try-and-test approach, ii) performs a communication-aware simulation for the rapid evaluation of different neural network rearrangements, and ii
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#23545;&#20165;&#25991;&#26412;&#20219;&#21153;&#30340;&#34920;&#29616;&#26159;&#21542;&#26377;&#25552;&#39640;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#22871;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#22810;&#27169;&#24577;&#35757;&#32451;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12513</link><description>&lt;p&gt;
BERT&#26159;&#21542;&#30450;&#30446;&#65311;&#25506;&#32034;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#23545;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding. (arXiv:2303.12513v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#23545;&#20165;&#25991;&#26412;&#20219;&#21153;&#30340;&#34920;&#29616;&#26159;&#21542;&#26377;&#25552;&#39640;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#22871;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#22810;&#27169;&#24577;&#35757;&#32451;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20154;&#20351;&#29992;&#35270;&#35273;&#24819;&#35937;&#26469;&#29702;&#35299;&#21644;&#25512;&#29702;&#35821;&#35328;&#65292;&#20294;&#26159;&#20687;BERT&#36825;&#26679;&#30340;&#27169;&#22411;&#20351;&#29992;&#22312;&#20165;&#21253;&#25324;&#25991;&#26412;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#33719;&#21462;&#30340;&#30693;&#35782;&#26469;&#25512;&#29702;&#35821;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#22312;&#28041;&#21450;&#38544;&#21547;&#35270;&#35273;&#25512;&#29702;&#30340;&#20165;&#25991;&#26412;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#37325;&#28857;&#26159;&#38646;&#26679;&#26412;&#25506;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#29992;&#20110;&#25506;&#27979;&#25991;&#26412;&#32534;&#30721;&#22120;&#27169;&#22411;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#30340;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#65288;VLU&#65289;&#20219;&#21153;&#65292;&#20197;&#21450;&#21508;&#31181;&#38750;&#35270;&#35273;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#29992;&#20110;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#36129;&#29486;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#38646;&#26679;&#26412;&#30693;&#35782;&#25506;&#27979;&#26041;&#27861;&#65292;Stroop probing&#65292;&#29992;&#20110;&#23558;&#20687;CLIP&#36825;&#26679;&#30340;&#27169;&#22411;&#24212;&#29992;&#20110;&#20165;&#25991;&#26412;&#20219;&#21153;&#65292;&#32780;&#19981;&#38656;&#35201;&#20687;BERT&#27169;&#22411;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#22836;&#37027;&#26679;&#30340;&#39044;&#27979;&#22836;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SOTA&#22810;&#27169;&#24577;&#35757;&#32451;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;VLU&#20219;&#21153;&#19978;&#20248;&#20110;&#21333;&#27169;&#24577;&#35757;&#32451;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20294;&#22312;NLU&#20219;&#21153;&#19978;&#19981;&#21450;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most humans use visual imagination to understand and reason about language, but models such as BERT reason about language using knowledge acquired during text-only pretraining. In this work, we investigate whether vision-and-language pretraining can improve performance on text-only tasks that involve implicit visual reasoning, focusing primarily on zero-shot probing methods. We propose a suite of visual language understanding (VLU) tasks for probing the visual reasoning abilities of text encoder models, as well as various non-visual natural language understanding (NLU) tasks for comparison. We also contribute a novel zero-shot knowledge probing method, Stroop probing, for applying models such as CLIP to text-only tasks without needing a prediction head such as the masked language modelling head of models like BERT. We show that SOTA multimodally trained text encoders outperform unimodally trained text encoders on the VLU tasks while being underperformed by them on the NLU tasks, lendin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#20892;&#19994;&#26426;&#22120;&#20154;&#30340;&#35821;&#20041;&#24863;&#30693;&#20013;&#36890;&#36807;&#29305;&#23450;&#39046;&#22495;&#30340;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#20197;&#20943;&#23569;&#26631;&#31614;&#25968;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#32452;&#29305;&#23450;&#39046;&#22495;&#30340;&#22686;&#24378;&#31574;&#30053;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.12499</link><description>&lt;p&gt;
&#20851;&#20110;&#20892;&#19994;&#26426;&#22120;&#20154;&#26377;&#25928;&#30340;&#39046;&#22495;&#29305;&#23450;&#39044;&#35757;&#32451;&#23545;&#20110;&#35821;&#20041;&#24863;&#30693;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Domain-Specific Pre-Training for Effective Semantic Perception in Agricultural Robotics. (arXiv:2303.12499v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#20892;&#19994;&#26426;&#22120;&#20154;&#30340;&#35821;&#20041;&#24863;&#30693;&#20013;&#36890;&#36807;&#29305;&#23450;&#39046;&#22495;&#30340;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#20197;&#20943;&#23569;&#26631;&#31614;&#25968;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#32452;&#29305;&#23450;&#39046;&#22495;&#30340;&#22686;&#24378;&#31574;&#30053;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20892;&#19994;&#26426;&#22120;&#20154;&#21487;&#20197;&#25552;&#39640;&#31918;&#39135;&#12289;&#39282;&#26009;&#21644;&#32420;&#32500;&#30340;&#29983;&#20135;&#25928;&#29575;&#21644;&#21487;&#25345;&#32493;&#24615;&#12290;&#20316;&#29289;&#21644;&#26434;&#33609;&#30340;&#24863;&#30693;&#26159;&#20892;&#19994;&#26426;&#22120;&#20154;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65292;&#26088;&#22312;&#33258;&#21160;&#30417;&#27979;&#30000;&#37326;&#21644;&#35780;&#20272;&#26893;&#29289;&#21450;&#20854;&#29983;&#38271;&#38454;&#27573;&#12290;&#35821;&#20041;&#24863;&#30693;&#20027;&#35201;&#20381;&#36182;&#20110;&#20351;&#29992;&#26377;&#30417;&#30563;&#26041;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#65292;&#36825;&#38656;&#35201;&#26102;&#38388;&#21644;&#35757;&#32451;&#26377;&#32032;&#30340;&#24037;&#20316;&#20154;&#21592;&#26631;&#35760;&#22823;&#37327;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#19981;&#24433;&#21709;&#26368;&#32456;&#20998;&#21106;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#26631;&#31614;&#25968;&#37327;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#26469;&#33258;&#30446;&#26631;&#39046;&#22495;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#24335;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#29305;&#23450;&#39046;&#22495;&#30340;&#22686;&#24378;&#31574;&#30053;&#12290;&#25105;&#20204;&#23545;&#35821;&#20041;&#20998;&#21106;&#24615;&#33021;&#30340;&#39044;&#35757;&#32451;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agricultural robots have the prospect to enable more efficient and sustainable agricultural production of food, feed, and fiber. Perception of crops and weeds is a central component of agricultural robots that aim to monitor fields and assess the plants as well as their growth stage in an automatic manner. Semantic perception mostly relies on deep learning using supervised approaches, which require time and qualified workers to label fairly large amounts of data. In this paper, we look into the problem of reducing the amount of labels without compromising the final segmentation performance. For robots operating in the field, pre-training networks in a supervised way is already a popular method to reduce the number of required labeled images. We investigate the possibility of pre-training in a self-supervised fashion using data from the target domain. To better exploit this data, we propose a set of domain-specific augmentation strategies. We evaluate our pre-training on semantic segmen
&lt;/p&gt;</description></item><item><title>&#26032;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#35745;&#31639;&#36125;&#21494;&#26031;&#39118;&#38505;&#19979;&#30028;&#65292;&#20801;&#35768;&#20351;&#29992;&#20960;&#20046;&#20219;&#20309;&#20449;&#24687;&#24230;&#37327;&#65292;&#33021;&#25552;&#20379;&#19982;&#20272;&#35745;&#22120;&#26080;&#20851;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#12290;&#24050;&#24212;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#21442;&#25968;&#38382;&#39064;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2303.12497</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#24230;&#37327;&#30340;&#19979;&#30028;&#36125;&#21494;&#26031;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Lower Bound on the Bayesian Risk via Information Measure. (arXiv:2303.12497v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12497
&lt;/p&gt;
&lt;p&gt;
&#26032;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#35745;&#31639;&#36125;&#21494;&#26031;&#39118;&#38505;&#19979;&#30028;&#65292;&#20801;&#35768;&#20351;&#29992;&#20960;&#20046;&#20219;&#20309;&#20449;&#24687;&#24230;&#37327;&#65292;&#33021;&#25552;&#20379;&#19982;&#20272;&#35745;&#22120;&#26080;&#20851;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#12290;&#24050;&#24212;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#21442;&#25968;&#38382;&#39064;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#21442;&#25968;&#20272;&#35745;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#36125;&#21494;&#26031;&#39118;&#38505;&#19979;&#30028;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#20351;&#29992;&#20960;&#20046;&#20219;&#20309;&#20449;&#24687;&#24230;&#37327;&#65292;&#21253;&#25324;R&#233;nyi&#30340;&#945;&#65292;&#966;-&#20998;&#27495;&#21644;Sibson&#30340;&#945;-&#20114;&#20449;&#24687;&#12290;&#35813; &#26041;&#27861;&#23558;&#20998;&#27495;&#35270;&#20026;&#24230;&#37327;&#30340;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#24230;&#37327;&#31354;&#38388;&#21644;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#30340;&#23545;&#20598;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#19981;&#31561;&#24335;&#23545;&#20854;&#23545;&#20598;&#36827;&#34892;&#19978;&#30028;&#38480;&#21046;&#65292;&#23601;&#21487;&#20197;&#29992;&#20219;&#20309;&#20449;&#24687;&#24230;&#37327;&#35745;&#31639;&#39118;&#38505;&#30340;&#19979;&#30028;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;&#20998;&#27495;&#28385;&#36275;&#25968;&#25454;&#22788;&#29702;&#19981;&#31561;&#24335;&#65292;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#19982;&#20272;&#35745;&#22120;&#26080;&#20851;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#12290;&#28982;&#21518;&#23558;&#36825;&#20123;&#32467;&#26524;&#24212;&#29992;&#20110;&#28041;&#21450;&#31163;&#25955;&#21644;&#36830;&#32493;&#21442;&#25968;&#30340;&#26377;&#36259;&#38382;&#39064;&#65292;&#21253;&#25324;&#8220;&#25417;&#36855;&#34255;&#8221;&#38382;&#39064;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#12290;&#37325;&#35201;&#30340;&#35266;&#23519;&#26159;&#19979;&#30028;&#22312;&#26679;&#26412;&#25968;&#19978;&#30340;&#34892;&#20026;&#21463;&#21040;t&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on parameter estimation and introduces a new method for lower bounding the Bayesian risk. The method allows for the use of virtually \emph{any} information measure, including R\'enyi's $\alpha$, $\varphi$-Divergences, and Sibson's $\alpha$-Mutual Information. The approach considers divergences as functionals of measures and exploits the duality between spaces of measures and spaces of functions. In particular, we show that one can lower bound the risk with any information measure by upper bounding its dual via Markov's inequality. We are thus able to provide estimator-independent impossibility results thanks to the Data-Processing Inequalities that divergences satisfy. The results are then applied to settings of interest involving both discrete and continuous parameters, including the ``Hide-and-Seek'' problem, and compared to the state-of-the-art techniques. An important observation is that the behaviour of the lower bound in the number of samples is influenced by t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#24494;&#35843;&#26694;&#26550;&#65292;&#38024;&#23545;&#23569;&#26679;&#26412;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#23398;&#20064;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#23427;&#37319;&#29992;&#19968;&#31181;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#39592;&#26550;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#21644;&#27599;&#20010;&#20219;&#21153;&#30340;&#35821;&#35328;&#29305;&#23450;&#30340;&#24494;&#35843;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.12489</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#12289;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22810;&#35821;&#35328;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-shot Multimodal Multitask Multilingual Learning. (arXiv:2303.12489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12489
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#24494;&#35843;&#26694;&#26550;&#65292;&#38024;&#23545;&#23569;&#26679;&#26412;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#23398;&#20064;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#23427;&#37319;&#29992;&#19968;&#31181;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#39592;&#26550;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#21644;&#27599;&#20010;&#20219;&#21153;&#30340;&#35821;&#35328;&#29305;&#23450;&#30340;&#24494;&#35843;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#65292;&#22312;&#25968;&#25454;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;&#20027;&#35201;&#26159;&#22312;&#26500;&#24314;&#21333;&#27169;&#24577;&#21333;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#25506;&#35752;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#29616;&#26377;&#30340;&#23569;&#26679;&#26412;&#22810;&#20219;&#21153;&#23398;&#20064;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#25991;&#29486;&#37117;&#26159;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30340;&#65292;&#38656;&#35201;&#25163;&#21160;&#29983;&#25104;&#25552;&#31034;&#20316;&#20026;&#36755;&#20837;&#65292;&#36825;&#23548;&#33268;&#20102;&#32467;&#26524;&#30340;&#24046;&#24322;&#65292;&#21462;&#20915;&#20110;&#25163;&#21160;&#25552;&#31034;&#24037;&#31243;&#30340;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20250;&#24102;&#26469;&#30456;&#24403;&#22823;&#30340;&#35745;&#31639;&#12289;&#20869;&#23384;&#12289;&#23384;&#20648;&#25104;&#26412;&#65292;&#26368;&#32456;&#23548;&#33268;&#39640;&#25512;&#29702;&#24310;&#36831;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#27599;&#27425;&#36827;&#34892;&#39044;&#27979;&#26102;&#37117;&#35201;&#36890;&#36807;&#27169;&#22411;&#36816;&#34892;&#25152;&#26377;&#25552;&#31034;&#30340;&#31034;&#20363;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36890;&#36807;&#24494;&#35843;&#33539;&#24335;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#36991;&#20813;&#20102;&#19978;&#36848;&#38382;&#39064;&#65292;&#22312;&#20219;&#21153;&#30340;&#22522;&#30784;&#19978;&#20197;&#19968;&#27425;&#24615;&#30340;&#20195;&#20215;&#24494;&#35843;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#23569;&#26679;&#26412;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#32463;&#39564;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#24494;&#35843;&#26694;&#26550;&#65292;&#38024;&#23545;&#23569;&#26679;&#26412;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#22810;&#35821;&#35328;&#23398;&#20064;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#19968;&#31181;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#39592;&#26550;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#21644;&#27599;&#20010;&#20219;&#21153;&#30340;&#35821;&#35328;&#29305;&#23450;&#30340;&#24494;&#35843;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#34920;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
While few-shot learning as a transfer learning paradigm has gained significant traction for scenarios with limited data, it has primarily been explored in the context of building unimodal and unilingual models. Furthermore, a significant part of the existing literature in the domain of few-shot multitask learning perform in-context learning which requires manually generated prompts as the input, yielding varying outcomes depending on the level of manual prompt-engineering. In addition, in-context learning suffers from substantial computational, memory, and storage costs which eventually leads to high inference latency because it involves running all of the prompt's examples through the model every time a prediction is made. In contrast, methods based on the transfer learning via the fine-tuning paradigm avoid the aforementioned issues at a one-time cost of fine-tuning weights on a per-task basis. However, such methods lack exposure to few-shot multimodal multitask learning. In this pap
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#20174;&#20154;&#31867;&#31034;&#33539;&#20013;&#23398;&#20064;&#27010;&#29575;&#21147;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#22312;&#26426;&#22120;&#20154;&#32452;&#35013;&#20219;&#21153;&#20013;&#23545;&#20302;&#38646;&#20214;&#38388;&#38553;&#21644;&#20301;&#32622;&#21464;&#21270;&#20570;&#20986;&#21453;&#24212;&#65292;&#35299;&#20915;&#20102;&#20174;&#31163;&#32447;&#27169;&#25311;&#25968;&#25454;&#23398;&#20064;&#31574;&#30053;&#26080;&#27861;&#30452;&#25509;&#22312;&#32447;&#24212;&#29992;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12440</link><description>&lt;p&gt;
&#23398;&#20064;&#20154;&#31867;&#21551;&#21457;&#30340;&#21147;&#31574;&#30053;&#29992;&#20110;&#26426;&#22120;&#20154;&#32452;&#35013;
&lt;/p&gt;
&lt;p&gt;
Learning Human-Inspired Force Strategies for Robotic Assembly. (arXiv:2303.12440v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#20174;&#20154;&#31867;&#31034;&#33539;&#20013;&#23398;&#20064;&#27010;&#29575;&#21147;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#22312;&#26426;&#22120;&#20154;&#32452;&#35013;&#20219;&#21153;&#20013;&#23545;&#20302;&#38646;&#20214;&#38388;&#38553;&#21644;&#20301;&#32622;&#21464;&#21270;&#20570;&#20986;&#21453;&#24212;&#65292;&#35299;&#20915;&#20102;&#20174;&#31163;&#32447;&#27169;&#25311;&#25968;&#25454;&#23398;&#20064;&#31574;&#30053;&#26080;&#27861;&#30452;&#25509;&#22312;&#32447;&#24212;&#29992;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#32452;&#35013;&#20219;&#21153;&#30340;&#32534;&#31243;&#26159;&#21046;&#36896;&#19994;&#21644;&#33258;&#21160;&#21270;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20855;&#26377;&#21147;&#25935;&#24863;&#23646;&#24615;&#30340;&#32452;&#35013;&#65292;&#24120;&#24120;&#38656;&#35201;&#21453;&#24212;&#24615;&#31574;&#30053;&#26469;&#22788;&#29702;&#24494;&#23567;&#30340;&#20301;&#32622;&#21464;&#21270;&#21644;&#24847;&#22806;&#30340;&#38646;&#20214;&#21345;&#27515;&#12290;&#20174;&#20154;&#31867;&#34920;&#29616;&#20013;&#23398;&#20064;&#36825;&#26679;&#30340;&#31574;&#30053;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20294;&#38754;&#20020;&#20004;&#20010;&#24120;&#35265;&#30340;&#25361;&#25112;&#65306;&#22788;&#29702;&#20302;&#37096;&#20998;&#38388;&#38553;&#30340;&#38590;&#24230;&#24456;&#22823;&#65292;&#38590;&#20197;&#20174;&#28436;&#31034;&#20013;&#33719;&#21462;&#65292;&#24182;&#23398;&#20064;&#30452;&#35266;&#30340;&#31574;&#30053;&#65292;&#32780;&#26080;&#38656;&#22312;&#31163;&#32447;&#29366;&#24577;&#19979;&#35775;&#38382;&#30495;&#23454;&#30828;&#20214;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#25805;&#32437;&#26438;&#30340;&#20154;&#31867;&#28436;&#31034;&#20013;&#22312;&#27809;&#26377;&#26426;&#22120;&#20154;&#30340;&#27169;&#25311;&#29615;&#22659;&#20013;&#36731;&#26494;&#33719;&#21462;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#27010;&#29575;&#21147;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#65288;MDN&#65289;&#26469;&#27169;&#25311;&#20154;&#31867;&#21551;&#21457;&#24335;&#34892;&#20026;&#65292;&#20351;&#23398;&#20064;&#30340;&#31574;&#30053;&#26131;&#20110;&#36716;&#31227;&#21040;&#30495;&#23454;&#30828;&#20214;&#19978;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;UR10e&#26426;&#22120;&#20154;&#21487;&#20197;&#23436;&#25104;&#22609;&#26009;&#32452;&#35013;&#20219;&#21153;&#65292;&#20854;&#38388;&#38553;&#23567;&#20110;100&#24494;&#31859;&#12290;
&lt;/p&gt;
&lt;p&gt;
The programming of robotic assembly tasks is a key component in manufacturing and automation. Force-sensitive assembly, however, often requires reactive strategies to handle slight changes in positioning and unforeseen part jamming. Learning such strategies from human performance is a promising approach, but faces two common challenges: the handling of low part clearances which is difficult to capture from demonstrations and learning intuitive strategies offline without access to the real hardware. We address these two challenges by learning probabilistic force strategies from data that are easily acquired offline in a robot-less simulation from human demonstrations with a joystick. We combine a Long Short Term Memory (LSTM) and a Mixture Density Network (MDN) to model human-inspired behavior in such a way that the learned strategies transfer easily onto real hardware. The experiments show a UR10e robot that completes a plastic assembly with clearances of less than 100 micrometers whos
&lt;/p&gt;</description></item><item><title>&#21457;&#23637;&#20102;&#19968;&#31181;&#38750;&#30417;&#30563;&#22495;&#36866;&#24212;&#31639;&#27861;&#65292;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#19981;&#30456;&#20851;&#26465;&#20214;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#28041;&#21450;&#20107;&#20214;&#30456;&#26426;&#22270;&#29255;&#20998;&#31867;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#24212;&#23545;&#25968;&#25454;&#31232;&#32570;&#30340;&#22256;&#22659;&#65292;&#24182;&#19988;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.12424</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#19981;&#30456;&#20851;&#26465;&#20214;&#30340;&#38750;&#30417;&#30563;&#22495;&#36866;&#24212;&#65292;&#35757;&#32451;&#38754;&#21521;&#20107;&#20214;&#30340;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Domain Adaptation for Training Event-Based Networks Using Contrastive Learning and Uncorrelated Conditioning. (arXiv:2303.12424v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12424
&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#20102;&#19968;&#31181;&#38750;&#30417;&#30563;&#22495;&#36866;&#24212;&#31639;&#27861;&#65292;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#19981;&#30456;&#20851;&#26465;&#20214;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#28041;&#21450;&#20107;&#20214;&#30456;&#26426;&#22270;&#29255;&#20998;&#31867;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#24212;&#23545;&#25968;&#25454;&#31232;&#32570;&#30340;&#22256;&#22659;&#65292;&#24182;&#19988;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#30456;&#26426;&#22312;&#39640;&#21160;&#24577;&#33539;&#22260;&#29615;&#22659;&#21644;&#24555;&#36895;&#36816;&#21160;&#26426;&#21160;&#26399;&#38388;&#36827;&#34892;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#26102;&#25552;&#20379;&#21487;&#38752;&#30340;&#27979;&#37327;&#12290;&#28982;&#32780;&#65292;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#20107;&#20214;&#39537;&#21160;&#35270;&#35273;&#38754;&#20020;&#30528;&#30001;&#20110;&#20107;&#20214;&#30456;&#26426;&#30340;&#26368;&#36817;&#24615;&#32780;&#23548;&#33268;&#30340;&#27880;&#37322;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#12290;&#20174;&#20256;&#32479;&#30456;&#26426;&#27880;&#37322;&#25968;&#25454;&#20013;&#33719;&#21462;&#30340;&#30693;&#35782;&#36716;&#31227;&#25552;&#20379;&#20102;&#36825;&#19968;&#25361;&#25112;&#30340;&#23454;&#29992;&#35299;&#20915;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38750;&#30417;&#30563;&#22495;&#36866;&#24212;&#31639;&#27861;&#65292;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#19981;&#30456;&#20851;&#25968;&#25454;&#26465;&#20214;&#26469;&#35757;&#32451;&#38754;&#21521;&#20107;&#20214;&#30340;&#25968;&#25454;&#22270;&#20687;&#20998;&#31867;&#30340;&#28145;&#24230;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20248;&#20110;&#29616;&#26377;&#30340;&#27492;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event-based cameras offer reliable measurements for preforming computer vision tasks in high-dynamic range environments and during fast motion maneuvers. However, adopting deep learning in event-based vision faces the challenge of annotated data scarcity due to recency of event cameras. Transferring the knowledge that can be obtained from conventional camera annotated data offers a practical solution to this challenge. We develop an unsupervised domain adaptation algorithm for training a deep network for event-based data image classification using contrastive learning and uncorrelated conditioning of data. Our solution outperforms the existing algorithms for this purpose.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#24310;&#36831;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;(DFL)&#65292;&#36890;&#36807;&#35299;&#20915;&#36793;&#32536;&#21644;&#20113;&#20043;&#38388;&#30340;&#36890;&#20449;&#24310;&#36831;&#65292;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20123;&#25919;&#31574;&#20197;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#21644;&#36793;&#32536;&#21040;&#20113;&#31471;&#30340;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2303.12414</link><description>&lt;p&gt;
&#24310;&#36831;&#24863;&#30693;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Delay-Aware Hierarchical Federated Learning. (arXiv:2303.12414v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#24310;&#36831;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;(DFL)&#65292;&#36890;&#36807;&#35299;&#20915;&#36793;&#32536;&#21644;&#20113;&#20043;&#38388;&#30340;&#36890;&#20449;&#24310;&#36831;&#65292;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20123;&#25919;&#31574;&#20197;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#21644;&#36793;&#32536;&#21040;&#20113;&#31471;&#30340;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#35757;&#32451;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#24310;&#36831;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;(DFL)&#65292;&#36890;&#36807;&#35299;&#20915;&#36793;&#32536;&#21644;&#20113;&#20043;&#38388;&#30340;&#36890;&#20449;&#24310;&#36831;&#65292;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#25928;&#29575;&#12290;DFL&#22312;&#27599;&#20010;&#20840;&#23616;&#32858;&#21512;&#38388;&#38548;&#26399;&#38388;&#23545;&#35774;&#22791;&#25968;&#25454;&#38598;&#25191;&#34892;&#22810;&#20010;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36845;&#20195;&#65292;&#24182;&#36890;&#36807;&#36793;&#32536;&#26381;&#21153;&#22120;&#22312;&#26412;&#22320;&#23376;&#32593;&#32476;&#20013;&#38388;&#26029;&#22320;&#32858;&#21512;&#27169;&#22411;&#21442;&#25968;&#12290;&#20113;&#26381;&#21153;&#22120;&#36890;&#36807;&#23616;&#37096;-&#20840;&#23616;&#21512;&#24182;&#22120;&#23558;&#26412;&#22320;&#27169;&#22411;&#19982;&#20840;&#23616;&#37096;&#32626;&#27169;&#22411;&#21516;&#27493;&#12290;DFL&#30340;&#25910;&#25947;&#34892;&#20026;&#22312;&#24191;&#20041;&#25968;&#25454;&#24322;&#36136;&#24615;&#24230;&#37327;&#19979;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#12290;&#24471;&#20986;&#20102;&#19968;&#32452;&#26465;&#20214;&#65292;&#20197;&#23454;&#29616;O(1/k)&#30340;&#27425;&#32447;&#24615;&#25910;&#25947;&#29575;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#25511;&#21046;&#31639;&#27861;&#26469;&#23454;&#29616;DFL&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20123;&#25919;&#31574;&#20197;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#21644;&#36793;&#32536;&#21040;&#20113;&#31471;&#30340;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has gained popularity as a means of training models distributed across the wireless edge. The paper introduces delay-aware federated learning (DFL) to improve the efficiency of distributed machine learning (ML) model training by addressing communication delays between edge and cloud. DFL employs multiple stochastic gradient descent iterations on device datasets during each global aggregation interval and intermittently aggregates model parameters through edge servers in local subnetworks. The cloud server synchronizes the local models with the global deployed model computed via a local-global combiner at global synchronization. The convergence behavior of DFL is theoretically investigated under a generalized data heterogeneity metric. A set of conditions is obtained to achieve the sub-linear convergence rate of O(1/k). Based on these findings, an adaptive control algorithm is developed for DFL, implementing policies to mitigate energy consumption and edge-to-cloud co
&lt;/p&gt;</description></item><item><title>EDGI&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#31639;&#27861;&#65292;&#36890;&#36807;&#31561;&#21464;&#25193;&#25955;&#22788;&#29702;&#20869;&#22312;&#23545;&#31216;&#24615;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#37319;&#26679;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#20869;&#22312;&#23545;&#31216;&#24615;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.12410</link><description>&lt;p&gt;
EDGI: &#20869;&#22312;&#23545;&#31216;&#24615;&#35268;&#21010;&#30340;&#31561;&#21464;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
EDGI: Equivariant Diffusion for Planning with Embodied Agents. (arXiv:2303.12410v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12410
&lt;/p&gt;
&lt;p&gt;
EDGI&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#31639;&#27861;&#65292;&#36890;&#36807;&#31561;&#21464;&#25193;&#25955;&#22788;&#29702;&#20869;&#22312;&#23545;&#31216;&#24615;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#37319;&#26679;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#20869;&#22312;&#23545;&#31216;&#24615;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#22312;&#23545;&#31216;&#24615;&#26159;&#26102;&#31354;&#21644;&#25490;&#21015;&#19978;&#30340;&#65292;&#22823;&#22810;&#25968;&#35745;&#21010;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#27809;&#26377;&#32771;&#34385;&#36825;&#31181;&#20016;&#23500;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#23548;&#33268;&#37319;&#26679;&#25928;&#29575;&#20302;&#21644;&#27867;&#21270;&#33021;&#21147;&#24369;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#22312;&#23545;&#31216;&#24615;&#35268;&#21010;&#30340;&#31561;&#21464;&#25193;&#25955;&#31639;&#27861;(EDGI), &#21487;&#29992;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#25903;&#25345;&#22810;&#31181;&#34920;&#31034;&#24418;&#24335;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied agents operate in a structured world, often solving tasks with spatial, temporal, and permutation symmetries. Most algorithms for planning and model-based reinforcement learning (MBRL) do not take this rich geometric structure into account, leading to sample inefficiency and poor generalization. We introduce the Equivariant Diffuser for Generating Interactions (EDGI), an algorithm for MBRL and planning that is equivariant with respect to the product of the spatial symmetry group $\mathrm{SE(3)}$, the discrete-time translation group $\mathbb{Z}$, and the object permutation group $\mathrm{S}_n$. EDGI follows the Diffuser framework (Janner et al. 2022) in treating both learning a world model and planning in it as a conditional generative modeling problem, training a diffusion model on an offline trajectory dataset. We introduce a new $\mathrm{SE(3)} \times \mathbb{Z} \times \mathrm{S}_n$-equivariant diffusion model that supports multiple representations. We integrate this model i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#20174;&#23616;&#37096;&#25193;&#23637;&#21040;&#20840;&#22495;&#21644;&#22810;&#23610;&#24230;&#33539;&#22260;&#20869;&#65292;&#21462;&#24471;&#20102;&#27604;ViT&#21644;AFNO&#26356;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2303.12398</link><description>&lt;p&gt;
&#36890;&#36807;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23454;&#29616;Transformers&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Multiscale Attention via Wavelet Neural Operators for Vision Transformers. (arXiv:2303.12398v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#20174;&#23616;&#37096;&#25193;&#23637;&#21040;&#20840;&#22495;&#21644;&#22810;&#23610;&#24230;&#33539;&#22260;&#20869;&#65292;&#21462;&#24471;&#20102;&#27604;ViT&#21644;AFNO&#26356;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#25104;&#21151;&#12290;&#20854;&#26680;&#24515;&#26159;&#33258;&#27880;&#24847;&#26426;&#21046;&#65288;SA&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#24402;&#32435;&#20559;&#35265;&#65292;&#36890;&#36807;&#21152;&#26435;&#22522;&#30784;&#23558;&#36755;&#20837;&#20013;&#30340;&#27599;&#20010;token&#19982;&#27599;&#20010;&#20854;&#20182;token&#30456;&#20851;&#32852;&#12290;&#26631;&#20934;&#30340;SA&#26426;&#21046;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;&#38590;&#20197;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20013;&#20986;&#29616;&#30340;&#38271;&#24207;&#21015;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#30340;Multiscale Wavelet Attention&#65288;MWA&#65289;&#65292;&#20351;&#29992;&#23567;&#27874;&#31070;&#32463;&#31639;&#23376;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#20174;&#23616;&#37096;&#25193;&#23637;&#21040;&#20840;&#22495;&#21644;&#22810;&#23610;&#24230;&#33539;&#22260;&#20869;&#12290;&#25105;&#20204;&#29992;CIFAR&#21644;ImageNet&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;MWA&#27604;ViT&#21644;AFNO&#37117;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have achieved widespread success in computer vision. At their heart, there is a Self-Attention (SA) mechanism, an inductive bias that associates each token in the input with every other token through a weighted basis. The standard SA mechanism has quadratic complexity with the sequence length, which impedes its utility to long sequences appearing in high resolution vision. Recently, inspired by operator learning for PDEs, Adaptive Fourier Neural Operators (AFNO) were introduced for high resolution attention based on global convolution that is efficiently implemented via FFT. However, the AFNO global filtering cannot well represent small and moderate scale structures that commonly appear in natural images. To leverage the coarse-to-fine scale structures we introduce a Multiscale Wavelet Attention (MWA) by leveraging wavelet neural operators which incurs linear complexity in the sequence size. We replace the attention in ViT with MWA and our experiments with CIFAR and ImageN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24178;&#25200;&#27880;&#20837;&#30340;&#40065;&#26834;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#38024;&#23545;&#37096;&#20998;&#33258;&#21160;&#21270;&#19979;&#38271;&#26102;&#38388;&#20219;&#21153;&#30340;&#24212;&#29992;&#36827;&#34892;&#20248;&#21270;&#65292;&#36890;&#36807;&#21464;&#37327;&#24178;&#25200;&#26469;&#25552;&#21319;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.12375</link><description>&lt;p&gt;
&#37096;&#20998;&#33258;&#21160;&#21270;&#19979;&#30340;&#24178;&#25200;&#27880;&#20837;&#65306;&#38271;&#26102;&#38388;&#20219;&#21153;&#30340;&#40065;&#26834;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Disturbance Injection under Partial Automation: Robust Imitation Learning for Long-horizon Tasks. (arXiv:2303.12375v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24178;&#25200;&#27880;&#20837;&#30340;&#40065;&#26834;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#38024;&#23545;&#37096;&#20998;&#33258;&#21160;&#21270;&#19979;&#38271;&#26102;&#38388;&#20219;&#21153;&#30340;&#24212;&#29992;&#36827;&#34892;&#20248;&#21270;&#65292;&#36890;&#36807;&#21464;&#37327;&#24178;&#25200;&#26469;&#25552;&#21319;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#33258;&#21160;&#21270; (PA) &#25216;&#26415;&#36890;&#36807;&#24341;&#20837;&#26234;&#33021;&#25903;&#25345;&#31995;&#32479;&#65292;&#24050;&#32463;&#24212;&#29992;&#20110;&#24037;&#19994;&#26426;&#26800;&#21644;&#39640;&#32423;&#27773;&#36710;&#20013;&#65292;&#20197;&#20943;&#23569;&#20154;&#31867;&#25805;&#20316;&#30340;&#38271;&#26102;&#38388;&#36127;&#25285;&#12290;&#22312; PA &#19979;&#65292;&#25805;&#20316;&#21592;&#25191;&#34892;&#25163;&#21160;&#25805;&#20316;&#65288;&#25552;&#20379;&#21160;&#20316;&#65289;&#21644;&#33258;&#21160; / &#25163;&#21160;&#27169;&#24335;&#30340;&#25805;&#20316;&#65288;&#27169;&#24335;&#20999;&#25442;&#65289;&#12290;&#30001;&#20110; PA &#32553;&#30701;&#20102;&#25163;&#21160;&#25805;&#20316;&#30340;&#24635;&#26102;&#38388;&#65292;&#22240;&#27492;&#36825;&#20004;&#31181;&#25805;&#20316;&#21487;&#20197;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#39640;&#25928;&#22320;&#22797;&#21046;&#12290;&#38024;&#23545;&#27492;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102; Disturbance Injection under Partial Automation (DIPA) &#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312; DIPA &#20013;&#65292;&#20551;&#35774;&#29366;&#24577;&#20013;&#30340;&#27169;&#24335;&#21644;&#25163;&#21160;&#27169;&#24335;&#19979;&#30340;&#21160;&#20316;&#26159;&#21487;&#35266;&#27979;&#30340;&#65292;&#29992;&#20110;&#23398;&#20064;&#21160;&#20316;&#21644;&#27169;&#24335;&#36716;&#25442;&#31574;&#30053;&#12290;&#20026;&#20102;&#20248;&#21270;&#22312; PA &#19979;&#30340;&#21327;&#21464;&#37327;&#20559;&#31227;&#26368;&#23567;&#21270;&#65292;&#25105;&#20204;&#36890;&#36807;&#21521;&#25805;&#20316;&#21592;&#30340;&#21160;&#20316;&#27880;&#20837;&#24178;&#25200;&#26469;&#40065;&#26834;&#21270;&#19978;&#36848;&#23398;&#20064;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial Automation (PA) with intelligent support systems has been introduced in industrial machinery and advanced automobiles to reduce the burden of long hours of human operation. Under PA, operators perform manual operations (providing actions) and operations that switch to automatic/manual mode (mode-switching). Since PA reduces the total duration of manual operation, these two action and mode-switching operations can be replicated by imitation learning with high sample efficiency. To this end, this paper proposes Disturbance Injection under Partial Automation (DIPA) as a novel imitation learning framework. In DIPA, mode and actions (in the manual mode) are assumed to be observables in each state and are used to learn both action and mode-switching policies. The above learning is robustified by injecting disturbances into the operator's actions to optimize the disturbance's level for minimizing the covariate shift under PA. We experimentally validated the effectiveness of our method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#34588;&#32592;&#65292;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#21644;&#19982;&#25915;&#20987;&#32773;&#20132;&#20114;&#65292;&#24182;&#26377;&#25928;&#25552;&#39640;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#23433;&#20840;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2303.12367</link><description>&lt;p&gt;
AIIPot: &#36866;&#24212;&#24615;&#29289;&#32852;&#32593;&#35774;&#22791;&#26234;&#33021;&#20132;&#20114;&#34588;&#32592;
&lt;/p&gt;
&lt;p&gt;
AIIPot: Adaptive Intelligent-Interaction Honeypot for IoT Devices. (arXiv:2303.12367v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#34588;&#32592;&#65292;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#21644;&#19982;&#25915;&#20987;&#32773;&#20132;&#20114;&#65292;&#24182;&#26377;&#25928;&#25552;&#39640;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#23433;&#20840;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#36830;&#25509;&#35774;&#22791;&#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#12290;&#24212;&#35813;&#24320;&#21457;&#36866;&#21512;&#30340;&#12289;&#32463;&#27982;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;IoT&#35774;&#22791;&#30340;&#28431;&#27934;&#65292;&#20197;&#20415;&#22312;&#25915;&#20987;&#32773;&#21033;&#29992;&#23427;&#20204;&#20043;&#21069;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#27450;&#39575;&#25216;&#26415;&#26159;&#25913;&#21892;IoT&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#19968;&#31181;&#37325;&#35201;&#26041;&#27861;&#65292;&#20854;&#20013;&#34588;&#32592;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#27450;&#39575;&#25216;&#26415;&#65292;&#21487;&#20197;&#27169;&#25311;&#30495;&#23454;&#20132;&#20114;&#24182;&#40723;&#21169;&#26410;&#32463;&#25480;&#26435;&#30340;&#29992;&#25143;&#65288;&#25915;&#20987;&#32773;&#65289;&#21457;&#21160;&#25915;&#20987;&#12290;&#30001;&#20110;IoT&#35774;&#22791;&#30340;&#25968;&#37327;&#21644;&#24322;&#26500;&#24615;&#24040;&#22823;&#65292;&#25163;&#21160;&#21046;&#20316;&#20302;&#20132;&#20114;&#21644;&#39640;&#20132;&#20114;&#34588;&#32592;&#26159;&#19981;&#21487;&#25215;&#21463;&#30340;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#20063;&#35201;&#23547;&#27714;&#21019;&#26032;&#30340;&#26041;&#24335;&#26469;&#26500;&#24314;IoT&#35774;&#22791;&#30340;&#34588;&#32592;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;IoT&#35774;&#22791;&#30340;&#34588;&#32592;&#65292;&#23427;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26469;&#33258;&#21160;&#23398;&#20064;&#21644;&#33258;&#21160;&#19982;&#25915;&#20987;&#32773;&#20132;&#20114;&#12290;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#25552;&#39640;&#20250;&#35805;&#30340;&#20197;&#21450;&#23433;&#20840;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of the Internet of Things (IoT) has raised concerns about the security of connected devices. There is a need to develop suitable and cost-efficient methods to identify vulnerabilities in IoT devices in order to address them before attackers seize opportunities to compromise them. The deception technique is a prominent approach to improving the security posture of IoT systems. Honeypot is a popular deception technique that mimics interaction in real fashion and encourages unauthorised users (attackers) to launch attacks. Due to the large number and the heterogeneity of IoT devices, manually crafting the low and high-interaction honeypots is not affordable. This has forced researchers to seek innovative ways to build honeypots for IoT devices. In this paper, we propose a honeypot for IoT devices that uses machine learning techniques to learn and interact with attackers automatically. The evaluation of the proposed model indicates that our system can improve the session 
&lt;/p&gt;</description></item><item><title>ExBEHRT&#26159;&#19968;&#31181;&#25193;&#23637;Transformer&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#65292;&#23558;&#22810;&#31181;&#31867;&#22411;&#30340;&#35760;&#24405;&#21253;&#25324;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#39044;&#27979;&#19981;&#21516;&#30142;&#30149;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#24182;&#20351;&#29992;&#39044;&#26399;&#26799;&#24230;&#23545;&#32467;&#26524;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.12364</link><description>&lt;p&gt;
ExBEHRT&#65306;&#22522;&#20110;&#30005;&#23376;&#30149;&#21382;&#30340;&#25193;&#23637;Transformer&#39044;&#27979;&#30142;&#30149;&#20122;&#22411;&#21644;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
ExBEHRT: Extended Transformer for Electronic Health Records to Predict Disease Subtypes &amp; Progressions. (arXiv:2303.12364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12364
&lt;/p&gt;
&lt;p&gt;
ExBEHRT&#26159;&#19968;&#31181;&#25193;&#23637;Transformer&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#65292;&#23558;&#22810;&#31181;&#31867;&#22411;&#30340;&#35760;&#24405;&#21253;&#25324;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#39044;&#27979;&#19981;&#21516;&#30142;&#30149;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#24182;&#20351;&#29992;&#39044;&#26399;&#26799;&#24230;&#23545;&#32467;&#26524;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;ExBEHRT&#65292;&#23427;&#26159;BEHRT&#65288;&#24212;&#29992;&#20110;&#30005;&#23376;&#30149;&#21382;&#30340;BERT&#65289;&#30340;&#25193;&#23637;&#29256;&#26412;&#65292;&#24182;&#24212;&#29992;&#19981;&#21516;&#30340;&#31639;&#27861;&#26469;&#35299;&#37322;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#29305;&#24449;&#31354;&#38388;&#20174;&#20165;&#32771;&#34385;&#35786;&#26029;&#21644;&#24739;&#32773;&#24180;&#40836;&#25193;&#23637;&#21040;&#21253;&#25324;&#22810;&#31181;&#31867;&#22411;&#30340;&#35760;&#24405;&#65292;&#21253;&#25324;&#20154;&#21475;&#32479;&#35745;&#23398;&#12289;&#20020;&#24202;&#29305;&#24449;&#12289;&#29983;&#21629;&#20307;&#24449;&#12289;&#21560;&#28895;&#29366;&#24577;&#12289;&#35786;&#26029;&#12289;&#25163;&#26415;&#12289;&#33647;&#29289;&#21644;&#23454;&#39564;&#23460;&#26816;&#26597;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#32479;&#19968;&#19981;&#21516;&#29305;&#24449;&#30340;&#39057;&#29575;&#21644;&#26102;&#38388;&#32500;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38468;&#21152;&#29305;&#24449;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#19981;&#21516;&#30142;&#30149;&#19979;&#28216;&#20219;&#21153;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#20445;&#35777;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#39044;&#26399;&#26799;&#24230;&#30340;&#25913;&#36827;&#26041;&#27861;&#23545;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#35299;&#37322;&#65292;&#35813;&#26041;&#27861;&#20197;&#21069;&#26410;&#24212;&#29992;&#20110;&#23558;EHR&#25968;&#25454;&#19982;Transformer&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#27604;&#20197;&#21069;&#26041;&#27861;&#26356;&#32454;&#31890;&#24230;&#30340;&#35299;&#37322;&#65292;&#22914;&#29305;&#24449;&#21644;&#20196;&#29260;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#32959;&#30244;&#23398;&#24739;&#32773;&#30340;&#27169;&#22411;&#34920;&#31034;&#36827;&#34892;&#32858;&#31867;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ExBEHRT&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#30142;&#30149;&#20122;&#22411;&#21644;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce ExBEHRT, an extended version of BEHRT (BERT applied to electronic health records), and apply different algorithms to interpret its results. While BEHRT considers only diagnoses and patient age, we extend the feature space to several multimodal records, namely demographics, clinical characteristics, vital signs, smoking status, diagnoses, procedures, medications, and laboratory tests, by applying a novel method to unify the frequencies and temporal dimensions of the different features. We show that additional features significantly improve model performance for various downstream tasks in different diseases. To ensure robustness, we interpret model predictions using an adaptation of expected gradients, which has not been previously applied to transformers with EHR data and provides more granular interpretations than previous approaches such as feature and token importances. Furthermore, by clustering the model representations of oncology patients, we show tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#24433;&#21709;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#22240;&#32032;&#26159;softmax&#23545;&#20110;&#38750;&#30495;&#23454;&#26631;&#31614;&#26679;&#26412;&#30340;&#20540;&#30340;&#20998;&#24067;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#32422;&#26463;&#30340;softmax&#25439;&#22833;&#20989;&#25968;&#20197;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12363</link><description>&lt;p&gt;
&#20998;&#24067;&#32422;&#26463;&#30340;softmax&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Distribution-restrained Softmax Loss for the Model Robustness. (arXiv:2303.12363v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#24433;&#21709;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#22240;&#32032;&#26159;softmax&#23545;&#20110;&#38750;&#30495;&#23454;&#26631;&#31614;&#26679;&#26412;&#30340;&#20540;&#30340;&#20998;&#24067;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#32422;&#26463;&#30340;softmax&#25439;&#22833;&#20989;&#25968;&#20197;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36817;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#25239;&#35757;&#32451;&#12289;&#27169;&#22411;&#26550;&#26500;&#20462;&#25913;&#12289;&#25439;&#22833;&#20989;&#25968;&#30340;&#35774;&#35745;&#12289;&#35748;&#35777;&#38450;&#24481;&#31561;&#12290;&#28982;&#32780;&#65292;&#40065;&#26834;&#24615;&#21407;&#21017;&#20173;&#26410;&#34987;&#20805;&#20998;&#29702;&#35299;&#65292;&#30456;&#20851;&#30740;&#31350;&#23578;&#19981;&#36275;&#22815;&#12290;&#26412;&#25991;&#21457;&#29616;&#24433;&#21709;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#19968;&#20010;&#26174;&#33879;&#22240;&#32032;&#26159;softmax&#23545;&#20110;&#38750;&#30495;&#23454;&#26631;&#31614;&#26679;&#26412;&#30340;&#20540;&#30340;&#20998;&#24067;&#29305;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#25915;&#20987;&#21518;&#30340;&#32467;&#26524;&#19982;&#20998;&#24067;&#29305;&#24449;&#39640;&#24230;&#30456;&#20851;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25233;&#21046;softmax&#20998;&#24067;&#22810;&#26679;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#32780;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#26102;&#38388;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the robustness of deep learning models has received widespread attention, and various methods for improving model robustness have been proposed, including adversarial training, model architecture modification, design of loss functions, certified defenses, and so on. However, the principle of the robustness to attacks is still not fully understood, also the related research is still not sufficient. Here, we have identified a significant factor that affects the robustness of models: the distribution characteristics of softmax values for non-real label samples. We found that the results after an attack are highly correlated with the distribution characteristics, and thus we proposed a loss function to suppress the distribution diversity of softmax. A large number of experiments have shown that our method can improve robustness without significant time consumption.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;WPGD&#65292;&#21033;&#29992;Wasserstein&#36317;&#31163;&#38480;&#21046;&#27491;&#24120;&#26679;&#26412;&#21644;&#23545;&#25239;&#24615;&#20363;&#23376;&#20043;&#38388;&#30340;&#25200;&#21160;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25104;&#21151;&#22320;&#23545;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.12357</link><description>&lt;p&gt;
Wasserstein&#31354;&#38388;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#29992;&#20110;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Adversarial Examples on Univariant Time Series Data. (arXiv:2303.12357v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12357
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;WPGD&#65292;&#21033;&#29992;Wasserstein&#36317;&#31163;&#38480;&#21046;&#27491;&#24120;&#26679;&#26412;&#21644;&#23545;&#25239;&#24615;&#20363;&#23376;&#20043;&#38388;&#30340;&#25200;&#21160;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25104;&#21151;&#22320;&#23545;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#20363;&#23376;&#26159;&#36890;&#36807;&#21521;&#27491;&#24120;&#26679;&#26412;&#28155;&#21152;&#26080;&#27861;&#21306;&#20998;&#30340;&#25200;&#21160;&#26469;&#27450;&#39575;&#33391;&#22909;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#20854;&#38169;&#35823;&#22320;&#20998;&#31867;&#12290;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#32972;&#26223;&#19979;&#65292;&#21306;&#20998;&#24230;&#30340;&#27010;&#24565;&#36890;&#24120;&#30001;$L_{\infty}$&#25110;&#20854;&#20182;&#35268;&#33539;&#26469;&#38480;&#23450;&#65292;&#20294;&#26159;&#36825;&#20123;&#35268;&#33539;&#19981;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#21306;&#20998;&#24230;&#34913;&#37327;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25552;&#20986;&#20102;Wasserstein&#31354;&#38388;&#20013;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#65292;&#24182;&#21033;&#29992;Wasserstein&#36317;&#31163;&#26469;&#38480;&#23450;&#27491;&#24120;&#26679;&#26412;&#21644;&#23545;&#25239;&#24615;&#20363;&#23376;&#20043;&#38388;&#30340;&#25200;&#21160;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Wasserstein&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#65288;WPGD&#65289;&#65292;&#19968;&#31181;&#25200;&#21160;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;1D&#31354;&#38388;&#20013;Wasserstein&#36317;&#31163;&#30340;&#23553;&#38381;&#24418;&#24335;&#35299;&#26469;&#35745;&#31639;WPGD&#30340;&#25237;&#24433;&#27493;&#39588;&#65292;&#20197;&#26368;&#23567;&#21270;&#25200;&#21160;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20004;&#27493;&#25237;&#24433;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#36827;&#34892;&#22312;Wasserstein&#31354;&#38388;&#20013;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#25628;&#32034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;WPGD&#33021;&#22815;&#25104;&#21151;&#22320;&#23545;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples are crafted by adding indistinguishable perturbations to normal examples in order to fool a well-trained deep learning model to misclassify. In the context of computer vision, this notion of indistinguishability is typically bounded by $L_{\infty}$ or other norms. However, these norms are not appropriate for measuring indistinguishiability for time series data. In this work, we propose adversarial examples in the Wasserstein space for time series data for the first time and utilize Wasserstein distance to bound the perturbation between normal examples and adversarial examples. We introduce Wasserstein projected gradient descent (WPGD), an adversarial attack method for perturbing univariant time series data. We leverage the closed-form solution of Wasserstein distance in the 1D space to calculate the projection step of WPGD efficiently with the gradient descent method. We further propose a two-step projection so that the search of adversarial examples in the Wassers
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#36890;&#36807;&#37327;&#23376;&#36864;&#28779;&#31639;&#27861;&#26469;&#35757;&#32451;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#26679;&#24341;&#25806;&#23454;&#29616;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#35757;&#32451;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#30740;&#31350;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.12352</link><description>&lt;p&gt;
&#21033;&#29992;&#37327;&#23376;&#36864;&#28779;&#22120;&#23545;&#22810;&#23618;&#24863;&#30693;&#26426;&#36827;&#34892;&#37319;&#26679;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Training Multilayer Perceptrons by Sampling with Quantum Annealers. (arXiv:2303.12352v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#36890;&#36807;&#37327;&#23376;&#36864;&#28779;&#31639;&#27861;&#26469;&#35757;&#32451;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#26679;&#24341;&#25806;&#23454;&#29616;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#35757;&#32451;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#30740;&#31350;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#36864;&#28779;&#31639;&#27861;&#22312;&#35757;&#32451;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#30340;&#24212;&#29992;&#36739;&#20026;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29992;&#20110;&#35270;&#35273;&#24212;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#26159;&#21069;&#21521;&#32467;&#26500;&#65292;&#22914;&#22810;&#23618;&#24863;&#30693;&#26426;&#12290;&#21453;&#21521;&#20256;&#25773;&#26159;&#30446;&#21069;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#35757;&#32451;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#26368;&#26377;&#25928;&#25216;&#26415;&#12290;&#26412;&#25991;&#26088;&#22312;&#23637;&#26395;&#26410;&#26469;&#65292;&#25506;&#32034;&#21033;&#29992;&#37327;&#23376;&#36864;&#28779;&#22120;&#36827;&#34892;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#26426;&#19982;&#33021;&#37327;&#27169;&#22411;&#30340;&#31561;&#20215;&#24615;&#65288;EBM&#65289;&#65292;EBM&#26159;&#19968;&#31181;&#20855;&#26377;&#26368;&#22823;&#26465;&#20214;&#20284;&#28982;&#30446;&#26631;&#30340;RBM&#21464;&#31181;&#12290;&#36825;&#23548;&#33268;&#37319;&#29992;&#37327;&#23376;&#36864;&#28779;&#22120;&#20316;&#20026;&#37319;&#26679;&#24341;&#25806;&#26469;&#35757;&#32451;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;Sigmoid&#28608;&#27963;&#20989;&#25968;&#21644;&#19968;&#20010;&#38544;&#34255;&#23618;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#35774;&#32622;&#65292;&#24182;&#28436;&#31034;&#20102;&#22312;MNIST&#21644;Fashion-MNIST&#25968;&#25454;&#38598;&#30340;&#23567;&#23376;&#38598;&#19978;&#20351;&#29992;D-Wave&#37327;&#23376;&#36864;&#28779;&#22120;&#35757;&#32451;&#20108;&#36827;&#21046;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#24403;&#21069;&#37327;&#23376;&#36864;&#28779;&#22120;&#21487;&#34892;&#30340;&#38382;&#39064;&#35268;&#27169;&#26377;&#38480;&#65292;&#20294;&#25105;&#20204;&#33719;&#24471;&#20102;&#20840;&#38754;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A successful application of quantum annealing to machine learning is training restricted Boltzmann machines (RBM). However, many neural networks for vision applications are feedforward structures, such as multilayer perceptrons (MLP). Backpropagation is currently the most effective technique to train MLPs for supervised learning. This paper aims to be forward-looking by exploring the training of MLPs using quantum annealers. We exploit an equivalence between MLPs and energy-based models (EBM), which are a variation of RBMs with a maximum conditional likelihood objective. This leads to a strategy to train MLPs with quantum annealers as a sampling engine. We prove our setup for MLPs with sigmoid activation functions and one hidden layer, and demonstrated training of binary image classifiers on small subsets of the MNIST and Fashion-MNIST datasets using the D-Wave quantum annealer. Although problem sizes that are feasible on current annealers are limited, we obtained comprehensive results
&lt;/p&gt;</description></item><item><title>EasyDGL &#25552;&#20986;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#23398;&#20064;&#30340;&#27969;&#27700;&#32447;&#65292;&#20854;&#20013;&#21253;&#21547;&#32534;&#30721;&#12289;&#35757;&#32451;&#21644;&#35299;&#37322;&#19977;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#23427;&#20351;&#29992;&#26102;&#38388;&#28857;&#36807;&#31243;&#35843;&#21046;&#30340;&#27880;&#24847;&#21147;&#26550;&#26500;&#26469;&#22788;&#29702;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#65292;&#22312;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#25439;&#22833;&#21644;&#20219;&#21153;&#24863;&#30693;&#25439;&#22833;&#30340;&#32452;&#21512;&#19979;&#23454;&#29616;&#21160;&#24577;&#38142;&#25509;&#39044;&#27979;&#12289;&#21160;&#24577;&#33410;&#28857;&#20998;&#31867;&#21644;&#33410;&#28857;&#27969;&#37327;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#25935;&#24863;&#24615;&#20998;&#26512;&#23545;&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.12341</link><description>&lt;p&gt;
EasyDGL: &#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#23398;&#20064;&#30340;&#32534;&#30721;&#12289;&#35757;&#32451;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
EasyDGL: Encode, Train and Interpret for Continuous-time Dynamic Graph Learning. (arXiv:2303.12341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12341
&lt;/p&gt;
&lt;p&gt;
EasyDGL &#25552;&#20986;&#20102;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#23398;&#20064;&#30340;&#27969;&#27700;&#32447;&#65292;&#20854;&#20013;&#21253;&#21547;&#32534;&#30721;&#12289;&#35757;&#32451;&#21644;&#35299;&#37322;&#19977;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#23427;&#20351;&#29992;&#26102;&#38388;&#28857;&#36807;&#31243;&#35843;&#21046;&#30340;&#27880;&#24847;&#21147;&#26550;&#26500;&#26469;&#22788;&#29702;&#36830;&#32493;&#26102;&#38388;&#21160;&#24577;&#22270;&#65292;&#22312;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#25439;&#22833;&#21644;&#20219;&#21153;&#24863;&#30693;&#25439;&#22833;&#30340;&#32452;&#21512;&#19979;&#23454;&#29616;&#21160;&#24577;&#38142;&#25509;&#39044;&#27979;&#12289;&#21160;&#24577;&#33410;&#28857;&#20998;&#31867;&#21644;&#33410;&#28857;&#27969;&#37327;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#25935;&#24863;&#24615;&#20998;&#26512;&#23545;&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#37117;&#24456;&#24120;&#35265;&#65292;&#30452;&#25509;&#22312;&#36830;&#32493;&#26102;&#38388;&#22495;&#20013;&#24314;&#27169;&#21160;&#24577;&#22270;&#20197;&#23454;&#29616;&#28789;&#27963;&#24615;&#26159;&#34987;&#27426;&#36814;&#30340;&#36873;&#25321;&#12290;&#26412;&#25991;&#26088;&#22312;&#35774;&#35745;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#27969;&#27700;&#32447;&#65288;&#21517;&#20026;EasyDGL&#65292;&#20063;&#22240;&#20854;&#30001;DGL&#24037;&#20855;&#21253;&#23454;&#29616;&#32780;&#24471;&#21517;&#65289;&#65292;&#23427;&#30001;&#19977;&#20010;&#20851;&#38190;&#27169;&#22359;&#32452;&#25104;&#65292;&#26082;&#20855;&#26377;&#24378;&#22823;&#30340;&#25311;&#21512;&#33021;&#21147;&#65292;&#21448;&#26131;&#20110;&#35299;&#37322;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25152;&#25552;&#20986;&#30340;&#27969;&#27700;&#32447;&#28041;&#21450;&#32534;&#30721;&#12289;&#35757;&#32451;&#21644;&#35299;&#37322;&#65306;i&#65289;&#26102;&#38388;&#28857;&#36807;&#31243;&#65288;TPP&#65289;&#35843;&#21046;&#30340;&#27880;&#24847;&#21147;&#26550;&#26500;&#23558;&#36830;&#32493;&#26102;&#38388;&#20998;&#36776;&#29575;&#36171;&#20104;&#35266;&#23519;&#21040;&#30340;&#20855;&#26377;&#36793;&#28155;&#21152;&#20107;&#20214;&#30340;&#22270;&#30340;&#32806;&#21512;&#26102;&#31354;&#21160;&#24577;&#65307;ii&#65289;&#19968;&#20010;&#21512;&#29702;&#30340;&#25439;&#22833;&#65292;&#30001;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#22270;&#20013;&#20107;&#20214;&#30340;TPP&#21518;&#39564;&#26368;&#22823;&#21270;&#21644;&#19968;&#20010;&#24102;&#36974;&#34109;&#31574;&#30053;&#30340;&#20219;&#21153;&#24863;&#30693;&#25439;&#22833;&#32452;&#25104;&#65292;&#20854;&#20013;&#28085;&#30422;&#30340;&#20219;&#21153;&#21253;&#25324;&#21160;&#24577;&#38142;&#25509;&#39044;&#27979;&#12289;&#21160;&#24577;&#33410;&#28857;&#20998;&#31867;&#21644;&#33410;&#28857;&#27969;&#37327;&#39044;&#27979;&#65307;iii&#65289;&#36890;&#36807;&#25935;&#24863;&#24615;&#20998;&#26512;&#35299;&#37322;&#27169;&#22411;&#36755;&#20986;&#65288;&#20363;&#22914;&#33410;&#28857;&#21644;&#36793;&#30340;&#34920;&#31034;&#65289;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#29702;&#35299;&#27599;&#20010;&#36793;&#21644;&#33410;&#28857;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic graphs arise in various real-world applications, and it is often welcomed to model the dynamics directly in continuous time domain for its flexibility. This paper aims to design an easy-to-use pipeline (termed as EasyDGL which is also due to its implementation by DGL toolkit) composed of three key modules with both strong fitting ability and interpretability. Specifically the proposed pipeline which involves encoding, training and interpreting: i) a temporal point process (TPP) modulated attention architecture to endow the continuous-time resolution with the coupled spatiotemporal dynamics of the observed graph with edge-addition events; ii) a principled loss composed of task-agnostic TPP posterior maximization based on observed events on the graph, and a task-aware loss with a masking strategy over dynamic graph, where the covered tasks include dynamic link prediction, dynamic node classification and node traffic forecasting; iii) interpretation of the model outputs (e.g., rep
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#20027;&#21160;&#23398;&#20064;&#37319;&#26679;&#31574;&#30053;LoGo&#65292;&#23427;&#33021;&#22815;&#25269;&#24481;&#19981;&#21516;&#30340;&#26412;&#22320;&#24322;&#36136;&#24615;&#27700;&#24179;&#21644;&#20840;&#23616;&#19981;&#24179;&#34913;&#27604;&#20363;&#65292;&#36890;&#36807;&#25972;&#21512;&#20840;&#23616;&#21644;&#20165;&#26412;&#22320;&#26597;&#35810;&#36873;&#25321;&#22120;&#27169;&#22411;&#26469;&#35299;&#20915;&#21452;&#26041;&#19981;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2303.12317</link><description>&lt;p&gt;
&#22522;&#20110;&#31867;&#38388;&#24046;&#24322;&#30340;&#32852;&#37030;&#20027;&#21160;&#23398;&#20064;&#37325;&#26032;&#24605;&#32771;
&lt;/p&gt;
&lt;p&gt;
Re-thinking Federated Active Learning based on Inter-class Diversity. (arXiv:2303.12317v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#20027;&#21160;&#23398;&#20064;&#37319;&#26679;&#31574;&#30053;LoGo&#65292;&#23427;&#33021;&#22815;&#25269;&#24481;&#19981;&#21516;&#30340;&#26412;&#22320;&#24322;&#36136;&#24615;&#27700;&#24179;&#21644;&#20840;&#23616;&#19981;&#24179;&#34913;&#27604;&#20363;&#65292;&#36890;&#36807;&#25972;&#21512;&#20840;&#23616;&#21644;&#20165;&#26412;&#22320;&#26597;&#35810;&#36873;&#25321;&#22120;&#27169;&#22411;&#26469;&#35299;&#20915;&#21452;&#26041;&#19981;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#32852;&#37030;&#23398;&#20064;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#26159;&#23436;&#20840;&#26631;&#35760;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#21487;&#33021;&#20250;&#26377;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#23454;&#20363;&#12290;&#22312;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;&#32852;&#37030;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#20998;&#25955;&#30340;&#35774;&#32622;&#20013;&#65292;&#26377;&#20004;&#31181;&#21487;&#29992;&#30340;&#26597;&#35810;&#36873;&#25321;&#22120;&#27169;&#22411;&#65292;&#21363;&#8220;&#20840;&#23616;&#8221;&#21644;&#8220;&#20165;&#26412;&#22320;&#8221;&#27169;&#22411;&#65292;&#20294;&#24456;&#23569;&#26377;&#25991;&#29486;&#35752;&#35770;&#23427;&#20204;&#30340;&#24615;&#33021;&#20248;&#21155;&#21450;&#20854;&#21407;&#22240;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#20004;&#20010;&#36873;&#25321;&#22120;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#21462;&#20915;&#20110;&#20840;&#23616;&#21644;&#26412;&#22320;&#31867;&#38388;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20840;&#29699;&#21644;&#20165;&#26412;&#22320;&#27169;&#22411;&#26159;&#35299;&#20915;&#21452;&#26041;&#19981;&#24179;&#34913;&#30340;&#20851;&#38190;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;FAL&#25277;&#26679;&#31574;&#30053;LoGo&#65292;&#23427;&#33021;&#22815;&#25269;&#24481;&#19981;&#21516;&#30340;&#26412;&#22320;&#24322;&#36136;&#24615;&#27700;&#24179;&#21644;&#20840;&#23616;&#19981;&#24179;&#34913;&#27604;&#20363;&#65292;&#36890;&#36807;&#20004;&#20010;&#27493;&#39588;&#25972;&#21512;&#20004;&#31181;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although federated learning has made awe-inspiring advances, most studies have assumed that the client's data are fully labeled. However, in a real-world scenario, every client may have a significant amount of unlabeled instances. Among the various approaches to utilizing unlabeled data, a federated active learning framework has emerged as a promising solution. In the decentralized setting, there are two types of available query selector models, namely 'global' and 'local-only' models, but little literature discusses their performance dominance and its causes. In this work, we first demonstrate that the superiority of two selector models depends on the global and local inter-class diversity. Furthermore, we observe that the global and local-only models are the keys to resolving the imbalance of each side. Based on our findings, we propose LoGo, a FAL sampling strategy robust to varying local heterogeneity levels and global imbalance ratio, that integrates both models by two steps of ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#35299;&#37322;&#31639;&#27861;TsSHAP&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#35299;&#37322;&#40657;&#30418;&#39044;&#27979;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#22312;&#21487;&#35299;&#37322;&#29305;&#24449;&#31354;&#38388;&#21644;&#39044;&#27979;&#32467;&#26524;&#20043;&#38388;&#24314;&#31435;&#20102;&#26144;&#23556;&#12290;&#36825;&#31181;&#31639;&#27861;&#21487;&#20197;&#25552;&#20379;&#23616;&#37096;&#65292;&#21322;&#23616;&#37096;&#21644;&#20840;&#23616;&#35299;&#37322;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12316</link><description>&lt;p&gt;
TsSHAP: &#24378;&#22823;&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#29305;&#24449;&#35299;&#37322;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TsSHAP: Robust model agnostic feature-based explainability for time series forecasting. (arXiv:2303.12316v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#35299;&#37322;&#31639;&#27861;TsSHAP&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#35299;&#37322;&#40657;&#30418;&#39044;&#27979;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#22312;&#21487;&#35299;&#37322;&#29305;&#24449;&#31354;&#38388;&#21644;&#39044;&#27979;&#32467;&#26524;&#20043;&#38388;&#24314;&#31435;&#20102;&#26144;&#23556;&#12290;&#36825;&#31181;&#31639;&#27861;&#21487;&#20197;&#25552;&#20379;&#23616;&#37096;&#65292;&#21322;&#23616;&#37096;&#21644;&#20840;&#23616;&#35299;&#37322;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#35813;&#26159;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#12290;&#29702;&#35299;&#27169;&#22411;&#20026;&#20160;&#20040;&#20570;&#20986;&#26576;&#20123;&#20915;&#31574;&#23450;&#20041;&#20102;&#21487;&#35299;&#37322;&#24615;&#30340;&#27010;&#24565;&#12290;&#34429;&#28982;&#24050;&#32463;&#30740;&#31350;&#20102;&#21508;&#31181;&#35299;&#37322;&#24615;&#27169;&#22411;&#65292;&#24182;&#24191;&#27867;&#24212;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#65292;&#22914;&#20998;&#31867;&#21644;&#22238;&#24402;&#65292;&#20294;&#26159;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#35299;&#37322;&#24615;&#25991;&#29486;&#30456;&#23545;&#36739;&#23569;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29305;&#24449;&#35299;&#37322;&#31639;&#27861;TsSHAP&#65292;&#23427;&#21487;&#20197;&#35299;&#37322;&#20219;&#20309;&#40657;&#30418;&#39044;&#27979;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#23545;&#39044;&#27979;&#27169;&#22411;&#19981;&#21487;&#30693;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#39044;&#20808;&#23450;&#20041;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#26469;&#35299;&#37322;&#39044;&#27979;&#32467;&#26524;&#12290; SHAP&#20540;&#30340;&#35299;&#37322;&#26159;&#36890;&#36807;&#22312;&#26367;&#20195;&#27169;&#22411;&#19978;&#24212;&#29992;TreeSHAP&#31639;&#27861;&#33719;&#24471;&#30340;&#65292;&#35813;&#26367;&#20195;&#27169;&#22411;&#23398;&#20064;&#20102;&#21487;&#35299;&#37322;&#29305;&#24449;&#31354;&#38388;&#21644;&#40657;&#30418;&#27169;&#22411;&#39044;&#27979;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#36824;&#35268;&#33539;&#20102;&#23616;&#37096;&#12289;&#21322;&#23616;&#37096;&#21644;&#20840;&#23616;&#35299;&#37322;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
A trustworthy machine learning model should be accurate as well as explainable. Understanding why a model makes a certain decision defines the notion of explainability. While various flavors of explainability have been well-studied in supervised learning paradigms like classification and regression, literature on explainability for time series forecasting is relatively scarce.  In this paper, we propose a feature-based explainability algorithm, TsSHAP, that can explain the forecast of any black-box forecasting model. The method is agnostic of the forecasting model and can provide explanations for a forecast in terms of interpretable features defined by the user a prior.  The explanations are in terms of the SHAP values obtained by applying the TreeSHAP algorithm on a surrogate model that learns a mapping between the interpretable feature space and the forecast of the black-box model.  Moreover, we formalize the notion of local, semi-local, and global explanations in the context of time
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;SUPMER&#65292;&#21253;&#25324;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;&#65292;&#36890;&#36807;&#38170;&#23450;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#21644;&#22522;&#20110;&#35838;&#31243;&#30340;&#20219;&#21153;&#22686;&#24378;&#20016;&#23500;&#20102;&#20219;&#21153;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#33391;&#22909;&#21021;&#22987;&#21270;&#36719;&#25552;&#31034;&#21644;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12314</link><description>&lt;p&gt;
&#20855;&#26377;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#30340;&#33258;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization. (arXiv:2303.12314v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12314
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;SUPMER&#65292;&#21253;&#25324;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;&#65292;&#36890;&#36807;&#38170;&#23450;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#21644;&#22522;&#20110;&#35838;&#31243;&#30340;&#20219;&#21153;&#22686;&#24378;&#20016;&#23500;&#20102;&#20219;&#21153;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#33391;&#22909;&#21021;&#22987;&#21270;&#36719;&#25552;&#31034;&#21644;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#26159;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#36719;&#25552;&#31034;&#24182;&#20351;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#23613;&#31649;&#26377;&#25928;&#65292;&#20294;&#25552;&#31034;&#35843;&#25972;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#19968;&#26041;&#38754;&#20005;&#37325;&#20381;&#36182;&#20110;&#33391;&#22909;&#30340;&#36719;&#25552;&#31034;&#21021;&#22987;&#21270;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#24456;&#23481;&#26131;&#23548;&#33268;&#36807;&#24230;&#25311;&#21512;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#25110;&#30417;&#30563;&#20803;&#23398;&#20064;&#26469;&#21021;&#22987;&#21270;&#36719;&#25552;&#31034;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#23545;&#26410;&#35265;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#25968;&#25454;&#26377;&#25928;&#30340;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;&#65288;SUPMER&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#32452;&#33258;&#30417;&#30563;&#38170;&#23450;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#20219;&#21153;&#26684;&#24335;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#35838;&#31243;&#30340;&#20219;&#21153;&#22686;&#24378;&#36827;&#19968;&#27493;&#20016;&#23500;&#20102;&#20219;&#21153;&#20998;&#24067;&#12290;&#28982;&#21518;&#23558;&#19968;&#31181;&#26032;&#30340;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;&#38598;&#25104;&#21040;&#20803;&#25552;&#31034;&#23398;&#20064;&#20013;&#12290;&#23427;&#20803;&#23398;&#20064;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#22914;&#20309;&#36716;&#25442;&#21407;&#22987;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning is a parameter-efficient method, which learns soft prompts and conditions frozen language models to perform specific downstream tasks. Though effective, prompt tuning under few-shot settings on the one hand heavily relies on a good initialization of soft prompts. On the other hand, it can easily result in overfitting. Existing works leverage pre-training or supervised meta-learning to initialize soft prompts but they cannot data-efficiently generalize to unseen downstream tasks. To address the above problems, this paper proposes a novel Self-sUpervised meta-Prompt learning framework with meta-gradient Regularization for few-shot generalization (SUPMER). We first design a set of self-supervised anchor meta-training tasks with different task formats and further enrich the task distribution with curriculum-based task augmentation. Then a novel meta-gradient regularization method is integrated into meta-prompt learning. It meta-learns to transform the raw gradients during few
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;METS&#30340;&#24515;&#30005;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#24515;&#30005;&#32534;&#30721;&#22120;&#21644;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#20998;&#21035;&#23884;&#20837;&#37197;&#23545;&#30340;&#24515;&#30005;&#22270;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#26426;&#22120;&#20020;&#24202;&#25253;&#21578;&#65292;&#20197;&#23454;&#29616;&#24515;&#30005;&#22270;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#24515;&#30005;&#22270;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12311</link><description>&lt;p&gt;
&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;&#21161;&#21147;&#24515;&#30005;&#20449;&#21495;&#38646;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Frozen Language Model Helps ECG Zero-Shot Learning. (arXiv:2303.12311v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12311
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;METS&#30340;&#24515;&#30005;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#24515;&#30005;&#32534;&#30721;&#22120;&#21644;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#20998;&#21035;&#23884;&#20837;&#37197;&#23545;&#30340;&#24515;&#30005;&#22270;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#26426;&#22120;&#20020;&#24202;&#25253;&#21578;&#65292;&#20197;&#23454;&#29616;&#24515;&#30005;&#22270;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#24515;&#30005;&#22270;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#38750;&#20405;&#20837;&#24335;&#12289;&#26041;&#20415;&#30340;&#21307;&#30103;&#30417;&#27979;&#24037;&#20855;&#65292;&#21487;&#36741;&#21161;&#20020;&#24202;&#35786;&#26029;&#24515;&#33039;&#30142;&#30149;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#65292;&#22312;&#24515;&#30005;&#22270;&#20998;&#31867;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#22312;&#24494;&#35843;&#20043;&#21518;&#65292;SSL&#30340;&#39044;&#35757;&#32451;&#20165;&#20381;&#38752;&#23569;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#23601;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;SSL&#26041;&#27861;&#20381;&#36182;&#20110;&#27880;&#37322;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#26080;&#27861;&#39044;&#27979;&#24494;&#35843;&#25968;&#25454;&#38598;&#20013;&#19981;&#23384;&#22312;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#24515;&#30005;&#22270;&#25991;&#26412;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65288;METS&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#21033;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#20020;&#24202;&#25253;&#21578;&#26469;&#25351;&#23548;&#24515;&#30005;&#22270;SSL&#39044;&#35757;&#32451;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#24515;&#30005;&#32534;&#30721;&#22120;&#21644;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#20998;&#21035;&#23884;&#20837;&#37197;&#23545;&#30340;&#24515;&#30005;&#22270;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#26426;&#22120;&#20020;&#24202;&#25253;&#21578;&#12290;SSL&#26088;&#22312;&#26368;&#22823;&#21270;&#24515;&#30005;&#22270;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#25253;&#21578;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#30830;&#20445;&#24515;&#30005;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#23884;&#20837;&#23545;&#40784;&#33391;&#22909;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#24515;&#30005;&#22270;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;METS&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#24515;&#30005;&#22270;&#20998;&#31867;&#24615;&#33021;&#65292;&#20248;&#20110;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The electrocardiogram (ECG) is one of the most commonly used non-invasive, convenient medical monitoring tools that assist in the clinical diagnosis of heart diseases. Recently, deep learning (DL) techniques, particularly self-supervised learning (SSL), have demonstrated great potential in the classification of ECG. SSL pre-training has achieved competitive performance with only a small amount of annotated data after fine-tuning. However, current SSL methods rely on the availability of annotated data and are unable to predict labels not existing in fine-tuning datasets. To address this challenge, we propose Multimodal ECG-Text Self-supervised pre-training (METS), the first work to utilize the auto-generated clinical reports to guide ECG SSL pre-training. We use a trainable ECG encoder and a frozen language model to embed paired ECG and automatically machine-generated clinical reports separately. The SSL aims to maximize the similarity between paired ECG and auto-generated report while 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#20998;&#26512;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#38754;&#30340;&#36923;&#36753;&#34920;&#36798;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20174;&#20998;&#32423;&#27169;&#24577;&#36923;&#36753;&#20013;&#25429;&#33719;&#36923;&#36753;&#35268;&#21017;&#65292;&#20174;&#32780;&#35774;&#35745;&#20986;&#26356;&#22909;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.12306</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36923;&#36753;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Logical Expressiveness of Graph Neural Network for Knowledge Graph Reasoning. (arXiv:2303.12306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#20998;&#26512;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#38754;&#30340;&#36923;&#36753;&#34920;&#36798;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20174;&#20998;&#32423;&#27169;&#24577;&#36923;&#36753;&#20013;&#25429;&#33719;&#36923;&#36753;&#35268;&#21017;&#65292;&#20174;&#32780;&#35774;&#35745;&#20986;&#26356;&#22909;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#34987;&#24341;&#20837;&#29992;&#20110;&#23398;&#20064;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23427;&#20204;&#33391;&#22909;&#30340;&#32463;&#39564;&#24615;&#33021;&#32570;&#20047;&#29702;&#35770;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#36923;&#36753;&#23545;&#20110;&#24402;&#32435;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#29702;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#29616;&#26377;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#21482;&#26159;&#20026;&#20102;&#36866;&#24212;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#19988;&#23545;&#23427;&#20204;&#30340;&#36923;&#36753;&#34920;&#36798;&#33021;&#21147;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#19978;&#36848;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#36923;&#36753;&#30340;&#34920;&#36798;&#33021;&#21147;&#23545;GNN&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#25214;&#20986;&#30693;&#35782;&#22270;&#35889;&#20013;&#21487;&#20197;&#25429;&#33719;&#21738;&#20123;&#36923;&#36753;&#35268;&#21017;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39318;&#20808;&#34920;&#26126;&#65292;GNN&#21487;&#20197;&#20174;&#20998;&#32423;&#27169;&#24577;&#36923;&#36753;&#20013;&#25429;&#33719;&#36923;&#36753;&#35268;&#21017;&#65292;&#20026;&#20998;&#26512;GNN&#22312;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#38754;&#30340;&#34920;&#36798;&#33021;&#21147;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#24037;&#20855;&#65307;&#32780;&#19968;&#20010;&#26597;&#35810;&#26631;&#35760;&#25216;&#24039;&#20351;&#24471;GNN&#26356;&#23481;&#26131;&#25429;&#33719;&#36923;&#36753;&#35268;&#21017;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#26631;&#35760;&#25216;&#24039;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#30340;&#35265;&#35299;&#20419;&#36827;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;GNN&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26041;&#27861;&#30340;&#35774;&#35745;&#65292;&#23427;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#36923;&#36753;&#34920;&#36798;&#33021;&#21147;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have been recently introduced to learn from knowledge graph (KG) and achieved state-of-the-art performance in KG reasoning. However, a theoretical certification for their good empirical performance is still absent. Besides, while logic in KG is important for inductive and interpretable inference, existing GNN-based methods are just designed to fit data distributions with limited knowledge of their logical expressiveness. We propose to fill the above gap in this paper. Specifically, we theoretically analyze GNN from logical expressiveness and find out what kind of logical rules can be captured from KG. Our results first show that GNN can capture logical rules from graded modal logic, providing a new theoretical tool for analyzing the expressiveness of GNN for KG reasoning; and a query labeling trick makes it easier for GNN to capture logical rules, explaining why SOTA methods are mainly based on labeling trick. Finally, insights from our theory motivate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#39134;&#34892;&#25805;&#20316;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#20351;&#29992;&#37327;&#23376;&#20860;&#23481;&#31163;&#25955;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;DVAE&#65289;&#65292;&#20854;&#20013;&#20855;&#26377;&#38480;&#21046;&#29627;&#23572;&#20857;&#26364;&#26426;&#30340;&#20808;&#39564;&#65292;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12302</link><description>&lt;p&gt;
&#37327;&#23376;&#20860;&#23481;&#31163;&#25955;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#33322;&#31354;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Anomaly Detection in Aeronautics Data with Quantum-compatible Discrete Deep Generative Model. (arXiv:2303.12302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#39134;&#34892;&#25805;&#20316;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#20351;&#29992;&#37327;&#23376;&#20860;&#23481;&#31163;&#25955;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;DVAE&#65289;&#65292;&#20854;&#20013;&#20855;&#26377;&#38480;&#21046;&#29627;&#23572;&#20857;&#26364;&#26426;&#30340;&#20808;&#39564;&#65292;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#23398;&#20064;&#19981;&#20165;&#21487;&#29992;&#20110;&#29983;&#25104;&#32479;&#35745;&#29305;&#24449;&#28304;&#20110;&#36755;&#20837;&#25968;&#25454;&#30340;&#26032;&#25968;&#25454;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#26681;&#25454;&#37325;&#26500;&#36136;&#37327;&#23558;&#21407;&#26412;&#25968;&#25454;&#20998;&#20026;&#27491;&#24120;&#21644;&#24322;&#24120;&#23454;&#20363;&#65292;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19977;&#31181;&#26080;&#30417;&#30563;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;&#20855;&#26377;&#39640;&#26031;&#12289;&#20271;&#21162;&#21033;&#21644;&#29627;&#23572;&#20857;&#26364;&#20808;&#39564;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#65289;&#22312;&#21830;&#19994;&#39134;&#34892;&#30340;&#39134;&#34892;&#25805;&#20316;&#25968;&#25454;&#19978;&#26816;&#27979;&#24322;&#24120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#20004;&#31181;&#20855;&#26377;&#31163;&#25955;&#28508;&#21464;&#37327;&#30340;VAE&#27169;&#22411;&#65292;&#19968;&#31181;&#20855;&#26377;&#20998;&#35299;&#20271;&#21162;&#21033;&#20808;&#39564;&#65292;&#19968;&#31181;&#20855;&#26377;&#38480;&#21046;&#29627;&#23572;&#20857;&#26364;&#26426;&#20316;&#20026;&#20808;&#39564;&#65292;&#22240;&#20026;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#38656;&#35201;&#31163;&#25955;&#21464;&#37327;&#27169;&#22411;&#65292;&#32780;&#22522;&#20110;&#21452;&#24577;&#37327;&#23376;&#31995;&#32479;&#30340;&#37327;&#23376;&#35774;&#22791;&#35201;&#27714;&#35813;&#27169;&#22411;&#12290;&#20351;&#29992;RBM&#20808;&#39564;&#30340;DVAE&#22312;&#33322;&#31354;&#20013;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;&#20013;&#26377;&#26174;&#33879;&#20248;&#24322;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20998;&#35299;&#20271;&#21162;&#21033;&#20808;&#39564;&#30340;DVAE&#12289;&#27491;&#24120;&#30340;&#39640;&#26031;VAE&#21644;&#22522;&#20110;Mahalanobis&#36317;&#31163;&#30340;&#32463;&#20856;&#26041;&#27861;&#30456;&#27604;&#65292;&#20351;&#29992;RBM&#20808;&#39564;&#30340;DVAE&#20855;&#26377;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative learning cannot only be used for generating new data with statistical characteristics derived from input data but also for anomaly detection, by separating nominal and anomalous instances based on their reconstruction quality. In this paper, we explore the performance of three unsupervised deep generative models -- variational autoencoders (VAEs) with Gaussian, Bernoulli, and Boltzmann priors -- in detecting anomalies in flight-operations data of commercial flights consisting of multivariate time series. We devised two VAE models with discrete latent variables (DVAEs), one with a factorized Bernoulli prior and one with a restricted Boltzmann machine (RBM) as prior, because of the demand for discrete-variable models in machine-learning applications and because the integration of quantum devices based on two-level quantum systems requires such models. The DVAE with RBM prior, using a relatively simple -- and classically or quantum-mechanically enhanceable -- sampling tech
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31639;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#26356;&#36890;&#29992;&#30340;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35774;&#35745;&#20855;&#26377;&#21487;&#35777;&#25910;&#25947;&#20445;&#35777;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.12298</link><description>&lt;p&gt;
&#19968;&#31181;&#35299;&#20915;&#31209;&#19968;&#30697;&#38453;&#24863;&#30693;&#30340;&#36890;&#29992;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A General Algorithm for Solving Rank-one Matrix Sensing. (arXiv:2303.12298v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#31639;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#26356;&#36890;&#29992;&#30340;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35774;&#35745;&#20855;&#26377;&#21487;&#35777;&#25910;&#25947;&#20445;&#35777;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30697;&#38453;&#24863;&#30693;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#20363;&#22914;&#31995;&#32479;&#25511;&#21046;&#65292;&#36317;&#31163;&#23884;&#20837;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#12290;&#30697;&#38453;&#24863;&#30693;&#30340;&#30446;&#26631;&#26159;&#22522;&#20110;&#19968;&#31995;&#21015;&#27979;&#37327;$(u_i,b_i) \in \mathbb{R}^{n} \times \mathbb{R}$&#65292;&#20174;&#20013;&#24674;&#22797;&#30697;&#38453;$A_\star \in \mathbb{R}^{n \times n}$&#65292;&#20351;&#24471; $u_i^\top A_\star u_i = b_i$&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;[ZJD15]&#32858;&#28966;&#20110;&#30697;&#38453;$A_\star$&#20855;&#26377;&#23567;&#31209;&#65292;&#20363;&#22914;&#31209;-$k$&#30340;&#24773;&#24418;&#12290;&#20182;&#20204;&#30340;&#20998;&#26512;&#20005;&#37325;&#20381;&#36182;&#20110;RIP&#20551;&#35774;&#65292;&#20351;&#20854;&#19981;&#28165;&#26970;&#22914;&#20309;&#25512;&#24191;&#21040;&#39640;&#31209;&#30697;&#38453;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25918;&#26494;&#20102;&#31209;-$k$&#30340;&#20551;&#35774;&#65292;&#24182;&#35299;&#20915;&#20102;&#26356;&#36890;&#29992;&#30340;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#12290;&#32473;&#23450;&#31934;&#24230;&#21442;&#25968;$\delta\in (0,1)$&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;$\widetilde{O}(m^{3/2}n^2\delta^{-1})$&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20869;&#27714;&#35299;&#30697;&#38453;$ A \in \mathbb{R}^{n \times n}$&#65292;&#28385;&#36275;&#23545;&#20110;&#25152;&#26377;$i \in[m]$&#65292;&#37117;&#26377;$|u_i^\top A u_i - b_i|\leq \delta$&#12290;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35774;&#35745;&#20102;&#19968;&#31181;&#20855;&#26377;&#21487;&#35777;&#25910;&#25947;&#20445;&#35777;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Matrix sensing has many real-world applications in science and engineering, such as system control, distance embedding, and computer vision. The goal of matrix sensing is to recover a matrix $A_\star \in \mathbb{R}^{n \times n}$, based on a sequence of measurements $(u_i,b_i) \in \mathbb{R}^{n} \times \mathbb{R}$ such that $u_i^\top A_\star u_i = b_i$. Previous work [ZJD15] focused on the scenario where matrix $A_{\star}$ has a small rank, e.g. rank-$k$. Their analysis heavily relies on the RIP assumption, making it unclear how to generalize to high-rank matrices. In this paper, we relax that rank-$k$ assumption and solve a much more general matrix sensing problem. Given an accuracy parameter $\delta \in (0,1)$, we can compute $A \in \mathbb{R}^{n \times n}$ in $\widetilde{O}(m^{3/2} n^2 \delta^{-1} )$, such that $ |u_i^\top A u_i - b_i| \leq \delta$ for all $i \in [m]$. We design an efficient algorithm with provable convergence guarantees using stochastic gradient descent for this pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21482;&#23545;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#23569;&#37327;&#26356;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#27169;&#22411;&#25512;&#26029;&#30340;&#24615;&#33021;&#21644;&#31934;&#24230;&#65292;&#24182;&#23454;&#29616;&#39640;&#25928;&#30340;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2303.12296</link><description>&lt;p&gt;
&#21407;&#22411;&#26377;&#21161;&#20110;&#32852;&#37030;&#23398;&#20064;&#65306;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Prototype Helps Federated Learning: Towards Faster Convergence. (arXiv:2303.12296v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21482;&#23545;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#23569;&#37327;&#26356;&#25913;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#27169;&#22411;&#25512;&#26029;&#30340;&#24615;&#33021;&#21644;&#31934;&#24230;&#65292;&#24182;&#23454;&#29616;&#39640;&#25928;&#30340;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#21512;&#20316;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#32780;&#19981;&#20132;&#25442;&#21407;&#22987;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#30340;&#24322;&#36136;&#24615;&#36890;&#24120;&#23548;&#33268;&#27169;&#22411;&#25512;&#26029;&#33021;&#21147;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#32780;&#21482;&#38656;&#23545;&#20856;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#30340;&#26368;&#21518;&#19968;&#20010;&#20840;&#23616;&#36845;&#20195;&#36827;&#34892;&#23569;&#37327;&#26356;&#25913;&#65292;&#21363;&#21487;&#23454;&#29616;&#26356;&#22909;&#30340;&#25512;&#26029;&#24615;&#33021;&#12290;&#22312;&#26368;&#21518;&#19968;&#27425;&#36845;&#20195;&#20013;&#65292;&#26381;&#21153;&#22120;&#32858;&#21512;&#20174;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#20256;&#36755;&#30340;&#21407;&#22411;&#65292;&#28982;&#21518;&#23558;&#20854;&#21457;&#36865;&#22238;&#26412;&#22320;&#23458;&#25143;&#31471;&#65292;&#20197;&#29992;&#20110;&#21508;&#33258;&#30340;&#27169;&#22411;&#25512;&#26029;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#22312;&#19981;&#21516;&#30340;&#24322;&#36136;&#24615;&#35774;&#32622;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#27604;&#20004;&#20010;&#27969;&#34892;&#22522;&#20934;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65288;&#33267;&#23569;1&#65285;&#65289;&#21644;&#30456;&#23545;&#39640;&#25928;&#30340;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a distributed machine learning technique in which multiple clients cooperate to train a shared model without exchanging their raw data. However, heterogeneity of data distribution among clients usually leads to poor model inference. In this paper, a prototype-based federated learning framework is proposed, which can achieve better inference performance with only a few changes to the last global iteration of the typical federated learning process. In the last iteration, the server aggregates the prototypes transmitted from distributed clients and then sends them back to local clients for their respective model inferences. Experiments on two baseline datasets show that our proposal can achieve higher accuracy (at least 1%) and relatively efficient communication than two popular baselines under different heterogeneous settings.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#24615;&#27491;&#21017;&#21270;&#22120;&#65288;FR&#65289;&#65292;&#23427;&#36890;&#36807;&#35268;&#33539;&#20219;&#24847;&#20004;&#20010;&#23376;&#32676;&#20307;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#26469;&#25552;&#39640;&#38271;&#23614;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#21516;&#26102;&#36991;&#20813;&#20260;&#23475;&#20219;&#20309;&#19968;&#20010;&#23376;&#32676;&#20307;&#12290;</title><link>http://arxiv.org/abs/2303.12291</link><description>&lt;p&gt;
&#20844;&#24179;&#24615;&#25552;&#39640;&#20102;&#20174;&#22024;&#26434;&#26631;&#35760;&#30340;&#38271;&#23614;&#25968;&#25454;&#20013;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fairness Improves Learning from Noisily Labeled Long-Tailed Data. (arXiv:2303.12291v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12291
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#24615;&#27491;&#21017;&#21270;&#22120;&#65288;FR&#65289;&#65292;&#23427;&#36890;&#36807;&#35268;&#33539;&#20219;&#24847;&#20004;&#20010;&#23376;&#32676;&#20307;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#26469;&#25552;&#39640;&#38271;&#23614;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#24182;&#21516;&#26102;&#36991;&#20813;&#20260;&#23475;&#20219;&#20309;&#19968;&#20010;&#23376;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#21644;&#22024;&#26434;&#26631;&#35760;&#30340;&#25968;&#25454;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#32463;&#24120;&#20986;&#29616;&#65292;&#23545;&#23398;&#20064;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#23558;&#20854;&#20013;&#19968;&#31181;&#38382;&#39064;&#21333;&#29420;&#22788;&#29702;&#65292;&#24182;&#26410;&#26126;&#30830;&#32771;&#34385;&#20004;&#32773;&#30340;&#32806;&#21512;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35266;&#23519;&#34920;&#26126;&#65292;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#22312;&#25968;&#25454;&#38598;&#26159;&#38271;&#23614;&#30340;&#26631;&#31614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#22987;&#32456;&#25913;&#21892;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#22312;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#29616;&#26377;&#26041;&#27861;&#24182;&#26410;&#35266;&#23519;&#21040;&#21508;&#20010;&#23376;&#32676;&#20307;&#26222;&#36941;&#30340;&#25552;&#21319;;&#25442;&#21477;&#35805;&#35828;&#65292;&#26576;&#20123;&#23376;&#32676;&#20307;&#33719;&#24471;&#20102;&#25552;&#39640;&#31934;&#24230;&#30340;&#22909;&#22788;&#65292;&#20294;&#20195;&#20215;&#26159;&#20260;&#23475;&#20854;&#20182;&#20154;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20844;&#24179;&#24615;&#27491;&#21017;&#21270;&#22120;&#65288;FR&#65289;&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;&#22312;&#20219;&#20309;&#20004;&#20010;&#23376;&#32676;&#20307;&#20043;&#38388;&#35268;&#33539;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24341;&#20837;&#30340;&#20844;&#24179;&#24615;&#27491;&#21017;&#21270;&#22120;&#25552;&#39640;&#20102;&#23614;&#37096;&#23376;&#32676;&#20307;&#21644;&#24635;&#20307;&#23398;&#20064;&#24615;&#33021;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Both long-tailed and noisily labeled data frequently appear in real-world applications and impose significant challenges for learning. Most prior works treat either problem in an isolated way and do not explicitly consider the coupling effects of the two. Our empirical observation reveals that such solutions fail to consistently improve the learning when the dataset is long-tailed with label noise. Moreover, with the presence of label noise, existing methods do not observe universal improvements across different sub-populations; in other words, some sub-populations enjoyed the benefits of improved accuracy at the cost of hurting others. Based on these observations, we introduce the Fairness Regularizer (FR), inspired by regularizing the performance gap between any two sub-populations. We show that the introduced fairness regularizer improves the performances of sub-populations on the tail and the overall learning performance. Extensive experiments demonstrate the effectiveness of the p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#25506;&#35752;&#22914;&#20309;&#21160;&#24577;&#29983;&#25104;&#34892;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710; ROW &#35745;&#21010;&#65292;&#20248;&#21270;&#20132;&#36890;&#27969;&#30340;&#25928;&#29575;&#65292;&#20026;&#34892;&#20154;&#20998;&#37197;&#26356;&#22810;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.12289</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25913;&#21892;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#19982;&#34892;&#20154;&#20132;&#20114;&#30340;&#33258;&#36866;&#24212;&#36947;&#36335;&#37197;&#32622;
&lt;/p&gt;
&lt;p&gt;
Adaptive Road Configurations for Improved Autonomous Vehicle-Pedestrian Interactions using Reinforcement Learning. (arXiv:2303.12289v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12289
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#25506;&#35752;&#22914;&#20309;&#21160;&#24577;&#29983;&#25104;&#34892;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710; ROW &#35745;&#21010;&#65292;&#20248;&#21270;&#20132;&#36890;&#27969;&#30340;&#25928;&#29575;&#65292;&#20026;&#34892;&#20154;&#20998;&#37197;&#26356;&#22810;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#37096;&#32626;&#20026;&#26410;&#26469;&#22478;&#24066;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#30340;&#35774;&#35745;&#21644;&#31649;&#29702;&#25552;&#20379;&#20102;&#37325;&#22823;&#25361;&#25112;&#21644;&#29420;&#29305;&#30340;&#26426;&#36935;&#12290;&#20026;&#20102;&#37325;&#26032;&#23450;&#20041;&#36947;&#36335;&#31354;&#38388;&#30340; ROW &#26500;&#25104;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#35774;&#35745;&#26041;&#27861;&#21644;&#26234;&#33021;&#25511;&#21046;&#27169;&#22411;&#65292;&#20294;&#32570;&#20047;&#19968;&#20010;&#21487;&#20197;&#26681;&#25454;&#23454;&#26102;&#38656;&#27714;&#21160;&#24577;&#29983;&#25104;&#34892;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710; ROW &#35745;&#21010;&#30340;&#25805;&#20316;&#26694;&#26550;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#24494;&#35266;&#20132;&#36890;&#20223;&#30495;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#25913;&#36827; ROW &#26500;&#25104;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20998;&#21035;&#23454;&#29616;&#20102;&#38598;&#20013;&#24335;&#33539;&#24335;&#21644;&#20998;&#24067;&#24335;&#23398;&#20064;&#33539;&#24335;&#65292;&#20197;&#20998;&#21035;&#23545;&#22810;&#20010;&#36335;&#32593;&#37197;&#32622;&#36827;&#34892;&#21160;&#24577;&#25511;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#31639;&#27861;&#26377;&#28508;&#21147;&#25552;&#39640;&#20132;&#36890;&#27969;&#30340;&#25928;&#29575;&#24182;&#20998;&#37197;&#26356;&#22810;&#30340;&#31354;&#38388;&#32473;&#34892;&#20154;&#12290;&#27492;&#22806;&#65292;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#20248;&#20110;&#38598;&#20013;&#24335;&#31639;&#27861;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment of Autonomous Vehicles (AVs) poses considerable challenges and unique opportunities for the design and management of future urban road infrastructure. In light of this disruptive transformation, the Right-Of-Way (ROW) composition of road space has the potential to be renewed. Design approaches and intelligent control models have been proposed to address this problem, but we lack an operational framework that can dynamically generate ROW plans for AVs and pedestrians in response to real-time demand. Based on microscopic traffic simulation, this study explores Reinforcement Learning (RL) methods for evolving ROW compositions. We implement a centralised paradigm and a distributive learning paradigm to separately perform the dynamic control on several road network configurations. Experimental results indicate that the algorithms have the potential to improve traffic flow efficiency and allocate more space for pedestrians. Furthermore, the distributive learning algorithm outp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#26631;&#20934;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#26694;&#26550;&#19979;&#19981;&#23384;&#22312;&#21487;&#33719;&#24471;&#32435;&#20160;&#22343;&#34913;&#19988;&#21487;&#29420;&#31435;&#23398;&#20064;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.12287</link><description>&lt;p&gt;
&#29420;&#31435;&#23398;&#20064;&#21644;&#31232;&#30095;&#22343;&#34913;&#35745;&#31639;&#22312;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#30340;&#38590;&#24230;
&lt;/p&gt;
&lt;p&gt;
Hardness of Independent Learning and Sparse Equilibrium Computation in Markov Games. (arXiv:2303.12287v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#26631;&#20934;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#26694;&#26550;&#19979;&#19981;&#23384;&#22312;&#21487;&#33719;&#24471;&#32435;&#20160;&#22343;&#34913;&#19988;&#21487;&#29420;&#31435;&#23398;&#20064;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#65292;&#26159;&#21542;&#23384;&#22312;&#31639;&#27861;&#65292;&#24403;&#25152;&#26377;&#20195;&#29702;&#37319;&#29992;&#24182;&#22312;&#20998;&#25955;&#26041;&#24335;&#19979;&#29420;&#31435;&#36816;&#34892;&#26102;&#65292;&#27599;&#20010;&#29609;&#23478;&#37117;&#21487;&#20197;&#19981;&#21518;&#24724;&#22320;&#36827;&#23637;&#65292;&#31867;&#20284;&#20110;&#27491;&#24120;&#24418;&#24335;&#28216;&#25103;&#20013;&#30340;&#33879;&#21517;&#25910;&#25947;&#32467;&#26524;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65288;&#29305;&#21035;&#26159;&#24403;&#21518;&#24724;&#19982;&#39532;&#23572;&#21487;&#22827;&#31574;&#30053;&#30340;&#20559;&#31163;&#26377;&#20851;&#26102;&#65289;&#65292;&#36825;&#31181;&#31639;&#27861;&#23384;&#22312;&#65292;&#20294;&#26159;&#29420;&#31435;&#30340;&#19981;&#21518;&#24724;&#23398;&#20064;&#26159;&#21542;&#33021;&#22312;&#26631;&#20934;&#30340;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#26694;&#26550;&#19979;&#23454;&#29616;&#26159;&#20540;&#24471;&#25506;&#35752;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#35745;&#31639;&#21644;&#32479;&#35745;&#35282;&#24230;&#30456;&#24212;&#22320;&#25552;&#20986;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#21542;&#23450;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of decentralized multi-agent reinforcement learning in Markov games. A fundamental question is whether there exist algorithms that, when adopted by all agents and run independently in a decentralized fashion, lead to no-regret for each player, analogous to celebrated convergence results in normal-form games. While recent work has shown that such algorithms exist for restricted settings (notably, when regret is defined with respect to deviations to Markovian policies), the question of whether independent no-regret learning can be achieved in the standard Markov game framework was open. We provide a decisive negative resolution this problem, both from a computational and statistical perspective. We show that:  - Under the widely-believed assumption that PPAD-hard problems cannot be solved in polynomial time, there is no polynomial-time algorithm that attains no-regret in general-sum Markov games when executed independently by all players, even when the game is kno
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23558;&#24037;&#19994;&#29983;&#20135;&#23545;&#21608;&#36793;&#22478;&#24066;&#30340;&#31354;&#27668;&#27745;&#26579;&#24433;&#21709;&#38477;&#33267;&#26368;&#20302;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#20135;&#27963;&#21160;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#39044;&#27979;&#21644;&#25351;&#23548;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#23558;&#29615;&#22659;&#24433;&#21709;&#20943;&#23569;&#21644;&#29983;&#20135;&#32500;&#25345;&#20043;&#38388;&#36798;&#25104;&#20102;&#22810;&#31181;&#26435;&#34913;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.12285</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31354;&#27668;&#27745;&#26579;&#38477;&#20302;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reducing Air Pollution through Machine Learning. (arXiv:2303.12285v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23558;&#24037;&#19994;&#29983;&#20135;&#23545;&#21608;&#36793;&#22478;&#24066;&#30340;&#31354;&#27668;&#27745;&#26579;&#24433;&#21709;&#38477;&#33267;&#26368;&#20302;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#20135;&#27963;&#21160;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#39044;&#27979;&#21644;&#25351;&#23548;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#23558;&#29615;&#22659;&#24433;&#21709;&#20943;&#23569;&#21644;&#29983;&#20135;&#32500;&#25345;&#20043;&#38388;&#36798;&#25104;&#20102;&#22810;&#31181;&#26435;&#34913;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#24037;&#19994;&#29983;&#20135;&#23545;&#21608;&#36793;&#22478;&#24066;&#30340;&#31354;&#27668;&#27745;&#26579;&#25928;&#24212;&#65292;&#35813;&#26041;&#27861;&#23558;&#36816;&#33829;&#20915;&#31574;&#19982;&#22825;&#27668;&#26465;&#20214;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#39044;&#27979;&#21644;&#25351;&#23548;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#30701;&#26399;&#39118;&#36895;&#21644;&#26041;&#21521;&#65292;&#24182;&#25512;&#33616;&#36816;&#33829;&#20915;&#31574;&#20197;&#20943;&#23569;&#25110;&#26242;&#20572;&#24037;&#19994;&#29983;&#20135;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20943;&#23569;&#29615;&#22659;&#24433;&#21709;&#21644;&#32500;&#25345;&#29983;&#20135;&#27963;&#21160;&#20043;&#38388;&#30340;&#20960;&#31181;&#26435;&#34913;&#12290;&#25105;&#20204;&#26694;&#26550;&#30340;&#39044;&#27979;&#32452;&#20214;&#37319;&#29992;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#26799;&#24230;&#25552;&#21319;&#26641;&#27169;&#22411;&#21644;&#38598;&#25104;&#26041;&#27861;&#65292;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#25351;&#23548;&#24615;&#32452;&#20214;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#26368;&#20248;&#31574;&#30053;&#26641;&#25552;&#20986;&#22810;&#20010;&#26435;&#34913;&#26041;&#26696;&#65292;&#20363;&#22914;&#23558;&#21361;&#38505;&#25490;&#25918;&#29289;&#20943;&#23569;33-47%&#21644;&#23558;&#19981;&#24517;&#35201;&#30340;&#25104;&#26412;&#38477;&#20302;40-63%&#12290;&#25105;&#20204;&#37096;&#32626;&#30340;&#27169;&#22411;&#26174;&#33879;&#38477;&#20302;&#20102;&#39044;&#27979;&#35823;&#24046;&#65292;&#23545;&#20110;&#23567;&#20110;12&#23567;&#26102;&#30340;&#39044;&#27979;&#65292;&#38477;&#20302;&#20102;38-52%&#30340;&#35823;&#24046;&#33539;&#22260;&#65292;&#23545;&#20110;12&#21040;48&#23567;&#26102;&#30340;&#39044;&#27979;&#65292;&#38477;&#20302;&#20102;14-46%&#30340;&#35823;&#24046;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a data-driven approach to mitigate the effects of air pollution from industrial plants on nearby cities by linking operational decisions with weather conditions. Our method combines predictive and prescriptive machine learning models to forecast short-term wind speed and direction and recommend operational decisions to reduce or pause the industrial plant's production. We exhibit several trade-offs between reducing environmental impact and maintaining production activities. The predictive component of our framework employs various machine learning models, such as gradient-boosted tree-based models and ensemble methods, for time series forecasting. The prescriptive component utilizes interpretable optimal policy trees to propose multiple trade-offs, such as reducing dangerous emissions by 33-47% and unnecessary costs by 40-63%. Our deployed models significantly reduced forecasting errors, with a range of 38-52% for less than 12-hour lead time and 14-46% for 12 to 48-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#20855;&#26377;&#37325;&#23614;&#22122;&#22768;&#30340;&#38543;&#26426;&#38750;&#20809;&#28369;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22635;&#34917;&#20102;&#22312;&#20989;&#25968;&#38750;&#20809;&#28369;&#22330;&#26223;&#19979;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2303.12277</link><description>&lt;p&gt;
&#24102;&#26377;&#37325;&#23614;&#22122;&#22768;&#30340;&#38543;&#26426;&#38750;&#20809;&#28369;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stochastic Nonsmooth Convex Optimization with Heavy-Tailed Noises. (arXiv:2303.12277v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20855;&#26377;&#37325;&#23614;&#22122;&#22768;&#30340;&#38543;&#26426;&#38750;&#20809;&#28369;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22635;&#34917;&#20102;&#22312;&#20989;&#25968;&#38750;&#20809;&#28369;&#22330;&#26223;&#19979;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#23558;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#32771;&#34385;&#22312;&#37325;&#23614;&#22122;&#22768;&#33539;&#24335;&#19979;&#65292;&#21363;&#20551;&#35774;&#38543;&#26426;&#26799;&#24230;&#21644;&#30495;&#23454;&#26799;&#24230;&#20043;&#38388;&#30340;&#24046;&#24322;&#20855;&#26377;&#26377;&#38480;&#30340; $p$ &#38454;&#30697;&#65288;&#20363;&#22914;&#34987;&#26576;&#20010; $\sigma \geq0$ &#19978;&#30028;&#38480;&#21046;&#20026; $\sigma^{p}$&#65289;&#65292;&#20854;&#20013; $p\in (1,2]$&#65292;&#36825;&#19981;&#20165;&#27867;&#21270;&#20102;&#20256;&#32479;&#30340;&#26377;&#38480;&#26041;&#24046;&#20551;&#35774;&#65288;$p=2$&#65289;&#65292;&#32780;&#19988;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#37117;&#34987;&#35266;&#23519;&#21040;&#12290;&#22312;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20551;&#35774;&#19979;&#65292;&#38024;&#23545;&#20984;&#25110;&#38750;&#20984;&#38382;&#39064;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22810;&#26032;&#36827;&#23637;&#65292;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#21482;&#32771;&#34385;&#20809;&#28369;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#30456;&#21453;&#65292;&#22312;&#20989;&#25968;&#38750;&#20809;&#28369;&#26102;&#65292;&#20154;&#20204;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#24182;&#23436;&#20840;&#29702;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23545;&#24102;&#26377;&#37325;&#23614;&#22122;&#22768;&#30340;&#38543;&#26426;&#38750;&#20809;&#28369;&#20984;&#20248;&#21270;&#25552;&#20379;&#20840;&#38754;&#20998;&#26512;&#26469;&#22635;&#34917;&#36825;&#19968;&#20851;&#38190;&#31354;&#30333;&#12290;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20110;&#35009;&#21098;&#30340;&#31639;&#27861;&#65292;&#28982;&#32780;&#65292;&#36825;&#20010;&#31639;&#27861;&#21482;&#34987;&#35777;&#26126;&#33021;&#20197;&#26399;&#26395;&#26041;&#24335;&#25910;&#25947;&#65292;&#20294;&#22312;&#38468;&#21152;
&lt;/p&gt;
&lt;p&gt;
Recently, several studies consider the stochastic optimization problem but in a heavy-tailed noise regime, i.e., the difference between the stochastic gradient and the true gradient is assumed to have a finite $p$-th moment (say being upper bounded by $\sigma^{p}$ for some $\sigma\geq0$) where $p\in(1,2]$, which not only generalizes the traditional finite variance assumption ($p=2$) but also has been observed in practice for several different tasks. Under this challenging assumption, lots of new progress has been made for either convex or nonconvex problems, however, most of which only consider smooth objectives. In contrast, people have not fully explored and well understood this problem when functions are nonsmooth. This paper aims to fill this crucial gap by providing a comprehensive analysis of stochastic nonsmooth convex optimization with heavy-tailed noises. We revisit a simple clipping-based algorithm, whereas, which is only proved to converge in expectation but under the additi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;AUTO&#30340;&#26041;&#27861;&#65292;&#22312;&#22312;&#32447;&#27979;&#35797;&#26102;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#22312;&#32447;&#25968;&#25454;&#30452;&#25509;&#25552;&#39640;OOD&#26816;&#27979;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#24182;&#22312;&#32447;&#26816;&#27979;OOD&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.12267</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#31163;&#32676;&#20540;&#20248;&#21270;&#65306;&#29992;&#20110;&#22312;&#32447;&#27979;&#35797;&#26102;OOD&#26816;&#27979;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AUTO: Adaptive Outlier Optimization for Online Test-Time OOD Detection. (arXiv:2303.12267v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;AUTO&#30340;&#26041;&#27861;&#65292;&#22312;&#22312;&#32447;&#27979;&#35797;&#26102;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#22312;&#32447;&#25968;&#25454;&#30452;&#25509;&#25552;&#39640;OOD&#26816;&#27979;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#24182;&#22312;&#32447;&#26816;&#27979;OOD&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;OOD&#65288;out-of-distribution&#65289;&#26816;&#27979;&#26159;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#36741;&#21161;&#31163;&#32676;&#20540;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;OOD&#26816;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31163;&#32676;&#20540;&#36890;&#24120;&#19982;&#27979;&#35797;OOD&#25968;&#25454;&#23384;&#22312;&#20998;&#24067;&#24046;&#36317;&#65292;&#24182;&#19988;&#19981;&#33021;&#35206;&#30422;&#25152;&#26377;&#21487;&#33021;&#30340;&#27979;&#35797;OOD&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#32467;&#21512;&#36825;&#20123;&#31163;&#32676;&#20540;&#36824;&#20250;&#22686;&#21152;&#35757;&#32451;&#30340;&#36127;&#25285;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#27979;&#35797;&#26102;OOD&#26816;&#27979;&#30340;&#26032;&#33539;&#24335;&#65292;&#35813;&#33539;&#24335;&#30452;&#25509;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#22312;&#32447;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;OOD&#26816;&#27979;&#24615;&#33021;&#12290;&#34429;&#28982;&#36825;&#31181;&#33539;&#24335;&#24456;&#39640;&#25928;&#65292;&#20294;&#23427;&#20063;&#38754;&#20020;&#30528;&#35832;&#22914;&#28798;&#38590;&#24615;&#36951;&#24536;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#31163;&#32676;&#20540;&#20248;&#21270;&#65288;AUTO&#65289;&#65292;&#23427;&#30001;&#20869;&#22806;&#24863;&#30693;&#28388;&#27874;&#22120;&#12289;ID&#23384;&#20648;&#22120;&#21644;&#35821;&#20041;&#19968;&#33268;&#30340;&#30446;&#26631;&#32452;&#25104;&#12290;AUTO&#33258;&#36866;&#24212;&#22320;&#20174;&#27979;&#35797;&#25968;&#25454;&#20013;&#25366;&#25496;&#20266;ID&#21644;&#20266;OOD&#26679;&#26412;&#65292;&#21033;&#29992;&#23427;&#20204;&#26469;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#24182;&#22312;&#32447;&#26816;&#27979;OOD&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AUTO&#22312;&#21508;&#31181;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#19978;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is a crucial aspect of deploying machine learning models in open-world applications. Empirical evidence suggests that training with auxiliary outliers substantially improves OOD detection. However, such outliers typically exhibit a distribution gap compared to the test OOD data and do not cover all possible test OOD scenarios. Additionally, incorporating these outliers introduces additional training burdens. In this paper, we introduce a novel paradigm called test-time OOD detection, which utilizes unlabeled online data directly at test time to improve OOD detection performance. While this paradigm is efficient, it also presents challenges such as catastrophic forgetting. To address these challenges, we propose adaptive outlier optimization (AUTO), which consists of an in-out-aware filter, an ID memory bank, and a semantically-consistent objective. AUTO adaptively mines pseudo-ID and pseudo-OOD samples from test data, utilizing them to optimize netwo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#22797;&#26434;&#22810;&#23610;&#24230;&#24314;&#27169;&#21644;&#27169;&#25311;&#20013;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#25351;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#20316;&#20026;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#30340;&#20195;&#29702;&#12289;&#21152;&#36895;&#25110;&#22686;&#24378;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#26356;&#22909;&#30340;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#26469;&#20419;&#36827;&#22797;&#26434;&#30340;&#22810;&#23610;&#24230;&#31995;&#32479;&#30340;&#35299;&#20915;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12261</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#22810;&#23610;&#24230;&#35745;&#31639;&#27169;&#25311;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Challenges and opportunities for machine learning in multiscale computational modeling. (arXiv:2303.12261v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#22797;&#26434;&#22810;&#23610;&#24230;&#24314;&#27169;&#21644;&#27169;&#25311;&#20013;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#25351;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#20316;&#20026;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#30340;&#20195;&#29702;&#12289;&#21152;&#36895;&#25110;&#22686;&#24378;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#26356;&#22909;&#30340;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#26469;&#20419;&#36827;&#22797;&#26434;&#30340;&#22810;&#23610;&#24230;&#31995;&#32479;&#30340;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#26800;&#24037;&#31243;&#24212;&#29992;&#38656;&#35201;&#22810;&#23610;&#24230;&#35745;&#31639;&#24314;&#27169;&#21644;&#27169;&#25311;&#12290;&#28982;&#32780;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;&#22810;&#23610;&#24230;&#31995;&#32479;&#20173;&#28982;&#20855;&#26377;&#39640;&#32500;&#24230;&#30340;&#35299;&#31354;&#38388;&#65292;&#35745;&#31639;&#37327;&#24040;&#22823;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#20316;&#20026;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#30340;&#20195;&#29702;&#12289;&#21152;&#36895;&#25110;&#22686;&#24378;&#12290;&#20808;&#39537;&#24615;&#30340;&#24037;&#20316;&#24050;&#32463;&#35777;&#26126;&#65292;ML&#21487;&#20197;&#25552;&#20379;&#19982;&#30452;&#25509;&#25968;&#20540;&#26041;&#27861;&#30456;&#24403;&#31934;&#24230;&#30340;&#25511;&#21046;&#26041;&#31243;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#35745;&#31639;&#36895;&#24230;&#26174;&#33879;&#26356;&#24555;&#12290;&#36825;&#20123;&#39640;&#36895;&#12289;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#21487;&#20197;&#36890;&#36807;&#20026;&#20256;&#32479;&#35299;&#31639;&#22120;&#25552;&#20379;&#26356;&#22909;&#30340;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#65292;&#20419;&#36827;&#22797;&#26434;&#30340;&#22810;&#23610;&#24230;&#31995;&#32479;&#30340;&#35299;&#20915;&#12290;&#26412;&#25991;&#23601;&#20351;&#29992;ML&#36827;&#34892;&#22797;&#26434;&#22810;&#23610;&#24230;&#24314;&#27169;&#21644;&#27169;&#25311;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#25552;&#20379;&#20102;&#19968;&#20010;&#35270;&#35282;&#12290;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#29992;&#20110;&#27169;&#25311;&#22810;&#23610;&#24230;&#31995;&#32479;&#30340;&#29616;&#26377;ML&#26041;&#27861;&#65292;&#28982;&#21518;&#35752;&#35770;&#20102;&#23427;&#20204;&#25552;&#39640;&#22810;&#23610;&#24230;&#27169;&#25311;&#36895;&#24230;&#21644;&#31934;&#24230;&#30340;&#28508;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20026;&#22810;&#23610;&#24230;&#20223;&#30495;&#24320;&#21457;ML&#31639;&#27861;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26410;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#21487;&#33021;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many mechanical engineering applications call for multiscale computational modeling and simulation. However, solving for complex multiscale systems remains computationally onerous due to the high dimensionality of the solution space. Recently, machine learning (ML) has emerged as a promising solution that can either serve as a surrogate for, accelerate or augment traditional numerical methods. Pioneering work has demonstrated that ML provides solutions to governing systems of equations with comparable accuracy to those obtained using direct numerical methods, but with significantly faster computational speed. These high-speed, high-fidelity estimations can facilitate the solving of complex multiscale systems by providing a better initial solution to traditional solvers. This paper provides a perspective on the opportunities and challenges of using ML for complex multiscale modeling and simulation. We first outline the current state-of-the-art ML approaches for simulating multiscale sys
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27969;&#22330;&#20272;&#35745;&#30340;&#20256;&#24863;&#22120;&#36873;&#25321;&#26694;&#26550;&#65292;&#33021;&#22815;&#20351;&#29992;&#23569;&#37327;&#20256;&#24863;&#22120;&#39640;&#25928;&#22320;&#20272;&#35745;&#39640;&#25915;&#35282;&#19979;&#26426;&#32764;&#21518;&#27969;&#30340;&#27969;&#22330;&#12290;</title><link>http://arxiv.org/abs/2303.12260</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#30340;&#20256;&#24863;&#22120;&#25918;&#32622;&#29992;&#20110;&#26080;&#23450;&#24120;&#27969;&#37327;&#30340;&#25968;&#25454;&#39537;&#21160;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Information-Based Sensor Placement for Data-Driven Estimation of Unsteady Flows. (arXiv:2303.12260v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27969;&#22330;&#20272;&#35745;&#30340;&#20256;&#24863;&#22120;&#36873;&#25321;&#26694;&#26550;&#65292;&#33021;&#22815;&#20351;&#29992;&#23569;&#37327;&#20256;&#24863;&#22120;&#39640;&#25928;&#22320;&#20272;&#35745;&#39640;&#25915;&#35282;&#19979;&#26426;&#32764;&#21518;&#27969;&#30340;&#27969;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#39134;&#34892;&#22120;&#21608;&#22260;&#30340;&#26080;&#23450;&#24120;&#27969;&#22330;&#30340;&#20272;&#35745;&#21487;&#33021;&#20250;&#25913;&#21892;&#27969;&#22330;&#20132;&#20114;&#24182;&#23548;&#33268;&#39134;&#34892;&#22120;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;&#23613;&#31649;&#27969;&#22330;&#34920;&#31034;&#21487;&#20197;&#26159;&#38750;&#24120;&#39640;&#32500;&#30340;&#65292;&#20294;&#23427;&#20204;&#30340;&#21160;&#24577;&#21487;&#20197;&#20855;&#26377;&#20302;&#38454;&#34920;&#31034;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#19968;&#20123;&#36866;&#24403;&#25918;&#32622;&#30340;&#27979;&#37327;&#26469;&#20272;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20256;&#24863;&#22120;&#36873;&#25321;&#26694;&#26550;&#65292;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27969;&#22330;&#20272;&#35745;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#12289;&#31283;&#24577;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#35774;&#35745;&#21644;&#19968;&#31181;&#29992;&#20110;&#39034;&#24207;&#36873;&#25321;&#20256;&#24863;&#22120;&#30340;&#31232;&#30095;&#21270;&#25216;&#26415;&#12290;&#26412;&#25991;&#36824;&#20351;&#29992;&#20256;&#24863;&#22120;&#36873;&#25321;&#26694;&#26550;&#35774;&#35745;&#20102;&#20256;&#24863;&#22120;&#38453;&#21015;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#25805;&#20316;&#26465;&#20214;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;&#25968;&#20540;&#25968;&#25454;&#19978;&#30340;&#27969;&#37327;&#20272;&#35745;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20351;&#29992;&#23884;&#20837;&#24335;&#21387;&#21147;&#20256;&#24863;&#22120;&#33021;&#22815;&#39640;&#25928;&#22320;&#20272;&#35745;&#39640;&#25915;&#35282;&#19979;&#26426;&#32764;&#21518;&#27969;&#30340;&#27969;&#22330;&#12290;&#27969;&#22330;&#20998;&#26512;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
Estimation of unsteady flow fields around flight vehicles may improve flow interactions and lead to enhanced vehicle performance. Although flow-field representations can be very high-dimensional, their dynamics can have low-order representations and may be estimated using a few, appropriately placed measurements. This paper presents a sensor-selection framework for the intended application of data-driven, flow-field estimation. This framework combines data-driven modeling, steady-state Kalman Filter design, and a sparsification technique for sequential selection of sensors. This paper also uses the sensor selection framework to design sensor arrays that can perform well across a variety of operating conditions. Flow estimation results on numerical data show that the proposed framework produces arrays that are highly effective at flow-field estimation for the flow behind and an airfoil at a high angle of attack using embedded pressure sensors. Analysis of the flow fields reveals that pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20108;&#20803;&#27491;&#21017;&#21270;&#26041;&#27861;&#20197;&#20415;&#20110;&#23398;&#20064;&#20108;&#20803;&#27010;&#24565;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#24182;&#25913;&#36827;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12255</link><description>&lt;p&gt;
&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#31354;&#38388;&#20013;&#32534;&#30721;&#20108;&#20803;&#27010;&#24565;&#20197;&#22686;&#24378;&#25968;&#25454;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Encoding Binary Concepts in the Latent Space of Generative Models for Enhancing Data Representation. (arXiv:2303.12255v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20108;&#20803;&#27491;&#21017;&#21270;&#26041;&#27861;&#20197;&#20415;&#20110;&#23398;&#20064;&#20108;&#20803;&#27010;&#24565;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#24182;&#25913;&#36827;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#32463;&#39564;&#20027;&#20041;&#22320;&#20351;&#29992;&#20108;&#20803;&#27010;&#24565;&#26469;&#39640;&#25928;&#22320;&#25512;&#24191;&#12290;&#36825;&#20123;&#27010;&#24565;&#22522;&#20110;&#20271;&#21162;&#21033;&#20998;&#24067;&#65292;&#26159;&#20449;&#24687;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20123;&#27010;&#24565;&#28085;&#30422;&#20102;&#20302;&#32423;&#21644;&#39640;&#32423;&#29305;&#24449;&#65292;&#22914;&#8220;&#22823; vs &#23567;&#8221;&#21644;&#8220;&#31070;&#32463;&#20803;&#22788;&#20110;&#27963;&#36291;&#25110;&#38750;&#27963;&#36291;&#29366;&#24577;&#8221;&#12290;&#20108;&#20803;&#27010;&#24565;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#29305;&#24449;&#65292;&#21487;&#20197;&#29992;&#20110;&#20256;&#36882;&#30693;&#35782;&#65292;&#20197;&#25913;&#36827;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20108;&#20803;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20197;&#20415;&#20110;&#23398;&#20064;&#20108;&#20803;&#27010;&#24565;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#20108;&#20803;&#21270;&#36229;&#21442;&#25968;$r$&#65292;&#20197;&#23545;&#31216;&#22320;&#35299;&#24320;&#28508;&#22312;&#31354;&#38388;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#29616;&#26377;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#21464;&#20307;&#65292;&#20197;&#40723;&#21169;&#23545;&#31216;&#35299;&#32544;&#12289;&#25552;&#39640;&#37325;&#24314;&#36136;&#37327;&#24182;&#38450;&#27490;&#21518;&#39564;&#23849;&#28291;&#32780;&#26080;&#38656;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#29616;&#26377;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#23398;&#20064;&#21487;&#20256;&#36882;&#24615;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Binary concepts are empirically used by humans to generalize efficiently. And they are based on Bernoulli distribution which is the building block of information. These concepts span both low-level and high-level features such as "large vs small" and "a neuron is active or inactive". Binary concepts are ubiquitous features and can be used to transfer knowledge to improve model generalization. We propose a novel binarized regularization to facilitate learning of binary concepts to improve the quality of data generation in autoencoders. We introduce a binarizing hyperparameter $r$ in data generation process to disentangle the latent space symmetrically. We demonstrate that this method can be applied easily to existing variational autoencoder (VAE) variants to encourage symmetric disentanglement, improve reconstruction quality, and prevent posterior collapse without computation overhead. We also demonstrate that this method can boost existing models to learn more transferable representati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20013;&#21033;&#29992;&#35270;&#35273;&#25552;&#31034;&#65288;VP&#65289;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#22909;&#22788;&#12290;VP&#19982;PATE&#30456;&#37197;&#21512;&#65292;&#22312;&#38544;&#31169;&#39044;&#31639;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#38544;&#31169;-&#25928;&#29992;&#24179;&#34913;&#65292;&#22312;&#36328;&#22495;&#22270;&#20687;&#20998;&#31867;&#20013;&#20063;&#26174;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;VP&#22312;DP&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.12247</link><description>&lt;p&gt;
&#25506;&#32034;&#24046;&#20998;&#38544;&#31169;&#20013;&#35270;&#35273;&#25552;&#31034;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
Exploring the Benefits of Visual Prompting in Differential Privacy. (arXiv:2303.12247v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20013;&#21033;&#29992;&#35270;&#35273;&#25552;&#31034;&#65288;VP&#65289;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#22909;&#22788;&#12290;VP&#19982;PATE&#30456;&#37197;&#21512;&#65292;&#22312;&#38544;&#31169;&#39044;&#31639;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#38544;&#31169;-&#25928;&#29992;&#24179;&#34913;&#65292;&#22312;&#36328;&#22495;&#22270;&#20687;&#20998;&#31867;&#20013;&#20063;&#26174;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;VP&#22312;DP&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25552;&#31034;&#65288;VP&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#19988;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;&#20923;&#32467;&#28304;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#26679;&#26412;&#39640;&#25928;&#36866;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#20013;&#21033;&#29992;VP&#26500;&#24314;&#24341;&#20154;&#27880;&#30446;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#25506;&#32034;&#24182;&#23558;VP&#25972;&#21512;&#21040;&#32463;&#20856;&#30340;DP&#35757;&#32451;&#26041;&#27861;&#20013;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#31616;&#21333;&#24615;&#21644;&#25928;&#29575;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;VP&#19982;PATE&#65288;&#19968;&#31181;&#21033;&#29992;&#25945;&#24072;&#38598;&#21512;&#30340;&#30693;&#35782;&#36716;&#31227;&#30340;&#26368;&#20808;&#36827;&#30340;DP&#35757;&#32451;&#26041;&#27861;&#65289;&#30456;&#37197;&#21512;&#65292;&#22312;&#38544;&#31169;&#39044;&#31639;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#38544;&#31169;-&#25928;&#29992;&#24179;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#36328;&#22495;&#22270;&#20687;&#20998;&#31867;&#36827;&#34892;&#20102;&#39069;&#22806;&#30340;&#23454;&#39564;&#65292;&#20197;&#36827;&#19968;&#27493;&#25581;&#31034;&#22312;DP&#20013;VP&#30340;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#39564;&#35777;VP&#22312;DP&#32771;&#34385;&#19979;&#30340;&#26377;&#25928;&#24615;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Prompting (VP) is an emerging and powerful technique that allows sample-efficient adaptation to downstream tasks by engineering a well-trained frozen source model. In this work, we explore the benefits of VP in constructing compelling neural network classifiers with differential privacy (DP). We explore and integrate VP into canonical DP training methods and demonstrate its simplicity and efficiency. In particular, we discover that VP in tandem with PATE, a state-of-the-art DP training method that leverages the knowledge transfer from an ensemble of teachers, achieves the state-of-the-art privacy-utility trade-off with minimum expenditure of privacy budget. Moreover, we conduct additional experiments on cross-domain image classification with a sufficient domain gap to further unveil the advantage of VP in DP. Lastly, we also conduct extensive ablation studies to validate the effectiveness and contribution of VP under DP consideration.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;(PINN)&#26041;&#27861;&#36924;&#36817;&#19968;&#31867;&#20108;&#38454;&#26102;&#38388;&#21160;&#21147;&#23398;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#24418;&#24335;&#65292;&#26377;&#25928;&#38480;&#21046;&#20102;&#36924;&#36817;&#35823;&#24046;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#26032;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12245</link><description>&lt;p&gt;
&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#20108;&#38454;&#26102;&#38388;&#21160;&#21147;&#23398;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35823;&#24046;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Error Analysis of Physics-Informed Neural Networks for Approximating Dynamic PDEs of Second Order in Time. (arXiv:2303.12245v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;(PINN)&#26041;&#27861;&#36924;&#36817;&#19968;&#31867;&#20108;&#38454;&#26102;&#38388;&#21160;&#21147;&#23398;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#24418;&#24335;&#65292;&#26377;&#25928;&#38480;&#21046;&#20102;&#36924;&#36817;&#35823;&#24046;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#26032;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#21033;&#29992;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;(PINN)&#26041;&#27861;&#36924;&#36817;&#19968;&#31867;&#20108;&#38454;&#26102;&#38388;&#21160;&#21147;&#23398;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#65292;&#24182;&#23545;&#27874;&#21160;&#26041;&#31243;&#12289;&#27491;&#24358;-&#25096;&#30331;&#26041;&#31243;&#21644;&#32447;&#24615;&#24377;&#24615;&#21160;&#21147;&#23398;&#26041;&#31243;&#36827;&#34892;PINN&#30340;&#35823;&#24046;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#20004;&#20010;&#38544;&#34255;&#23618;&#21644;$\tanh$&#28608;&#27963;&#20989;&#25968;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;PINN&#36924;&#36817;&#35823;&#24046;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#25439;&#22833;&#21644;&#35757;&#32451;&#25968;&#25454;&#28857;&#25968;&#37327;(&#31215;&#20998;&#28857;)&#26469;&#26377;&#25928;&#22320;&#38480;&#21046;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24314;&#35758;&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#24418;&#24335;&#65292;&#20854;&#20013;&#21253;&#21547;&#26576;&#20123;&#27531;&#24046;&#65292;&#36825;&#20123;&#27531;&#24046;&#23545;&#35823;&#24046;&#20272;&#35745;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22312;&#32463;&#20856;&#30340;PINN&#25439;&#22833;&#20844;&#24335;&#20013;&#21487;&#33021;&#19981;&#23384;&#22312;&#12290;&#37319;&#29992;&#36825;&#20123;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#24418;&#24335;&#21487;&#20197;&#23548;&#33268;&#19968;&#31181;&#21464;&#24322;&#30340;PINN&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#26032;&#30340;PINN&#31639;&#27861;&#23545;&#27874;&#21160;&#26041;&#31243;&#12289;&#27491;&#24358;-&#25096;&#30331;&#26041;&#31243;&#21644;&#32447;&#24615;&#24377;&#24615;&#21160;&#21147;&#23398;&#26041;&#31243;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#35823;&#24046;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#24182;&#23637;&#31034;&#20102;&#26032;PINN&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the approximation of a class of dynamic partial differential equations (PDE) of second order in time by the physics-informed neural network (PINN) approach, and provide an error analysis of PINN for the wave equation, the Sine-Gordon equation and the linear elastodynamic equation. Our analyses show that, with feed-forward neural networks having two hidden layers and the $\tanh$ activation function, the PINN approximation errors for the solution field, its time derivative and its gradient field can be effectively bounded by the training loss and the number of training data points (quadrature points). Our analyses further suggest new forms for the training loss function, which contain certain residuals that are crucial to the error estimate but would be absent from the canonical PINN loss formulation. Adopting these new forms for the loss function leads to a variant PINN algorithm. We present ample numerical experiments with the new PINN algorithm for the wave equation, the S
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DG-Trans&#30340;&#20132;&#36890;&#20107;&#25925;&#24433;&#21709;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#22270;&#23398;&#20064;&#39044;&#27979;&#20132;&#36890;&#20107;&#25925;&#30340;&#24433;&#21709;&#65292;&#21253;&#21547;&#19968;&#20010;&#21452;&#23618;&#31354;&#38388;&#21464;&#25442;&#22120;&#21644;&#19968;&#20010;&#22522;&#20110;&#37325;&#35201;&#24615;&#24471;&#20998;&#30340;&#26102;&#24577;&#21464;&#25442;&#22120;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#20174;&#21160;&#24577;&#22270;&#20013;&#25552;&#21462;&#24322;&#24120;&#23376;&#22270;&#25110;&#23376;&#26102;&#38388;&#24207;&#21015;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12238</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#23618;&#22270;&#21464;&#25442;&#22120;&#30340;&#26102;&#31354;&#20107;&#25925;&#24433;&#21709;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DG-Trans: Dual-level Graph Transformer for Spatiotemporal Incident Impact Prediction on Traffic Networks. (arXiv:2303.12238v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12238
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DG-Trans&#30340;&#20132;&#36890;&#20107;&#25925;&#24433;&#21709;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#22270;&#23398;&#20064;&#39044;&#27979;&#20132;&#36890;&#20107;&#25925;&#30340;&#24433;&#21709;&#65292;&#21253;&#21547;&#19968;&#20010;&#21452;&#23618;&#31354;&#38388;&#21464;&#25442;&#22120;&#21644;&#19968;&#20010;&#22522;&#20110;&#37325;&#35201;&#24615;&#24471;&#20998;&#30340;&#26102;&#24577;&#21464;&#25442;&#22120;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#20174;&#21160;&#24577;&#22270;&#20013;&#25552;&#21462;&#24322;&#24120;&#23376;&#22270;&#25110;&#23376;&#26102;&#38388;&#24207;&#21015;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21450;&#26102;&#39044;&#20272;&#20132;&#36890;&#20107;&#25925;&#30340;&#24433;&#21709;&#21487;&#20197;&#25351;&#23548;&#20154;&#20204;&#30340;&#20986;&#34892;&#35268;&#21010;&#65292;&#25552;&#39640;&#20132;&#36890;&#26426;&#26500;&#20915;&#31574;&#38887;&#24615;&#12290;&#28982;&#32780;&#65292;&#30456;&#27604;&#20110;&#33410;&#28857;&#32423;&#21644;&#22270;&#32423;&#39044;&#27979;&#20219;&#21153;&#65292;&#23427;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#20174;&#21160;&#24577;&#22270;&#20013;&#25552;&#21462;&#24322;&#24120;&#23376;&#22270;&#25110;&#23376;&#26102;&#38388;&#24207;&#21015;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DG-Trans&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#36890;&#20107;&#25925;&#24433;&#21709;&#39044;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#22270;&#23398;&#20064;&#39044;&#35265;&#20132;&#36890;&#20107;&#25925;&#30340;&#24433;&#21709;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21253;&#21547;&#19968;&#20010;&#21452;&#23618;&#31354;&#38388;&#21464;&#25442;&#22120;&#21644;&#19968;&#20010;&#22522;&#20110;&#37325;&#35201;&#24615;&#24471;&#20998;&#30340;&#26102;&#24577;&#21464;&#25442;&#22120;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#26032;&#26500;&#24314;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;&#21452;&#23618;&#31354;&#38388;&#21464;&#25442;&#22120;&#21024;&#38500;&#33410;&#28857;&#20043;&#38388;&#30340;&#19981;&#24517;&#35201;&#30340;&#36793;&#32536;&#65292;&#23558;&#21463;&#24433;&#21709;&#30340;&#23376;&#22270;&#19982;&#20854;&#20182;&#33410;&#28857;&#38548;&#31163;&#24320;&#26469;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22522;&#20110;&#37325;&#35201;&#24615;&#24471;&#20998;&#30340;&#26102;&#24577;&#21464;&#25442;&#22120;&#35782;&#21035;&#33410;&#28857;&#29305;&#24449;&#30340;&#24322;&#24120;&#21464;&#21270;&#65292;&#20351;&#39044;&#27979;&#26356;&#20381;&#36182;&#20110;&#26368;&#36817;&#30340;&#36807;&#21435;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prompt estimation of traffic incident impacts can guide commuters in their trip planning and improve the resilience of transportation agencies' decision-making on resilience. However, it is more challenging than node-level and graph-level forecasting tasks, as it requires extracting the anomaly subgraph or sub-time-series from dynamic graphs. In this paper, we propose DG-Trans, a novel traffic incident impact prediction framework, to foresee the impact of traffic incidents through dynamic graph learning. The proposed framework contains a dual-level spatial transformer and an importance-score-based temporal transformer, and the performance of this framework is justified by two newly constructed benchmark datasets. The dual-level spatial transformer removes unnecessary edges between nodes to isolate the affected subgraph from the other nodes. Meanwhile, the importance-score-based temporal transformer identifies abnormal changes in node features, causing the predictions to rely more o
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#34429;&#28982;&#33021;&#22815;&#28040;&#38500;&#25968;&#25454;&#20849;&#20139;&#65292;&#20294;&#20849;&#20139;&#30340;&#26799;&#24230;&#21487;&#33021;&#20250;&#21253;&#21547;&#31169;&#23494;&#20449;&#24687;&#65292;&#24182;&#19988;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#24694;&#24847;&#20462;&#25913;&#26550;&#26500;&#21644;&#21442;&#25968;&#25110;&#20351;&#29992;&#20248;&#21270;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#36817;&#20284;&#29992;&#25143;&#25968;&#25454;&#65292;&#23548;&#33268;&#29992;&#25143;&#25968;&#25454;&#27844;&#38706;&#12290;</title><link>http://arxiv.org/abs/2303.12233</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23433;&#20840;&#32858;&#21512;&#24182;&#38750;&#38544;&#31169;&#65306;&#36890;&#36807;&#27169;&#22411;&#20462;&#25913;&#22823;&#35268;&#27169;&#27844;&#38706;&#29992;&#25143;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Secure Aggregation in Federated Learning is not Private: Leaking User Data at Large Scale through Model Modification. (arXiv:2303.12233v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12233
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#34429;&#28982;&#33021;&#22815;&#28040;&#38500;&#25968;&#25454;&#20849;&#20139;&#65292;&#20294;&#20849;&#20139;&#30340;&#26799;&#24230;&#21487;&#33021;&#20250;&#21253;&#21547;&#31169;&#23494;&#20449;&#24687;&#65292;&#24182;&#19988;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#24694;&#24847;&#20462;&#25913;&#26550;&#26500;&#21644;&#21442;&#25968;&#25110;&#20351;&#29992;&#20248;&#21270;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#36817;&#20284;&#29992;&#25143;&#25968;&#25454;&#65292;&#23548;&#33268;&#29992;&#25143;&#25968;&#25454;&#27844;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#21644;&#38544;&#31169;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#32456;&#31471;&#29992;&#25143;&#35774;&#22791;&#36890;&#24120;&#21253;&#21547;&#22823;&#37327;&#25935;&#24863;&#25968;&#25454;&#65292;&#19981;&#24212;&#19982;&#26381;&#21153;&#22120;&#25110;&#20225;&#19994;&#20998;&#20139;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#34987;&#24341;&#20837;&#20197;&#22312;&#22823;&#35268;&#27169;&#20998;&#25955;&#24335;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#65292;&#21516;&#26102;&#36890;&#36807;&#28040;&#38500;&#25968;&#25454;&#20849;&#20139;&#26469;&#20445;&#35777;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20849;&#20139;&#30340;&#26799;&#24230;&#36890;&#24120;&#21253;&#21547;&#31169;&#23494;&#20449;&#24687;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#24694;&#24847;&#20462;&#25913;&#26550;&#26500;&#21644;&#21442;&#25968;&#25110;&#20351;&#29992;&#20248;&#21270;&#20174;&#20849;&#20139;&#26799;&#24230;&#20013;&#36817;&#20284;&#29992;&#25143;&#25968;&#25454;&#26469;&#33719;&#24471;&#30693;&#35782;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22823;&#22810;&#25968;&#25915;&#20987;&#33267;&#20170;&#20173;&#21463;&#21040;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#22312;&#20351;&#29992;&#23433;&#20840;&#27169;&#22411;&#32858;&#21512;&#23558;&#23458;&#25143;&#31471;&#26799;&#24230;&#32858;&#21512;&#22312;&#19968;&#36215;&#26102;&#20250;&#22833;&#36133;&#12290;&#30446;&#21069;&#20173;&#28982;&#21487;&#34892;&#30340;&#25915;&#20987;&#22312;&#34987;&#25915;&#20987;&#30340;&#23458;&#25143;&#31471;&#25968;&#37327;&#12289;&#27844;&#28431;&#30340;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#25110;&#35757;&#32451;&#25152;&#38656;&#30340;&#36845;&#20195;&#27425;&#25968;&#26041;&#38754;&#37117;&#21463;&#21040;&#20005;&#26684;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Security and privacy are important concerns in machine learning. End user devices often contain a wealth of data and this information is sensitive and should not be shared with servers or enterprises. As a result, federated learning was introduced to enable machine learning over large decentralized datasets while promising privacy by eliminating the need for data sharing. However, prior work has shown that shared gradients often contain private information and attackers can gain knowledge either through malicious modification of the architecture and parameters or by using optimization to approximate user data from the shared gradients. Despite this, most attacks have so far been limited in scale of number of clients, especially failing when client gradients are aggregated together using secure model aggregation. The attacks that still function are strongly limited in the number of clients attacked, amount of training samples they leak, or number of iterations they take to be trained. I
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#26469;&#39044;&#27979;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#25925;&#38556;&#21644;&#21361;&#38505;&#39550;&#39542;&#21592;&#65292;&#24182;&#22312;&#32553;&#23567;&#30340;&#24494;&#22411;&#22478;&#24066;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2303.12224</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#35774;&#26045;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#21644;&#39550;&#39542;&#21592;&#22833;&#35823;&#39044;&#38450;
&lt;/p&gt;
&lt;p&gt;
Infrastructure-based End-to-End Learning and Prevention of Driver Failure. (arXiv:2303.12224v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#26041;&#27861;&#26469;&#39044;&#27979;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#25925;&#38556;&#21644;&#21361;&#38505;&#39550;&#39542;&#21592;&#65292;&#24182;&#22312;&#32553;&#23567;&#30340;&#24494;&#22411;&#22478;&#24066;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20132;&#21449;&#36335;&#21475;&#31649;&#29702;&#22120;&#21487;&#20197;&#36890;&#36807;&#26816;&#27979;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#30340;&#21361;&#38505;&#39550;&#39542;&#21592;&#25110;&#25925;&#38556;&#27169;&#24335;&#65292;&#35686;&#21578;&#25509;&#36817;&#20132;&#21449;&#36335;&#21475;&#30340;&#26469;&#36710;&#65292;&#20174;&#32780;&#25552;&#39640;&#23433;&#20840;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FailureNet&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32553;&#23567;&#30340;&#24494;&#22411;&#22478;&#24066;&#20013;&#22522;&#20110;&#36712;&#36857;&#23545;&#27491;&#24120;&#21644;&#19981;&#23433;&#20840;&#39550;&#39542;&#21592;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12290;FailureNet&#35266;&#23519;&#36710;&#36742;&#25509;&#36817;&#20132;&#21449;&#21475;&#26102;&#30340;&#23039;&#24577;&#65292;&#26816;&#27979;&#33258;&#20027;&#22534;&#26632;&#20013;&#26159;&#21542;&#23384;&#22312;&#25925;&#38556;&#65292;&#24182;&#35686;&#21578;&#20132;&#21449;&#27969;&#37327;&#26377;&#28508;&#22312;&#21361;&#38505;&#30340;&#39550;&#39542;&#21592;&#12290;FailureNet&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#25511;&#21046;&#25925;&#38556;&#12289;&#19978;&#28216;&#24863;&#30693;&#38169;&#35823;&#21644;&#36229;&#36895;&#39550;&#39542;&#21592;&#65292;&#19982;&#27491;&#24120;&#39550;&#39542;&#21152;&#20197;&#21306;&#21035;&#12290;&#35813;&#32593;&#32476;&#22312;MiniCity&#20013;&#36827;&#34892;&#35757;&#32451;&#21644;&#37096;&#32626;&#65292;&#19982;&#22522;&#20110;&#36895;&#24230;&#25110;&#39057;&#29575;&#30340;&#39044;&#27979;&#22120;&#30456;&#27604;&#65292;FailureNet&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24403;&#37096;&#32626;&#22312;&#30828;&#20214;&#19978;&#26102;&#65292;&#20854;&#20934;&#30830;&#24615;&#39640;&#36798;84%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent intersection managers can improve safety by detecting dangerous drivers or failure modes in autonomous vehicles, warning oncoming vehicles as they approach an intersection. In this work, we present FailureNet, a recurrent neural network trained end-to-end on trajectories of both nominal and reckless drivers in a scaled miniature city. FailureNet observes the poses of vehicles as they approach an intersection and detects whether a failure is present in the autonomy stack, warning cross-traffic of potentially dangerous drivers. FailureNet can accurately identify control failures, upstream perception errors, and speeding drivers, distinguishing them from nominal driving. The network is trained and deployed with autonomous vehicles in the MiniCity. Compared to speed or frequency-based predictors, FailureNet's recurrent neural network structure provides improved predictive power, yielding upwards of 84% accuracy when deployed on hardware.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#31639;&#27861;&#26694;&#26550;&#26469;&#23558;&#23618;&#27425;&#32858;&#31867;&#26041;&#27861;&#24212;&#29992;&#20110;&#22270;&#20013;&#30340;&#31038;&#21306;&#26816;&#27979;&#12290;&#30456;&#20284;&#24230;&#30697;&#38453;&#12289;&#29305;&#24449;&#21521;&#37327;&#30697;&#38453;&#21644;&#33410;&#28857;&#27431;&#20960;&#37324;&#24471;&#21521;&#37327;&#34920;&#31034;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#21487;&#20197;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#21644;&#28857;&#38388;&#20114;&#20449;&#24687;&#27491;&#21521;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.12212</link><description>&lt;p&gt;
&#22522;&#20110;&#33410;&#28857;&#30456;&#20284;&#24230;&#12289;&#22270;&#34920;&#31034;&#23398;&#20064;&#21644;&#23618;&#27425;&#32858;&#31867;&#30340;&#22797;&#26434;&#32593;&#32476;&#31038;&#32676;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Community detection in complex networks via node similarity, graph representation learning, and hierarchical clustering. (arXiv:2303.12212v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#31639;&#27861;&#26694;&#26550;&#26469;&#23558;&#23618;&#27425;&#32858;&#31867;&#26041;&#27861;&#24212;&#29992;&#20110;&#22270;&#20013;&#30340;&#31038;&#21306;&#26816;&#27979;&#12290;&#30456;&#20284;&#24230;&#30697;&#38453;&#12289;&#29305;&#24449;&#21521;&#37327;&#30697;&#38453;&#21644;&#33410;&#28857;&#27431;&#20960;&#37324;&#24471;&#21521;&#37327;&#34920;&#31034;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#21487;&#20197;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#21644;&#28857;&#38388;&#20114;&#20449;&#24687;&#27491;&#21521;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#32676;&#26816;&#27979;&#26159;&#20998;&#26512;&#23454;&#38469;&#22270;&#21644;&#22797;&#26434;&#32593;&#32476;&#65288;&#22914;&#31038;&#20132;&#12289;&#20132;&#36890;&#12289;&#24341;&#29992;&#12289;&#32593;&#32476;&#23433;&#20840;&#20197;&#21450;&#39135;&#29289;&#38142;&#65289;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#21463;&#21040;&#31038;&#21306;&#26816;&#27979;&#19982;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#32858;&#31867;&#20043;&#38388;&#30340;&#35768;&#22810;&#30456;&#20284;&#24615;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#23558;&#23618;&#27425;&#32858;&#31867;&#26041;&#27861;&#24212;&#29992;&#20110;&#22270;&#20013;&#30340;&#31038;&#21306;&#26816;&#27979;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22522;&#20110;&#33410;&#28857;&#30456;&#20284;&#24230;&#30697;&#38453;&#12289;&#20854;&#29305;&#24449;&#21521;&#37327;&#30697;&#38453;&#20197;&#21450;&#33410;&#28857;&#30340;&#27431;&#20960;&#37324;&#24471;&#21521;&#37327;&#34920;&#31034;&#65292;&#24212;&#29992;&#21508;&#31181;&#22522;&#20110;&#36830;&#25509;&#30340;&#32858;&#31867;&#31639;&#27861;&#65288;&#21333;&#38142;&#25509;&#12289;&#20840;&#36830;&#25509;&#12289;&#24179;&#22343;&#38142;&#25509;&#12289;Ward&#12289;Genie&#65289;&#26469;&#26597;&#25214;&#31038;&#21306;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26694;&#26550;&#36873;&#25321;&#20998;&#26512;&#65292;&#21253;&#25324;&#26368;&#20808;&#36827;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#22270;&#34920;&#31034;&#65289;&#20197;&#21450;&#19968;&#20010;&#24050;&#30693;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#32467;&#26524;&#30340;&#28857;&#38388;&#20114;&#20449;&#24687;&#27491;&#21521;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community detection is a critical challenge in the analysis of real-world graphs and complex networks, including social, transportation, citation, cybersecurity networks, and food webs. Motivated by many similarities between community detection and clustering in Euclidean spaces, we propose three algorithm frameworks to apply hierarchical clustering methods for community detection in graphs. We show that using our methods, it is possible to apply various linkage-based (single-, complete-, average- linkage, Ward, Genie) clustering algorithms to find communities based on vertex similarity matrices, eigenvector matrices thereof, and Euclidean vector representations of nodes. We convey a comprehensive analysis of choices for each framework, including state-of-the-art graph representation learning algorithms, such as Deep Neural Graph Representation, and a vertex proximity matrix known to yield high-quality results in machine learning -- Positive Pointwise Mutual Information. Overall, we te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#25237;&#24433;&#30340;kNN&#38598;&#25104;&#20998;&#31867;&#22120;&#65292;&#20351;&#29992;&#25193;&#23637;&#37051;&#22495;&#35268;&#21017;&#21644;&#38477;&#32500;&#26469;&#22686;&#21152;&#22522;&#26412;&#23398;&#20064;&#32773;&#30340;&#38543;&#26426;&#24615;&#24182;&#20445;&#30041;&#29305;&#24449;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.12210</link><description>&lt;p&gt;
&#38543;&#26426;&#25237;&#24433;k&#26368;&#36817;&#37051;&#38598;&#25104;&#20998;&#31867;&#22120; via &#25193;&#23637;&#37051;&#22495;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Random Projection k Nearest Neighbours Ensemble for Classification via Extended Neighbourhood Rule. (arXiv:2303.12210v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#25237;&#24433;&#30340;kNN&#38598;&#25104;&#20998;&#31867;&#22120;&#65292;&#20351;&#29992;&#25193;&#23637;&#37051;&#22495;&#35268;&#21017;&#21644;&#38477;&#32500;&#26469;&#22686;&#21152;&#22522;&#26412;&#23398;&#20064;&#32773;&#30340;&#38543;&#26426;&#24615;&#24182;&#20445;&#30041;&#29305;&#24449;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;k&#26368;&#36817;&#37051;&#65288;kNN&#65289;&#30340;&#38598;&#25104;&#23558;&#35768;&#22810;&#22522;&#26412;&#23398;&#20064;&#32773;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#27599;&#20010;&#23398;&#20064;&#32773;&#37117;&#26159;&#22522;&#20110;&#32473;&#23450;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#19968;&#20010;&#26679;&#26412;&#26500;&#24314;&#30340;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#25237;&#24433;&#25193;&#23637;&#37051;&#22495;&#35268;&#21017;&#65288;RPExNRule&#65289;&#38598;&#25104;&#65292;&#20854;&#20013;&#26469;&#33258;&#32473;&#23450;&#35757;&#32451;&#25968;&#25454;&#30340;&#33258;&#20030;&#26679;&#26412;&#22312;&#38477;&#20302;&#30340;&#32500;&#24230;&#20013;&#34987;&#38543;&#26426;&#25237;&#24433;&#65292;&#20197;&#22686;&#21152;&#22522;&#26412;&#27169;&#22411;&#30340;&#38543;&#26426;&#24615;&#24182;&#20445;&#30041;&#29305;&#24449;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#25193;&#23637;&#37051;&#22495;&#35268;&#21017;&#65288;ExNRule&#65289;&#23558;kNN&#20316;&#20026;&#22522;&#26412;&#23398;&#20064;&#32773;&#25311;&#21512;&#38543;&#26426;&#25237;&#24433;&#30340;&#33258;&#20030;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensembles based on k nearest neighbours (kNN) combine a large number of base learners, each constructed on a sample taken from a given training data. Typical kNN based ensembles determine the k closest observations in the training data bounded to a test sample point by a spherical region to predict its class. In this paper, a novel random projection extended neighbourhood rule (RPExNRule) ensemble is proposed where bootstrap samples from the given training data are randomly projected into lower dimensions for additional randomness in the base models and to preserve features information. It uses the extended neighbourhood rule (ExNRule) to fit kNN as base learners on randomly projected bootstrap samples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; MGVLT &#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#22270;&#20687;&#21644;&#25991;&#26412;&#24207;&#21015;&#65292;&#36890;&#36807;&#38750;&#33258;&#22238;&#24402;&#25513;&#30721;&#39044;&#27979;&#23454;&#29616;&#20102;&#21452;&#21521;&#19978;&#19979;&#25991;&#32534;&#30721;&#21644;&#24555;&#36895;&#35299;&#30721;&#31561;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.12208</link><description>&lt;p&gt;
MAGVLT: &#24102;&#25513;&#30721;&#30340;&#29983;&#25104;&#35270;&#35273;&#35821;&#35328;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
MAGVLT: Masked Generative Vision-and-Language Transformer. (arXiv:2303.12208v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; MGVLT &#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#22270;&#20687;&#21644;&#25991;&#26412;&#24207;&#21015;&#65292;&#36890;&#36807;&#38750;&#33258;&#22238;&#24402;&#25513;&#30721;&#39044;&#27979;&#23454;&#29616;&#20102;&#21452;&#21521;&#19978;&#19979;&#25991;&#32534;&#30721;&#21644;&#24555;&#36895;&#35299;&#30721;&#31561;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22312;&#22810;&#27169;&#24577;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#29983;&#25104;&#24314;&#27169;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#21457;&#23637;&#65292;&#20294;&#20173;&#26377;&#35768;&#22810;&#38480;&#21046;&#65292;&#20363;&#22914;&#20165;&#29983;&#25104;&#19968;&#31181;&#27169;&#24577;&#30340;&#22266;&#23450;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#21487;&#20197;&#29983;&#25104;&#22270;&#20687;&#21644;&#25991;&#26412;&#24207;&#21015;&#30340;&#32479;&#19968;&#29983;&#25104;&#24335;&#35270;&#35273;&#35821;&#35328;&#65288;VL&#65289;&#27169;&#22411;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#33258;&#22238;&#24402;&#25513;&#30721;&#39044;&#27979;&#30340;&#29983;&#25104;VL&#21464;&#21387;&#22120;&#65292;&#21517;&#20026;MAGVLT&#65292;&#24182;&#23558;&#20854;&#19982;&#33258;&#22238;&#24402;&#29983;&#25104;VL&#21464;&#21387;&#22120;&#65288;ARGVLT&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#19982;ARGVLT&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;MAGVLT&#23454;&#29616;&#20102;&#21452;&#21521;&#19978;&#19979;&#25991;&#32534;&#30721;&#65292;&#36890;&#36807;&#36845;&#20195;&#32454;&#21270;&#30340;&#24182;&#34892;&#26631;&#35760;&#39044;&#27979;&#23454;&#29616;&#20102;&#24555;&#36895;&#35299;&#30721;&#65292;&#20855;&#26377;&#22270;&#20687;&#21644;&#25991;&#26412;&#22635;&#20805;&#31561;&#25193;&#23637;&#32534;&#36753;&#21151;&#33021;&#12290;&#20026;&#20102;&#20174;&#22836;&#24320;&#22987;&#20005;&#26684;&#35757;&#32451;&#25105;&#20204;&#30340;MAGVLT&#27169;&#22411;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#20174;&#22270;&#20687;&#21040;&#25991;&#26412;&#12289;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#12289;&#20197;&#21450;&#32852;&#21512;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#25513;&#30721;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While generative modeling on multimodal image-text data has been actively developed with large-scale paired datasets, there have been limited attempts to generate both image and text data by a single model rather than a generation of one fixed modality conditioned on the other modality. In this paper, we explore a unified generative vision-and-language (VL) model that can produce both images and text sequences. Especially, we propose a generative VL transformer based on the non-autoregressive mask prediction, named MAGVLT, and compare it with an autoregressive generative VL transformer (ARGVLT). In comparison to ARGVLT, the proposed MAGVLT enables bidirectional context encoding, fast decoding by parallel token predictions in an iterative refinement, and extended editing capabilities such as image and text infilling. For rigorous training of our MAGVLT with image-text pairs from scratch, we combine the image-to-text, text-to-image, and joint image-and-text mask prediction tasks. Moreove
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#25968;&#23383;&#24179;&#21488;&#20256;&#36882;&#30340;&#34892;&#20026;&#20581;&#24247;&#20171;&#20837;&#26368;&#22823;&#21270;&#20581;&#24247;&#32467;&#26524;&#21644;&#27835;&#30103;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DecompPI&#30340;&#26032;&#31639;&#27861;&#65292;&#20174;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#65292;&#20943;&#36731;&#20102;&#22312;&#32447;&#23454;&#39564;&#30340;&#38656;&#35201;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28176;&#36817;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12206</link><description>&lt;p&gt;
&#34892;&#20026;&#20581;&#24247;&#20010;&#24615;&#21270;&#20171;&#20837;&#30340;&#25919;&#31574;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Policy Optimization for Personalized Interventions in Behavioral Health. (arXiv:2303.12206v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12206
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#25968;&#23383;&#24179;&#21488;&#20256;&#36882;&#30340;&#34892;&#20026;&#20581;&#24247;&#20171;&#20837;&#26368;&#22823;&#21270;&#20581;&#24247;&#32467;&#26524;&#21644;&#27835;&#30103;&#25104;&#26412;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DecompPI&#30340;&#26032;&#31639;&#27861;&#65292;&#20174;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#65292;&#20943;&#36731;&#20102;&#22312;&#32447;&#23454;&#39564;&#30340;&#38656;&#35201;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28176;&#36817;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#23450;&#20041;&#65306;&#36890;&#36807;&#25968;&#23383;&#24179;&#21488;&#20256;&#36882;&#30340;&#34892;&#20026;&#20581;&#24247;&#20171;&#20837;&#65292;&#36890;&#36807;&#25945;&#32946;&#65292;&#28608;&#21169;&#65292;&#25552;&#37266;&#21644;&#22806;&#23637;&#65292;&#26377;&#26395;&#26174;&#30528;&#25913;&#21892;&#20581;&#24247;&#32467;&#26524;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20171;&#20837;&#20855;&#26377;&#25104;&#26412;&#21644;&#33021;&#21147;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#20248;&#21270;&#24739;&#32773;&#20010;&#24615;&#21270;&#20171;&#20837;&#20197;&#26368;&#22823;&#21270;&#26576;&#31181;&#38271;&#26399;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#26041;&#27861;/&#32467;&#26524;&#65306;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26469;&#33258;&#22686;&#24378;&#23398;&#20064;&#25991;&#29486;&#30340;&#36890;&#29992;&#26080;&#27169;&#22411;&#26041;&#27861;&#23545;&#20110;&#21307;&#30103;&#24212;&#29992;&#26469;&#35828;&#36807;&#20110;&#25968;&#25454;&#23494;&#38598;&#65292;&#32780;&#26356;&#31616;&#21333;&#30340;&#36172;&#33218;&#38382;&#39064;&#26041;&#27861;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#24573;&#30053;&#20102;&#38271;&#26399;&#24739;&#32773;&#21160;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#31216;&#20026;DecompPI&#65292;&#23427;&#36817;&#20284;&#20110;&#19968;&#27493;&#25919;&#31574;&#36845;&#20195;&#12290;&#23454;&#29616;DecompPI&#21482;&#38656;&#20174;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#65292;&#20943;&#36731;&#20102;&#22312;&#32447;&#23454;&#39564;&#30340;&#38656;&#35201;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19968;&#31181;&#33258;&#28982;&#30340;&#32467;&#26500;&#20551;&#35774;&#19979;&#65292;DecompPI&#21487;&#20197;&#33719;&#24471;&#31639;&#27861;&#22797;&#26434;&#24230;&#30340;&#28176;&#36817;&#25910;&#25947;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;.
&lt;/p&gt;
&lt;p&gt;
Problem definition: Behavioral health interventions, delivered through digital platforms, have the potential to significantly improve health outcomes, through education, motivation, reminders, and outreach. We study the problem of optimizing personalized interventions for patients to maximize some long-term outcome, in a setting where interventions are costly and capacity-constrained.  Methodology/results: This paper provides a model-free approach to solving this problem. We find that generic model-free approaches from the reinforcement learning literature are too data intensive for healthcare applications, while simpler bandit approaches make progress at the expense of ignoring long-term patient dynamics. We present a new algorithm we dub DecompPI that approximates one step of policy iteration. Implementing DecompPI simply consists of a prediction task from offline data, alleviating the need for online experimentation. Theoretically, we show that under a natural set of structural assu
&lt;/p&gt;</description></item><item><title>EZtune&#26159;&#19968;&#20010;&#21487;&#20197;&#22312;R&#20013;&#33258;&#21160;&#35843;&#25972;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;adaboost&#12289;&#26799;&#24230;&#25552;&#21319;&#26426;&#21644;&#24377;&#24615;&#32593;&#32476;&#31561;&#27169;&#22411;&#30340;&#36719;&#20214;&#21253;&#65292;&#20855;&#26377;&#31616;&#21333;&#26131;&#29992;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#36866;&#21512;&#26032;&#25163;&#25110;R&#19978;&#30340;&#32479;&#35745;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.12177</link><description>&lt;p&gt;
EZtune&#65306; &#22312;R&#20013;&#33258;&#21160;&#35843;&#21442;&#30340;&#36719;&#20214;&#21253;&#65288;arXiv:2303.12177v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
EZtune: A Package for Automated Hyperparameter Tuning in R. (arXiv:2303.12177v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12177
&lt;/p&gt;
&lt;p&gt;
EZtune&#26159;&#19968;&#20010;&#21487;&#20197;&#22312;R&#20013;&#33258;&#21160;&#35843;&#25972;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;adaboost&#12289;&#26799;&#24230;&#25552;&#21319;&#26426;&#21644;&#24377;&#24615;&#32593;&#32476;&#31561;&#27169;&#22411;&#30340;&#36719;&#20214;&#21253;&#65292;&#20855;&#26377;&#31616;&#21333;&#26131;&#29992;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#36866;&#21512;&#26032;&#25163;&#25110;R&#19978;&#30340;&#32479;&#35745;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32479;&#35745;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#35768;&#22810;&#36825;&#20123;&#27169;&#22411;&#37117;&#26377;&#24517;&#39035;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#25165;&#33021;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36825;&#24182;&#19981;&#26159;&#20214;&#31616;&#21333;&#30340;&#20107;&#24773;&#12290;EZtune &#26159;&#19968;&#20010; R &#21253;&#65292;&#20855;&#26377;&#31616;&#21333;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#21487;&#20197;&#35843;&#25972;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;adaboost&#12289;&#26799;&#24230;&#25552;&#21319;&#26426;&#21644;&#24377;&#24615;&#32593;&#32476;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#31616;&#35201;&#24635;&#32467;&#20102; EZtune &#21487;&#20197;&#35843;&#25972;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#27599;&#20010;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102; EZtune&#12289;caret &#21644; tidymodels &#30340;&#26131;&#29992;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20351;&#29992; EZtune &#21644; tidymodels &#35843;&#25972;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#26102;&#38388;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992; EZtune &#26469;&#24110;&#21161;&#36873;&#25321;&#20855;&#26377;&#26368;&#20248;&#39044;&#27979;&#33021;&#21147;&#30340;&#26368;&#32456;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27604;&#36739;&#34920;&#26126;&#65292;&#8220;EZtune&#8221;&#21487;&#20197;&#35843;&#25972;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#26799;&#24230;&#25552;&#21319;&#26426;&#65292;&#24182;&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#36866;&#21512;&#26032;&#25163;&#25110;R&#19978;&#30340;&#32479;&#35745;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical learning models have been growing in popularity in recent years. Many of these models have hyperparameters that must be tuned for models to perform well. Tuning these parameters is not trivial. EZtune is an R package with a simple user interface that can tune support vector machines, adaboost, gradient boosting machines, and elastic net. We first provide a brief summary of the the models that EZtune can tune, including a discussion of each of their hyperparameters. We then compare the ease of using EZtune, caret, and tidymodels. This is followed with a comparison of the accuracy and computation times for models tuned with EZtune and tidymodels. We conclude with a demonstration of how how EZtune can be used to help select a final model with optimal predictive power. Our comparison shows that EZtune can tune support vector machines and gradient boosting machines with EZtune also provides a user interface that is easy to use for a novice to statistical learning models or R.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#21644;&#27880;&#20837;&#23545;&#25239;&#24615;&#21442;&#25968;&#22122;&#22768;&#65292;&#23558;SNN&#24212;&#29992;&#31243;&#24207;&#31163;&#32447;&#35757;&#32451;&#21644;&#37096;&#32626;&#21040;Dynap-SE2&#28151;&#21512;&#20449;&#21495;&#31070;&#32463;&#24418;&#24577;&#22788;&#29702;&#22120;&#30340;&#26032;&#26041;&#27861;&#12290;&#20248;&#21270;&#21518;&#30340;&#32593;&#32476;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#23545;&#20110;&#30828;&#20214;&#32422;&#26463;&#30340;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#31243;&#24207;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.12167</link><description>&lt;p&gt;
&#20351;&#29992;Rockpool&#23558;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#31243;&#24207;&#35757;&#32451;&#37096;&#32626;&#21040;&#28151;&#21512;&#20449;&#21495;&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;Dynap-SE2&#19978;
&lt;/p&gt;
&lt;p&gt;
Training and Deploying Spiking NN Applications to the Mixed-Signal Neuromorphic Chip Dynap-SE2 with Rockpool. (arXiv:2303.12167v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#21644;&#27880;&#20837;&#23545;&#25239;&#24615;&#21442;&#25968;&#22122;&#22768;&#65292;&#23558;SNN&#24212;&#29992;&#31243;&#24207;&#31163;&#32447;&#35757;&#32451;&#21644;&#37096;&#32626;&#21040;Dynap-SE2&#28151;&#21512;&#20449;&#21495;&#31070;&#32463;&#24418;&#24577;&#22788;&#29702;&#22120;&#30340;&#26032;&#26041;&#27861;&#12290;&#20248;&#21270;&#21518;&#30340;&#32593;&#32476;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#23545;&#20110;&#30828;&#20214;&#32422;&#26463;&#30340;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#31243;&#24207;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#20449;&#21495;&#31070;&#32463;&#24418;&#24577;&#22788;&#29702;&#22120;&#21033;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#20869;&#30340;&#31232;&#30095;&#24322;&#27493;&#35745;&#31639;&#25552;&#20379;&#26497;&#20302;&#21151;&#32791;&#30340;&#36793;&#32536;&#25512;&#29702;&#36127;&#36733;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#25311;&#30828;&#20214;&#21442;&#25968;&#30340;&#21463;&#38480;&#21487;&#25511;&#24615;&#20197;&#21450;&#30001;&#20110;&#21046;&#36896;&#38750;&#29702;&#24819;&#24615;&#25152;&#23548;&#33268;&#30340;&#27169;&#25311;&#30005;&#36335;&#30340;&#26080;&#24847;&#21442;&#25968;&#21644;&#21160;&#24577;&#21464;&#21270;&#65292;&#23558;&#31283;&#20581;&#30340;&#24212;&#29992;&#31243;&#24207;&#37096;&#32626;&#21040;&#36825;&#20123;&#35774;&#22791;&#26159;&#22797;&#26434;&#30340;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#29992;&#20110;&#23558;SNN&#24212;&#29992;&#31243;&#24207;&#31163;&#32447;&#35757;&#32451;&#21644;&#37096;&#32626;&#21040;&#28151;&#21512;&#20449;&#21495;&#31070;&#32463;&#24418;&#24577;&#22788;&#29702;&#22120;Dynap-SE2&#30340;&#26032;&#22411;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#37325;&#37327;&#37327;&#21270;&#26041;&#27861;&#26469;&#20248;&#21270;&#32593;&#32476;&#30340;&#21442;&#25968;&#65292;&#24182;&#32467;&#21512;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27880;&#20837;&#23545;&#25239;&#24615;&#21442;&#25968;&#22122;&#22768;&#12290;&#20248;&#21270;&#30340;&#32593;&#32476;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#25269;&#24481;&#37327;&#21270;&#21644;&#35774;&#22791;&#19981;&#21305;&#37197;&#30340;&#24433;&#21709;&#65292;&#20351;&#35813;&#26041;&#27861;&#25104;&#20026;&#20855;&#26377;&#30828;&#20214;&#32422;&#26463;&#30340;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#31243;&#24207;&#30340;&#26377;&#21069;&#26223;&#30340;&#20505;&#36873;&#26041;&#27861;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#24320;&#28304;&#35774;&#35745;&#24037;&#20855;Rockpool&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixed-signal neuromorphic processors provide extremely low-power operation for edge inference workloads, taking advantage of sparse asynchronous computation within Spiking Neural Networks (SNNs). However, deploying robust applications to these devices is complicated by limited controllability over analog hardware parameters, unintended parameter and dynamics variations of analog circuits due to fabrication non-idealities. Here we demonstrate a novel methodology for offline training and deployment of spiking neural networks (SNNs) to the mixed-signal neuromorphic processor Dynap-SE2. The methodology utilizes an unsupervised weight quantization method to optimize the network's parameters, coupled with adversarial parameter noise injection during training. The optimized network is shown to be robust to the effects of quantization and device mismatch, making the method a promising candidate for real-world applications with hardware constraints. This work extends Rockpool, an open-source de
&lt;/p&gt;</description></item><item><title>vCANNs&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#26377;&#38480;&#24212;&#21464;&#19979;&#30340;&#21508;&#21521;&#24322;&#24615;&#38750;&#32447;&#24615;&#31896;&#24377;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#24191;&#20041;Maxwell&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#38750;&#32447;&#24615;&#24212;&#21464;&#65288;&#29575;&#65289;&#30456;&#20851;&#23646;&#24615;&#65292;vCANNs&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#24191;&#27867;&#26448;&#26009;&#30340;&#20934;&#30830;&#19988;&#31232;&#30095;&#30340;&#26412;&#26500;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.12164</link><description>&lt;p&gt;
Viscoelastic Constitutive Artificial Neural Networks (vCANNs) $-$ &#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#21508;&#21521;&#24322;&#24615;&#38750;&#32447;&#24615;&#26377;&#38480;&#31896;&#24377;&#24615;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Viscoelastic Constitutive Artificial Neural Networks (vCANNs) $-$ a framework for data-driven anisotropic nonlinear finite viscoelasticity. (arXiv:2303.12164v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12164
&lt;/p&gt;
&lt;p&gt;
vCANNs&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#26377;&#38480;&#24212;&#21464;&#19979;&#30340;&#21508;&#21521;&#24322;&#24615;&#38750;&#32447;&#24615;&#31896;&#24377;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#24191;&#20041;Maxwell&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#38750;&#32447;&#24615;&#24212;&#21464;&#65288;&#29575;&#65289;&#30456;&#20851;&#23646;&#24615;&#65292;vCANNs&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#24191;&#27867;&#26448;&#26009;&#30340;&#20934;&#30830;&#19988;&#31232;&#30095;&#30340;&#26412;&#26500;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20998;&#23376;&#26448;&#26009;&#30340;&#26412;&#26500;&#34892;&#20026;&#36890;&#24120;&#30001;&#26377;&#38480;&#32447;&#24615;&#31896;&#24377;&#24615;&#65288;FLV&#65289;&#25110;&#20934;&#32447;&#24615;&#31896;&#24377;&#24615;&#65288;QLV&#65289;&#27169;&#22411;&#24314;&#27169;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27969;&#34892;&#30340;&#27169;&#22411;&#26159;&#31616;&#21270;&#30340;&#65292;&#36890;&#24120;&#19981;&#33021;&#20934;&#30830;&#25429;&#25417;&#26448;&#26009;&#30340;&#38750;&#32447;&#24615;&#31896;&#24377;&#24615;&#34892;&#20026;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31896;&#24377;&#24615;&#26412;&#26500;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;vCANNs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#26377;&#38480;&#24212;&#21464;&#19979;&#21508;&#21521;&#24322;&#24615;&#38750;&#32447;&#24615;&#31896;&#24377;&#24615;&#30340;&#26032;&#22411;&#29289;&#29702;&#35266;&#27979;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;vCANNs&#22522;&#20110;&#24191;&#20041;Maxwell&#27169;&#22411;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#38750;&#32447;&#24615;&#24212;&#21464;&#65288;&#29575;&#65289;&#30456;&#20851;&#23646;&#24615;&#12290;vCANNs&#30340;&#28789;&#27963;&#24615;&#20351;&#20854;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#24191;&#27867;&#26448;&#26009;&#30340;&#20934;&#30830;&#19988;&#31232;&#30095;&#30340;&#26412;&#26500;&#27169;&#22411;&#12290;&#20026;&#20102;&#27979;&#35797;vCANNs&#65292;&#25105;&#20204;&#23558;&#20854;&#29992;&#20110;&#20174;Polyvinyl Butyral&#12289;&#30005;&#27963;&#24615;&#32858;&#21512;&#29289;VHB 4910&#21644;4&#25152;&#33719;&#24471;&#30340;&#24212;&#21147;&#24212;&#21464;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The constitutive behavior of polymeric materials is often modeled by finite linear viscoelastic (FLV) or quasi-linear viscoelastic (QLV) models. These popular models are simplifications that typically cannot accurately capture the nonlinear viscoelastic behavior of materials. For example, the success of attempts to capture strain rate-dependent behavior has been limited so far. To overcome this problem, we introduce viscoelastic Constitutive Artificial Neural Networks (vCANNs), a novel physics-informed machine learning framework for anisotropic nonlinear viscoelasticity at finite strains. vCANNs rely on the concept of generalized Maxwell models enhanced with nonlinear strain (rate)-dependent properties represented by neural networks. The flexibility of vCANNs enables them to automatically identify accurate and sparse constitutive models of a broad range of materials. To test vCANNs, we trained them on stress-strain data from Polyvinyl Butyral, the electro-active polymers VHB 4910 and 4
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#28145;&#24230;&#21327;&#26041;&#24046;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#23545;&#28145;&#24230;&#34917;&#20840;&#12289;&#25414;&#38598;&#35843;&#25972;&#21644;&#21333;&#30446;&#23494;&#38598;&#35270;&#35273;&#37324;&#31243;&#35745;&#31561;&#20960;&#20309;&#35270;&#35273;&#20219;&#21153;&#36827;&#34892;&#20102;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2303.12157</link><description>&lt;p&gt;
&#23398;&#20064;&#28145;&#24230;&#21327;&#26041;&#24046;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Learning a Depth Covariance Function. (arXiv:2303.12157v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12157
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23398;&#20064;&#28145;&#24230;&#21327;&#26041;&#24046;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#23545;&#28145;&#24230;&#34917;&#20840;&#12289;&#25414;&#38598;&#35843;&#25972;&#21644;&#21333;&#30446;&#23494;&#38598;&#35270;&#35273;&#37324;&#31243;&#35745;&#31561;&#20960;&#20309;&#35270;&#35273;&#20219;&#21153;&#36827;&#34892;&#20102;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#28145;&#24230;&#21327;&#26041;&#24046;&#20989;&#25968;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20960;&#20309;&#35270;&#35273;&#20219;&#21153;&#12290;&#32473;&#23450;RGB&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#65292;&#21327;&#26041;&#24046;&#20989;&#25968;&#21487;&#28789;&#27963;&#22320;&#29992;&#20110;&#23450;&#20041;&#28145;&#24230;&#20989;&#25968;&#20808;&#39564;&#65292;&#32473;&#23450;&#35266;&#27979;&#30340;&#39044;&#27979;&#20998;&#24067;&#20197;&#21450;&#20027;&#21160;&#28857;&#36873;&#25321;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#26469;&#35299;&#20915;&#19968;&#31995;&#21015;&#19979;&#28216;&#20219;&#21153;&#65306;&#28145;&#24230;&#34917;&#20840;&#12289;&#25414;&#38598;&#35843;&#25972;&#21644;&#21333;&#30446;&#23494;&#38598;&#35270;&#35273;&#37324;&#31243;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose learning a depth covariance function with applications to geometric vision tasks. Given RGB images as input, the covariance function can be flexibly used to define priors over depth functions, predictive distributions given observations, and methods for active point selection. We leverage these techniques for a selection of downstream tasks: depth completion, bundle adjustment, and monocular dense visual odometry.
&lt;/p&gt;</description></item><item><title>NPP&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#22836;&#37096;MRI&#39044;&#22788;&#29702;&#30340;&#25152;&#26377;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;&#21253;&#25324;&#21435;&#38500;&#22836;&#39592;&#12289;&#24378;&#24230;&#24402;&#19968;&#21270;&#21644;&#31354;&#38388;&#24402;&#19968;&#21270;&#12290;&#19982;&#26368;&#26032;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#23450;&#37327;&#32467;&#26524;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#25512;&#26029;&#26102;&#28789;&#27963;&#25511;&#21046;&#27599;&#20010;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.12148</link><description>&lt;p&gt;
&#31070;&#32463;&#39044;&#22788;&#29702;&#65306;&#19968;&#31181;&#29992;&#20110;&#31471;&#21040;&#31471;&#33041;&#37096;MRI&#39044;&#22788;&#29702;&#30340;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Neural Pre-Processing: A Learning Framework for End-to-end Brain MRI Pre-processing. (arXiv:2303.12148v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12148
&lt;/p&gt;
&lt;p&gt;
NPP&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#22836;&#37096;MRI&#39044;&#22788;&#29702;&#30340;&#25152;&#26377;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;&#21253;&#25324;&#21435;&#38500;&#22836;&#39592;&#12289;&#24378;&#24230;&#24402;&#19968;&#21270;&#21644;&#31354;&#38388;&#24402;&#19968;&#21270;&#12290;&#19982;&#26368;&#26032;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#23450;&#37327;&#32467;&#26524;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#25512;&#26029;&#26102;&#28789;&#27963;&#25511;&#21046;&#27599;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22836;&#37096;MRI&#39044;&#22788;&#29702;&#28041;&#21450;&#23558;&#21407;&#22987;&#22270;&#20687;&#36716;&#25442;&#20026;&#22312;&#26631;&#20934;&#22352;&#26631;&#31354;&#38388;&#20013;&#30340;&#24378;&#24230;&#24402;&#19968;&#21270;&#12289;&#21435;&#38500;&#22836;&#39592;&#30340;&#22823;&#33041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#39044;&#22788;&#29702;&#65288;NPP&#65289;&#30340;&#31471;&#21040;&#31471;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21516;&#26102;&#35299;&#20915;&#25152;&#26377;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;&#35813;&#32593;&#32476;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#19981;&#38656;&#35201;&#21333;&#29420;&#30340;&#23376;&#20219;&#21153;&#30417;&#30563;&#12290;&#30001;&#20110;&#24635;&#20307;&#30446;&#26631;&#39640;&#24230;&#19981;&#23436;&#20840;&#32422;&#26463;&#65292;&#25105;&#20204;&#26126;&#30830;&#20998;&#31163;&#20102;&#20445;&#25345;&#20960;&#20309;&#24418;&#29366;&#30340;&#24378;&#24230;&#26144;&#23556;&#65288;&#21435;&#38500;&#22836;&#39592;&#21644;&#24378;&#24230;&#24402;&#19968;&#21270;&#65289;&#21644;&#31354;&#38388;&#36716;&#25442;&#65288;&#31354;&#38388;&#24402;&#19968;&#21270;&#65289;&#12290;&#23450;&#37327;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#21482;&#35299;&#20915;&#21333;&#20010;&#23376;&#20219;&#21153;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#28040;&#34701;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#36873;&#25321;NPP&#26550;&#26500;&#35774;&#35745;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;NPP&#20801;&#35768;&#29992;&#25143;&#22312;&#25512;&#26029;&#26102;&#28789;&#27963;&#25511;&#21046;&#27599;&#20010;&#20219;&#21153;&#12290;&#20195;&#30721;&#21644;&#27169;&#22411;&#21487;&#22312;\url{https://github.com/Novestars/Neu}&#19978;&#20813;&#36153;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Head MRI pre-processing involves converting raw images to an intensity-normalized, skull-stripped brain in a standard coordinate space. In this paper, we propose an end-to-end weakly supervised learning approach, called Neural Pre-processing (NPP), for solving all three sub-tasks simultaneously via a neural network, trained on a large dataset without individual sub-task supervision. Because the overall objective is highly under-constrained, we explicitly disentangle geometric-preserving intensity mapping (skull-stripping and intensity normalization) and spatial transformation (spatial normalization). Quantitative results show that our model outperforms state-of-the-art methods which tackle only a single sub-task. Our ablation experiments demonstrate the importance of the architecture design we chose for NPP. Furthermore, NPP affords the user the flexibility to control each of these tasks at inference time. The code and model are freely-available at \url{https://github.com/Novestars/Neu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#21270;&#30340;&#21704;&#23494;&#39039;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#24341;&#36215;&#30340;Hamiltonian&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#27969;&#21487;&#20197;&#36880;&#28176;&#36924;&#36817;&#32039;&#33268;&#22495;&#19978;&#30340;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#65292;&#20026;&#23454;&#38469;&#20351;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2303.12147</link><description>&lt;p&gt;
Hamiltonian&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19975;&#33021;&#36924;&#36817;&#24615;&#36136;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Universal Approximation Property of Hamiltonian Deep Neural Networks. (arXiv:2303.12147v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#21270;&#30340;&#21704;&#23494;&#39039;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#24341;&#36215;&#30340;Hamiltonian&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#36924;&#36817;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#27969;&#21487;&#20197;&#36880;&#28176;&#36924;&#36817;&#32039;&#33268;&#22495;&#19978;&#30340;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#65292;&#20026;&#23454;&#38469;&#20351;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30001;&#31163;&#25955;&#21270;&#30340;&#21704;&#23494;&#39039;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#24341;&#36215;&#30340;Hamiltonian&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;HDNN&#65289;&#30340;&#36890;&#29992;&#36924;&#36817;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;HDNN&#22240;&#35774;&#35745;&#32780;&#20855;&#26377;&#38750;&#28040;&#22833;&#26799;&#24230;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#25968;&#20540;&#31283;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#20960;&#20010;&#24212;&#29992;&#20013;HDNN&#24050;&#32463;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#32570;&#23569;&#37327;&#21270;&#20854;&#34920;&#29616;&#21147;&#30340;&#20840;&#38754;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;HDNN&#30340;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#65292;&#24182;&#35777;&#26126;&#20102;HDNN&#30340;&#19968;&#37096;&#20998;&#27969;&#21487;&#20197;&#36880;&#28176;&#36924;&#36817;&#32039;&#33268;&#22495;&#19978;&#30340;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#12290;&#27492;&#32467;&#26524;&#20026;&#23454;&#38469;&#20351;&#29992;HDNN&#25552;&#20379;&#20102;&#29282;&#22266;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the universal approximation capabilities of Hamiltonian Deep Neural Networks (HDNNs) that arise from the discretization of Hamiltonian Neural Ordinary Differential Equations. Recently, it has been shown that HDNNs enjoy, by design, non-vanishing gradients, which provide numerical stability during training. However, although HDNNs have demonstrated state-of-the-art performance in several applications, a comprehensive study to quantify their expressivity is missing. In this regard, we provide a universal approximation theorem for HDNNs and prove that a portion of the flow of HDNNs can approximate arbitrary well any continuous function over a compact domain. This result provides a solid theoretical foundation for the practical use of HDNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#32416;&#27491;&#21046;&#36896;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#38598;&#25104;&#32435;&#31859;&#20809;&#23376;&#22120;&#20214;&#30340;&#21046;&#36896;&#31934;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#20809;&#23398;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12136</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25552;&#39640;&#38598;&#25104;&#32435;&#31859;&#20809;&#23376;&#22120;&#20214;&#21046;&#36896;&#31934;&#24230;
&lt;/p&gt;
&lt;p&gt;
Improving Fabrication Fidelity of Integrated Nanophotonic Devices Using Deep Learning. (arXiv:2303.12136v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#32416;&#27491;&#21046;&#36896;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#38598;&#25104;&#32435;&#31859;&#20809;&#23376;&#22120;&#20214;&#30340;&#21046;&#36896;&#31934;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#20809;&#23398;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#19968;&#20195;&#38598;&#25104;&#32435;&#31859;&#20809;&#23376;&#22120;&#20214;&#35774;&#35745;&#21033;&#29992;&#39640;&#32423;&#20248;&#21270;&#25216;&#26415;&#65292;&#22914;&#21453;&#21521;&#35774;&#35745;&#21644;&#25299;&#25169;&#20248;&#21270;&#65292;&#22312;&#23567;&#29305;&#24449;&#23610;&#23544;&#30340;&#25903;&#25345;&#19979;&#20248;&#21270;&#22823;&#37327;&#22797;&#26434;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#24615;&#33021;&#21644;&#26497;&#23567;&#21270;&#12290;&#28982;&#32780;&#65292;&#38500;&#38750;&#20248;&#21270;&#21463;&#21040;&#20005;&#26684;&#38480;&#21046;&#65292;&#21542;&#21017;&#29983;&#25104;&#30340;&#23567;&#29305;&#24449;&#26080;&#27861;&#21487;&#38752;&#22320;&#21152;&#24037;&#65292;&#23548;&#33268;&#20809;&#23398;&#24615;&#33021;&#19979;&#38477;&#12290;&#21363;&#20351;&#23545;&#20110;&#26356;&#31616;&#21333;&#30340;&#20256;&#32479;&#35774;&#35745;&#65292;&#21046;&#36896;&#24341;&#36215;&#30340;&#24615;&#33021;&#38477;&#32423;&#20173;&#20250;&#21457;&#29983;&#12290;&#20559;&#24046;&#31243;&#24230;&#19981;&#20165;&#21462;&#20915;&#20110;&#29305;&#24449;&#30340;&#22823;&#23567;&#21644;&#24418;&#29366;&#65292;&#32780;&#19988;&#36824;&#21462;&#20915;&#20110;&#29305;&#24449;&#30340;&#20998;&#24067;&#21644;&#21608;&#22260;&#29615;&#22659;&#65292;&#21576;&#29616;&#20986;&#22797;&#26434;&#30340;&#36817;&#36317;&#31163;&#20381;&#36182;&#34892;&#20026;&#12290;&#22312;&#27809;&#26377;&#19987;&#26377;&#21046;&#36896;&#24037;&#33402;&#35268;&#33539;&#30340;&#24773;&#20917;&#19979;&#65292;&#21482;&#33021;&#22312;&#26657;&#20934;&#21046;&#36896;&#36807;&#31243;&#20043;&#21518;&#36827;&#34892;&#35774;&#35745;&#20462;&#27491;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#32416;&#27491;&#21046;&#36896;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Next-generation integrated nanophotonic device designs leverage advanced optimization techniques such as inverse design and topology optimization which achieve high performance and extreme miniaturization by optimizing a massively complex design space enabled by small feature sizes. However, unless the optimization is heavily constrained, the generated small features are not reliably fabricated, leading to optical performance degradation. Even for simpler, conventional designs, fabrication-induced performance degradation still occurs. The degree of deviation from the original design not only depends on the size and shape of its features, but also on the distribution of features and the surrounding environment, presenting complex, proximity-dependent behavior. Without proprietary fabrication process specifications, design corrections can only be made after calibrating fabrication runs take place. In this work, we introduce a general deep machine learning model that automatically correct
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#25913;&#36827;&#24341;&#36215;&#20102;&#20844;&#20247;&#24191;&#27867;&#20851;&#27880;&#12290;&#23427;&#20204;&#24191;&#27867;&#24212;&#29992;&#21450;&#20854;&#30495;&#23454;&#33021;&#21147;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#20294;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#23545;&#20854;&#21487;&#33021;&#30340;&#24694;&#24847;&#29992;&#36884;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#35201;&#27010;&#36848;&#20197;&#21450;&#22312;&#32593;&#32476;&#38450;&#24481;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.12132</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19982;&#22312;&#32593;&#32476;&#38450;&#24481;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense. (arXiv:2303.12132v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12132
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#25913;&#36827;&#24341;&#36215;&#20102;&#20844;&#20247;&#24191;&#27867;&#20851;&#27880;&#12290;&#23427;&#20204;&#24191;&#27867;&#24212;&#29992;&#21450;&#20854;&#30495;&#23454;&#33021;&#21147;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#20294;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#23545;&#20854;&#21487;&#33021;&#30340;&#24694;&#24847;&#29992;&#36884;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#35201;&#27010;&#36848;&#20197;&#21450;&#22312;&#32593;&#32476;&#38450;&#24481;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;2022&#24180;&#24213;&#21644;2023&#24180;&#21021;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#24341;&#20837;&#20102;&#19982;&#29992;&#25143;&#26399;&#26395;&#30340;AI&#20132;&#20114;&#19968;&#33268;&#30340;&#27169;&#22411;&#65288;&#23545;&#35805;&#27169;&#22411;&#65289;&#12290;&#20154;&#20204;&#20851;&#27880;&#30340;&#28966;&#28857;&#21487;&#20197;&#35828;&#26159;GPT3&#27169;&#22411;&#30340;&#36825;&#31181;&#25913;&#36827;&#8212;&#8212;ChatGPT&#21450;&#20854;&#38543;&#21518;&#19982;&#36741;&#21161;&#21151;&#33021;&#38598;&#25104;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20316;&#20026;Microsoft Bing&#30340;&#25628;&#32034;&#37096;&#20998;&#12290;&#23613;&#31649;&#27492;&#21069;&#24050;&#32463;&#25237;&#20837;&#20102;&#22823;&#37327;&#30740;&#31350;&#36827;&#34892;&#24320;&#21457;&#65292;&#20294;&#23427;&#20204;&#22312;&#21508;&#31181;&#26085;&#24120;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#36866;&#29992;&#24615;&#20173;&#19981;&#28165;&#26970;&#19988;&#29421;&#31364;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#24182;&#19981;&#38656;&#35201;&#25216;&#26415;&#19987;&#19994;&#30693;&#35782;&#65292;&#36825;&#22312;&#30456;&#24403;&#22823;&#30340;&#31243;&#24230;&#19978;&#26159;&#36890;&#36807;&#23545;&#35805;&#24494;&#35843;&#23454;&#29616;&#30340;&#65292;&#25581;&#31034;&#20102;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#23427;&#20204;&#30495;&#23454;&#33021;&#21147;&#30340;&#33539;&#22260;&#12290;&#36825;&#24341;&#36215;&#20102;&#20844;&#20247;&#23545;&#20854;&#28508;&#22312;&#24212;&#29992;&#30340;&#20852;&#22859;&#21644;&#23545;&#20854;&#33021;&#21147;&#21450;&#21487;&#33021;&#30340;&#24694;&#24847;&#29992;&#36884;&#30340;&#25285;&#24551;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#35201;&#27010;&#36848;&#20197;&#21450;&#22312;&#32593;&#32476;&#38450;&#24481;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Language Models gained significant attention in late 2022 / early 2023, notably with the introduction of models refined to act consistently with users' expectations of interactions with AI (conversational models). Arguably the focal point of public attention has been such a refinement of the GPT3 model -- the ChatGPT and its subsequent integration with auxiliary capabilities, including search as part of Microsoft Bing. Despite extensive prior research invested in their development, their performance and applicability to a range of daily tasks remained unclear and niche. However, their wider utilization without a requirement for technical expertise, made in large part possible through conversational fine-tuning, revealed the extent of their true capabilities in a real-world environment. This has garnered both public excitement for their potential applications and concerns about their capabilities and potential malicious uses. This review aims to provide a brief overview of th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#31227;&#21160;&#36793;&#32536;&#32531;&#23384;&#32593;&#32476;&#27969;&#34892;&#24230;&#39044;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#22810;&#20869;&#23481;&#39034;&#24207;&#35831;&#27714;&#27169;&#24335;&#26102;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12097</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;MEC&#32593;&#32476;&#27969;&#34892;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CLSA: Contrastive Learning-based Survival Analysis for Popularity Prediction in MEC Networks. (arXiv:2303.12097v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#31227;&#21160;&#36793;&#32536;&#32531;&#23384;&#32593;&#32476;&#27969;&#34892;&#24230;&#39044;&#27979;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#22810;&#20869;&#23481;&#39034;&#24207;&#35831;&#27714;&#27169;&#24335;&#26102;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#36793;&#32536;&#32531;&#23384;&#65288;MEC&#65289;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#38598;&#25104;&#30340;&#21019;&#26032;&#25216;&#26415;&#20855;&#26377;&#26497;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#29992;&#25143;&#30340;&#24310;&#36831;&#12290;&#28982;&#32780;&#65292;MEC&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#20381;&#36182;&#20110;&#20854;&#33021;&#21147;&#26469;&#39044;&#27979;&#21644;&#21160;&#24577;&#26356;&#26032;&#32531;&#23384;&#33410;&#28857;&#23384;&#20648;&#26368;&#21463;&#27426;&#36814;&#30340;&#20869;&#23481;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#26102;&#38388;&#24207;&#21015;DNN&#27169;&#22411;&#36890;&#36807;&#21516;&#26102;&#23558;&#22810;&#20010;&#20869;&#23481;&#30340;&#39034;&#24207;&#35831;&#27714;&#27169;&#24335;&#36755;&#20837;&#21040;&#32593;&#32476;&#20013;&#26469;&#25429;&#25417;&#21518;&#32773;&#65292;&#20174;&#32780;&#22823;&#22823;&#22686;&#21152;&#20102;&#36755;&#20837;&#26679;&#26412;&#30340;&#22823;&#23567;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;DNN&#27969;&#34892;&#24230;&#39044;&#27979;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile Edge Caching (MEC) integrated with Deep Neural Networks (DNNs) is an innovative technology with significant potential for the future generation of wireless networks, resulting in a considerable reduction in users' latency. The MEC network's effectiveness, however, heavily relies on its capacity to predict and dynamically update the storage of caching nodes with the most popular contents. To be effective, a DNN-based popularity prediction model needs to have the ability to understand the historical request patterns of content, including their temporal and spatial correlations. Existing state-of-the-art time-series DNN models capture the latter by simultaneously inputting the sequential request patterns of multiple contents to the network, considerably increasing the size of the input sample. This motivates us to address this challenge by proposing a DNN-based popularity prediction framework based on the idea of contrasting input samples against each other, designed for the Unmann
&lt;/p&gt;</description></item><item><title>&#22238;&#22797;&#35780;&#35770;&#65292;&#35748;&#20026;&#35780;&#35770;&#32858;&#28966;&#20110;&#19968;&#31181;&#38750;&#20856;&#22411;&#38382;&#39064;&#19988;&#36807;&#20110;&#31616;&#21270;&#65292;&#24378;&#35843;&#20102;&#21407;&#22987;&#24037;&#20316;&#32972;&#21518;&#26356;&#24191;&#27867;&#30340;&#31639;&#27861;&#24320;&#21457;&#20197;&#21450;&#23454;&#39564;&#25968;&#25454;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#25351;&#20986;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;&#35299;&#21078;&#19982;&#36138;&#24515;&#31639;&#27861;&#30340;&#26412;&#36136;&#22823;&#19981;&#30456;&#21516;&#65292;&#22240;&#27492;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.12096</link><description>&lt;p&gt;
&#20851;&#20110;&#12298;&#26080;&#27861;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#32988;&#36807;&#36138;&#24515;&#31639;&#27861;&#12299;&#30340;&#22238;&#22797;(arXiv:2303.12096v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Reply to: Inability of a graph neural network heuristic to outperform greedy algorithms in solving combinatorial optimization problems. (arXiv:2303.12096v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12096
&lt;/p&gt;
&lt;p&gt;
&#22238;&#22797;&#35780;&#35770;&#65292;&#35748;&#20026;&#35780;&#35770;&#32858;&#28966;&#20110;&#19968;&#31181;&#38750;&#20856;&#22411;&#38382;&#39064;&#19988;&#36807;&#20110;&#31616;&#21270;&#65292;&#24378;&#35843;&#20102;&#21407;&#22987;&#24037;&#20316;&#32972;&#21518;&#26356;&#24191;&#27867;&#30340;&#31639;&#27861;&#24320;&#21457;&#20197;&#21450;&#23454;&#39564;&#25968;&#25454;&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#25351;&#20986;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;&#35299;&#21078;&#19982;&#36138;&#24515;&#31639;&#27861;&#30340;&#26412;&#36136;&#22823;&#19981;&#30456;&#21516;&#65292;&#22240;&#27492;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;Stefan Boettcher [arXiv:2210.00623]&#30340;&#35780;&#35770;&#20570;&#20102;&#20840;&#38754;&#22238;&#22797;&#65292;&#24182;&#35748;&#20026;&#36825;&#20010;&#35780;&#35770;&#20165;&#38024;&#23545;&#19968;&#31181;&#38750;&#20856;&#22411;&#30340;&#38382;&#39064;&#65292;&#20165;&#20851;&#27880;&#31232;&#30095;&#22270;&#19978;&#30340;&#26368;&#22823;&#21106;&#38382;&#39064;(MaxCut)&#65292;&#32780;&#36138;&#24515;&#31639;&#27861;&#22312;&#36825;&#31181;&#38382;&#39064;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#25105;&#20204;&#21407;&#22987;&#24037;&#20316;&#32972;&#21518;&#26356;&#24191;&#27867;&#30340;&#31639;&#27861;&#24320;&#21457;&#65292;&#24182;&#22312;&#25105;&#20204;&#21407;&#22987;&#26694;&#26550;&#20869;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#25968;&#20540;&#32467;&#26524;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#21407;&#22987;&#25968;&#25454;&#19978;&#26377;&#26174;&#30528;&#25913;&#36827;&#65292;&#22240;&#27492;&#25512;&#32763;&#20102;&#35780;&#35770;&#30340;&#21407;&#22987;&#24615;&#33021;&#38472;&#36848;&#12290;&#27492;&#22806;&#65292;&#24050;&#32463;&#35777;&#26126;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(PI-GNN)&#33021;&#22815;&#32988;&#36807;&#36138;&#24515;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#22256;&#38590;&#30340;&#12289;&#23494;&#38598;&#30340;&#23454;&#20363;&#19978;&#12290;&#25105;&#20204;&#36824;&#25351;&#20986;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;(&#24182;&#34892;)&#35299;&#21078;&#19982;&#36138;&#24515;&#31639;&#27861;&#30340;(&#39034;&#24207;)&#26412;&#36136;&#26377;&#24456;&#22823;&#19981;&#21516;&#65292;&#22522;&#20110;&#23427;&#20204;&#22312;&#23454;&#38469;&#31038;&#20132;&#32593;&#32476;&#35268;&#27169;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#25351;&#20986;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a comprehensive reply to the comment written by Stefan Boettcher [arXiv:2210.00623] and argue that the comment singles out one particular non-representative example problem, entirely focusing on the maximum cut problem (MaxCut) on sparse graphs, for which greedy algorithms are expected to perform well. Conversely, we highlight the broader algorithmic development underlying our original work, and (within our original framework) provide additional numerical results showing sizable improvements over our original data, thereby refuting the comment's original performance statements. Furthermore, it has already been shown that physics-inspired graph neural networks (PI-GNNs) can outperform greedy algorithms, in particular on hard, dense instances. We also argue that the internal (parallel) anatomy of graph neural networks is very different from the (sequential) nature of greedy algorithms, and (based on their usage at the scale of real-world social networks) point out that graph n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ANEDL&#26694;&#26550;&#65292;&#24212;&#29992;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#37327;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#36866;&#24212;&#24615;&#36127;&#20248;&#21270;&#31574;&#30053;&#65292;&#26377;&#25928;&#24212;&#23545;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20869;&#37096;&#20540;&#21644;&#24322;&#24120;&#20540;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.12091</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#36127;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised Learning. (arXiv:2303.12091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ANEDL&#26694;&#26550;&#65292;&#24212;&#29992;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#37327;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#36866;&#24212;&#24615;&#36127;&#20248;&#21270;&#31574;&#30053;&#65292;&#26377;&#25928;&#24212;&#23545;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20869;&#37096;&#20540;&#21644;&#24322;&#24120;&#20540;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20551;&#35774;&#26631;&#35760;&#25968;&#25454;&#12289;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#21516;&#19968;&#20998;&#24067;&#12290;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#32771;&#34385;&#21040;&#19968;&#20010;&#26356;&#23454;&#38469;&#30340;&#24773;&#20917;&#65292;&#21363;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#21253;&#21547;&#26631;&#35760;&#25968;&#25454;&#20013;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#31867;&#21035;&#65288;&#24322;&#24120;&#20540;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#8212;&#8212;&#36866;&#24212;&#24615;&#36127;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;ANEDL&#65289;&#65292;&#20197;&#24212;&#23545;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#22914;&#32570;&#20047;&#21487;&#25193;&#23637;&#24615;&#21644;&#26080;&#27861;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;EDL&#65289;&#20316;&#20026;&#19968;&#31181;&#24322;&#24120;&#26816;&#27979;&#22120;&#26469;&#37327;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#26041;&#27861;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#21644;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#24212;&#24615;&#36127;&#20248;&#21270;&#31574;&#30053;&#65292;&#20351;EDL&#26356;&#21152;&#36866;&#21512;&#21253;&#21547;&#20869;&#37096;&#20540;&#21644;&#24322;&#24120;&#20540;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;ANEDL&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) methods assume that labeled data, unlabeled data and test data are from the same distribution. Open-set semi-supervised learning (Open-set SSL) considers a more practical scenario, where unlabeled data and test data contain new categories (outliers) not observed in labeled data (inliers). Most previous works focused on outlier detection via binary classifiers, which suffer from insufficient scalability and inability to distinguish different types of uncertainty. In this paper, we propose a novel framework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these limitations. Concretely, we first introduce evidential deep learning (EDL) as an outlier detector to quantify different types of uncertainty, and design different uncertainty metrics for self-training and inference. Furthermore, we propose a novel adaptive negative optimization strategy, making EDL more tailored to the unlabeled dataset containing both inliers and outliers. As demonstrat
&lt;/p&gt;</description></item><item><title>Thrill-K&#26550;&#26500;&#23558;&#31070;&#32463;&#23398;&#20064;&#19982;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20026;&#35299;&#20915;&#37096;&#32626;&#31471;&#21040;&#31471;&#23398;&#20064;&#31995;&#32479;&#25152;&#38656;&#35201;&#30340;&#35745;&#31639;&#38656;&#27714;&#22686;&#21152;&#12289;&#20197;&#21450;&#32570;&#20047;&#28789;&#27963;&#24615;&#12289;&#36866;&#24212;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#25512;&#29702;&#21644;&#39564;&#35777;&#33021;&#21147;&#31561;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.12084</link><description>&lt;p&gt;
Thrill-K&#26550;&#26500;&#65306;&#36808;&#21521;&#22522;&#20110;&#30693;&#35782;&#30340;&#29702;&#35299;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Thrill-K Architecture: Towards a Solution to the Problem of Knowledge Based Understanding. (arXiv:2303.12084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12084
&lt;/p&gt;
&lt;p&gt;
Thrill-K&#26550;&#26500;&#23558;&#31070;&#32463;&#23398;&#20064;&#19982;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20026;&#35299;&#20915;&#37096;&#32626;&#31471;&#21040;&#31471;&#23398;&#20064;&#31995;&#32479;&#25152;&#38656;&#35201;&#30340;&#35745;&#31639;&#38656;&#27714;&#22686;&#21152;&#12289;&#20197;&#21450;&#32570;&#20047;&#28789;&#27963;&#24615;&#12289;&#36866;&#24212;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#25512;&#29702;&#21644;&#39564;&#35777;&#33021;&#21147;&#31561;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#31471;&#21040;&#31471;&#23398;&#20064;&#31995;&#32479;&#27491;&#36805;&#36895;&#33719;&#24471;&#33021;&#21147;&#21644;&#27969;&#34892;&#24230;&#65292;&#20294;&#37096;&#32626;&#36825;&#31181;&#31995;&#32479;&#25152;&#38656;&#30340;&#35745;&#31639;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#21152;&#20043;&#28789;&#27963;&#24615;&#12289;&#36866;&#24212;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#25512;&#29702;&#21644;&#39564;&#35777;&#33021;&#21147;&#30340;&#32570;&#20047;&#65292;&#38656;&#35201;&#26032;&#31867;&#22411;&#30340;&#26550;&#26500;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#28151;&#21512;&#31995;&#32479;&#30340;&#20998;&#31867;&#65292;&#35813;&#20998;&#31867;&#22522;&#20110;&#23545;&#20154;&#31867;&#30693;&#35782;&#21644;&#26234;&#33021;&#30340;&#20998;&#26512;&#65292;&#23558;&#31070;&#32463;&#23398;&#20064;&#19982;&#21508;&#31181;&#31867;&#22411;&#30340;&#30693;&#35782;&#21644;&#30693;&#35782;&#26469;&#28304;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Thrill-K&#26550;&#26500;&#20316;&#20026;&#23558;&#30636;&#26102;&#30693;&#35782;&#12289;&#22791;&#29992;&#30693;&#35782;&#21644;&#22806;&#37096;&#30693;&#35782;&#28304;&#38598;&#25104;&#21040;&#19968;&#20010;&#20855;&#26377;&#25512;&#29702;&#12289;&#23398;&#20064;&#21644;&#26234;&#33021;&#25511;&#21046;&#33021;&#21147;&#30340;&#26694;&#26550;&#20013;&#30340;&#21407;&#22411;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
While end-to-end learning systems are rapidly gaining capabilities and popularity, the increasing computational demands for deploying such systems, along with a lack of flexibility, adaptability, explainability, reasoning and verification capabilities, require new types of architectures. Here we introduce a classification of hybrid systems which, based on an analysis of human knowledge and intelligence, combines neural learning with various types of knowledge and knowledge sources. We present the Thrill-K architecture as a prototypical solution for integrating instantaneous knowledge, standby knowledge and external knowledge sources in a framework capable of inference, learning and intelligent control.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30456;&#37051;&#30340;&#22810;&#20445;&#30495;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#20849;&#20139;&#20302;&#20445;&#30495;&#24230;&#21644;&#39640;&#20445;&#30495;&#24230;&#35299;&#20915;&#26041;&#26696;&#30340;&#29305;&#24449;&#31354;&#38388;&#26469;&#20943;&#23569;&#25110;&#28040;&#38500;&#23545;&#39640;&#31934;&#24230;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#36825;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26102;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.11577</link><description>&lt;p&gt;
&#29305;&#24449;&#30456;&#37051;&#22810;&#20445;&#30495;&#29289;&#29702;&#23398;&#20064;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Feature-adjacent multi-fidelity physics-informed machine learning for partial differential equations. (arXiv:2303.11577v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11577
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#30456;&#37051;&#30340;&#22810;&#20445;&#30495;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#20849;&#20139;&#20302;&#20445;&#30495;&#24230;&#21644;&#39640;&#20445;&#30495;&#24230;&#35299;&#20915;&#26041;&#26696;&#30340;&#29305;&#24449;&#31354;&#38388;&#26469;&#20943;&#23569;&#25110;&#28040;&#38500;&#23545;&#39640;&#31934;&#24230;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#36825;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26102;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#22791;&#36873;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22797;&#26434;&#38382;&#39064;&#65292;&#36825;&#31181;&#32593;&#32476;&#30340;&#35757;&#32451;&#20173;&#28982;&#38656;&#35201;&#39640;&#31934;&#24230;&#30340;&#25968;&#25454;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#30340;&#29983;&#25104;&#25104;&#26412;&#21487;&#33021;&#24456;&#39640;&#12290;&#20026;&#20102;&#20943;&#23569;&#29978;&#33267;&#28040;&#38500;&#23545;&#39640;&#20445;&#30495;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#30340;&#22810;&#20445;&#30495;&#20307;&#31995;&#32467;&#26500;&#65292;&#35813;&#31354;&#38388;&#30001;&#20302;&#20445;&#30495;&#24230;&#21644;&#39640;&#20445;&#30495;&#24230;&#35299;&#20915;&#26041;&#26696;&#20849;&#20139;&#12290;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#20302;&#31934;&#24230;&#21644;&#39640;&#31934;&#24230;&#35299;&#20915;&#26041;&#26696;&#30340;&#25237;&#24433;&#30456;&#37051;&#65292;&#24182;&#36890;&#36807;&#32422;&#26463;&#23427;&#20204;&#30340;&#30456;&#23545;&#36317;&#31163;&#26469;&#23454;&#29616;&#12290;&#29305;&#24449;&#31354;&#38388;&#30001;&#32534;&#30721;&#22120;&#34920;&#31034;&#65292;&#20854;&#26144;&#23556;&#21040;&#21407;&#22987;&#35299;&#31354;&#38388;&#36890;&#36807;&#35299;&#30721;&#22120;&#23454;&#29616;&#12290;&#25152;&#25552;&#20986;&#30340;&#22810;&#20445;&#30495;&#26041;&#27861;&#22312;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#25551;&#36848;&#30340;&#23450;&#24577;&#21644;&#38750;&#23450;&#24577;&#38382;&#39064;&#30340;&#27491;&#38382;&#39064;&#21644;&#36870;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks have emerged as an alternative method for solving partial differential equations. However, for complex problems, the training of such networks can still require high-fidelity data which can be expensive to generate. To reduce or even eliminate the dependency on high-fidelity data, we propose a novel multi-fidelity architecture which is based on a feature space shared by the low- and high-fidelity solutions. In the feature space, the projections of the low-fidelity and high-fidelity solutions are adjacent by constraining their relative distance. The feature space is represented with an encoder and its mapping to the original solution space is effected through a decoder. The proposed multi-fidelity approach is validated on forward and inverse problems for steady and unsteady problems described by partial differential equations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21160;&#24577;&#39030;&#28857;&#26367;&#25442;&#25991;&#27861;&#65288;DyVeRG&#65289;&#65292;&#23427;&#20204;&#25552;&#20379;&#19968;&#31181;&#22312;&#26102;&#38388;&#22495;&#20869;&#26356;&#26032;&#23398;&#20064;&#22270;&#25991;&#27861;&#30340;&#24418;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#39044;&#27979;&#30495;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#22270;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11553</link><description>&lt;p&gt;
&#21160;&#24577;&#39030;&#28857;&#26367;&#25442;&#25991;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dynamic Vertex Replacement Grammars. (arXiv:2303.11553v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21160;&#24577;&#39030;&#28857;&#26367;&#25442;&#25991;&#27861;&#65288;DyVeRG&#65289;&#65292;&#23427;&#20204;&#25552;&#20379;&#19968;&#31181;&#22312;&#26102;&#38388;&#22495;&#20869;&#26356;&#26032;&#23398;&#20064;&#22270;&#25991;&#27861;&#30340;&#24418;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#21644;&#39044;&#27979;&#30495;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#22270;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#26080;&#20851;&#22270;&#25991;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24314;&#27169;&#32467;&#26500;&#30340;&#24778;&#20154;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22270;&#25991;&#27861;&#32570;&#20047;&#25429;&#25417;&#26102;&#21464;&#29616;&#35937;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#20135;&#29983;&#35268;&#21017;&#30340;&#20174;&#24038;&#21040;&#21491;&#30340;&#36716;&#25442;&#19981;&#34920;&#31034;&#26102;&#38388;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#21160;&#24577;&#39030;&#28857;&#26367;&#25442;&#25991;&#27861;&#65288;DyVeRG&#65289;&#65292;&#23427;&#20204;&#22312;&#26102;&#38388;&#22495;&#20013;&#25512;&#24191;&#20102;&#39030;&#28857;&#26367;&#25442;&#25991;&#27861;&#65292;&#36890;&#36807;&#20026;&#23398;&#20064;&#22270;&#25991;&#27861;&#26356;&#26032;&#25552;&#20379;&#24418;&#24335;&#26694;&#26550;&#65292;&#20197;&#31526;&#21512;&#20854;&#22522;&#30784;&#25968;&#25454;&#30340;&#20462;&#25913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DyVeRG&#25991;&#27861;&#21487;&#20197;&#20174;&#30495;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#22270;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#34987;&#29992;&#20110;&#24544;&#23454;&#22320;&#29983;&#25104;&#36825;&#20123;&#22270;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#35745;&#31639;DyVeRG&#26354;&#32447;&#29983;&#25104;&#30340;&#24503;&#27779;&#23572;&#36125;&#26684;&#24046;&#24322;&#20998;&#25968;&#65288;dyvergence scores&#65289;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#36825;&#26159;&#30001;&#35813;&#26694;&#26550;&#24341;&#20986;&#30340;&#19968;&#31181;&#26032;&#30340;&#22270;&#30456;&#20284;&#24230;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context-free graph grammars have shown a remarkable ability to model structures in real-world relational data. However, graph grammars lack the ability to capture time-changing phenomena since the left-to-right transitions of a production rule do not represent temporal change. In the present work, we describe dynamic vertex-replacement grammars (DyVeRG), which generalize vertex replacement grammars in the time domain by providing a formal framework for updating a learned graph grammar in accordance with modifications to its underlying data. We show that DyVeRG grammars can be learned from, and used to generate, real-world dynamic graphs faithfully while remaining human-interpretable. We also demonstrate their ability to forecast by computing dyvergence scores, a novel graph similarity measurement exposed by this framework.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#35757;&#32451;&#22235;&#36275;&#26426;&#22120;&#20154;&#20351;&#29992;&#21069;&#32930;&#25191;&#34892;&#25805;&#32437;&#20219;&#21153;&#65292;&#22914;&#25856;&#29228;&#12289;&#25353;&#19979;&#25353;&#38062;&#21644;&#19982;&#29289;&#20307;&#20132;&#20114;&#65292;&#24182;&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#23558;&#36825;&#20123;&#25216;&#33021;&#20174;&#27169;&#25311;&#29615;&#22659;&#36716;&#31227;&#21040;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#24182;&#33719;&#24471;&#20102;&#25104;&#21151;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.11330</link><description>&lt;p&gt;
&#33151;&#37096;&#20316;&#20026;&#26426;&#26800;&#25163;&#33218;&#65306;&#23558;&#22235;&#36275;&#26426;&#22120;&#20154;&#25935;&#25463;&#24615;&#25512;&#21521;&#36229;&#20986;&#36816;&#21160;&#30340;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Legs as Manipulator: Pushing Quadrupedal Agility Beyond Locomotion. (arXiv:2303.11330v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11330
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#35757;&#32451;&#22235;&#36275;&#26426;&#22120;&#20154;&#20351;&#29992;&#21069;&#32930;&#25191;&#34892;&#25805;&#32437;&#20219;&#21153;&#65292;&#22914;&#25856;&#29228;&#12289;&#25353;&#19979;&#25353;&#38062;&#21644;&#19982;&#29289;&#20307;&#20132;&#20114;&#65292;&#24182;&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#23558;&#36825;&#20123;&#25216;&#33021;&#20174;&#27169;&#25311;&#29615;&#22659;&#36716;&#31227;&#21040;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#24182;&#33719;&#24471;&#20102;&#25104;&#21151;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#34892;&#36208;&#25110;&#22868;&#36305;&#22312;&#21508;&#31181;&#22797;&#26434;&#22320;&#24418;&#19978;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#19982;&#29399;&#31561;&#29983;&#29289;&#30456;&#27604;&#65292;&#26426;&#22120;&#20154;&#22235;&#36275;&#21160;&#29289;&#20173;&#28982;&#36828;&#36828;&#33853;&#21518;&#65292;&#29399;&#33021;&#22815;&#23637;&#31034;&#22810;&#31181;&#25935;&#25463;&#25216;&#33021;&#65292;&#24182;&#33021;&#20351;&#29992;&#33151;&#37096;&#36229;&#20986;&#36816;&#21160;&#30340;&#33539;&#22260;&#65292;&#25191;&#34892;&#20960;&#20010;&#22522;&#26412;&#30340;&#25805;&#32437;&#20219;&#21153;&#65292;&#22914;&#19982;&#29289;&#20307;&#36827;&#34892;&#20132;&#20114;&#21644;&#25856;&#29228;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#22235;&#36275;&#26426;&#22120;&#20154;&#19981;&#20165;&#34892;&#36208;&#65292;&#36824;&#20351;&#29992;&#21069;&#32930;&#25856;&#29228;&#22681;&#22721;&#12289;&#25353;&#19979;&#25353;&#38062;&#12289;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25191;&#34892;&#29289;&#20307;&#20132;&#20114;&#31561;&#20219;&#21153;&#65292;&#26469;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#25216;&#33021;&#24191;&#27867;&#20998;&#20026;&#20004;&#31867;&#65306;&#36816;&#21160;&#65292;&#21253;&#25324;&#20219;&#20309;&#28041;&#21450;&#36816;&#21160;&#30340;&#20107;&#29289;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;&#34892;&#36208;&#36824;&#26159;&#25856;&#29228;&#22681;&#22721;&#65307;&#25805;&#32437;&#65292;&#28041;&#21450;&#20351;&#29992;&#19968;&#26465;&#33151;&#36827;&#34892;&#20132;&#20114;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#20182;&#19977;&#26465;&#33151;&#30340;&#24179;&#34913;&#12290;&#21033;&#29992;&#35838;&#31243;&#23398;&#20064;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#36825;&#20123;&#25216;&#33021;&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340; sim2real &#26041;&#27861;&#23558;&#20854;&#36716;&#31227;&#21040;&#30495;&#23454;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#22235;&#36275;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#20445;&#25345;&#31283;&#23450;&#30340;&#21516;&#26102;&#25191;&#34892;&#22810;&#31181;&#25805;&#32437;&#20219;&#21153;&#65292;&#22914;&#25171;&#24320;&#38376;&#12289;&#25353;&#19979;&#25353;&#38062;&#21644;&#25552;&#36215;&#29289;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
Locomotion has seen dramatic progress for walking or running across challenging terrains. However, robotic quadrupeds are still far behind their biological counterparts, such as dogs, which display a variety of agile skills and can use the legs beyond locomotion to perform several basic manipulation tasks like interacting with objects and climbing. In this paper, we take a step towards bridging this gap by training quadruped robots not only to walk but also to use the front legs to climb walls, press buttons, and perform object interaction in the real world. To handle this challenging optimization, we decouple the skill learning broadly into locomotion, which involves anything that involves movement whether via walking or climbing a wall, and manipulation, which involves using one leg to interact while balancing on the other three legs. These skills are trained in simulation using curriculum and transferred to the real world using our proposed sim2real variant that builds upon recent l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31995;&#32479;Touch Dexterity&#65292;&#36890;&#36807;&#23494;&#38598;&#20108;&#36827;&#21046;&#21147;&#20256;&#24863;&#22120;&#23454;&#29616;&#20102;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;&#29289;&#20307;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#25104;&#26412;&#21644;&#19982;&#23454;&#38469;&#24212;&#29992;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2303.10880</link><description>&lt;p&gt;
&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;: &#36890;&#36807;&#35302;&#35273;&#23454;&#29616;&#25163;&#37096;&#28789;&#27963;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rotating without Seeing: Towards In-hand Dexterity through Touch. (arXiv:2303.10880v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31995;&#32479;Touch Dexterity&#65292;&#36890;&#36807;&#23494;&#38598;&#20108;&#36827;&#21046;&#21147;&#20256;&#24863;&#22120;&#23454;&#29616;&#20102;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;&#29289;&#20307;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#25104;&#26412;&#21644;&#19982;&#23454;&#38469;&#24212;&#29992;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35302;&#24863;&#20449;&#24687;&#22312;&#20154;&#31867;&#28789;&#24039;&#24615;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#26377;&#29992;&#30340;&#25509;&#35302;&#20449;&#24687;&#65292;&#30452;&#25509;&#20174;&#35270;&#35273;&#20013;&#26080;&#27861;&#25512;&#26029;&#12290;&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#26159;&#21542;&#33021;&#22815;&#20351;&#22810;&#25351;&#26426;&#22120;&#20154;&#25163;&#20855;&#22791;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;&#29289;&#20307;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;Touch Dexterity&#65292;&#36890;&#36807;&#20351;&#29992;&#35206;&#30422;&#25972;&#20010;&#26426;&#22120;&#20154;&#25163;&#30340;&#23494;&#38598;&#20108;&#36827;&#21046;&#21147;&#20256;&#24863;&#22120;&#65288;&#35302;&#25720;&#25110;&#26410;&#35302;&#25720;&#65289;&#20195;&#26367;&#20165;&#20165;&#22312;&#23567;&#21306;&#22495;&#20869;&#36827;&#34892;&#31934;&#20934;&#30340;&#35302;&#35273;&#20256;&#24863;&#65292;&#20351;&#31995;&#32479;&#20855;&#26377;&#20302;&#25104;&#26412;&#12289;&#35206;&#30422;&#33539;&#22260;&#24191;&#31561;&#20248;&#28857;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#22810;&#26679;&#30340;&#29289;&#20307;&#27169;&#25311;&#20013;&#35757;&#32451;&#20986;&#20102;&#19968;&#31181;&#35302;&#24863;&#26059;&#36716;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#30495;&#23454;&#30340;&#26426;&#22120;&#20154;&#25163;&#19978;&#30452;&#25509;&#23454;&#26045;&#19981;&#30475;&#23601;&#33021;&#26059;&#36716;&#26032;&#22411;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tactile information plays a critical role in human dexterity. It reveals useful contact information that may not be inferred directly from vision. In fact, humans can even perform in-hand dexterous manipulation without using vision. Can we enable the same ability for the multi-finger robot hand? In this paper, we present Touch Dexterity, a new system that can perform in-hand object rotation using only touching without seeing the object. Instead of relying on precise tactile sensing in a small region, we introduce a new system design using dense binary force sensors (touch or no touch) overlaying one side of the whole robot hand (palm, finger links, fingertips). Such a design is low-cost, giving a larger coverage of the object, and minimizing the Sim2Real gap at the same time. We train an in-hand rotation policy using Reinforcement Learning on diverse objects in simulation. Relying on touch-only sensing, we can directly deploy the policy in a real robot hand and rotate novel objects tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24320;&#21457;&#22810;&#32423;&#21035;&#20379;&#24212;&#38142;&#30340;&#21160;&#24577;&#24211;&#23384;&#35746;&#36141;&#31574;&#30053;&#65292;&#22312;&#19977;&#32423;&#20379;&#24212;&#38142;&#20223;&#30495;&#27979;&#35797;&#20013;&#35777;&#26126;&#20102;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21516;&#26102;&#20855;&#22791;&#20256;&#32479;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.10382</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#22312;&#24211;&#23384;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Interpretable Reinforcement Learning via Neural Additive Models for Inventory Management. (arXiv:2303.10382v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24320;&#21457;&#22810;&#32423;&#21035;&#20379;&#24212;&#38142;&#30340;&#21160;&#24577;&#24211;&#23384;&#35746;&#36141;&#31574;&#30053;&#65292;&#22312;&#19977;&#32423;&#20379;&#24212;&#38142;&#20223;&#30495;&#27979;&#35797;&#20013;&#35777;&#26126;&#20102;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21516;&#26102;&#20855;&#22791;&#20256;&#32479;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30123;&#24773;&#24432;&#26174;&#20102;&#20379;&#24212;&#38142;&#30340;&#37325;&#35201;&#24615;&#21644;&#25968;&#23383;&#21270;&#31649;&#29702;&#22312;&#24212;&#23545;&#29615;&#22659;&#30340;&#21160;&#24577;&#21464;&#21270;&#20013;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#20026;&#22810;&#32423;&#21035;&#21363;&#22810;&#38454;&#27573;&#30340;&#20379;&#24212;&#38142;&#24320;&#21457;&#21160;&#24577;&#24211;&#23384;&#35746;&#36141;&#31574;&#30053;&#12290;&#20256;&#32479;&#30340;&#24211;&#23384;&#20248;&#21270;&#26041;&#27861;&#26088;&#22312;&#30830;&#23450;&#38745;&#24577;&#35746;&#36141;&#31574;&#30053;&#65292;&#36825;&#20123;&#31574;&#30053;&#19981;&#33021;&#36866;&#24212;&#22914;COVID-19&#21361;&#26426;&#20013;&#35266;&#23519;&#21040;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#31574;&#30053;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#20248;&#21183;&#65292;&#36825;&#26159;&#20379;&#24212;&#38142;&#31649;&#29702;&#32773;&#27807;&#36890;&#20915;&#31574;&#19982;&#30456;&#20851;&#26041;&#38656;&#35201;&#20855;&#22791;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#24615;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26082;&#20855;&#26377;&#20256;&#32479;&#38745;&#24577;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21448;&#20855;&#26377;&#20854;&#20182;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#21644;&#29615;&#22659;&#26080;&#20851;&#24615;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#31070;&#32463;&#21152;&#24615;&#27169;&#22411;&#20316;&#20026;&#24211;&#23384;&#35746;&#36141;&#31574;&#30053;&#30340;&#35299;&#37322;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#32423;&#20379;&#24212;&#38142;&#20223;&#30495;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#19982;&#20256;&#32479;&#24211;&#23384;&#31574;&#30053;&#20197;&#21450;&#20854;&#20182;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#31574;&#30053;&#65292;&#24182;&#22312;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has highlighted the importance of supply chains and the role of digital management to react to dynamic changes in the environment. In this work, we focus on developing dynamic inventory ordering policies for a multi-echelon, i.e. multi-stage, supply chain. Traditional inventory optimization methods aim to determine a static reordering policy. Thus, these policies are not able to adjust to dynamic changes such as those observed during the COVID-19 crisis. On the other hand, conventional strategies offer the advantage of being interpretable, which is a crucial feature for supply chain managers in order to communicate decisions to their stakeholders. To address this limitation, we propose an interpretable reinforcement learning approach that aims to be as interpretable as the traditional static policies while being as flexible and environment-agnostic as other deep learning-based reinforcement learning solutions. We propose to use Neural Additive Models as an interpr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;FES&#24247;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#35814;&#32454;&#30340;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#23547;&#25214;&#24490;&#29615;&#21050;&#28608;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#30340;&#33258;&#34892;&#36710;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#35813;&#26041;&#27861;&#21487;&#30001;&#38750;&#25216;&#26415;&#20154;&#21592;&#20351;&#29992;&#32780;&#26080;&#38656;&#39069;&#22806;&#30828;&#20214;&#25110;&#20256;&#24863;&#22120;&#12290;</title><link>http://arxiv.org/abs/2303.09986</link><description>&lt;p&gt;
&#23454;&#29616;AI&#25511;&#21046;&#30340;FES&#36816;&#21160;&#24247;&#22797;&#65306;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#24490;&#29615;&#21050;&#28608;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Towards AI-controlled FES-restoration of movements: Learning cycling stimulation pattern with reinforcement learning. (arXiv:2303.09986v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;FES&#24247;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#35814;&#32454;&#30340;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#23547;&#25214;&#24490;&#29615;&#21050;&#28608;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#30340;&#33258;&#34892;&#36710;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#35813;&#26041;&#27861;&#21487;&#30001;&#38750;&#25216;&#26415;&#20154;&#21592;&#20351;&#29992;&#32780;&#26080;&#38656;&#39069;&#22806;&#30828;&#20214;&#25110;&#20256;&#24863;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#24615;&#30005;&#21050;&#28608;&#65288;FES&#65289;&#24050;&#32463;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#20854;&#20182;&#24247;&#22797;&#35774;&#22791;&#65288;&#21253;&#25324;&#26426;&#22120;&#20154;&#65289;&#38598;&#25104;&#22312;&#19968;&#36215;&#12290; FES&#24490;&#29615;&#26159;&#24247;&#22797;&#27835;&#30103;&#20013;&#24120;&#29992;&#30340;FES&#24212;&#29992;&#20043;&#19968;&#65292;&#23427;&#36890;&#36807;&#21050;&#28608;&#33151;&#37096;&#32908;&#32905;&#20197;&#29305;&#23450;&#27169;&#24335;&#36827;&#34892;&#12290; &#36866;&#24403;&#30340;&#27169;&#24335;&#22240;&#20154;&#32780;&#24322;&#65292;&#38656;&#35201;&#25163;&#21160;&#35843;&#25972;&#65292;&#36825;&#21487;&#33021;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#24182;&#23545;&#20010;&#20307;&#29992;&#25143;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#23547;&#25214;&#24490;&#29615;&#21050;&#28608;&#27169;&#24335;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#30828;&#20214;&#25110;&#20256;&#24863;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65292;&#39318;&#20808;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#35814;&#32454;&#30340;&#32908;&#32905;&#39592;&#39612;&#27169;&#22411;&#25214;&#21040;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#24335;&#65292;&#20351;&#29992;&#24320;&#28304;&#36719;&#20214;&#26500;&#24314;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#25105;&#20204;&#30340;&#33258;&#21160;&#33050;&#26412;&#36827;&#34892;&#23450;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#30001;&#38750;&#25216;&#26415;&#20154;&#21592;&#20351;&#29992;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#36153;&#29992;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#30495;&#23454;&#30340;&#33258;&#34892;&#36710;&#25968;&#25454;&#23545;&#27169;&#24335;&#36827;&#34892;&#24494;&#35843;&#12290; &#25105;&#20204;&#22312;&#38745;&#27490;&#19977;&#36718;&#36710;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27169;&#25311;&#27979;&#35797;&#21644;&#23454;&#39564;&#27979;&#35797;&#12290;&#22312;&#27169;&#25311;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
Functional electrical stimulation (FES) has been increasingly integrated with other rehabilitation devices, including robots. FES cycling is one of the common FES applications in rehabilitation, which is performed by stimulating leg muscles in a certain pattern. The appropriate pattern varies across individuals and requires manual tuning which can be time-consuming and challenging for the individual user. Here, we present an AI-based method for finding the patterns, which requires no extra hardware or sensors. Our method has two phases, starting with finding model-based patterns using reinforcement learning and detailed musculoskeletal models. The models, built using open-source software, can be customised through our automated script and can be therefore used by non-technical individuals without extra cost. Next, our method fine-tunes the pattern using real cycling data. We test our both in simulation and experimentally on a stationary tricycle. In the simulation test, our method can 
&lt;/p&gt;</description></item><item><title>MedNeXt&#26159;&#19968;&#20010;&#23450;&#21046;&#21270;&#30340;&#29616;&#20195;&#21270;&#21487;&#25193;&#23637;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#30340;&#21307;&#23398;&#29615;&#22659;&#25361;&#25112;&#12290;&#35813;&#32593;&#32476;&#21253;&#21547;&#65306;&#23436;&#20840;ConvNeXt 3D&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#12289;&#27531;&#24046;ConvNeXt&#19978;&#19979;&#37319;&#26679;&#22359;&#21644;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#22686;&#21152;&#26680;&#22823;&#23567;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2303.09975</link><description>&lt;p&gt;
MedNeXt&#65306;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#21464;&#21387;&#22120;&#39537;&#21160;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image Segmentation. (arXiv:2303.09975v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09975
&lt;/p&gt;
&lt;p&gt;
MedNeXt&#26159;&#19968;&#20010;&#23450;&#21046;&#21270;&#30340;&#29616;&#20195;&#21270;&#21487;&#25193;&#23637;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#30340;&#21307;&#23398;&#29615;&#22659;&#25361;&#25112;&#12290;&#35813;&#32593;&#32476;&#21253;&#21547;&#65306;&#23436;&#20840;ConvNeXt 3D&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#12289;&#27531;&#24046;ConvNeXt&#19978;&#19979;&#37319;&#26679;&#22359;&#21644;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#22686;&#21152;&#26680;&#22823;&#23567;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#20351;&#29992;&#22522;&#20110; Transformer &#30340;&#26550;&#26500;&#36234;&#26469;&#36234;&#22810;&#65292;&#20294;&#26159;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#26631;&#27880;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#20351;&#24471;&#20854;&#24615;&#33021;&#36828;&#19981;&#22914;&#33258;&#28982;&#22270;&#20687;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#39640;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#22240;&#27492;&#26356;&#23481;&#26131;&#35757;&#32451;&#21040;&#39640;&#24615;&#33021;&#27700;&#24179;&#12290;&#26368;&#36817;&#65292;ConvNeXt &#26550;&#26500;&#23581;&#35797;&#36890;&#36807;&#38236;&#20687;&#21464;&#21387;&#22120;&#22359;&#26469;&#29616;&#20195;&#21270;&#26631;&#20934;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#36825;&#19968;&#26550;&#26500;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#29616;&#20195;&#21270;&#19988;&#21487;&#25193;&#23637;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#24212;&#23545;&#25968;&#25454;&#31232;&#32570;&#30340;&#21307;&#23398;&#29615;&#22659;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837; MedNeXt&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#21464;&#21387;&#22120;&#21551;&#21457;&#30340;&#22823;&#26680;&#20998;&#21106;&#32593;&#32476;&#65292;&#20854;&#20013;&#21253;&#25324;&#65306;1&#65289;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#23436;&#20840; ConvNeXt 3D &#32534;&#30721;&#22120; - &#35299;&#30721;&#22120;&#32593;&#32476;&#65292;2&#65289;&#27531;&#24046; ConvNeXt &#19978;&#19979;&#37319;&#26679;&#22359;&#65292;&#20197;&#22312;&#21508;&#20010;&#23610;&#24230;&#19978;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#65292;3&#65289;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#19978;&#37319;&#26679;&#23567;&#26680;&#26469;&#36845;&#20195;&#22686;&#21152;&#26680;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been exploding interest in embracing Transformer-based architectures for medical image segmentation. However, the lack of large-scale annotated medical datasets make achieving performances equivalent to those in natural images challenging. Convolutional networks, in contrast, have higher inductive biases and consequently, are easily trainable to high performance. Recently, the ConvNeXt architecture attempted to modernize the standard ConvNet by mirroring Transformer blocks. In this work, we improve upon this to design a modernized and scalable convolutional architecture customized to challenges of data-scarce medical settings. We introduce MedNeXt, a Transformer-inspired large kernel segmentation network which introduces - 1) A fully ConvNeXt 3D Encoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up and downsampling blocks to preserve semantic richness across scales, 3) A novel technique to iteratively increase kernel sizes by upsampling small kernel 
&lt;/p&gt;</description></item><item><title>SemDeDup&#26159;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#26469;&#35782;&#21035;&#21644;&#21024;&#38500;&#35821;&#20041;&#37325;&#22797;&#39033;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;LAION&#30340;&#23376;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;SemDeDup&#21487;&#20197;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;&#30340;&#21516;&#26102;&#21024;&#38500;50%&#30340;&#25968;&#25454;&#65292;&#23454;&#38469;&#19978;&#23558;&#35757;&#32451;&#26102;&#38388;&#20943;&#21322;&#12290;&#27492;&#22806;&#65292;SemDeDup&#22312;&#25552;&#20379;&#25928;&#29575;&#25910;&#30410;&#30340;&#21516;&#26102;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09540</link><description>&lt;p&gt;
SemDeDup:&#36890;&#36807;&#35821;&#20041;&#21435;&#37325;&#23454;&#29616;&#32593;&#32476;&#35268;&#27169;&#25968;&#25454;&#30340;&#39640;&#25928;&#23398;&#20064;&#65288;arXiv:2303.09540v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
SemDeDup: Data-efficient learning at web-scale through semantic deduplication. (arXiv:2303.09540v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09540
&lt;/p&gt;
&lt;p&gt;
SemDeDup&#26159;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#26469;&#35782;&#21035;&#21644;&#21024;&#38500;&#35821;&#20041;&#37325;&#22797;&#39033;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;LAION&#30340;&#23376;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;SemDeDup&#21487;&#20197;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;&#30340;&#21516;&#26102;&#21024;&#38500;50%&#30340;&#25968;&#25454;&#65292;&#23454;&#38469;&#19978;&#23558;&#35757;&#32451;&#26102;&#38388;&#20943;&#21322;&#12290;&#27492;&#22806;&#65292;SemDeDup&#22312;&#25552;&#20379;&#25928;&#29575;&#25910;&#30410;&#30340;&#21516;&#26102;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#36827;&#23637;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#30001;&#28023;&#37327;&#25968;&#25454;&#30340;&#22686;&#21152;&#25512;&#21160;&#30340;&#12290;&#28982;&#32780;&#65292;&#20687;LAION&#36825;&#26679;&#30340;&#22823;&#22411;&#32593;&#32476;&#35268;&#27169;&#25968;&#25454;&#38598;&#22312;&#38500;&#26597;&#25214;&#31934;&#30830;&#37325;&#22797;&#39033;&#22806;&#65292;&#22823;&#37096;&#20998;&#26410;&#32463;&#31934;&#24515;&#31579;&#36873;&#65292;&#21487;&#33021;&#23384;&#22312;&#24456;&#22810;&#20887;&#20313;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;SemDeDup&#65292;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#26469;&#35782;&#21035;&#21644;&#21024;&#38500;&#35821;&#20041;&#37325;&#22797;&#39033;&#30340;&#26041;&#27861;&#65306;&#21363;&#35821;&#20041;&#19978;&#30456;&#20284;&#20294;&#24182;&#38750;&#23436;&#20840;&#30456;&#21516;&#30340;&#25968;&#25454;&#23545;&#12290;&#21435;&#38500;&#35821;&#20041;&#37325;&#22797;&#39033;&#21487;&#20197;&#20445;&#25345;&#24615;&#33021;&#24182;&#21152;&#36895;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;LAION&#30340;&#23376;&#38598;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SemDeDup&#21487;&#20197;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;&#30340;&#21516;&#26102;&#21024;&#38500;50%&#30340;&#25968;&#25454;&#65292;&#23454;&#38469;&#19978;&#23558;&#35757;&#32451;&#26102;&#38388;&#20943;&#21322;&#12290;&#27492;&#22806;&#65292;&#24615;&#33021;&#22312;&#20998;&#24067;&#20197;&#22806;&#24471;&#21040;&#25552;&#39640;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20998;&#26512;&#22312;&#37096;&#20998;&#31579;&#36873;&#36807;&#30340;&#25968;&#25454;&#38598;C4&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SemDeDup&#22312;&#25552;&#20379;&#25928;&#29575;&#25910;&#30410;&#30340;&#21516;&#26102;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;SemDeDup&#25552;&#20379;&#20102;&#19968;&#20010;&#21033;&#29992;&#36136;&#37327;&#23884;&#20837;&#31616;&#21333;&#26041;&#27861;&#26469;&#20351;&#27169;&#22411;&#26356;&#24555;&#22320;&#23398;&#20064;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Progress in machine learning has been driven in large part by massive increases in data. However, large web-scale datasets such as LAION are largely uncurated beyond searches for exact duplicates, potentially leaving much redundancy. Here, we introduce SemDeDup, a method which leverages embeddings from pre-trained models to identify and remove semantic duplicates: data pairs which are semantically similar, but not exactly identical. Removing semantic duplicates preserves performance and speeds up learning. Analyzing a subset of LAION, we show that SemDeDup can remove 50% of the data with minimal performance loss, effectively halving training time. Moreover, performance increases out of distribution. Also, analyzing language models trained on C4, a partially curated dataset, we show that SemDeDup improves over prior approaches while providing efficiency gains. SemDeDup provides an example of how simple ways of leveraging quality embeddings can be used to make models learn faster with le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#35299;&#20915;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26102;&#31354;&#32858;&#21512;&#35299;&#20915;&#20102;&#30001;&#20110;&#32593;&#32476;&#35268;&#27169;&#22823;&#12289;&#33410;&#28857;&#29305;&#24449;&#24322;&#26500;&#31561;&#21407;&#22240;&#32780;&#23548;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#29616;&#26377;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2303.08996</link><description>&lt;p&gt;
&#23398;&#20064;&#22823;&#35268;&#27169;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#30340;&#26102;&#31354;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Learning Spatio-Temporal Aggregations for Large-Scale Capacity Expansion Problems. (arXiv:2303.08996v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#35299;&#20915;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26102;&#31354;&#32858;&#21512;&#35299;&#20915;&#20102;&#30001;&#20110;&#32593;&#32476;&#35268;&#27169;&#22823;&#12289;&#33410;&#28857;&#29305;&#24449;&#24322;&#26500;&#31561;&#21407;&#22240;&#32780;&#23548;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#29616;&#26377;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#25237;&#36164;&#35268;&#21010;&#20915;&#31574;&#23545;&#20110;&#30830;&#20445;&#32593;&#32476;&#29289;&#29702;&#22522;&#30784;&#35774;&#26045;&#22312;&#25193;&#23637;&#26399;&#20869;&#28385;&#36275;&#24615;&#33021;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;&#35745;&#31639;&#36825;&#20123;&#20915;&#31574;&#36890;&#24120;&#38656;&#35201;&#35299;&#20915;&#23481;&#37327;&#25193;&#23637;&#38382;&#39064;&#12290;&#22312;&#21306;&#22495;&#35268;&#27169;&#30340;&#33021;&#28304;&#31995;&#32479;&#20013;&#65292;&#30001;&#20110;&#32593;&#32476;&#35268;&#27169;&#22823;&#12289;&#33410;&#28857;&#29305;&#24449;&#24322;&#26500;&#12289;&#25805;&#20316;&#21608;&#26399;&#20247;&#22810;&#31561;&#21407;&#22240;&#65292;&#36825;&#20123;&#38382;&#39064;&#24448;&#24448;&#38590;&#20197;&#35299;&#20915;&#12290;&#20026;&#20102;&#20445;&#25345;&#21487;&#34892;&#24615;&#65292;&#20256;&#32479;&#26041;&#27861;&#20250;&#32858;&#21512;&#32593;&#32476;&#33410;&#28857;&#21644;/&#25110;&#36873;&#25321;&#19968;&#32452;&#20195;&#34920;&#24615;&#26102;&#27573;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31616;&#21270;&#26410;&#33021;&#25429;&#25417;&#21040;&#20379;&#38656;&#21464;&#21270;&#23545; CEP &#25104;&#26412;&#21644;&#32422;&#26463;&#30340;&#37325;&#35201;&#24433;&#21709;&#65292;&#23548;&#33268;&#27425;&#20248;&#20915;&#31574;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#21040;&#24322;&#26500;&#33410;&#28857;&#30340;&#27867;&#22411; CEP &#30340;&#26102;&#31354;&#32858;&#21512;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#21033;&#29992;&#22270;&#27744;&#21270;&#26469;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#33410;&#28857;&#65292;&#24182;&#26368;&#23567;&#21270;&#19968;&#20010;&#22810;&#30446;&#26631;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#21253;&#25324;&#25104;&#26412;&#21644;&#24615;&#33021;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#30005;&#32593;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#21644;&#29616;&#26377;&#22522;&#20934;&#12290;&#22240;&#27492;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;&#33021;&#28304;&#22522;&#30784;&#35774;&#26045;&#30340;&#23481;&#37327;&#25193;&#23637;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective investment planning decisions are crucial to ensure cyber-physical infrastructures satisfy performance requirements over an extended time horizon. Computing these decisions often requires solving Capacity Expansion Problems (CEPs). In the context of regional-scale energy systems, these problems are prohibitively expensive to solve due to large network sizes, heterogeneous node characteristics, and a large number of operational periods. To maintain tractability, traditional approaches aggregate network nodes and/or select a set of representative time periods. Often, these reductions do not capture supply-demand variations that crucially impact CEP costs and constraints, leading to suboptimal decisions. Here, we propose a novel graph convolutional autoencoder approach for spatio-temporal aggregation of a generic CEP with heterogeneous nodes (CEPHN). Our architecture leverages graph pooling to identify nodes with similar characteristics and minimizes a multi-objective loss funct
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WDiscOOD&#30340;&#26032;&#22411;OOD&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#21028;&#21035;&#23376;&#31354;&#38388;&#21644;&#27531;&#30041;&#23376;&#31354;&#38388;&#20013;&#65292;&#30830;&#23450;OOD&#20998;&#25968;&#12290;&#22312;&#22823;&#35268;&#27169;ImageNet-1k&#22522;&#20934;&#27979;&#35797;&#21644;&#20845;&#20010;OOD&#25968;&#25454;&#38598;&#20013;&#65292;WDiscOOD&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07543</link><description>&lt;p&gt;
WDiscOOD&#65306;&#36890;&#36807;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#36827;&#34892;&#21306;&#20998;&#24230;&#20248;&#21270;&#30340;OOD&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminative Analysis. (arXiv:2303.07543v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WDiscOOD&#30340;&#26032;&#22411;OOD&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#21028;&#21035;&#23376;&#31354;&#38388;&#21644;&#27531;&#30041;&#23376;&#31354;&#38388;&#20013;&#65292;&#30830;&#23450;OOD&#20998;&#25968;&#12290;&#22312;&#22823;&#35268;&#27169;ImageNet-1k&#22522;&#20934;&#27979;&#35797;&#21644;&#20845;&#20010;OOD&#25968;&#25454;&#38598;&#20013;&#65292;WDiscOOD&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#22312;&#36935;&#21040;&#26410;&#30693;&#27010;&#24565;&#30340;&#24773;&#24418;&#19979;&#20135;&#29983;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#36825;&#20010;&#25361;&#25112;&#31361;&#26174;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#26816;&#27979;OOD&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#31354;&#38388;OOD&#26816;&#27979;&#20998;&#25968;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#31867;&#21035;&#29305;&#23450;&#21644;&#31867;&#21035;&#19981;&#21487;&#30693;&#30340;&#20449;&#24687;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#30333;&#21270;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#23558;&#29305;&#24449;&#25237;&#24433;&#21040;&#20004;&#20010;&#23376;&#31354;&#38388;&#20013;&#8212;&#8212;&#21028;&#21035;&#23376;&#31354;&#38388;&#21644;&#27531;&#30041;&#23376;&#31354;&#38388;&#65292;&#20854;&#20013;ID&#31867;&#22312;&#21028;&#21035;&#23376;&#31354;&#38388;&#20013;&#34987;&#26368;&#22823;&#21270;&#22320;&#20998;&#31163;&#65292;&#24182;&#22312;&#27531;&#24046;&#23376;&#31354;&#38388;&#20013;&#34987;&#32039;&#23494;&#22320;&#32858;&#31867;&#12290;&#28982;&#21518;&#65292;&#22312;&#20004;&#20010;&#23376;&#31354;&#38388;&#20013;&#23558;&#26469;&#33258;&#36755;&#20837;&#25968;&#25454;&#19982;ID&#20998;&#24067;&#30340;&#20559;&#24046;&#32452;&#21512;&#36215;&#26469;&#30830;&#23450;OOD&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;WDiscOOD&#65292;&#22312;&#35206;&#30422;&#22810;&#31181;&#20998;&#24067;&#20559;&#31227;&#30340;&#20845;&#20010;OOD&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#39640;&#25928;&#24615;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;ImageNet-1k&#22522;&#20934;&#27979;&#35797;&#12290;WDiscOOD&#22312;&#28145;&#24230;&#20998;&#31867;&#22120;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are susceptible to generating overconfident yet erroneous predictions when presented with data beyond known concepts. This challenge underscores the importance of detecting out-of-distribution (OOD) samples in the open world. In this work, we propose a novel feature-space OOD detection score that jointly reasons with both class-specific and class-agnostic information. Specifically, our approach utilizes Whitened Linear Discriminative Analysis to project features into two subspaces - the discriminative and residual subspaces - in which the ID classes are maximally separated and closely clustered, respectively. The OOD score is then determined by combining the deviation from the input data to the ID distribution in both subspaces. The efficacy of our method, named WDiscOOD, is verified on the large-scale ImageNet-1k benchmark, with six OOD datasets that covers a variety of distribution shifts. WDiscOOD demonstrates superior performance on deep classifiers with divers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.00848</link><description>&lt;p&gt;
&#20197;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#29702;&#35299;&#25193;&#25955;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Understanding the Diffusion Objective as a Weighted Integral of ELBOs. (arXiv:2303.00848v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#29486;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#29992;&#19981;&#21516;&#30340;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#19988;&#36825;&#20123;&#30446;&#26631;&#37117;&#26159;&#21152;&#26435;&#25439;&#22833;&#30340;&#29305;&#20363;&#65292;&#20854;&#20013;&#21152;&#26435;&#20989;&#25968;&#25351;&#23450;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#30340;&#26435;&#37325;&#12290;&#22343;&#21248;&#21152;&#26435;&#23545;&#24212;&#20110;&#26368;&#22823;&#20284;&#28982;&#30340;&#21407;&#21017;&#24615;&#36817;&#20284;ELBO&#30340;&#26368;&#22823;&#21270;&#12290;&#20294;&#26159;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#26356;&#22909;&#30340;&#26679;&#26412;&#36136;&#37327;&#65292;&#30446;&#21069;&#30340;&#25193;&#25955;&#27169;&#22411;&#20351;&#29992;&#38750;&#22343;&#21248;&#21152;&#26435;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#65288;&#24102;&#26377;&#20219;&#20309;&#21152;&#26435;&#65289;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21487;&#20197;&#34987;&#20889;&#25104;&#19968;&#31181;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#24418;&#24335;&#65292;&#20854;&#20013;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#37117;&#26377;&#19968;&#20010;ELBO&#12290;&#22914;&#26524;&#26435;&#37325;&#20989;&#25968;&#26159;&#21333;&#35843;&#30340;&#65292;&#37027;&#20040;&#21152;&#26435;&#25439;&#22833;&#26159;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#30446;&#26631;&#65306;&#23427;&#22312;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#19979;&#65288;&#21363;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#65289;&#19979;&#26368;&#22823;&#21270;ELBO&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#20294;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#20123;&#27604;&#36739;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#26435;&#37325;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models in the literature are optimized with various objectives that are special cases of a weighted loss, where the weighting function specifies the weight per noise level. Uniform weighting corresponds to maximizing the ELBO, a principled approximation of maximum likelihood. In current practice diffusion models are optimized with non-uniform weighting due to better results in terms of sample quality. In this work we expose a direct relationship between the weighted loss (with any weighting) and the ELBO objective.  We show that the weighted loss can be written as a weighted integral of ELBOs, with one ELBO per noise level. If the weighting function is monotonic, then the weighted loss is a likelihood-based objective: it maximizes the ELBO under simple data augmentation, namely Gaussian noise perturbation. Our main contribution is a deeper theoretical understanding of the diffusion objective, but we also performed some experiments comparing monotonic with non-monotonic weight
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#32534;&#30721;&#22120;&#30340;&#26597;&#35810;-FAQ&#21305;&#37197;&#27169;&#22411;&#65292;&#31216;&#20026;MFBE&#65292;&#21033;&#29992;FAQ&#30340;&#22810;&#20010;&#39046;&#22495;&#32452;&#21512;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#33719;&#30410;&#65292;&#35299;&#20915;&#20102;&#35832;&#22914;&#22266;&#26377;&#35789;&#27719;&#24046;&#36317;&#12289;FAQ&#26631;&#39064;&#20013;&#32570;&#20047;&#36275;&#22815;&#30340;&#19978;&#19979;&#25991;&#31561;&#38382;&#39064;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.11953</link><description>&lt;p&gt;
MFBE&#65306;&#21033;&#29992;FAQ&#30340;&#22810;&#39046;&#22495;&#20449;&#24687;&#36827;&#34892;&#39640;&#25928;&#23494;&#38598;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
MFBE: Leveraging Multi-Field Information of FAQs for Efficient Dense Retrieval. (arXiv:2302.11953v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#32534;&#30721;&#22120;&#30340;&#26597;&#35810;-FAQ&#21305;&#37197;&#27169;&#22411;&#65292;&#31216;&#20026;MFBE&#65292;&#21033;&#29992;FAQ&#30340;&#22810;&#20010;&#39046;&#22495;&#32452;&#21512;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#33719;&#30410;&#65292;&#35299;&#20915;&#20102;&#35832;&#22914;&#22266;&#26377;&#35789;&#27719;&#24046;&#36317;&#12289;FAQ&#26631;&#39064;&#20013;&#32570;&#20047;&#36275;&#22815;&#30340;&#19978;&#19979;&#25991;&#31561;&#38382;&#39064;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#38382;&#31572;&#39046;&#22495;&#20013;&#65292;&#39057;&#32321;&#35810;&#38382;&#38382;&#39064;&#65288;FAQ&#65289;&#30340;&#26816;&#32034;&#26159;&#19968;&#20010;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#37325;&#35201;&#23376;&#39046;&#22495;&#12290;&#36825;&#37324;&#65292;&#22312;&#22238;&#24212;&#29992;&#25143;&#26597;&#35810;&#26102;&#65292;&#26816;&#32034;&#31995;&#32479;&#36890;&#24120;&#20250;&#20174;&#30693;&#35782;&#24211;&#36820;&#22238;&#30456;&#20851;&#30340;FAQ&#12290;&#36825;&#31181;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#20854;&#22312;&#23454;&#26102;&#24314;&#31435;&#26597;&#35810;&#21644;FAQ&#20043;&#38388;&#30340;&#35821;&#20041;&#21305;&#37197;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#26597;&#35810;&#21644;FAQ&#20043;&#38388;&#30340;&#22266;&#26377;&#35789;&#27719;&#24046;&#36317;&#65292;FAQ&#26631;&#39064;&#20013;&#32570;&#20047;&#36275;&#22815;&#30340;&#19978;&#19979;&#25991;&#65292;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#21644;&#39640;&#26816;&#32034;&#24310;&#36831;&#65292;&#36825;&#39033;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21452;&#32534;&#30721;&#22120;&#30340;&#26597;&#35810;-FAQ&#21305;&#37197;&#27169;&#22411;&#65292;&#23427;&#22312;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#21033;&#29992;FAQ&#30340;&#22810;&#20010;&#39046;&#22495;&#32452;&#21512;&#65288;&#22914;&#38382;&#39064;&#12289;&#31572;&#26696;&#21644;&#31867;&#21035;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22810;&#39046;&#22495;&#21452;&#32534;&#30721;&#22120;&#65288;MFBE&#65289;&#27169;&#22411;&#20174;&#22810;&#20010;FAQ&#39046;&#22495;&#30340;&#39069;&#22806;&#19978;&#19979;&#25991;&#20013;&#33719;&#30410;&#65292;&#24182;&#19988;&#21363;&#20351;&#21482;&#26377;&#24456;&#23569;&#30340;&#26631;&#35760;&#25968;&#25454;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the domain of question-answering in NLP, the retrieval of Frequently Asked Questions (FAQ) is an important sub-area which is well researched and has been worked upon for many languages. Here, in response to a user query, a retrieval system typically returns the relevant FAQs from a knowledge-base. The efficacy of such a system depends on its ability to establish semantic match between the query and the FAQs in real-time. The task becomes challenging due to the inherent lexical gap between queries and FAQs, lack of sufficient context in FAQ titles, scarcity of labeled data and high retrieval latency. In this work, we propose a bi-encoder-based query-FAQ matching model that leverages multiple combinations of FAQ fields (like, question, answer, and category) both during model training and inference. Our proposed Multi-Field Bi-Encoder (MFBE) model benefits from the additional context resulting from multiple FAQ fields and performs well even with minimal labeled data. We empirically sup
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27169;&#25311;&#20998;&#23376;&#20307;&#31995;&#31232;&#26377;&#20107;&#20214;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#36716;&#25442;&#36335;&#24452;&#37319;&#26679;&#21644;&#37197;&#32622;&#31354;&#38388;&#30340;&#22686;&#24378;&#25506;&#32034;&#65292;&#24182;&#20381;&#38752;&#24191;&#20041;&#38598;&#21512;&#19978;&#22522;&#20110;&#37197;&#32622;&#21644;&#36712;&#36857;&#31354;&#38388;&#20043;&#38388;&#30340;&#20132;&#25442;&#31227;&#21160;&#23454;&#29616;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#22686;&#24378;&#20102;&#36716;&#25442;&#36335;&#24452;&#37319;&#26679;&#27169;&#25311;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2302.08757</link><description>&lt;p&gt;
&#36890;&#36807;&#30528;&#23556;&#28857;&#20132;&#25442;&#22686;&#24378;&#24191;&#20041;&#38598;&#21512;&#20013;&#37197;&#32622;&#21644;&#36335;&#24452;&#31354;&#38388;&#30340;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Enhanced Sampling of Configuration and Path Space in a Generalized Ensemble by Shooting Point Exchange. (arXiv:2302.08757v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27169;&#25311;&#20998;&#23376;&#20307;&#31995;&#31232;&#26377;&#20107;&#20214;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#36716;&#25442;&#36335;&#24452;&#37319;&#26679;&#21644;&#37197;&#32622;&#31354;&#38388;&#30340;&#22686;&#24378;&#25506;&#32034;&#65292;&#24182;&#20381;&#38752;&#24191;&#20041;&#38598;&#21512;&#19978;&#22522;&#20110;&#37197;&#32622;&#21644;&#36712;&#36857;&#31354;&#38388;&#20043;&#38388;&#30340;&#20132;&#25442;&#31227;&#21160;&#23454;&#29616;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#22686;&#24378;&#20102;&#36716;&#25442;&#36335;&#24452;&#37319;&#26679;&#27169;&#25311;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#27169;&#25311;&#35768;&#22810;&#20998;&#23376;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#22312;&#20110;&#30001;&#38271;&#23551;&#21629;&#29366;&#24577;&#20043;&#38388;&#30340;&#31232;&#26377;&#36716;&#25442;&#24341;&#36215;&#30340;&#38271;&#26102;&#38388;&#23610;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#27169;&#25311;&#36825;&#31181;&#31232;&#26377;&#20107;&#20214;&#65292;&#23427;&#32467;&#21512;&#20102;&#36716;&#25442;&#36335;&#24452;&#37319;&#26679;&#21644;&#37197;&#32622;&#31354;&#38388;&#30340;&#22686;&#24378;&#25506;&#32034;&#12290;&#35813;&#26041;&#27861;&#20381;&#38752;&#22312;&#24191;&#20041;&#38598;&#21512;&#19978;&#22522;&#20110;&#37197;&#32622;&#21644;&#36712;&#36857;&#31354;&#38388;&#20043;&#38388;&#30340;&#20132;&#25442;&#31227;&#21160;&#12290;&#36825;&#31181;&#26041;&#26696;&#26174;&#33879;&#22686;&#24378;&#20102;&#36716;&#25442;&#36335;&#24452;&#37319;&#26679;&#27169;&#25311;&#30340;&#25928;&#29575;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#22810;&#20010;&#36716;&#25442;&#36890;&#36947;&#30340;&#31995;&#32479;&#65292;&#24182;&#19988;&#22312;&#19981;&#25197;&#26354;&#20854;&#21160;&#21147;&#23398;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#26377;&#20851;&#20998;&#23376;&#36807;&#31243;&#30340;&#28909;&#21147;&#23398;&#12289;&#21160;&#21147;&#23398;&#21644;&#21453;&#24212;&#22352;&#26631;&#30340;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#20197;KPTP&#22235;&#32957;&#20013;&#33071;&#27688;&#37240;&#30340;&#24322;&#26500;&#21270;&#20026;&#20363;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
The computer simulation of many molecular processes is complicated by long time scales caused by rare transitions between long-lived states. Here, we propose a new approach to simulate such rare events, which combines transition path sampling with enhanced exploration of configuration space. The method relies on exchange moves between configuration and trajectory space, carried out based on a generalized ensemble. This scheme substantially enhances the efficiency of the transition path sampling simulations, particularly for systems with multiple transition channels, and yields information on thermodynamics, kinetics and reaction coordinates of molecular processes without distorting their dynamics. The method is illustrated using the isomerization of proline in the KPTP tetrapeptide.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36924;&#36817;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;Fisher-Rao&#36317;&#31163;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#25955;&#21270;&#26354;&#32447;&#36830;&#25509;&#27491;&#24577;&#20998;&#24067;&#24182;&#36924;&#36817;&#30456;&#37051;&#27491;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;Rao&#36317;&#31163;&#65292;&#35780;&#20272;&#20102;&#36924;&#36817;&#25216;&#26415;&#30340;&#36136;&#37327;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#20123;&#20449;&#24687;&#20960;&#20309;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2302.08175</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;Fisher-Rao&#36317;&#31163;&#30340;&#25968;&#20540;&#36924;&#36817;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A numerical approximation method for the Fisher-Rao distance between multivariate normal distributions. (arXiv:2302.08175v5 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36924;&#36817;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;Fisher-Rao&#36317;&#31163;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#36890;&#36807;&#31163;&#25955;&#21270;&#26354;&#32447;&#36830;&#25509;&#27491;&#24577;&#20998;&#24067;&#24182;&#36924;&#36817;&#30456;&#37051;&#27491;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;Rao&#36317;&#31163;&#65292;&#35780;&#20272;&#20102;&#36924;&#36817;&#25216;&#26415;&#30340;&#36136;&#37327;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#20123;&#20449;&#24687;&#20960;&#20309;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#36924;&#36817;&#22522;&#20110;&#31163;&#25955;&#21270;&#26354;&#32447;&#36830;&#25509;&#24444;&#27492;&#30340;&#27491;&#24577;&#20998;&#24067;&#21644;&#36924;&#36817;&#36825;&#20123;&#26354;&#32447;&#19978;&#30456;&#37051;&#27491;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;Rao&#36317;&#31163;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;Jeffreys&#31163;&#25955;&#24230;&#30340;&#24179;&#26041;&#26681;&#65292;&#21363;&#23545;&#31216;&#21270;&#30340;Kullback-Leibler&#31163;&#25955;&#24230;&#12290;&#25105;&#20204;&#23454;&#39564;&#32771;&#34385;&#20102;&#26222;&#36890;&#12289;&#33258;&#28982;&#21644;&#26399;&#26395;&#21442;&#25968;&#21270;&#30340;&#27491;&#24577;&#20998;&#24067;&#30340;&#32447;&#24615;&#25554;&#20540;&#26354;&#32447;&#65292;&#24182;&#23558;&#36825;&#20123;&#26354;&#32447;&#19982;&#28304;&#33258;Calvo &#21644;Oller&#23558;Fisher-Rao $d$-variate&#27491;&#24120;&#27969;&#24418;&#31561;&#36317;&#23884;&#20837;$(d+1)\times (d+1)$&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#38181;&#20307;&#30340;&#19968;&#26465;&#26354;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#23558;&#25968;&#20540;&#36924;&#36817;&#19982;&#19978;&#38480;&#21644;&#19979;&#38480;&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#25105;&#20204;&#36924;&#36817;&#25216;&#26415;&#30340;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Calvo&#21644;Oller&#30340;&#19968;&#20123;&#20449;&#24687;&#20960;&#20309;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simple method to approximate Rao's distance between multivariate normal distributions based on discretizing curves joining normal distributions and approximating Rao's distances between successive nearby normal distributions on the curves by the square root of Jeffreys divergence, the symmetrized Kullback-Leibler divergence. We consider experimentally the linear interpolation curves in the ordinary, natural and expectation parameterizations of the normal distributions, and compare these curves with a curve derived from the Calvo and Oller's isometric embedding of the Fisher-Rao $d$-variate normal manifold into the cone of $(d+1)\times (d+1)$ symmetric positive-definite matrices [Journal of multivariate analysis 35.2 (1990): 223-242]. We report on our experiments and assess the quality of our approximation technique by comparing the numerical approximations with both lower and upper bounds. Finally, we present several information-geometric properties of the Calvo and Oller'
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24102;&#38543;&#26426;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#39640;&#32500;&#36755;&#20986;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#21487;&#26377;&#25928;&#22320;&#22788;&#29702;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#39640;&#32500;&#24230;&#21521;&#37327;&#31354;&#38388;&#25110;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20013;&#20063;&#33021;&#36817;&#20284;&#21151;&#33021;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.07260</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#20808;&#39564;&#32593;&#32476;&#30340;&#39640;&#32500;&#36755;&#20986;&#21487;&#25193;&#23637;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks. (arXiv:2302.07260v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24102;&#38543;&#26426;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#39640;&#32500;&#36755;&#20986;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#21487;&#26377;&#25928;&#22320;&#22788;&#29702;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#39640;&#32500;&#24230;&#21521;&#37327;&#31354;&#38388;&#25110;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20013;&#20063;&#33021;&#36817;&#20284;&#21151;&#33021;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#30340;&#19968;&#20123;&#22522;&#26412;&#38382;&#39064;&#28041;&#21450;&#21040;&#26410;&#30693;&#30340;&#39640;&#32500;&#24230;&#26144;&#23556;&#19968;&#32452;&#21487;&#25511;&#21464;&#37327;&#21040;&#26114;&#36149;&#23454;&#39564;&#32467;&#26524;&#30340;&#40657;&#30418;&#20989;&#25968;&#30340;&#20840;&#23616;&#20248;&#21270;&#20219;&#21153;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#25216;&#26415;&#24050;&#34987;&#35777;&#26126;&#22312;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#26102;&#22788;&#29702;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#26102;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#24403;&#22788;&#29702;&#39640;&#32500;&#36755;&#20986;&#26102;&#65292;&#20854;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#20811;&#26381;&#32500;&#24230;&#20027;&#35201;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24102;&#38543;&#26426;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#20030;&#38598;&#25104;&#30340;BO&#21644;&#24207;&#36143;&#20915;&#31574;&#21046;&#23450;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;&#20351;&#29992;&#36866;&#24403;&#30340;&#20307;&#31995;&#32467;&#26500;&#36873;&#25321;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#36817;&#20284;&#35774;&#35745;&#21464;&#37327;&#21644;&#24863;&#20852;&#36259;&#37327;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#65292;&#21363;&#20351;&#22312;&#21518;&#32773;&#21462;&#20540;&#20110;&#39640;&#32500;&#21521;&#37327;&#31354;&#38388;&#25110;&#29978;&#33267;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#32972;&#26223;&#19979;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#22788;&#29702;&#39640;&#32500;&#24230;&#40657;&#30418;&#20989;&#25968;&#30340;&#20840;&#23616;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several fundamental problems in science and engineering consist of global optimization tasks involving unknown high-dimensional (black-box) functions that map a set of controllable variables to the outcomes of an expensive experiment. Bayesian Optimization (BO) techniques are known to be effective in tackling global optimization problems using a relatively small number objective function evaluations, but their performance suffers when dealing with high-dimensional outputs. To overcome the major challenge of dimensionality, here we propose a deep learning framework for BO and sequential decision making based on bootstrapped ensembles of neural architectures with randomized priors. Using appropriate architecture choices, we show that the proposed framework can approximate functional relationships between design variables and quantities of interest, even in cases where the latter take values in high-dimensional vector spaces or even infinite-dimensional function spaces. In the context of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24809;&#32602;&#30340;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#19979;&#23618;&#38750;&#24378;&#20984;&#32422;&#26463;&#21452;&#23618;&#38382;&#39064;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#31639;&#27861;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.05185</link><description>&lt;p&gt;
&#22522;&#20110;&#24809;&#32602;&#30340;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Penalty-based Bilevel Gradient Descent Method. (arXiv:2302.05185v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24809;&#32602;&#30340;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#19979;&#23618;&#38750;&#24378;&#20984;&#32422;&#26463;&#21452;&#23618;&#38382;&#39064;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#31639;&#27861;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a penalty-based bilevel gradient descent algorithm to solve the constrained bilevel problem without lower-level strong convexity, and experiments show its efficiency.
&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#22312;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#20803;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#26159;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#38590;&#20197;&#35299;&#20915;&#12290;&#26368;&#36817;&#30340;&#21487;&#25193;&#23637;&#21452;&#23618;&#31639;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#19979;&#23618;&#30446;&#26631;&#20989;&#25968;&#26159;&#24378;&#20984;&#25110;&#26080;&#32422;&#26463;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24809;&#32602;&#26041;&#27861;&#26469;&#35299;&#20915;&#21452;&#23618;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#24809;&#32602;&#37325;&#26500;&#21487;&#20197;&#24674;&#22797;&#21407;&#22987;&#21452;&#23618;&#38382;&#39064;&#30340;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24809;&#32602;&#30340;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#65288;PBGD&#65289;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#19979;&#23618;&#38750;&#24378;&#20984;&#32422;&#26463;&#21452;&#23618;&#38382;&#39064;&#19978;&#30340;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#24615;&#12290;&#23454;&#39564;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;PBGD&#31639;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization enjoys a wide range of applications in hyper-parameter optimization, meta-learning and reinforcement learning. However, bilevel optimization problems are difficult to solve. Recent progress on scalable bilevel algorithms mainly focuses on bilevel optimization problems where the lower-level objective is either strongly convex or unconstrained. In this work, we tackle the bilevel problem through the lens of the penalty method. We show that under certain conditions, the penalty reformulation recovers the solutions of the original bilevel problem. Further, we propose the penalty-based bilevel gradient descent (PBGD) algorithm and establish its finite-time convergence for the constrained bilevel problem without lower-level strong convexity. Experiments showcase the efficiency of the proposed PBGD algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25269;&#25239;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20854;&#26131;&#21463;&#25915;&#20987;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#26102;&#38388;&#27493;&#39588;&#30340;&#20013;&#38388;&#27493;&#39588;&#12290;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#65292;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2302.03262</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Membership Inference Attacks against Diffusion Models. (arXiv:2302.03262v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25269;&#25239;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20854;&#26131;&#21463;&#25915;&#20987;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#26102;&#38388;&#27493;&#39588;&#30340;&#20013;&#38388;&#27493;&#39588;&#12290;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#65292;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#21019;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25269;&#25239;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65292;&#20197;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#25105;&#20204;&#20027;&#35201;&#20174;&#19982;&#20256;&#32479;&#27169;&#22411;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#27604;&#36739;&#21644;&#25193;&#25955;&#27169;&#22411;&#29420;&#29305;&#30340;&#36229;&#21442;&#25968;&#65292;&#21363;&#26102;&#38388;&#27493;&#38271;&#12289;&#37319;&#26679;&#27493;&#38271;&#21644;&#37319;&#26679;&#26041;&#24046;&#30340;&#35282;&#24230;&#35752;&#35770;&#20102;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;CelebA&#21644;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;DDIM&#20316;&#20026;&#25193;&#25955;&#27169;&#22411;&#12289;DCGAN&#20316;&#20026;GAN&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30333;&#30418;&#21644;&#40657;&#30418;&#23454;&#39564;&#65292;&#28982;&#21518;&#30830;&#35748;&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20687;GAN&#19968;&#26679;&#20855;&#26377;&#25269;&#25239;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#33021;&#21147;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35777;&#26126;&#26102;&#38388;&#27493;&#38271;&#30340;&#24433;&#21709;&#26174;&#33879;&#65292;&#24182;&#19988;&#22312;&#22122;&#22768;&#35745;&#21010;&#20013;&#30340;&#20013;&#38388;&#27493;&#39588;&#26368;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#20004;&#20010;&#20851;&#38190;&#35265;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25915;&#20987;&#30340;&#20301;&#32622;&#12290;&#20854;&#27425;&#65292;&#22312;&#25915;&#20987;&#20013;&#21152;&#20837;&#22122;&#22768;&#20250;&#26174;&#33879;&#38477;&#20302;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have attracted attention in recent years as innovative generative models. In this paper, we investigate whether a diffusion model is resistant to a membership inference attack, which evaluates the privacy leakage of a machine learning model. We primarily discuss the diffusion model from the standpoints of comparison with a generative adversarial network (GAN) as conventional models and hyperparameters unique to the diffusion model, i.e., time steps, sampling steps, and sampling variances. We conduct extensive experiments with DDIM as a diffusion model and DCGAN as a GAN on the CelebA and CIFAR-10 datasets in both white-box and black-box settings and then confirm if the diffusion model is comparably resistant to a membership inference attack as GAN. Next, we demonstrate that the impact of time steps is significant and intermediate steps in a noise schedule are the most vulnerable to the attack. We also found two key insights through further analysis. First, we identify 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#32447;&#32447;&#24615;&#20248;&#21270;&#65288;OLO&#65289;&#28041;&#21450;&#26080;&#32422;&#26463;&#38382;&#39064;&#21644;&#21160;&#24577;&#36951;&#25022;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#26032;&#26500;&#36896;&#38382;&#39064;&#20026;&#31232;&#30095;&#32534;&#30721;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#26041;&#24335;&#65292;&#22312;&#36866;&#24212;&#24615;&#21644;&#24212;&#29992;&#19978;&#26377;&#36739;&#22909;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2301.13349</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#23454;&#29616;&#26080;&#32422;&#26463;&#21160;&#24577;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Unconstrained Dynamic Regret via Sparse Coding. (arXiv:2301.13349v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#32447;&#32447;&#24615;&#20248;&#21270;&#65288;OLO&#65289;&#28041;&#21450;&#26080;&#32422;&#26463;&#38382;&#39064;&#21644;&#21160;&#24577;&#36951;&#25022;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#26032;&#26500;&#36896;&#38382;&#39064;&#20026;&#31232;&#30095;&#32534;&#30721;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#26041;&#24335;&#65292;&#22312;&#36866;&#24212;&#24615;&#21644;&#24212;&#29992;&#19978;&#26377;&#36739;&#22909;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#32447;&#32447;&#24615;&#20248;&#21270;&#65288;OLO&#65289;&#22312;&#20004;&#20010;&#38382;&#39064;&#32467;&#26500;&#30340;&#32806;&#21512;&#19979;&#30340;&#24773;&#20917;&#65306;&#22495;&#26080;&#30028;&#65292;&#32780;&#31639;&#27861;&#30340;&#24615;&#33021;&#26159;&#36890;&#36807;&#21160;&#24577;&#36951;&#25022;&#26469;&#34913;&#37327;&#30340;&#12290;&#22788;&#29702;&#20219;&#19968;&#38382;&#39064;&#37117;&#35201;&#27714;&#36951;&#25022;&#30028;&#38480;&#20381;&#36182;&#20110;&#27604;&#36739;&#24207;&#21015;&#30340;&#26576;&#20123;&#22797;&#26434;&#24230;&#37327;&#24230; - &#29305;&#21035;&#26159;&#26080;&#32422;&#26463;OLO&#20013;&#30340;&#27604;&#36739;&#22120;&#33539;&#25968;&#65292;&#20197;&#21450;&#21160;&#24577;&#36951;&#25022;&#20013;&#30340;&#36335;&#24452;&#38271;&#24230;&#12290;&#19982;&#26368;&#36817;&#19968;&#31687;&#25991;&#31456;(Jacobsen&amp; Cutkosky&#65292;2022)&#36866;&#24212;&#36825;&#20004;&#20010;&#22797;&#26434;&#24230;&#37327;&#24230;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#26032;&#26500;&#36896;&#38382;&#39064;&#20026;&#31232;&#30095;&#32534;&#30721;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#26041;&#24335;&#12290;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#23454;&#29616;&#36866;&#24212;&#24615;&#65292;&#36825;&#20010;&#26694;&#26550;&#33258;&#28982;&#22320;&#21033;&#29992;&#20102;&#29615;&#22659;&#26356;&#22797;&#26434;&#30340;&#21069;&#32622;&#30693;&#35782;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38745;&#24577;&#26080;&#32422;&#26463;OLO&#26799;&#24230;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#20351;&#29992;&#20102;&#26032;&#39062;&#30340;&#36830;&#32493;&#26102;&#38388;&#26426;&#21046;&#35774;&#35745;&#12290;&#36825;&#21487;&#33021;&#26159;&#20855;&#26377;&#29420;&#31435;&#20852;&#36259;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by time series forecasting, we study Online Linear Optimization (OLO) under the coupling of two problem structures: the domain is unbounded, and the performance of an algorithm is measured by its dynamic regret. Handling either of them requires the regret bound to depend on certain complexity measure of the comparator sequence -- specifically, the comparator norm in unconstrained OLO, and the path length in dynamic regret. In contrast to a recent work (Jacobsen &amp; Cutkosky, 2022) that adapts to the combination of these two complexity measures, we propose an alternative complexity measure by recasting the problem into sparse coding. Adaptivity can be achieved by a simple modular framework, which naturally exploits more intricate prior knowledge of the environment. Along the way, we also present a new gradient adaptive algorithm for static unconstrained OLO, designed using novel continuous time machinery. This could be of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26080;&#21160;&#20316;&#31163;&#32447;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;AF-Guide&#65292;&#23454;&#29616;&#21464;&#20307;&#30340;Upside-Down&#24378;&#21270;&#23398;&#20064;&#21644;&#25351;&#23548;&#22312;&#32447;&#23398;&#20064;&#30340;Guided SAC&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#25913;&#21892;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#19981;&#23384;&#22312;&#21160;&#20316;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12876</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#21160;&#20316;&#31163;&#32447;&#25968;&#25454;&#39044;&#35757;&#32451;&#25351;&#23548;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Guiding Online Reinforcement Learning with Action-Free Offline Pretraining. (arXiv:2301.12876v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26080;&#21160;&#20316;&#31163;&#32447;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;AF-Guide&#65292;&#23454;&#29616;&#21464;&#20307;&#30340;Upside-Down&#24378;&#21270;&#23398;&#20064;&#21644;&#25351;&#23548;&#22312;&#32447;&#23398;&#20064;&#30340;Guided SAC&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#25913;&#21892;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#19981;&#23384;&#22312;&#21160;&#20316;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;RL&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#20195;&#29702;&#20351;&#29992;&#31163;&#32447;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#20943;&#23569;&#29615;&#22659;&#20132;&#20114;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22312;&#25968;&#25454;&#25910;&#38598;&#26399;&#38388;&#35760;&#24405;&#21160;&#20316;&#20449;&#24687;&#65292;&#36825;&#22312;&#26576;&#20123;&#23454;&#38469;&#24773;&#20917;&#19979;&#21487;&#33021;&#26159;&#22256;&#38590;&#29978;&#33267;&#19981;&#21487;&#33021;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#26080;&#21160;&#20316;&#31163;&#32447;&#25968;&#25454;&#38598;&#26469;&#25913;&#21892;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#28508;&#21147;&#65292;&#24182;&#23558;&#27492;&#38382;&#39064;&#21629;&#21517;&#20026;&#20855;&#26377;&#26080;&#21160;&#20316;&#31163;&#32447;&#39044;&#35757;&#32451;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;AFP-RL&#65289;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;AF-Guide&#65292;&#19968;&#31181;&#36890;&#36807;&#20174;&#26080;&#21160;&#20316;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30693;&#35782;&#26469;&#25351;&#23548;&#22312;&#32447;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;AF-Guide&#21253;&#25324;&#23454;&#26045;Upside-Down&#24378;&#21270;&#23398;&#20064;&#21464;&#20307;&#30340;&#26080;&#21160;&#20316;&#20915;&#31574;Transformer&#65288;AFDT&#65289;&#65292;&#23427;&#23398;&#20064;&#20174;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#35268;&#21010;&#19979;&#19968;&#20010;&#29366;&#24577;&#65292;&#20197;&#21450;&#19968;&#20010;&#36890;&#36807;AFDT&#25351;&#23548;&#22312;&#32447;&#23398;&#20064;&#30340;Guided Soft Actor-Critic&#65288;Guided SAC&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#19981;&#23384;&#22312;&#21160;&#20316;&#20449;&#24687;&#30340;&#29615;&#22659;&#20013;&#65292;AF-Guide&#21487;&#20197;&#25552;&#39640;&#22312;&#32447;RL&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline RL methods have been shown to reduce the need for environment interaction by training agents using offline collected episodes. However, these methods typically require action information to be logged during data collection, which can be difficult or even impossible in some practical cases. In this paper, we investigate the potential of using action-free offline datasets to improve online reinforcement learning, name this problem Reinforcement Learning with Action-Free Offline Pretraining (AFP-RL). We introduce Action-Free Guide (AF-Guide), a method that guides online training by extracting knowledge from action-free offline datasets. AF-Guide consists of an Action-Free Decision Transformer (AFDT) implementing a variant of Upside-Down Reinforcement Learning. It learns to plan the next states from the offline dataset, and a Guided Soft Actor-Critic (Guided SAC) that learns online with guidance from AFDT. Experimental results show that AF-Guide can improve sample efficiency and pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#19968;&#31181;&#21333;&#27425;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#20027;&#35201;&#32858;&#28966;&#20110;&#23376;&#24207;&#21015;&#30456;&#20284;&#24615;&#22914;&#20309;&#24433;&#21709;&#25972;&#20010;&#24207;&#21015;&#30456;&#20284;&#24615;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#23376;&#24207;&#21015;&#30456;&#20284;&#30340;&#24207;&#21015;&#26469;&#22686;&#24378;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2301.08403</link><description>&lt;p&gt;
&#36890;&#36807;&#23376;&#24207;&#21015;&#30456;&#20284;&#24615;&#29983;&#25104;&#24207;&#21015;&#65306;&#29702;&#35770;&#21450;&#20854;&#22312;&#26080;&#20154;&#26426;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Sequence Generation via Subsequence Similarity: Theory and Application to UAV Identification. (arXiv:2301.08403v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#19968;&#31181;&#21333;&#27425;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#20027;&#35201;&#32858;&#28966;&#20110;&#23376;&#24207;&#21015;&#30456;&#20284;&#24615;&#22914;&#20309;&#24433;&#21709;&#25972;&#20010;&#24207;&#21015;&#30456;&#20284;&#24615;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#23376;&#24207;&#21015;&#30456;&#20284;&#30340;&#24207;&#21015;&#26469;&#22686;&#24378;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#21512;&#25104;&#24207;&#21015;&#30340;&#33021;&#21147;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#21644;&#29983;&#25104;&#26694;&#26550;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#36825;&#19968;&#36807;&#31243;&#12290;&#26412;&#25991;&#20351;&#29992;&#19968;&#31181;&#21333;&#27425;&#29983;&#25104;&#27169;&#22411;&#26469;&#37319;&#26679;&#65292;&#36890;&#36807;&#30456;&#20284;&#24615;&#29983;&#25104;&#23376;&#24207;&#21015;&#65292;&#24182;&#35777;&#26126;&#20102;&#23376;&#24207;&#21015;&#30456;&#20284;&#24615;&#23545;&#25972;&#20010;&#24207;&#21015;&#30456;&#20284;&#24615;&#30340;&#24433;&#21709;&#65292;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#19968;&#27425;&#24615;&#29983;&#25104;&#27169;&#22411;&#26469;&#20174;&#21333;&#20010;&#24207;&#21015;&#30340;&#33539;&#22260;&#20869;&#21462;&#26679;&#65292;&#24182;&#29983;&#25104;&#23376;&#24207;&#21015;&#30456;&#20284;&#30340;&#24207;&#21015;&#65292;&#35777;&#26126;&#20102;&#25968;&#25454;&#38598;&#22686;&#24378;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to generate synthetic sequences is crucial for a wide range of applications, and recent advances in deep learning architectures and generative frameworks have greatly facilitated this process. Particularly, unconditional one-shot generative models constitute an attractive line of research that focuses on capturing the internal information of a single image or video to generate samples with similar contents. Since many of those one-shot models are shifting toward efficient non-deep and non-adversarial approaches, we examine the versatility of a one-shot generative model for augmenting whole datasets. In this work, we focus on how similarity at the subsequence level affects similarity at the sequence level, and derive bounds on the optimal transport of real and generated sequences based on that of corresponding subsequences. We use a one-shot generative model to sample from the vicinity of individual sequences and generate subsequence-similar ones and demonstrate the improvem
&lt;/p&gt;</description></item><item><title>SparseGPT&#36890;&#36807;&#19968;&#31181;&#39640;&#25928;&#12289;&#31934;&#30830;&#30340;&#26041;&#27861;&#65292;&#33021;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#33267;&#23569;&#21098;&#26525;50%&#30340;&#31232;&#30095;&#24230;&#65292;&#32780;&#20934;&#30830;&#24230;&#19979;&#38477;&#24456;&#23567;&#12290;</title><link>http://arxiv.org/abs/2301.00774</link><description>&lt;p&gt;
SparseGPT&#65306;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#21363;&#21487;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#31934;&#30830;&#21098;&#26525;&#33267;&#33267;&#23569;50%&#31232;&#30095;&#24230;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot. (arXiv:2301.00774v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00774
&lt;/p&gt;
&lt;p&gt;
SparseGPT&#36890;&#36807;&#19968;&#31181;&#39640;&#25928;&#12289;&#31934;&#30830;&#30340;&#26041;&#27861;&#65292;&#33021;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#33267;&#23569;&#21098;&#26525;50%&#30340;&#31232;&#30095;&#24230;&#65292;&#32780;&#20934;&#30830;&#24230;&#19979;&#38477;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#19968;&#31181;&#21517;&#20026;SparseGPT&#30340;&#26032;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#20934;&#30830;&#22320;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;GPT&#23478;&#26063;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#33267;&#23569;&#21098;&#26525;50%&#30340;&#31232;&#30095;&#24230;&#65292;&#19988;&#20934;&#30830;&#24230;&#19979;&#38477;&#24456;&#23567;&#12290;&#25105;&#20204;&#33021;&#22815;&#22312;&#19981;&#21040;4.5&#23567;&#26102;&#20869;&#21363;&#21487;&#22312;&#26368;&#22823;&#30340;&#24320;&#28304;&#27169;&#22411;OPT-175B&#21644;BLOOM-176B&#19978;&#25191;&#34892;SparseGPT&#65292;&#24182;&#22312;&#20960;&#20046;&#19981;&#22686;&#21152;&#22256;&#24785;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;60%&#30340;&#26080;&#32467;&#26500;&#31232;&#30095;&#24230;&#65306;&#36825;&#20123;&#27169;&#22411;&#30340;&#36229;&#36807;1000&#20159;&#20010;&#21442;&#25968;&#21487;&#20197;&#22312;&#25512;&#29702;&#26102;&#24573;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;NextG&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#20013;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#21644;AI&#23433;&#20840;&#20445;&#35777;&#20449;&#21495;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#22312;&#20445;&#35777;&#20449;&#21495;&#20256;&#36755;&#25928;&#29575;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.09668</link><description>&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#30340;&#36890;&#20449;&#31995;&#32479;&#65306;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#21644;AI&#23433;&#20840;&#30340;NextG&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Task-Oriented Communications for NextG: End-to-End Deep Learning and AI Security Aspects. (arXiv:2212.09668v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;NextG&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#20013;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#21644;AI&#23433;&#20840;&#20445;&#35777;&#20449;&#21495;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#22312;&#20445;&#35777;&#20449;&#21495;&#20256;&#36755;&#25928;&#29575;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#36890;&#20449;&#31995;&#32479;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#21487;&#38752;&#22320;&#20256;&#36755;&#25968;&#23383;&#25968;&#25454;&#27969;&#65288;&#27604;&#29305;&#27969;&#65289;&#65292;&#32780;&#26032;&#19968;&#20195;&#36890;&#20449;&#31995;&#32479;&#65288;NextG&#65289;&#24320;&#22987;&#25506;&#32034;&#25226;&#35774;&#35745;&#33539;&#24335;&#36716;&#21521;&#21487;&#38752;&#22320;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#65292;&#20363;&#22914;&#20219;&#21153;&#23548;&#21521;&#30340;&#36890;&#20449;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26080;&#32447;&#20449;&#21495;&#20998;&#31867;&#20316;&#20026;NextG&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#65288;RAN&#65289;&#30340;&#20219;&#21153;&#65292;&#22312;&#35813;&#20219;&#21153;&#20013;&#65292;&#36793;&#32536;&#35774;&#22791;&#25910;&#38598;&#26080;&#32447;&#20449;&#21495;&#20197;&#33719;&#21462;&#39057;&#35889;&#24863;&#30693;&#65292;&#28982;&#21518;&#19982;&#38656;&#35201;&#35782;&#21035;&#20449;&#21495;&#26631;&#31614;&#30340;NextG&#22522;&#31449;&#65288;gNodeB&#65289;&#36827;&#34892;&#36890;&#20449;&#12290;&#28982;&#32780;&#36793;&#32536;&#35774;&#22791;&#21487;&#33021;&#26080;&#27861;&#36275;&#22815;&#22788;&#29702;&#20449;&#21495;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#19988;&#30001;&#20110;&#20005;&#26684;&#30340;&#24310;&#36831;&#12289;&#36895;&#29575;&#21644;&#33021;&#37327;&#38480;&#21046;&#65292;&#23558;&#20449;&#21495;&#20256;&#36755;&#21040;gNodeB&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#21457;&#23556;&#26426;&#12289;&#25509;&#25910;&#26426;&#21644;&#20998;&#31867;&#22120;&#30340;&#21151;&#33021;&#20316;&#20026;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#65292;&#20026;&#36793;&#32536;&#35774;&#22791;&#21644;gNodeB&#23454;&#29616;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communications systems to date are primarily designed with the goal of reliable transfer of digital sequences (bits). Next generation (NextG) communication systems are beginning to explore shifting this design paradigm to reliably executing a given task such as in task-oriented communications. In this paper, wireless signal classification is considered as the task for the NextG Radio Access Network (RAN), where edge devices collect wireless signals for spectrum awareness and communicate with the NextG base station (gNodeB) that needs to identify the signal label. Edge devices may not have sufficient processing power and may not be trusted to perform the signal classification task, whereas the transfer of signals to the gNodeB may not be feasible due to stringent delay, rate, and energy restrictions. Task-oriented communications is considered by jointly training the transmitter, receiver and classifier functionalities as an encoder-decoder pair for the edge device and the gNodeB. This a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#65292;&#25903;&#25345;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#24418;&#29366;&#31561;&#22810;&#31181;&#36755;&#20837;&#27169;&#24335;&#65292;&#27169;&#22411;&#20855;&#26377;&#36739;&#24378;&#30340;&#28789;&#27963;&#24615;&#21644;&#24615;&#33021;&#34920;&#29616;&#65292;&#22312;&#30446;&#21069;&#30456;&#20851;&#24037;&#20316;&#20013;&#22788;&#20110;&#39046;&#20808;&#22320;&#20301;&#65292;&#33021;&#22815;&#23558;&#22810;&#31181;&#20219;&#21153;&#25972;&#21512;&#25104;&#19968;&#20010;&#24037;&#20855;&#65292;&#20026;&#19994;&#20313;&#29992;&#25143;&#31616;&#21270;&#20102;&#19977;&#32500;&#36164;&#28304;&#29983;&#25104;&#30340;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2212.04493</link><description>&lt;p&gt;
SDFusion&#65306;&#22810;&#27169;&#24577;&#19977;&#32500;&#24418;&#29366;&#23436;&#25104;&#12289;&#37325;&#24314;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation. (arXiv:2212.04493v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#30340;&#26032;&#26694;&#26550;&#65292;&#25903;&#25345;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#24418;&#29366;&#31561;&#22810;&#31181;&#36755;&#20837;&#27169;&#24335;&#65292;&#27169;&#22411;&#20855;&#26377;&#36739;&#24378;&#30340;&#28789;&#27963;&#24615;&#21644;&#24615;&#33021;&#34920;&#29616;&#65292;&#22312;&#30446;&#21069;&#30456;&#20851;&#24037;&#20316;&#20013;&#22788;&#20110;&#39046;&#20808;&#22320;&#20301;&#65292;&#33021;&#22815;&#23558;&#22810;&#31181;&#20219;&#21153;&#25972;&#21512;&#25104;&#19968;&#20010;&#24037;&#20855;&#65292;&#20026;&#19994;&#20313;&#29992;&#25143;&#31616;&#21270;&#20102;&#19977;&#32500;&#36164;&#28304;&#29983;&#25104;&#30340;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#31616;&#21270;&#19994;&#20313;&#29992;&#25143;&#30340;&#19977;&#32500;&#36164;&#28304;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#25903;&#25345;&#21508;&#31181;&#36755;&#20837;&#27169;&#24335;&#65292;&#21253;&#25324;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#24418;&#29366;&#21644;&#36825;&#20123;&#30340;&#32452;&#21512;&#65292;&#36827;&#19968;&#27493;&#20801;&#35768;&#35843;&#25972;&#27599;&#20010;&#36755;&#20837;&#30340;&#24378;&#24230;&#65292;&#20197;&#23454;&#29616;&#20132;&#20114;&#24335;&#29983;&#25104;&#12290;&#22312;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65292;&#23558;&#19977;&#32500;&#24418;&#29366;&#21387;&#32553;&#25104;&#19968;&#20010;&#32039;&#20945;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#28982;&#21518;&#23398;&#20064;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#25903;&#25345;&#21508;&#31181;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20855;&#26377;&#36749;&#23398;&#30340;&#20219;&#21153;&#29305;&#23450;&#32534;&#30721;&#22120;&#65292;&#21518;&#36319;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#12290;&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33258;&#28982;&#25903;&#25345;&#21508;&#31181;&#20219;&#21153;&#65292;&#22312;&#24418;&#29366;&#23436;&#25104;&#12289;&#22522;&#20110;&#22270;&#20687;&#30340;&#19977;&#32500;&#37325;&#24314;&#21644;&#25991;&#26412;&#21040;3D&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#22343;&#20248;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#12290;&#26368;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#23558;&#25152;&#26377;&#36825;&#20123;&#20219;&#21153;&#32452;&#21512;&#25104;&#19968;&#20010;&#8220;&#29790;&#22763;&#20891;&#20992;&#8221;&#24037;&#20855;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#20351;&#29992;&#19981;&#23436;&#25972;&#30340;&#24418;&#29366;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#25551;&#36848;&#26469;&#36827;&#34892;&#24418;&#29366;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a novel framework built to simplify 3D asset generation for amateur users. To enable interactive generation, our method supports a variety of input modalities that can be easily provided by a human, including images, text, partially observed shapes and combinations of these, further allowing to adjust the strength of each input. At the core of our approach is an encoder-decoder, compressing 3D shapes into a compact latent representation, upon which a diffusion model is learned. To enable a variety of multi-modal inputs, we employ task-specific encoders with dropout followed by a cross-attention mechanism. Due to its flexibility, our model naturally supports a variety of tasks, outperforming prior works on shape completion, image-based 3D reconstruction, and text-to-3D. Most interestingly, our model can combine all these tasks into one swiss-army-knife tool, enabling the user to perform shape generation using incomplete shapes, images, and textual descriptions a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#24418;&#29366;&#24341;&#23548;&#25193;&#25955;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22806;&#37096;&#27880;&#24847;&#26426;&#21046;&#23558;&#24418;&#29366;&#38480;&#21046;&#24212;&#29992;&#20110;&#36328;&#27880;&#24847;&#21147;&#22270;&#21644;&#33258;&#27880;&#24847;&#21147;&#22270;&#19978;&#65292;&#20174;&#32780;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#32771;&#34385;&#21040;&#23545;&#35937;&#24418;&#29366;&#65292;&#36827;&#32780;&#21487;&#20197;&#23454;&#29616;&#23545;&#35937;&#24418;&#29366;&#24544;&#23454;&#24230;&#26356;&#39640;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2212.00210</link><description>&lt;p&gt;
&#24102;&#26377;&#20869;&#22806;&#37096;&#27880;&#24847;&#21147;&#30340;&#24418;&#29366;&#24341;&#23548;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Shape-Guided Diffusion with Inside-Outside Attention. (arXiv:2212.00210v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00210
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#24418;&#29366;&#24341;&#23548;&#25193;&#25955;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22806;&#37096;&#27880;&#24847;&#26426;&#21046;&#23558;&#24418;&#29366;&#38480;&#21046;&#24212;&#29992;&#20110;&#36328;&#27880;&#24847;&#21147;&#22270;&#21644;&#33258;&#27880;&#24847;&#21147;&#22270;&#19978;&#65292;&#20174;&#32780;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#32771;&#34385;&#21040;&#23545;&#35937;&#24418;&#29366;&#65292;&#36827;&#32780;&#21487;&#20197;&#23454;&#29616;&#23545;&#35937;&#24418;&#29366;&#24544;&#23454;&#24230;&#26356;&#39640;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25805;&#20316;&#23545;&#35937;&#26102;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#36890;&#24120;&#24573;&#30053;&#23545;&#35937;&#30340;&#24418;&#29366;&#24182;&#29983;&#25104;&#38169;&#35823;&#27604;&#20363;&#12289;&#34987;&#25130;&#26029;&#25110;&#34987;&#32972;&#26223;&#20869;&#23481;&#26367;&#25442;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861; Shape-Guided Diffusion&#65292;&#35813;&#26041;&#27861;&#20462;&#25913;&#20102;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#65292;&#20351;&#20043;&#23545;&#29992;&#25143;&#25351;&#23450;&#30340;&#24418;&#29366;&#36755;&#20837;&#25110;&#20174;&#25991;&#26412;&#20013;&#33258;&#21160;&#25512;&#26029;&#30340;&#24418;&#29366;&#25935;&#24863;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22806;&#37096;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#21453;&#28436;&#21644;&#29983;&#25104;&#36807;&#31243;&#20013;&#23558;&#27492;&#24418;&#29366;&#38480;&#21046;&#24212;&#29992;&#20110;&#36328;&#27880;&#24847;&#21147;&#22270;&#21644;&#33258;&#27880;&#24847;&#21147;&#22270;&#19978;&#12290;&#25105;&#20204;&#30340;&#26426;&#21046;&#25351;&#23450;&#31354;&#38388;&#21306;&#22495;&#26159;&#23545;&#35937;&#65288;&#20869;&#37096;&#65289;&#36824;&#26159;&#32972;&#26223;&#65288;&#22806;&#37096;&#65289;&#65292;&#28982;&#21518;&#23558;&#25991;&#26412;&#25552;&#31034;&#25351;&#23450;&#30340;&#32534;&#36753;&#19982;&#27491;&#30830;&#30340;&#21306;&#22495;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#22312;&#24418;&#29366;&#24341;&#23548;&#32534;&#36753;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#27169;&#22411;&#24517;&#39035;&#26681;&#25454;&#25991;&#26412;&#25552;&#31034;&#21644;&#23545;&#35937;&#25513;&#30721;&#26367;&#25442;&#23545;&#35937;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#20174; MS-COCO &#34893;&#29983;&#30340; ShapePrompts &#22522;&#20934;&#65292;&#24182;&#22312;&#24418;&#29366;&#24544;&#23454;&#24230;&#26041;&#38754;&#23454;&#29616;&#20102; SOTA &#30340;&#32467;&#26524;&#65292;&#32780;&#19981;&#38656;&#35201;&#38477;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
When manipulating an object, existing text-to-image diffusion models often ignore the shape of the object and generate content that is incorrectly scaled, cut off, or replaced with background content. We propose a training-free method, Shape-Guided Diffusion, that modifies pretrained diffusion models to be sensitive to shape input specified by a user or automatically inferred from text. We use a novel Inside-Outside Attention mechanism during the inversion and generation process to apply this shape constraint to the cross- and self-attention maps. Our mechanism designates which spatial region is the object (inside) vs. background (outside) then associates edits specified by text prompts to the correct region. We demonstrate the efficacy of our method on the shape-guided editing task, where the model must replace an object according to a text prompt and object mask. We curate a new ShapePrompts benchmark derived from MS-COCO and achieve SOTA results in shape faithfulness without a degra
&lt;/p&gt;</description></item><item><title>AIREPAIR&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#20462;&#22797;&#24179;&#21488;&#65292;&#23427;&#33021;&#22815;&#38598;&#25104;&#29616;&#26377;&#32593;&#32476;&#20462;&#22797;&#24037;&#20855;&#24182;&#23454;&#29616;&#19981;&#21516;&#26041;&#27861;&#30340;&#20844;&#24179;&#27604;&#36739;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.15387</link><description>&lt;p&gt;
AIREPAIR&#8212;&#8212;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#20462;&#22797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
AIREPAIR: A Repair Platform for Neural Networks. (arXiv:2211.15387v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15387
&lt;/p&gt;
&lt;p&gt;
AIREPAIR&#26159;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#20462;&#22797;&#24179;&#21488;&#65292;&#23427;&#33021;&#22815;&#38598;&#25104;&#29616;&#26377;&#32593;&#32476;&#20462;&#22797;&#24037;&#20855;&#24182;&#23454;&#29616;&#19981;&#21516;&#26041;&#27861;&#30340;&#20844;&#24179;&#27604;&#36739;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AIREPAIR&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#22797;&#24179;&#21488;&#12290;&#23427;&#21253;&#25324;&#20102;&#29616;&#26377;&#32593;&#32476;&#20462;&#22797;&#24037;&#20855;&#30340;&#38598;&#25104;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#22312;&#21516;&#19968;&#20010;&#27169;&#22411;&#19978;&#36816;&#34892;&#19981;&#21516;&#30340;&#32593;&#32476;&#20462;&#22797;&#26041;&#27861;&#24182;&#36827;&#34892;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#26368;&#20808;&#36827;&#30340;&#20462;&#22797;&#24037;&#20855;&#22312;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#35780;&#20272;&#20102;AIREPAIR&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;AIREPAIR&#30340;&#23454;&#29992;&#24615;&#65292;&#36890;&#36807;&#27604;&#36739;&#21644;&#20998;&#26512;&#19981;&#21516;&#20462;&#22797;&#25216;&#26415;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#28436;&#31034;&#35270;&#39057;&#21487;&#22312; https://youtu.be/UkKw5neeWhw &#26597;&#30475;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present AIREPAIR, a platform for repairing neural networks. It features the integration of existing network repair tools. Based on AIREPAIR, one can run different repair methods on the same model, thus enabling the fair comparison of different repair techniques. We evaluate AIREPAIR with three state-of-the-art repair tools on popular deep-learning datasets and models. Our evaluation confirms the utility of AIREPAIR, by comparing and analyzing the results from different repair techniques. A demonstration is available at https://youtu.be/UkKw5neeWhw.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#21452;&#23618;&#32972;&#21253;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#27604;&#31934;&#30830;&#31639;&#27861;&#24555;500&#20493;&#65292;&#21487;&#25214;&#21040;&#21487;&#34892;&#24615;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2211.13436</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#21452;&#23618;&#32972;&#21253;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Bilevel Knapsack Problem using Graph Neural Networks. (arXiv:2211.13436v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#21452;&#23618;&#32972;&#21253;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#27604;&#31934;&#30830;&#31639;&#27861;&#24555;500&#20493;&#65292;&#21487;&#25214;&#21040;&#21487;&#34892;&#24615;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#26159;&#19968;&#31181;&#20855;&#26377;&#20004;&#20010;&#20195;&#29702;&#20154;&#65288;&#39046;&#23548;&#32773;&#21644;&#36861;&#38543;&#32773;&#65289;&#30340;&#23618;&#27425;&#20248;&#21270;&#38382;&#39064;&#12290;&#39046;&#23548;&#32773;&#39318;&#20808;&#20570;&#20986;&#33258;&#24049;&#30340;&#20915;&#31574;&#65292;&#36861;&#38543;&#32773;&#38543;&#21518;&#20570;&#20986;&#26368;&#20339;&#36873;&#25321;&#12290;&#39046;&#23548;&#32773;&#20102;&#35299;&#36861;&#38543;&#32773;&#30340;&#20449;&#24687;&#65292;&#38382;&#39064;&#30340;&#30446;&#26631;&#26159;&#20174;&#39046;&#23548;&#32773;&#30340;&#35282;&#24230;&#32771;&#34385;&#36861;&#38543;&#32773;&#30340;&#21453;&#24212;&#65292;&#25214;&#21040;&#26368;&#20248;&#35299;&#12290;&#23545;&#20110;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#26469;&#35828;&#65292;&#27809;&#26377;&#36890;&#29992;&#30340;&#39640;&#25928;&#31639;&#27861;&#25110;&#21830;&#29992;&#27714;&#35299;&#22120;&#21487;&#20197;&#24471;&#21040;&#26368;&#20248;&#35299;&#65292;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#38382;&#39064;&#20063;&#24456;&#38590;&#24471;&#21040;&#33391;&#22909;&#30340;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#21452;&#23618;&#32972;&#21253;&#38382;&#39064;&#12290;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#26469;&#39044;&#27979;&#39046;&#23548;&#32773;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#23558;&#23618;&#27425;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#21333;&#23618;&#20248;&#21270;&#38382;&#39064;&#20197;&#33719;&#21462;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21457;&#29616;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36895;&#24230;&#27604;&#31934;&#30830;&#31639;&#27861;&#24555;500&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Bilevel Optimization Problem is a hierarchical optimization problem with two agents, a leader and a follower. The leader make their own decisions first, and the followers make the best choices accordingly. The leader knows the information of the followers, and the goal of the problem is to find the optimal solution by considering the reactions of the followers from the leader's point of view. For the Bilevel Optimization Problem, there are no general and efficient algorithms or commercial solvers to get an optimal solution, and it is very difficult to get a good solution even for a simple problem. In this paper, we propose a deep learning approach using Graph Neural Networks to solve the bilevel knapsack problem. We train the model to predict the leader's solution and use it to transform the hierarchical optimization problem into a single-level optimization problem to get the solution. Our model found the feasible solution that was about 500 times faster than the exact algorithm wi
&lt;/p&gt;</description></item><item><title>uSplit&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#33639;&#20809;&#26174;&#24494;&#38236;&#22270;&#20687;&#30340;&#39640;&#25928;&#22270;&#20687;&#20998;&#35299;&#26041;&#27861;&#65292;&#38598;&#25104;&#20102;&#27178;&#21521;&#19978;&#19979;&#25991;&#21270;&#65292;&#24110;&#21161;&#35757;&#32451;&#26356;&#28145;&#30340;&#20998;&#23618;&#27169;&#22411;&#65292;&#24182;&#26377;&#25928;&#22320;&#20943;&#23569;&#24179;&#38138;&#20266;&#24433;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.12872</link><description>&lt;p&gt;
{\mu}Split: &#26174;&#24494;&#38236;&#25968;&#25454;&#30340;&#39640;&#25928;&#22270;&#20687;&#20998;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
{\mu}Split: efficient image decomposition for microscopy data. (arXiv:2211.12872v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12872
&lt;/p&gt;
&lt;p&gt;
uSplit&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#33639;&#20809;&#26174;&#24494;&#38236;&#22270;&#20687;&#30340;&#39640;&#25928;&#22270;&#20687;&#20998;&#35299;&#26041;&#27861;&#65292;&#38598;&#25104;&#20102;&#27178;&#21521;&#19978;&#19979;&#25991;&#21270;&#65292;&#24110;&#21161;&#35757;&#32451;&#26356;&#28145;&#30340;&#20998;&#23618;&#27169;&#22411;&#65292;&#24182;&#26377;&#25928;&#22320;&#20943;&#23569;&#24179;&#38138;&#20266;&#24433;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; uSplit&#65292;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#33639;&#20809;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#30340;&#35757;&#32451;&#22270;&#20687;&#20998;&#35299;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#24120;&#35268;&#30340;&#28145;&#24230;&#32467;&#26500;&#20307;&#31995;&#32467;&#26500;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#22823;&#22270;&#20687;&#22359;&#20250;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#65292;&#20351;&#20869;&#23384;&#28040;&#32791;&#25104;&#20026;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#30340;&#38480;&#21046;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27178;&#21521;&#19978;&#19979;&#25991;&#21270;&#65288;LC&#65289;&#65292;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#24378;&#22823;&#30340;&#32593;&#32476;&#65292;&#24182;&#23637;&#31034;LC&#22312;&#22788;&#29702;&#20219;&#21153;&#26102;&#22987;&#32456;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#23558;LC&#19982;U-Nets&#12289;&#20998;&#23618;&#33258;&#32534;&#30721;&#22120;&#21644;&#20998;&#23618;VAEs&#38598;&#25104;&#65292;&#20026;&#27492;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;ELBO loss&#12290;&#27492;&#22806;&#65292;LC&#20351;&#24471;&#35757;&#32451;&#27604;&#21407;&#26412;&#26356;&#28145;&#30340;&#20998;&#23618;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#19988;&#26377;&#21161;&#20110;&#20943;&#23569;&#20351;&#29992;&#20998;&#21106;VAE&#39044;&#27979;&#26102;&#19981;&#21487;&#36991;&#20813;&#30340;&#24179;&#38138;&#20266;&#24433;&#12290;&#25105;&#20204;&#23558;uSplit&#24212;&#29992;&#20110;&#20116;&#20010;&#20998;&#35299;&#20219;&#21153;&#65292;&#19968;&#20010;&#26159;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#21478;&#22806;&#22235;&#20010;&#26469;&#33258;&#23454;&#38469;&#26174;&#24494;&#38236;&#25968;&#25454;&#12290;LC&#23454;&#29616;&#20102;SOTA&#30340;&#32467;&#26524;&#65288;&#24179;&#22343;im&#65289;
&lt;/p&gt;
&lt;p&gt;
We present uSplit, a dedicated approach for trained image decomposition in the context of fluorescence microscopy images. We find that best results using regular deep architectures are achieved when large image patches are used during training, making memory consumption the limiting factor to further improving performance. We therefore introduce lateral contextualization (LC), a memory efficient way to train powerful networks and show that LC leads to consistent and significant improvements on the task at hand. We integrate LC with U-Nets, Hierarchical AEs, and Hierarchical VAEs, for which we formulate a modified ELBO loss. Additionally, LC enables training deeper hierarchical models than otherwise possible and, interestingly, helps to reduce tiling artefacts that are inherently impossible to avoid when using tiled VAE predictions. We apply uSplit to five decomposition tasks, one on a synthetic dataset, four others derived from real microscopy data. LC achieves SOTA results (average im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27867;&#21270;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#35270;&#35273;&#20027;&#24178;&#32593;&#32476;&#65292;&#36890;&#36807;&#20248;&#20808;&#35753;&#27169;&#22411;&#23398;&#20064;&#8220;&#26356;&#23481;&#26131;&#23398;&#20064;&#8221;&#30340;&#27169;&#24335;&#65292;&#19981;&#26029;&#24341;&#20837;&#26356;&#38590;&#30340;&#27169;&#24335;&#65292;&#20174;&#32780;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2211.09703</link><description>&lt;p&gt;
&#39640;&#25928;&#35757;&#32451;&#65306;&#25506;&#32034;&#27867;&#21270;&#35838;&#31243;&#23398;&#20064;&#26469;&#35757;&#32451;&#35270;&#35273;&#20027;&#24178;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
EfficientTrain: Exploring Generalized Curriculum Learning for Training Visual Backbones. (arXiv:2211.09703v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27867;&#21270;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#35270;&#35273;&#20027;&#24178;&#32593;&#32476;&#65292;&#36890;&#36807;&#20248;&#20808;&#35753;&#27169;&#22411;&#23398;&#20064;&#8220;&#26356;&#23481;&#26131;&#23398;&#20064;&#8221;&#30340;&#27169;&#24335;&#65292;&#19981;&#26029;&#24341;&#20837;&#26356;&#38590;&#30340;&#27169;&#24335;&#65292;&#20174;&#32780;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#32593;&#32476;&#30340;&#21331;&#36234;&#24615;&#33021;&#36890;&#24120;&#20276;&#38543;&#30528;&#26114;&#36149;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#35270;&#35273;&#20027;&#24178;&#32593;&#32476;&#65288;&#20363;&#22914;&#35270;&#35273;Transformer&#65289;&#12290;&#26412;&#25991;&#21551;&#21457;&#20110;&#28145;&#24230;&#32593;&#32476;&#30340;&#20869;&#22312;&#23398;&#20064;&#21160;&#21147;&#23398;&#65306;&#25105;&#20204;&#23454;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#22312;&#36739;&#26089;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#27169;&#22411;&#20027;&#35201;&#23398;&#20064;&#22312;&#27599;&#20010;&#31034;&#20363;&#20013;&#35782;&#21035;&#19968;&#20123;&#8220;&#26356;&#23481;&#26131;&#23398;&#20064;&#8221;&#30340;&#21028;&#21035;&#27169;&#24335;&#65292;&#20363;&#22914;&#22270;&#20687;&#30340;&#20302;&#39057;&#25104;&#20998;&#21644;&#25968;&#25454;&#22686;&#24191;&#20043;&#21069;&#30340;&#21407;&#22987;&#20449;&#24687;&#12290;&#22522;&#20110;&#27492;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#65292;&#20854;&#20013;&#27169;&#22411;&#24635;&#26159;&#22312;&#27599;&#20010;&#26102;&#26399;&#21033;&#29992;&#25152;&#26377;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#35838;&#31243;&#22987;&#20110;&#20165;&#26292;&#38706;&#27599;&#20010;&#31034;&#20363;&#30340;&#8220;&#26356;&#23481;&#26131;&#23398;&#20064;&#8221;&#30340;&#27169;&#24335;&#65292;&#24182;&#36880;&#28176;&#24341;&#20837;&#26356;&#38590;&#30340;&#27169;&#24335;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;1&#65289;&#22312;&#36755;&#20837;&#30340;&#20613;&#37324;&#21494;&#35889;&#20013;&#24341;&#20837;&#19968;&#20010;&#35009;&#21098;&#25805;&#20316;&#65292;&#20351;&#27169;&#22411;&#21482;&#33021;&#20174;&#20302;&#39057;&#32452;&#20998;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The superior performance of modern deep networks usually comes with a costly training procedure. This paper presents a new curriculum learning approach for the efficient training of visual backbones (e.g., vision Transformers). Our work is inspired by the inherent learning dynamics of deep networks: we experimentally show that at an earlier training stage, the model mainly learns to recognize some 'easier-to-learn' discriminative patterns within each example, e.g., the lower-frequency components of images and the original information before data augmentation. Driven by this phenomenon, we propose a curriculum where the model always leverages all the training data at each epoch, while the curriculum starts with only exposing the 'easier-to-learn' patterns of each example, and introduces gradually more difficult patterns. To implement this idea, we 1) introduce a cropping operation in the Fourier spectrum of the inputs, which enables the model to learn from only the lower-frequency compo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#24418;&#24335;&#65288;epsilon-SupInfoNCE&#65289;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#21435;&#20559;&#27491;&#21017;&#21270;&#25439;&#22833;&#65288;FairKL&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#20174;&#26377;&#20559;&#25968;&#25454;&#20013;&#23398;&#20064;&#26080;&#20559;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.05568</link><description>&lt;p&gt;
&#26080;&#20559;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unbiased Supervised Contrastive Learning. (arXiv:2211.05568v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#24418;&#24335;&#65288;epsilon-SupInfoNCE&#65289;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#21435;&#20559;&#27491;&#21017;&#21270;&#25439;&#22833;&#65288;FairKL&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#20174;&#26377;&#20559;&#25968;&#25454;&#20013;&#23398;&#20064;&#26080;&#20559;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#25968;&#25454;&#38598;&#23384;&#22312;&#20559;&#24046;&#65292;&#21363;&#23427;&#20204;&#21253;&#21547;&#20165;&#22312;&#25968;&#25454;&#38598;&#20013;&#19982;&#30446;&#26631;&#31867;&#39640;&#24230;&#30456;&#20851;&#30340;&#26131;&#20110;&#23398;&#20064;&#30340;&#29305;&#24449;&#65292;&#20294;&#19981;&#22312;&#30495;&#23454;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#12290;&#22240;&#27492;&#65292;&#20174;&#26377;&#20559;&#25968;&#25454;&#20013;&#23398;&#20064;&#26080;&#20559;&#27169;&#22411;&#24050;&#25104;&#20026;&#36817;&#24180;&#26469;&#38750;&#24120;&#30456;&#20851;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#23398;&#20064;&#23545;&#20559;&#24046;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#34920;&#24449;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#28548;&#28165;&#20026;&#20160;&#20040;&#26368;&#36817;&#30340;&#23545;&#27604;&#25439;&#22833;&#65288;InfoNCE&#65292;SupCon&#31561;&#65289;&#22312;&#22788;&#29702;&#20559;&#24046;&#25968;&#25454;&#26102;&#21487;&#33021;&#22833;&#36133;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#24418;&#24335;&#65288;epsilon-SupInfoNCE&#65289;&#65292;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#27491;&#36127;&#26679;&#26412;&#20043;&#38388;&#26368;&#23567;&#36317;&#31163;&#30340;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;FairKL&#65292;&#19968;&#31181;&#26032;&#30340;&#21435;&#20559;&#27491;&#21017;&#21270;&#25439;&#22833;&#65292;&#21363;&#20351;&#22312;&#26497;&#24230;&#20559;&#24046;&#30340;&#25968;&#25454;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#24456;&#22909;&#22320;&#24037;&#20316;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many datasets are biased, namely they contain easy-to-learn features that are highly correlated with the target class only in the dataset but not in the true underlying distribution of the data. For this reason, learning unbiased models from biased data has become a very relevant research topic in the last years. In this work, we tackle the problem of learning representations that are robust to biases. We first present a margin-based theoretical framework that allows us to clarify why recent contrastive losses (InfoNCE, SupCon, etc.) can fail when dealing with biased data. Based on that, we derive a novel formulation of the supervised contrastive loss (epsilon-SupInfoNCE), providing more accurate control of the minimal distance between positive and negative samples. Furthermore, thanks to our theoretical framework, we also propose FairKL, a new debiasing regularization loss, that works well even with extremely biased data. We validate the proposed losses on standard vision datasets inc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#23610;&#24230;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#24322;&#36136;&#39640;&#33021;&#26448;&#26009;&#22312;&#38663;&#29190;&#36716;&#21464;&#36807;&#31243;&#20013;&#30340;&#33021;&#37327;&#37322;&#25918;&#21644;&#28789;&#25935;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#29289;&#29702;&#23398;&#30693;&#35782;&#35757;&#32451;&#20102;&#24490;&#29615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#20934;&#30830;&#23450;&#20301;&#24494;&#35266;&#32467;&#26500;&#20869;&#30340;&#28909;&#28857;&#28857;&#28779;&#21644;&#29983;&#38271;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.04561</link><description>&lt;p&gt;
&#19968;&#31181;&#24212;&#29992;&#29289;&#29702;&#23398;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22810;&#23610;&#24230;&#38663;&#29190;&#36716;&#21464;&#20013;&#20934;&#30830;&#23450;&#20301;&#33021;&#37327;&#37322;&#25918;&#30340;&#24322;&#36136;&#39640;&#33021;&#26448;&#26009;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A physics-aware deep learning model for energy localization in multiscale shock-to-detonation simulations of heterogeneous energetic materials. (arXiv:2211.04561v2 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#23610;&#24230;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#24322;&#36136;&#39640;&#33021;&#26448;&#26009;&#22312;&#38663;&#29190;&#36716;&#21464;&#36807;&#31243;&#20013;&#30340;&#33021;&#37327;&#37322;&#25918;&#21644;&#28789;&#25935;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#29289;&#29702;&#23398;&#30693;&#35782;&#35757;&#32451;&#20102;&#24490;&#29615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#20934;&#30830;&#23450;&#20301;&#24494;&#35266;&#32467;&#26500;&#20869;&#30340;&#28909;&#28857;&#28857;&#28779;&#21644;&#29983;&#38271;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#24322;&#36136;&#39640;&#33021;&#26448;&#26009;&#22312;&#38663;&#29190;&#36716;&#21464;&#36807;&#31243;&#20013;&#30340;&#22797;&#26434;&#28909;&#21147;&#23398;&#29305;&#24615;&#65292;&#38656;&#35201;&#20934;&#30830;&#22320;&#25429;&#33719;&#20854;&#23439;&#35266;&#21709;&#24212;&#21644;&#20122;&#32593;&#26684;&#20171;&#35266;&#33021;&#37327;&#23450;&#20301;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#32780;&#20934;&#30830;&#30340;&#22810;&#23610;&#24230;&#38663;&#29190;&#36716;&#21464;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23545;&#38663;&#29190;&#36716;&#21464;&#20013;&#24494;&#35266;&#32467;&#26500;&#30340;&#20171;&#35266;&#33021;&#37327;&#23450;&#20301;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#21033;&#29992;&#26377;&#38480;&#20803;&#26041;&#27861;&#24314;&#31435;&#23439;&#35266;&#23610;&#24230;&#30340;&#21709;&#24212;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#20445;&#25345;&#39640;&#31934;&#24230;&#65292;&#20934;&#30830;&#22320;&#25429;&#33719;&#24322;&#36136;&#39640;&#33021;&#26448;&#26009;&#30340;&#20171;&#35266;&#23610;&#24230;&#33021;&#37327;&#23450;&#20301;&#21644;&#23439;&#35266;&#23610;&#24230;&#38663;&#29190;&#36716;&#21464;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive simulations of the shock-to-detonation transition (SDT) in heterogeneous energetic materials (EM) are vital to the design and control of their energy release and sensitivity. Due to the complexity of the thermo-mechanics of EM during the SDT, both macro-scale response and sub-grid mesoscale energy localization must be captured accurately. This work proposes an efficient and accurate multiscale framework for SDT simulations of EM. We introduce a new approach for SDT simulation by using deep learning to model the mesoscale energy localization of shock-initiated EM microstructures. The proposed multiscale modeling framework is divided into two stages. First, a physics-aware recurrent convolutional neural network (PARC) is used to model the mesoscale energy localization of shock-initiated heterogeneous EM microstructures. PARC is trained using direct numerical simulations (DNS) of hotspot ignition and growth within microstructures of pressed HMX material subjected to different i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GPTQ&#30340;&#26032;&#22411;&#19968;&#27425;&#24615;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#39640;&#24230;&#20934;&#30830;&#21644;&#39640;&#24230;&#26377;&#25928;&#30340;&#21516;&#26102;&#23558;&#27604;&#29305;&#23485;&#24230;&#38477;&#33267;&#27599;&#20010;&#26435;&#37325;3&#25110;4&#20301;&#65292;&#36866;&#29992;&#20110;&#24040;&#22823;&#30340;GPT&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.17323</link><description>&lt;p&gt;
GPTQ: &#38754;&#21521;&#29983;&#25104;&#39044;&#35757;&#32451;transformer&#30340;&#20934;&#30830;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers. (arXiv:2210.17323v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GPTQ&#30340;&#26032;&#22411;&#19968;&#27425;&#24615;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#39640;&#24230;&#20934;&#30830;&#21644;&#39640;&#24230;&#26377;&#25928;&#30340;&#21516;&#26102;&#23558;&#27604;&#29305;&#23485;&#24230;&#38477;&#33267;&#27599;&#20010;&#26435;&#37325;3&#25110;4&#20301;&#65292;&#36866;&#29992;&#20110;&#24040;&#22823;&#30340;GPT&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#39044;&#35757;&#32451;transformer&#27169;&#22411;&#20197;&#20854;&#22312;&#22797;&#26434;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#30340;&#31361;&#30772;&#24615;&#34920;&#29616;&#21644;&#26497;&#39640;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#20108;&#38454;&#20449;&#24687;&#30340;&#26032;&#22411;&#19968;&#27425;&#24615;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;GPTQ&#65292;&#21487;&#20197;&#22312;&#32422;&#22235;&#20010;GPU&#23567;&#26102;&#20869;&#23558;GPT&#27169;&#22411;&#37327;&#21270;&#20026;&#27599;&#20010;&#26435;&#37325;3&#25110;4&#27604;&#29305;&#65292;&#31934;&#24230;&#20960;&#20046;&#27809;&#26377;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-trained Transformer models, known as GPT or OPT, set themselves apart through breakthrough performance across complex language modelling tasks, but also by their extremely high computational and storage costs. Specifically, due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs, which limits the usability of such models. While there is emerging work on relieving this pressure via model compression, the applicability and performance of existing compression techniques is limited by the scale and complexity of GPT models. In this paper, we address this challenge, and propose GPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3 or 4 bits per weight, with negligible accuracy degradation rel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#24515;&#29702;&#27979;&#37327;&#23398;&#39046;&#22495;&#26368;&#36817;&#24320;&#21457;&#30340;&#25506;&#32034;&#24615;&#22270;&#20998;&#26512;&#65288;EGA&#65289;&#21644;&#21807;&#19968;&#21464;&#37327;&#20998;&#26512;&#65288;UVA&#65289;&#31561;&#20004;&#31181;&#38477;&#32500;&#25216;&#26415;&#19982;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#24120;&#35265;&#30340;&#20004;&#31181;&#38477;&#32500;&#25216;&#26415;&#65288;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65289;&#22312;&#20943;&#23569;&#25968;&#25454;&#32500;&#24230;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#38477;&#32500;&#21487;&#33021;&#20250;&#38477;&#20302;&#12289;&#25552;&#39640;&#25110;&#32773;&#19981;&#24433;&#21709;&#21464;&#37327;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.13230</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#38477;&#32500;&#26041;&#27861;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#23454;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Experimental Study of Dimension Reduction Methods on Machine Learning Algorithms with Applications to Psychometrics. (arXiv:2210.13230v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#24515;&#29702;&#27979;&#37327;&#23398;&#39046;&#22495;&#26368;&#36817;&#24320;&#21457;&#30340;&#25506;&#32034;&#24615;&#22270;&#20998;&#26512;&#65288;EGA&#65289;&#21644;&#21807;&#19968;&#21464;&#37327;&#20998;&#26512;&#65288;UVA&#65289;&#31561;&#20004;&#31181;&#38477;&#32500;&#25216;&#26415;&#19982;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#24120;&#35265;&#30340;&#20004;&#31181;&#38477;&#32500;&#25216;&#26415;&#65288;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65289;&#22312;&#20943;&#23569;&#25968;&#25454;&#32500;&#24230;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#38477;&#32500;&#21487;&#33021;&#20250;&#38477;&#20302;&#12289;&#25552;&#39640;&#25110;&#32773;&#19981;&#24433;&#21709;&#21464;&#37327;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#20010;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#38477;&#32500;&#25216;&#26415;&#26159;&#25968;&#25454;&#31185;&#23398;&#23478;&#24320;&#21457;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#31181;&#38477;&#32500;&#25216;&#26415;&#65292;&#21253;&#25324;&#24515;&#29702;&#27979;&#37327;&#23398;&#39046;&#22495;&#26368;&#36817;&#24320;&#21457;&#30340;&#25506;&#32034;&#24615;&#22270;&#20998;&#26512;&#65288;EGA&#65289;&#21644;&#21807;&#19968;&#21464;&#37327;&#20998;&#26512;&#65288;UVA&#65289;&#31561;&#20004;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;EGA&#21644;UVA&#19982;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#24120;&#35265;&#30340;&#20004;&#31181;&#38477;&#32500;&#25216;&#26415;&#65288;&#20027;&#25104;&#20998;&#20998;&#26512;&#21644;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#65289;&#20197;&#21450;&#27809;&#26377;&#38477;&#32500;&#30340;&#21464;&#37327;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#26174;&#31034;EGA&#21644;UVA&#19982;&#20854;&#20182;&#38477;&#32500;&#25216;&#26415;&#25110;&#32773;&#19981;&#38477;&#32500;&#30456;&#24403;&#12290;&#19968;&#33268;&#20110;&#21069;&#26399;&#30340;&#25991;&#29486;&#65292;&#25105;&#20204;&#21457;&#29616;&#38477;&#32500;&#21487;&#33021;&#20250;&#38477;&#20302;&#12289;&#25552;&#39640;&#25110;&#32773;&#19981;&#24433;&#21709;&#21464;&#37327;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#21021;&#27493;&#30340;&#32467;&#26524;&#21457;&#29616;&#38477;&#32500;&#24448;&#24448;&#23548;&#33268;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing interpretable machine learning models has become an increasingly important issue. One way in which data scientists have been able to develop interpretable models has been to use dimension reduction techniques. In this paper, we examine several dimension reduction techniques including two recent approaches developed in the network psychometrics literature called exploratory graph analysis (EGA) and unique variable analysis (UVA). We compared EGA and UVA with two other dimension reduction techniques common in the machine learning literature (principal component analysis and independent component analysis) as well as no reduction to the variables real data. We show that EGA and UVA perform as well as the other reduction techniques or no reduction. Consistent with previous literature, we show that dimension reduction can decrease, increase, or provide the same accuracy as no reduction of variables. Our tentative results find that dimension reduction tends to lead to better perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#32467;&#21512;&#39046;&#22495;&#30693;&#35782;&#65292;&#21487;&#20197;&#22312;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;X&#23556;&#32447;&#33639;&#20809;&#20998;&#26512;&#65292;&#36866;&#29992;&#20110;&#30719;&#23665;&#31561;&#23454;&#38469;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2210.12239</link><description>&lt;p&gt;
&#33258;&#21160;&#32534;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#30340;X&#23556;&#32447;&#33639;&#20809;&#22522;&#26412;&#21442;&#25968;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Auto-Encoder Neural Network Incorporating X-Ray Fluorescence Fundamental Parameters with Machine Learning. (arXiv:2210.12239v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#32467;&#21512;&#39046;&#22495;&#30693;&#35782;&#65292;&#21487;&#20197;&#22312;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;X&#23556;&#32447;&#33639;&#20809;&#20998;&#26512;&#65292;&#36866;&#29992;&#20110;&#30719;&#23665;&#31561;&#23454;&#38469;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#33021;&#37327;&#33394;&#25955;X&#23556;&#32447;&#33639;&#20809;&#65288;EDXRF&#65289;&#24212;&#29992;&#20013;&#22240;&#20202;&#22120;&#21442;&#25968;&#19981;&#21487;&#29992;&#32780;&#26080;&#27861;&#20351;&#29992;&#22522;&#26412;&#21442;&#25968;&#26041;&#27861;&#30340;&#24773;&#20917;&#36827;&#34892;&#30740;&#31350;&#12290;&#20363;&#22914;&#65292;&#22312;&#37319;&#30719;&#38130;&#36710;&#25110;&#20256;&#36865;&#24102;&#19978;&#65292;&#23721;&#30707;&#19981;&#26029;&#31227;&#21160;&#65288;&#23548;&#33268;&#20837;&#23556;&#35282;&#24230;&#21644;&#36317;&#31163;&#19981;&#26029;&#21464;&#21270;&#65289;&#65292;&#36824;&#21487;&#33021;&#23384;&#22312;&#20854;&#20182;&#26410;&#32771;&#34385;&#30340;&#22240;&#32032;&#65288;&#22914;&#28784;&#23576;&#65289;&#12290;&#31070;&#32463;&#32593;&#32476;&#19981;&#38656;&#35201;&#20202;&#22120;&#21644;&#22522;&#26412;&#21442;&#25968;&#65292;&#20294;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#24102;&#26377;&#20803;&#32032;&#32452;&#25104;&#26631;&#31614;&#30340;XRF&#20809;&#35889;&#65292;&#30001;&#20110;&#25104;&#26412;&#38480;&#21046;&#65292;&#36825;&#31181;&#26631;&#31614;&#25968;&#25454;&#36890;&#24120;&#24456;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#21453;&#28436;&#27491;&#28436;&#27169;&#22411;&#26469;&#21463;&#30410;&#20110;&#39046;&#22495;&#30693;&#35782;&#12290;&#27491;&#28436;&#27169;&#22411;&#20351;&#29992;&#25152;&#26377;&#20803;&#32032;&#30340;&#36291;&#36801;&#33021;&#37327;&#21644;&#27010;&#29575;&#20197;&#21450;&#21442;&#25968;&#20998;&#24067;&#26469;&#36924;&#36817;&#20854;&#20182;&#22522;&#26412;&#21644;&#20202;&#22120;&#21442;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#38146;&#30719;&#21208;&#25506;&#39033;&#30446;&#30340;&#23721;&#30707;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#35813;&#27169;&#22411;&#21644;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider energy-dispersive X-ray Fluorescence (EDXRF) applications where the fundamental parameters method is impractical such as when instrument parameters are unavailable. For example, on a mining shovel or conveyor belt, rocks are constantly moving (leading to varying angles of incidence and distances) and there may be other factors not accounted for (like dust). Neural networks do not require instrument and fundamental parameters but training neural networks requires XRF spectra labelled with elemental composition, which is often limited because of its expense. We develop a neural network model that learns from limited labelled data and also benefits from domain knowledge by learning to invert a forward model. The forward model uses transition energies and probabilities of all elements and parameterized distributions to approximate other fundamental and instrument parameters. We evaluate the model and baseline models on a rock dataset from a lithium mineral exploration project. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#27491;&#20132;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22522;&#20110;&#26368;&#22823;&#29109;&#21407;&#21017;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20445;&#35777;&#20102;&#30697;&#38453;&#30340;&#27491;&#20132;&#24615;&#21644;&#31232;&#30095;&#24615;&#20197;&#21450;&#38750;&#36127;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#24433;&#21709;&#36817;&#20284;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#36895;&#24230;&#21644;&#20248;&#20110;&#25991;&#29486;&#20013;&#31867;&#20284;&#26041;&#27861;&#30340;&#31232;&#30095;&#24615;&#12289;&#27491;&#20132;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.02672</link><description>&lt;p&gt;
&#27491;&#20132;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;:&#26368;&#22823;&#29109;&#21407;&#21017;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Orthogonal Non-negative Matrix Factorization: a Maximum-Entropy-Principle Approach. (arXiv:2210.02672v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#27491;&#20132;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22522;&#20110;&#26368;&#22823;&#29109;&#21407;&#21017;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20445;&#35777;&#20102;&#30697;&#38453;&#30340;&#27491;&#20132;&#24615;&#21644;&#31232;&#30095;&#24615;&#20197;&#21450;&#38750;&#36127;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#24433;&#21709;&#36817;&#20284;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#36895;&#24230;&#21644;&#20248;&#20110;&#25991;&#29486;&#20013;&#31867;&#20284;&#26041;&#27861;&#30340;&#31232;&#30095;&#24615;&#12289;&#27491;&#20132;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#27491;&#20132;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;ONMF&#65289;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#38382;&#39064;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20004;&#20010;&#38750;&#36127;&#30697;&#38453;&#65288;&#29305;&#24449;&#30697;&#38453;&#21644;&#28151;&#21512;&#30697;&#38453;&#65289;&#30340;&#20056;&#31215;&#26469;&#36817;&#20284;&#36755;&#20837;&#25968;&#25454;&#30697;&#38453;&#65292;&#20854;&#20013;&#19968;&#20010;&#30697;&#38453;&#26159;&#27491;&#20132;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;ONMF&#35299;&#37322;&#20026;&#29305;&#23450;&#30340;&#35774;&#26045;&#23450;&#20301;&#38382;&#39064;&#65292;&#24182;&#38024;&#23545;ONMF&#38382;&#39064;&#37319;&#29992;&#22522;&#20110;&#26368;&#22823;&#29109;&#21407;&#21017;&#30340;FLP&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#35843;&#25972;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20445;&#35777;&#20102;&#29305;&#24449;&#30697;&#38453;&#25110;&#28151;&#21512;&#30697;&#38453;&#30340;&#27491;&#20132;&#24615;&#21644;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#20004;&#32773;&#30340;&#38750;&#36127;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#23450;&#37327;&#30340;&#8220;&#30495;&#23454;&#8221;&#28508;&#22312;&#29305;&#24449;&#25968;&#37327;&#30340;&#29305;&#24449;-&#36229;&#21442;&#25968;&#29992;&#20110;ONMF&#12290;&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#20197;&#21450;&#26631;&#20934;&#30340;&#22522;&#22240;&#33455;&#29255;&#25968;&#32452;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#24433;&#21709;&#36817;&#20284;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#36739;&#22909;&#30340;&#31232;&#30095;&#24615;&#12289;&#27491;&#20132;&#24615;&#21644;&#24615;&#33021;&#36895;&#24230;&#65292;&#30456;&#23545;&#20110;&#25991;&#29486;&#20013;&#31867;&#20284;&#26041;&#27861;&#26377;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new methodology to solve the orthogonal nonnegative matrix factorization (ONMF) problem, where the objective is to approximate an input data matrix by a product of two nonnegative matrices, the features matrix and the mixing matrix, where one of them is orthogonal. We show how the ONMF can be interpreted as a specific facility-location problem (FLP), and adapt a maximum-entropy-principle based solution for FLP to the ONMF problem. The proposed approach guarantees orthogonality and sparsity of the features or the mixing matrix, while ensuring nonnegativity of both. Additionally, our methodology develops a quantitative characterization of ``true" number of underlying features - a hyperparameter required for the ONMF. An evaluation of the proposed method conducted on synthetic datasets, as well as a standard genetic microarray dataset indicates significantly better sparsity, orthogonality, and performance speed compared to similar methods in the literature, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#26631;&#20934;&#21270;&#23545;&#33041;&#30005;&#39046;&#22495;&#36866;&#24212;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#19977;&#20010;EEG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2210.01081</link><description>&lt;p&gt;
&#25968;&#25454;&#26631;&#20934;&#21270;&#22312;&#33041;&#30005;&#39046;&#22495;&#36866;&#24212;&#20013;&#30340;&#24433;&#21709;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On The Effects Of Data Normalisation For Domain Adaptation On EEG Data. (arXiv:2210.01081v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#26631;&#20934;&#21270;&#23545;&#33041;&#30005;&#39046;&#22495;&#36866;&#24212;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#19977;&#20010;EEG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#65292;&#25968;&#25454;&#38598;&#36716;&#25442;&#38382;&#39064;&#19968;&#30452;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#65292;&#19982;&#26426;&#22120;&#23398;&#20064;&#26631;&#20934;&#20551;&#35774;&#19981;&#21516;&#30340;&#26159;&#65292;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20013;&#30340;&#25968;&#25454;&#21487;&#33021;&#36981;&#24490;&#19981;&#21516;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#23548;&#33268;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#27867;&#21270;&#24615;&#33021;&#19981;&#20339;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#33041;&#26426;&#25509;&#21475;&#39046;&#22495;&#23588;&#20026;&#31361;&#20986;&#65292;&#22240;&#20026;&#29983;&#29289;&#20449;&#21495;&#22914;&#33041;&#30005;&#22270;&#20449;&#21495;&#36890;&#24120;&#34987;&#29992;&#20110;&#25968;&#25454;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#33041;&#30005;&#20449;&#21495;&#22312;&#26102;&#38388;&#21644;&#19981;&#21516;&#21463;&#35797;&#32773;&#20043;&#38388;&#37117;&#38750;&#24120;&#19981;&#31283;&#23450;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35768;&#22810;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#27861;&#37117;&#22522;&#20110;&#26368;&#36817;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#39046;&#22495;&#36866;&#24212;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25913;&#36827;&#30340;&#23454;&#38469;&#21407;&#22240;&#20173;&#28982;&#27169;&#31946;&#12290;&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#25968;&#25454;&#26631;&#20934;&#21270;&#25110;&#26631;&#20934;&#21270;&#31574;&#30053;&#22312;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#20013;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#20351;&#29992;SEED&#12289;DEAP&#21644;BCI&#31454;&#36187;IV 2a EEG&#25968;&#25454;&#38598;&#65292;&#23545;&#25968;&#25454;&#26631;&#20934;&#21270;&#31574;&#30053;&#24212;&#29992;&#20110;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#20013;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the Machine Learning (ML) literature, a well-known problem is the Dataset Shift problem where, differently from the ML standard hypothesis, the data in the training and test sets can follow different probability distributions, leading ML systems toward poor generalisation performances. This problem is intensely felt in the Brain-Computer Interface (BCI) context, where bio-signals as Electroencephalographic (EEG) are often used. In fact, EEG signals are highly non-stationary both over time and between different subjects. To overcome this problem, several proposed solutions are based on recent transfer learning approaches such as Domain Adaption (DA). In several cases, however, the actual causes of the improvements remain ambiguous. This paper focuses on the impact of data normalisation, or standardisation strategies applied together with DA methods. In particular, using \textit{SEED}, \textit{DEAP}, and \textit{BCI Competition IV 2a} EEG datasets, we experimentally evaluated the impa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#38170;&#28857;&#30340;&#23376;&#31354;&#38388;&#23398;&#20064;&#12289;&#21452;&#21521;&#22270;&#23398;&#20064;&#21644;&#31163;&#25955;&#20248;&#21270;&#23454;&#29616;&#20102;&#21333;&#35270;&#22270;&#21644;&#19968;&#33268;&#24615;&#22270;&#30340;&#32852;&#21512;&#23398;&#20064;&#65292;&#36991;&#20813;&#20102;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2209.04187</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#36807;&#32479;&#19968;&#31163;&#25955;&#30340;&#21452;&#21521;&#22270;&#23398;&#20064;&#23454;&#29616;&#39640;&#25928;&#22810;&#35270;&#22270;&#32858;&#31867;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Multi-view Clustering via Unified and Discrete Bipartite Graph Learning. (arXiv:2209.04187v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#38170;&#28857;&#30340;&#23376;&#31354;&#38388;&#23398;&#20064;&#12289;&#21452;&#21521;&#22270;&#23398;&#20064;&#21644;&#31163;&#25955;&#20248;&#21270;&#23454;&#29616;&#20102;&#21333;&#35270;&#22270;&#21644;&#19968;&#33268;&#24615;&#22270;&#30340;&#32852;&#21512;&#23398;&#20064;&#65292;&#36991;&#20813;&#20102;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20197;&#24448;&#22522;&#20110;&#22270;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#31639;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#20173;&#28982;&#38754;&#20020;&#19977;&#20010;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#24448;&#24448;&#36973;&#21463;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#22256;&#25200;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#36890;&#24120;&#22312;&#21333;&#35270;&#22270;&#32423;&#21035;&#25110;&#35270;&#22270;&#19968;&#33268;&#24615;&#32423;&#21035;&#19978;&#25191;&#34892;&#22270;&#23398;&#20064;&#65292;&#20294;&#24120;&#24120;&#24573;&#30053;&#20102;&#21333;&#35270;&#22270;&#21644;&#19968;&#33268;&#24615;&#22270;&#30340;&#32852;&#21512;&#23398;&#20064;&#21487;&#33021;&#24615;&#12290;&#31532;&#19977;&#65292;&#35768;&#22810;&#31639;&#27861;&#20381;&#36182;&#20110;k-means&#23545;&#35889;&#23884;&#20837;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#20294;&#32570;&#20047;&#30452;&#25509;&#23398;&#20064;&#24102;&#31163;&#25955;&#32858;&#31867;&#32467;&#26500;&#30340;&#22270;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32479;&#19968;&#31163;&#25955;&#30340;&#21452;&#21521;&#22270;&#23398;&#20064;&#23454;&#29616;&#39640;&#25928;&#22810;&#35270;&#22270;&#32858;&#31867;&#30340;&#26041;&#27861;&#65288;UDBGL&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#22522;&#20110;&#38170;&#28857;&#30340;&#23376;&#31354;&#38388;&#23398;&#20064;&#65292;&#20174;&#22810;&#20010;&#35270;&#22270;&#20013;&#23398;&#20064;&#20102;&#35270;&#22270;&#29305;&#23450;&#30340;&#21452;&#21521;&#22270;&#65292;&#28982;&#21518;&#21033;&#29992;&#21452;&#21521;&#22270;&#34701;&#21512;&#23398;&#20064;&#20102;&#35270;&#22270;&#19968;&#33268;&#24615;&#21452;&#21521;&#22270;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;UDBGL&#36890;&#36807;&#37319;&#29992;&#20302;&#31209;&#30697;&#38453;&#36817;&#20284;&#21644;&#21152;&#36895;&#20132;&#26367;&#26041;&#21521;&#20056;&#25968;&#27861;&#36991;&#20813;&#20102;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#32479;&#19968;&#30446;&#26631;&#20989;&#25968;&#65292;&#23427;&#23454;&#29616;&#20102;&#21333;&#35270;&#22270;&#21644;&#19968;&#33268;&#24615;&#22270;&#30340;&#32852;&#21512;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#35299;&#20915;&#31532;&#19977;&#20010;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;&#65292;&#30452;&#25509;&#23398;&#20064;&#25152;&#23398;&#22270;&#30340;&#31163;&#25955;&#32858;&#31867;&#32467;&#26500;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UDBGL&#20248;&#20110;&#20960;&#31181;&#26368;&#26032;&#30340;&#22810;&#35270;&#22270;&#32858;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although previous graph-based multi-view clustering algorithms have gained significant progress, most of them are still faced with three limitations. First, they often suffer from high computational complexity, which restricts their applications in large-scale scenarios. Second, they usually perform graph learning either at the single-view level or at the view-consensus level, but often neglect the possibility of the joint learning of single-view and consensus graphs. Third, many of them rely on the k-means for discretization of the spectral embeddings, which lack the ability to directly learn the graph with discrete cluster structure. In light of this, this paper presents an efficient multi-view clustering approach via unified and discrete bipartite graph learning (UDBGL). Specifically, the anchor-based subspace learning is incorporated to learn the view-specific bipartite graphs from multiple views, upon which the bipartite graph fusion is leveraged to learn a view-consensus bipartit
&lt;/p&gt;</description></item><item><title>&#38463;&#23572;&#20271;&#22612;&#35745;&#21010;&#26159;&#19968;&#39033;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#35745;&#21010;&#65292;&#22312;&#38463;&#23572;&#20271;&#22612;&#30340;&#30740;&#31350;&#22242;&#38431;&#21450;&#20840;&#29699;&#24535;&#21516;&#36947;&#21512;&#30340;&#20154;&#22763;&#20013;&#36827;&#34892;&#65292;&#26088;&#22312;&#25506;&#32034;AI&#30340;&#30740;&#31350;&#26041;&#27861;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2208.11173</link><description>&lt;p&gt;
&#38463;&#23572;&#20271;&#22612;AI&#30740;&#31350;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
The Alberta Plan for AI Research. (arXiv:2208.11173v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11173
&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#20271;&#22612;&#35745;&#21010;&#26159;&#19968;&#39033;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#35745;&#21010;&#65292;&#22312;&#38463;&#23572;&#20271;&#22612;&#30340;&#30740;&#31350;&#22242;&#38431;&#21450;&#20840;&#29699;&#24535;&#21516;&#36947;&#21512;&#30340;&#20154;&#22763;&#20013;&#36827;&#34892;&#65292;&#26088;&#22312;&#25506;&#32034;AI&#30340;&#30740;&#31350;&#26041;&#27861;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#38463;&#23572;&#20271;&#22612;&#35745;&#21010;&#12290;&#38463;&#23572;&#20271;&#22612;&#35745;&#21010;&#22312;&#38463;&#23572;&#20271;&#22612;&#30340;&#30740;&#31350;&#22242;&#38431;&#20197;&#21450;&#19990;&#30028;&#21508;&#22320;&#24535;&#21516;&#36947;&#21512;&#30340;&#20154;&#22763;&#20013;&#24320;&#23637;&#12290;&#25105;&#20204;&#27426;&#36814;&#25152;&#26377;&#24895;&#24847;&#21152;&#20837;&#25105;&#20204;&#36825;&#19968;&#36861;&#27714;&#30340;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
Herein we describe our approach to artificial intelligence research, which we call the Alberta Plan. The Alberta Plan is pursued within our research groups in Alberta and by others who are like minded throughout the world. We welcome all who would join us in this pursuit.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36328;&#24230;&#38598;&#20013;&#37319;&#26679;&#65288;CCS&#65289;&#30340;&#26032;&#22411;&#37319;&#26679;&#31574;&#30053;&#65292;&#21487;&#25552;&#20379;&#39069;&#22806;&#30340;&#28789;&#27963;&#24615;&#26469;&#33410;&#30465;&#30495;&#23454;&#24212;&#29992;&#20013;&#30340;&#37319;&#26679;&#25104;&#26412;&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;CCS&#30697;&#38453;&#34917;&#20840;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38750;&#20984;&#31639;&#27861;ICURC&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;CCS&#21644;ICURC&#20248;&#20110;&#22343;&#21248;&#37319;&#26679;&#21644;&#22522;&#32447;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.09723</link><description>&lt;p&gt;
&#22522;&#20110;&#36328;&#24230;&#38598;&#20013;&#37319;&#26679;&#30340;&#30697;&#38453;&#34917;&#20840;&#65306;&#36830;&#25509;&#22343;&#21248;&#37319;&#26679;&#21644;CUR&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Matrix Completion with Cross-Concentrated Sampling: Bridging Uniform Sampling and CUR Sampling. (arXiv:2208.09723v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36328;&#24230;&#38598;&#20013;&#37319;&#26679;&#65288;CCS&#65289;&#30340;&#26032;&#22411;&#37319;&#26679;&#31574;&#30053;&#65292;&#21487;&#25552;&#20379;&#39069;&#22806;&#30340;&#28789;&#27963;&#24615;&#26469;&#33410;&#30465;&#30495;&#23454;&#24212;&#29992;&#20013;&#30340;&#37319;&#26679;&#25104;&#26412;&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;CCS&#30697;&#38453;&#34917;&#20840;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38750;&#20984;&#31639;&#27861;ICURC&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;CCS&#21644;ICURC&#20248;&#20110;&#22343;&#21248;&#37319;&#26679;&#21644;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30697;&#38453;&#34917;&#20840;&#39046;&#22495;&#65292;&#34429;&#28982;&#22343;&#21248;&#37319;&#26679;&#24471;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;CUR&#37319;&#26679;&#36890;&#36807;&#34892;&#21644;&#21015;&#26679;&#26412;&#36924;&#36817;&#20302;&#31209;&#30697;&#38453;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#36825;&#20004;&#31181;&#37319;&#26679;&#27169;&#22411;&#37117;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#26131;&#20110;&#23454;&#29616;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#31216;&#20026;&#36328;&#24230;&#38598;&#20013;&#37319;&#26679;&#65288;CCS&#65289;&#12290;&#36890;&#36807;&#36830;&#25509;&#22343;&#21248;&#37319;&#26679;&#21644;CUR&#37319;&#26679;&#65292;CCS&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#28789;&#27963;&#24615;&#65292;&#26377;&#21487;&#33021;&#22312;&#24212;&#29992;&#20013;&#33410;&#30465;&#37319;&#26679;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;CCS&#22411;&#30697;&#38453;&#34917;&#20840;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38750;&#20984;&#31639;&#27861;&#65292;&#31216;&#20026;&#36845;&#20195;CUR&#34917;&#20840;&#65288;ICURC&#65289;&#65292;&#29992;&#20110;&#25152;&#25552;&#20986;&#30340;CCS&#27169;&#22411;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;CCS&#21644;ICURC&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20110;&#22343;&#21248;&#37319;&#26679;&#21450;&#20854;&#22522;&#32447;&#31639;&#27861;&#30340;&#23454;&#38469;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
While uniform sampling has been widely studied in the matrix completion literature, CUR sampling approximates a low-rank matrix via row and column samples. Unfortunately, both sampling models lack flexibility for various circumstances in real-world applications. In this work, we propose a novel and easy-to-implement sampling strategy, coined Cross-Concentrated Sampling (CCS). By bridging uniform sampling and CUR sampling, CCS provides extra flexibility that can potentially save sampling costs in applications. In addition, we also provide a sufficient condition for CCS-based matrix completion. Moreover, we propose a highly efficient non-convex algorithm, termed Iterative CUR Completion (ICURC), for the proposed CCS model. Numerical experiments verify the empirical advantages of CCS and ICURC against uniform sampling and its baseline algorithms, on both synthetic and real-world datasets.
&lt;/p&gt;</description></item><item><title>Laser &#26159;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#20195;&#29702;&#34920;&#31034;&#23398;&#20064;&#30340;&#20272;&#35745;&#38271;&#26399;&#22240;&#26524;&#25928;&#24212;&#30340;&#28789;&#27963;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20195;&#29702;&#21644;&#20854;&#20195;&#29702;&#28151;&#21512;&#22312;&#19968;&#36215;&#30340;&#30495;&#23454;&#19990;&#30028;&#24773;&#26223;&#20013;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2208.04589</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#20195;&#29702;&#34920;&#31034;&#23398;&#20064;&#30340;&#38271;&#26399;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Long-term Causal Effects Estimation via Latent Surrogates Representation Learning. (arXiv:2208.04589v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04589
&lt;/p&gt;
&lt;p&gt;
Laser &#26159;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#20195;&#29702;&#34920;&#31034;&#23398;&#20064;&#30340;&#20272;&#35745;&#38271;&#26399;&#22240;&#26524;&#25928;&#24212;&#30340;&#28789;&#27963;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20195;&#29702;&#21644;&#20854;&#20195;&#29702;&#28151;&#21512;&#22312;&#19968;&#36215;&#30340;&#30495;&#23454;&#19990;&#30028;&#24773;&#26223;&#20013;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#65292;&#22914;&#33829;&#38144;&#21644;&#21307;&#23398;&#20013;&#65292;&#22522;&#20110;&#30701;&#26399;&#20195;&#29702;&#26469;&#20272;&#35745;&#38271;&#26399;&#22240;&#26524;&#25928;&#24212;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#26576;&#20123;&#39046;&#22495;&#20013;&#24050;&#26377;&#25152;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20197;&#19968;&#31181;&#29702;&#24819;&#21270;&#21644;&#31616;&#21270;&#30340;&#26041;&#24335;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24573;&#30053;&#20102;&#30701;&#26399;&#32467;&#26524;&#20043;&#38388;&#30340;&#22240;&#26524;&#32467;&#26500;&#65292;&#24182;&#23558;&#23427;&#20204;&#20840;&#37096;&#35270;&#20026;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#27861;&#24456;&#22909;&#22320;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#26223;&#65292;&#20854;&#20013;&#23616;&#37096;&#35266;&#23519;&#21040;&#30340;&#20195;&#29702;&#19982;&#23427;&#20204;&#22312;&#30701;&#26399;&#32467;&#26524;&#20013;&#30340;&#20195;&#29702;&#28151;&#21512;&#22312;&#19968;&#36215;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Laser&#65292;&#20197;&#22312;&#26356;&#29616;&#23454;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#38271;&#26399;&#22240;&#26524;&#25928;&#24212;&#65292;&#20854;&#20013;&#35266;&#23519;&#21040;&#20195;&#29702;&#25110;&#20855;&#26377;&#35266;&#23519;&#20195;&#29702;&#12290;&#37492;&#20110;&#20195;&#29702;&#21644;&#20195;&#29702;&#20043;&#38388;&#30340;&#19981;&#21487;&#21306;&#20998;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#21487;&#35782;&#21035;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;iVAE&#65289;&#22312;&#19981;&#38656;&#35201;&#21306;&#20998;&#35266;&#23519;&#21040;&#30340;&#20195;&#29702;&#25110;&#20808;&#39564;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#25152;&#26377;&#26377;&#25928;&#20195;&#29702;&#20505;&#36873;&#32773;&#19978;&#30340;&#25972;&#20010;&#26377;&#25928;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating long-term causal effects based on short-term surrogates is a significant but challenging problem in many real-world applications, e.g., marketing and medicine. Despite its success in certain domains, most existing methods estimate causal effects in an idealistic and simplistic way - ignoring the causal structure among short-term outcomes and treating all of them as surrogates. However, such methods cannot be well applied to real-world scenarios, in which the partially observed surrogates are mixed with their proxies among short-term outcomes. To this end, we develop our flexible method, Laser, to estimate long-term causal effects in the more realistic situation that the surrogates are observed or have observed proxies.Given the indistinguishability between the surrogates and proxies, we utilize identifiable variational auto-encoder (iVAE) to recover the whole valid surrogates on all the surrogates candidates without the need of distinguishing the observed surrogates or the p
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#39640;&#25928;&#21576;&#29616;&#31070;&#32463;&#29616;&#22330;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32441;&#29702;&#22810;&#36793;&#24418;&#32780;&#19981;&#26159;&#22522;&#20110;&#23556;&#32447;&#34892;&#36827;&#30340;&#20307;&#31215;&#28210;&#26579;&#31639;&#27861;&#65292;&#24182;&#19988;&#21033;&#29992;&#20256;&#32479;&#28210;&#26579;&#31649;&#32447;&#20013;&#30340; z-&#32531;&#20914;&#22120;&#65292;&#20351;&#24471; NeRF &#21487;&#20197;&#36890;&#36807;&#20687;&#32032;&#32423;&#24182;&#34892;&#24615;&#26469;&#23454;&#29616;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#20132;&#20114;&#24335;&#24103;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2208.00277</link><description>&lt;p&gt;
MobileNeRF&#65306;&#21033;&#29992;&#22810;&#36793;&#24418;&#20809;&#26629;&#21270;&#31649;&#32447;&#22312;&#31227;&#21160;&#26550;&#26500;&#19978;&#39640;&#25928;&#21576;&#29616;&#31070;&#32463;&#29616;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures. (arXiv:2208.00277v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00277
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#39640;&#25928;&#21576;&#29616;&#31070;&#32463;&#29616;&#22330;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32441;&#29702;&#22810;&#36793;&#24418;&#32780;&#19981;&#26159;&#22522;&#20110;&#23556;&#32447;&#34892;&#36827;&#30340;&#20307;&#31215;&#28210;&#26579;&#31639;&#27861;&#65292;&#24182;&#19988;&#21033;&#29992;&#20256;&#32479;&#28210;&#26579;&#31649;&#32447;&#20013;&#30340; z-&#32531;&#20914;&#22120;&#65292;&#20351;&#24471; NeRF &#21487;&#20197;&#36890;&#36807;&#20687;&#32032;&#32423;&#24182;&#34892;&#24615;&#26469;&#23454;&#29616;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#20132;&#20114;&#24335;&#24103;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#23637;&#31034;&#20102;&#20174;&#26032;&#39062;&#30340;&#35270;&#35282;&#21512;&#25104;3D&#22330;&#26223;&#22270;&#20687;&#30340;&#24778;&#20154;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#22522;&#20110;&#23556;&#32447;&#34892;&#36827;&#30340;&#19987;&#19994;&#20307;&#31215;&#28210;&#26579;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#19982;&#24191;&#27867;&#37096;&#32626;&#30340;&#22270;&#24418;&#30828;&#20214;&#30340;&#33021;&#21147;&#19981;&#21305;&#37197;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32441;&#29702;&#22810;&#36793;&#24418;&#30340;&#26032;&#22411;NeRF&#34920;&#31034;&#27861;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#28210;&#26579;&#31649;&#32447;&#39640;&#25928;&#22320;&#21512;&#25104;&#26032;&#30340;&#22270;&#20687;&#12290;&#23558;NeRF&#34920;&#31034;&#20026;&#19968;&#32452;&#22810;&#36793;&#24418;&#65292;&#32441;&#29702;&#34920;&#31034;&#20108;&#20803;&#19981;&#36879;&#26126;&#24230;&#21644;&#29305;&#24449;&#21521;&#37327;&#12290;&#20351;&#29992;z-&#32531;&#20914;&#22120;&#20256;&#32479;&#21576;&#29616;&#22810;&#36793;&#24418;&#65292;&#21487;&#20197;&#22312;&#27599;&#20010;&#20687;&#32032;&#22788;&#33719;&#24471;&#29305;&#24449;&#30340;&#22270;&#20687;&#65292;&#36825;&#20123;&#29305;&#24449;&#30001;&#23567;&#22411;&#12289;&#35270;&#22270;&#30456;&#20851;&#30340;MLP&#22312;&#29255;&#27573;&#30528;&#33394;&#22120;&#20013;&#35299;&#37322;&#20197;&#29983;&#25104;&#26368;&#32456;&#30340;&#20687;&#32032;&#39068;&#33394;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;NeRF&#21487;&#20197;&#20351;&#29992;&#20256;&#32479;&#30340;&#22810;&#36793;&#24418;&#20809;&#26629;&#21270;&#31649;&#32447;&#36827;&#34892;&#21576;&#29616;&#65292;&#36825;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30340;&#20687;&#32032;&#32423;&#24182;&#34892;&#24615;&#65292;&#22312;&#21253;&#25324;&#31227;&#21160;&#30005;&#35805;&#22312;&#20869;&#30340;&#21508;&#31181;&#35745;&#31639;&#24179;&#21488;&#19978;&#23454;&#29616;&#20132;&#20114;&#24335;&#24103;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Fields (NeRFs) have demonstrated amazing ability to synthesize images of 3D scenes from novel views. However, they rely upon specialized volumetric rendering algorithms based on ray marching that are mismatched to the capabilities of widely deployed graphics hardware. This paper introduces a new NeRF representation based on textured polygons that can synthesize novel images efficiently with standard rendering pipelines. The NeRF is represented as a set of polygons with textures representing binary opacities and feature vectors. Traditional rendering of the polygons with a z-buffer yields an image with features at every pixel, which are interpreted by a small, view-dependent MLP running in a fragment shader to produce a final pixel color. This approach enables NeRFs to be rendered with the traditional polygon rasterization pipeline, which provides massive pixel-level parallelism, achieving interactive frame rates on a wide range of compute platforms, including mobile pho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; Safe reWard-frEe ExploraTion (SWEET)&#26694;&#26550;&#65292;&#22312;RF-RL&#20219;&#21153;&#20013;&#21487;&#23558;&#23433;&#20840;&#32422;&#26463;&#21644;&#25506;&#32034;&#25928;&#29575;&#21516;&#26102;&#23454;&#29616;&#65292;&#20351;&#24471;&#23433;&#20840;&#25506;&#32034;&#20960;&#20046;&#19981;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2206.14057</link><description>&lt;p&gt;
&#23433;&#20840;&#25506;&#32034;&#22312;&#27809;&#26377;&#22870;&#21169;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#20960;&#20046;&#19981;&#20250;&#22686;&#21152;&#26679;&#26412;&#30340;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Safe Exploration Incurs Nearly No Additional Sample Complexity for Reward-free RL. (arXiv:2206.14057v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; Safe reWard-frEe ExploraTion (SWEET)&#26694;&#26550;&#65292;&#22312;RF-RL&#20219;&#21153;&#20013;&#21487;&#23558;&#23433;&#20840;&#32422;&#26463;&#21644;&#25506;&#32034;&#25928;&#29575;&#21516;&#26102;&#23454;&#29616;&#65292;&#20351;&#24471;&#23433;&#20840;&#25506;&#32034;&#20960;&#20046;&#19981;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#22870;&#21169;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RF-RL&#65289;&#26159;&#26368;&#36817;&#24341;&#20837;&#30340;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#65292;&#20381;&#38752;&#38543;&#26426;&#37319;&#21462;&#34892;&#21160;&#26469;&#25506;&#32034;&#26410;&#30693;&#30340;&#29615;&#22659;&#65292;&#27809;&#26377;&#20219;&#20309;&#22870;&#21169;&#21453;&#39304;&#20449;&#24687;&#12290;&#34429;&#28982;RF-RL&#20013;&#25506;&#32034;&#38454;&#27573;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#26368;&#23569;&#36712;&#36857;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#23545;&#20272;&#35745;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#26234;&#33021;&#20307;&#32463;&#24120;&#38656;&#35201;&#21516;&#26102;&#36981;&#23432;&#26576;&#20123;&#23433;&#20840;&#32422;&#26463;&#12290;&#30446;&#21069;&#23578;&#19981;&#26126;&#30830;&#36825;&#31181;&#23433;&#20840;&#25506;&#32034;&#35201;&#27714;&#20250;&#22914;&#20309;&#24433;&#21709;&#30456;&#24212;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20197;&#20415;&#22312;&#35268;&#21010;&#20013;&#23454;&#29616;&#25152;&#24471;&#21040;&#31574;&#30053;&#30340;&#25152;&#38656;&#26368;&#20248;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#24050;&#30693;&#19968;&#20010;&#23433;&#20840;&#22522;&#32447;&#31574;&#30053;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#23433;&#20840;&#26080;&#22870;&#21169;&#25506;&#32034;&#65288;SWEET&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#28982;&#21518;&#29305;&#21270;SWEET&#26694;&#26550;&#21040;&#34920;&#26684;&#21644;&#20302;&#31209;MDP&#35774;&#32622;&#20013;&#65292;&#24182;&#20998;&#21035;&#24320;&#21457;&#20102;&#34987;&#31216;&#20026;Tabular-SWEET&#21644;Low-rank-SWEET&#30340;&#31639;&#27861;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#33021;&#22815;&#22312;RF-RL&#30340;&#25506;&#32034;&#38454;&#27573;&#20013;&#24182;&#20837;&#23433;&#20840;&#32422;&#26463;&#65292;&#24182;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#20445;&#35777;&#21487;&#35777;&#26126;&#30340;&#23433;&#20840;&#24615;&#65292;&#21516;&#26102;&#23454;&#29616;&#25152;&#38656;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#20013;&#65292;&#23433;&#20840;&#25506;&#32034;&#24341;&#21457;&#30340;&#38468;&#21152;&#26679;&#26412;&#22797;&#26434;&#24230;&#20960;&#20046;&#20026;&#38646;&#65292;&#36825;&#34920;&#26126;&#23433;&#20840;&#32422;&#26463;&#21644;&#26368;&#20248;&#24615;&#30446;&#26631;&#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#32780;&#19981;&#20250;&#22826;&#22823;&#22320;&#38477;&#20302;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward-free reinforcement learning (RF-RL), a recently introduced RL paradigm, relies on random action-taking to explore the unknown environment without any reward feedback information. While the primary goal of the exploration phase in RF-RL is to reduce the uncertainty in the estimated model with minimum number of trajectories, in practice, the agent often needs to abide by certain safety constraint at the same time. It remains unclear how such safe exploration requirement would affect the corresponding sample complexity in order to achieve the desired optimality of the obtained policy in planning. In this work, we make a first attempt to answer this question. In particular, we consider the scenario where a safe baseline policy is known beforehand, and propose a unified Safe reWard-frEe ExploraTion (SWEET) framework. We then particularize the SWEET framework to the tabular and the low-rank MDP settings, and develop algorithms coined Tabular-SWEET and Low-rank-SWEET, respectively. Bot
&lt;/p&gt;</description></item><item><title>LargeKernel3D&#26159;&#19968;&#31181;&#35299;&#20915;&#30452;&#25509;&#24212;&#29992;&#22823;&#22411;&#21367;&#31215;&#26680;&#22312;3D CNN&#20013;&#36935;&#21040;&#22256;&#38590;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#31354;&#38388;&#20998;&#21306;&#21367;&#31215;&#21644;&#20854;&#22823;&#26680;&#27169;&#22359;&#26469;&#36991;&#20813;&#20248;&#21270;&#21644;&#25928;&#29575;&#38382;&#39064;&#12290;&#35813;&#32593;&#32476;&#22312;&#22810;&#39033;3D&#20219;&#21153;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#22312;&#35821;&#20041;&#20998;&#21106;&#21644;&#29289;&#20307;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#20013;&#20998;&#21035;&#21462;&#24471;&#20102;&#39640;&#20998;&#25968;&#65292;&#24182;&#21487;&#20197;&#22312; Waymo 3D &#29289;&#20307;&#26816;&#27979;&#20013;&#25193;&#23637;&#21040;17x17x17&#30340;&#26680;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2206.10555</link><description>&lt;p&gt;
&#22823;&#26680;3D&#65306;&#22312;3D&#31232;&#30095;CNN&#20013;&#25193;&#23637;&#26680;
&lt;/p&gt;
&lt;p&gt;
LargeKernel3D: Scaling up Kernels in 3D Sparse CNNs. (arXiv:2206.10555v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10555
&lt;/p&gt;
&lt;p&gt;
LargeKernel3D&#26159;&#19968;&#31181;&#35299;&#20915;&#30452;&#25509;&#24212;&#29992;&#22823;&#22411;&#21367;&#31215;&#26680;&#22312;3D CNN&#20013;&#36935;&#21040;&#22256;&#38590;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#31354;&#38388;&#20998;&#21306;&#21367;&#31215;&#21644;&#20854;&#22823;&#26680;&#27169;&#22359;&#26469;&#36991;&#20813;&#20248;&#21270;&#21644;&#25928;&#29575;&#38382;&#39064;&#12290;&#35813;&#32593;&#32476;&#22312;&#22810;&#39033;3D&#20219;&#21153;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#22312;&#35821;&#20041;&#20998;&#21106;&#21644;&#29289;&#20307;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#20013;&#20998;&#21035;&#21462;&#24471;&#20102;&#39640;&#20998;&#25968;&#65292;&#24182;&#21487;&#20197;&#22312; Waymo 3D &#29289;&#20307;&#26816;&#27979;&#20013;&#25193;&#23637;&#21040;17x17x17&#30340;&#26680;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;2D CNN&#30340;&#36827;&#23637;&#34920;&#26126;&#22823;&#26680;&#26159;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#30452;&#25509;&#24212;&#29992;&#22823;&#30340;&#21367;&#31215;&#26680;&#22312;3D CNN&#20013;&#26102;&#65292;&#20250;&#36935;&#21040;&#20005;&#37325;&#22256;&#38590;&#65292;2D&#32593;&#32476;&#20013;&#25104;&#21151;&#30340;&#27169;&#22359;&#35774;&#35745;&#22312;3D&#32593;&#32476;&#19978;&#21464;&#24471;&#24778;&#20154;&#22320;&#26080;&#25928;&#65292;&#21253;&#25324;&#27969;&#34892;&#30340;&#28145;&#24230;&#21367;&#31215;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31354;&#38388;&#20998;&#21306;&#21367;&#31215;&#21450;&#20854;&#22823;&#26680;&#27169;&#22359;&#12290;&#36825;&#36991;&#20813;&#20102;&#32431;&#31929;3D&#22823;&#26680;&#30340;&#20248;&#21270;&#21644;&#25928;&#29575;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#22823;&#26680;3D CNN&#32593;&#32476;LargeKernel3D&#65292;&#22312;&#35821;&#20041;&#20998;&#21106;&#21644;&#29289;&#20307;&#26816;&#27979;&#30340;3D&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25552;&#39640;&#12290;&#23427;&#22312;ScanNetv2&#35821;&#20041;&#20998;&#21106;&#21644;nuScenes&#29289;&#20307;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#20013;&#20998;&#21035;&#36798;&#21040;73.9%&#30340;mIoU&#21644;72.8%&#30340;NDS&#65292;&#20301;&#21015;nuScenes LIDAR&#25490;&#34892;&#27036;&#31532;&#19968;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#65292;&#24615;&#33021;&#36827;&#19968;&#27493;&#25552;&#21319;&#33267;74.2%&#30340;NDS&#12290;&#27492;&#22806;&#65292;LargeKernel3D &#21487;&#20197;&#22312; Waymo 3D &#29289;&#20307;&#26816;&#27979;&#20013;&#25193;&#23637;&#21040;17x17x17&#30340;&#26680;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advance in 2D CNNs has revealed that large kernels are important. However, when directly applying large convolutional kernels in 3D CNNs, severe difficulties are met, where those successful module designs in 2D become surprisingly ineffective on 3D networks, including the popular depth-wise convolution. To address this vital challenge, we instead propose the spatial-wise partition convolution and its large-kernel module. As a result, it avoids the optimization and efficiency issues of naive 3D large kernels. Our large-kernel 3D CNN network, LargeKernel3D, yields notable improvement in 3D tasks of semantic segmentation and object detection. It achieves 73.9% mIoU on the ScanNetv2 semantic segmentation and 72.8% NDS nuScenes object detection benchmarks, ranking 1st on the nuScenes LIDAR leaderboard. The performance further boosts to 74.2% NDS with a simple multi-modal fusion. In addition, LargeKernel3D can be scaled to 17x17x17 kernel size on Waymo 3D object detection. For the fir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#38543;&#26426;&#30005;&#36335;&#38598;&#21512;&#19982;&#31934;&#30830;&#38543;&#26426;&#24615;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#35813;&#31639;&#27861;&#22797;&#26434;&#24230;&#23545;&#20110;&#27973;&#23618;&#30005;&#36335;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#65292;&#22312;&#23616;&#37096;&#21644;&#24182;&#34892;&#38543;&#26426;&#30005;&#36335;&#20013;&#39564;&#35777;&#20102;&#22797;&#26434;&#24230;&#32447;&#24615;&#22686;&#38271;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#21464;&#20998;&#31639;&#27861;&#20013;&#35299;&#20915;&#8220;&#36139;&#30240;&#39640;&#21407;&#8221;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.09900</link><description>&lt;p&gt;
&#35780;&#20272;&#22810;&#36798;50&#37327;&#23376;&#27604;&#29305;&#30340;&#37327;&#23376;&#30005;&#36335;&#38598;&#21512;&#30340;&#38543;&#26426;&#24615;
&lt;/p&gt;
&lt;p&gt;
Estimating the randomness of quantum circuit ensembles up to 50 qubits. (arXiv:2205.09900v3 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#38543;&#26426;&#30005;&#36335;&#38598;&#21512;&#19982;&#31934;&#30830;&#38543;&#26426;&#24615;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#35813;&#31639;&#27861;&#22797;&#26434;&#24230;&#23545;&#20110;&#27973;&#23618;&#30005;&#36335;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#65292;&#22312;&#23616;&#37096;&#21644;&#24182;&#34892;&#38543;&#26426;&#30005;&#36335;&#20013;&#39564;&#35777;&#20102;&#22797;&#26434;&#24230;&#32447;&#24615;&#22686;&#38271;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#21464;&#20998;&#31639;&#27861;&#20013;&#35299;&#20915;&#8220;&#36139;&#30240;&#39640;&#21407;&#8221;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38543;&#26426;&#37327;&#23376;&#30005;&#36335;&#22312;&#37327;&#23376;&#38712;&#26435;&#23637;&#31034;&#12289;&#29992;&#20110;&#21270;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#20197;&#21450;&#40657;&#27934;&#20449;&#24687;&#31561;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#38543;&#26426;&#30005;&#36335;&#27491;&#30830;&#36817;&#20284;&#20219;&#24847;&#38543;&#26426;&#24186;&#27491;&#30697;&#38453;&#30340;&#33021;&#21147;&#23545;&#20110;&#20102;&#35299;&#23427;&#20204;&#30340;&#22797;&#26434;&#24615;&#12289;&#21487;&#34920;&#36798;&#24615;&#21644;&#35757;&#32451;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#25968;&#20540;&#21327;&#35758;&#65292;&#29992;&#20110;&#35780;&#20272;&#19968;&#20010;&#32473;&#23450;&#30005;&#36335;&#38598;&#21512;&#19982;&#31934;&#30830;&#38543;&#26426;&#24615;&#20043;&#38388;&#30340;&#26694;&#26550;&#28508;&#21147;&#36317;&#31163;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#23545;&#20110;&#27973;&#23618;&#30005;&#36335;&#20855;&#26377;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#65292;&#21487;&#20351;&#29992;CPU&#21644;GPU&#24182;&#34892;&#36827;&#34892;&#39640;&#25928;&#35745;&#31639;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;1.&#23616;&#37096;&#21644;&#24182;&#34892;&#38543;&#26426;&#30005;&#36335;&#20197;&#39564;&#35777;&#24067;&#26391;-&#33832;&#26031;&#37329;&#29468;&#24819;&#25152;&#36848;&#30340;&#22797;&#26434;&#24230;&#32447;&#24615;&#22686;&#38271;&#65307;2.&#30828;&#20214;&#26377;&#25928;&#36817;&#20284;&#31572;&#26696;&#20197;&#22312;&#21464;&#20998;&#31639;&#27861;&#20013;&#35299;&#20915;&#8220;&#36139;&#30240;&#39640;&#21407;&#8221;&#38382;&#39064;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#22823;&#35268;&#27169;&#24352;&#37327;&#32593;&#32476;&#27169;&#25311;&#21487;&#20197;&#25552;&#20379;&#37325;&#35201;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Random quantum circuits have been utilized in the contexts of quantum supremacy demonstrations, variational quantum algorithms for chemistry and machine learning, and blackhole information. The ability of random circuits to approximate any random unitaries has consequences on their complexity, expressibility, and trainability. To study this property of random circuits, we develop numerical protocols for estimating the frame potential, the distance between a given ensemble and the exact randomness. Our tensor-network-based algorithm has polynomial complexity for shallow circuits and is high-performing using CPU and GPU parallelism. We study 1. local and parallel random circuits to verify the linear growth in complexity as stated by the Brown-Susskind conjecture, and; 2. hardware-efficient ans\"atze to shed light on its expressibility and the barren plateau problem in the context of variational algorithms. Our work shows that large-scale tensor network simulations could provide important
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#30340;&#24515;&#30005;&#22270;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#26032;&#21457;&#31958;&#23615;&#30149;&#25104;&#20154;&#24739;&#32773;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;ADA&#39118;&#38505;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#29305;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.02900</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#30340;&#24515;&#30005;&#22270;&#36827;&#34892;&#26032;&#21457;&#31958;&#23615;&#30149;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
New-Onset Diabetes Assessment Using Artificial Intelligence-Enhanced Electrocardiography. (arXiv:2205.02900v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#30340;&#24515;&#30005;&#22270;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#26032;&#21457;&#31958;&#23615;&#30149;&#25104;&#20154;&#24739;&#32773;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;ADA&#39118;&#38505;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#29305;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#35786;&#26029;&#30340;&#31958;&#23615;&#30149;&#22312;&#24739;&#32773;&#20013;&#21344;21.4&#65285;&#65292;&#30001;&#20110;&#31579;&#26597;&#29575;&#30340;&#38480;&#21046;&#65292;&#31958;&#23615;&#30149;&#21487;&#33021;&#28508;&#20239;&#26080;&#30151;&#29366;&#32780;&#26410;&#34987;&#26816;&#27979;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22686;&#24378;&#30340;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26469;&#30830;&#23450;&#26032;&#21457;&#31958;&#23615;&#30149;&#30340;&#25104;&#20154;&#24739;&#32773;&#12290; &#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;12&#23548;&#32852;&#24515;&#30005;&#22270;&#21644;&#21487;&#29992;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#26469;&#20272;&#35745;HbA1c&#12290; &#25105;&#20204;&#22238;&#39038;&#24615;&#22320;&#25910;&#38598;&#20102;&#19968;&#32452;&#21253;&#21547;&#26377;&#37197;&#23545;&#30340;ECG&#21644;HbA1c&#25968;&#25454;&#30340;&#30149;&#20154;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;ADA&#39118;&#38505;&#26816;&#27979;&#65292;&#22522;&#20110;ECG&#30340;&#35780;&#20272;&#25928;&#26524;&#26356;&#22909;&#12290;AI&#22686;&#24378;&#30340;ECG&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#36798;&#21040;81&#65285;&#65292;&#28789;&#25935;&#24230;&#20026;80&#65285;&#65292;&#29305;&#24322;&#24615;&#20026;82&#65285;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#30340;ECG&#21487;&#20197;&#25104;&#20026;&#26032;&#21457;&#31958;&#23615;&#30149;&#25104;&#20154;&#24739;&#32773;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#22312;&#20256;&#32479;&#31579;&#26597;&#26041;&#27861;&#26377;&#38480;&#30340;&#20154;&#32676;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Undiagnosed diabetes is present in 21.4% of adults with diabetes. Diabetes can remain asymptomatic and undetected due to limitations in screening rates. To address this issue, questionnaires, such as the American Diabetes Association (ADA) Risk test, have been recommended for use by physicians and the public. Based on evidence that blood glucose concentration can affect cardiac electrophysiology, we hypothesized that an artificial intelligence (AI)-enhanced electrocardiogram (ECG) could identify adults with new-onset diabetes. We trained a neural network to estimate HbA1c using a 12-lead ECG and readily available demographics. We retrospectively assembled a dataset comprised of patients with paired ECG and HbA1c data. The population of patients who receive both an ECG and HbA1c may a biased sample of the complete outpatient population, so we adjusted the importance placed on each patient to generate a more representative pseudo-population. We found ECG-based assessment outperforms the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8220;FixNoise&#8221;&#65292;&#22312;StyleGAN&#30340;&#36801;&#31227;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#31616;&#21333;&#30340;&#29305;&#24449;&#21305;&#37197;&#25439;&#22833;&#26469;&#25913;&#21892;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#22312;&#30446;&#26631;&#29305;&#24449;&#31354;&#38388;&#30340;&#19968;&#37096;&#20998;&#20013;&#20165;&#20445;&#30041;&#28304;&#29305;&#24449;&#20197;&#25511;&#21046;&#28304;&#29305;&#24449;&#30340;&#31243;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#39046;&#22495;&#32763;&#35793;&#21644;&#23646;&#24615;&#32534;&#36753;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.14079</link><description>&lt;p&gt;
&#20462;&#27491;&#22122;&#22768;&#65306;Disentangling Source Feature&#29992;&#20110;StyleGAN&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fix the Noise: Disentangling Source Feature for Transfer Learning of StyleGAN. (arXiv:2204.14079v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.14079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8220;FixNoise&#8221;&#65292;&#22312;StyleGAN&#30340;&#36801;&#31227;&#23398;&#20064;&#20013;&#24341;&#20837;&#20102;&#31616;&#21333;&#30340;&#29305;&#24449;&#21305;&#37197;&#25439;&#22833;&#26469;&#25913;&#21892;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#22312;&#30446;&#26631;&#29305;&#24449;&#31354;&#38388;&#30340;&#19968;&#37096;&#20998;&#20013;&#20165;&#20445;&#30041;&#28304;&#29305;&#24449;&#20197;&#25511;&#21046;&#28304;&#29305;&#24449;&#30340;&#31243;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#39046;&#22495;&#32763;&#35793;&#21644;&#23646;&#24615;&#32534;&#36753;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;StyleGAN&#30340;&#36801;&#31227;&#23398;&#20064;&#22312;&#35299;&#20915;&#19981;&#21516;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#39046;&#22495;&#32763;&#35793;&#19978;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#36807;&#20132;&#25442;&#25110;&#20923;&#32467;&#26435;&#37325;&#26469;&#21033;&#29992;&#28304;&#27169;&#22411;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#20294;&#20854;&#22312;&#35270;&#35273;&#36136;&#37327;&#21644;&#25511;&#21046;&#28304;&#29305;&#24449;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31616;&#21333;&#30340;&#29305;&#24449;&#21305;&#37197;&#25439;&#22833;&#26469;&#25913;&#21892;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#8220;FixNoise&#8221;&#35757;&#32451;&#30446;&#26631;&#27169;&#22411;&#20197;&#20165;&#22312;&#30446;&#26631;&#29305;&#24449;&#31354;&#38388;&#30340;&#19968;&#37096;&#20998;&#20013;&#20445;&#30041;&#28304;&#29305;&#24449;&#65292;&#20174;&#32780;&#25511;&#21046;&#28304;&#29305;&#24449;&#30340;&#31243;&#24230;&#12290;&#30001;&#20110;&#29305;&#24449;&#31354;&#38388;&#26159;&#20998;&#31163;&#30340;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21333;&#20010;&#27169;&#22411;&#20013;&#24179;&#28369;&#22320;&#25511;&#21046;&#28304;&#29305;&#24449;&#30340;&#31243;&#24230;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39046;&#22495;&#32763;&#35793;&#21644;&#23646;&#24615;&#32534;&#36753;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning of StyleGAN has recently shown great potential to solve diverse tasks, especially in domain translation. Previous methods utilized a source model by swapping or freezing weights during transfer learning, however, they have limitations on visual quality and controlling source features. In other words, they require additional models that are computationally demanding and have restricted control steps that prevent a smooth transition. In this paper, we propose a new approach to overcome these limitations. Instead of swapping or freezing, we introduce a simple feature matching loss to improve generation quality. In addition, to control the degree of source features, we train a target model with the proposed strategy, FixNoise, to preserve the source features only in a disentangled subspace of a target feature space. Owing to the disentangled feature space, our method can smoothly control the degree of the source features in a single model. Extensive experiments demonstrat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35770;&#25991;&#35752;&#35770;&#20102;&#35270;&#35273;&#34920;&#24449;&#23398;&#20064;&#20013;&#32570;&#20047;&#35299;&#37322;&#24615;&#65292;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22240;&#26524;&#25512;&#29702;&#33539;&#24335;&#26469;&#23454;&#29616;&#36825;&#20123;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.12037</link><description>&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#19982;&#35270;&#35273;&#34920;&#24449;&#23398;&#20064;&#30340;&#20132;&#27719;&#65306;&#19968;&#39033;&#21069;&#30651;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Causal Reasoning Meets Visual Representation Learning: A Prospective Study. (arXiv:2204.12037v8 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.12037
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35770;&#25991;&#35752;&#35770;&#20102;&#35270;&#35273;&#34920;&#24449;&#23398;&#20064;&#20013;&#32570;&#20047;&#35299;&#37322;&#24615;&#65292;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22240;&#26524;&#25512;&#29702;&#33539;&#24335;&#26469;&#23454;&#29616;&#36825;&#20123;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#34920;&#24449;&#23398;&#20064;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#37117;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#65292;&#21253;&#25324;&#35270;&#35273;&#29702;&#35299;&#12289;&#35270;&#39057;&#29702;&#35299;&#12289;&#22810;&#27169;&#24577;&#20998;&#26512;&#12289;&#20154;&#26426;&#20132;&#20114;&#21644;&#22478;&#24066;&#35745;&#31639;&#12290;&#30001;&#20110;&#22823;&#25968;&#25454;&#26102;&#20195;&#28044;&#29616;&#30340;&#22823;&#37327;&#22810;&#27169;&#24577;&#24322;&#26500;&#30340;&#26102;&#31354;/&#26102;&#38388;/&#26102;&#31354;&#25968;&#25454;&#65292;&#29616;&#26377;&#35270;&#35273;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#24615;&#32570;&#20047;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#20013;&#22823;&#22810;&#25968;&#20542;&#21521;&#20110;&#36866;&#24212;&#21407;&#22987;&#25968;&#25454;/&#21464;&#37327;&#20998;&#24067;&#65292;&#24573;&#30053;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#32972;&#21518;&#30340;&#22522;&#26412;&#22240;&#26524;&#20851;&#31995;&#65292;&#32570;&#20047;&#32479;&#19968;&#30340;&#25351;&#23548;&#21644;&#20998;&#26512;&#65292;&#26080;&#27861;&#35299;&#37322;&#20026;&#20160;&#20040;&#29616;&#20195;&#35270;&#35273;&#34920;&#24449;&#23398;&#20064;&#26041;&#27861;&#23481;&#26131;&#38519;&#20837;&#25968;&#25454;&#20559;&#35265;&#65292;&#24182;&#19988;&#20855;&#26377;&#26377;&#38480;&#30340;&#27867;&#21270;&#21644;&#35748;&#30693;&#33021;&#21147;&#12290;&#21463;&#21040;&#20154;&#31867;&#32423;&#21035;&#20195;&#29702;&#30340;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#20184;&#20986;&#20102;&#24456;&#22823;&#30340;&#21162;&#21147;&#24320;&#21457;&#22240;&#26524;&#25512;&#29702;&#33539;&#24335;&#65292;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual representation learning is ubiquitous in various real-world applications, including visual comprehension, video understanding, multi-modal analysis, human-computer interaction, and urban computing. Due to the emergence of huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal data in big data era, the lack of interpretability, robustness, and out-of-distribution generalization are becoming the challenges of the existing visual models. The majority of the existing methods tend to fit the original data/variable distributions and ignore the essential causal relations behind the multi-modal knowledge, which lacks unified guidance and analysis about why modern visual representation learning methods easily collapse into data bias and have limited generalization and cognitive abilities. Inspired by the strong inference ability of human-level agents, recent years have therefore witnessed great effort in developing causal reasoning paradigms to realize robust represe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CgAT&#26041;&#27861;&#65292;&#22522;&#20110;&#20013;&#24515;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#25552;&#21319;&#28145;&#24230;Hashing&#32593;&#32476;&#26816;&#32034;&#30340;&#40065;&#26834;&#24615;&#65292;&#21462;&#24471;&#20102;&#39046;&#20808;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2204.10779</link><description>&lt;p&gt;
CgAT&#65306;&#22522;&#20110;&#20013;&#24515;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#25552;&#21319;Hashing&#26816;&#32034;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
CgAT: Center-Guided Adversarial Training for Deep Hashing-Based Retrieval. (arXiv:2204.10779v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CgAT&#26041;&#27861;&#65292;&#22522;&#20110;&#20013;&#24515;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#25552;&#21319;&#28145;&#24230;Hashing&#32593;&#32476;&#26816;&#32034;&#30340;&#40065;&#26834;&#24615;&#65292;&#21462;&#24471;&#20102;&#39046;&#20808;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;Hashing&#22312;&#22270;&#20687;&#26816;&#32034;&#39046;&#22495;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#24448;&#24448;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;min-max&#30340;&#20013;&#24515;&#24341;&#23548;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65288;CgAT&#65289;&#65292;&#36890;&#36807;&#26368;&#22351;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#26469;&#25552;&#39640;&#28145;&#24230;Hashing&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#30446;&#21069;&#28145;&#24230;Hashing&#26816;&#32034;&#39046;&#22495;&#20013;&#30340;&#26368;&#26032;&#23545;&#25239;&#24615;&#38450;&#24481;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep hashing has been extensively utilized in massive image retrieval because of its efficiency and effectiveness. However, deep hashing models are vulnerable to adversarial examples, making it essential to develop adversarial defense methods for image retrieval. Existing solutions achieved limited defense performance because of using weak adversarial samples for training and lacking discriminative optimization objectives to learn robust features. In this paper, we present a min-max based Center-guided Adversarial Training, namely CgAT, to improve the robustness of deep hashing networks through worst adversarial examples. Specifically, we first formulate the center code as a semantically-discriminative representative of the input image content, which preserves the semantic similarity with positive samples and dissimilarity with negative examples. We prove that a mathematical formula can calculate the center code immediately. After obtaining the center codes in each optimization iterati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#19968;&#20010;$n$&#20154;&#38543;&#26426;&#21338;&#24328;&#30340;&#23376;&#31867;&#65292;&#35774;&#35745;&#20102;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#20986;$\epsilon$-NE&#31574;&#30053;&#65292;&#22312;&#22870;&#21169;&#20989;&#25968;&#20026;&#31038;&#20250;&#20984;&#24615;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#19978;&#38480;&#12290;</title><link>http://arxiv.org/abs/2201.12224</link><description>&lt;p&gt;
&#29420;&#31435;&#38142;$n$&#20154;&#38543;&#26426;&#21338;&#24328;&#20013;&#23398;&#20064;&#31283;&#23450;&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Stationary Nash Equilibrium Policies in $n$-Player Stochastic Games with Independent Chains. (arXiv:2201.12224v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19968;&#20010;$n$&#20154;&#38543;&#26426;&#21338;&#24328;&#30340;&#23376;&#31867;&#65292;&#35774;&#35745;&#20102;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#23398;&#20064;&#20986;$\epsilon$-NE&#31574;&#30053;&#65292;&#22312;&#22870;&#21169;&#20989;&#25968;&#20026;&#31038;&#20250;&#20984;&#24615;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;$n$&#20154;&#38543;&#26426;&#21338;&#24328;&#30340;&#23376;&#31867;&#65292;&#22312;&#35813;&#23376;&#31867;&#20013;&#65292;&#29609;&#23478;&#20855;&#26377;&#33258;&#24049;&#30340;&#20869;&#37096;&#29366;&#24577;/&#21160;&#20316;&#31354;&#38388;&#65292;&#20294;&#36890;&#36807;&#20854;&#25903;&#20184;&#21151;&#33021;&#30456;&#20114;&#32806;&#21512;&#12290;&#20551;&#35774;&#29609;&#23478;&#30340;&#20869;&#37096;&#38142;&#26159;&#30001;&#29420;&#31435;&#30340;&#36716;&#31227;&#27010;&#29575;&#39537;&#21160;&#30340;&#12290;&#27492;&#22806;&#65292;&#29609;&#23478;&#21482;&#33021;&#25910;&#21040;&#20182;&#20204;&#30340;&#25903;&#20184;&#23454;&#29616;&#65292;&#32780;&#19981;&#26159;&#23454;&#38469;&#30340;&#21151;&#33021;&#65292;&#24182;&#19988;&#19981;&#33021;&#35266;&#23519;&#24444;&#27492;&#30340;&#29366;&#24577;/&#21160;&#20316;&#12290;&#38024;&#23545;&#36825;&#31867;&#28216;&#25103;&#65292;&#25105;&#20204;&#39318;&#20808;&#34920;&#26126;&#65292;&#19981;&#20551;&#35774;&#22870;&#21169;&#20989;&#25968;&#65292;&#27714;&#35299;&#22266;&#23450;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#31574;&#30053;&#26159;&#19968;&#20010;&#19981;&#21487;&#20132;&#20114;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#25105;&#20204;&#22522;&#20110;&#23545;&#20598;&#24179;&#22343;&#21644;&#23545;&#20598;&#38236;&#38754;&#19979;&#38477;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#23398;&#20064;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#20197;&#26399;&#26395;&#25110;&#20960;&#20046;&#30830;&#23450;&#22320;&#20197;&#24179;&#22343;&#30340;Nikaido-Isoda&#36317;&#31163;&#25910;&#25947;&#21040;$\epsilon$-NE&#31574;&#30053;&#38598;&#21512;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#39069;&#22806;&#20551;&#35774;&#22870;&#21169;&#20989;&#25968;&#20026;&#31038;&#20250;&#20984;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25512;&#23548;&#20986;&#22810;&#39033;&#24335;&#19978;&#30028;&#65292;&#20197;&#23454;&#29616;$\epsilon$-NE&#31574;&#30053;&#25152;&#38656;&#30340;&#36845;&#20195;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a subclass of $n$-player stochastic games, in which players have their own internal state/action spaces while they are coupled through their payoff functions. It is assumed that players' internal chains are driven by independent transition probabilities. Moreover, players can receive only realizations of their payoffs, not the actual functions, and cannot observe each other's states/actions. For this class of games, we first show that finding a stationary Nash equilibrium (NE) policy without any assumption on the reward functions is interactable. However, for general reward functions, we develop polynomial-time learning algorithms based on dual averaging and dual mirror descent, which converge in terms of the averaged Nikaido-Isoda distance to the set of $\epsilon$-NE policies almost surely or in expectation. In particular, under extra assumptions on the reward functions such as social concavity, we derive polynomial upper bounds on the number of iterates to achieve an $\ep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SWTR-Unet&#30340;&#28151;&#21512;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#23558;&#21367;&#31215;&#21644;&#22522;&#20110;Transformer&#30340;&#32467;&#26500;&#20803;&#32032;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;MRI&#20013;&#32925;&#33039;&#21644;&#32925;&#37096;&#25439;&#20260;&#32852;&#21512;&#20998;&#21106;&#30340;&#39640;&#25928;&#12289;&#31934;&#30830;&#21644;&#21487;&#38752;&#12290;&#35813;&#32593;&#32476;&#22312;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#19978;&#37117;&#20855;&#26377;&#36229;&#36807;&#26368;&#26032;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2201.10981</link><description>&lt;p&gt;
MRI&#20013;&#32925;&#33039;&#21644;&#32925;&#37096;&#25439;&#20260;&#32852;&#21512;&#20998;&#21106;&#30340;&#28151;&#21512;CNN&#19982;Transformer&#23618;
&lt;/p&gt;
&lt;p&gt;
Joint Liver and Hepatic Lesion Segmentation in MRI using a Hybrid CNN with Transformer Layers. (arXiv:2201.10981v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.10981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SWTR-Unet&#30340;&#28151;&#21512;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#23558;&#21367;&#31215;&#21644;&#22522;&#20110;Transformer&#30340;&#32467;&#26500;&#20803;&#32032;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;MRI&#20013;&#32925;&#33039;&#21644;&#32925;&#37096;&#25439;&#20260;&#32852;&#21512;&#20998;&#21106;&#30340;&#39640;&#25928;&#12289;&#31934;&#30830;&#21644;&#21487;&#38752;&#12290;&#35813;&#32593;&#32476;&#22312;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#19978;&#37117;&#20855;&#26377;&#36229;&#36807;&#26368;&#26032;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32925;&#33039;&#21644;&#20854;&#20013;&#32925;&#37096;&#25439;&#20260;&#30340;&#20998;&#21106;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#37325;&#35270;&#65292;&#22240;&#20026;&#27599;&#24180;&#32925;&#30284;&#30340;&#21457;&#30149;&#29575;&#19981;&#26029;&#22686;&#21152;&#12290;&#34429;&#28982;&#36807;&#21435;&#20960;&#24180;&#20013;&#24050;&#32463;&#25104;&#21151;&#24320;&#21457;&#20102;&#20855;&#26377;&#24635;&#20307;&#26377;&#24076;&#26395;&#32467;&#26524;&#30340;&#21508;&#31181;&#32593;&#32476;&#21464;&#20307;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#39046;&#22495;&#65292;&#20294;&#20960;&#20046;&#25152;&#26377;&#36825;&#20123;&#32593;&#32476;&#37117;&#38590;&#20197;&#31934;&#20934;&#22320;&#20998;&#21106;MRI&#20013;&#30340;&#32925;&#37096;&#25439;&#20260;&#12290;&#36825;&#23548;&#33268;&#20102;&#23558;&#21367;&#31215;&#21644;&#22522;&#20110;Transformer&#30340;&#32467;&#26500;&#20803;&#32032;&#30456;&#32467;&#21512;&#30340;&#24819;&#27861;&#65292;&#20197;&#20811;&#26381;&#29616;&#26377;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#32593;&#32476;SWTR-Unet&#65292;&#23427;&#30001;&#39044;&#35757;&#32451;&#30340;ResNet&#12289;Transformer&#22359;&#20197;&#21450;&#24120;&#35265;&#30340;Unet-style&#35299;&#30721;&#22120;&#36335;&#24452;&#32452;&#25104;&#12290;&#35813;&#32593;&#32476;&#20027;&#35201;&#24212;&#29992;&#20110;&#21333;&#27169;&#24577;&#38750;&#22686;&#24378;&#32925;&#33039;MRI&#65292;&#24182;&#39069;&#22806;&#24212;&#29992;&#20110;&#20844;&#24320;&#21487;&#29992;&#30340;CT&#25968;&#25454;&#65292;&#20197;&#39564;&#35777;&#20854;&#22312;&#20854;&#20182;&#27169;&#24577;&#25968;&#25454;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;SWTR-Unet&#30456;&#23545;&#20110;&#26368;&#26032;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#35757;&#32451;&#26102;&#38388;&#26041;&#38754;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based segmentation of the liver and hepatic lesions therein steadily gains relevance in clinical practice due to the increasing incidence of liver cancer each year. Whereas various network variants with overall promising results in the field of medical image segmentation have been successfully developed over the last years, almost all of them struggle with the challenge of accurately segmenting hepatic lesions in magnetic resonance imaging (MRI). This led to the idea of combining elements of convolutional and transformer-based architectures to overcome the existing limitations. This work presents a hybrid network called SWTR-Unet, consisting of a pretrained ResNet, transformer blocks as well as a common Unet-style decoder path. This network was primarily applied to single-modality non-contrast-enhanced liver MRI and additionally to the publicly available computed tomography (CT) data of the liver tumor segmentation (LiTS) challenge to verify the applicability on other mod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#20851;&#32852;&#23398;&#20064;&#27169;&#22411;&#26469;&#36827;&#34892;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#25152;&#26377;&#25903;&#25345;&#22270;&#20687;&#21644;&#26597;&#35810;&#22270;&#20687;&#19978;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#26500;&#24314;&#21160;&#24577;GCN&#65292;&#24182;&#36866;&#24212;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2108.02235</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#30340;&#21160;&#24577;&#20851;&#32852;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamic Relevance Learning for Few-Shot Object Detection. (arXiv:2108.02235v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.02235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#20851;&#32852;&#23398;&#20064;&#27169;&#22411;&#26469;&#36827;&#34892;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#25152;&#26377;&#25903;&#25345;&#22270;&#20687;&#21644;&#26597;&#35810;&#22270;&#20687;&#19978;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#26500;&#24314;&#21160;&#24577;GCN&#65292;&#24182;&#36866;&#24212;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26114;&#36149;&#30340;&#36793;&#30028;&#26694;&#27880;&#37322;&#38480;&#21046;&#20102;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#30340;&#21457;&#23637;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#20851;&#27880;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#23569;&#26679;&#26412;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#12290;&#36825;&#38656;&#35201;&#20165;&#20351;&#29992;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#26816;&#27979;&#22120;&#35782;&#21035;&#26032;&#31867;&#21035;&#30340;&#23545;&#35937;&#12290;&#24403;&#20170;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#27969;&#34892;&#26041;&#27861;&#37319;&#29992;&#31867;&#20284;&#20110;&#20803;&#23398;&#20064;&#30340;&#35757;&#32451;&#26041;&#24335;&#65292;&#22914;Meta R-CNN&#31995;&#21015;&#65292;&#24050;&#32463;&#21462;&#24471;&#20102;&#26377;&#21069;&#36884;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25903;&#25345;&#25968;&#25454;&#20165;&#20316;&#20026;&#31867;&#21035;&#27880;&#24847;&#21147;&#65292;&#20197;&#24341;&#23548;&#27599;&#27425;&#26597;&#35810;&#22270;&#20687;&#30340;&#26816;&#27979;&#12290;&#23427;&#20204;&#24444;&#27492;&#30340;&#30456;&#20851;&#24615;&#20173;&#28982;&#26410;&#34987;&#24320;&#21457;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#26368;&#36817;&#30340;&#24037;&#20316;&#23558;&#25903;&#25345;&#25968;&#25454;&#21644;&#26597;&#35810;&#22270;&#20687;&#35270;&#20026;&#29420;&#31435;&#20998;&#25903;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#20851;&#32852;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#25152;&#26377;&#25903;&#25345;&#22270;&#20687;&#21644;&#26597;&#35810;&#22270;&#20687;&#19978;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;RoI&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#26500;&#24314;&#19968;&#20010;&#21160;&#24577;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#12290;&#36890;&#36807;&#35843;&#25972;&#39044;&#27979;&#20998;&#24067;&#65292;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expensive bounding-box annotations have limited the development of object detection task. Thus, it is necessary to focus on more challenging task of few-shot object detection. It requires the detector to recognize objects of novel classes with only a few training samples. Nowadays, many existing popular methods adopting training way similar to meta-learning have achieved promising performance, such as Meta R-CNN series. However, support data is only used as the class attention to guide the detecting of query images each time. Their relevance to each other remains unexploited. Moreover, a lot of recent works treat the support data and query images as independent branch without considering the relationship between them. To address this issue, we propose a dynamic relevance learning model, which utilizes the relationship between all support images and Region of Interest (RoI) on the query images to construct a dynamic graph convolutional network (GCN). By adjusting the prediction distribu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#22312;&#32447;&#21435;&#20559;&#20272;&#35745;&#30340;&#26041;&#27861;&#26469;&#20462;&#27491;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#28176;&#36817;&#20559;&#24046;&#65292;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#25552;&#20379;&#26356;&#38160;&#21033;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2107.02266</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#36817;&#26368;&#20248;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Near-optimal inference in adaptive linear regression. (arXiv:2107.02266v3 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.02266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#22312;&#32447;&#21435;&#20559;&#20272;&#35745;&#30340;&#26041;&#27861;&#26469;&#20462;&#27491;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#28176;&#36817;&#20559;&#24046;&#65292;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#25552;&#20379;&#26356;&#38160;&#21033;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25968;&#25454;&#20197;&#33258;&#36866;&#24212;&#26041;&#24335;&#25910;&#38598;&#26102;&#65292;&#21363;&#20351;&#26159;&#26368;&#31616;&#21333;&#30340;&#26041;&#27861;&#22914;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#27861;&#20063;&#21487;&#33021;&#34920;&#29616;&#20986;&#38750;&#27491;&#24120;&#30340;&#28176;&#36817;&#34892;&#20026;&#12290; &#20316;&#20026;&#19981;&#33391;&#21518;&#26524;&#65292;&#22522;&#20110;&#28176;&#36817;&#27491;&#24120;&#24615;&#30340;&#20551;&#35774;&#26816;&#39564;&#21644;&#32622;&#20449;&#21306;&#38388;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#32467;&#26524;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#22312;&#32447;&#21435;&#20559;&#20272;&#35745;&#30340;&#26041;&#27861;&#26469;&#32416;&#27491;&#36825;&#20123;&#35823;&#24046;&#65292;&#24182;&#21033;&#29992;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#21327;&#26041;&#24046;&#32467;&#26500;&#65292;&#22312;&#20854;&#26356;&#22810;&#20449;&#24687;&#24050;&#32047;&#31215;&#30340;&#26041;&#21521;&#19978;&#25552;&#20379;&#26356;&#38160;&#21033;&#30340;&#20272;&#35745;&#12290; &#25105;&#20204;&#22312;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#30340;&#28201;&#21644;&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#22312;&#32447;&#21435;&#20559;&#20272;&#35745;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#36136;&#65292;&#24182;&#25552;&#20379;&#20102;&#28176;&#36817;&#31934;&#30830;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290; &#25105;&#20204;&#36824;&#38024;&#23545;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#19979;&#30028;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#27604;&#36739;&#20272;&#35745;&#22120;&#30340;&#22522;&#32447;&#12290; &#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#36798;&#21040;&#26368;&#23567;&#20540;&#30340;&#21508;&#31181;&#26465;&#20214;&#19979;&#65292;&#26368;&#23567;&#21270;&#19979;&#30028;&#22343;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
When data is collected in an adaptive manner, even simple methods like ordinary least squares can exhibit non-normal asymptotic behavior. As an undesirable consequence, hypothesis tests and confidence intervals based on asymptotic normality can lead to erroneous results. We propose a family of online debiasing estimators to correct these distributional anomalies in least squares estimation. Our proposed methods take advantage of the covariance structure present in the dataset and provide sharper estimates in directions for which more information has accrued. We establish an asymptotic normality property for our proposed online debiasing estimators under mild conditions on the data collection process and provide asymptotically exact confidence intervals. We additionally prove a minimax lower bound for the adaptive linear regression problem, thereby providing a baseline by which to compare estimators. There are various conditions under which our proposed estimators achieve the minimax lo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20219;&#21153;&#26080;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#26631;&#35760;&#21644;&#36890;&#20449;&#25104;&#26412;&#65292;&#24182;&#20445;&#35777;&#35299;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2106.10836</link><description>&lt;p&gt;
&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Learning for Deep Neural Networks on Edge Devices. (arXiv:2106.10836v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.10836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20219;&#21153;&#26080;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#26631;&#35760;&#21644;&#36890;&#20449;&#25104;&#26412;&#65292;&#24182;&#20445;&#35777;&#35299;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22788;&#29702;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#26102;&#65292;&#25345;&#32493;&#26356;&#26032;&#27169;&#22411;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;&#20351;&#29992;&#23454;&#26102;&#25968;&#25454;&#26469;&#26356;&#26032;&#27169;&#22411;&#26159;&#29702;&#24819;&#30340;&#65292;&#20294;&#30001;&#20110;&#26631;&#35760;&#21644;&#36890;&#20449;&#25104;&#26412;&#31561;&#38480;&#21046;&#65292;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#30340;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#22312;&#35774;&#22791;&#19978;&#36807;&#28388;&#21644;&#36873;&#25321;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#65288;&#21363;&#20027;&#21160;&#23398;&#20064;&#65289;&#12290;&#26412;&#25991;&#35268;&#33539;&#20102;&#36793;&#32536;&#35774;&#22791;&#19978;DNN&#30340;&#23454;&#38469;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20219;&#21153;&#26080;&#20851;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#20854;&#20943;&#23569;&#21040;&#27969;&#23376;&#27169;&#22359;&#26368;&#22823;&#21270;&#12290;&#36825;&#20010;&#26694;&#26550;&#36275;&#22815;&#36731;&#20415;&#65292;&#21487;&#20197;&#29992;&#20302;&#35745;&#31639;&#36164;&#28304;&#36816;&#34892;&#65292;&#20294;&#30001;&#20110;&#23376;&#27169;&#22359;&#24615;&#36136;&#65292;&#25552;&#20379;&#20854;&#36136;&#37327;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#30340;&#35299;&#12290;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#28789;&#27963;&#22320;&#37197;&#32622;&#25968;&#25454;&#36873;&#25321;&#26631;&#20934;&#65292;&#21253;&#25324;&#20351;&#29992;&#20197;&#21069;&#30340;&#20027;&#21160;&#23398;&#20064;&#30740;&#31350;&#20013;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20998;&#31867;&#21644;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
When dealing with deep neural network (DNN) applications on edge devices, continuously updating the model is important. Although updating a model with real incoming data is ideal, using all of them is not always feasible due to limits, such as labeling and communication costs. Thus, it is necessary to filter and select the data to use for training (i.e., active learning) on the device. In this paper, we formalize a practical active learning problem for DNNs on edge devices and propose a general task-agnostic framework to tackle this problem, which reduces it to a stream submodular maximization. This framework is light enough to be run with low computational resources, yet provides solutions whose quality is theoretically guaranteed thanks to the submodular property. Through this framework, we can configure data selection criteria flexibly, including using methods proposed in previous active learning studies. We evaluate our approach on both classification and object detection tasks in 
&lt;/p&gt;</description></item><item><title>RoBIC&#26159;&#19968;&#20010;&#26080;&#21442;&#25968;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#29992;&#20110;&#20844;&#27491;&#22320;&#35780;&#20272;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#29420;&#31435;&#20110;&#20934;&#30830;&#24615;&#65292;&#24182;&#21487;&#20197;&#35780;&#20272;&#20854;&#23545;&#30333;&#30418;&#21644;&#40657;&#30418;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2102.05368</link><description>&lt;p&gt;
RoBIC&#65306;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20998;&#31867;&#22120;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
RoBIC: A benchmark suite for assessing classifiers robustness. (arXiv:2102.05368v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.05368
&lt;/p&gt;
&lt;p&gt;
RoBIC&#26159;&#19968;&#20010;&#26080;&#21442;&#25968;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#29992;&#20110;&#20844;&#27491;&#22320;&#35780;&#20272;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#65292;&#29420;&#31435;&#20110;&#20934;&#30830;&#24615;&#65292;&#24182;&#21487;&#20197;&#35780;&#20272;&#20854;&#23545;&#30333;&#30418;&#21644;&#40657;&#30418;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#21457;&#23637;&#65292;&#35768;&#22810;&#38450;&#24481;&#25514;&#26045;&#24050;&#32463;&#28044;&#29616;&#20986;&#26469;&#12290;&#22240;&#27492;&#65292;&#24517;&#39035;&#26377;&#30456;&#24212;&#30340;&#23458;&#35266;&#35780;&#20272;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#26080;&#21442;&#25968;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;RoBIC&#26469;&#31995;&#32479;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;RoBIC&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#21322;&#25197;&#26354;&#24230;&#37327;&#26041;&#27861;&#65292;&#20844;&#27491;&#22320;&#35780;&#20272;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#23427;&#29420;&#31435;&#20110;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#65292;&#35780;&#20272;&#20854;&#23545;&#30333;&#30418;&#21644;&#40657;&#30418;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;RoBIC&#27604;&#20854;&#20182;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#26356;&#24555;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;16&#20010;&#26368;&#36817;&#27169;&#22411;&#22312;RoBIC&#35780;&#20272;&#19979;&#40065;&#26834;&#24615;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many defenses have emerged with the development of adversarial attacks. Models must be objectively evaluated accordingly. This paper systematically tackles this concern by proposing a new parameter-free benchmark we coin RoBIC. RoBIC fairly evaluates the robustness of image classifiers using a new half-distortion measure. It gauges the robustness of the network against white and black box attacks, independently of its accuracy. RoBIC is faster than the other available benchmarks. We present the significant differences in the robustness of 16 recent models as assessed by RoBIC.
&lt;/p&gt;</description></item></channel></rss>